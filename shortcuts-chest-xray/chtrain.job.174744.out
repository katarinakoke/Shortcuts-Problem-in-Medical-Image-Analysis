Running on desktop22:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
1
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_val.csv",
    "pred_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Balanced_Sex_0_0.csv",
    "pred_model": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Balanced_Sex_0_01.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.05,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-14 12:05:30, Train, Epoch : 1, Step : 10, Loss : 0.64349, Acc : 0.616, Sensitive_Loss : 1.13761, Sensitive_Acc : 17.200, Run Time : 9.45 sec
INFO:root:2024-04-14 12:05:37, Train, Epoch : 1, Step : 20, Loss : 0.67042, Acc : 0.634, Sensitive_Loss : 1.07521, Sensitive_Acc : 16.100, Run Time : 6.82 sec
INFO:root:2024-04-14 12:05:44, Train, Epoch : 1, Step : 30, Loss : 0.72305, Acc : 0.594, Sensitive_Loss : 1.05297, Sensitive_Acc : 10.800, Run Time : 7.27 sec
INFO:root:2024-04-14 12:05:51, Train, Epoch : 1, Step : 40, Loss : 0.68745, Acc : 0.653, Sensitive_Loss : 0.87246, Sensitive_Acc : 18.300, Run Time : 7.28 sec
INFO:root:2024-04-14 12:05:58, Train, Epoch : 1, Step : 50, Loss : 0.61921, Acc : 0.669, Sensitive_Loss : 0.93113, Sensitive_Acc : 18.200, Run Time : 6.86 sec
INFO:root:2024-04-14 12:06:05, Train, Epoch : 1, Step : 60, Loss : 0.68855, Acc : 0.644, Sensitive_Loss : 0.97243, Sensitive_Acc : 17.300, Run Time : 7.29 sec
INFO:root:2024-04-14 12:06:12, Train, Epoch : 1, Step : 70, Loss : 0.64360, Acc : 0.637, Sensitive_Loss : 0.88327, Sensitive_Acc : 17.500, Run Time : 7.01 sec
INFO:root:2024-04-14 12:06:20, Train, Epoch : 1, Step : 80, Loss : 0.64527, Acc : 0.688, Sensitive_Loss : 0.86256, Sensitive_Acc : 23.700, Run Time : 7.10 sec
INFO:root:2024-04-14 12:06:27, Train, Epoch : 1, Step : 90, Loss : 0.61295, Acc : 0.703, Sensitive_Loss : 0.77934, Sensitive_Acc : 22.300, Run Time : 7.08 sec
INFO:root:2024-04-14 12:06:34, Train, Epoch : 1, Step : 100, Loss : 0.68490, Acc : 0.681, Sensitive_Loss : 0.75406, Sensitive_Acc : 20.700, Run Time : 7.46 sec
INFO:root:2024-04-14 12:08:03, Dev, Step : 100, Loss : 0.73865, Acc : 0.609, Auc : 0.719, Sensitive_Loss : 0.81538, Sensitive_Acc : 15.932, Sensitive_Auc : 0.831, Mean auc: 0.719, Run Time : 88.47 sec
INFO:root:2024-04-14 12:08:03, Best, Step : 100, Loss : 0.73865, Acc : 0.609, Auc : 0.719, Sensitive_Loss : 0.81538, Sensitive_Acc : 15.932, Sensitive_Auc : 0.831, Best Auc : 0.719
INFO:root:2024-04-14 12:08:09, Train, Epoch : 1, Step : 110, Loss : 0.67839, Acc : 0.681, Sensitive_Loss : 0.80583, Sensitive_Acc : 15.500, Run Time : 94.81 sec
INFO:root:2024-04-14 12:08:16, Train, Epoch : 1, Step : 120, Loss : 0.56341, Acc : 0.697, Sensitive_Loss : 0.67989, Sensitive_Acc : 17.100, Run Time : 7.10 sec
INFO:root:2024-04-14 12:08:23, Train, Epoch : 1, Step : 130, Loss : 0.58417, Acc : 0.713, Sensitive_Loss : 0.63996, Sensitive_Acc : 21.600, Run Time : 7.06 sec
INFO:root:2024-04-14 12:08:30, Train, Epoch : 1, Step : 140, Loss : 0.63017, Acc : 0.669, Sensitive_Loss : 0.66598, Sensitive_Acc : 23.800, Run Time : 7.29 sec
INFO:root:2024-04-14 12:08:37, Train, Epoch : 1, Step : 150, Loss : 0.60765, Acc : 0.666, Sensitive_Loss : 0.67454, Sensitive_Acc : 19.000, Run Time : 6.91 sec
INFO:root:2024-04-14 12:08:44, Train, Epoch : 1, Step : 160, Loss : 0.58092, Acc : 0.684, Sensitive_Loss : 0.73021, Sensitive_Acc : 18.100, Run Time : 7.13 sec
INFO:root:2024-04-14 12:08:52, Train, Epoch : 1, Step : 170, Loss : 0.64521, Acc : 0.688, Sensitive_Loss : 0.58554, Sensitive_Acc : 24.300, Run Time : 7.19 sec
INFO:root:2024-04-14 12:08:59, Train, Epoch : 1, Step : 180, Loss : 0.56017, Acc : 0.703, Sensitive_Loss : 0.57302, Sensitive_Acc : 23.300, Run Time : 7.29 sec
INFO:root:2024-04-14 12:09:06, Train, Epoch : 1, Step : 190, Loss : 0.59920, Acc : 0.691, Sensitive_Loss : 0.60778, Sensitive_Acc : 20.000, Run Time : 6.90 sec
INFO:root:2024-04-14 12:09:13, Train, Epoch : 1, Step : 200, Loss : 0.63261, Acc : 0.666, Sensitive_Loss : 0.54295, Sensitive_Acc : 23.600, Run Time : 7.27 sec
INFO:root:2024-04-14 12:10:41, Dev, Step : 200, Loss : 0.69081, Acc : 0.649, Auc : 0.738, Sensitive_Loss : 0.52367, Sensitive_Acc : 20.444, Sensitive_Auc : 0.888, Mean auc: 0.738, Run Time : 87.63 sec
INFO:root:2024-04-14 12:10:41, Best, Step : 200, Loss : 0.69081, Acc : 0.649, Auc : 0.738, Sensitive_Loss : 0.52367, Sensitive_Acc : 20.444, Sensitive_Auc : 0.888, Best Auc : 0.738
INFO:root:2024-04-14 12:10:47, Train, Epoch : 1, Step : 210, Loss : 0.62525, Acc : 0.631, Sensitive_Loss : 0.54892, Sensitive_Acc : 20.900, Run Time : 93.74 sec
INFO:root:2024-04-14 12:10:54, Train, Epoch : 1, Step : 220, Loss : 0.61584, Acc : 0.713, Sensitive_Loss : 0.61655, Sensitive_Acc : 20.600, Run Time : 7.24 sec
INFO:root:2024-04-14 12:11:01, Train, Epoch : 1, Step : 230, Loss : 0.65289, Acc : 0.659, Sensitive_Loss : 0.65856, Sensitive_Acc : 22.100, Run Time : 7.14 sec
INFO:root:2024-04-14 12:11:08, Train, Epoch : 1, Step : 240, Loss : 0.66574, Acc : 0.672, Sensitive_Loss : 0.58482, Sensitive_Acc : 19.000, Run Time : 6.67 sec
INFO:root:2024-04-14 12:11:15, Train, Epoch : 1, Step : 250, Loss : 0.56368, Acc : 0.728, Sensitive_Loss : 0.47664, Sensitive_Acc : 19.000, Run Time : 7.51 sec
INFO:root:2024-04-14 12:11:22, Train, Epoch : 1, Step : 260, Loss : 0.57074, Acc : 0.700, Sensitive_Loss : 0.67818, Sensitive_Acc : 18.900, Run Time : 6.97 sec
INFO:root:2024-04-14 12:11:29, Train, Epoch : 1, Step : 270, Loss : 0.55586, Acc : 0.709, Sensitive_Loss : 0.53081, Sensitive_Acc : 23.600, Run Time : 7.18 sec
INFO:root:2024-04-14 12:11:37, Train, Epoch : 1, Step : 280, Loss : 0.57716, Acc : 0.694, Sensitive_Loss : 0.51407, Sensitive_Acc : 22.200, Run Time : 7.29 sec
INFO:root:2024-04-14 12:11:43, Train, Epoch : 1, Step : 290, Loss : 0.55477, Acc : 0.741, Sensitive_Loss : 0.53003, Sensitive_Acc : 14.700, Run Time : 6.56 sec
INFO:root:2024-04-14 12:11:51, Train, Epoch : 1, Step : 300, Loss : 0.57461, Acc : 0.691, Sensitive_Loss : 0.53716, Sensitive_Acc : 20.400, Run Time : 7.35 sec
INFO:root:2024-04-14 12:13:18, Dev, Step : 300, Loss : 0.64815, Acc : 0.660, Auc : 0.763, Sensitive_Loss : 0.45523, Sensitive_Acc : 20.609, Sensitive_Auc : 0.948, Mean auc: 0.763, Run Time : 87.78 sec
INFO:root:2024-04-14 12:13:19, Best, Step : 300, Loss : 0.64815, Acc : 0.660, Auc : 0.763, Sensitive_Loss : 0.45523, Sensitive_Acc : 20.609, Sensitive_Auc : 0.948, Best Auc : 0.763
INFO:root:2024-04-14 12:13:25, Train, Epoch : 1, Step : 310, Loss : 0.64686, Acc : 0.675, Sensitive_Loss : 0.46984, Sensitive_Acc : 18.300, Run Time : 93.98 sec
INFO:root:2024-04-14 12:13:32, Train, Epoch : 1, Step : 320, Loss : 0.54750, Acc : 0.703, Sensitive_Loss : 0.32564, Sensitive_Acc : 23.000, Run Time : 7.01 sec
INFO:root:2024-04-14 12:13:39, Train, Epoch : 1, Step : 330, Loss : 0.55963, Acc : 0.713, Sensitive_Loss : 0.41364, Sensitive_Acc : 15.400, Run Time : 7.54 sec
INFO:root:2024-04-14 12:13:46, Train, Epoch : 1, Step : 340, Loss : 0.53262, Acc : 0.709, Sensitive_Loss : 0.32598, Sensitive_Acc : 24.000, Run Time : 7.02 sec
INFO:root:2024-04-14 12:13:53, Train, Epoch : 1, Step : 350, Loss : 0.61277, Acc : 0.681, Sensitive_Loss : 0.49404, Sensitive_Acc : 25.800, Run Time : 7.02 sec
INFO:root:2024-04-14 12:14:01, Train, Epoch : 1, Step : 360, Loss : 0.60587, Acc : 0.684, Sensitive_Loss : 0.37686, Sensitive_Acc : 16.100, Run Time : 7.33 sec
INFO:root:2024-04-14 12:14:07, Train, Epoch : 1, Step : 370, Loss : 0.62053, Acc : 0.678, Sensitive_Loss : 0.40869, Sensitive_Acc : 16.400, Run Time : 6.60 sec
INFO:root:2024-04-14 12:14:14, Train, Epoch : 1, Step : 380, Loss : 0.62848, Acc : 0.669, Sensitive_Loss : 0.45014, Sensitive_Acc : 20.600, Run Time : 7.18 sec
INFO:root:2024-04-14 12:14:21, Train, Epoch : 1, Step : 390, Loss : 0.51718, Acc : 0.756, Sensitive_Loss : 0.50918, Sensitive_Acc : 25.200, Run Time : 7.10 sec
INFO:root:2024-04-14 12:14:29, Train, Epoch : 1, Step : 400, Loss : 0.55935, Acc : 0.762, Sensitive_Loss : 0.47052, Sensitive_Acc : 19.000, Run Time : 7.40 sec
INFO:root:2024-04-14 12:15:57, Dev, Step : 400, Loss : 0.57227, Acc : 0.736, Auc : 0.808, Sensitive_Loss : 0.73852, Sensitive_Acc : 16.579, Sensitive_Auc : 0.969, Mean auc: 0.808, Run Time : 87.69 sec
INFO:root:2024-04-14 12:15:57, Best, Step : 400, Loss : 0.57227, Acc : 0.736, Auc : 0.808, Sensitive_Loss : 0.73852, Sensitive_Acc : 16.579, Sensitive_Auc : 0.969, Best Auc : 0.808
INFO:root:2024-04-14 12:16:02, Train, Epoch : 1, Step : 410, Loss : 0.63874, Acc : 0.691, Sensitive_Loss : 0.34665, Sensitive_Acc : 20.800, Run Time : 93.42 sec
INFO:root:2024-04-14 12:16:10, Train, Epoch : 1, Step : 420, Loss : 0.59708, Acc : 0.697, Sensitive_Loss : 0.40752, Sensitive_Acc : 21.500, Run Time : 7.63 sec
INFO:root:2024-04-14 12:16:17, Train, Epoch : 1, Step : 430, Loss : 0.51256, Acc : 0.756, Sensitive_Loss : 0.37389, Sensitive_Acc : 18.600, Run Time : 6.82 sec
INFO:root:2024-04-14 12:16:24, Train, Epoch : 1, Step : 440, Loss : 0.61577, Acc : 0.697, Sensitive_Loss : 0.46098, Sensitive_Acc : 16.500, Run Time : 7.40 sec
INFO:root:2024-04-14 12:16:32, Train, Epoch : 1, Step : 450, Loss : 0.57395, Acc : 0.697, Sensitive_Loss : 0.39283, Sensitive_Acc : 21.000, Run Time : 7.45 sec
INFO:root:2024-04-14 12:16:38, Train, Epoch : 1, Step : 460, Loss : 0.59378, Acc : 0.706, Sensitive_Loss : 0.41884, Sensitive_Acc : 26.800, Run Time : 6.70 sec
INFO:root:2024-04-14 12:16:46, Train, Epoch : 1, Step : 470, Loss : 0.49759, Acc : 0.700, Sensitive_Loss : 0.46114, Sensitive_Acc : 20.000, Run Time : 7.25 sec
INFO:root:2024-04-14 12:16:53, Train, Epoch : 1, Step : 480, Loss : 0.56679, Acc : 0.656, Sensitive_Loss : 0.47091, Sensitive_Acc : 21.600, Run Time : 7.39 sec
INFO:root:2024-04-14 12:17:00, Train, Epoch : 1, Step : 490, Loss : 0.60601, Acc : 0.709, Sensitive_Loss : 0.41671, Sensitive_Acc : 15.500, Run Time : 6.83 sec
INFO:root:2024-04-14 12:17:07, Train, Epoch : 1, Step : 500, Loss : 0.54712, Acc : 0.747, Sensitive_Loss : 0.36124, Sensitive_Acc : 21.200, Run Time : 6.99 sec
INFO:root:2024-04-14 12:18:34, Dev, Step : 500, Loss : 0.57707, Acc : 0.730, Auc : 0.802, Sensitive_Loss : 0.50763, Sensitive_Acc : 18.805, Sensitive_Auc : 0.951, Mean auc: 0.802, Run Time : 87.76 sec
INFO:root:2024-04-14 12:18:40, Train, Epoch : 1, Step : 510, Loss : 0.54301, Acc : 0.731, Sensitive_Loss : 0.38399, Sensitive_Acc : 23.400, Run Time : 93.02 sec
INFO:root:2024-04-14 12:18:47, Train, Epoch : 1, Step : 520, Loss : 0.50644, Acc : 0.731, Sensitive_Loss : 0.36965, Sensitive_Acc : 22.400, Run Time : 7.53 sec
INFO:root:2024-04-14 12:18:54, Train, Epoch : 1, Step : 530, Loss : 0.59370, Acc : 0.722, Sensitive_Loss : 0.36306, Sensitive_Acc : 21.200, Run Time : 7.04 sec
INFO:root:2024-04-14 12:19:02, Train, Epoch : 1, Step : 540, Loss : 0.54469, Acc : 0.731, Sensitive_Loss : 0.33926, Sensitive_Acc : 17.700, Run Time : 7.64 sec
INFO:root:2024-04-14 12:19:08, Train, Epoch : 1, Step : 550, Loss : 0.54181, Acc : 0.734, Sensitive_Loss : 0.36056, Sensitive_Acc : 20.100, Run Time : 6.42 sec
INFO:root:2024-04-14 12:19:16, Train, Epoch : 1, Step : 560, Loss : 0.54983, Acc : 0.744, Sensitive_Loss : 0.34527, Sensitive_Acc : 15.700, Run Time : 7.36 sec
INFO:root:2024-04-14 12:19:23, Train, Epoch : 1, Step : 570, Loss : 0.57762, Acc : 0.731, Sensitive_Loss : 0.44876, Sensitive_Acc : 17.000, Run Time : 6.96 sec
INFO:root:2024-04-14 12:19:30, Train, Epoch : 1, Step : 580, Loss : 0.52641, Acc : 0.738, Sensitive_Loss : 0.41260, Sensitive_Acc : 24.100, Run Time : 7.07 sec
INFO:root:2024-04-14 12:19:37, Train, Epoch : 1, Step : 590, Loss : 0.52480, Acc : 0.756, Sensitive_Loss : 0.28746, Sensitive_Acc : 19.900, Run Time : 7.19 sec
INFO:root:2024-04-14 12:19:44, Train, Epoch : 1, Step : 600, Loss : 0.55359, Acc : 0.731, Sensitive_Loss : 0.30298, Sensitive_Acc : 22.000, Run Time : 7.45 sec
INFO:root:2024-04-14 12:21:12, Dev, Step : 600, Loss : 0.57481, Acc : 0.729, Auc : 0.802, Sensitive_Loss : 0.35906, Sensitive_Acc : 20.158, Sensitive_Auc : 0.964, Mean auc: 0.802, Run Time : 87.27 sec
INFO:root:2024-04-14 12:21:17, Train, Epoch : 1, Step : 610, Loss : 0.54616, Acc : 0.681, Sensitive_Loss : 0.32341, Sensitive_Acc : 23.900, Run Time : 93.01 sec
INFO:root:2024-04-14 12:21:24, Train, Epoch : 1, Step : 620, Loss : 0.56169, Acc : 0.722, Sensitive_Loss : 0.34307, Sensitive_Acc : 25.900, Run Time : 6.98 sec
INFO:root:2024-04-14 12:21:32, Train, Epoch : 1, Step : 630, Loss : 0.53581, Acc : 0.731, Sensitive_Loss : 0.39277, Sensitive_Acc : 24.300, Run Time : 7.17 sec
INFO:root:2024-04-14 12:23:00
INFO:root:y_pred: [0.46291712 0.01541526 0.48870242 ... 0.27007344 0.042569   0.22252943]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.68841593e-02 8.52677450e-02 1.28243044e-02 4.23089601e-02
 9.18294583e-03 1.11328855e-01 1.42521383e-02 3.41662057e-02
 2.99346773e-03 9.99137104e-01 4.21348333e-01 3.14101838e-02
 2.10629050e-02 2.15102197e-03 9.97453511e-01 8.05280954e-02
 4.29422110e-01 9.99032736e-01 9.98808742e-01 3.75495516e-02
 9.29933846e-01 2.38382537e-03 6.61412597e-01 1.88514158e-01
 1.04680158e-01 2.02675551e-01 3.01152142e-03 1.31380018e-02
 2.39496380e-02 3.75430012e-04 1.73305139e-01 9.98296916e-01
 1.57149732e-01 7.90431976e-01 1.34785962e-03 1.04207816e-02
 6.08077738e-03 8.86207446e-02 2.00167354e-02 1.58929169e-01
 1.99936613e-01 9.42437947e-01 4.75250408e-02 2.60289367e-02
 9.98811364e-01 8.57511699e-01 9.49626684e-01 3.89058471e-01
 5.93075335e-01 9.75962579e-01 9.54404414e-01 9.98538613e-01
 9.99013782e-01 7.79013615e-03 1.67001873e-01 8.98314655e-01
 2.33755309e-05 5.28149545e-01 9.98964787e-01 3.68798636e-02
 1.99853107e-02 1.27628045e-02 8.44508316e-03 7.82619521e-04
 9.92094815e-01 2.30385456e-02 4.03674506e-03 4.56535906e-01
 1.95638131e-04 9.74761546e-01 9.99792755e-01 9.99796450e-01
 6.46160185e-01 9.15748000e-01 1.84881929e-02 5.42739153e-01
 1.18917584e-01 6.64594630e-03 8.10064655e-03 7.48181995e-03
 8.65748078e-02 9.39185824e-03 6.66694522e-01 9.74148393e-01
 1.01984791e-01 1.51390985e-01 1.19568631e-01 1.99307855e-02
 5.11125922e-01 7.54610170e-03 3.82147543e-02 9.41502750e-01
 1.76554767e-03 1.93290096e-02 2.15740129e-01 1.05559081e-01
 6.41672546e-03 1.13450103e-02 9.28763580e-03 4.42795865e-02
 4.23209816e-02 2.21268870e-02 5.04261792e-01 4.63809483e-02
 5.19981459e-02 2.34605800e-02 6.65471971e-01 7.15645909e-01
 2.68108845e-01 2.75567710e-01 4.99112299e-03 9.98865485e-01
 9.99937057e-01 3.82775697e-03 7.48414099e-02 8.10387611e-01
 2.20287547e-01 4.59264033e-02 2.11578552e-02 2.57622391e-01
 4.62969951e-02 3.34308459e-03 9.20646861e-02 5.23580844e-03
 7.71143883e-02 2.45636314e-01 2.00136378e-03 9.97636914e-01
 3.49432737e-01 2.49607727e-01 3.04958783e-02 6.33036971e-01
 9.27299669e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-14 12:23:00, Dev, Step : 634, Loss : 0.66438, Acc : 0.692, Auc : 0.771, Sensitive_Loss : 0.36732, Sensitive_Acc : 19.977, Sensitive_Auc : 0.963, Mean auc: 0.771, Run Time : 85.85 sec
INFO:root:2024-04-14 12:23:06, Train, Epoch : 2, Step : 640, Loss : 0.34409, Acc : 0.450, Sensitive_Loss : 0.18149, Sensitive_Acc : 15.300, Run Time : 5.44 sec
INFO:root:2024-04-14 12:23:13, Train, Epoch : 2, Step : 650, Loss : 0.49288, Acc : 0.738, Sensitive_Loss : 0.28026, Sensitive_Acc : 21.600, Run Time : 6.68 sec
INFO:root:2024-04-14 12:23:20, Train, Epoch : 2, Step : 660, Loss : 0.57583, Acc : 0.747, Sensitive_Loss : 0.38537, Sensitive_Acc : 15.000, Run Time : 7.05 sec
INFO:root:2024-04-14 12:23:27, Train, Epoch : 2, Step : 670, Loss : 0.50939, Acc : 0.766, Sensitive_Loss : 0.32486, Sensitive_Acc : 25.300, Run Time : 7.14 sec
INFO:root:2024-04-14 12:23:34, Train, Epoch : 2, Step : 680, Loss : 0.52881, Acc : 0.769, Sensitive_Loss : 0.36914, Sensitive_Acc : 20.100, Run Time : 6.86 sec
INFO:root:2024-04-14 12:23:41, Train, Epoch : 2, Step : 690, Loss : 0.47870, Acc : 0.741, Sensitive_Loss : 0.29057, Sensitive_Acc : 19.700, Run Time : 7.40 sec
INFO:root:2024-04-14 12:23:48, Train, Epoch : 2, Step : 700, Loss : 0.63723, Acc : 0.694, Sensitive_Loss : 0.23498, Sensitive_Acc : 21.600, Run Time : 7.00 sec
INFO:root:2024-04-14 12:25:18, Dev, Step : 700, Loss : 0.59262, Acc : 0.726, Auc : 0.815, Sensitive_Loss : 0.36605, Sensitive_Acc : 20.053, Sensitive_Auc : 0.967, Mean auc: 0.815, Run Time : 90.37 sec
INFO:root:2024-04-14 12:25:19, Best, Step : 700, Loss : 0.59262, Acc : 0.726, Auc : 0.815, Sensitive_Loss : 0.36605, Sensitive_Acc : 20.053, Sensitive_Auc : 0.967, Best Auc : 0.815
INFO:root:2024-04-14 12:25:24, Train, Epoch : 2, Step : 710, Loss : 0.55831, Acc : 0.756, Sensitive_Loss : 0.28155, Sensitive_Acc : 22.100, Run Time : 96.29 sec
INFO:root:2024-04-14 12:25:32, Train, Epoch : 2, Step : 720, Loss : 0.54405, Acc : 0.747, Sensitive_Loss : 0.30589, Sensitive_Acc : 23.400, Run Time : 7.12 sec
INFO:root:2024-04-14 12:25:39, Train, Epoch : 2, Step : 730, Loss : 0.54516, Acc : 0.719, Sensitive_Loss : 0.29643, Sensitive_Acc : 21.200, Run Time : 7.18 sec
INFO:root:2024-04-14 12:25:46, Train, Epoch : 2, Step : 740, Loss : 0.48959, Acc : 0.753, Sensitive_Loss : 0.19536, Sensitive_Acc : 15.900, Run Time : 7.52 sec
INFO:root:2024-04-14 12:25:53, Train, Epoch : 2, Step : 750, Loss : 0.48120, Acc : 0.787, Sensitive_Loss : 0.24465, Sensitive_Acc : 21.100, Run Time : 6.61 sec
INFO:root:2024-04-14 12:26:00, Train, Epoch : 2, Step : 760, Loss : 0.56608, Acc : 0.747, Sensitive_Loss : 0.32221, Sensitive_Acc : 22.300, Run Time : 6.92 sec
INFO:root:2024-04-14 12:26:07, Train, Epoch : 2, Step : 770, Loss : 0.53268, Acc : 0.753, Sensitive_Loss : 0.36277, Sensitive_Acc : 21.500, Run Time : 7.32 sec
INFO:root:2024-04-14 12:26:14, Train, Epoch : 2, Step : 780, Loss : 0.58914, Acc : 0.738, Sensitive_Loss : 0.31832, Sensitive_Acc : 22.000, Run Time : 7.21 sec
INFO:root:2024-04-14 12:26:22, Train, Epoch : 2, Step : 790, Loss : 0.51428, Acc : 0.728, Sensitive_Loss : 0.32538, Sensitive_Acc : 20.000, Run Time : 7.25 sec
INFO:root:2024-04-14 12:26:28, Train, Epoch : 2, Step : 800, Loss : 0.49817, Acc : 0.728, Sensitive_Loss : 0.30260, Sensitive_Acc : 20.500, Run Time : 6.67 sec
INFO:root:2024-04-14 12:27:55, Dev, Step : 800, Loss : 0.57981, Acc : 0.740, Auc : 0.819, Sensitive_Loss : 0.35206, Sensitive_Acc : 20.880, Sensitive_Auc : 0.974, Mean auc: 0.819, Run Time : 86.81 sec
INFO:root:2024-04-14 12:27:56, Best, Step : 800, Loss : 0.57981, Acc : 0.740, Auc : 0.819, Sensitive_Loss : 0.35206, Sensitive_Acc : 20.880, Sensitive_Auc : 0.974, Best Auc : 0.819
INFO:root:2024-04-14 12:28:01, Train, Epoch : 2, Step : 810, Loss : 0.55605, Acc : 0.741, Sensitive_Loss : 0.40994, Sensitive_Acc : 23.600, Run Time : 92.78 sec
INFO:root:2024-04-14 12:28:10, Train, Epoch : 2, Step : 820, Loss : 0.55238, Acc : 0.750, Sensitive_Loss : 0.34955, Sensitive_Acc : 24.500, Run Time : 8.74 sec
INFO:root:2024-04-14 12:28:18, Train, Epoch : 2, Step : 830, Loss : 0.50714, Acc : 0.744, Sensitive_Loss : 0.23778, Sensitive_Acc : 22.700, Run Time : 8.47 sec
INFO:root:2024-04-14 12:28:25, Train, Epoch : 2, Step : 840, Loss : 0.52114, Acc : 0.734, Sensitive_Loss : 0.42289, Sensitive_Acc : 21.900, Run Time : 7.30 sec
INFO:root:2024-04-14 12:28:32, Train, Epoch : 2, Step : 850, Loss : 0.57172, Acc : 0.709, Sensitive_Loss : 0.39252, Sensitive_Acc : 22.300, Run Time : 6.44 sec
INFO:root:2024-04-14 12:28:39, Train, Epoch : 2, Step : 860, Loss : 0.53764, Acc : 0.728, Sensitive_Loss : 0.27194, Sensitive_Acc : 20.600, Run Time : 7.19 sec
INFO:root:2024-04-14 12:28:47, Train, Epoch : 2, Step : 870, Loss : 0.53617, Acc : 0.725, Sensitive_Loss : 0.30492, Sensitive_Acc : 22.900, Run Time : 7.71 sec
INFO:root:2024-04-14 12:28:54, Train, Epoch : 2, Step : 880, Loss : 0.57314, Acc : 0.759, Sensitive_Loss : 0.43723, Sensitive_Acc : 26.300, Run Time : 6.79 sec
INFO:root:2024-04-14 12:29:00, Train, Epoch : 2, Step : 890, Loss : 0.53376, Acc : 0.709, Sensitive_Loss : 0.21223, Sensitive_Acc : 23.400, Run Time : 6.62 sec
INFO:root:2024-04-14 12:29:08, Train, Epoch : 2, Step : 900, Loss : 0.44224, Acc : 0.794, Sensitive_Loss : 0.28468, Sensitive_Acc : 20.400, Run Time : 7.46 sec
INFO:root:2024-04-14 12:30:35, Dev, Step : 900, Loss : 0.56182, Acc : 0.754, Auc : 0.829, Sensitive_Loss : 0.29608, Sensitive_Acc : 20.820, Sensitive_Auc : 0.981, Mean auc: 0.829, Run Time : 87.09 sec
INFO:root:2024-04-14 12:30:35, Best, Step : 900, Loss : 0.56182, Acc : 0.754, Auc : 0.829, Sensitive_Loss : 0.29608, Sensitive_Acc : 20.820, Sensitive_Auc : 0.981, Best Auc : 0.829
INFO:root:2024-04-14 12:30:41, Train, Epoch : 2, Step : 910, Loss : 0.45334, Acc : 0.766, Sensitive_Loss : 0.36087, Sensitive_Acc : 19.200, Run Time : 93.50 sec
INFO:root:2024-04-14 12:30:48, Train, Epoch : 2, Step : 920, Loss : 0.55428, Acc : 0.759, Sensitive_Loss : 0.24786, Sensitive_Acc : 20.200, Run Time : 6.72 sec
INFO:root:2024-04-14 12:30:55, Train, Epoch : 2, Step : 930, Loss : 0.53080, Acc : 0.747, Sensitive_Loss : 0.34940, Sensitive_Acc : 19.300, Run Time : 7.38 sec
INFO:root:2024-04-14 12:31:02, Train, Epoch : 2, Step : 940, Loss : 0.53080, Acc : 0.747, Sensitive_Loss : 0.32226, Sensitive_Acc : 21.700, Run Time : 7.01 sec
INFO:root:2024-04-14 12:31:09, Train, Epoch : 2, Step : 950, Loss : 0.47469, Acc : 0.772, Sensitive_Loss : 0.24479, Sensitive_Acc : 18.200, Run Time : 6.66 sec
INFO:root:2024-04-14 12:31:17, Train, Epoch : 2, Step : 960, Loss : 0.50690, Acc : 0.747, Sensitive_Loss : 0.23747, Sensitive_Acc : 22.800, Run Time : 7.88 sec
INFO:root:2024-04-14 12:31:24, Train, Epoch : 2, Step : 970, Loss : 0.51320, Acc : 0.784, Sensitive_Loss : 0.27090, Sensitive_Acc : 21.900, Run Time : 6.97 sec
INFO:root:2024-04-14 12:31:31, Train, Epoch : 2, Step : 980, Loss : 0.54318, Acc : 0.722, Sensitive_Loss : 0.26655, Sensitive_Acc : 20.600, Run Time : 7.09 sec
INFO:root:2024-04-14 12:31:38, Train, Epoch : 2, Step : 990, Loss : 0.52210, Acc : 0.747, Sensitive_Loss : 0.25389, Sensitive_Acc : 21.700, Run Time : 7.19 sec
INFO:root:2024-04-14 12:31:45, Train, Epoch : 2, Step : 1000, Loss : 0.50919, Acc : 0.722, Sensitive_Loss : 0.30471, Sensitive_Acc : 22.900, Run Time : 7.11 sec
INFO:root:2024-04-14 12:33:16, Dev, Step : 1000, Loss : 0.57625, Acc : 0.741, Auc : 0.828, Sensitive_Loss : 0.46687, Sensitive_Acc : 18.744, Sensitive_Auc : 0.990, Mean auc: 0.828, Run Time : 90.56 sec
INFO:root:2024-04-14 12:33:21, Train, Epoch : 2, Step : 1010, Loss : 0.53683, Acc : 0.769, Sensitive_Loss : 0.35669, Sensitive_Acc : 18.000, Run Time : 96.03 sec
INFO:root:2024-04-14 12:33:28, Train, Epoch : 2, Step : 1020, Loss : 0.54135, Acc : 0.734, Sensitive_Loss : 0.19604, Sensitive_Acc : 22.500, Run Time : 6.83 sec
INFO:root:2024-04-14 12:33:35, Train, Epoch : 2, Step : 1030, Loss : 0.55277, Acc : 0.759, Sensitive_Loss : 0.28746, Sensitive_Acc : 19.600, Run Time : 7.19 sec
INFO:root:2024-04-14 12:33:42, Train, Epoch : 2, Step : 1040, Loss : 0.49649, Acc : 0.766, Sensitive_Loss : 0.24742, Sensitive_Acc : 23.200, Run Time : 7.14 sec
INFO:root:2024-04-14 12:33:49, Train, Epoch : 2, Step : 1050, Loss : 0.53338, Acc : 0.719, Sensitive_Loss : 0.25904, Sensitive_Acc : 20.100, Run Time : 7.01 sec
INFO:root:2024-04-14 12:33:57, Train, Epoch : 2, Step : 1060, Loss : 0.54142, Acc : 0.778, Sensitive_Loss : 0.36867, Sensitive_Acc : 19.200, Run Time : 7.28 sec
INFO:root:2024-04-14 12:34:03, Train, Epoch : 2, Step : 1070, Loss : 0.50175, Acc : 0.766, Sensitive_Loss : 0.29526, Sensitive_Acc : 21.100, Run Time : 6.71 sec
INFO:root:2024-04-14 12:34:10, Train, Epoch : 2, Step : 1080, Loss : 0.51449, Acc : 0.756, Sensitive_Loss : 0.20292, Sensitive_Acc : 24.100, Run Time : 6.98 sec
INFO:root:2024-04-14 12:34:18, Train, Epoch : 2, Step : 1090, Loss : 0.56457, Acc : 0.738, Sensitive_Loss : 0.19454, Sensitive_Acc : 19.500, Run Time : 7.31 sec
INFO:root:2024-04-14 12:34:25, Train, Epoch : 2, Step : 1100, Loss : 0.45616, Acc : 0.819, Sensitive_Loss : 0.24811, Sensitive_Acc : 24.300, Run Time : 7.13 sec
INFO:root:2024-04-14 12:35:52, Dev, Step : 1100, Loss : 0.54740, Acc : 0.744, Auc : 0.830, Sensitive_Loss : 0.40188, Sensitive_Acc : 19.782, Sensitive_Auc : 0.994, Mean auc: 0.830, Run Time : 87.35 sec
INFO:root:2024-04-14 12:35:53, Best, Step : 1100, Loss : 0.54740, Acc : 0.744, Auc : 0.830, Sensitive_Loss : 0.40188, Sensitive_Acc : 19.782, Sensitive_Auc : 0.994, Best Auc : 0.830
INFO:root:2024-04-14 12:35:58, Train, Epoch : 2, Step : 1110, Loss : 0.56534, Acc : 0.703, Sensitive_Loss : 0.22314, Sensitive_Acc : 22.000, Run Time : 93.27 sec
INFO:root:2024-04-14 12:36:06, Train, Epoch : 2, Step : 1120, Loss : 0.58547, Acc : 0.734, Sensitive_Loss : 0.31638, Sensitive_Acc : 19.700, Run Time : 7.57 sec
INFO:root:2024-04-14 12:36:12, Train, Epoch : 2, Step : 1130, Loss : 0.53462, Acc : 0.756, Sensitive_Loss : 0.27976, Sensitive_Acc : 23.400, Run Time : 6.76 sec
INFO:root:2024-04-14 12:36:19, Train, Epoch : 2, Step : 1140, Loss : 0.43838, Acc : 0.797, Sensitive_Loss : 0.24832, Sensitive_Acc : 23.400, Run Time : 6.90 sec
INFO:root:2024-04-14 12:36:27, Train, Epoch : 2, Step : 1150, Loss : 0.48975, Acc : 0.762, Sensitive_Loss : 0.25651, Sensitive_Acc : 23.700, Run Time : 7.37 sec
INFO:root:2024-04-14 12:36:34, Train, Epoch : 2, Step : 1160, Loss : 0.55626, Acc : 0.722, Sensitive_Loss : 0.28589, Sensitive_Acc : 21.800, Run Time : 6.96 sec
INFO:root:2024-04-14 12:36:40, Train, Epoch : 2, Step : 1170, Loss : 0.46101, Acc : 0.762, Sensitive_Loss : 0.29922, Sensitive_Acc : 18.100, Run Time : 6.60 sec
INFO:root:2024-04-14 12:36:47, Train, Epoch : 2, Step : 1180, Loss : 0.50617, Acc : 0.731, Sensitive_Loss : 0.31562, Sensitive_Acc : 24.100, Run Time : 7.14 sec
INFO:root:2024-04-14 12:36:55, Train, Epoch : 2, Step : 1190, Loss : 0.56627, Acc : 0.756, Sensitive_Loss : 0.25302, Sensitive_Acc : 21.400, Run Time : 7.31 sec
INFO:root:2024-04-14 12:37:02, Train, Epoch : 2, Step : 1200, Loss : 0.57668, Acc : 0.766, Sensitive_Loss : 0.35340, Sensitive_Acc : 18.100, Run Time : 7.00 sec
INFO:root:2024-04-14 12:38:29, Dev, Step : 1200, Loss : 0.66111, Acc : 0.694, Auc : 0.841, Sensitive_Loss : 0.49748, Sensitive_Acc : 18.519, Sensitive_Auc : 0.996, Mean auc: 0.841, Run Time : 87.42 sec
INFO:root:2024-04-14 12:38:30, Best, Step : 1200, Loss : 0.66111, Acc : 0.694, Auc : 0.841, Sensitive_Loss : 0.49748, Sensitive_Acc : 18.519, Sensitive_Auc : 0.996, Best Auc : 0.841
INFO:root:2024-04-14 12:38:35, Train, Epoch : 2, Step : 1210, Loss : 0.55331, Acc : 0.722, Sensitive_Loss : 0.22148, Sensitive_Acc : 21.100, Run Time : 93.53 sec
INFO:root:2024-04-14 12:38:43, Train, Epoch : 2, Step : 1220, Loss : 0.49812, Acc : 0.766, Sensitive_Loss : 0.22399, Sensitive_Acc : 21.900, Run Time : 7.68 sec
INFO:root:2024-04-14 12:38:50, Train, Epoch : 2, Step : 1230, Loss : 0.46443, Acc : 0.766, Sensitive_Loss : 0.34784, Sensitive_Acc : 21.800, Run Time : 6.98 sec
INFO:root:2024-04-14 12:38:57, Train, Epoch : 2, Step : 1240, Loss : 0.55470, Acc : 0.738, Sensitive_Loss : 0.20347, Sensitive_Acc : 18.800, Run Time : 6.93 sec
INFO:root:2024-04-14 12:39:04, Train, Epoch : 2, Step : 1250, Loss : 0.56412, Acc : 0.734, Sensitive_Loss : 0.25663, Sensitive_Acc : 14.200, Run Time : 6.87 sec
INFO:root:2024-04-14 12:39:11, Train, Epoch : 2, Step : 1260, Loss : 0.50838, Acc : 0.762, Sensitive_Loss : 0.26237, Sensitive_Acc : 26.800, Run Time : 7.39 sec
INFO:root:2024-04-14 12:40:43
INFO:root:y_pred: [0.1520505  0.02871016 0.0680797  ... 0.34561795 0.17011812 0.39286318]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [4.66007506e-03 3.26341018e-02 4.66388911e-02 3.27257700e-02
 4.85700928e-03 9.95868817e-02 9.64662582e-02 1.21518057e-02
 8.63539113e-04 9.99461710e-01 3.14831495e-01 8.34550057e-03
 1.04621766e-04 3.28479777e-03 9.94979322e-01 2.75637303e-02
 7.70975556e-03 9.98102486e-01 9.96244311e-01 2.47291215e-02
 9.34322774e-01 1.23035244e-03 7.59748161e-01 3.70691389e-01
 1.98544472e-01 2.04341874e-01 6.58516437e-05 1.13962412e-01
 9.63537314e-04 8.15690844e-04 7.69998059e-02 9.95414019e-01
 1.05411135e-01 7.72892475e-01 5.65604190e-04 4.80297022e-04
 4.17709583e-03 4.02713418e-02 1.72326230e-02 4.67318475e-01
 5.80875814e-01 9.48127329e-01 4.08180384e-03 1.14612198e-02
 9.81458247e-01 7.55216658e-01 4.43806410e-01 8.51587579e-02
 5.07084966e-01 9.64390397e-01 9.96440470e-01 9.95832503e-01
 9.99208391e-01 1.01500787e-02 5.27118035e-02 5.39199114e-02
 9.75527975e-04 5.72413169e-02 9.68883038e-01 9.60368738e-02
 3.62693594e-04 2.31822859e-03 1.10765733e-02 3.17470345e-04
 9.96618330e-01 8.39681700e-02 2.98169325e-04 7.05169514e-02
 5.72166136e-05 9.20026541e-01 9.99134362e-01 9.99951959e-01
 6.00327505e-04 3.66562873e-01 1.00849196e-02 1.30650207e-01
 3.37060168e-02 1.13098731e-03 6.44010201e-04 1.38851989e-03
 1.44356325e-01 3.35277291e-03 9.96042848e-01 9.95147765e-01
 1.41413147e-02 2.65217036e-01 4.26718682e-01 3.56814153e-02
 3.29893944e-03 8.14989768e-03 2.68839039e-02 5.28111279e-01
 2.13779087e-04 3.67924018e-04 1.14352629e-02 4.94600832e-02
 2.04476435e-03 3.74856442e-01 3.54349390e-02 4.65284027e-02
 1.63750593e-02 2.30026748e-02 2.13834196e-01 6.76001087e-02
 2.13749018e-02 1.39958467e-02 2.16053456e-01 6.15801334e-01
 6.18279159e-01 3.47633839e-01 2.50875164e-04 9.99010444e-01
 9.99542713e-01 3.36286728e-04 8.33358243e-02 3.88799399e-01
 6.47767708e-02 1.06856681e-03 7.80973911e-01 9.55030322e-02
 6.62858859e-02 3.38524551e-04 7.01264851e-03 6.57662097e-03
 3.65078785e-02 1.49975687e-01 9.66578955e-05 9.58405852e-01
 3.44774276e-01 1.05050787e-01 8.21533613e-03 2.15871423e-01
 2.53407488e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-14 12:40:43, Dev, Step : 1268, Loss : 0.53754, Acc : 0.743, Auc : 0.839, Sensitive_Loss : 0.25708, Sensitive_Acc : 21.045, Sensitive_Auc : 0.995, Mean auc: 0.839, Run Time : 86.60 sec
INFO:root:2024-04-14 12:40:46, Train, Epoch : 3, Step : 1270, Loss : 0.09558, Acc : 0.156, Sensitive_Loss : 0.08638, Sensitive_Acc : 2.900, Run Time : 3.05 sec
INFO:root:2024-04-14 12:40:53, Train, Epoch : 3, Step : 1280, Loss : 0.46845, Acc : 0.803, Sensitive_Loss : 0.28167, Sensitive_Acc : 20.600, Run Time : 6.83 sec
INFO:root:2024-04-14 12:41:00, Train, Epoch : 3, Step : 1290, Loss : 0.45109, Acc : 0.819, Sensitive_Loss : 0.30622, Sensitive_Acc : 24.800, Run Time : 7.25 sec
INFO:root:2024-04-14 12:41:07, Train, Epoch : 3, Step : 1300, Loss : 0.42774, Acc : 0.825, Sensitive_Loss : 0.22944, Sensitive_Acc : 22.100, Run Time : 6.53 sec
INFO:root:2024-04-14 12:42:34, Dev, Step : 1300, Loss : 0.50891, Acc : 0.765, Auc : 0.850, Sensitive_Loss : 0.26488, Sensitive_Acc : 20.684, Sensitive_Auc : 0.992, Mean auc: 0.850, Run Time : 87.00 sec
INFO:root:2024-04-14 12:42:35, Best, Step : 1300, Loss : 0.50891, Acc : 0.765, Auc : 0.850, Sensitive_Loss : 0.26488, Sensitive_Acc : 20.684, Sensitive_Auc : 0.992, Best Auc : 0.850
INFO:root:2024-04-14 12:42:40, Train, Epoch : 3, Step : 1310, Loss : 0.48092, Acc : 0.784, Sensitive_Loss : 0.20387, Sensitive_Acc : 19.800, Run Time : 92.85 sec
INFO:root:2024-04-14 12:42:47, Train, Epoch : 3, Step : 1320, Loss : 0.46243, Acc : 0.806, Sensitive_Loss : 0.17806, Sensitive_Acc : 20.200, Run Time : 7.35 sec
INFO:root:2024-04-14 12:42:54, Train, Epoch : 3, Step : 1330, Loss : 0.41731, Acc : 0.819, Sensitive_Loss : 0.19861, Sensitive_Acc : 19.300, Run Time : 7.26 sec
INFO:root:2024-04-14 12:43:02, Train, Epoch : 3, Step : 1340, Loss : 0.39238, Acc : 0.809, Sensitive_Loss : 0.16437, Sensitive_Acc : 19.900, Run Time : 7.56 sec
INFO:root:2024-04-14 12:43:09, Train, Epoch : 3, Step : 1350, Loss : 0.43216, Acc : 0.841, Sensitive_Loss : 0.15338, Sensitive_Acc : 26.200, Run Time : 7.09 sec
INFO:root:2024-04-14 12:43:16, Train, Epoch : 3, Step : 1360, Loss : 0.49731, Acc : 0.787, Sensitive_Loss : 0.13261, Sensitive_Acc : 23.800, Run Time : 6.74 sec
INFO:root:2024-04-14 12:43:23, Train, Epoch : 3, Step : 1370, Loss : 0.44843, Acc : 0.816, Sensitive_Loss : 0.14895, Sensitive_Acc : 21.700, Run Time : 6.87 sec
INFO:root:2024-04-14 12:43:30, Train, Epoch : 3, Step : 1380, Loss : 0.41756, Acc : 0.809, Sensitive_Loss : 0.21921, Sensitive_Acc : 22.900, Run Time : 7.04 sec
INFO:root:2024-04-14 12:43:37, Train, Epoch : 3, Step : 1390, Loss : 0.35693, Acc : 0.812, Sensitive_Loss : 0.21662, Sensitive_Acc : 23.100, Run Time : 6.86 sec
INFO:root:2024-04-14 12:43:44, Train, Epoch : 3, Step : 1400, Loss : 0.45418, Acc : 0.822, Sensitive_Loss : 0.25989, Sensitive_Acc : 24.300, Run Time : 7.18 sec
INFO:root:2024-04-14 12:45:11, Dev, Step : 1400, Loss : 0.50730, Acc : 0.773, Auc : 0.855, Sensitive_Loss : 0.31370, Sensitive_Acc : 19.752, Sensitive_Auc : 0.995, Mean auc: 0.855, Run Time : 87.32 sec
INFO:root:2024-04-14 12:45:12, Best, Step : 1400, Loss : 0.50730, Acc : 0.773, Auc : 0.855, Sensitive_Loss : 0.31370, Sensitive_Acc : 19.752, Sensitive_Auc : 0.995, Best Auc : 0.855
INFO:root:2024-04-14 12:45:17, Train, Epoch : 3, Step : 1410, Loss : 0.48052, Acc : 0.806, Sensitive_Loss : 0.18141, Sensitive_Acc : 22.500, Run Time : 93.42 sec
INFO:root:2024-04-14 12:45:24, Train, Epoch : 3, Step : 1420, Loss : 0.44917, Acc : 0.797, Sensitive_Loss : 0.18070, Sensitive_Acc : 24.100, Run Time : 7.01 sec
INFO:root:2024-04-14 12:45:32, Train, Epoch : 3, Step : 1430, Loss : 0.46716, Acc : 0.803, Sensitive_Loss : 0.22305, Sensitive_Acc : 19.900, Run Time : 7.66 sec
INFO:root:2024-04-14 12:45:39, Train, Epoch : 3, Step : 1440, Loss : 0.41391, Acc : 0.825, Sensitive_Loss : 0.17121, Sensitive_Acc : 20.100, Run Time : 6.60 sec
INFO:root:2024-04-14 12:45:46, Train, Epoch : 3, Step : 1450, Loss : 0.43122, Acc : 0.828, Sensitive_Loss : 0.18777, Sensitive_Acc : 21.200, Run Time : 7.20 sec
INFO:root:2024-04-14 12:45:53, Train, Epoch : 3, Step : 1460, Loss : 0.49834, Acc : 0.762, Sensitive_Loss : 0.26313, Sensitive_Acc : 19.000, Run Time : 7.52 sec
INFO:root:2024-04-14 12:46:00, Train, Epoch : 3, Step : 1470, Loss : 0.40846, Acc : 0.806, Sensitive_Loss : 0.24232, Sensitive_Acc : 15.800, Run Time : 6.81 sec
INFO:root:2024-04-14 12:46:07, Train, Epoch : 3, Step : 1480, Loss : 0.48365, Acc : 0.775, Sensitive_Loss : 0.20448, Sensitive_Acc : 24.000, Run Time : 7.12 sec
INFO:root:2024-04-14 12:46:14, Train, Epoch : 3, Step : 1490, Loss : 0.41003, Acc : 0.797, Sensitive_Loss : 0.15633, Sensitive_Acc : 19.600, Run Time : 7.10 sec
INFO:root:2024-04-14 12:46:21, Train, Epoch : 3, Step : 1500, Loss : 0.42450, Acc : 0.819, Sensitive_Loss : 0.23712, Sensitive_Acc : 24.000, Run Time : 7.18 sec
INFO:root:2024-04-14 12:47:49, Dev, Step : 1500, Loss : 0.49972, Acc : 0.774, Auc : 0.857, Sensitive_Loss : 0.26843, Sensitive_Acc : 20.654, Sensitive_Auc : 0.992, Mean auc: 0.857, Run Time : 87.10 sec
INFO:root:2024-04-14 12:47:49, Best, Step : 1500, Loss : 0.49972, Acc : 0.774, Auc : 0.857, Sensitive_Loss : 0.26843, Sensitive_Acc : 20.654, Sensitive_Auc : 0.992, Best Auc : 0.857
INFO:root:2024-04-14 12:47:55, Train, Epoch : 3, Step : 1510, Loss : 0.39185, Acc : 0.838, Sensitive_Loss : 0.15672, Sensitive_Acc : 23.900, Run Time : 93.51 sec
INFO:root:2024-04-14 12:48:02, Train, Epoch : 3, Step : 1520, Loss : 0.44493, Acc : 0.775, Sensitive_Loss : 0.26277, Sensitive_Acc : 19.600, Run Time : 6.74 sec
INFO:root:2024-04-14 12:48:09, Train, Epoch : 3, Step : 1530, Loss : 0.42500, Acc : 0.809, Sensitive_Loss : 0.18599, Sensitive_Acc : 24.500, Run Time : 7.36 sec
INFO:root:2024-04-14 12:48:16, Train, Epoch : 3, Step : 1540, Loss : 0.45030, Acc : 0.806, Sensitive_Loss : 0.17715, Sensitive_Acc : 24.800, Run Time : 6.69 sec
INFO:root:2024-04-14 12:48:23, Train, Epoch : 3, Step : 1550, Loss : 0.44251, Acc : 0.791, Sensitive_Loss : 0.18225, Sensitive_Acc : 15.900, Run Time : 7.06 sec
INFO:root:2024-04-14 12:48:30, Train, Epoch : 3, Step : 1560, Loss : 0.43283, Acc : 0.794, Sensitive_Loss : 0.20238, Sensitive_Acc : 18.500, Run Time : 7.04 sec
INFO:root:2024-04-14 12:48:38, Train, Epoch : 3, Step : 1570, Loss : 0.49048, Acc : 0.762, Sensitive_Loss : 0.25511, Sensitive_Acc : 19.700, Run Time : 7.94 sec
INFO:root:2024-04-14 12:48:45, Train, Epoch : 3, Step : 1580, Loss : 0.41790, Acc : 0.812, Sensitive_Loss : 0.22465, Sensitive_Acc : 24.700, Run Time : 6.86 sec
INFO:root:2024-04-14 12:48:51, Train, Epoch : 3, Step : 1590, Loss : 0.54808, Acc : 0.769, Sensitive_Loss : 0.17247, Sensitive_Acc : 22.500, Run Time : 6.39 sec
INFO:root:2024-04-14 12:48:59, Train, Epoch : 3, Step : 1600, Loss : 0.48451, Acc : 0.800, Sensitive_Loss : 0.14671, Sensitive_Acc : 16.600, Run Time : 7.65 sec
INFO:root:2024-04-14 12:50:26, Dev, Step : 1600, Loss : 0.49917, Acc : 0.773, Auc : 0.857, Sensitive_Loss : 0.29506, Sensitive_Acc : 20.278, Sensitive_Auc : 0.996, Mean auc: 0.857, Run Time : 87.04 sec
INFO:root:2024-04-14 12:50:26, Best, Step : 1600, Loss : 0.49917, Acc : 0.773, Auc : 0.857, Sensitive_Loss : 0.29506, Sensitive_Acc : 20.278, Sensitive_Auc : 0.996, Best Auc : 0.857
INFO:root:2024-04-14 12:50:32, Train, Epoch : 3, Step : 1610, Loss : 0.46506, Acc : 0.784, Sensitive_Loss : 0.24715, Sensitive_Acc : 13.900, Run Time : 92.92 sec
INFO:root:2024-04-14 12:50:39, Train, Epoch : 3, Step : 1620, Loss : 0.43640, Acc : 0.794, Sensitive_Loss : 0.17941, Sensitive_Acc : 17.500, Run Time : 7.21 sec
INFO:root:2024-04-14 12:50:46, Train, Epoch : 3, Step : 1630, Loss : 0.47504, Acc : 0.803, Sensitive_Loss : 0.17583, Sensitive_Acc : 20.900, Run Time : 7.01 sec
INFO:root:2024-04-14 12:50:53, Train, Epoch : 3, Step : 1640, Loss : 0.40419, Acc : 0.812, Sensitive_Loss : 0.16705, Sensitive_Acc : 22.500, Run Time : 7.18 sec
INFO:root:2024-04-14 12:51:00, Train, Epoch : 3, Step : 1650, Loss : 0.45544, Acc : 0.803, Sensitive_Loss : 0.17101, Sensitive_Acc : 24.800, Run Time : 6.78 sec
INFO:root:2024-04-14 12:51:07, Train, Epoch : 3, Step : 1660, Loss : 0.42206, Acc : 0.809, Sensitive_Loss : 0.23925, Sensitive_Acc : 20.000, Run Time : 7.30 sec
INFO:root:2024-04-14 12:51:15, Train, Epoch : 3, Step : 1670, Loss : 0.44817, Acc : 0.800, Sensitive_Loss : 0.16110, Sensitive_Acc : 25.200, Run Time : 7.58 sec
INFO:root:2024-04-14 12:51:22, Train, Epoch : 3, Step : 1680, Loss : 0.44538, Acc : 0.825, Sensitive_Loss : 0.18461, Sensitive_Acc : 20.000, Run Time : 6.86 sec
INFO:root:2024-04-14 12:51:29, Train, Epoch : 3, Step : 1690, Loss : 0.44847, Acc : 0.778, Sensitive_Loss : 0.22968, Sensitive_Acc : 19.700, Run Time : 7.12 sec
INFO:root:2024-04-14 12:51:36, Train, Epoch : 3, Step : 1700, Loss : 0.45240, Acc : 0.803, Sensitive_Loss : 0.19604, Sensitive_Acc : 16.500, Run Time : 7.05 sec
INFO:root:2024-04-14 12:53:03, Dev, Step : 1700, Loss : 0.51034, Acc : 0.778, Auc : 0.860, Sensitive_Loss : 0.29201, Sensitive_Acc : 20.098, Sensitive_Auc : 0.997, Mean auc: 0.860, Run Time : 87.11 sec
INFO:root:2024-04-14 12:53:04, Best, Step : 1700, Loss : 0.51034, Acc : 0.778, Auc : 0.860, Sensitive_Loss : 0.29201, Sensitive_Acc : 20.098, Sensitive_Auc : 0.997, Best Auc : 0.860
INFO:root:2024-04-14 12:53:09, Train, Epoch : 3, Step : 1710, Loss : 0.42806, Acc : 0.794, Sensitive_Loss : 0.15261, Sensitive_Acc : 22.200, Run Time : 93.41 sec
INFO:root:2024-04-14 12:53:16, Train, Epoch : 3, Step : 1720, Loss : 0.44531, Acc : 0.766, Sensitive_Loss : 0.20627, Sensitive_Acc : 25.300, Run Time : 7.00 sec
INFO:root:2024-04-14 12:53:23, Train, Epoch : 3, Step : 1730, Loss : 0.47303, Acc : 0.781, Sensitive_Loss : 0.21722, Sensitive_Acc : 22.300, Run Time : 7.26 sec
INFO:root:2024-04-14 12:53:30, Train, Epoch : 3, Step : 1740, Loss : 0.40669, Acc : 0.812, Sensitive_Loss : 0.19412, Sensitive_Acc : 21.600, Run Time : 6.53 sec
INFO:root:2024-04-14 12:53:37, Train, Epoch : 3, Step : 1750, Loss : 0.48121, Acc : 0.772, Sensitive_Loss : 0.24080, Sensitive_Acc : 21.000, Run Time : 7.48 sec
INFO:root:2024-04-14 12:53:45, Train, Epoch : 3, Step : 1760, Loss : 0.43396, Acc : 0.797, Sensitive_Loss : 0.21136, Sensitive_Acc : 23.000, Run Time : 7.29 sec
INFO:root:2024-04-14 12:53:52, Train, Epoch : 3, Step : 1770, Loss : 0.51252, Acc : 0.775, Sensitive_Loss : 0.21770, Sensitive_Acc : 22.200, Run Time : 6.89 sec
INFO:root:2024-04-14 12:53:59, Train, Epoch : 3, Step : 1780, Loss : 0.38092, Acc : 0.797, Sensitive_Loss : 0.23833, Sensitive_Acc : 20.000, Run Time : 7.26 sec
INFO:root:2024-04-14 12:54:06, Train, Epoch : 3, Step : 1790, Loss : 0.45190, Acc : 0.809, Sensitive_Loss : 0.35613, Sensitive_Acc : 22.600, Run Time : 7.19 sec
INFO:root:2024-04-14 12:54:13, Train, Epoch : 3, Step : 1800, Loss : 0.48982, Acc : 0.787, Sensitive_Loss : 0.22854, Sensitive_Acc : 22.500, Run Time : 7.00 sec
INFO:root:2024-04-14 12:55:41, Dev, Step : 1800, Loss : 0.49567, Acc : 0.777, Auc : 0.859, Sensitive_Loss : 0.35216, Sensitive_Acc : 19.692, Sensitive_Auc : 0.997, Mean auc: 0.859, Run Time : 87.54 sec
INFO:root:2024-04-14 12:55:46, Train, Epoch : 3, Step : 1810, Loss : 0.44662, Acc : 0.828, Sensitive_Loss : 0.26798, Sensitive_Acc : 19.900, Run Time : 93.17 sec
INFO:root:2024-04-14 12:55:53, Train, Epoch : 3, Step : 1820, Loss : 0.46584, Acc : 0.816, Sensitive_Loss : 0.21954, Sensitive_Acc : 18.000, Run Time : 6.96 sec
INFO:root:2024-04-14 12:56:01, Train, Epoch : 3, Step : 1830, Loss : 0.51571, Acc : 0.784, Sensitive_Loss : 0.17290, Sensitive_Acc : 20.000, Run Time : 7.70 sec
INFO:root:2024-04-14 12:56:08, Train, Epoch : 3, Step : 1840, Loss : 0.40795, Acc : 0.831, Sensitive_Loss : 0.18189, Sensitive_Acc : 22.900, Run Time : 6.81 sec
INFO:root:2024-04-14 12:56:14, Train, Epoch : 3, Step : 1850, Loss : 0.43252, Acc : 0.781, Sensitive_Loss : 0.20670, Sensitive_Acc : 21.600, Run Time : 6.75 sec
INFO:root:2024-04-14 12:56:21, Train, Epoch : 3, Step : 1860, Loss : 0.50639, Acc : 0.803, Sensitive_Loss : 0.26647, Sensitive_Acc : 18.900, Run Time : 7.02 sec
INFO:root:2024-04-14 12:56:29, Train, Epoch : 3, Step : 1870, Loss : 0.42590, Acc : 0.806, Sensitive_Loss : 0.20824, Sensitive_Acc : 23.700, Run Time : 7.57 sec
INFO:root:2024-04-14 12:56:36, Train, Epoch : 3, Step : 1880, Loss : 0.42513, Acc : 0.797, Sensitive_Loss : 0.16473, Sensitive_Acc : 15.900, Run Time : 6.68 sec
INFO:root:2024-04-14 12:56:43, Train, Epoch : 3, Step : 1890, Loss : 0.42964, Acc : 0.800, Sensitive_Loss : 0.19110, Sensitive_Acc : 21.500, Run Time : 6.97 sec
INFO:root:2024-04-14 12:56:52, Train, Epoch : 3, Step : 1900, Loss : 0.43680, Acc : 0.797, Sensitive_Loss : 0.22091, Sensitive_Acc : 24.200, Run Time : 9.80 sec
INFO:root:2024-04-14 12:58:19, Dev, Step : 1900, Loss : 0.49627, Acc : 0.782, Auc : 0.861, Sensitive_Loss : 0.27697, Sensitive_Acc : 20.699, Sensitive_Auc : 0.998, Mean auc: 0.861, Run Time : 86.64 sec
INFO:root:2024-04-14 12:58:20, Best, Step : 1900, Loss : 0.49627, Acc : 0.782, Auc : 0.861, Sensitive_Loss : 0.27697, Sensitive_Acc : 20.699, Sensitive_Auc : 0.998, Best Auc : 0.861
INFO:root:2024-04-14 12:59:47
INFO:root:y_pred: [0.09904733 0.00648987 0.06110447 ... 0.29832262 0.02930884 0.08544286]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.73645951e-02 4.08470184e-02 1.16425656e-01 9.98767689e-02
 2.42009666e-02 1.46082103e-01 1.51042625e-01 6.13193400e-03
 2.16719080e-02 9.99890327e-01 4.58638698e-01 2.12501138e-02
 7.63904594e-04 6.18656306e-03 9.98851180e-01 3.81996445e-02
 1.09528909e-02 9.99444425e-01 9.97917950e-01 5.25265746e-02
 9.96354342e-01 1.75261020e-03 6.61145568e-01 3.63093823e-01
 1.33113667e-01 4.96283531e-01 1.92200023e-04 2.41198286e-01
 1.36373832e-03 5.57548506e-03 1.38077289e-01 9.96373951e-01
 1.18273720e-01 7.73368239e-01 5.52506361e-04 5.86905808e-04
 2.48627318e-03 3.71377841e-02 2.66691186e-02 1.81654170e-01
 3.12076747e-01 9.74734962e-01 3.36620607e-03 1.11024857e-01
 9.98821199e-01 9.36757565e-01 7.06778944e-01 1.68606147e-01
 8.74511302e-01 9.71371830e-01 9.99347985e-01 9.99374807e-01
 9.98449922e-01 2.01303493e-02 1.68569878e-01 6.74779713e-01
 3.58018023e-03 1.70300886e-01 9.73689675e-01 3.44982371e-02
 4.44667367e-03 1.56521834e-02 2.04205830e-02 1.53122749e-03
 9.99505877e-01 2.91596472e-01 4.56973648e-04 2.79259473e-01
 3.13590863e-03 9.82544839e-01 9.99891996e-01 9.99981880e-01
 5.13853366e-03 7.85506010e-01 1.61620900e-02 2.38132879e-01
 6.26082122e-02 5.84593625e-04 2.91298423e-03 7.32401619e-03
 2.15269372e-01 1.19883865e-02 9.99575078e-01 9.98333752e-01
 2.31783800e-02 1.80577129e-01 3.91352981e-01 2.39067264e-02
 3.48994806e-02 4.88429749e-03 3.05646695e-02 6.63285613e-01
 1.74138974e-03 1.55687996e-03 7.72030139e-03 2.53136232e-02
 2.27321172e-03 8.33062172e-01 3.96385938e-02 5.56532368e-02
 7.35701621e-02 3.27135548e-02 6.37117326e-01 2.58796439e-02
 1.28585503e-01 1.77144557e-02 3.37519318e-01 7.42123604e-01
 5.15316486e-01 4.59446043e-01 1.22844370e-03 9.99837279e-01
 9.99782979e-01 7.15889095e-04 3.18455666e-01 4.52006996e-01
 1.96114555e-01 9.76397889e-04 7.20377743e-01 4.95834313e-02
 6.00341670e-02 1.34100916e-03 3.50773372e-02 5.37402323e-03
 9.17460173e-02 1.20717347e-01 1.80037780e-04 9.77771521e-01
 4.32248205e-01 3.77815932e-01 3.07857916e-02 1.57913864e-01
 5.16912369e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-14 12:59:47, Dev, Step : 1902, Loss : 0.49704, Acc : 0.781, Auc : 0.861, Sensitive_Loss : 0.29073, Sensitive_Acc : 20.594, Sensitive_Auc : 0.998, Mean auc: 0.861, Run Time : 86.12 sec
INFO:root:2024-04-14 12:59:47, Best, Step : 1902, Loss : 0.49704, Acc : 0.781,Auc : 0.861, Best Auc : 0.861, Sensitive_Loss : 0.29073, Sensitive_Acc : 20.594, Sensitive_Auc : 0.998
INFO:root:2024-04-14 12:59:56, Train, Epoch : 4, Step : 1910, Loss : 0.35175, Acc : 0.637, Sensitive_Loss : 0.15474, Sensitive_Acc : 16.500, Run Time : 7.51 sec
INFO:root:2024-04-14 13:00:02, Train, Epoch : 4, Step : 1920, Loss : 0.41471, Acc : 0.828, Sensitive_Loss : 0.25571, Sensitive_Acc : 20.500, Run Time : 6.53 sec
INFO:root:2024-04-14 13:00:09, Train, Epoch : 4, Step : 1930, Loss : 0.36422, Acc : 0.841, Sensitive_Loss : 0.16390, Sensitive_Acc : 22.500, Run Time : 6.90 sec
INFO:root:2024-04-14 13:00:16, Train, Epoch : 4, Step : 1940, Loss : 0.40751, Acc : 0.831, Sensitive_Loss : 0.19578, Sensitive_Acc : 21.600, Run Time : 7.28 sec
INFO:root:2024-04-14 13:00:23, Train, Epoch : 4, Step : 1950, Loss : 0.40269, Acc : 0.803, Sensitive_Loss : 0.16759, Sensitive_Acc : 21.700, Run Time : 7.03 sec
INFO:root:2024-04-14 13:00:31, Train, Epoch : 4, Step : 1960, Loss : 0.37179, Acc : 0.831, Sensitive_Loss : 0.22544, Sensitive_Acc : 23.000, Run Time : 7.38 sec
INFO:root:2024-04-14 13:00:37, Train, Epoch : 4, Step : 1970, Loss : 0.34725, Acc : 0.831, Sensitive_Loss : 0.19311, Sensitive_Acc : 19.900, Run Time : 6.56 sec
INFO:root:2024-04-14 13:00:45, Train, Epoch : 4, Step : 1980, Loss : 0.34723, Acc : 0.875, Sensitive_Loss : 0.15533, Sensitive_Acc : 20.600, Run Time : 7.35 sec
INFO:root:2024-04-14 13:00:52, Train, Epoch : 4, Step : 1990, Loss : 0.37154, Acc : 0.834, Sensitive_Loss : 0.20249, Sensitive_Acc : 19.600, Run Time : 7.44 sec
INFO:root:2024-04-14 13:00:59, Train, Epoch : 4, Step : 2000, Loss : 0.41512, Acc : 0.822, Sensitive_Loss : 0.23496, Sensitive_Acc : 23.500, Run Time : 6.86 sec
INFO:root:2024-04-14 13:02:26, Dev, Step : 2000, Loss : 0.50194, Acc : 0.780, Auc : 0.861, Sensitive_Loss : 0.26204, Sensitive_Acc : 20.474, Sensitive_Auc : 0.996, Mean auc: 0.861, Run Time : 87.01 sec
INFO:root:2024-04-14 13:02:27, Best, Step : 2000, Loss : 0.50194, Acc : 0.780, Auc : 0.861, Sensitive_Loss : 0.26204, Sensitive_Acc : 20.474, Sensitive_Auc : 0.996, Best Auc : 0.861
INFO:root:2024-04-14 13:02:32, Train, Epoch : 4, Step : 2010, Loss : 0.45554, Acc : 0.819, Sensitive_Loss : 0.24595, Sensitive_Acc : 23.500, Run Time : 93.08 sec
INFO:root:2024-04-14 13:02:39, Train, Epoch : 4, Step : 2020, Loss : 0.45437, Acc : 0.775, Sensitive_Loss : 0.19431, Sensitive_Acc : 20.600, Run Time : 7.15 sec
INFO:root:2024-04-14 13:02:46, Train, Epoch : 4, Step : 2030, Loss : 0.46005, Acc : 0.784, Sensitive_Loss : 0.28256, Sensitive_Acc : 20.300, Run Time : 7.14 sec
INFO:root:2024-04-14 13:02:53, Train, Epoch : 4, Step : 2040, Loss : 0.43674, Acc : 0.812, Sensitive_Loss : 0.15294, Sensitive_Acc : 24.500, Run Time : 7.02 sec
INFO:root:2024-04-14 13:03:00, Train, Epoch : 4, Step : 2050, Loss : 0.50531, Acc : 0.816, Sensitive_Loss : 0.17203, Sensitive_Acc : 20.500, Run Time : 7.05 sec
INFO:root:2024-04-14 13:03:08, Train, Epoch : 4, Step : 2060, Loss : 0.44934, Acc : 0.800, Sensitive_Loss : 0.20278, Sensitive_Acc : 15.900, Run Time : 7.21 sec
INFO:root:2024-04-14 13:03:15, Train, Epoch : 4, Step : 2070, Loss : 0.40750, Acc : 0.809, Sensitive_Loss : 0.13694, Sensitive_Acc : 17.800, Run Time : 7.26 sec
INFO:root:2024-04-14 13:03:22, Train, Epoch : 4, Step : 2080, Loss : 0.39534, Acc : 0.838, Sensitive_Loss : 0.16953, Sensitive_Acc : 22.300, Run Time : 6.85 sec
INFO:root:2024-04-14 13:03:29, Train, Epoch : 4, Step : 2090, Loss : 0.41613, Acc : 0.803, Sensitive_Loss : 0.19083, Sensitive_Acc : 24.100, Run Time : 7.61 sec
INFO:root:2024-04-14 13:03:36, Train, Epoch : 4, Step : 2100, Loss : 0.46419, Acc : 0.775, Sensitive_Loss : 0.15729, Sensitive_Acc : 24.900, Run Time : 6.85 sec
INFO:root:2024-04-14 13:05:03, Dev, Step : 2100, Loss : 0.50805, Acc : 0.780, Auc : 0.862, Sensitive_Loss : 0.26543, Sensitive_Acc : 20.534, Sensitive_Auc : 0.995, Mean auc: 0.862, Run Time : 86.91 sec
INFO:root:2024-04-14 13:05:04, Best, Step : 2100, Loss : 0.50805, Acc : 0.780, Auc : 0.862, Sensitive_Loss : 0.26543, Sensitive_Acc : 20.534, Sensitive_Auc : 0.995, Best Auc : 0.862
INFO:root:2024-04-14 13:05:09, Train, Epoch : 4, Step : 2110, Loss : 0.40032, Acc : 0.844, Sensitive_Loss : 0.21264, Sensitive_Acc : 19.900, Run Time : 93.19 sec
INFO:root:2024-04-14 13:05:16, Train, Epoch : 4, Step : 2120, Loss : 0.45085, Acc : 0.806, Sensitive_Loss : 0.19249, Sensitive_Acc : 24.500, Run Time : 6.72 sec
INFO:root:2024-04-14 13:05:23, Train, Epoch : 4, Step : 2130, Loss : 0.37523, Acc : 0.828, Sensitive_Loss : 0.17473, Sensitive_Acc : 18.300, Run Time : 7.07 sec
INFO:root:2024-04-14 13:05:30, Train, Epoch : 4, Step : 2140, Loss : 0.44265, Acc : 0.834, Sensitive_Loss : 0.17415, Sensitive_Acc : 23.400, Run Time : 7.15 sec
INFO:root:2024-04-14 13:05:38, Train, Epoch : 4, Step : 2150, Loss : 0.39700, Acc : 0.841, Sensitive_Loss : 0.19764, Sensitive_Acc : 19.800, Run Time : 7.16 sec
INFO:root:2024-04-14 13:05:44, Train, Epoch : 4, Step : 2160, Loss : 0.38226, Acc : 0.809, Sensitive_Loss : 0.19613, Sensitive_Acc : 20.800, Run Time : 6.88 sec
INFO:root:2024-04-14 13:05:52, Train, Epoch : 4, Step : 2170, Loss : 0.47723, Acc : 0.825, Sensitive_Loss : 0.15482, Sensitive_Acc : 24.800, Run Time : 7.05 sec
INFO:root:2024-04-14 13:05:58, Train, Epoch : 4, Step : 2180, Loss : 0.47490, Acc : 0.828, Sensitive_Loss : 0.16063, Sensitive_Acc : 22.200, Run Time : 6.91 sec
INFO:root:2024-04-14 13:06:05, Train, Epoch : 4, Step : 2190, Loss : 0.43717, Acc : 0.809, Sensitive_Loss : 0.15334, Sensitive_Acc : 15.900, Run Time : 6.92 sec
INFO:root:2024-04-14 13:06:13, Train, Epoch : 4, Step : 2200, Loss : 0.42262, Acc : 0.841, Sensitive_Loss : 0.17600, Sensitive_Acc : 18.400, Run Time : 7.68 sec
INFO:root:2024-04-14 13:07:40, Dev, Step : 2200, Loss : 0.50411, Acc : 0.781, Auc : 0.859, Sensitive_Loss : 0.27187, Sensitive_Acc : 20.444, Sensitive_Auc : 0.995, Mean auc: 0.859, Run Time : 87.15 sec
INFO:root:2024-04-14 13:07:46, Train, Epoch : 4, Step : 2210, Loss : 0.51242, Acc : 0.794, Sensitive_Loss : 0.21261, Sensitive_Acc : 20.100, Run Time : 92.75 sec
INFO:root:2024-04-14 13:07:53, Train, Epoch : 4, Step : 2220, Loss : 0.38661, Acc : 0.844, Sensitive_Loss : 0.15560, Sensitive_Acc : 23.800, Run Time : 7.24 sec
INFO:root:2024-04-14 13:08:00, Train, Epoch : 4, Step : 2230, Loss : 0.46408, Acc : 0.794, Sensitive_Loss : 0.16851, Sensitive_Acc : 23.000, Run Time : 7.00 sec
INFO:root:2024-04-14 13:08:07, Train, Epoch : 4, Step : 2240, Loss : 0.37861, Acc : 0.841, Sensitive_Loss : 0.16661, Sensitive_Acc : 23.800, Run Time : 6.61 sec
INFO:root:2024-04-14 13:08:13, Train, Epoch : 4, Step : 2250, Loss : 0.43880, Acc : 0.791, Sensitive_Loss : 0.23652, Sensitive_Acc : 21.700, Run Time : 6.79 sec
INFO:root:2024-04-14 13:08:21, Train, Epoch : 4, Step : 2260, Loss : 0.43032, Acc : 0.816, Sensitive_Loss : 0.18446, Sensitive_Acc : 21.700, Run Time : 7.59 sec
INFO:root:2024-04-14 13:08:28, Train, Epoch : 4, Step : 2270, Loss : 0.44169, Acc : 0.803, Sensitive_Loss : 0.24674, Sensitive_Acc : 21.100, Run Time : 7.39 sec
INFO:root:2024-04-14 13:08:35, Train, Epoch : 4, Step : 2280, Loss : 0.38664, Acc : 0.822, Sensitive_Loss : 0.16880, Sensitive_Acc : 19.500, Run Time : 6.36 sec
INFO:root:2024-04-14 13:08:42, Train, Epoch : 4, Step : 2290, Loss : 0.49928, Acc : 0.822, Sensitive_Loss : 0.18888, Sensitive_Acc : 23.300, Run Time : 7.14 sec
INFO:root:2024-04-14 13:08:49, Train, Epoch : 4, Step : 2300, Loss : 0.52397, Acc : 0.787, Sensitive_Loss : 0.13298, Sensitive_Acc : 24.400, Run Time : 7.16 sec
INFO:root:2024-04-14 13:10:16, Dev, Step : 2300, Loss : 0.50518, Acc : 0.781, Auc : 0.861, Sensitive_Loss : 0.27283, Sensitive_Acc : 20.564, Sensitive_Auc : 0.996, Mean auc: 0.861, Run Time : 87.26 sec
INFO:root:2024-04-14 13:10:22, Train, Epoch : 4, Step : 2310, Loss : 0.39605, Acc : 0.791, Sensitive_Loss : 0.13444, Sensitive_Acc : 22.900, Run Time : 92.61 sec
INFO:root:2024-04-14 13:10:29, Train, Epoch : 4, Step : 2320, Loss : 0.46521, Acc : 0.784, Sensitive_Loss : 0.17352, Sensitive_Acc : 19.800, Run Time : 7.19 sec
INFO:root:2024-04-14 13:10:36, Train, Epoch : 4, Step : 2330, Loss : 0.45831, Acc : 0.797, Sensitive_Loss : 0.16566, Sensitive_Acc : 24.500, Run Time : 7.23 sec
INFO:root:2024-04-14 13:10:43, Train, Epoch : 4, Step : 2340, Loss : 0.37358, Acc : 0.841, Sensitive_Loss : 0.17804, Sensitive_Acc : 15.900, Run Time : 6.88 sec
INFO:root:2024-04-14 13:10:50, Train, Epoch : 4, Step : 2350, Loss : 0.48119, Acc : 0.759, Sensitive_Loss : 0.16763, Sensitive_Acc : 24.100, Run Time : 7.18 sec
INFO:root:2024-04-14 13:10:57, Train, Epoch : 4, Step : 2360, Loss : 0.37291, Acc : 0.816, Sensitive_Loss : 0.15490, Sensitive_Acc : 19.300, Run Time : 7.06 sec
INFO:root:2024-04-14 13:11:04, Train, Epoch : 4, Step : 2370, Loss : 0.33165, Acc : 0.819, Sensitive_Loss : 0.25662, Sensitive_Acc : 19.500, Run Time : 6.97 sec
INFO:root:2024-04-14 13:11:11, Train, Epoch : 4, Step : 2380, Loss : 0.46140, Acc : 0.772, Sensitive_Loss : 0.34292, Sensitive_Acc : 22.400, Run Time : 6.86 sec
INFO:root:2024-04-14 13:11:18, Train, Epoch : 4, Step : 2390, Loss : 0.37160, Acc : 0.847, Sensitive_Loss : 0.23794, Sensitive_Acc : 25.600, Run Time : 6.86 sec
INFO:root:2024-04-14 13:11:25, Train, Epoch : 4, Step : 2400, Loss : 0.37222, Acc : 0.812, Sensitive_Loss : 0.25709, Sensitive_Acc : 25.200, Run Time : 7.46 sec
INFO:root:2024-04-14 13:12:53, Dev, Step : 2400, Loss : 0.50336, Acc : 0.779, Auc : 0.860, Sensitive_Loss : 0.27071, Sensitive_Acc : 20.549, Sensitive_Auc : 0.998, Mean auc: 0.860, Run Time : 87.71 sec
INFO:root:2024-04-14 13:12:59, Train, Epoch : 4, Step : 2410, Loss : 0.41065, Acc : 0.847, Sensitive_Loss : 0.18405, Sensitive_Acc : 21.900, Run Time : 93.36 sec
INFO:root:2024-04-14 13:13:06, Train, Epoch : 4, Step : 2420, Loss : 0.41287, Acc : 0.838, Sensitive_Loss : 0.27082, Sensitive_Acc : 25.400, Run Time : 6.86 sec
INFO:root:2024-04-14 13:13:13, Train, Epoch : 4, Step : 2430, Loss : 0.41667, Acc : 0.825, Sensitive_Loss : 0.20631, Sensitive_Acc : 23.400, Run Time : 7.03 sec
INFO:root:2024-04-14 13:13:20, Train, Epoch : 4, Step : 2440, Loss : 0.45582, Acc : 0.797, Sensitive_Loss : 0.24939, Sensitive_Acc : 19.400, Run Time : 7.18 sec
INFO:root:2024-04-14 13:13:27, Train, Epoch : 4, Step : 2450, Loss : 0.37268, Acc : 0.850, Sensitive_Loss : 0.13226, Sensitive_Acc : 15.600, Run Time : 7.51 sec
INFO:root:2024-04-14 13:13:34, Train, Epoch : 4, Step : 2460, Loss : 0.36308, Acc : 0.834, Sensitive_Loss : 0.17539, Sensitive_Acc : 23.700, Run Time : 6.37 sec
INFO:root:2024-04-14 13:13:41, Train, Epoch : 4, Step : 2470, Loss : 0.38362, Acc : 0.822, Sensitive_Loss : 0.19014, Sensitive_Acc : 21.200, Run Time : 7.26 sec
INFO:root:2024-04-14 13:13:48, Train, Epoch : 4, Step : 2480, Loss : 0.34608, Acc : 0.834, Sensitive_Loss : 0.19598, Sensitive_Acc : 18.500, Run Time : 7.34 sec
INFO:root:2024-04-14 13:13:55, Train, Epoch : 4, Step : 2490, Loss : 0.49954, Acc : 0.800, Sensitive_Loss : 0.15902, Sensitive_Acc : 25.400, Run Time : 6.78 sec
INFO:root:2024-04-14 13:14:02, Train, Epoch : 4, Step : 2500, Loss : 0.40395, Acc : 0.822, Sensitive_Loss : 0.17009, Sensitive_Acc : 17.900, Run Time : 7.20 sec
INFO:root:2024-04-14 13:15:30, Dev, Step : 2500, Loss : 0.50343, Acc : 0.779, Auc : 0.862, Sensitive_Loss : 0.25926, Sensitive_Acc : 20.398, Sensitive_Auc : 0.998, Mean auc: 0.862, Run Time : 87.65 sec
INFO:root:2024-04-14 13:15:31, Best, Step : 2500, Loss : 0.50343, Acc : 0.779, Auc : 0.862, Sensitive_Loss : 0.25926, Sensitive_Acc : 20.398, Sensitive_Auc : 0.998, Best Auc : 0.862
INFO:root:2024-04-14 13:15:36, Train, Epoch : 4, Step : 2510, Loss : 0.31505, Acc : 0.853, Sensitive_Loss : 0.19822, Sensitive_Acc : 18.500, Run Time : 93.95 sec
INFO:root:2024-04-14 13:15:43, Train, Epoch : 4, Step : 2520, Loss : 0.41973, Acc : 0.816, Sensitive_Loss : 0.15599, Sensitive_Acc : 23.300, Run Time : 6.98 sec
INFO:root:2024-04-14 13:15:50, Train, Epoch : 4, Step : 2530, Loss : 0.39077, Acc : 0.838, Sensitive_Loss : 0.13113, Sensitive_Acc : 17.100, Run Time : 7.03 sec
INFO:root:2024-04-14 13:17:21
INFO:root:y_pred: [0.05465082 0.00397242 0.04901875 ... 0.22178383 0.01931306 0.04310953]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.23165417e-02 2.59331521e-02 4.85975593e-02 1.29298583e-01
 1.28341485e-02 7.09398612e-02 7.55720735e-02 6.89466018e-03
 2.05852874e-02 9.99686241e-01 5.29052377e-01 2.61618197e-02
 1.04799645e-03 5.10822982e-03 9.99319077e-01 4.22588401e-02
 7.74322404e-03 9.99410272e-01 9.97176170e-01 3.25511768e-02
 9.94337618e-01 5.90937096e-04 4.04752642e-01 3.18436205e-01
 8.34823400e-02 5.50850987e-01 8.32474398e-05 1.70269087e-01
 8.84180365e-04 1.25639550e-02 2.06524581e-01 9.94318902e-01
 1.78402975e-01 8.52409065e-01 2.80687702e-04 4.57858987e-04
 4.23175003e-03 1.79825053e-02 2.58750208e-02 6.37286454e-02
 1.19213670e-01 9.84247983e-01 9.37563775e-04 4.56253551e-02
 9.98794556e-01 8.30097139e-01 7.44042695e-01 2.34871730e-01
 6.93967342e-01 9.80807006e-01 9.98010337e-01 9.99310017e-01
 9.98542905e-01 1.93412155e-02 1.72506347e-01 7.30386198e-01
 1.03696657e-03 1.41031995e-01 9.86394644e-01 1.87688544e-02
 2.45668413e-03 5.43232262e-03 2.72725932e-02 1.39188999e-03
 9.99501348e-01 2.86912411e-01 2.16516986e-04 2.01487169e-01
 2.05093971e-03 9.81247485e-01 9.99843001e-01 9.99993920e-01
 5.69368992e-03 7.07853854e-01 2.89637689e-02 4.14006650e-01
 4.37546857e-02 4.12613183e-04 3.72981140e-03 6.40081242e-03
 1.61544636e-01 1.11067826e-02 9.99588072e-01 9.99377966e-01
 2.22491045e-02 1.21523954e-01 2.21012339e-01 1.61793791e-02
 4.80623394e-02 2.78386474e-03 2.38341633e-02 5.86374640e-01
 2.26311246e-03 1.40927639e-03 1.36200420e-03 1.49281695e-02
 1.82390353e-03 7.55335093e-01 2.72364113e-02 1.91863459e-02
 4.33583520e-02 2.32046079e-02 5.94808936e-01 1.89382005e-02
 1.74287304e-01 7.37693766e-03 4.29025561e-01 7.22008348e-01
 5.83675206e-01 3.34828854e-01 1.31497998e-03 9.99765098e-01
 9.99705851e-01 9.12102580e-04 1.28050148e-01 5.36707759e-01
 1.10522896e-01 1.27972919e-03 5.58377862e-01 4.19950113e-02
 2.29901541e-02 1.55732839e-03 3.64587754e-02 3.81164858e-03
 6.86644539e-02 1.28882021e-01 1.54491339e-04 9.79258001e-01
 3.29852611e-01 2.81803221e-01 3.22757997e-02 2.75932968e-01
 2.49930290e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-14 13:17:21, Dev, Step : 2536, Loss : 0.50447, Acc : 0.780, Auc : 0.863, Sensitive_Loss : 0.27501, Sensitive_Acc : 20.398, Sensitive_Auc : 1.000, Mean auc: 0.863, Run Time : 86.59 sec
INFO:root:2024-04-14 13:17:21, Best, Step : 2536, Loss : 0.50447, Acc : 0.780,Auc : 0.863, Best Auc : 0.863, Sensitive_Loss : 0.27501, Sensitive_Acc : 20.398, Sensitive_Auc : 1.000
INFO:root:2024-04-14 13:17:26, Train, Epoch : 5, Step : 2540, Loss : 0.17074, Acc : 0.312, Sensitive_Loss : 0.05393, Sensitive_Acc : 5.900, Run Time : 4.05 sec
INFO:root:2024-04-14 13:17:33, Train, Epoch : 5, Step : 2550, Loss : 0.37929, Acc : 0.856, Sensitive_Loss : 0.13667, Sensitive_Acc : 19.800, Run Time : 6.90 sec
INFO:root:2024-04-14 13:17:40, Train, Epoch : 5, Step : 2560, Loss : 0.39977, Acc : 0.834, Sensitive_Loss : 0.16735, Sensitive_Acc : 22.600, Run Time : 7.13 sec
INFO:root:2024-04-14 13:17:47, Train, Epoch : 5, Step : 2570, Loss : 0.32904, Acc : 0.850, Sensitive_Loss : 0.16588, Sensitive_Acc : 22.500, Run Time : 6.67 sec
INFO:root:2024-04-14 13:17:54, Train, Epoch : 5, Step : 2580, Loss : 0.40116, Acc : 0.822, Sensitive_Loss : 0.16882, Sensitive_Acc : 23.100, Run Time : 7.39 sec
INFO:root:2024-04-14 13:18:01, Train, Epoch : 5, Step : 2590, Loss : 0.38919, Acc : 0.838, Sensitive_Loss : 0.24392, Sensitive_Acc : 22.400, Run Time : 6.93 sec
INFO:root:2024-04-14 13:18:09, Train, Epoch : 5, Step : 2600, Loss : 0.37068, Acc : 0.834, Sensitive_Loss : 0.14670, Sensitive_Acc : 18.700, Run Time : 7.36 sec
INFO:root:2024-04-14 13:19:36, Dev, Step : 2600, Loss : 0.50326, Acc : 0.782, Auc : 0.861, Sensitive_Loss : 0.23683, Sensitive_Acc : 21.211, Sensitive_Auc : 0.998, Mean auc: 0.861, Run Time : 87.56 sec
INFO:root:2024-04-14 13:19:42, Train, Epoch : 5, Step : 2610, Loss : 0.42153, Acc : 0.800, Sensitive_Loss : 0.13409, Sensitive_Acc : 21.600, Run Time : 93.34 sec
INFO:root:2024-04-14 13:19:49, Train, Epoch : 5, Step : 2620, Loss : 0.42322, Acc : 0.828, Sensitive_Loss : 0.17656, Sensitive_Acc : 23.700, Run Time : 6.83 sec
INFO:root:2024-04-14 13:19:56, Train, Epoch : 5, Step : 2630, Loss : 0.35242, Acc : 0.853, Sensitive_Loss : 0.15397, Sensitive_Acc : 23.600, Run Time : 6.84 sec
INFO:root:2024-04-14 13:20:03, Train, Epoch : 5, Step : 2640, Loss : 0.42049, Acc : 0.800, Sensitive_Loss : 0.12686, Sensitive_Acc : 23.000, Run Time : 7.06 sec
INFO:root:2024-04-14 13:20:10, Train, Epoch : 5, Step : 2650, Loss : 0.42430, Acc : 0.825, Sensitive_Loss : 0.22481, Sensitive_Acc : 17.800, Run Time : 7.31 sec
INFO:root:2024-04-14 13:20:17, Train, Epoch : 5, Step : 2660, Loss : 0.43171, Acc : 0.809, Sensitive_Loss : 0.19037, Sensitive_Acc : 22.400, Run Time : 6.99 sec
INFO:root:2024-04-14 13:20:24, Train, Epoch : 5, Step : 2670, Loss : 0.40083, Acc : 0.791, Sensitive_Loss : 0.19262, Sensitive_Acc : 24.900, Run Time : 7.04 sec
INFO:root:2024-04-14 13:20:31, Train, Epoch : 5, Step : 2680, Loss : 0.44800, Acc : 0.809, Sensitive_Loss : 0.21539, Sensitive_Acc : 23.100, Run Time : 6.98 sec
INFO:root:2024-04-14 13:20:38, Train, Epoch : 5, Step : 2690, Loss : 0.36944, Acc : 0.834, Sensitive_Loss : 0.16572, Sensitive_Acc : 23.700, Run Time : 7.32 sec
INFO:root:2024-04-14 13:20:45, Train, Epoch : 5, Step : 2700, Loss : 0.43823, Acc : 0.803, Sensitive_Loss : 0.15859, Sensitive_Acc : 24.500, Run Time : 6.89 sec
INFO:root:2024-04-14 13:22:13, Dev, Step : 2700, Loss : 0.50236, Acc : 0.781, Auc : 0.861, Sensitive_Loss : 0.24335, Sensitive_Acc : 21.120, Sensitive_Auc : 0.995, Mean auc: 0.861, Run Time : 87.67 sec
INFO:root:2024-04-14 13:22:18, Train, Epoch : 5, Step : 2710, Loss : 0.37223, Acc : 0.800, Sensitive_Loss : 0.14311, Sensitive_Acc : 15.800, Run Time : 93.20 sec
INFO:root:2024-04-14 13:22:26, Train, Epoch : 5, Step : 2720, Loss : 0.41771, Acc : 0.791, Sensitive_Loss : 0.12169, Sensitive_Acc : 23.900, Run Time : 7.50 sec
INFO:root:2024-04-14 13:22:33, Train, Epoch : 5, Step : 2730, Loss : 0.38739, Acc : 0.831, Sensitive_Loss : 0.19673, Sensitive_Acc : 18.500, Run Time : 6.90 sec
INFO:root:2024-04-14 13:22:40, Train, Epoch : 5, Step : 2740, Loss : 0.40882, Acc : 0.834, Sensitive_Loss : 0.14315, Sensitive_Acc : 22.200, Run Time : 7.20 sec
INFO:root:2024-04-14 13:22:47, Train, Epoch : 5, Step : 2750, Loss : 0.37557, Acc : 0.828, Sensitive_Loss : 0.11717, Sensitive_Acc : 19.300, Run Time : 6.61 sec
INFO:root:2024-04-14 13:22:54, Train, Epoch : 5, Step : 2760, Loss : 0.38200, Acc : 0.850, Sensitive_Loss : 0.17659, Sensitive_Acc : 19.700, Run Time : 7.04 sec
INFO:root:2024-04-14 13:23:00, Train, Epoch : 5, Step : 2770, Loss : 0.37381, Acc : 0.838, Sensitive_Loss : 0.15662, Sensitive_Acc : 21.500, Run Time : 6.59 sec
INFO:root:2024-04-14 13:23:07, Train, Epoch : 5, Step : 2780, Loss : 0.38405, Acc : 0.844, Sensitive_Loss : 0.17751, Sensitive_Acc : 24.600, Run Time : 7.14 sec
INFO:root:2024-04-14 13:23:15, Train, Epoch : 5, Step : 2790, Loss : 0.52185, Acc : 0.769, Sensitive_Loss : 0.20921, Sensitive_Acc : 24.200, Run Time : 7.47 sec
INFO:root:2024-04-14 13:23:22, Train, Epoch : 5, Step : 2800, Loss : 0.36589, Acc : 0.831, Sensitive_Loss : 0.21054, Sensitive_Acc : 26.800, Run Time : 6.86 sec
INFO:root:2024-04-14 13:24:49, Dev, Step : 2800, Loss : 0.52906, Acc : 0.767, Auc : 0.860, Sensitive_Loss : 0.25007, Sensitive_Acc : 21.241, Sensitive_Auc : 0.995, Mean auc: 0.860, Run Time : 87.68 sec
INFO:root:2024-04-14 13:24:55, Train, Epoch : 5, Step : 2810, Loss : 0.48203, Acc : 0.784, Sensitive_Loss : 0.16908, Sensitive_Acc : 18.900, Run Time : 93.44 sec
INFO:root:2024-04-14 13:25:02, Train, Epoch : 5, Step : 2820, Loss : 0.37240, Acc : 0.816, Sensitive_Loss : 0.19832, Sensitive_Acc : 19.600, Run Time : 6.84 sec
INFO:root:2024-04-14 13:25:09, Train, Epoch : 5, Step : 2830, Loss : 0.35527, Acc : 0.841, Sensitive_Loss : 0.18054, Sensitive_Acc : 26.300, Run Time : 7.21 sec
INFO:root:2024-04-14 13:25:16, Train, Epoch : 5, Step : 2840, Loss : 0.34790, Acc : 0.838, Sensitive_Loss : 0.21124, Sensitive_Acc : 21.800, Run Time : 6.79 sec
INFO:root:2024-04-14 13:25:24, Train, Epoch : 5, Step : 2850, Loss : 0.38593, Acc : 0.825, Sensitive_Loss : 0.19140, Sensitive_Acc : 24.300, Run Time : 7.63 sec
INFO:root:2024-04-14 13:25:31, Train, Epoch : 5, Step : 2860, Loss : 0.30552, Acc : 0.878, Sensitive_Loss : 0.13973, Sensitive_Acc : 16.000, Run Time : 7.05 sec
INFO:root:2024-04-14 13:25:37, Train, Epoch : 5, Step : 2870, Loss : 0.40273, Acc : 0.822, Sensitive_Loss : 0.13161, Sensitive_Acc : 20.900, Run Time : 6.79 sec
INFO:root:2024-04-14 13:25:44, Train, Epoch : 5, Step : 2880, Loss : 0.28976, Acc : 0.825, Sensitive_Loss : 0.16149, Sensitive_Acc : 19.700, Run Time : 7.01 sec
INFO:root:2024-04-14 13:25:52, Train, Epoch : 5, Step : 2890, Loss : 0.35473, Acc : 0.819, Sensitive_Loss : 0.12557, Sensitive_Acc : 21.300, Run Time : 7.30 sec
INFO:root:2024-04-14 13:25:59, Train, Epoch : 5, Step : 2900, Loss : 0.38313, Acc : 0.831, Sensitive_Loss : 0.15075, Sensitive_Acc : 23.000, Run Time : 7.09 sec
INFO:root:2024-04-14 13:27:26, Dev, Step : 2900, Loss : 0.51063, Acc : 0.778, Auc : 0.863, Sensitive_Loss : 0.23192, Sensitive_Acc : 21.376, Sensitive_Auc : 0.996, Mean auc: 0.863, Run Time : 86.94 sec
INFO:root:2024-04-14 13:27:26, Best, Step : 2900, Loss : 0.51063, Acc : 0.778, Auc : 0.863, Sensitive_Loss : 0.23192, Sensitive_Acc : 21.376, Sensitive_Auc : 0.996, Best Auc : 0.863
INFO:root:2024-04-14 13:27:32, Train, Epoch : 5, Step : 2910, Loss : 0.34484, Acc : 0.806, Sensitive_Loss : 0.21100, Sensitive_Acc : 22.600, Run Time : 93.44 sec
INFO:root:2024-04-14 13:27:39, Train, Epoch : 5, Step : 2920, Loss : 0.46213, Acc : 0.787, Sensitive_Loss : 0.24569, Sensitive_Acc : 21.900, Run Time : 7.11 sec
INFO:root:2024-04-14 13:27:46, Train, Epoch : 5, Step : 2930, Loss : 0.42819, Acc : 0.822, Sensitive_Loss : 0.19093, Sensitive_Acc : 22.200, Run Time : 6.81 sec
INFO:root:2024-04-14 13:27:54, Train, Epoch : 5, Step : 2940, Loss : 0.46913, Acc : 0.816, Sensitive_Loss : 0.19862, Sensitive_Acc : 14.800, Run Time : 7.58 sec
INFO:root:2024-04-14 13:28:00, Train, Epoch : 5, Step : 2950, Loss : 0.36357, Acc : 0.872, Sensitive_Loss : 0.18797, Sensitive_Acc : 26.500, Run Time : 6.38 sec
INFO:root:2024-04-14 13:28:08, Train, Epoch : 5, Step : 2960, Loss : 0.38105, Acc : 0.822, Sensitive_Loss : 0.11589, Sensitive_Acc : 22.600, Run Time : 7.78 sec
INFO:root:2024-04-14 13:28:15, Train, Epoch : 5, Step : 2970, Loss : 0.47715, Acc : 0.809, Sensitive_Loss : 0.17843, Sensitive_Acc : 16.600, Run Time : 6.84 sec
INFO:root:2024-04-14 13:28:22, Train, Epoch : 5, Step : 2980, Loss : 0.38970, Acc : 0.847, Sensitive_Loss : 0.19401, Sensitive_Acc : 19.100, Run Time : 7.51 sec
INFO:root:2024-04-14 13:28:30, Train, Epoch : 5, Step : 2990, Loss : 0.38138, Acc : 0.831, Sensitive_Loss : 0.14273, Sensitive_Acc : 25.500, Run Time : 7.31 sec
INFO:root:2024-04-14 13:28:36, Train, Epoch : 5, Step : 3000, Loss : 0.41988, Acc : 0.800, Sensitive_Loss : 0.16603, Sensitive_Acc : 24.500, Run Time : 6.76 sec
INFO:root:2024-04-14 13:30:04, Dev, Step : 3000, Loss : 0.52704, Acc : 0.775, Auc : 0.864, Sensitive_Loss : 0.29481, Sensitive_Acc : 20.504, Sensitive_Auc : 0.997, Mean auc: 0.864, Run Time : 87.40 sec
INFO:root:2024-04-14 13:30:04, Best, Step : 3000, Loss : 0.52704, Acc : 0.775, Auc : 0.864, Sensitive_Loss : 0.29481, Sensitive_Acc : 20.504, Sensitive_Auc : 0.997, Best Auc : 0.864
INFO:root:2024-04-14 13:30:10, Train, Epoch : 5, Step : 3010, Loss : 0.49225, Acc : 0.800, Sensitive_Loss : 0.26207, Sensitive_Acc : 20.700, Run Time : 93.72 sec
INFO:root:2024-04-14 13:30:17, Train, Epoch : 5, Step : 3020, Loss : 0.45106, Acc : 0.797, Sensitive_Loss : 0.14482, Sensitive_Acc : 26.200, Run Time : 6.98 sec
INFO:root:2024-04-14 13:30:24, Train, Epoch : 5, Step : 3030, Loss : 0.45109, Acc : 0.800, Sensitive_Loss : 0.14466, Sensitive_Acc : 20.300, Run Time : 7.23 sec
INFO:root:2024-04-14 13:30:31, Train, Epoch : 5, Step : 3040, Loss : 0.35464, Acc : 0.841, Sensitive_Loss : 0.17822, Sensitive_Acc : 14.800, Run Time : 6.97 sec
INFO:root:2024-04-14 13:30:38, Train, Epoch : 5, Step : 3050, Loss : 0.38591, Acc : 0.828, Sensitive_Loss : 0.15417, Sensitive_Acc : 21.100, Run Time : 7.08 sec
INFO:root:2024-04-14 13:30:46, Train, Epoch : 5, Step : 3060, Loss : 0.38524, Acc : 0.850, Sensitive_Loss : 0.23393, Sensitive_Acc : 22.300, Run Time : 7.47 sec
INFO:root:2024-04-14 13:30:53, Train, Epoch : 5, Step : 3070, Loss : 0.35814, Acc : 0.809, Sensitive_Loss : 0.15389, Sensitive_Acc : 21.300, Run Time : 6.79 sec
INFO:root:2024-04-14 13:31:00, Train, Epoch : 5, Step : 3080, Loss : 0.38207, Acc : 0.866, Sensitive_Loss : 0.14583, Sensitive_Acc : 17.600, Run Time : 7.00 sec
INFO:root:2024-04-14 13:31:07, Train, Epoch : 5, Step : 3090, Loss : 0.48229, Acc : 0.816, Sensitive_Loss : 0.13340, Sensitive_Acc : 24.200, Run Time : 7.02 sec
INFO:root:2024-04-14 13:31:14, Train, Epoch : 5, Step : 3100, Loss : 0.40418, Acc : 0.844, Sensitive_Loss : 0.20139, Sensitive_Acc : 23.900, Run Time : 7.10 sec
INFO:root:2024-04-14 13:32:41, Dev, Step : 3100, Loss : 0.52234, Acc : 0.776, Auc : 0.864, Sensitive_Loss : 0.26368, Sensitive_Acc : 20.985, Sensitive_Auc : 0.997, Mean auc: 0.864, Run Time : 87.18 sec
INFO:root:2024-04-14 13:32:46, Train, Epoch : 5, Step : 3110, Loss : 0.36087, Acc : 0.853, Sensitive_Loss : 0.17420, Sensitive_Acc : 22.600, Run Time : 92.78 sec
INFO:root:2024-04-14 13:32:53, Train, Epoch : 5, Step : 3120, Loss : 0.36881, Acc : 0.809, Sensitive_Loss : 0.20916, Sensitive_Acc : 19.000, Run Time : 6.90 sec
INFO:root:2024-04-14 13:33:01, Train, Epoch : 5, Step : 3130, Loss : 0.43813, Acc : 0.812, Sensitive_Loss : 0.18254, Sensitive_Acc : 20.200, Run Time : 7.44 sec
INFO:root:2024-04-14 13:33:08, Train, Epoch : 5, Step : 3140, Loss : 0.32262, Acc : 0.859, Sensitive_Loss : 0.18441, Sensitive_Acc : 23.200, Run Time : 7.27 sec
INFO:root:2024-04-14 13:33:15, Train, Epoch : 5, Step : 3150, Loss : 0.40786, Acc : 0.797, Sensitive_Loss : 0.15713, Sensitive_Acc : 21.400, Run Time : 6.66 sec
INFO:root:2024-04-14 13:33:22, Train, Epoch : 5, Step : 3160, Loss : 0.42640, Acc : 0.819, Sensitive_Loss : 0.14809, Sensitive_Acc : 26.000, Run Time : 7.31 sec
INFO:root:2024-04-14 13:33:30, Train, Epoch : 5, Step : 3170, Loss : 0.44746, Acc : 0.809, Sensitive_Loss : 0.15831, Sensitive_Acc : 23.000, Run Time : 7.49 sec
INFO:root:2024-04-14 13:34:58
INFO:root:y_pred: [0.04817967 0.00310326 0.0507259  ... 0.33778226 0.01538473 0.0673483 ]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [9.54304636e-03 3.25816423e-02 5.29920161e-02 5.06073907e-02
 1.93611030e-02 8.52004886e-02 7.46665969e-02 6.38962258e-03
 4.56809923e-02 9.99615669e-01 5.45688033e-01 2.91816611e-02
 2.03027390e-03 3.47247859e-03 9.98981535e-01 4.33659181e-02
 4.79986332e-03 9.99000490e-01 9.98074055e-01 3.52630727e-02
 9.93464410e-01 9.73319286e-04 3.84237170e-01 1.35452420e-01
 3.71528156e-02 4.76897508e-01 6.28185953e-05 6.50226548e-02
 5.18533518e-04 4.82307300e-02 1.68278858e-01 9.88569677e-01
 1.55284002e-01 8.95577252e-01 4.43101861e-04 2.97344348e-04
 1.96295488e-03 8.56760889e-03 1.11869993e-02 3.43482979e-02
 1.45075992e-01 9.71920609e-01 3.06863250e-04 5.58394827e-02
 9.99102116e-01 7.71930873e-01 7.57341146e-01 2.40373775e-01
 6.09860957e-01 9.63389516e-01 9.98080730e-01 9.99314070e-01
 9.97772634e-01 1.24159073e-02 1.14295259e-01 5.60283542e-01
 2.97406426e-04 8.33263174e-02 9.86982405e-01 1.37969991e-02
 1.43080787e-03 2.12951191e-03 3.68678458e-02 9.29326925e-04
 9.99546230e-01 3.79421890e-01 3.63950559e-04 1.57292068e-01
 8.53387231e-04 9.76782382e-01 9.99837041e-01 9.99995589e-01
 2.68749590e-03 5.68454087e-01 2.20454913e-02 4.07833874e-01
 5.16896434e-02 2.43810777e-04 1.77070498e-03 3.62320314e-03
 1.92624509e-01 1.03070838e-02 9.99527335e-01 9.99556482e-01
 9.95169487e-03 1.15859374e-01 2.73204237e-01 9.37013142e-03
 3.00972518e-02 2.72226520e-03 1.81680396e-02 6.53713822e-01
 3.15550878e-03 4.95156331e-04 1.80400175e-03 1.32997688e-02
 2.20815139e-03 5.56485176e-01 3.53768207e-02 1.63732469e-02
 4.53657210e-02 1.11113191e-02 4.70457345e-01 1.12296408e-02
 2.55517513e-01 4.87446645e-03 3.09086412e-01 6.78898752e-01
 4.54564840e-01 3.07469547e-01 1.08832377e-03 9.99670863e-01
 9.99396682e-01 1.01694325e-03 9.68261510e-02 4.22015160e-01
 5.42436242e-02 5.76552411e-04 4.02563930e-01 1.82664115e-02
 1.58858635e-02 1.09045173e-03 4.14292626e-02 2.12488626e-03
 7.30216876e-02 7.96100348e-02 2.64114671e-04 9.87648010e-01
 1.51509151e-01 1.48158506e-01 2.98157483e-02 1.56061873e-01
 2.00302129e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-14 13:34:58, Dev, Step : 3170, Loss : 0.50418, Acc : 0.783, Auc : 0.863, Sensitive_Loss : 0.26051, Sensitive_Acc : 21.015, Sensitive_Auc : 0.997, Mean auc: 0.863, Run Time : 88.12 sec
INFO:root:2024-04-14 13:35:07, Train, Epoch : 6, Step : 3180, Loss : 0.40788, Acc : 0.794, Sensitive_Loss : 0.23221, Sensitive_Acc : 20.900, Run Time : 8.26 sec
INFO:root:2024-04-14 13:35:14, Train, Epoch : 6, Step : 3190, Loss : 0.38761, Acc : 0.850, Sensitive_Loss : 0.15245, Sensitive_Acc : 25.600, Run Time : 7.05 sec
INFO:root:2024-04-14 13:35:21, Train, Epoch : 6, Step : 3200, Loss : 0.40441, Acc : 0.825, Sensitive_Loss : 0.14927, Sensitive_Acc : 22.000, Run Time : 6.87 sec
INFO:root:2024-04-14 13:36:48, Dev, Step : 3200, Loss : 0.50800, Acc : 0.784, Auc : 0.863, Sensitive_Loss : 0.27227, Sensitive_Acc : 20.835, Sensitive_Auc : 0.996, Mean auc: 0.863, Run Time : 87.21 sec
INFO:root:2024-04-14 13:36:53, Train, Epoch : 6, Step : 3210, Loss : 0.38657, Acc : 0.822, Sensitive_Loss : 0.16143, Sensitive_Acc : 24.200, Run Time : 92.58 sec
INFO:root:2024-04-14 13:37:01, Train, Epoch : 6, Step : 3220, Loss : 0.32998, Acc : 0.859, Sensitive_Loss : 0.15682, Sensitive_Acc : 23.000, Run Time : 7.38 sec
INFO:root:2024-04-14 13:37:08, Train, Epoch : 6, Step : 3230, Loss : 0.37254, Acc : 0.794, Sensitive_Loss : 0.16864, Sensitive_Acc : 23.700, Run Time : 6.99 sec
INFO:root:2024-04-14 13:37:15, Train, Epoch : 6, Step : 3240, Loss : 0.39161, Acc : 0.853, Sensitive_Loss : 0.15723, Sensitive_Acc : 18.700, Run Time : 7.18 sec
INFO:root:2024-04-14 13:37:22, Train, Epoch : 6, Step : 3250, Loss : 0.35730, Acc : 0.834, Sensitive_Loss : 0.14314, Sensitive_Acc : 23.900, Run Time : 6.91 sec
INFO:root:2024-04-14 13:37:29, Train, Epoch : 6, Step : 3260, Loss : 0.33981, Acc : 0.887, Sensitive_Loss : 0.14930, Sensitive_Acc : 19.900, Run Time : 7.13 sec
INFO:root:2024-04-14 13:37:37, Train, Epoch : 6, Step : 3270, Loss : 0.34971, Acc : 0.856, Sensitive_Loss : 0.23837, Sensitive_Acc : 19.100, Run Time : 7.86 sec
INFO:root:2024-04-14 13:37:44, Train, Epoch : 6, Step : 3280, Loss : 0.33248, Acc : 0.863, Sensitive_Loss : 0.14327, Sensitive_Acc : 23.400, Run Time : 6.85 sec
INFO:root:2024-04-14 13:37:51, Train, Epoch : 6, Step : 3290, Loss : 0.42973, Acc : 0.822, Sensitive_Loss : 0.15317, Sensitive_Acc : 16.700, Run Time : 7.10 sec
INFO:root:2024-04-14 13:37:58, Train, Epoch : 6, Step : 3300, Loss : 0.40303, Acc : 0.844, Sensitive_Loss : 0.13644, Sensitive_Acc : 22.400, Run Time : 6.91 sec
INFO:root:2024-04-14 13:39:25, Dev, Step : 3300, Loss : 0.50537, Acc : 0.782, Auc : 0.859, Sensitive_Loss : 0.27701, Sensitive_Acc : 20.699, Sensitive_Auc : 0.994, Mean auc: 0.859, Run Time : 87.19 sec
INFO:root:2024-04-14 13:39:31, Train, Epoch : 6, Step : 3310, Loss : 0.33570, Acc : 0.841, Sensitive_Loss : 0.14712, Sensitive_Acc : 20.400, Run Time : 93.02 sec
INFO:root:2024-04-14 13:39:38, Train, Epoch : 6, Step : 3320, Loss : 0.38399, Acc : 0.825, Sensitive_Loss : 0.14947, Sensitive_Acc : 23.900, Run Time : 6.79 sec
INFO:root:2024-04-14 13:39:45, Train, Epoch : 6, Step : 3330, Loss : 0.34144, Acc : 0.875, Sensitive_Loss : 0.20385, Sensitive_Acc : 19.000, Run Time : 7.02 sec
INFO:root:2024-04-14 13:39:52, Train, Epoch : 6, Step : 3340, Loss : 0.38651, Acc : 0.841, Sensitive_Loss : 0.20154, Sensitive_Acc : 24.000, Run Time : 7.09 sec
INFO:root:2024-04-14 13:39:59, Train, Epoch : 6, Step : 3350, Loss : 0.36543, Acc : 0.828, Sensitive_Loss : 0.21004, Sensitive_Acc : 23.900, Run Time : 7.26 sec
INFO:root:2024-04-14 13:40:06, Train, Epoch : 6, Step : 3360, Loss : 0.36922, Acc : 0.828, Sensitive_Loss : 0.10276, Sensitive_Acc : 21.200, Run Time : 6.85 sec
INFO:root:2024-04-14 13:40:13, Train, Epoch : 6, Step : 3370, Loss : 0.49934, Acc : 0.794, Sensitive_Loss : 0.13453, Sensitive_Acc : 23.500, Run Time : 7.11 sec
INFO:root:2024-04-14 13:40:20, Train, Epoch : 6, Step : 3380, Loss : 0.37064, Acc : 0.812, Sensitive_Loss : 0.16305, Sensitive_Acc : 26.100, Run Time : 7.25 sec
INFO:root:2024-04-14 13:40:27, Train, Epoch : 6, Step : 3390, Loss : 0.35432, Acc : 0.856, Sensitive_Loss : 0.10465, Sensitive_Acc : 20.200, Run Time : 7.09 sec
INFO:root:2024-04-14 13:40:34, Train, Epoch : 6, Step : 3400, Loss : 0.38033, Acc : 0.831, Sensitive_Loss : 0.13805, Sensitive_Acc : 19.600, Run Time : 7.15 sec
INFO:root:2024-04-14 13:42:02, Dev, Step : 3400, Loss : 0.50022, Acc : 0.782, Auc : 0.863, Sensitive_Loss : 0.25177, Sensitive_Acc : 21.331, Sensitive_Auc : 0.994, Mean auc: 0.863, Run Time : 87.39 sec
INFO:root:2024-04-14 13:42:07, Train, Epoch : 6, Step : 3410, Loss : 0.36253, Acc : 0.831, Sensitive_Loss : 0.15357, Sensitive_Acc : 15.000, Run Time : 93.03 sec
INFO:root:2024-04-14 13:42:14, Train, Epoch : 6, Step : 3420, Loss : 0.38171, Acc : 0.819, Sensitive_Loss : 0.23556, Sensitive_Acc : 25.600, Run Time : 7.07 sec
INFO:root:2024-04-14 13:42:22, Train, Epoch : 6, Step : 3430, Loss : 0.41976, Acc : 0.828, Sensitive_Loss : 0.11395, Sensitive_Acc : 24.200, Run Time : 7.53 sec
INFO:root:2024-04-14 13:42:29, Train, Epoch : 6, Step : 3440, Loss : 0.45143, Acc : 0.806, Sensitive_Loss : 0.19565, Sensitive_Acc : 21.500, Run Time : 6.51 sec
INFO:root:2024-04-14 13:42:36, Train, Epoch : 6, Step : 3450, Loss : 0.36238, Acc : 0.828, Sensitive_Loss : 0.18190, Sensitive_Acc : 21.900, Run Time : 7.48 sec
INFO:root:2024-04-14 13:42:43, Train, Epoch : 6, Step : 3460, Loss : 0.34420, Acc : 0.834, Sensitive_Loss : 0.15881, Sensitive_Acc : 25.100, Run Time : 6.79 sec
INFO:root:2024-04-14 13:42:50, Train, Epoch : 6, Step : 3470, Loss : 0.40978, Acc : 0.819, Sensitive_Loss : 0.14630, Sensitive_Acc : 18.100, Run Time : 6.93 sec
INFO:root:2024-04-14 13:42:57, Train, Epoch : 6, Step : 3480, Loss : 0.35201, Acc : 0.838, Sensitive_Loss : 0.15776, Sensitive_Acc : 14.500, Run Time : 6.99 sec
INFO:root:2024-04-14 13:43:04, Train, Epoch : 6, Step : 3490, Loss : 0.37949, Acc : 0.819, Sensitive_Loss : 0.18849, Sensitive_Acc : 20.700, Run Time : 7.03 sec
INFO:root:2024-04-14 13:43:11, Train, Epoch : 6, Step : 3500, Loss : 0.35326, Acc : 0.838, Sensitive_Loss : 0.18925, Sensitive_Acc : 21.900, Run Time : 7.63 sec
INFO:root:2024-04-14 13:44:39, Dev, Step : 3500, Loss : 0.52072, Acc : 0.776, Auc : 0.863, Sensitive_Loss : 0.22103, Sensitive_Acc : 21.511, Sensitive_Auc : 0.993, Mean auc: 0.863, Run Time : 87.17 sec
INFO:root:2024-04-14 13:44:45, Train, Epoch : 6, Step : 3510, Loss : 0.35428, Acc : 0.853, Sensitive_Loss : 0.16398, Sensitive_Acc : 17.700, Run Time : 93.21 sec
INFO:root:2024-04-14 13:44:51, Train, Epoch : 6, Step : 3520, Loss : 0.37683, Acc : 0.859, Sensitive_Loss : 0.16850, Sensitive_Acc : 23.900, Run Time : 6.57 sec
INFO:root:2024-04-14 13:44:58, Train, Epoch : 6, Step : 3530, Loss : 0.44643, Acc : 0.803, Sensitive_Loss : 0.14675, Sensitive_Acc : 21.000, Run Time : 7.12 sec
INFO:root:2024-04-14 13:45:05, Train, Epoch : 6, Step : 3540, Loss : 0.41097, Acc : 0.831, Sensitive_Loss : 0.20175, Sensitive_Acc : 20.500, Run Time : 6.61 sec
INFO:root:2024-04-14 13:45:12, Train, Epoch : 6, Step : 3550, Loss : 0.32130, Acc : 0.884, Sensitive_Loss : 0.15871, Sensitive_Acc : 19.800, Run Time : 7.36 sec
INFO:root:2024-04-14 13:45:20, Train, Epoch : 6, Step : 3560, Loss : 0.31967, Acc : 0.856, Sensitive_Loss : 0.13659, Sensitive_Acc : 24.800, Run Time : 7.34 sec
INFO:root:2024-04-14 13:45:27, Train, Epoch : 6, Step : 3570, Loss : 0.35651, Acc : 0.866, Sensitive_Loss : 0.14557, Sensitive_Acc : 22.200, Run Time : 7.13 sec
INFO:root:2024-04-14 13:45:34, Train, Epoch : 6, Step : 3580, Loss : 0.37447, Acc : 0.850, Sensitive_Loss : 0.21851, Sensitive_Acc : 22.500, Run Time : 7.30 sec
INFO:root:2024-04-14 13:45:41, Train, Epoch : 6, Step : 3590, Loss : 0.38000, Acc : 0.828, Sensitive_Loss : 0.09890, Sensitive_Acc : 19.900, Run Time : 6.74 sec
INFO:root:2024-04-14 13:45:48, Train, Epoch : 6, Step : 3600, Loss : 0.41422, Acc : 0.831, Sensitive_Loss : 0.14921, Sensitive_Acc : 20.200, Run Time : 7.43 sec
INFO:root:2024-04-14 13:47:15, Dev, Step : 3600, Loss : 0.51242, Acc : 0.781, Auc : 0.863, Sensitive_Loss : 0.22681, Sensitive_Acc : 21.541, Sensitive_Auc : 0.994, Mean auc: 0.863, Run Time : 87.11 sec
INFO:root:2024-04-14 13:47:21, Train, Epoch : 6, Step : 3610, Loss : 0.32524, Acc : 0.834, Sensitive_Loss : 0.17267, Sensitive_Acc : 19.900, Run Time : 92.62 sec
INFO:root:2024-04-14 13:47:28, Train, Epoch : 6, Step : 3620, Loss : 0.33818, Acc : 0.847, Sensitive_Loss : 0.20174, Sensitive_Acc : 22.400, Run Time : 7.07 sec
INFO:root:2024-04-14 13:47:35, Train, Epoch : 6, Step : 3630, Loss : 0.34872, Acc : 0.831, Sensitive_Loss : 0.16548, Sensitive_Acc : 19.800, Run Time : 7.23 sec
INFO:root:2024-04-14 13:47:43, Train, Epoch : 6, Step : 3640, Loss : 0.35595, Acc : 0.856, Sensitive_Loss : 0.10520, Sensitive_Acc : 19.500, Run Time : 7.54 sec
INFO:root:2024-04-14 13:47:49, Train, Epoch : 6, Step : 3650, Loss : 0.39937, Acc : 0.853, Sensitive_Loss : 0.20417, Sensitive_Acc : 24.300, Run Time : 6.68 sec
INFO:root:2024-04-14 13:47:57, Train, Epoch : 6, Step : 3660, Loss : 0.38404, Acc : 0.825, Sensitive_Loss : 0.14019, Sensitive_Acc : 22.400, Run Time : 7.15 sec
INFO:root:2024-04-14 13:48:03, Train, Epoch : 6, Step : 3670, Loss : 0.41438, Acc : 0.812, Sensitive_Loss : 0.11980, Sensitive_Acc : 17.800, Run Time : 6.81 sec
INFO:root:2024-04-14 13:48:11, Train, Epoch : 6, Step : 3680, Loss : 0.40457, Acc : 0.841, Sensitive_Loss : 0.19819, Sensitive_Acc : 21.300, Run Time : 7.54 sec
INFO:root:2024-04-14 13:48:18, Train, Epoch : 6, Step : 3690, Loss : 0.35035, Acc : 0.841, Sensitive_Loss : 0.20718, Sensitive_Acc : 22.000, Run Time : 6.89 sec
INFO:root:2024-04-14 13:48:25, Train, Epoch : 6, Step : 3700, Loss : 0.32908, Acc : 0.834, Sensitive_Loss : 0.17548, Sensitive_Acc : 23.300, Run Time : 7.34 sec
INFO:root:2024-04-14 13:49:52, Dev, Step : 3700, Loss : 0.53583, Acc : 0.777, Auc : 0.862, Sensitive_Loss : 0.26265, Sensitive_Acc : 21.150, Sensitive_Auc : 0.997, Mean auc: 0.862, Run Time : 87.14 sec
INFO:root:2024-04-14 13:49:58, Train, Epoch : 6, Step : 3710, Loss : 0.31971, Acc : 0.844, Sensitive_Loss : 0.12816, Sensitive_Acc : 20.200, Run Time : 92.42 sec
INFO:root:2024-04-14 13:50:06, Train, Epoch : 6, Step : 3720, Loss : 0.34265, Acc : 0.859, Sensitive_Loss : 0.14872, Sensitive_Acc : 19.400, Run Time : 8.01 sec
INFO:root:2024-04-14 13:50:13, Train, Epoch : 6, Step : 3730, Loss : 0.46173, Acc : 0.806, Sensitive_Loss : 0.21662, Sensitive_Acc : 21.400, Run Time : 7.05 sec
INFO:root:2024-04-14 13:50:20, Train, Epoch : 6, Step : 3740, Loss : 0.37586, Acc : 0.853, Sensitive_Loss : 0.15171, Sensitive_Acc : 19.300, Run Time : 6.98 sec
INFO:root:2024-04-14 13:50:27, Train, Epoch : 6, Step : 3750, Loss : 0.33571, Acc : 0.853, Sensitive_Loss : 0.12573, Sensitive_Acc : 20.600, Run Time : 7.42 sec
INFO:root:2024-04-14 13:50:34, Train, Epoch : 6, Step : 3760, Loss : 0.31692, Acc : 0.850, Sensitive_Loss : 0.19347, Sensitive_Acc : 23.700, Run Time : 6.81 sec
INFO:root:2024-04-14 13:50:41, Train, Epoch : 6, Step : 3770, Loss : 0.38080, Acc : 0.853, Sensitive_Loss : 0.15980, Sensitive_Acc : 19.400, Run Time : 6.90 sec
INFO:root:2024-04-14 13:50:48, Train, Epoch : 6, Step : 3780, Loss : 0.32786, Acc : 0.863, Sensitive_Loss : 0.14223, Sensitive_Acc : 22.200, Run Time : 7.27 sec
INFO:root:2024-04-14 13:50:55, Train, Epoch : 6, Step : 3790, Loss : 0.44276, Acc : 0.816, Sensitive_Loss : 0.14061, Sensitive_Acc : 24.000, Run Time : 6.81 sec
INFO:root:2024-04-14 13:51:02, Train, Epoch : 6, Step : 3800, Loss : 0.46986, Acc : 0.828, Sensitive_Loss : 0.21262, Sensitive_Acc : 24.000, Run Time : 6.98 sec
INFO:root:2024-04-14 13:52:29, Dev, Step : 3800, Loss : 0.54175, Acc : 0.772, Auc : 0.859, Sensitive_Loss : 0.23996, Sensitive_Acc : 21.632, Sensitive_Auc : 0.995, Mean auc: 0.859, Run Time : 87.34 sec
INFO:root:2024-04-14 13:53:56
INFO:root:y_pred: [0.04226659 0.00124811 0.0357582  ... 0.3318882  0.00844867 0.07765808]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [5.0936621e-03 9.2176925e-03 1.8537015e-02 4.6008117e-02 1.5346748e-02
 3.1530228e-02 4.3768860e-02 4.2494750e-03 2.1862742e-02 9.9966931e-01
 2.8488669e-01 3.0323235e-02 8.3268067e-04 2.6278580e-03 9.9873406e-01
 2.4489183e-02 4.2088619e-03 9.9869210e-01 9.9849296e-01 2.8742388e-02
 9.8461306e-01 5.3264090e-04 2.3993158e-01 9.5844552e-02 1.2475734e-02
 4.0453088e-01 7.8377780e-05 8.8463217e-02 3.1308411e-04 2.4241982e-02
 8.8394664e-02 9.8576444e-01 1.5249106e-01 9.2242920e-01 1.7995146e-04
 1.8304428e-04 2.2741172e-03 7.6613240e-03 3.4327901e-03 3.3209503e-02
 9.0488806e-02 9.4952351e-01 4.1163317e-04 2.0377761e-02 9.9862194e-01
 8.0090326e-01 6.9257647e-01 2.5698197e-01 2.8560719e-01 9.7178835e-01
 9.9504954e-01 9.9762648e-01 9.9617285e-01 8.3755413e-03 9.0420507e-02
 5.8640409e-01 1.0601197e-04 7.9282098e-02 9.8333180e-01 5.6106881e-03
 7.0774724e-04 2.4890408e-03 2.4795549e-02 9.6766290e-04 9.9928290e-01
 2.8654695e-01 1.2969351e-04 1.2318399e-01 3.6080048e-04 9.6776521e-01
 9.9980849e-01 9.9998629e-01 1.1515570e-03 4.1613510e-01 1.1400354e-02
 3.9258328e-01 1.9887246e-02 2.4675223e-04 1.2451338e-03 2.1794972e-03
 1.1860370e-01 1.1918677e-02 9.9915445e-01 9.9796212e-01 2.8459246e-03
 1.1470990e-01 2.4130541e-01 4.1094651e-03 2.9530818e-02 3.2192601e-03
 8.0730235e-03 3.4924147e-01 7.8398548e-04 4.0751116e-04 6.8669021e-04
 5.8110384e-03 7.2755275e-04 2.5173274e-01 9.8022372e-03 1.2117213e-02
 1.8002756e-02 6.1267489e-03 2.6445705e-01 1.2936251e-02 1.3814950e-01
 4.9507711e-03 1.9523421e-01 5.7680619e-01 4.0587500e-01 1.5515639e-01
 3.9754270e-04 9.9948221e-01 9.9964023e-01 2.8597290e-04 1.0264070e-01
 3.6518508e-01 3.7795532e-02 6.5271882e-04 2.1787986e-01 8.2310289e-03
 1.6823787e-02 7.5126800e-04 3.0415544e-02 9.1966492e-04 4.5075879e-02
 7.3684976e-02 2.7650065e-04 9.7657317e-01 1.0925298e-01 6.6628210e-02
 3.7535999e-02 1.9663893e-01 3.1932042e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-14 13:53:56, Dev, Step : 3804, Loss : 0.52870, Acc : 0.776, Auc : 0.857, Sensitive_Loss : 0.22978, Sensitive_Acc : 21.632, Sensitive_Auc : 0.994, Mean auc: 0.857, Run Time : 85.83 sec
INFO:root:2024-04-14 13:54:03, Train, Epoch : 7, Step : 3810, Loss : 0.22956, Acc : 0.494, Sensitive_Loss : 0.12987, Sensitive_Acc : 10.900, Run Time : 5.83 sec
INFO:root:2024-04-14 13:54:10, Train, Epoch : 7, Step : 3820, Loss : 0.34673, Acc : 0.875, Sensitive_Loss : 0.12905, Sensitive_Acc : 23.800, Run Time : 6.83 sec
INFO:root:2024-04-14 13:54:17, Train, Epoch : 7, Step : 3830, Loss : 0.37378, Acc : 0.844, Sensitive_Loss : 0.13857, Sensitive_Acc : 23.700, Run Time : 7.14 sec
INFO:root:2024-04-14 13:54:24, Train, Epoch : 7, Step : 3840, Loss : 0.32000, Acc : 0.866, Sensitive_Loss : 0.20565, Sensitive_Acc : 24.600, Run Time : 7.02 sec
INFO:root:2024-04-14 13:54:31, Train, Epoch : 7, Step : 3850, Loss : 0.35054, Acc : 0.869, Sensitive_Loss : 0.18258, Sensitive_Acc : 21.600, Run Time : 6.97 sec
INFO:root:2024-04-14 13:54:38, Train, Epoch : 7, Step : 3860, Loss : 0.28857, Acc : 0.884, Sensitive_Loss : 0.18975, Sensitive_Acc : 22.900, Run Time : 7.03 sec
INFO:root:2024-04-14 13:54:45, Train, Epoch : 7, Step : 3870, Loss : 0.39093, Acc : 0.856, Sensitive_Loss : 0.20503, Sensitive_Acc : 18.300, Run Time : 7.47 sec
INFO:root:2024-04-14 13:54:52, Train, Epoch : 7, Step : 3880, Loss : 0.30773, Acc : 0.825, Sensitive_Loss : 0.16675, Sensitive_Acc : 18.600, Run Time : 7.00 sec
INFO:root:2024-04-14 13:55:00, Train, Epoch : 7, Step : 3890, Loss : 0.33448, Acc : 0.856, Sensitive_Loss : 0.18379, Sensitive_Acc : 25.600, Run Time : 7.23 sec
INFO:root:2024-04-14 13:55:07, Train, Epoch : 7, Step : 3900, Loss : 0.37180, Acc : 0.856, Sensitive_Loss : 0.14861, Sensitive_Acc : 19.500, Run Time : 7.02 sec
INFO:root:2024-04-14 13:56:34, Dev, Step : 3900, Loss : 0.53103, Acc : 0.775, Auc : 0.860, Sensitive_Loss : 0.23663, Sensitive_Acc : 21.632, Sensitive_Auc : 0.997, Mean auc: 0.860, Run Time : 87.39 sec
INFO:root:2024-04-14 13:56:39, Train, Epoch : 7, Step : 3910, Loss : 0.35285, Acc : 0.841, Sensitive_Loss : 0.14680, Sensitive_Acc : 20.200, Run Time : 92.86 sec
INFO:root:2024-04-14 13:56:46, Train, Epoch : 7, Step : 3920, Loss : 0.34171, Acc : 0.831, Sensitive_Loss : 0.11224, Sensitive_Acc : 22.900, Run Time : 7.00 sec
INFO:root:2024-04-14 13:56:54, Train, Epoch : 7, Step : 3930, Loss : 0.35424, Acc : 0.875, Sensitive_Loss : 0.17364, Sensitive_Acc : 26.600, Run Time : 7.11 sec
INFO:root:2024-04-14 13:57:01, Train, Epoch : 7, Step : 3940, Loss : 0.35945, Acc : 0.819, Sensitive_Loss : 0.12147, Sensitive_Acc : 18.000, Run Time : 7.20 sec
INFO:root:2024-04-14 13:57:08, Train, Epoch : 7, Step : 3950, Loss : 0.39406, Acc : 0.834, Sensitive_Loss : 0.18217, Sensitive_Acc : 21.600, Run Time : 7.38 sec
INFO:root:2024-04-14 13:57:15, Train, Epoch : 7, Step : 3960, Loss : 0.31482, Acc : 0.841, Sensitive_Loss : 0.12438, Sensitive_Acc : 21.900, Run Time : 6.87 sec
INFO:root:2024-04-14 13:57:22, Train, Epoch : 7, Step : 3970, Loss : 0.35165, Acc : 0.866, Sensitive_Loss : 0.16364, Sensitive_Acc : 25.000, Run Time : 7.01 sec
INFO:root:2024-04-14 13:57:29, Train, Epoch : 7, Step : 3980, Loss : 0.35939, Acc : 0.844, Sensitive_Loss : 0.15553, Sensitive_Acc : 23.600, Run Time : 7.46 sec
INFO:root:2024-04-14 13:57:36, Train, Epoch : 7, Step : 3990, Loss : 0.36204, Acc : 0.834, Sensitive_Loss : 0.10192, Sensitive_Acc : 19.600, Run Time : 6.83 sec
INFO:root:2024-04-14 13:57:43, Train, Epoch : 7, Step : 4000, Loss : 0.31992, Acc : 0.875, Sensitive_Loss : 0.17181, Sensitive_Acc : 24.400, Run Time : 6.83 sec
INFO:root:2024-04-14 13:59:11, Dev, Step : 4000, Loss : 0.53800, Acc : 0.777, Auc : 0.859, Sensitive_Loss : 0.23259, Sensitive_Acc : 21.632, Sensitive_Auc : 0.997, Mean auc: 0.859, Run Time : 87.57 sec
INFO:root:2024-04-14 13:59:16, Train, Epoch : 7, Step : 4010, Loss : 0.36354, Acc : 0.853, Sensitive_Loss : 0.12019, Sensitive_Acc : 17.600, Run Time : 93.05 sec
INFO:root:2024-04-14 13:59:23, Train, Epoch : 7, Step : 4020, Loss : 0.37723, Acc : 0.831, Sensitive_Loss : 0.12823, Sensitive_Acc : 21.300, Run Time : 7.14 sec
INFO:root:2024-04-14 13:59:30, Train, Epoch : 7, Step : 4030, Loss : 0.36923, Acc : 0.828, Sensitive_Loss : 0.18890, Sensitive_Acc : 20.100, Run Time : 7.06 sec
INFO:root:2024-04-14 13:59:38, Train, Epoch : 7, Step : 4040, Loss : 0.35088, Acc : 0.844, Sensitive_Loss : 0.15733, Sensitive_Acc : 19.400, Run Time : 7.22 sec
INFO:root:2024-04-14 13:59:45, Train, Epoch : 7, Step : 4050, Loss : 0.34593, Acc : 0.850, Sensitive_Loss : 0.18463, Sensitive_Acc : 22.000, Run Time : 7.45 sec
INFO:root:2024-04-14 13:59:52, Train, Epoch : 7, Step : 4060, Loss : 0.36177, Acc : 0.822, Sensitive_Loss : 0.18453, Sensitive_Acc : 16.800, Run Time : 6.61 sec
INFO:root:2024-04-14 13:59:59, Train, Epoch : 7, Step : 4070, Loss : 0.41607, Acc : 0.816, Sensitive_Loss : 0.10753, Sensitive_Acc : 21.400, Run Time : 7.68 sec
INFO:root:2024-04-14 14:00:06, Train, Epoch : 7, Step : 4080, Loss : 0.39090, Acc : 0.853, Sensitive_Loss : 0.17060, Sensitive_Acc : 24.600, Run Time : 7.14 sec
INFO:root:2024-04-14 14:00:13, Train, Epoch : 7, Step : 4090, Loss : 0.34029, Acc : 0.872, Sensitive_Loss : 0.16028, Sensitive_Acc : 25.200, Run Time : 6.75 sec
INFO:root:2024-04-14 14:00:21, Train, Epoch : 7, Step : 4100, Loss : 0.31160, Acc : 0.878, Sensitive_Loss : 0.16852, Sensitive_Acc : 18.300, Run Time : 7.65 sec
INFO:root:2024-04-14 14:01:48, Dev, Step : 4100, Loss : 0.52083, Acc : 0.778, Auc : 0.860, Sensitive_Loss : 0.27432, Sensitive_Acc : 20.759, Sensitive_Auc : 0.997, Mean auc: 0.860, Run Time : 87.09 sec
INFO:root:2024-04-14 14:01:53, Train, Epoch : 7, Step : 4110, Loss : 0.37089, Acc : 0.856, Sensitive_Loss : 0.22510, Sensitive_Acc : 23.000, Run Time : 92.54 sec
INFO:root:2024-04-14 14:02:01, Train, Epoch : 7, Step : 4120, Loss : 0.41622, Acc : 0.806, Sensitive_Loss : 0.11432, Sensitive_Acc : 18.600, Run Time : 7.21 sec
INFO:root:2024-04-14 14:02:08, Train, Epoch : 7, Step : 4130, Loss : 0.30756, Acc : 0.859, Sensitive_Loss : 0.14878, Sensitive_Acc : 24.800, Run Time : 7.15 sec
INFO:root:2024-04-14 14:02:15, Train, Epoch : 7, Step : 4140, Loss : 0.34316, Acc : 0.844, Sensitive_Loss : 0.10634, Sensitive_Acc : 20.500, Run Time : 6.92 sec
INFO:root:2024-04-14 14:02:22, Train, Epoch : 7, Step : 4150, Loss : 0.34240, Acc : 0.844, Sensitive_Loss : 0.16031, Sensitive_Acc : 25.600, Run Time : 6.97 sec
INFO:root:2024-04-14 14:02:29, Train, Epoch : 7, Step : 4160, Loss : 0.34626, Acc : 0.838, Sensitive_Loss : 0.12862, Sensitive_Acc : 19.200, Run Time : 7.41 sec
INFO:root:2024-04-14 14:02:36, Train, Epoch : 7, Step : 4170, Loss : 0.31134, Acc : 0.859, Sensitive_Loss : 0.21017, Sensitive_Acc : 25.400, Run Time : 7.05 sec
INFO:root:2024-04-14 14:02:43, Train, Epoch : 7, Step : 4180, Loss : 0.44356, Acc : 0.812, Sensitive_Loss : 0.13740, Sensitive_Acc : 15.900, Run Time : 7.09 sec
INFO:root:2024-04-14 14:02:50, Train, Epoch : 7, Step : 4190, Loss : 0.34559, Acc : 0.853, Sensitive_Loss : 0.14849, Sensitive_Acc : 21.300, Run Time : 6.89 sec
INFO:root:2024-04-14 14:02:57, Train, Epoch : 7, Step : 4200, Loss : 0.34062, Acc : 0.853, Sensitive_Loss : 0.14595, Sensitive_Acc : 24.000, Run Time : 6.94 sec
INFO:root:2024-04-14 14:04:25, Dev, Step : 4200, Loss : 0.52996, Acc : 0.777, Auc : 0.862, Sensitive_Loss : 0.27839, Sensitive_Acc : 20.970, Sensitive_Auc : 0.993, Mean auc: 0.862, Run Time : 87.67 sec
INFO:root:2024-04-14 14:04:30, Train, Epoch : 7, Step : 4210, Loss : 0.37232, Acc : 0.847, Sensitive_Loss : 0.12545, Sensitive_Acc : 19.400, Run Time : 93.10 sec
INFO:root:2024-04-14 14:04:37, Train, Epoch : 7, Step : 4220, Loss : 0.34755, Acc : 0.859, Sensitive_Loss : 0.12512, Sensitive_Acc : 22.100, Run Time : 7.29 sec
INFO:root:2024-04-14 14:04:45, Train, Epoch : 7, Step : 4230, Loss : 0.32041, Acc : 0.847, Sensitive_Loss : 0.15284, Sensitive_Acc : 21.200, Run Time : 7.20 sec
INFO:root:2024-04-14 14:04:51, Train, Epoch : 7, Step : 4240, Loss : 0.33602, Acc : 0.891, Sensitive_Loss : 0.16206, Sensitive_Acc : 23.600, Run Time : 6.67 sec
INFO:root:2024-04-14 14:04:59, Train, Epoch : 7, Step : 4250, Loss : 0.43351, Acc : 0.806, Sensitive_Loss : 0.23526, Sensitive_Acc : 20.600, Run Time : 7.36 sec
INFO:root:2024-04-14 14:05:06, Train, Epoch : 7, Step : 4260, Loss : 0.36092, Acc : 0.869, Sensitive_Loss : 0.14012, Sensitive_Acc : 22.100, Run Time : 7.38 sec
INFO:root:2024-04-14 14:05:13, Train, Epoch : 7, Step : 4270, Loss : 0.39988, Acc : 0.859, Sensitive_Loss : 0.14912, Sensitive_Acc : 24.000, Run Time : 7.22 sec
INFO:root:2024-04-14 14:05:20, Train, Epoch : 7, Step : 4280, Loss : 0.30157, Acc : 0.881, Sensitive_Loss : 0.15022, Sensitive_Acc : 18.800, Run Time : 6.77 sec
INFO:root:2024-04-14 14:05:27, Train, Epoch : 7, Step : 4290, Loss : 0.43214, Acc : 0.806, Sensitive_Loss : 0.13702, Sensitive_Acc : 23.600, Run Time : 6.70 sec
INFO:root:2024-04-14 14:05:34, Train, Epoch : 7, Step : 4300, Loss : 0.40549, Acc : 0.809, Sensitive_Loss : 0.19179, Sensitive_Acc : 17.700, Run Time : 7.27 sec
INFO:root:2024-04-14 14:07:01, Dev, Step : 4300, Loss : 0.52341, Acc : 0.776, Auc : 0.858, Sensitive_Loss : 0.23652, Sensitive_Acc : 21.466, Sensitive_Auc : 0.994, Mean auc: 0.858, Run Time : 87.16 sec
INFO:root:2024-04-14 14:07:07, Train, Epoch : 7, Step : 4310, Loss : 0.39659, Acc : 0.853, Sensitive_Loss : 0.15336, Sensitive_Acc : 18.500, Run Time : 92.66 sec
INFO:root:2024-04-14 14:07:14, Train, Epoch : 7, Step : 4320, Loss : 0.35700, Acc : 0.844, Sensitive_Loss : 0.15132, Sensitive_Acc : 23.600, Run Time : 7.11 sec
INFO:root:2024-04-14 14:07:22, Train, Epoch : 7, Step : 4330, Loss : 0.40560, Acc : 0.822, Sensitive_Loss : 0.15899, Sensitive_Acc : 21.200, Run Time : 7.69 sec
INFO:root:2024-04-14 14:07:28, Train, Epoch : 7, Step : 4340, Loss : 0.38403, Acc : 0.831, Sensitive_Loss : 0.16185, Sensitive_Acc : 19.400, Run Time : 6.20 sec
INFO:root:2024-04-14 14:07:36, Train, Epoch : 7, Step : 4350, Loss : 0.36170, Acc : 0.853, Sensitive_Loss : 0.11585, Sensitive_Acc : 20.200, Run Time : 7.77 sec
INFO:root:2024-04-14 14:07:43, Train, Epoch : 7, Step : 4360, Loss : 0.34738, Acc : 0.856, Sensitive_Loss : 0.16748, Sensitive_Acc : 16.000, Run Time : 7.09 sec
INFO:root:2024-04-14 14:07:49, Train, Epoch : 7, Step : 4370, Loss : 0.37285, Acc : 0.822, Sensitive_Loss : 0.12287, Sensitive_Acc : 22.800, Run Time : 6.64 sec
INFO:root:2024-04-14 14:07:57, Train, Epoch : 7, Step : 4380, Loss : 0.29970, Acc : 0.863, Sensitive_Loss : 0.16535, Sensitive_Acc : 22.500, Run Time : 7.62 sec
INFO:root:2024-04-14 14:08:04, Train, Epoch : 7, Step : 4390, Loss : 0.30888, Acc : 0.878, Sensitive_Loss : 0.16446, Sensitive_Acc : 19.600, Run Time : 6.96 sec
INFO:root:2024-04-14 14:08:11, Train, Epoch : 7, Step : 4400, Loss : 0.37745, Acc : 0.838, Sensitive_Loss : 0.15741, Sensitive_Acc : 19.800, Run Time : 6.77 sec
INFO:root:2024-04-14 14:09:38, Dev, Step : 4400, Loss : 0.52618, Acc : 0.779, Auc : 0.855, Sensitive_Loss : 0.23737, Sensitive_Acc : 21.436, Sensitive_Auc : 0.994, Mean auc: 0.855, Run Time : 87.09 sec
INFO:root:2024-04-14 14:09:43, Train, Epoch : 7, Step : 4410, Loss : 0.39014, Acc : 0.841, Sensitive_Loss : 0.20918, Sensitive_Acc : 22.600, Run Time : 92.78 sec
INFO:root:2024-04-14 14:09:50, Train, Epoch : 7, Step : 4420, Loss : 0.35973, Acc : 0.863, Sensitive_Loss : 0.14115, Sensitive_Acc : 24.300, Run Time : 6.89 sec
INFO:root:2024-04-14 14:09:58, Train, Epoch : 7, Step : 4430, Loss : 0.39299, Acc : 0.850, Sensitive_Loss : 0.15261, Sensitive_Acc : 18.600, Run Time : 7.30 sec
INFO:root:2024-04-14 14:11:29
INFO:root:y_pred: [0.03001782 0.00152971 0.04120411 ... 0.20040423 0.01021877 0.04790255]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.10314107e-02 1.28144259e-02 3.80772725e-02 1.29375726e-01
 2.44764108e-02 5.94590753e-02 1.39101416e-01 6.65774569e-03
 5.11352867e-02 9.99791086e-01 6.55240536e-01 7.38038570e-02
 1.97845092e-03 3.29223135e-03 9.99777377e-01 6.75588399e-02
 7.14659737e-03 9.99713361e-01 9.98215973e-01 4.64748256e-02
 9.83729303e-01 1.49900315e-03 2.95851916e-01 1.28544241e-01
 4.11311239e-02 5.59570014e-01 8.11009304e-05 1.32231131e-01
 3.49244423e-04 6.97303712e-02 2.12778702e-01 9.94118810e-01
 2.83638835e-01 9.73229766e-01 3.47095891e-04 1.79134513e-04
 4.52378485e-03 1.48375724e-02 1.22112045e-02 3.42250802e-02
 1.96237668e-01 9.90156174e-01 3.41186678e-04 1.69894584e-02
 9.98266399e-01 8.86455953e-01 7.71683455e-01 6.05676651e-01
 4.25070792e-01 9.89540994e-01 9.96381104e-01 9.99424696e-01
 9.98530865e-01 1.07019385e-02 1.11151993e-01 8.10797930e-01
 6.69087691e-04 1.15477815e-01 9.95198667e-01 3.88811645e-03
 8.39520362e-04 2.19735038e-03 7.63918981e-02 1.40083989e-03
 9.99879718e-01 3.46697986e-01 2.35108251e-04 1.72759354e-01
 1.16365845e-03 9.80301559e-01 9.99943852e-01 9.99998331e-01
 2.91047711e-03 8.34549189e-01 3.51197124e-02 4.49346155e-01
 3.70644182e-02 6.87663385e-04 2.06047157e-03 7.04535749e-03
 2.95320600e-01 3.72688808e-02 9.99395490e-01 9.99569118e-01
 7.49472808e-03 1.32209152e-01 3.72642070e-01 1.10208550e-02
 3.80129404e-02 9.46734846e-03 3.35576497e-02 5.01856089e-01
 1.62443263e-03 2.43970513e-04 8.96428013e-04 1.45532116e-02
 1.43853668e-03 5.94943762e-01 1.01847485e-01 2.47704890e-02
 2.69779451e-02 1.09501220e-02 5.44381201e-01 2.16714516e-02
 2.13643640e-01 9.55833402e-03 4.29683954e-01 6.96937084e-01
 6.98744416e-01 3.32046092e-01 5.43050235e-04 9.99803007e-01
 9.99745190e-01 6.13929587e-04 7.88977966e-02 6.04705930e-01
 4.06795517e-02 1.15073449e-03 5.30387402e-01 1.78437401e-02
 4.49969843e-02 1.09495432e-03 8.09817165e-02 1.21745956e-03
 6.08892739e-02 1.60415828e-01 3.62516439e-04 9.93354201e-01
 2.70878404e-01 1.87922224e-01 5.24809361e-02 3.38485748e-01
 1.64418670e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-14 14:11:29, Dev, Step : 4438, Loss : 0.54069, Acc : 0.772, Auc : 0.854, Sensitive_Loss : 0.28665, Sensitive_Acc : 20.429, Sensitive_Auc : 0.997, Mean auc: 0.854, Run Time : 86.52 sec
INFO:root:2024-04-14 14:11:33, Train, Epoch : 8, Step : 4440, Loss : 0.09258, Acc : 0.159, Sensitive_Loss : 0.02586, Sensitive_Acc : 3.200, Run Time : 2.76 sec
INFO:root:2024-04-14 14:11:40, Train, Epoch : 8, Step : 4450, Loss : 0.29837, Acc : 0.881, Sensitive_Loss : 0.17828, Sensitive_Acc : 19.100, Run Time : 6.93 sec
INFO:root:2024-04-14 14:11:47, Train, Epoch : 8, Step : 4460, Loss : 0.33772, Acc : 0.859, Sensitive_Loss : 0.16776, Sensitive_Acc : 21.900, Run Time : 7.04 sec
INFO:root:2024-04-14 14:11:54, Train, Epoch : 8, Step : 4470, Loss : 0.36142, Acc : 0.856, Sensitive_Loss : 0.14911, Sensitive_Acc : 21.600, Run Time : 7.32 sec
INFO:root:2024-04-14 14:12:02, Train, Epoch : 8, Step : 4480, Loss : 0.31940, Acc : 0.841, Sensitive_Loss : 0.14024, Sensitive_Acc : 16.700, Run Time : 7.58 sec
INFO:root:2024-04-14 14:12:08, Train, Epoch : 8, Step : 4490, Loss : 0.28066, Acc : 0.847, Sensitive_Loss : 0.11657, Sensitive_Acc : 22.900, Run Time : 6.77 sec
INFO:root:2024-04-14 14:12:15, Train, Epoch : 8, Step : 4500, Loss : 0.27826, Acc : 0.891, Sensitive_Loss : 0.12816, Sensitive_Acc : 23.900, Run Time : 6.88 sec
INFO:root:2024-04-14 14:13:43, Dev, Step : 4500, Loss : 0.53977, Acc : 0.775, Auc : 0.857, Sensitive_Loss : 0.23909, Sensitive_Acc : 21.406, Sensitive_Auc : 0.995, Mean auc: 0.857, Run Time : 87.69 sec
INFO:root:2024-04-14 14:13:49, Train, Epoch : 8, Step : 4510, Loss : 0.32404, Acc : 0.881, Sensitive_Loss : 0.13497, Sensitive_Acc : 16.600, Run Time : 93.37 sec
INFO:root:2024-04-14 14:13:56, Train, Epoch : 8, Step : 4520, Loss : 0.36359, Acc : 0.812, Sensitive_Loss : 0.15021, Sensitive_Acc : 24.000, Run Time : 6.98 sec
INFO:root:2024-04-14 14:14:03, Train, Epoch : 8, Step : 4530, Loss : 0.33831, Acc : 0.850, Sensitive_Loss : 0.24836, Sensitive_Acc : 22.000, Run Time : 7.52 sec
INFO:root:2024-04-14 14:14:10, Train, Epoch : 8, Step : 4540, Loss : 0.34171, Acc : 0.844, Sensitive_Loss : 0.18546, Sensitive_Acc : 19.900, Run Time : 7.17 sec
INFO:root:2024-04-14 14:14:17, Train, Epoch : 8, Step : 4550, Loss : 0.30011, Acc : 0.863, Sensitive_Loss : 0.11214, Sensitive_Acc : 20.300, Run Time : 6.94 sec
INFO:root:2024-04-14 14:14:24, Train, Epoch : 8, Step : 4560, Loss : 0.31090, Acc : 0.872, Sensitive_Loss : 0.23609, Sensitive_Acc : 25.100, Run Time : 7.16 sec
INFO:root:2024-04-14 14:14:31, Train, Epoch : 8, Step : 4570, Loss : 0.28286, Acc : 0.894, Sensitive_Loss : 0.12762, Sensitive_Acc : 21.700, Run Time : 6.88 sec
INFO:root:2024-04-14 14:14:38, Train, Epoch : 8, Step : 4580, Loss : 0.29917, Acc : 0.884, Sensitive_Loss : 0.16338, Sensitive_Acc : 22.900, Run Time : 6.87 sec
INFO:root:2024-04-14 14:14:45, Train, Epoch : 8, Step : 4590, Loss : 0.36968, Acc : 0.853, Sensitive_Loss : 0.17768, Sensitive_Acc : 21.700, Run Time : 7.17 sec
INFO:root:2024-04-14 14:14:53, Train, Epoch : 8, Step : 4600, Loss : 0.33447, Acc : 0.847, Sensitive_Loss : 0.14960, Sensitive_Acc : 24.400, Run Time : 7.63 sec
INFO:root:2024-04-14 14:16:20, Dev, Step : 4600, Loss : 0.54590, Acc : 0.778, Auc : 0.858, Sensitive_Loss : 0.28738, Sensitive_Acc : 20.143, Sensitive_Auc : 0.998, Mean auc: 0.858, Run Time : 86.72 sec
INFO:root:2024-04-14 14:16:26, Train, Epoch : 8, Step : 4610, Loss : 0.28663, Acc : 0.847, Sensitive_Loss : 0.14762, Sensitive_Acc : 22.700, Run Time : 92.73 sec
INFO:root:2024-04-14 14:16:33, Train, Epoch : 8, Step : 4620, Loss : 0.27627, Acc : 0.875, Sensitive_Loss : 0.14378, Sensitive_Acc : 22.500, Run Time : 6.98 sec
INFO:root:2024-04-14 14:16:39, Train, Epoch : 8, Step : 4630, Loss : 0.31807, Acc : 0.881, Sensitive_Loss : 0.15054, Sensitive_Acc : 25.400, Run Time : 6.84 sec
INFO:root:2024-04-14 14:16:47, Train, Epoch : 8, Step : 4640, Loss : 0.31035, Acc : 0.878, Sensitive_Loss : 0.18228, Sensitive_Acc : 22.000, Run Time : 7.11 sec
INFO:root:2024-04-14 14:16:54, Train, Epoch : 8, Step : 4650, Loss : 0.39015, Acc : 0.850, Sensitive_Loss : 0.23143, Sensitive_Acc : 19.700, Run Time : 7.35 sec
INFO:root:2024-04-14 14:17:01, Train, Epoch : 8, Step : 4660, Loss : 0.33577, Acc : 0.859, Sensitive_Loss : 0.14865, Sensitive_Acc : 20.900, Run Time : 7.27 sec
INFO:root:2024-04-14 14:17:08, Train, Epoch : 8, Step : 4670, Loss : 0.33807, Acc : 0.838, Sensitive_Loss : 0.13849, Sensitive_Acc : 25.900, Run Time : 7.19 sec
INFO:root:2024-04-14 14:17:15, Train, Epoch : 8, Step : 4680, Loss : 0.38699, Acc : 0.831, Sensitive_Loss : 0.12676, Sensitive_Acc : 18.800, Run Time : 6.28 sec
INFO:root:2024-04-14 14:17:22, Train, Epoch : 8, Step : 4690, Loss : 0.31249, Acc : 0.887, Sensitive_Loss : 0.11159, Sensitive_Acc : 22.400, Run Time : 7.22 sec
INFO:root:2024-04-14 14:17:29, Train, Epoch : 8, Step : 4700, Loss : 0.34829, Acc : 0.844, Sensitive_Loss : 0.13080, Sensitive_Acc : 19.300, Run Time : 7.05 sec
INFO:root:2024-04-14 14:18:57, Dev, Step : 4700, Loss : 0.54585, Acc : 0.773, Auc : 0.854, Sensitive_Loss : 0.23980, Sensitive_Acc : 21.662, Sensitive_Auc : 0.998, Mean auc: 0.854, Run Time : 87.83 sec
INFO:root:2024-04-14 14:19:02, Train, Epoch : 8, Step : 4710, Loss : 0.41886, Acc : 0.816, Sensitive_Loss : 0.17150, Sensitive_Acc : 22.400, Run Time : 93.46 sec
INFO:root:2024-04-14 14:19:10, Train, Epoch : 8, Step : 4720, Loss : 0.30630, Acc : 0.863, Sensitive_Loss : 0.14165, Sensitive_Acc : 22.500, Run Time : 7.35 sec
INFO:root:2024-04-14 14:19:17, Train, Epoch : 8, Step : 4730, Loss : 0.36146, Acc : 0.831, Sensitive_Loss : 0.16944, Sensitive_Acc : 25.600, Run Time : 7.53 sec
INFO:root:2024-04-14 14:19:24, Train, Epoch : 8, Step : 4740, Loss : 0.35376, Acc : 0.838, Sensitive_Loss : 0.15109, Sensitive_Acc : 22.200, Run Time : 6.58 sec
INFO:root:2024-04-14 14:19:31, Train, Epoch : 8, Step : 4750, Loss : 0.34276, Acc : 0.856, Sensitive_Loss : 0.14912, Sensitive_Acc : 24.000, Run Time : 7.13 sec
INFO:root:2024-04-14 14:19:38, Train, Epoch : 8, Step : 4760, Loss : 0.35733, Acc : 0.819, Sensitive_Loss : 0.14986, Sensitive_Acc : 21.800, Run Time : 6.94 sec
INFO:root:2024-04-14 14:19:45, Train, Epoch : 8, Step : 4770, Loss : 0.32974, Acc : 0.838, Sensitive_Loss : 0.12269, Sensitive_Acc : 19.400, Run Time : 7.54 sec
INFO:root:2024-04-14 14:19:52, Train, Epoch : 8, Step : 4780, Loss : 0.42853, Acc : 0.841, Sensitive_Loss : 0.17643, Sensitive_Acc : 22.500, Run Time : 6.94 sec
INFO:root:2024-04-14 14:19:59, Train, Epoch : 8, Step : 4790, Loss : 0.28938, Acc : 0.884, Sensitive_Loss : 0.15398, Sensitive_Acc : 21.100, Run Time : 6.87 sec
INFO:root:2024-04-14 14:20:07, Train, Epoch : 8, Step : 4800, Loss : 0.36637, Acc : 0.838, Sensitive_Loss : 0.13544, Sensitive_Acc : 22.700, Run Time : 7.33 sec
INFO:root:2024-04-14 14:21:34, Dev, Step : 4800, Loss : 0.54456, Acc : 0.776, Auc : 0.856, Sensitive_Loss : 0.23410, Sensitive_Acc : 21.737, Sensitive_Auc : 0.998, Mean auc: 0.856, Run Time : 87.29 sec
INFO:root:2024-04-14 14:21:40, Train, Epoch : 8, Step : 4810, Loss : 0.32369, Acc : 0.866, Sensitive_Loss : 0.14022, Sensitive_Acc : 19.900, Run Time : 93.65 sec
INFO:root:2024-04-14 14:21:47, Train, Epoch : 8, Step : 4820, Loss : 0.30483, Acc : 0.869, Sensitive_Loss : 0.16880, Sensitive_Acc : 22.900, Run Time : 6.31 sec
INFO:root:2024-04-14 14:22:00, Train, Epoch : 8, Step : 4830, Loss : 0.30841, Acc : 0.856, Sensitive_Loss : 0.12837, Sensitive_Acc : 22.900, Run Time : 13.57 sec
INFO:root:2024-04-14 14:22:07, Train, Epoch : 8, Step : 4840, Loss : 0.32878, Acc : 0.894, Sensitive_Loss : 0.18624, Sensitive_Acc : 25.700, Run Time : 7.08 sec
INFO:root:2024-04-14 14:22:15, Train, Epoch : 8, Step : 4850, Loss : 0.34082, Acc : 0.850, Sensitive_Loss : 0.17537, Sensitive_Acc : 22.600, Run Time : 7.31 sec
INFO:root:2024-04-14 14:22:22, Train, Epoch : 8, Step : 4860, Loss : 0.27414, Acc : 0.875, Sensitive_Loss : 0.12019, Sensitive_Acc : 20.300, Run Time : 7.11 sec
INFO:root:2024-04-14 14:22:29, Train, Epoch : 8, Step : 4870, Loss : 0.41184, Acc : 0.853, Sensitive_Loss : 0.11512, Sensitive_Acc : 22.500, Run Time : 7.10 sec
INFO:root:2024-04-14 14:22:36, Train, Epoch : 8, Step : 4880, Loss : 0.34006, Acc : 0.847, Sensitive_Loss : 0.12319, Sensitive_Acc : 20.300, Run Time : 7.66 sec
INFO:root:2024-04-14 14:22:43, Train, Epoch : 8, Step : 4890, Loss : 0.33184, Acc : 0.856, Sensitive_Loss : 0.13924, Sensitive_Acc : 21.100, Run Time : 6.88 sec
INFO:root:2024-04-14 14:22:51, Train, Epoch : 8, Step : 4900, Loss : 0.33546, Acc : 0.831, Sensitive_Loss : 0.18743, Sensitive_Acc : 24.100, Run Time : 7.29 sec
INFO:root:2024-04-14 14:24:19, Dev, Step : 4900, Loss : 0.56370, Acc : 0.774, Auc : 0.856, Sensitive_Loss : 0.23943, Sensitive_Acc : 21.406, Sensitive_Auc : 0.997, Mean auc: 0.856, Run Time : 88.01 sec
INFO:root:2024-04-14 14:24:24, Train, Epoch : 8, Step : 4910, Loss : 0.37213, Acc : 0.831, Sensitive_Loss : 0.17054, Sensitive_Acc : 20.200, Run Time : 93.68 sec
INFO:root:2024-04-14 14:24:32, Train, Epoch : 8, Step : 4920, Loss : 0.33297, Acc : 0.884, Sensitive_Loss : 0.12063, Sensitive_Acc : 21.500, Run Time : 7.67 sec
INFO:root:2024-04-14 14:24:40, Train, Epoch : 8, Step : 4930, Loss : 0.30390, Acc : 0.881, Sensitive_Loss : 0.16004, Sensitive_Acc : 23.200, Run Time : 8.07 sec
INFO:root:2024-04-14 14:24:48, Train, Epoch : 8, Step : 4940, Loss : 0.36041, Acc : 0.825, Sensitive_Loss : 0.25126, Sensitive_Acc : 26.500, Run Time : 7.74 sec
INFO:root:2024-04-14 14:24:55, Train, Epoch : 8, Step : 4950, Loss : 0.35058, Acc : 0.853, Sensitive_Loss : 0.17456, Sensitive_Acc : 20.800, Run Time : 7.66 sec
INFO:root:2024-04-14 14:25:03, Train, Epoch : 8, Step : 4960, Loss : 0.29500, Acc : 0.850, Sensitive_Loss : 0.09868, Sensitive_Acc : 22.700, Run Time : 8.04 sec
INFO:root:2024-04-14 14:25:12, Train, Epoch : 8, Step : 4970, Loss : 0.38847, Acc : 0.831, Sensitive_Loss : 0.13075, Sensitive_Acc : 17.800, Run Time : 8.24 sec
INFO:root:2024-04-14 14:25:19, Train, Epoch : 8, Step : 4980, Loss : 0.33785, Acc : 0.831, Sensitive_Loss : 0.22671, Sensitive_Acc : 23.800, Run Time : 7.54 sec
INFO:root:2024-04-14 14:25:27, Train, Epoch : 8, Step : 4990, Loss : 0.35962, Acc : 0.828, Sensitive_Loss : 0.16464, Sensitive_Acc : 18.900, Run Time : 8.06 sec
INFO:root:2024-04-14 14:25:35, Train, Epoch : 8, Step : 5000, Loss : 0.30937, Acc : 0.834, Sensitive_Loss : 0.11728, Sensitive_Acc : 19.600, Run Time : 8.00 sec
INFO:root:2024-04-14 14:27:18, Dev, Step : 5000, Loss : 0.55587, Acc : 0.774, Auc : 0.855, Sensitive_Loss : 0.25521, Sensitive_Acc : 20.714, Sensitive_Auc : 0.995, Mean auc: 0.855, Run Time : 102.92 sec
INFO:root:2024-04-14 14:27:24, Train, Epoch : 8, Step : 5010, Loss : 0.35590, Acc : 0.844, Sensitive_Loss : 0.15847, Sensitive_Acc : 21.700, Run Time : 108.81 sec
INFO:root:2024-04-14 14:27:32, Train, Epoch : 8, Step : 5020, Loss : 0.35088, Acc : 0.866, Sensitive_Loss : 0.10306, Sensitive_Acc : 23.500, Run Time : 8.04 sec
INFO:root:2024-04-14 14:27:40, Train, Epoch : 8, Step : 5030, Loss : 0.37418, Acc : 0.841, Sensitive_Loss : 0.24495, Sensitive_Acc : 22.700, Run Time : 8.20 sec
INFO:root:2024-04-14 14:27:49, Train, Epoch : 8, Step : 5040, Loss : 0.33473, Acc : 0.863, Sensitive_Loss : 0.13348, Sensitive_Acc : 24.200, Run Time : 8.28 sec
INFO:root:2024-04-14 14:27:56, Train, Epoch : 8, Step : 5050, Loss : 0.31351, Acc : 0.856, Sensitive_Loss : 0.11277, Sensitive_Acc : 19.700, Run Time : 7.70 sec
INFO:root:2024-04-14 14:28:04, Train, Epoch : 8, Step : 5060, Loss : 0.32549, Acc : 0.853, Sensitive_Loss : 0.16549, Sensitive_Acc : 19.900, Run Time : 7.99 sec
INFO:root:2024-04-14 14:28:12, Train, Epoch : 8, Step : 5070, Loss : 0.26403, Acc : 0.887, Sensitive_Loss : 0.16445, Sensitive_Acc : 16.100, Run Time : 7.90 sec
INFO:root:2024-04-14 14:29:40
INFO:root:y_pred: [0.0152572  0.00063867 0.04236701 ... 0.23312145 0.01187893 0.04510663]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [4.93766228e-03 1.14387562e-02 7.04021305e-02 4.83029820e-02
 1.45001989e-02 6.11622632e-02 1.34187341e-01 5.24721481e-03
 4.36257087e-02 9.99522448e-01 6.63505912e-01 3.15260477e-02
 3.17873131e-03 1.02139171e-03 9.99672174e-01 3.49182673e-02
 3.68524855e-03 9.99403954e-01 9.98931587e-01 5.18866964e-02
 9.76647139e-01 6.69279369e-04 2.39198387e-01 2.13625059e-01
 2.99659856e-02 4.22744751e-01 5.94748490e-05 7.51381293e-02
 1.59829142e-04 2.00515967e-02 1.63699925e-01 9.91519034e-01
 1.27196461e-01 9.65910792e-01 2.85852671e-04 1.02581085e-04
 3.00302217e-03 8.61067418e-03 4.44492185e-03 2.49241404e-02
 2.35261261e-01 9.61410582e-01 5.07225923e-04 1.74882002e-02
 9.99619961e-01 7.04384804e-01 7.94281900e-01 2.75882870e-01
 2.45134607e-01 9.88253355e-01 9.97105896e-01 9.99120057e-01
 9.98295724e-01 3.30071175e-03 1.22126363e-01 7.18761623e-01
 1.32428700e-04 1.44198179e-01 9.93418217e-01 2.13340740e-03
 8.46783456e-04 2.20239721e-03 5.42938188e-02 2.13398831e-03
 9.99686718e-01 3.00484478e-01 8.78455248e-05 9.87527892e-02
 3.40855244e-04 9.79320884e-01 9.99933839e-01 9.99998212e-01
 1.46872178e-03 6.33163035e-01 2.00275108e-02 3.34266990e-01
 6.20344505e-02 3.18275852e-04 2.73763179e-03 4.40776488e-03
 3.72646064e-01 7.77027989e-03 9.98938620e-01 9.99511838e-01
 1.32509116e-02 1.21999688e-01 2.57260710e-01 5.81714883e-03
 1.15341125e-02 7.51420390e-03 2.51536090e-02 4.91073400e-01
 2.30164919e-03 1.85465731e-04 9.46957443e-04 1.10556623e-02
 1.27398584e-03 3.58420670e-01 3.38179655e-02 3.09969503e-02
 2.13651694e-02 7.18590198e-03 5.18305838e-01 2.42896117e-02
 2.68573612e-01 1.03822001e-03 3.66158128e-01 6.47363067e-01
 5.55129826e-01 1.66712165e-01 2.47783202e-04 9.99882579e-01
 9.99797165e-01 3.77562857e-04 7.97471330e-02 5.31417847e-01
 5.49731962e-02 4.41338692e-04 4.21149701e-01 2.77935546e-02
 4.14388664e-02 4.96197201e-04 3.49387713e-02 9.31916118e-04
 4.25282046e-02 6.62246272e-02 2.01592440e-04 9.91202474e-01
 1.74672529e-01 5.08515276e-02 6.73351288e-02 2.31879190e-01
 4.12201407e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-14 14:29:40, Dev, Step : 5072, Loss : 0.55429, Acc : 0.777, Auc : 0.852, Sensitive_Loss : 0.25366, Sensitive_Acc : 21.030, Sensitive_Auc : 0.995, Mean auc: 0.852, Run Time : 86.76 sec
INFO:root:2024-04-14 14:29:48, Train, Epoch : 9, Step : 5080, Loss : 0.25949, Acc : 0.713, Sensitive_Loss : 0.08978, Sensitive_Acc : 18.800, Run Time : 7.19 sec
INFO:root:2024-04-14 14:29:56, Train, Epoch : 9, Step : 5090, Loss : 0.26659, Acc : 0.859, Sensitive_Loss : 0.11109, Sensitive_Acc : 24.600, Run Time : 7.36 sec
INFO:root:2024-04-14 14:30:04, Train, Epoch : 9, Step : 5100, Loss : 0.31347, Acc : 0.863, Sensitive_Loss : 0.18803, Sensitive_Acc : 21.900, Run Time : 8.00 sec
INFO:root:2024-04-14 14:31:32, Dev, Step : 5100, Loss : 0.54421, Acc : 0.777, Auc : 0.856, Sensitive_Loss : 0.23155, Sensitive_Acc : 21.406, Sensitive_Auc : 0.994, Mean auc: 0.856, Run Time : 88.14 sec
INFO:root:2024-04-14 14:31:37, Train, Epoch : 9, Step : 5110, Loss : 0.31113, Acc : 0.875, Sensitive_Loss : 0.21916, Sensitive_Acc : 21.700, Run Time : 93.90 sec
INFO:root:2024-04-14 14:31:45, Train, Epoch : 9, Step : 5120, Loss : 0.29665, Acc : 0.863, Sensitive_Loss : 0.13093, Sensitive_Acc : 24.100, Run Time : 7.55 sec
INFO:root:2024-04-14 14:31:53, Train, Epoch : 9, Step : 5130, Loss : 0.37710, Acc : 0.838, Sensitive_Loss : 0.13696, Sensitive_Acc : 21.400, Run Time : 7.61 sec
INFO:root:2024-04-14 14:32:00, Train, Epoch : 9, Step : 5140, Loss : 0.24308, Acc : 0.884, Sensitive_Loss : 0.17762, Sensitive_Acc : 21.100, Run Time : 7.33 sec
INFO:root:2024-04-14 14:32:08, Train, Epoch : 9, Step : 5150, Loss : 0.26638, Acc : 0.912, Sensitive_Loss : 0.19262, Sensitive_Acc : 26.100, Run Time : 7.81 sec
INFO:root:2024-04-14 14:32:15, Train, Epoch : 9, Step : 5160, Loss : 0.31736, Acc : 0.853, Sensitive_Loss : 0.18395, Sensitive_Acc : 16.800, Run Time : 7.05 sec
INFO:root:2024-04-14 14:32:23, Train, Epoch : 9, Step : 5170, Loss : 0.26766, Acc : 0.894, Sensitive_Loss : 0.15969, Sensitive_Acc : 21.000, Run Time : 7.78 sec
INFO:root:2024-04-14 14:32:30, Train, Epoch : 9, Step : 5180, Loss : 0.39507, Acc : 0.850, Sensitive_Loss : 0.15898, Sensitive_Acc : 18.900, Run Time : 7.42 sec
INFO:root:2024-04-14 14:32:37, Train, Epoch : 9, Step : 5190, Loss : 0.30883, Acc : 0.856, Sensitive_Loss : 0.15814, Sensitive_Acc : 18.500, Run Time : 7.44 sec
INFO:root:2024-04-14 14:32:45, Train, Epoch : 9, Step : 5200, Loss : 0.33037, Acc : 0.869, Sensitive_Loss : 0.13031, Sensitive_Acc : 23.000, Run Time : 7.46 sec
INFO:root:2024-04-14 14:34:13, Dev, Step : 5200, Loss : 0.55242, Acc : 0.780, Auc : 0.854, Sensitive_Loss : 0.25684, Sensitive_Acc : 21.256, Sensitive_Auc : 0.997, Mean auc: 0.854, Run Time : 87.77 sec
INFO:root:2024-04-14 14:34:18, Train, Epoch : 9, Step : 5210, Loss : 0.40263, Acc : 0.831, Sensitive_Loss : 0.18252, Sensitive_Acc : 22.600, Run Time : 93.10 sec
INFO:root:2024-04-14 14:34:26, Train, Epoch : 9, Step : 5220, Loss : 0.35372, Acc : 0.869, Sensitive_Loss : 0.20973, Sensitive_Acc : 24.300, Run Time : 7.60 sec
INFO:root:2024-04-14 14:34:33, Train, Epoch : 9, Step : 5230, Loss : 0.33251, Acc : 0.866, Sensitive_Loss : 0.15471, Sensitive_Acc : 22.200, Run Time : 7.34 sec
INFO:root:2024-04-14 14:34:41, Train, Epoch : 9, Step : 5240, Loss : 0.33886, Acc : 0.847, Sensitive_Loss : 0.09948, Sensitive_Acc : 23.600, Run Time : 7.72 sec
INFO:root:2024-04-14 14:34:48, Train, Epoch : 9, Step : 5250, Loss : 0.28523, Acc : 0.863, Sensitive_Loss : 0.12393, Sensitive_Acc : 22.600, Run Time : 7.54 sec
INFO:root:2024-04-14 14:34:56, Train, Epoch : 9, Step : 5260, Loss : 0.34209, Acc : 0.863, Sensitive_Loss : 0.17350, Sensitive_Acc : 21.200, Run Time : 7.67 sec
INFO:root:2024-04-14 14:35:03, Train, Epoch : 9, Step : 5270, Loss : 0.34788, Acc : 0.872, Sensitive_Loss : 0.10032, Sensitive_Acc : 23.800, Run Time : 7.47 sec
INFO:root:2024-04-14 14:35:11, Train, Epoch : 9, Step : 5280, Loss : 0.30138, Acc : 0.847, Sensitive_Loss : 0.15821, Sensitive_Acc : 19.500, Run Time : 7.57 sec
INFO:root:2024-04-14 14:35:19, Train, Epoch : 9, Step : 5290, Loss : 0.30988, Acc : 0.875, Sensitive_Loss : 0.13880, Sensitive_Acc : 25.200, Run Time : 7.93 sec
INFO:root:2024-04-14 14:35:26, Train, Epoch : 9, Step : 5300, Loss : 0.30079, Acc : 0.891, Sensitive_Loss : 0.14275, Sensitive_Acc : 17.900, Run Time : 7.27 sec
INFO:root:2024-04-14 14:36:54, Dev, Step : 5300, Loss : 0.56079, Acc : 0.772, Auc : 0.851, Sensitive_Loss : 0.23762, Sensitive_Acc : 21.602, Sensitive_Auc : 0.997, Mean auc: 0.851, Run Time : 87.65 sec
INFO:root:2024-04-14 14:36:59, Train, Epoch : 9, Step : 5310, Loss : 0.25517, Acc : 0.881, Sensitive_Loss : 0.14540, Sensitive_Acc : 23.700, Run Time : 93.29 sec
INFO:root:2024-04-14 14:37:06, Train, Epoch : 9, Step : 5320, Loss : 0.30526, Acc : 0.869, Sensitive_Loss : 0.15671, Sensitive_Acc : 24.100, Run Time : 7.02 sec
INFO:root:2024-04-14 14:37:14, Train, Epoch : 9, Step : 5330, Loss : 0.27085, Acc : 0.900, Sensitive_Loss : 0.19845, Sensitive_Acc : 23.400, Run Time : 8.02 sec
INFO:root:2024-04-14 14:37:29, Train, Epoch : 9, Step : 5340, Loss : 0.26048, Acc : 0.891, Sensitive_Loss : 0.16899, Sensitive_Acc : 20.000, Run Time : 14.99 sec
INFO:root:2024-04-14 14:37:38, Train, Epoch : 9, Step : 5350, Loss : 0.29609, Acc : 0.887, Sensitive_Loss : 0.11997, Sensitive_Acc : 23.200, Run Time : 8.43 sec
INFO:root:2024-04-14 14:37:45, Train, Epoch : 9, Step : 5360, Loss : 0.28779, Acc : 0.891, Sensitive_Loss : 0.10375, Sensitive_Acc : 20.700, Run Time : 7.39 sec
INFO:root:2024-04-14 14:37:53, Train, Epoch : 9, Step : 5370, Loss : 0.32286, Acc : 0.881, Sensitive_Loss : 0.14614, Sensitive_Acc : 24.300, Run Time : 7.39 sec
INFO:root:2024-04-14 14:38:01, Train, Epoch : 9, Step : 5380, Loss : 0.26784, Acc : 0.872, Sensitive_Loss : 0.14873, Sensitive_Acc : 21.700, Run Time : 8.20 sec
INFO:root:2024-04-14 14:38:09, Train, Epoch : 9, Step : 5390, Loss : 0.27489, Acc : 0.897, Sensitive_Loss : 0.12718, Sensitive_Acc : 24.900, Run Time : 7.94 sec
INFO:root:2024-04-14 14:38:16, Train, Epoch : 9, Step : 5400, Loss : 0.32744, Acc : 0.853, Sensitive_Loss : 0.17462, Sensitive_Acc : 21.400, Run Time : 7.67 sec
INFO:root:2024-04-14 14:39:48, Dev, Step : 5400, Loss : 0.56463, Acc : 0.773, Auc : 0.855, Sensitive_Loss : 0.23821, Sensitive_Acc : 21.602, Sensitive_Auc : 0.997, Mean auc: 0.855, Run Time : 91.03 sec
INFO:root:2024-04-14 14:39:54, Train, Epoch : 9, Step : 5410, Loss : 0.29191, Acc : 0.863, Sensitive_Loss : 0.23652, Sensitive_Acc : 21.000, Run Time : 97.03 sec
INFO:root:2024-04-14 14:40:09, Train, Epoch : 9, Step : 5420, Loss : 0.31440, Acc : 0.863, Sensitive_Loss : 0.14539, Sensitive_Acc : 19.800, Run Time : 15.17 sec
INFO:root:2024-04-14 14:40:17, Train, Epoch : 9, Step : 5430, Loss : 0.32389, Acc : 0.869, Sensitive_Loss : 0.09419, Sensitive_Acc : 18.900, Run Time : 8.40 sec
INFO:root:2024-04-14 14:40:24, Train, Epoch : 9, Step : 5440, Loss : 0.42609, Acc : 0.847, Sensitive_Loss : 0.17136, Sensitive_Acc : 24.000, Run Time : 7.26 sec
INFO:root:2024-04-14 14:40:32, Train, Epoch : 9, Step : 5450, Loss : 0.31854, Acc : 0.847, Sensitive_Loss : 0.11532, Sensitive_Acc : 20.300, Run Time : 7.94 sec
INFO:root:2024-04-14 14:40:41, Train, Epoch : 9, Step : 5460, Loss : 0.26186, Acc : 0.912, Sensitive_Loss : 0.11014, Sensitive_Acc : 20.500, Run Time : 8.44 sec
INFO:root:2024-04-14 14:40:49, Train, Epoch : 9, Step : 5470, Loss : 0.30065, Acc : 0.872, Sensitive_Loss : 0.12299, Sensitive_Acc : 23.500, Run Time : 8.50 sec
INFO:root:2024-04-14 14:40:58, Train, Epoch : 9, Step : 5480, Loss : 0.35928, Acc : 0.819, Sensitive_Loss : 0.19321, Sensitive_Acc : 18.100, Run Time : 8.91 sec
INFO:root:2024-04-14 14:41:06, Train, Epoch : 9, Step : 5490, Loss : 0.31218, Acc : 0.875, Sensitive_Loss : 0.15084, Sensitive_Acc : 24.400, Run Time : 7.46 sec
INFO:root:2024-04-14 14:41:14, Train, Epoch : 9, Step : 5500, Loss : 0.36146, Acc : 0.841, Sensitive_Loss : 0.15126, Sensitive_Acc : 23.000, Run Time : 8.34 sec
INFO:root:2024-04-14 14:43:05, Dev, Step : 5500, Loss : 0.55355, Acc : 0.773, Auc : 0.854, Sensitive_Loss : 0.24172, Sensitive_Acc : 21.496, Sensitive_Auc : 0.998, Mean auc: 0.854, Run Time : 111.22 sec
INFO:root:2024-04-14 14:43:11, Train, Epoch : 9, Step : 5510, Loss : 0.27585, Acc : 0.887, Sensitive_Loss : 0.11128, Sensitive_Acc : 18.600, Run Time : 117.30 sec
INFO:root:2024-04-14 14:43:19, Train, Epoch : 9, Step : 5520, Loss : 0.30064, Acc : 0.872, Sensitive_Loss : 0.10041, Sensitive_Acc : 25.900, Run Time : 7.84 sec
INFO:root:2024-04-14 14:43:27, Train, Epoch : 9, Step : 5530, Loss : 0.30450, Acc : 0.853, Sensitive_Loss : 0.13683, Sensitive_Acc : 19.200, Run Time : 7.96 sec
INFO:root:2024-04-14 14:43:34, Train, Epoch : 9, Step : 5540, Loss : 0.29511, Acc : 0.878, Sensitive_Loss : 0.13647, Sensitive_Acc : 22.700, Run Time : 7.43 sec
INFO:root:2024-04-14 14:43:43, Train, Epoch : 9, Step : 5550, Loss : 0.32973, Acc : 0.866, Sensitive_Loss : 0.11627, Sensitive_Acc : 24.200, Run Time : 8.29 sec
INFO:root:2024-04-14 14:43:50, Train, Epoch : 9, Step : 5560, Loss : 0.29041, Acc : 0.903, Sensitive_Loss : 0.15867, Sensitive_Acc : 19.100, Run Time : 7.57 sec
INFO:root:2024-04-14 14:43:58, Train, Epoch : 9, Step : 5570, Loss : 0.28216, Acc : 0.878, Sensitive_Loss : 0.13700, Sensitive_Acc : 16.200, Run Time : 7.65 sec
INFO:root:2024-04-14 14:44:06, Train, Epoch : 9, Step : 5580, Loss : 0.30703, Acc : 0.875, Sensitive_Loss : 0.10276, Sensitive_Acc : 22.700, Run Time : 7.77 sec
INFO:root:2024-04-14 14:44:14, Train, Epoch : 9, Step : 5590, Loss : 0.29534, Acc : 0.859, Sensitive_Loss : 0.15967, Sensitive_Acc : 19.800, Run Time : 7.97 sec
INFO:root:2024-04-14 14:44:21, Train, Epoch : 9, Step : 5600, Loss : 0.33629, Acc : 0.859, Sensitive_Loss : 0.10800, Sensitive_Acc : 17.700, Run Time : 7.67 sec
INFO:root:2024-04-14 14:45:52, Dev, Step : 5600, Loss : 0.56590, Acc : 0.778, Auc : 0.854, Sensitive_Loss : 0.22420, Sensitive_Acc : 21.632, Sensitive_Auc : 0.994, Mean auc: 0.854, Run Time : 90.84 sec
INFO:root:2024-04-14 14:45:58, Train, Epoch : 9, Step : 5610, Loss : 0.30790, Acc : 0.856, Sensitive_Loss : 0.20971, Sensitive_Acc : 20.700, Run Time : 96.31 sec
INFO:root:2024-04-14 14:46:06, Train, Epoch : 9, Step : 5620, Loss : 0.28120, Acc : 0.866, Sensitive_Loss : 0.18062, Sensitive_Acc : 21.600, Run Time : 8.34 sec
INFO:root:2024-04-14 14:46:14, Train, Epoch : 9, Step : 5630, Loss : 0.34376, Acc : 0.859, Sensitive_Loss : 0.13792, Sensitive_Acc : 21.700, Run Time : 7.92 sec
INFO:root:2024-04-14 14:46:22, Train, Epoch : 9, Step : 5640, Loss : 0.27513, Acc : 0.894, Sensitive_Loss : 0.14742, Sensitive_Acc : 19.100, Run Time : 7.93 sec
INFO:root:2024-04-14 14:46:29, Train, Epoch : 9, Step : 5650, Loss : 0.31252, Acc : 0.863, Sensitive_Loss : 0.11216, Sensitive_Acc : 19.100, Run Time : 7.60 sec
INFO:root:2024-04-14 14:46:37, Train, Epoch : 9, Step : 5660, Loss : 0.27831, Acc : 0.875, Sensitive_Loss : 0.13065, Sensitive_Acc : 20.400, Run Time : 7.77 sec
INFO:root:2024-04-14 14:46:45, Train, Epoch : 9, Step : 5670, Loss : 0.35019, Acc : 0.856, Sensitive_Loss : 0.14773, Sensitive_Acc : 20.400, Run Time : 7.60 sec
INFO:root:2024-04-14 14:46:53, Train, Epoch : 9, Step : 5680, Loss : 0.32540, Acc : 0.872, Sensitive_Loss : 0.10404, Sensitive_Acc : 21.900, Run Time : 8.21 sec
INFO:root:2024-04-14 14:47:00, Train, Epoch : 9, Step : 5690, Loss : 0.30086, Acc : 0.856, Sensitive_Loss : 0.22315, Sensitive_Acc : 21.800, Run Time : 7.24 sec
INFO:root:2024-04-14 14:47:07, Train, Epoch : 9, Step : 5700, Loss : 0.31951, Acc : 0.859, Sensitive_Loss : 0.14402, Sensitive_Acc : 20.200, Run Time : 7.13 sec
INFO:root:2024-04-14 14:48:39, Dev, Step : 5700, Loss : 0.55732, Acc : 0.775, Auc : 0.853, Sensitive_Loss : 0.22082, Sensitive_Acc : 21.632, Sensitive_Auc : 0.996, Mean auc: 0.853, Run Time : 91.98 sec
INFO:root:2024-04-14 14:50:08
INFO:root:y_pred: [0.02761908 0.00059227 0.0309789  ... 0.33148754 0.00464994 0.04729323]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.86018611e-03 7.00573530e-03 2.43110843e-02 9.65879634e-02
 2.19126660e-02 5.33620268e-02 6.63399994e-02 4.61574644e-03
 1.97180640e-02 9.99363840e-01 5.02462029e-01 2.18094438e-02
 2.27125315e-03 5.88070136e-04 9.99447644e-01 2.78770998e-02
 3.90108977e-03 9.99414086e-01 9.99127328e-01 2.46555544e-02
 9.65653062e-01 7.36668590e-04 2.24521115e-01 1.29050314e-01
 4.58713528e-03 2.88011491e-01 8.63386231e-05 5.20391427e-02
 8.92443422e-05 3.27487364e-02 1.26766801e-01 9.87625062e-01
 1.88052341e-01 9.63602722e-01 1.39897951e-04 5.89196607e-05
 2.12221779e-03 6.31933613e-03 5.98204229e-03 7.97017198e-03
 1.32864192e-01 9.77823734e-01 3.59133381e-04 1.02467444e-02
 9.99773443e-01 6.42981708e-01 7.46001363e-01 2.41817772e-01
 2.35435858e-01 9.82904553e-01 9.97325897e-01 9.98494148e-01
 9.98603404e-01 3.68055585e-03 8.03051516e-02 7.06119537e-01
 4.57014066e-05 7.42134303e-02 9.89890695e-01 1.40735111e-03
 3.21190862e-04 1.13220979e-03 4.00659814e-02 9.11649026e-04
 9.99688506e-01 1.87393159e-01 2.27455050e-04 8.73303339e-02
 1.61904958e-04 9.90091562e-01 9.99835253e-01 9.99999523e-01
 1.31469383e-03 4.80815917e-01 2.64132470e-02 4.03679788e-01
 6.62556365e-02 5.32850827e-05 2.40663532e-04 2.86064832e-03
 2.70039409e-01 5.28920768e-03 9.98130620e-01 9.98507321e-01
 6.65249443e-03 9.20386985e-02 2.57262617e-01 3.03102518e-03
 1.28132375e-02 3.64252063e-03 5.79177262e-03 2.61534661e-01
 1.66264945e-03 9.45130741e-05 9.15243116e-04 6.72291871e-03
 9.20143502e-04 2.92690456e-01 3.31811681e-02 1.61693487e-02
 1.27989026e-02 8.76096543e-03 3.81710112e-01 1.00014061e-02
 8.68059546e-02 1.41225022e-03 3.34872007e-01 5.77992141e-01
 4.49993759e-01 7.80067667e-02 3.05969035e-04 9.99801099e-01
 9.99651790e-01 1.25955368e-04 1.10247746e-01 3.26201320e-01
 2.23923940e-02 2.55475112e-04 1.85866877e-01 1.02999024e-02
 2.44048927e-02 4.95368033e-04 3.19077857e-02 6.87980617e-04
 2.77609192e-02 7.46027902e-02 3.80187790e-04 9.81073618e-01
 2.03793883e-01 5.27486801e-02 2.74549033e-02 2.00478554e-01
 1.95307639e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-14 14:50:08, Dev, Step : 5706, Loss : 0.55576, Acc : 0.772, Auc : 0.852, Sensitive_Loss : 0.23186, Sensitive_Acc : 21.496, Sensitive_Auc : 0.996, Mean auc: 0.852, Run Time : 86.20 sec
INFO:root:2024-04-14 14:50:13, Train, Epoch : 10, Step : 5710, Loss : 0.12280, Acc : 0.347, Sensitive_Loss : 0.04254, Sensitive_Acc : 10.600, Run Time : 4.39 sec
INFO:root:2024-04-14 14:50:20, Train, Epoch : 10, Step : 5720, Loss : 0.28381, Acc : 0.891, Sensitive_Loss : 0.09608, Sensitive_Acc : 17.300, Run Time : 6.44 sec
INFO:root:2024-04-14 14:50:28, Train, Epoch : 10, Step : 5730, Loss : 0.27336, Acc : 0.887, Sensitive_Loss : 0.14982, Sensitive_Acc : 18.600, Run Time : 7.79 sec
INFO:root:2024-04-14 14:50:34, Train, Epoch : 10, Step : 5740, Loss : 0.25428, Acc : 0.897, Sensitive_Loss : 0.16101, Sensitive_Acc : 23.800, Run Time : 6.83 sec
INFO:root:2024-04-14 14:50:42, Train, Epoch : 10, Step : 5750, Loss : 0.26606, Acc : 0.897, Sensitive_Loss : 0.17840, Sensitive_Acc : 20.400, Run Time : 7.16 sec
INFO:root:2024-04-14 14:50:49, Train, Epoch : 10, Step : 5760, Loss : 0.32291, Acc : 0.856, Sensitive_Loss : 0.17017, Sensitive_Acc : 23.000, Run Time : 7.55 sec
INFO:root:2024-04-14 14:50:56, Train, Epoch : 10, Step : 5770, Loss : 0.30390, Acc : 0.891, Sensitive_Loss : 0.13534, Sensitive_Acc : 20.800, Run Time : 6.57 sec
INFO:root:2024-04-14 14:51:03, Train, Epoch : 10, Step : 5780, Loss : 0.28848, Acc : 0.887, Sensitive_Loss : 0.11308, Sensitive_Acc : 20.300, Run Time : 7.42 sec
INFO:root:2024-04-14 14:51:10, Train, Epoch : 10, Step : 5790, Loss : 0.28567, Acc : 0.853, Sensitive_Loss : 0.13091, Sensitive_Acc : 17.500, Run Time : 6.93 sec
INFO:root:2024-04-14 14:51:18, Train, Epoch : 10, Step : 5800, Loss : 0.37936, Acc : 0.841, Sensitive_Loss : 0.13557, Sensitive_Acc : 22.300, Run Time : 7.67 sec
INFO:root:2024-04-14 14:52:52, Dev, Step : 5800, Loss : 0.58092, Acc : 0.769, Auc : 0.852, Sensitive_Loss : 0.25344, Sensitive_Acc : 21.180, Sensitive_Auc : 0.991, Mean auc: 0.852, Run Time : 94.44 sec
INFO:root:2024-04-14 14:52:58, Train, Epoch : 10, Step : 5810, Loss : 0.25098, Acc : 0.894, Sensitive_Loss : 0.15664, Sensitive_Acc : 19.900, Run Time : 100.44 sec
INFO:root:2024-04-14 14:53:05, Train, Epoch : 10, Step : 5820, Loss : 0.29830, Acc : 0.866, Sensitive_Loss : 0.15711, Sensitive_Acc : 25.300, Run Time : 7.28 sec
INFO:root:2024-04-14 14:53:12, Train, Epoch : 10, Step : 5830, Loss : 0.27935, Acc : 0.878, Sensitive_Loss : 0.14687, Sensitive_Acc : 19.600, Run Time : 6.90 sec
INFO:root:2024-04-14 14:53:20, Train, Epoch : 10, Step : 5840, Loss : 0.23155, Acc : 0.897, Sensitive_Loss : 0.15697, Sensitive_Acc : 23.400, Run Time : 7.49 sec
INFO:root:2024-04-14 14:53:27, Train, Epoch : 10, Step : 5850, Loss : 0.28345, Acc : 0.881, Sensitive_Loss : 0.16316, Sensitive_Acc : 21.100, Run Time : 7.63 sec
INFO:root:2024-04-14 14:53:35, Train, Epoch : 10, Step : 5860, Loss : 0.27004, Acc : 0.903, Sensitive_Loss : 0.11598, Sensitive_Acc : 16.200, Run Time : 7.40 sec
INFO:root:2024-04-14 14:53:43, Train, Epoch : 10, Step : 5870, Loss : 0.30531, Acc : 0.872, Sensitive_Loss : 0.18400, Sensitive_Acc : 21.100, Run Time : 8.00 sec
INFO:root:2024-04-14 14:53:50, Train, Epoch : 10, Step : 5880, Loss : 0.28926, Acc : 0.887, Sensitive_Loss : 0.14577, Sensitive_Acc : 23.900, Run Time : 7.54 sec
INFO:root:2024-04-14 14:53:58, Train, Epoch : 10, Step : 5890, Loss : 0.34982, Acc : 0.850, Sensitive_Loss : 0.14932, Sensitive_Acc : 22.700, Run Time : 7.73 sec
INFO:root:2024-04-14 14:54:05, Train, Epoch : 10, Step : 5900, Loss : 0.32622, Acc : 0.850, Sensitive_Loss : 0.11287, Sensitive_Acc : 23.000, Run Time : 6.86 sec
INFO:root:2024-04-14 14:55:55, Dev, Step : 5900, Loss : 0.59879, Acc : 0.770, Auc : 0.853, Sensitive_Loss : 0.25354, Sensitive_Acc : 21.030, Sensitive_Auc : 0.994, Mean auc: 0.853, Run Time : 109.65 sec
INFO:root:2024-04-14 14:56:00, Train, Epoch : 10, Step : 5910, Loss : 0.31774, Acc : 0.872, Sensitive_Loss : 0.16281, Sensitive_Acc : 25.400, Run Time : 115.02 sec
INFO:root:2024-04-14 14:56:07, Train, Epoch : 10, Step : 5920, Loss : 0.32423, Acc : 0.863, Sensitive_Loss : 0.12335, Sensitive_Acc : 19.400, Run Time : 7.17 sec
INFO:root:2024-04-14 14:56:15, Train, Epoch : 10, Step : 5930, Loss : 0.30101, Acc : 0.859, Sensitive_Loss : 0.08907, Sensitive_Acc : 22.400, Run Time : 7.43 sec
INFO:root:2024-04-14 14:56:22, Train, Epoch : 10, Step : 5940, Loss : 0.33191, Acc : 0.856, Sensitive_Loss : 0.13924, Sensitive_Acc : 21.700, Run Time : 6.95 sec
INFO:root:2024-04-14 14:56:29, Train, Epoch : 10, Step : 5950, Loss : 0.27769, Acc : 0.878, Sensitive_Loss : 0.17519, Sensitive_Acc : 21.300, Run Time : 7.16 sec
INFO:root:2024-04-14 14:56:36, Train, Epoch : 10, Step : 5960, Loss : 0.33250, Acc : 0.831, Sensitive_Loss : 0.09955, Sensitive_Acc : 23.400, Run Time : 7.05 sec
INFO:root:2024-04-14 14:56:43, Train, Epoch : 10, Step : 5970, Loss : 0.29118, Acc : 0.875, Sensitive_Loss : 0.15496, Sensitive_Acc : 26.600, Run Time : 7.75 sec
INFO:root:2024-04-14 14:56:50, Train, Epoch : 10, Step : 5980, Loss : 0.27777, Acc : 0.863, Sensitive_Loss : 0.12948, Sensitive_Acc : 20.900, Run Time : 6.84 sec
INFO:root:2024-04-14 14:56:57, Train, Epoch : 10, Step : 5990, Loss : 0.27711, Acc : 0.884, Sensitive_Loss : 0.14173, Sensitive_Acc : 18.200, Run Time : 7.07 sec
INFO:root:2024-04-14 14:57:05, Train, Epoch : 10, Step : 6000, Loss : 0.31388, Acc : 0.863, Sensitive_Loss : 0.15728, Sensitive_Acc : 20.100, Run Time : 7.62 sec
INFO:root:2024-04-14 14:58:34, Dev, Step : 6000, Loss : 0.57434, Acc : 0.770, Auc : 0.851, Sensitive_Loss : 0.22503, Sensitive_Acc : 21.917, Sensitive_Auc : 0.997, Mean auc: 0.851, Run Time : 88.68 sec
INFO:root:2024-04-14 14:58:39, Train, Epoch : 10, Step : 6010, Loss : 0.22409, Acc : 0.925, Sensitive_Loss : 0.12499, Sensitive_Acc : 19.000, Run Time : 94.32 sec
INFO:root:2024-04-14 14:58:47, Train, Epoch : 10, Step : 6020, Loss : 0.25362, Acc : 0.900, Sensitive_Loss : 0.16826, Sensitive_Acc : 25.400, Run Time : 7.47 sec
INFO:root:2024-04-14 14:58:54, Train, Epoch : 10, Step : 6030, Loss : 0.26694, Acc : 0.878, Sensitive_Loss : 0.07503, Sensitive_Acc : 19.800, Run Time : 7.38 sec
INFO:root:2024-04-14 14:59:01, Train, Epoch : 10, Step : 6040, Loss : 0.29479, Acc : 0.897, Sensitive_Loss : 0.15916, Sensitive_Acc : 25.600, Run Time : 7.30 sec
INFO:root:2024-04-14 14:59:09, Train, Epoch : 10, Step : 6050, Loss : 0.34596, Acc : 0.872, Sensitive_Loss : 0.22021, Sensitive_Acc : 21.500, Run Time : 7.26 sec
INFO:root:2024-04-14 14:59:16, Train, Epoch : 10, Step : 6060, Loss : 0.25167, Acc : 0.912, Sensitive_Loss : 0.18452, Sensitive_Acc : 22.800, Run Time : 6.88 sec
INFO:root:2024-04-14 14:59:23, Train, Epoch : 10, Step : 6070, Loss : 0.27345, Acc : 0.894, Sensitive_Loss : 0.16924, Sensitive_Acc : 21.700, Run Time : 7.30 sec
INFO:root:2024-04-14 14:59:30, Train, Epoch : 10, Step : 6080, Loss : 0.29041, Acc : 0.872, Sensitive_Loss : 0.16578, Sensitive_Acc : 25.700, Run Time : 6.98 sec
INFO:root:2024-04-14 14:59:37, Train, Epoch : 10, Step : 6090, Loss : 0.33190, Acc : 0.831, Sensitive_Loss : 0.17444, Sensitive_Acc : 21.900, Run Time : 7.39 sec
INFO:root:2024-04-14 14:59:45, Train, Epoch : 10, Step : 6100, Loss : 0.29856, Acc : 0.863, Sensitive_Loss : 0.16995, Sensitive_Acc : 18.100, Run Time : 7.32 sec
INFO:root:2024-04-14 15:01:13, Dev, Step : 6100, Loss : 0.59025, Acc : 0.772, Auc : 0.851, Sensitive_Loss : 0.22481, Sensitive_Acc : 21.917, Sensitive_Auc : 0.996, Mean auc: 0.851, Run Time : 88.20 sec
INFO:root:2024-04-14 15:01:18, Train, Epoch : 10, Step : 6110, Loss : 0.24759, Acc : 0.887, Sensitive_Loss : 0.16125, Sensitive_Acc : 24.600, Run Time : 93.49 sec
INFO:root:2024-04-14 15:01:25, Train, Epoch : 10, Step : 6120, Loss : 0.31163, Acc : 0.872, Sensitive_Loss : 0.08525, Sensitive_Acc : 18.400, Run Time : 7.37 sec
INFO:root:2024-04-14 15:01:33, Train, Epoch : 10, Step : 6130, Loss : 0.30527, Acc : 0.894, Sensitive_Loss : 0.09342, Sensitive_Acc : 16.900, Run Time : 7.29 sec
INFO:root:2024-04-14 15:01:40, Train, Epoch : 10, Step : 6140, Loss : 0.32587, Acc : 0.878, Sensitive_Loss : 0.14158, Sensitive_Acc : 21.900, Run Time : 6.88 sec
INFO:root:2024-04-14 15:01:47, Train, Epoch : 10, Step : 6150, Loss : 0.26395, Acc : 0.884, Sensitive_Loss : 0.16544, Sensitive_Acc : 19.400, Run Time : 7.42 sec
INFO:root:2024-04-14 15:01:54, Train, Epoch : 10, Step : 6160, Loss : 0.25826, Acc : 0.878, Sensitive_Loss : 0.15744, Sensitive_Acc : 20.200, Run Time : 7.14 sec
INFO:root:2024-04-14 15:02:02, Train, Epoch : 10, Step : 6170, Loss : 0.26237, Acc : 0.897, Sensitive_Loss : 0.16897, Sensitive_Acc : 19.900, Run Time : 7.71 sec
INFO:root:2024-04-14 15:02:09, Train, Epoch : 10, Step : 6180, Loss : 0.26596, Acc : 0.887, Sensitive_Loss : 0.17760, Sensitive_Acc : 21.400, Run Time : 7.04 sec
INFO:root:2024-04-14 15:02:16, Train, Epoch : 10, Step : 6190, Loss : 0.25793, Acc : 0.878, Sensitive_Loss : 0.10190, Sensitive_Acc : 21.000, Run Time : 6.98 sec
INFO:root:2024-04-14 15:02:23, Train, Epoch : 10, Step : 6200, Loss : 0.32801, Acc : 0.856, Sensitive_Loss : 0.15686, Sensitive_Acc : 20.000, Run Time : 7.22 sec
INFO:root:2024-04-14 15:03:53, Dev, Step : 6200, Loss : 0.63065, Acc : 0.767, Auc : 0.851, Sensitive_Loss : 0.22270, Sensitive_Acc : 21.827, Sensitive_Auc : 0.990, Mean auc: 0.851, Run Time : 89.82 sec
INFO:root:2024-04-14 15:03:58, Train, Epoch : 10, Step : 6210, Loss : 0.30675, Acc : 0.853, Sensitive_Loss : 0.16955, Sensitive_Acc : 24.400, Run Time : 95.20 sec
INFO:root:2024-04-14 15:04:05, Train, Epoch : 10, Step : 6220, Loss : 0.28007, Acc : 0.872, Sensitive_Loss : 0.13282, Sensitive_Acc : 18.600, Run Time : 7.08 sec
INFO:root:2024-04-14 15:04:13, Train, Epoch : 10, Step : 6230, Loss : 0.31442, Acc : 0.856, Sensitive_Loss : 0.23667, Sensitive_Acc : 18.400, Run Time : 7.21 sec
INFO:root:2024-04-14 15:04:20, Train, Epoch : 10, Step : 6240, Loss : 0.21563, Acc : 0.925, Sensitive_Loss : 0.11064, Sensitive_Acc : 16.400, Run Time : 7.54 sec
INFO:root:2024-04-14 15:04:27, Train, Epoch : 10, Step : 6250, Loss : 0.35134, Acc : 0.856, Sensitive_Loss : 0.13594, Sensitive_Acc : 20.000, Run Time : 7.19 sec
INFO:root:2024-04-14 15:04:35, Train, Epoch : 10, Step : 6260, Loss : 0.27190, Acc : 0.881, Sensitive_Loss : 0.14766, Sensitive_Acc : 19.400, Run Time : 7.28 sec
INFO:root:2024-04-14 15:04:42, Train, Epoch : 10, Step : 6270, Loss : 0.30665, Acc : 0.853, Sensitive_Loss : 0.15452, Sensitive_Acc : 18.600, Run Time : 6.86 sec
INFO:root:2024-04-14 15:04:49, Train, Epoch : 10, Step : 6280, Loss : 0.26372, Acc : 0.872, Sensitive_Loss : 0.11771, Sensitive_Acc : 26.700, Run Time : 7.17 sec
INFO:root:2024-04-14 15:04:56, Train, Epoch : 10, Step : 6290, Loss : 0.32036, Acc : 0.866, Sensitive_Loss : 0.12674, Sensitive_Acc : 22.000, Run Time : 7.69 sec
INFO:root:2024-04-14 15:05:03, Train, Epoch : 10, Step : 6300, Loss : 0.37429, Acc : 0.838, Sensitive_Loss : 0.14625, Sensitive_Acc : 17.300, Run Time : 6.84 sec
INFO:root:2024-04-14 15:06:32, Dev, Step : 6300, Loss : 0.62798, Acc : 0.766, Auc : 0.848, Sensitive_Loss : 0.23245, Sensitive_Acc : 21.662, Sensitive_Auc : 0.993, Mean auc: 0.848, Run Time : 88.62 sec
INFO:root:2024-04-14 15:06:38, Train, Epoch : 10, Step : 6310, Loss : 0.25773, Acc : 0.906, Sensitive_Loss : 0.12063, Sensitive_Acc : 20.600, Run Time : 94.44 sec
INFO:root:2024-04-14 15:06:45, Train, Epoch : 10, Step : 6320, Loss : 0.33820, Acc : 0.838, Sensitive_Loss : 0.10457, Sensitive_Acc : 17.300, Run Time : 6.97 sec
INFO:root:2024-04-14 15:06:52, Train, Epoch : 10, Step : 6330, Loss : 0.23273, Acc : 0.897, Sensitive_Loss : 0.17591, Sensitive_Acc : 22.700, Run Time : 7.51 sec
INFO:root:2024-04-14 15:06:59, Train, Epoch : 10, Step : 6340, Loss : 0.34564, Acc : 0.863, Sensitive_Loss : 0.10479, Sensitive_Acc : 20.900, Run Time : 6.70 sec
INFO:root:2024-04-14 15:08:27
INFO:root:y_pred: [3.0192776e-02 1.5457945e-04 1.0602664e-02 ... 1.5688995e-01 2.4269521e-03
 1.6659399e-02]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [8.64538737e-03 6.95298146e-03 5.26777543e-02 7.57778287e-02
 1.68436356e-02 6.16806224e-02 1.09365940e-01 3.30596021e-03
 3.55429426e-02 9.99822199e-01 4.55717236e-01 3.18981931e-02
 3.43817403e-03 5.77620056e-04 9.99812424e-01 3.64167951e-02
 3.22299427e-03 9.99522328e-01 9.99024153e-01 4.41659763e-02
 9.66888428e-01 1.77695986e-03 2.23435044e-01 2.34583914e-01
 1.22440874e-03 6.75510764e-01 6.60139194e-05 6.33257180e-02
 9.12787436e-05 2.75557246e-02 1.54557690e-01 9.85036016e-01
 2.87367374e-01 9.62652862e-01 1.55324262e-04 7.00096280e-05
 2.32975231e-03 4.45616990e-03 3.75230750e-03 7.36917276e-03
 1.56065673e-01 9.79479730e-01 2.37302214e-04 1.30056264e-02
 9.99683142e-01 8.26699495e-01 8.19510102e-01 3.19075823e-01
 2.95700103e-01 9.91960704e-01 9.99263346e-01 9.98868346e-01
 9.99009132e-01 4.57090372e-03 6.37410805e-02 8.00283909e-01
 1.23537975e-04 1.55721799e-01 9.91503119e-01 1.23292161e-03
 8.29870754e-04 1.38820068e-03 4.00265865e-02 9.68655397e-04
 9.99807894e-01 4.26877618e-01 1.10119596e-04 8.11988860e-02
 3.63173662e-04 9.94234025e-01 9.99873877e-01 9.99999762e-01
 1.16613740e-03 4.96001869e-01 1.18183428e-02 3.61668199e-01
 8.07575881e-02 9.24510023e-05 2.43537317e-04 1.12552010e-02
 4.44058031e-01 8.80090334e-03 9.98056471e-01 9.99157667e-01
 7.27971923e-03 4.87021580e-02 3.77915055e-01 2.70216679e-03
 1.29475016e-02 6.62292354e-03 1.43957315e-02 3.02426726e-01
 2.48880987e-03 1.23367645e-04 1.03443663e-03 7.27265887e-03
 1.92989793e-03 2.67592579e-01 8.79908539e-03 3.15511376e-02
 1.74318310e-02 5.40268049e-03 5.92860222e-01 1.62341632e-02
 1.27858251e-01 6.61201018e-04 2.29006171e-01 4.57760423e-01
 5.75238705e-01 5.44858463e-02 3.05077410e-04 9.99879003e-01
 9.99697804e-01 2.19710870e-04 1.07587546e-01 5.03069162e-01
 2.96009090e-02 1.79890558e-04 2.30846167e-01 1.08419936e-02
 1.82776432e-02 7.93337065e-04 4.16966639e-02 6.01707434e-04
 2.22285725e-02 7.78558776e-02 1.05754094e-04 9.89911258e-01
 4.29398328e-01 7.49314129e-02 3.30847427e-02 1.54465958e-01
 1.40896709e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-14 15:08:27, Dev, Step : 6340, Loss : 0.60497, Acc : 0.770, Auc : 0.848, Sensitive_Loss : 0.26078, Sensitive_Acc : 21.286, Sensitive_Auc : 0.993, Mean auc: 0.848, Run Time : 88.49 sec
