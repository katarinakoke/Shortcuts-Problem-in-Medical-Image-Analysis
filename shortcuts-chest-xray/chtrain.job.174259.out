Running on desktop22:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_val.csv",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-11 12:02:30, Train, Epoch : 1, Step : 10, Loss : 0.75938, Acc : 0.584, Sensitive_Loss : 1.18364, Sensitive_Acc : 6.400, Run Time : 10.45 sec
INFO:root:2024-04-11 12:02:38, Train, Epoch : 1, Step : 20, Loss : 0.74814, Acc : 0.584, Sensitive_Loss : 1.11246, Sensitive_Acc : 18.200, Run Time : 8.28 sec
INFO:root:2024-04-11 12:02:46, Train, Epoch : 1, Step : 30, Loss : 0.68379, Acc : 0.672, Sensitive_Loss : 1.17336, Sensitive_Acc : 17.800, Run Time : 8.01 sec
INFO:root:2024-04-11 12:02:55, Train, Epoch : 1, Step : 40, Loss : 0.61100, Acc : 0.666, Sensitive_Loss : 1.05785, Sensitive_Acc : 12.400, Run Time : 8.33 sec
INFO:root:2024-04-11 12:03:03, Train, Epoch : 1, Step : 50, Loss : 0.70521, Acc : 0.603, Sensitive_Loss : 1.11951, Sensitive_Acc : 15.700, Run Time : 8.24 sec
INFO:root:2024-04-11 12:03:11, Train, Epoch : 1, Step : 60, Loss : 0.63076, Acc : 0.678, Sensitive_Loss : 1.08545, Sensitive_Acc : 19.700, Run Time : 8.03 sec
INFO:root:2024-04-11 12:03:19, Train, Epoch : 1, Step : 70, Loss : 0.64554, Acc : 0.572, Sensitive_Loss : 1.07943, Sensitive_Acc : 15.800, Run Time : 8.02 sec
INFO:root:2024-04-11 12:03:27, Train, Epoch : 1, Step : 80, Loss : 0.58186, Acc : 0.681, Sensitive_Loss : 1.01287, Sensitive_Acc : 16.200, Run Time : 8.00 sec
INFO:root:2024-04-11 12:03:35, Train, Epoch : 1, Step : 90, Loss : 0.62845, Acc : 0.675, Sensitive_Loss : 1.03534, Sensitive_Acc : 18.100, Run Time : 8.03 sec
INFO:root:2024-04-11 12:03:43, Train, Epoch : 1, Step : 100, Loss : 0.64900, Acc : 0.653, Sensitive_Loss : 1.03400, Sensitive_Acc : 19.300, Run Time : 8.30 sec
INFO:root:2024-04-11 12:05:30, Dev, Step : 100, Loss : 0.63637, Acc : 0.668, Auc : 0.741, Sensitive_Loss : 1.00496, Sensitive_Acc : 15.902, Sensitive_Auc : 0.698, Mean auc: 0.741, Run Time : 106.34 sec
INFO:root:2024-04-11 12:05:30, Best, Step : 100, Loss : 0.63637, Acc : 0.668, Auc : 0.741, Sensitive_Loss : 1.00496, Sensitive_Acc : 15.902, Sensitive_Auc : 0.698, Best Auc : 0.741
INFO:root:2024-04-11 12:05:36, Train, Epoch : 1, Step : 110, Loss : 0.58748, Acc : 0.669, Sensitive_Loss : 1.05129, Sensitive_Acc : 15.900, Run Time : 113.00 sec
INFO:root:2024-04-11 12:05:45, Train, Epoch : 1, Step : 120, Loss : 0.59991, Acc : 0.684, Sensitive_Loss : 1.01106, Sensitive_Acc : 24.600, Run Time : 8.71 sec
INFO:root:2024-04-11 12:05:54, Train, Epoch : 1, Step : 130, Loss : 0.59037, Acc : 0.688, Sensitive_Loss : 0.94573, Sensitive_Acc : 20.900, Run Time : 8.80 sec
INFO:root:2024-04-11 12:06:02, Train, Epoch : 1, Step : 140, Loss : 0.73178, Acc : 0.613, Sensitive_Loss : 1.04227, Sensitive_Acc : 17.800, Run Time : 8.28 sec
INFO:root:2024-04-11 12:06:10, Train, Epoch : 1, Step : 150, Loss : 0.63995, Acc : 0.647, Sensitive_Loss : 1.03597, Sensitive_Acc : 26.000, Run Time : 8.11 sec
INFO:root:2024-04-11 12:06:20, Train, Epoch : 1, Step : 160, Loss : 0.62370, Acc : 0.684, Sensitive_Loss : 1.01304, Sensitive_Acc : 15.700, Run Time : 9.55 sec
INFO:root:2024-04-11 12:06:28, Train, Epoch : 1, Step : 170, Loss : 0.60621, Acc : 0.678, Sensitive_Loss : 0.94596, Sensitive_Acc : 14.800, Run Time : 8.20 sec
INFO:root:2024-04-11 12:06:36, Train, Epoch : 1, Step : 180, Loss : 0.66497, Acc : 0.684, Sensitive_Loss : 1.00448, Sensitive_Acc : 15.900, Run Time : 8.42 sec
INFO:root:2024-04-11 12:06:45, Train, Epoch : 1, Step : 190, Loss : 0.59567, Acc : 0.722, Sensitive_Loss : 0.94114, Sensitive_Acc : 19.700, Run Time : 8.17 sec
INFO:root:2024-04-11 12:06:53, Train, Epoch : 1, Step : 200, Loss : 0.60914, Acc : 0.681, Sensitive_Loss : 0.85727, Sensitive_Acc : 19.700, Run Time : 8.74 sec
INFO:root:2024-04-11 12:08:21, Dev, Step : 200, Loss : 0.64509, Acc : 0.649, Auc : 0.747, Sensitive_Loss : 0.94726, Sensitive_Acc : 20.293, Sensitive_Auc : 0.709, Mean auc: 0.747, Run Time : 88.19 sec
INFO:root:2024-04-11 12:08:22, Best, Step : 200, Loss : 0.64509, Acc : 0.649, Auc : 0.747, Sensitive_Loss : 0.94726, Sensitive_Acc : 20.293, Sensitive_Auc : 0.709, Best Auc : 0.747
INFO:root:2024-04-11 12:08:29, Train, Epoch : 1, Step : 210, Loss : 0.58691, Acc : 0.741, Sensitive_Loss : 0.93086, Sensitive_Acc : 24.000, Run Time : 95.26 sec
INFO:root:2024-04-11 12:08:41, Train, Epoch : 1, Step : 220, Loss : 0.60636, Acc : 0.681, Sensitive_Loss : 0.92954, Sensitive_Acc : 22.400, Run Time : 12.77 sec
INFO:root:2024-04-11 12:08:53, Train, Epoch : 1, Step : 230, Loss : 0.55410, Acc : 0.684, Sensitive_Loss : 0.88697, Sensitive_Acc : 17.800, Run Time : 12.07 sec
INFO:root:2024-04-11 12:09:04, Train, Epoch : 1, Step : 240, Loss : 0.62811, Acc : 0.700, Sensitive_Loss : 0.92772, Sensitive_Acc : 23.400, Run Time : 10.63 sec
INFO:root:2024-04-11 12:09:16, Train, Epoch : 1, Step : 250, Loss : 0.63417, Acc : 0.672, Sensitive_Loss : 0.92262, Sensitive_Acc : 25.400, Run Time : 12.43 sec
INFO:root:2024-04-11 12:09:28, Train, Epoch : 1, Step : 260, Loss : 0.63712, Acc : 0.634, Sensitive_Loss : 0.85719, Sensitive_Acc : 21.000, Run Time : 11.31 sec
INFO:root:2024-04-11 12:09:39, Train, Epoch : 1, Step : 270, Loss : 0.57995, Acc : 0.697, Sensitive_Loss : 0.95349, Sensitive_Acc : 20.300, Run Time : 10.92 sec
INFO:root:2024-04-11 12:09:51, Train, Epoch : 1, Step : 280, Loss : 0.56949, Acc : 0.700, Sensitive_Loss : 0.87830, Sensitive_Acc : 15.700, Run Time : 12.27 sec
INFO:root:2024-04-11 12:10:02, Train, Epoch : 1, Step : 290, Loss : 0.62845, Acc : 0.706, Sensitive_Loss : 0.79111, Sensitive_Acc : 19.700, Run Time : 11.10 sec
INFO:root:2024-04-11 12:10:13, Train, Epoch : 1, Step : 300, Loss : 0.61046, Acc : 0.706, Sensitive_Loss : 0.79771, Sensitive_Acc : 22.000, Run Time : 10.70 sec
INFO:root:2024-04-11 12:11:41, Dev, Step : 300, Loss : 0.60376, Acc : 0.714, Auc : 0.777, Sensitive_Loss : 0.76924, Sensitive_Acc : 19.150, Sensitive_Auc : 0.823, Mean auc: 0.777, Run Time : 88.56 sec
INFO:root:2024-04-11 12:11:42, Best, Step : 300, Loss : 0.60376, Acc : 0.714, Auc : 0.777, Sensitive_Loss : 0.76924, Sensitive_Acc : 19.150, Sensitive_Auc : 0.823, Best Auc : 0.777
INFO:root:2024-04-11 12:11:50, Train, Epoch : 1, Step : 310, Loss : 0.66446, Acc : 0.653, Sensitive_Loss : 0.87789, Sensitive_Acc : 20.200, Run Time : 97.04 sec
INFO:root:2024-04-11 12:12:02, Train, Epoch : 1, Step : 320, Loss : 0.58063, Acc : 0.697, Sensitive_Loss : 0.72945, Sensitive_Acc : 20.700, Run Time : 12.27 sec
INFO:root:2024-04-11 12:12:16, Train, Epoch : 1, Step : 330, Loss : 0.61713, Acc : 0.684, Sensitive_Loss : 0.79563, Sensitive_Acc : 18.000, Run Time : 13.52 sec
INFO:root:2024-04-11 12:12:27, Train, Epoch : 1, Step : 340, Loss : 0.55483, Acc : 0.694, Sensitive_Loss : 0.80756, Sensitive_Acc : 21.500, Run Time : 11.16 sec
INFO:root:2024-04-11 12:12:39, Train, Epoch : 1, Step : 350, Loss : 0.53558, Acc : 0.722, Sensitive_Loss : 0.66524, Sensitive_Acc : 24.200, Run Time : 12.10 sec
INFO:root:2024-04-11 12:12:50, Train, Epoch : 1, Step : 360, Loss : 0.59233, Acc : 0.688, Sensitive_Loss : 0.71639, Sensitive_Acc : 24.100, Run Time : 11.09 sec
INFO:root:2024-04-11 12:13:01, Train, Epoch : 1, Step : 370, Loss : 0.70528, Acc : 0.678, Sensitive_Loss : 0.67800, Sensitive_Acc : 19.200, Run Time : 10.71 sec
INFO:root:2024-04-11 12:13:12, Train, Epoch : 1, Step : 380, Loss : 0.63308, Acc : 0.666, Sensitive_Loss : 0.84688, Sensitive_Acc : 20.900, Run Time : 11.72 sec
INFO:root:2024-04-11 12:13:23, Train, Epoch : 1, Step : 390, Loss : 0.67733, Acc : 0.681, Sensitive_Loss : 0.69665, Sensitive_Acc : 17.300, Run Time : 10.95 sec
INFO:root:2024-04-11 12:13:34, Train, Epoch : 1, Step : 400, Loss : 0.58804, Acc : 0.669, Sensitive_Loss : 0.58462, Sensitive_Acc : 20.600, Run Time : 10.71 sec
INFO:root:2024-04-11 12:15:02, Dev, Step : 400, Loss : 0.61207, Acc : 0.685, Auc : 0.779, Sensitive_Loss : 0.62471, Sensitive_Acc : 18.970, Sensitive_Auc : 0.922, Mean auc: 0.779, Run Time : 88.09 sec
INFO:root:2024-04-11 12:15:03, Best, Step : 400, Loss : 0.61207, Acc : 0.685, Auc : 0.779, Sensitive_Loss : 0.62471, Sensitive_Acc : 18.970, Sensitive_Auc : 0.922, Best Auc : 0.779
INFO:root:2024-04-11 12:15:10, Train, Epoch : 1, Step : 410, Loss : 0.59829, Acc : 0.713, Sensitive_Loss : 0.70835, Sensitive_Acc : 21.900, Run Time : 96.09 sec
INFO:root:2024-04-11 12:15:21, Train, Epoch : 1, Step : 420, Loss : 0.61223, Acc : 0.709, Sensitive_Loss : 0.59900, Sensitive_Acc : 22.200, Run Time : 10.96 sec
INFO:root:2024-04-11 12:15:35, Train, Epoch : 1, Step : 430, Loss : 0.58405, Acc : 0.719, Sensitive_Loss : 0.54309, Sensitive_Acc : 23.400, Run Time : 13.96 sec
INFO:root:2024-04-11 12:15:45, Train, Epoch : 1, Step : 440, Loss : 0.60491, Acc : 0.703, Sensitive_Loss : 0.60638, Sensitive_Acc : 10.100, Run Time : 10.42 sec
INFO:root:2024-04-11 12:15:56, Train, Epoch : 1, Step : 450, Loss : 0.63420, Acc : 0.688, Sensitive_Loss : 0.54507, Sensitive_Acc : 17.900, Run Time : 10.82 sec
INFO:root:2024-04-11 12:16:07, Train, Epoch : 1, Step : 460, Loss : 0.54823, Acc : 0.697, Sensitive_Loss : 0.52920, Sensitive_Acc : 16.900, Run Time : 10.83 sec
INFO:root:2024-04-11 12:16:18, Train, Epoch : 1, Step : 470, Loss : 0.54530, Acc : 0.741, Sensitive_Loss : 0.71539, Sensitive_Acc : 21.600, Run Time : 10.62 sec
INFO:root:2024-04-11 12:16:28, Train, Epoch : 1, Step : 480, Loss : 0.61757, Acc : 0.691, Sensitive_Loss : 0.50803, Sensitive_Acc : 23.400, Run Time : 9.94 sec
INFO:root:2024-04-11 12:16:40, Train, Epoch : 1, Step : 490, Loss : 0.56191, Acc : 0.722, Sensitive_Loss : 0.55152, Sensitive_Acc : 21.500, Run Time : 12.24 sec
INFO:root:2024-04-11 12:16:50, Train, Epoch : 1, Step : 500, Loss : 0.53158, Acc : 0.722, Sensitive_Loss : 0.53033, Sensitive_Acc : 25.300, Run Time : 9.78 sec
INFO:root:2024-04-11 12:18:18, Dev, Step : 500, Loss : 0.58093, Acc : 0.725, Auc : 0.794, Sensitive_Loss : 0.63461, Sensitive_Acc : 16.368, Sensitive_Auc : 0.935, Mean auc: 0.794, Run Time : 88.83 sec
INFO:root:2024-04-11 12:18:19, Best, Step : 500, Loss : 0.58093, Acc : 0.725, Auc : 0.794, Sensitive_Loss : 0.63461, Sensitive_Acc : 16.368, Sensitive_Auc : 0.935, Best Auc : 0.794
INFO:root:2024-04-11 12:18:27, Train, Epoch : 1, Step : 510, Loss : 0.57732, Acc : 0.728, Sensitive_Loss : 0.53796, Sensitive_Acc : 15.800, Run Time : 96.91 sec
INFO:root:2024-04-11 12:18:38, Train, Epoch : 1, Step : 520, Loss : 0.49204, Acc : 0.731, Sensitive_Loss : 0.47477, Sensitive_Acc : 20.800, Run Time : 11.19 sec
INFO:root:2024-04-11 12:18:59, Train, Epoch : 1, Step : 530, Loss : 0.50441, Acc : 0.747, Sensitive_Loss : 0.42226, Sensitive_Acc : 22.500, Run Time : 21.42 sec
INFO:root:2024-04-11 12:19:11, Train, Epoch : 1, Step : 540, Loss : 0.59638, Acc : 0.747, Sensitive_Loss : 0.58644, Sensitive_Acc : 21.900, Run Time : 11.82 sec
INFO:root:2024-04-11 12:19:49, Train, Epoch : 1, Step : 550, Loss : 0.58982, Acc : 0.694, Sensitive_Loss : 0.48288, Sensitive_Acc : 20.100, Run Time : 37.73 sec
INFO:root:2024-04-11 12:19:59, Train, Epoch : 1, Step : 560, Loss : 0.64503, Acc : 0.703, Sensitive_Loss : 0.41804, Sensitive_Acc : 19.800, Run Time : 10.40 sec
INFO:root:2024-04-11 12:20:10, Train, Epoch : 1, Step : 570, Loss : 0.63910, Acc : 0.697, Sensitive_Loss : 0.40406, Sensitive_Acc : 18.400, Run Time : 10.61 sec
INFO:root:2024-04-11 12:20:20, Train, Epoch : 1, Step : 580, Loss : 0.58782, Acc : 0.713, Sensitive_Loss : 0.59884, Sensitive_Acc : 21.600, Run Time : 9.94 sec
INFO:root:2024-04-11 12:20:29, Train, Epoch : 1, Step : 590, Loss : 0.56916, Acc : 0.684, Sensitive_Loss : 0.50643, Sensitive_Acc : 18.100, Run Time : 9.35 sec
INFO:root:2024-04-11 12:20:41, Train, Epoch : 1, Step : 600, Loss : 0.57986, Acc : 0.719, Sensitive_Loss : 0.44529, Sensitive_Acc : 20.600, Run Time : 11.62 sec
INFO:root:2024-04-11 12:22:09, Dev, Step : 600, Loss : 0.59629, Acc : 0.694, Auc : 0.801, Sensitive_Loss : 0.37488, Sensitive_Acc : 20.820, Sensitive_Auc : 0.964, Mean auc: 0.801, Run Time : 88.72 sec
INFO:root:2024-04-11 12:22:10, Best, Step : 600, Loss : 0.59629, Acc : 0.694, Auc : 0.801, Sensitive_Loss : 0.37488, Sensitive_Acc : 20.820, Sensitive_Auc : 0.964, Best Auc : 0.801
INFO:root:2024-04-11 12:22:17, Train, Epoch : 1, Step : 610, Loss : 0.53630, Acc : 0.716, Sensitive_Loss : 0.52057, Sensitive_Acc : 23.100, Run Time : 96.33 sec
INFO:root:2024-04-11 12:22:26, Train, Epoch : 1, Step : 620, Loss : 0.54866, Acc : 0.719, Sensitive_Loss : 0.39384, Sensitive_Acc : 19.800, Run Time : 9.04 sec
INFO:root:2024-04-11 12:22:35, Train, Epoch : 1, Step : 630, Loss : 0.64694, Acc : 0.662, Sensitive_Loss : 0.35589, Sensitive_Acc : 18.600, Run Time : 8.48 sec
INFO:root:2024-04-11 12:24:05
INFO:root:y_pred: [0.33262393 0.29058462 0.35564655 ... 0.29155675 0.12106805 0.4918508 ]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [3.82565744e-02 3.75835150e-02 1.23319067e-02 1.32436439e-01
 8.73453841e-02 1.10664420e-01 6.98167190e-04 2.77300030e-02
 6.76331460e-01 9.11994338e-01 2.67323405e-01 2.08030865e-02
 2.91824430e-01 2.53308943e-04 9.15525854e-01 1.33888856e-01
 2.90340488e-03 9.01249647e-01 8.00411582e-01 4.88209575e-02
 7.60663569e-01 2.93012019e-02 3.13439399e-01 3.11685890e-01
 1.06688984e-01 1.76365748e-01 2.23993952e-03 2.17516646e-01
 8.13089355e-05 2.64152616e-01 4.84395437e-02 7.57398903e-01
 3.91549654e-02 4.68014359e-01 1.21485197e-03 5.54522208e-04
 2.61467620e-04 5.02963781e-01 2.10227355e-01 1.79471225e-02
 3.06500226e-01 1.85708091e-01 4.14529055e-01 2.94221342e-01
 9.02882993e-01 9.72668529e-02 2.21123636e-01 3.23796064e-01
 5.36711276e-01 8.43748808e-01 6.32320881e-01 9.38997328e-01
 8.93539727e-01 1.69392582e-02 3.15933198e-01 2.40683377e-01
 2.45427201e-03 1.38986399e-02 8.86059046e-01 1.22364452e-02
 2.99320742e-02 1.28845230e-01 3.97015400e-02 7.91261991e-05
 8.31102610e-01 4.29400265e-01 2.92644673e-03 3.65755767e-01
 4.22312140e-01 7.40677476e-01 9.34275150e-01 9.65883374e-01
 1.17216930e-01 2.79256135e-01 1.28825665e-01 1.17025316e-01
 6.73646899e-03 5.42094524e-04 8.32981837e-04 7.12735057e-02
 4.24003631e-01 1.40652340e-02 7.95281172e-01 9.25399244e-01
 4.80998009e-01 3.94307792e-01 6.49360823e-04 1.12721942e-01
 1.16581079e-02 1.21755833e-02 4.18081060e-02 8.47944915e-01
 1.23309891e-03 1.29085931e-03 1.25970900e-01 1.29064381e-01
 4.75851353e-04 4.93979990e-01 1.95394387e-03 7.11927860e-05
 6.47314265e-03 1.43565387e-01 2.65000403e-01 3.28593334e-04
 9.45112407e-02 2.89027896e-02 2.00644717e-01 6.73255861e-01
 3.68135601e-01 7.94041216e-01 1.77308591e-03 9.36711133e-01
 8.83374870e-01 1.45834332e-04 2.26142317e-01 2.81348944e-01
 6.78510964e-01 6.83530699e-04 1.23558514e-01 2.07277983e-01
 8.98367614e-02 1.34930958e-03 2.89473116e-01 2.44277995e-04
 6.92658424e-02 7.35099435e-01 5.39418310e-03 7.86022067e-01
 1.57441795e-02 1.44141763e-01 1.18755819e-02 2.13253982e-02
 7.64559663e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 12:24:05, Dev, Step : 634, Loss : 0.59552, Acc : 0.694, Auc : 0.790, Sensitive_Loss : 0.38856, Sensitive_Acc : 21.406, Sensitive_Auc : 0.973, Mean auc: 0.790, Run Time : 87.29 sec
INFO:root:2024-04-11 12:24:13, Train, Epoch : 2, Step : 640, Loss : 0.29323, Acc : 0.444, Sensitive_Loss : 0.25324, Sensitive_Acc : 11.100, Run Time : 6.94 sec
INFO:root:2024-04-11 12:24:21, Train, Epoch : 2, Step : 650, Loss : 0.52080, Acc : 0.744, Sensitive_Loss : 0.30253, Sensitive_Acc : 23.900, Run Time : 7.62 sec
INFO:root:2024-04-11 12:24:28, Train, Epoch : 2, Step : 660, Loss : 0.59555, Acc : 0.738, Sensitive_Loss : 0.38714, Sensitive_Acc : 23.800, Run Time : 7.58 sec
INFO:root:2024-04-11 12:24:36, Train, Epoch : 2, Step : 670, Loss : 0.56144, Acc : 0.728, Sensitive_Loss : 0.40553, Sensitive_Acc : 25.500, Run Time : 7.39 sec
INFO:root:2024-04-11 12:24:43, Train, Epoch : 2, Step : 680, Loss : 0.56542, Acc : 0.728, Sensitive_Loss : 0.37762, Sensitive_Acc : 24.300, Run Time : 7.59 sec
INFO:root:2024-04-11 12:24:51, Train, Epoch : 2, Step : 690, Loss : 0.56650, Acc : 0.697, Sensitive_Loss : 0.44824, Sensitive_Acc : 19.700, Run Time : 7.39 sec
INFO:root:2024-04-11 12:24:58, Train, Epoch : 2, Step : 700, Loss : 0.53750, Acc : 0.734, Sensitive_Loss : 0.37782, Sensitive_Acc : 21.200, Run Time : 7.06 sec
INFO:root:2024-04-11 12:26:30, Dev, Step : 700, Loss : 0.59693, Acc : 0.707, Auc : 0.788, Sensitive_Loss : 0.36581, Sensitive_Acc : 20.263, Sensitive_Auc : 0.973, Mean auc: 0.788, Run Time : 92.09 sec
INFO:root:2024-04-11 12:26:35, Train, Epoch : 2, Step : 710, Loss : 0.53633, Acc : 0.741, Sensitive_Loss : 0.34771, Sensitive_Acc : 17.700, Run Time : 97.75 sec
INFO:root:2024-04-11 12:26:43, Train, Epoch : 2, Step : 720, Loss : 0.49247, Acc : 0.762, Sensitive_Loss : 0.40912, Sensitive_Acc : 17.500, Run Time : 7.29 sec
INFO:root:2024-04-11 12:26:50, Train, Epoch : 2, Step : 730, Loss : 0.56752, Acc : 0.719, Sensitive_Loss : 0.38686, Sensitive_Acc : 21.900, Run Time : 7.62 sec
INFO:root:2024-04-11 12:26:59, Train, Epoch : 2, Step : 740, Loss : 0.52841, Acc : 0.716, Sensitive_Loss : 0.35218, Sensitive_Acc : 19.700, Run Time : 8.67 sec
INFO:root:2024-04-11 12:27:06, Train, Epoch : 2, Step : 750, Loss : 0.53631, Acc : 0.759, Sensitive_Loss : 0.52726, Sensitive_Acc : 20.300, Run Time : 7.30 sec
INFO:root:2024-04-11 12:27:13, Train, Epoch : 2, Step : 760, Loss : 0.51221, Acc : 0.722, Sensitive_Loss : 0.29046, Sensitive_Acc : 21.900, Run Time : 6.89 sec
INFO:root:2024-04-11 12:27:21, Train, Epoch : 2, Step : 770, Loss : 0.52254, Acc : 0.769, Sensitive_Loss : 0.34599, Sensitive_Acc : 18.400, Run Time : 7.71 sec
INFO:root:2024-04-11 12:27:28, Train, Epoch : 2, Step : 780, Loss : 0.56418, Acc : 0.734, Sensitive_Loss : 0.37346, Sensitive_Acc : 18.700, Run Time : 7.56 sec
INFO:root:2024-04-11 12:27:36, Train, Epoch : 2, Step : 790, Loss : 0.43770, Acc : 0.769, Sensitive_Loss : 0.26826, Sensitive_Acc : 20.600, Run Time : 7.72 sec
INFO:root:2024-04-11 12:27:44, Train, Epoch : 2, Step : 800, Loss : 0.58074, Acc : 0.688, Sensitive_Loss : 0.35510, Sensitive_Acc : 22.400, Run Time : 7.56 sec
INFO:root:2024-04-11 12:29:14, Dev, Step : 800, Loss : 0.58525, Acc : 0.725, Auc : 0.797, Sensitive_Loss : 0.30340, Sensitive_Acc : 21.030, Sensitive_Auc : 0.981, Mean auc: 0.797, Run Time : 90.60 sec
INFO:root:2024-04-11 12:29:20, Train, Epoch : 2, Step : 810, Loss : 0.50425, Acc : 0.762, Sensitive_Loss : 0.31299, Sensitive_Acc : 22.100, Run Time : 96.16 sec
INFO:root:2024-04-11 12:29:28, Train, Epoch : 2, Step : 820, Loss : 0.62533, Acc : 0.713, Sensitive_Loss : 0.32722, Sensitive_Acc : 19.100, Run Time : 8.28 sec
INFO:root:2024-04-11 12:29:37, Train, Epoch : 2, Step : 830, Loss : 0.62798, Acc : 0.716, Sensitive_Loss : 0.32874, Sensitive_Acc : 24.800, Run Time : 8.94 sec
INFO:root:2024-04-11 12:29:45, Train, Epoch : 2, Step : 840, Loss : 0.57230, Acc : 0.728, Sensitive_Loss : 0.32973, Sensitive_Acc : 24.000, Run Time : 7.48 sec
INFO:root:2024-04-11 12:29:53, Train, Epoch : 2, Step : 850, Loss : 0.57737, Acc : 0.719, Sensitive_Loss : 0.32522, Sensitive_Acc : 20.900, Run Time : 8.24 sec
INFO:root:2024-04-11 12:30:00, Train, Epoch : 2, Step : 860, Loss : 0.42919, Acc : 0.762, Sensitive_Loss : 0.34969, Sensitive_Acc : 23.500, Run Time : 7.11 sec
INFO:root:2024-04-11 12:30:08, Train, Epoch : 2, Step : 870, Loss : 0.49213, Acc : 0.756, Sensitive_Loss : 0.22943, Sensitive_Acc : 15.900, Run Time : 8.07 sec
INFO:root:2024-04-11 12:30:16, Train, Epoch : 2, Step : 880, Loss : 0.58080, Acc : 0.744, Sensitive_Loss : 0.30235, Sensitive_Acc : 22.900, Run Time : 7.64 sec
INFO:root:2024-04-11 12:30:23, Train, Epoch : 2, Step : 890, Loss : 0.54632, Acc : 0.731, Sensitive_Loss : 0.30421, Sensitive_Acc : 23.000, Run Time : 7.81 sec
INFO:root:2024-04-11 12:30:31, Train, Epoch : 2, Step : 900, Loss : 0.50808, Acc : 0.753, Sensitive_Loss : 0.34400, Sensitive_Acc : 19.500, Run Time : 7.79 sec
INFO:root:2024-04-11 12:32:03, Dev, Step : 900, Loss : 0.57168, Acc : 0.733, Auc : 0.807, Sensitive_Loss : 0.53882, Sensitive_Acc : 17.060, Sensitive_Auc : 0.960, Mean auc: 0.807, Run Time : 92.08 sec
INFO:root:2024-04-11 12:32:04, Best, Step : 900, Loss : 0.57168, Acc : 0.733, Auc : 0.807, Sensitive_Loss : 0.53882, Sensitive_Acc : 17.060, Sensitive_Auc : 0.960, Best Auc : 0.807
INFO:root:2024-04-11 12:32:10, Train, Epoch : 2, Step : 910, Loss : 0.55198, Acc : 0.731, Sensitive_Loss : 0.33407, Sensitive_Acc : 20.100, Run Time : 98.35 sec
INFO:root:2024-04-11 12:32:18, Train, Epoch : 2, Step : 920, Loss : 0.59683, Acc : 0.684, Sensitive_Loss : 0.32335, Sensitive_Acc : 13.600, Run Time : 8.35 sec
INFO:root:2024-04-11 12:32:28, Train, Epoch : 2, Step : 930, Loss : 0.56612, Acc : 0.741, Sensitive_Loss : 0.29314, Sensitive_Acc : 22.300, Run Time : 9.70 sec
INFO:root:2024-04-11 12:32:36, Train, Epoch : 2, Step : 940, Loss : 0.52197, Acc : 0.759, Sensitive_Loss : 0.37615, Sensitive_Acc : 24.600, Run Time : 8.03 sec
INFO:root:2024-04-11 12:32:44, Train, Epoch : 2, Step : 950, Loss : 0.53496, Acc : 0.709, Sensitive_Loss : 0.32275, Sensitive_Acc : 16.800, Run Time : 8.32 sec
INFO:root:2024-04-11 12:32:56, Train, Epoch : 2, Step : 960, Loss : 0.55844, Acc : 0.734, Sensitive_Loss : 0.42775, Sensitive_Acc : 18.200, Run Time : 12.06 sec
INFO:root:2024-04-11 12:33:04, Train, Epoch : 2, Step : 970, Loss : 0.54981, Acc : 0.744, Sensitive_Loss : 0.36826, Sensitive_Acc : 21.300, Run Time : 8.38 sec
INFO:root:2024-04-11 12:33:14, Train, Epoch : 2, Step : 980, Loss : 0.59664, Acc : 0.700, Sensitive_Loss : 0.34764, Sensitive_Acc : 20.000, Run Time : 9.42 sec
INFO:root:2024-04-11 12:33:21, Train, Epoch : 2, Step : 990, Loss : 0.51255, Acc : 0.728, Sensitive_Loss : 0.25642, Sensitive_Acc : 19.400, Run Time : 7.09 sec
INFO:root:2024-04-11 12:33:30, Train, Epoch : 2, Step : 1000, Loss : 0.53968, Acc : 0.731, Sensitive_Loss : 0.34113, Sensitive_Acc : 18.400, Run Time : 8.63 sec
INFO:root:2024-04-11 12:35:03, Dev, Step : 1000, Loss : 0.55314, Acc : 0.737, Auc : 0.823, Sensitive_Loss : 0.38553, Sensitive_Acc : 18.774, Sensitive_Auc : 0.992, Mean auc: 0.823, Run Time : 93.15 sec
INFO:root:2024-04-11 12:35:04, Best, Step : 1000, Loss : 0.55314, Acc : 0.737, Auc : 0.823, Sensitive_Loss : 0.38553, Sensitive_Acc : 18.774, Sensitive_Auc : 0.992, Best Auc : 0.823
INFO:root:2024-04-11 12:35:11, Train, Epoch : 2, Step : 1010, Loss : 0.58487, Acc : 0.703, Sensitive_Loss : 0.36145, Sensitive_Acc : 16.700, Run Time : 101.33 sec
INFO:root:2024-04-11 12:35:19, Train, Epoch : 2, Step : 1020, Loss : 0.61493, Acc : 0.728, Sensitive_Loss : 0.30624, Sensitive_Acc : 22.800, Run Time : 8.09 sec
INFO:root:2024-04-11 12:35:27, Train, Epoch : 2, Step : 1030, Loss : 0.51139, Acc : 0.738, Sensitive_Loss : 0.28963, Sensitive_Acc : 20.700, Run Time : 8.35 sec
INFO:root:2024-04-11 12:35:37, Train, Epoch : 2, Step : 1040, Loss : 0.46683, Acc : 0.762, Sensitive_Loss : 0.46843, Sensitive_Acc : 16.800, Run Time : 9.23 sec
INFO:root:2024-04-11 12:35:46, Train, Epoch : 2, Step : 1050, Loss : 0.55476, Acc : 0.756, Sensitive_Loss : 0.23902, Sensitive_Acc : 22.700, Run Time : 9.40 sec
INFO:root:2024-04-11 12:35:54, Train, Epoch : 2, Step : 1060, Loss : 0.58575, Acc : 0.744, Sensitive_Loss : 0.28220, Sensitive_Acc : 20.600, Run Time : 8.20 sec
INFO:root:2024-04-11 12:36:02, Train, Epoch : 2, Step : 1070, Loss : 0.47546, Acc : 0.744, Sensitive_Loss : 0.32540, Sensitive_Acc : 22.500, Run Time : 8.06 sec
INFO:root:2024-04-11 12:36:10, Train, Epoch : 2, Step : 1080, Loss : 0.56953, Acc : 0.731, Sensitive_Loss : 0.25647, Sensitive_Acc : 20.600, Run Time : 7.86 sec
INFO:root:2024-04-11 12:36:18, Train, Epoch : 2, Step : 1090, Loss : 0.64211, Acc : 0.744, Sensitive_Loss : 0.26315, Sensitive_Acc : 18.900, Run Time : 8.22 sec
INFO:root:2024-04-11 12:36:26, Train, Epoch : 2, Step : 1100, Loss : 0.64723, Acc : 0.672, Sensitive_Loss : 0.26582, Sensitive_Acc : 21.400, Run Time : 7.70 sec
INFO:root:2024-04-11 12:37:56, Dev, Step : 1100, Loss : 0.63813, Acc : 0.704, Auc : 0.820, Sensitive_Loss : 0.26518, Sensitive_Acc : 21.977, Sensitive_Auc : 0.990, Mean auc: 0.820, Run Time : 90.49 sec
INFO:root:2024-04-11 12:38:02, Train, Epoch : 2, Step : 1110, Loss : 0.50009, Acc : 0.747, Sensitive_Loss : 0.28133, Sensitive_Acc : 20.900, Run Time : 96.41 sec
INFO:root:2024-04-11 12:38:12, Train, Epoch : 2, Step : 1120, Loss : 0.55035, Acc : 0.750, Sensitive_Loss : 0.32312, Sensitive_Acc : 22.300, Run Time : 9.32 sec
INFO:root:2024-04-11 12:38:20, Train, Epoch : 2, Step : 1130, Loss : 0.48771, Acc : 0.741, Sensitive_Loss : 0.22090, Sensitive_Acc : 21.500, Run Time : 7.95 sec
INFO:root:2024-04-11 12:38:27, Train, Epoch : 2, Step : 1140, Loss : 0.59427, Acc : 0.688, Sensitive_Loss : 0.19967, Sensitive_Acc : 16.600, Run Time : 7.75 sec
INFO:root:2024-04-11 12:38:35, Train, Epoch : 2, Step : 1150, Loss : 0.54113, Acc : 0.697, Sensitive_Loss : 0.30619, Sensitive_Acc : 19.200, Run Time : 7.51 sec
INFO:root:2024-04-11 12:38:43, Train, Epoch : 2, Step : 1160, Loss : 0.58534, Acc : 0.713, Sensitive_Loss : 0.28141, Sensitive_Acc : 21.200, Run Time : 7.83 sec
INFO:root:2024-04-11 12:38:51, Train, Epoch : 2, Step : 1170, Loss : 0.52141, Acc : 0.728, Sensitive_Loss : 0.30232, Sensitive_Acc : 20.100, Run Time : 8.08 sec
INFO:root:2024-04-11 12:38:59, Train, Epoch : 2, Step : 1180, Loss : 0.52968, Acc : 0.741, Sensitive_Loss : 0.22686, Sensitive_Acc : 22.300, Run Time : 7.82 sec
INFO:root:2024-04-11 12:39:07, Train, Epoch : 2, Step : 1190, Loss : 0.55436, Acc : 0.719, Sensitive_Loss : 0.25242, Sensitive_Acc : 26.200, Run Time : 7.96 sec
INFO:root:2024-04-11 12:39:15, Train, Epoch : 2, Step : 1200, Loss : 0.59545, Acc : 0.722, Sensitive_Loss : 0.22295, Sensitive_Acc : 21.900, Run Time : 8.00 sec
INFO:root:2024-04-11 12:40:45, Dev, Step : 1200, Loss : 0.54169, Acc : 0.748, Auc : 0.827, Sensitive_Loss : 0.24119, Sensitive_Acc : 20.564, Sensitive_Auc : 0.991, Mean auc: 0.827, Run Time : 90.87 sec
INFO:root:2024-04-11 12:40:46, Best, Step : 1200, Loss : 0.54169, Acc : 0.748, Auc : 0.827, Sensitive_Loss : 0.24119, Sensitive_Acc : 20.564, Sensitive_Auc : 0.991, Best Auc : 0.827
INFO:root:2024-04-11 12:40:52, Train, Epoch : 2, Step : 1210, Loss : 0.47597, Acc : 0.753, Sensitive_Loss : 0.19000, Sensitive_Acc : 26.400, Run Time : 97.57 sec
INFO:root:2024-04-11 12:41:00, Train, Epoch : 2, Step : 1220, Loss : 0.56000, Acc : 0.766, Sensitive_Loss : 0.22914, Sensitive_Acc : 23.100, Run Time : 7.83 sec
INFO:root:2024-04-11 12:41:08, Train, Epoch : 2, Step : 1230, Loss : 0.51832, Acc : 0.719, Sensitive_Loss : 0.27481, Sensitive_Acc : 20.200, Run Time : 7.86 sec
INFO:root:2024-04-11 12:41:16, Train, Epoch : 2, Step : 1240, Loss : 0.49711, Acc : 0.762, Sensitive_Loss : 0.23635, Sensitive_Acc : 24.200, Run Time : 7.75 sec
INFO:root:2024-04-11 12:41:23, Train, Epoch : 2, Step : 1250, Loss : 0.47481, Acc : 0.766, Sensitive_Loss : 0.24333, Sensitive_Acc : 21.300, Run Time : 7.32 sec
INFO:root:2024-04-11 12:41:32, Train, Epoch : 2, Step : 1260, Loss : 0.52668, Acc : 0.791, Sensitive_Loss : 0.30278, Sensitive_Acc : 16.800, Run Time : 9.33 sec
INFO:root:2024-04-11 12:43:14
INFO:root:y_pred: [0.05774947 0.13390286 0.23929796 ... 0.392942   0.04591419 0.11974778]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.4248170e-04 2.0567442e-03 1.2686960e-03 3.3150345e-02 3.5285568e-03
 5.4714829e-04 8.5129533e-03 4.4193410e-05 1.8698201e-02 9.9715936e-01
 2.5995541e-01 6.7136403e-05 7.8033516e-04 1.4719624e-08 9.9714667e-01
 1.4493290e-02 6.0597644e-03 9.9770504e-01 9.9038154e-01 6.5514418e-05
 7.7080828e-01 2.9415429e-05 4.9970958e-02 1.4130669e-02 9.5677460e-03
 5.3983337e-01 6.6592392e-08 1.5928160e-03 1.1459387e-05 8.0156170e-02
 3.8108967e-02 8.5458589e-01 1.0820715e-02 9.2699772e-01 2.3299594e-05
 8.6013446e-05 2.1657425e-03 9.5379308e-02 1.8253726e-01 1.7937552e-03
 1.2303827e-02 8.4455001e-01 2.9508548e-02 1.0893780e-03 9.7313154e-01
 3.1656107e-01 9.9039720e-03 5.8341509e-01 2.2643674e-02 9.8614383e-01
 9.4963247e-01 9.9479997e-01 8.3635283e-01 2.0423310e-01 2.6975715e-01
 1.2745480e-01 3.0782138e-07 7.2955372e-06 9.8812217e-01 7.2793951e-03
 4.1635655e-04 4.8953276e-03 8.4659626e-04 8.0647231e-05 9.2173016e-01
 3.3508700e-01 2.7357237e-05 2.5045100e-01 2.3609100e-04 8.7286246e-01
 9.9798691e-01 9.9538678e-01 1.1741258e-03 3.3401141e-01 6.5796880e-06
 1.8085676e-01 1.3797441e-03 3.6248071e-05 6.0694019e-04 5.4043564e-03
 7.1636230e-02 7.0628402e-04 9.6851057e-01 8.0045277e-01 3.2375482e-01
 7.6044336e-02 2.6540205e-04 3.3775557e-04 6.4075191e-04 2.5819024e-06
 1.5912625e-05 6.2065744e-01 3.0179245e-07 5.6676589e-07 3.8284895e-03
 9.3207479e-02 4.2265572e-04 2.8359547e-01 1.8623680e-03 2.4284347e-05
 3.3834011e-05 1.1862332e-01 6.4682394e-01 1.2476029e-05 9.2481382e-02
 8.1373686e-03 1.1500546e-03 8.9784622e-01 3.6873323e-01 2.5100133e-01
 9.8040851e-04 9.9856782e-01 9.9324965e-01 2.2661693e-06 4.2567843e-01
 1.3516900e-01 5.0442213e-01 6.1362261e-05 1.7240827e-05 4.9762595e-03
 9.1218646e-04 4.2959361e-09 1.6250391e-03 1.2076641e-05 3.1545206e-03
 9.1188240e-01 1.9382026e-06 9.9057674e-01 3.1005928e-01 2.6546618e-01
 1.0100366e-06 1.2321585e-01 1.4929685e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 12:43:14, Dev, Step : 1268, Loss : 0.55513, Acc : 0.739, Auc : 0.818, Sensitive_Loss : 0.22772, Sensitive_Acc : 21.256, Sensitive_Auc : 0.990, Mean auc: 0.818, Run Time : 95.88 sec
INFO:root:2024-04-11 12:43:18, Train, Epoch : 3, Step : 1270, Loss : 0.10404, Acc : 0.150, Sensitive_Loss : 0.02152, Sensitive_Acc : 3.400, Run Time : 2.83 sec
INFO:root:2024-04-11 12:43:26, Train, Epoch : 3, Step : 1280, Loss : 0.57083, Acc : 0.716, Sensitive_Loss : 0.22170, Sensitive_Acc : 25.300, Run Time : 7.50 sec
INFO:root:2024-04-11 12:43:34, Train, Epoch : 3, Step : 1290, Loss : 0.58472, Acc : 0.731, Sensitive_Loss : 0.31732, Sensitive_Acc : 18.600, Run Time : 8.22 sec
INFO:root:2024-04-11 12:43:41, Train, Epoch : 3, Step : 1300, Loss : 0.46599, Acc : 0.766, Sensitive_Loss : 0.24203, Sensitive_Acc : 20.100, Run Time : 7.33 sec
INFO:root:2024-04-11 12:45:13, Dev, Step : 1300, Loss : 0.54253, Acc : 0.752, Auc : 0.832, Sensitive_Loss : 0.22223, Sensitive_Acc : 21.226, Sensitive_Auc : 0.993, Mean auc: 0.832, Run Time : 91.91 sec
INFO:root:2024-04-11 12:45:14, Best, Step : 1300, Loss : 0.54253, Acc : 0.752, Auc : 0.832, Sensitive_Loss : 0.22223, Sensitive_Acc : 21.226, Sensitive_Auc : 0.993, Best Auc : 0.832
INFO:root:2024-04-11 12:45:22, Train, Epoch : 3, Step : 1310, Loss : 0.53373, Acc : 0.759, Sensitive_Loss : 0.17324, Sensitive_Acc : 24.200, Run Time : 100.86 sec
INFO:root:2024-04-11 12:45:30, Train, Epoch : 3, Step : 1320, Loss : 0.47493, Acc : 0.794, Sensitive_Loss : 0.28133, Sensitive_Acc : 19.100, Run Time : 7.90 sec
INFO:root:2024-04-11 12:45:37, Train, Epoch : 3, Step : 1330, Loss : 0.49829, Acc : 0.762, Sensitive_Loss : 0.22207, Sensitive_Acc : 27.000, Run Time : 7.30 sec
INFO:root:2024-04-11 12:45:45, Train, Epoch : 3, Step : 1340, Loss : 0.49506, Acc : 0.784, Sensitive_Loss : 0.19906, Sensitive_Acc : 18.000, Run Time : 8.06 sec
INFO:root:2024-04-11 12:45:57, Train, Epoch : 3, Step : 1350, Loss : 0.44609, Acc : 0.762, Sensitive_Loss : 0.19133, Sensitive_Acc : 22.400, Run Time : 11.92 sec
INFO:root:2024-04-11 12:46:05, Train, Epoch : 3, Step : 1360, Loss : 0.50568, Acc : 0.800, Sensitive_Loss : 0.17409, Sensitive_Acc : 22.000, Run Time : 7.63 sec
INFO:root:2024-04-11 12:46:13, Train, Epoch : 3, Step : 1370, Loss : 0.45919, Acc : 0.769, Sensitive_Loss : 0.20159, Sensitive_Acc : 22.700, Run Time : 8.15 sec
INFO:root:2024-04-11 12:46:23, Train, Epoch : 3, Step : 1380, Loss : 0.47042, Acc : 0.756, Sensitive_Loss : 0.20062, Sensitive_Acc : 18.100, Run Time : 9.63 sec
INFO:root:2024-04-11 12:46:30, Train, Epoch : 3, Step : 1390, Loss : 0.47883, Acc : 0.781, Sensitive_Loss : 0.18538, Sensitive_Acc : 21.400, Run Time : 7.06 sec
INFO:root:2024-04-11 12:46:38, Train, Epoch : 3, Step : 1400, Loss : 0.44549, Acc : 0.787, Sensitive_Loss : 0.20447, Sensitive_Acc : 23.100, Run Time : 8.14 sec
INFO:root:2024-04-11 12:48:14, Dev, Step : 1400, Loss : 0.52488, Acc : 0.762, Auc : 0.841, Sensitive_Loss : 0.20479, Sensitive_Acc : 21.647, Sensitive_Auc : 0.994, Mean auc: 0.841, Run Time : 96.20 sec
INFO:root:2024-04-11 12:48:15, Best, Step : 1400, Loss : 0.52488, Acc : 0.762, Auc : 0.841, Sensitive_Loss : 0.20479, Sensitive_Acc : 21.647, Sensitive_Auc : 0.994, Best Auc : 0.841
INFO:root:2024-04-11 12:48:21, Train, Epoch : 3, Step : 1410, Loss : 0.53309, Acc : 0.731, Sensitive_Loss : 0.13470, Sensitive_Acc : 19.300, Run Time : 102.68 sec
INFO:root:2024-04-11 12:48:32, Train, Epoch : 3, Step : 1420, Loss : 0.48713, Acc : 0.778, Sensitive_Loss : 0.24395, Sensitive_Acc : 19.700, Run Time : 11.28 sec
INFO:root:2024-04-11 12:48:40, Train, Epoch : 3, Step : 1430, Loss : 0.43759, Acc : 0.772, Sensitive_Loss : 0.23318, Sensitive_Acc : 26.000, Run Time : 7.79 sec
INFO:root:2024-04-11 12:48:47, Train, Epoch : 3, Step : 1440, Loss : 0.42434, Acc : 0.816, Sensitive_Loss : 0.22974, Sensitive_Acc : 22.300, Run Time : 7.09 sec
INFO:root:2024-04-11 12:48:56, Train, Epoch : 3, Step : 1450, Loss : 0.53386, Acc : 0.753, Sensitive_Loss : 0.24279, Sensitive_Acc : 20.100, Run Time : 9.75 sec
INFO:root:2024-04-11 12:49:04, Train, Epoch : 3, Step : 1460, Loss : 0.50475, Acc : 0.759, Sensitive_Loss : 0.19472, Sensitive_Acc : 25.300, Run Time : 7.81 sec
INFO:root:2024-04-11 12:49:11, Train, Epoch : 3, Step : 1470, Loss : 0.48760, Acc : 0.769, Sensitive_Loss : 0.19668, Sensitive_Acc : 23.700, Run Time : 7.06 sec
INFO:root:2024-04-11 12:49:19, Train, Epoch : 3, Step : 1480, Loss : 0.48145, Acc : 0.797, Sensitive_Loss : 0.18142, Sensitive_Acc : 23.900, Run Time : 7.40 sec
INFO:root:2024-04-11 12:49:29, Train, Epoch : 3, Step : 1490, Loss : 0.41258, Acc : 0.816, Sensitive_Loss : 0.19594, Sensitive_Acc : 21.700, Run Time : 10.31 sec
INFO:root:2024-04-11 12:49:36, Train, Epoch : 3, Step : 1500, Loss : 0.48618, Acc : 0.797, Sensitive_Loss : 0.19905, Sensitive_Acc : 24.000, Run Time : 7.36 sec
INFO:root:2024-04-11 12:51:18, Dev, Step : 1500, Loss : 0.51732, Acc : 0.769, Auc : 0.845, Sensitive_Loss : 0.21817, Sensitive_Acc : 20.714, Sensitive_Auc : 0.995, Mean auc: 0.845, Run Time : 101.83 sec
INFO:root:2024-04-11 12:51:19, Best, Step : 1500, Loss : 0.51732, Acc : 0.769, Auc : 0.845, Sensitive_Loss : 0.21817, Sensitive_Acc : 20.714, Sensitive_Auc : 0.995, Best Auc : 0.845
INFO:root:2024-04-11 12:51:29, Train, Epoch : 3, Step : 1510, Loss : 0.53619, Acc : 0.753, Sensitive_Loss : 0.17474, Sensitive_Acc : 27.000, Run Time : 112.17 sec
INFO:root:2024-04-11 12:51:38, Train, Epoch : 3, Step : 1520, Loss : 0.46676, Acc : 0.806, Sensitive_Loss : 0.15037, Sensitive_Acc : 21.200, Run Time : 9.11 sec
INFO:root:2024-04-11 12:51:46, Train, Epoch : 3, Step : 1530, Loss : 0.43229, Acc : 0.787, Sensitive_Loss : 0.21486, Sensitive_Acc : 18.400, Run Time : 7.97 sec
INFO:root:2024-04-11 12:51:56, Train, Epoch : 3, Step : 1540, Loss : 0.50507, Acc : 0.769, Sensitive_Loss : 0.23867, Sensitive_Acc : 22.600, Run Time : 9.95 sec
INFO:root:2024-04-11 12:52:05, Train, Epoch : 3, Step : 1550, Loss : 0.41041, Acc : 0.819, Sensitive_Loss : 0.15669, Sensitive_Acc : 23.300, Run Time : 9.26 sec
INFO:root:2024-04-11 12:52:13, Train, Epoch : 3, Step : 1560, Loss : 0.45576, Acc : 0.828, Sensitive_Loss : 0.14237, Sensitive_Acc : 20.800, Run Time : 8.29 sec
INFO:root:2024-04-11 12:52:22, Train, Epoch : 3, Step : 1570, Loss : 0.48695, Acc : 0.784, Sensitive_Loss : 0.20835, Sensitive_Acc : 22.000, Run Time : 8.55 sec
INFO:root:2024-04-11 12:52:34, Train, Epoch : 3, Step : 1580, Loss : 0.45177, Acc : 0.800, Sensitive_Loss : 0.16851, Sensitive_Acc : 23.100, Run Time : 12.36 sec
INFO:root:2024-04-11 12:52:43, Train, Epoch : 3, Step : 1590, Loss : 0.55267, Acc : 0.769, Sensitive_Loss : 0.12457, Sensitive_Acc : 22.800, Run Time : 8.72 sec
INFO:root:2024-04-11 12:52:50, Train, Epoch : 3, Step : 1600, Loss : 0.48054, Acc : 0.794, Sensitive_Loss : 0.17893, Sensitive_Acc : 19.000, Run Time : 7.66 sec
INFO:root:2024-04-11 12:54:38, Dev, Step : 1600, Loss : 0.51511, Acc : 0.768, Auc : 0.847, Sensitive_Loss : 0.19683, Sensitive_Acc : 21.436, Sensitive_Auc : 0.997, Mean auc: 0.847, Run Time : 107.71 sec
INFO:root:2024-04-11 12:54:39, Best, Step : 1600, Loss : 0.51511, Acc : 0.768, Auc : 0.847, Sensitive_Loss : 0.19683, Sensitive_Acc : 21.436, Sensitive_Auc : 0.997, Best Auc : 0.847
INFO:root:2024-04-11 12:54:48, Train, Epoch : 3, Step : 1610, Loss : 0.49842, Acc : 0.772, Sensitive_Loss : 0.12329, Sensitive_Acc : 23.800, Run Time : 117.87 sec
INFO:root:2024-04-11 12:54:56, Train, Epoch : 3, Step : 1620, Loss : 0.43242, Acc : 0.784, Sensitive_Loss : 0.18471, Sensitive_Acc : 24.000, Run Time : 7.51 sec
INFO:root:2024-04-11 12:55:04, Train, Epoch : 3, Step : 1630, Loss : 0.50295, Acc : 0.791, Sensitive_Loss : 0.18700, Sensitive_Acc : 17.400, Run Time : 8.09 sec
INFO:root:2024-04-11 12:55:11, Train, Epoch : 3, Step : 1640, Loss : 0.40722, Acc : 0.800, Sensitive_Loss : 0.18833, Sensitive_Acc : 20.400, Run Time : 7.50 sec
INFO:root:2024-04-11 12:55:22, Train, Epoch : 3, Step : 1650, Loss : 0.43513, Acc : 0.797, Sensitive_Loss : 0.16757, Sensitive_Acc : 19.800, Run Time : 10.42 sec
INFO:root:2024-04-11 12:55:30, Train, Epoch : 3, Step : 1660, Loss : 0.44977, Acc : 0.803, Sensitive_Loss : 0.16207, Sensitive_Acc : 20.200, Run Time : 8.31 sec
INFO:root:2024-04-11 12:55:38, Train, Epoch : 3, Step : 1670, Loss : 0.49422, Acc : 0.750, Sensitive_Loss : 0.13616, Sensitive_Acc : 21.300, Run Time : 7.39 sec
INFO:root:2024-04-11 12:55:45, Train, Epoch : 3, Step : 1680, Loss : 0.47051, Acc : 0.794, Sensitive_Loss : 0.17282, Sensitive_Acc : 21.700, Run Time : 7.85 sec
INFO:root:2024-04-11 12:55:58, Train, Epoch : 3, Step : 1690, Loss : 0.52199, Acc : 0.784, Sensitive_Loss : 0.17076, Sensitive_Acc : 21.200, Run Time : 12.48 sec
INFO:root:2024-04-11 12:56:05, Train, Epoch : 3, Step : 1700, Loss : 0.38814, Acc : 0.822, Sensitive_Loss : 0.17238, Sensitive_Acc : 18.200, Run Time : 7.49 sec
INFO:root:2024-04-11 12:57:52, Dev, Step : 1700, Loss : 0.52442, Acc : 0.767, Auc : 0.846, Sensitive_Loss : 0.20604, Sensitive_Acc : 21.105, Sensitive_Auc : 0.997, Mean auc: 0.846, Run Time : 106.31 sec
INFO:root:2024-04-11 12:57:58, Train, Epoch : 3, Step : 1710, Loss : 0.47343, Acc : 0.803, Sensitive_Loss : 0.20316, Sensitive_Acc : 24.200, Run Time : 112.45 sec
INFO:root:2024-04-11 12:58:05, Train, Epoch : 3, Step : 1720, Loss : 0.42904, Acc : 0.800, Sensitive_Loss : 0.14548, Sensitive_Acc : 22.100, Run Time : 7.51 sec
INFO:root:2024-04-11 12:58:13, Train, Epoch : 3, Step : 1730, Loss : 0.50573, Acc : 0.772, Sensitive_Loss : 0.17430, Sensitive_Acc : 18.600, Run Time : 7.70 sec
INFO:root:2024-04-11 12:58:24, Train, Epoch : 3, Step : 1740, Loss : 0.46749, Acc : 0.806, Sensitive_Loss : 0.20787, Sensitive_Acc : 16.100, Run Time : 11.01 sec
INFO:root:2024-04-11 12:58:32, Train, Epoch : 3, Step : 1750, Loss : 0.46445, Acc : 0.787, Sensitive_Loss : 0.17970, Sensitive_Acc : 25.200, Run Time : 7.68 sec
INFO:root:2024-04-11 12:58:39, Train, Epoch : 3, Step : 1760, Loss : 0.47943, Acc : 0.775, Sensitive_Loss : 0.16048, Sensitive_Acc : 22.400, Run Time : 7.75 sec
INFO:root:2024-04-11 12:58:47, Train, Epoch : 3, Step : 1770, Loss : 0.40031, Acc : 0.825, Sensitive_Loss : 0.17955, Sensitive_Acc : 19.800, Run Time : 7.60 sec
INFO:root:2024-04-11 12:59:00, Train, Epoch : 3, Step : 1780, Loss : 0.38679, Acc : 0.809, Sensitive_Loss : 0.18997, Sensitive_Acc : 23.500, Run Time : 13.33 sec
INFO:root:2024-04-11 12:59:08, Train, Epoch : 3, Step : 1790, Loss : 0.46291, Acc : 0.797, Sensitive_Loss : 0.17644, Sensitive_Acc : 23.300, Run Time : 7.54 sec
INFO:root:2024-04-11 12:59:16, Train, Epoch : 3, Step : 1800, Loss : 0.45608, Acc : 0.794, Sensitive_Loss : 0.14769, Sensitive_Acc : 23.200, Run Time : 7.82 sec
INFO:root:2024-04-11 13:01:03, Dev, Step : 1800, Loss : 0.51597, Acc : 0.777, Auc : 0.849, Sensitive_Loss : 0.20000, Sensitive_Acc : 21.301, Sensitive_Auc : 0.996, Mean auc: 0.849, Run Time : 106.84 sec
INFO:root:2024-04-11 13:01:03, Best, Step : 1800, Loss : 0.51597, Acc : 0.777, Auc : 0.849, Sensitive_Loss : 0.20000, Sensitive_Acc : 21.301, Sensitive_Auc : 0.996, Best Auc : 0.849
INFO:root:2024-04-11 13:01:09, Train, Epoch : 3, Step : 1810, Loss : 0.44180, Acc : 0.806, Sensitive_Loss : 0.17332, Sensitive_Acc : 20.600, Run Time : 112.93 sec
INFO:root:2024-04-11 13:01:16, Train, Epoch : 3, Step : 1820, Loss : 0.45484, Acc : 0.784, Sensitive_Loss : 0.12794, Sensitive_Acc : 19.200, Run Time : 7.66 sec
INFO:root:2024-04-11 13:01:28, Train, Epoch : 3, Step : 1830, Loss : 0.45208, Acc : 0.787, Sensitive_Loss : 0.16804, Sensitive_Acc : 20.500, Run Time : 11.57 sec
INFO:root:2024-04-11 13:01:35, Train, Epoch : 3, Step : 1840, Loss : 0.44354, Acc : 0.809, Sensitive_Loss : 0.19899, Sensitive_Acc : 19.800, Run Time : 6.88 sec
INFO:root:2024-04-11 13:01:42, Train, Epoch : 3, Step : 1850, Loss : 0.40120, Acc : 0.809, Sensitive_Loss : 0.15579, Sensitive_Acc : 25.200, Run Time : 7.50 sec
INFO:root:2024-04-11 13:01:50, Train, Epoch : 3, Step : 1860, Loss : 0.43536, Acc : 0.806, Sensitive_Loss : 0.14665, Sensitive_Acc : 20.900, Run Time : 7.60 sec
INFO:root:2024-04-11 13:02:03, Train, Epoch : 3, Step : 1870, Loss : 0.48072, Acc : 0.766, Sensitive_Loss : 0.19025, Sensitive_Acc : 20.500, Run Time : 13.03 sec
INFO:root:2024-04-11 13:02:10, Train, Epoch : 3, Step : 1880, Loss : 0.48501, Acc : 0.800, Sensitive_Loss : 0.18090, Sensitive_Acc : 18.800, Run Time : 7.09 sec
INFO:root:2024-04-11 13:02:18, Train, Epoch : 3, Step : 1890, Loss : 0.50787, Acc : 0.756, Sensitive_Loss : 0.17550, Sensitive_Acc : 24.500, Run Time : 7.68 sec
INFO:root:2024-04-11 13:02:29, Train, Epoch : 3, Step : 1900, Loss : 0.46583, Acc : 0.831, Sensitive_Loss : 0.12869, Sensitive_Acc : 24.100, Run Time : 11.11 sec
INFO:root:2024-04-11 13:04:11, Dev, Step : 1900, Loss : 0.51062, Acc : 0.773, Auc : 0.851, Sensitive_Loss : 0.20818, Sensitive_Acc : 21.180, Sensitive_Auc : 0.997, Mean auc: 0.851, Run Time : 102.66 sec
INFO:root:2024-04-11 13:04:12, Best, Step : 1900, Loss : 0.51062, Acc : 0.773, Auc : 0.851, Sensitive_Loss : 0.20818, Sensitive_Acc : 21.180, Sensitive_Auc : 0.997, Best Auc : 0.851
INFO:root:2024-04-11 13:05:55
INFO:root:y_pred: [0.24436966 0.09244084 0.0303535  ... 0.25062206 0.03445918 0.07316724]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [9.00972882e-05 4.27665887e-03 1.41694769e-03 1.05629288e-01
 1.47498280e-01 2.95970040e-05 3.89094763e-02 1.27754553e-04
 2.30644215e-02 9.95374620e-01 2.25445747e-01 1.03967664e-04
 4.78337915e-06 5.35697602e-08 9.95060980e-01 5.12038171e-02
 6.13221293e-03 9.96800065e-01 9.89820480e-01 1.12132162e-04
 7.84611344e-01 2.92590048e-05 6.70796353e-03 2.20364574e-02
 1.97877251e-02 7.06383735e-02 2.60113284e-05 7.24019716e-03
 1.13986871e-05 1.71702299e-02 9.64698792e-02 9.35739577e-01
 2.80846696e-04 8.98171663e-01 5.69473286e-07 2.30752848e-05
 1.76273352e-05 2.99454600e-01 3.72937351e-01 5.10418117e-02
 1.28309103e-02 8.92430365e-01 5.99745736e-02 3.49739566e-04
 9.58720744e-01 4.57700580e-01 5.96508244e-03 4.01123971e-01
 8.17862824e-02 9.89629209e-01 9.77639437e-01 9.95228648e-01
 7.49593258e-01 6.52532140e-03 1.01607442e-02 2.65989214e-01
 1.09330733e-06 2.41445309e-06 9.94660437e-01 4.23119776e-03
 2.49387493e-04 9.34673771e-02 7.79674668e-03 2.15275600e-06
 9.25923228e-01 3.17664206e-01 2.69485781e-06 6.68340921e-01
 8.09922174e-04 9.76430118e-01 9.96944010e-01 9.97135162e-01
 6.11930882e-05 7.09321678e-01 8.43712551e-05 4.25053865e-01
 1.55400962e-03 9.07244484e-08 7.16603381e-08 7.28102541e-03
 3.89734679e-03 2.98528292e-04 9.70070601e-01 9.53457415e-01
 1.71088740e-01 2.16286294e-02 2.55426421e-04 1.10874651e-03
 6.18451100e-04 4.06369125e-08 7.00965393e-05 6.42538786e-01
 1.28075285e-07 1.15158272e-09 2.86252704e-03 2.65074894e-03
 5.02315379e-06 8.96081150e-01 8.14623854e-06 2.50165904e-05
 1.54706777e-05 1.37824833e-01 5.21283865e-01 2.50807261e-07
 2.00122520e-02 2.33475585e-02 1.01360682e-04 9.06281650e-01
 1.39807137e-02 2.32282847e-01 1.53034937e-03 9.98377800e-01
 9.90511358e-01 1.07549624e-06 5.08138657e-01 2.52265576e-02
 4.75507945e-01 7.36792572e-06 2.24368135e-03 6.67815749e-03
 1.13290429e-04 1.34797716e-07 2.00996903e-04 2.03522177e-09
 6.21116662e-04 8.19182098e-01 3.26812071e-08 9.93839204e-01
 7.48291519e-03 3.94809633e-01 3.55169405e-09 3.74537613e-03
 1.36483118e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 13:05:55, Dev, Step : 1902, Loss : 0.51484, Acc : 0.774, Auc : 0.851, Sensitive_Loss : 0.20581, Sensitive_Acc : 21.180, Sensitive_Auc : 0.997, Mean auc: 0.851, Run Time : 101.65 sec
INFO:root:2024-04-11 13:06:03, Train, Epoch : 4, Step : 1910, Loss : 0.38901, Acc : 0.631, Sensitive_Loss : 0.12392, Sensitive_Acc : 15.000, Run Time : 6.82 sec
INFO:root:2024-04-11 13:06:11, Train, Epoch : 4, Step : 1920, Loss : 0.48257, Acc : 0.812, Sensitive_Loss : 0.14096, Sensitive_Acc : 23.000, Run Time : 7.60 sec
INFO:root:2024-04-11 13:06:18, Train, Epoch : 4, Step : 1930, Loss : 0.48698, Acc : 0.766, Sensitive_Loss : 0.15730, Sensitive_Acc : 19.200, Run Time : 7.55 sec
INFO:root:2024-04-11 13:06:29, Train, Epoch : 4, Step : 1940, Loss : 0.52820, Acc : 0.772, Sensitive_Loss : 0.13868, Sensitive_Acc : 24.200, Run Time : 11.14 sec
INFO:root:2024-04-11 13:06:37, Train, Epoch : 4, Step : 1950, Loss : 0.38625, Acc : 0.834, Sensitive_Loss : 0.17969, Sensitive_Acc : 22.300, Run Time : 7.37 sec
INFO:root:2024-04-11 13:06:44, Train, Epoch : 4, Step : 1960, Loss : 0.43257, Acc : 0.769, Sensitive_Loss : 0.23513, Sensitive_Acc : 15.600, Run Time : 6.99 sec
INFO:root:2024-04-11 13:06:52, Train, Epoch : 4, Step : 1970, Loss : 0.42524, Acc : 0.794, Sensitive_Loss : 0.19394, Sensitive_Acc : 19.200, Run Time : 8.21 sec
INFO:root:2024-04-11 13:07:01, Train, Epoch : 4, Step : 1980, Loss : 0.43964, Acc : 0.825, Sensitive_Loss : 0.17321, Sensitive_Acc : 20.000, Run Time : 9.19 sec
INFO:root:2024-04-11 13:07:08, Train, Epoch : 4, Step : 1990, Loss : 0.35207, Acc : 0.856, Sensitive_Loss : 0.14910, Sensitive_Acc : 21.000, Run Time : 7.00 sec
INFO:root:2024-04-11 13:07:15, Train, Epoch : 4, Step : 2000, Loss : 0.47514, Acc : 0.791, Sensitive_Loss : 0.14538, Sensitive_Acc : 22.600, Run Time : 7.29 sec
INFO:root:2024-04-11 13:09:07, Dev, Step : 2000, Loss : 0.51580, Acc : 0.772, Auc : 0.850, Sensitive_Loss : 0.22604, Sensitive_Acc : 20.414, Sensitive_Auc : 0.997, Mean auc: 0.850, Run Time : 111.68 sec
INFO:root:2024-04-11 13:09:13, Train, Epoch : 4, Step : 2010, Loss : 0.47572, Acc : 0.747, Sensitive_Loss : 0.14554, Sensitive_Acc : 19.300, Run Time : 117.17 sec
INFO:root:2024-04-11 13:09:20, Train, Epoch : 4, Step : 2020, Loss : 0.39092, Acc : 0.834, Sensitive_Loss : 0.13572, Sensitive_Acc : 23.100, Run Time : 7.88 sec
INFO:root:2024-04-11 13:09:30, Train, Epoch : 4, Step : 2030, Loss : 0.39142, Acc : 0.838, Sensitive_Loss : 0.14217, Sensitive_Acc : 20.100, Run Time : 9.43 sec
INFO:root:2024-04-11 13:09:37, Train, Epoch : 4, Step : 2040, Loss : 0.44675, Acc : 0.775, Sensitive_Loss : 0.20021, Sensitive_Acc : 23.700, Run Time : 7.42 sec
INFO:root:2024-04-11 13:09:46, Train, Epoch : 4, Step : 2050, Loss : 0.39670, Acc : 0.828, Sensitive_Loss : 0.13759, Sensitive_Acc : 25.100, Run Time : 8.23 sec
INFO:root:2024-04-11 13:09:55, Train, Epoch : 4, Step : 2060, Loss : 0.47791, Acc : 0.784, Sensitive_Loss : 0.22888, Sensitive_Acc : 21.700, Run Time : 9.15 sec
INFO:root:2024-04-11 13:10:03, Train, Epoch : 4, Step : 2070, Loss : 0.43541, Acc : 0.800, Sensitive_Loss : 0.11962, Sensitive_Acc : 21.800, Run Time : 8.83 sec
INFO:root:2024-04-11 13:10:11, Train, Epoch : 4, Step : 2080, Loss : 0.47978, Acc : 0.803, Sensitive_Loss : 0.19683, Sensitive_Acc : 23.200, Run Time : 7.69 sec
INFO:root:2024-04-11 13:10:19, Train, Epoch : 4, Step : 2090, Loss : 0.40178, Acc : 0.825, Sensitive_Loss : 0.19851, Sensitive_Acc : 22.300, Run Time : 8.06 sec
INFO:root:2024-04-11 13:10:28, Train, Epoch : 4, Step : 2100, Loss : 0.42063, Acc : 0.797, Sensitive_Loss : 0.21596, Sensitive_Acc : 22.700, Run Time : 8.60 sec
INFO:root:2024-04-11 13:12:09, Dev, Step : 2100, Loss : 0.51588, Acc : 0.776, Auc : 0.853, Sensitive_Loss : 0.21709, Sensitive_Acc : 21.030, Sensitive_Auc : 0.997, Mean auc: 0.853, Run Time : 101.41 sec
INFO:root:2024-04-11 13:12:10, Best, Step : 2100, Loss : 0.51588, Acc : 0.776, Auc : 0.853, Sensitive_Loss : 0.21709, Sensitive_Acc : 21.030, Sensitive_Auc : 0.997, Best Auc : 0.853
INFO:root:2024-04-11 13:12:16, Train, Epoch : 4, Step : 2110, Loss : 0.44499, Acc : 0.794, Sensitive_Loss : 0.18474, Sensitive_Acc : 16.300, Run Time : 107.88 sec
INFO:root:2024-04-11 13:12:26, Train, Epoch : 4, Step : 2120, Loss : 0.51161, Acc : 0.791, Sensitive_Loss : 0.16544, Sensitive_Acc : 26.100, Run Time : 10.36 sec
INFO:root:2024-04-11 13:12:34, Train, Epoch : 4, Step : 2130, Loss : 0.44332, Acc : 0.800, Sensitive_Loss : 0.14840, Sensitive_Acc : 20.700, Run Time : 8.06 sec
INFO:root:2024-04-11 13:12:42, Train, Epoch : 4, Step : 2140, Loss : 0.50450, Acc : 0.756, Sensitive_Loss : 0.18394, Sensitive_Acc : 20.300, Run Time : 7.55 sec
INFO:root:2024-04-11 13:12:52, Train, Epoch : 4, Step : 2150, Loss : 0.45742, Acc : 0.812, Sensitive_Loss : 0.18783, Sensitive_Acc : 23.100, Run Time : 10.49 sec
INFO:root:2024-04-11 13:13:01, Train, Epoch : 4, Step : 2160, Loss : 0.39825, Acc : 0.800, Sensitive_Loss : 0.12049, Sensitive_Acc : 20.000, Run Time : 9.20 sec
INFO:root:2024-04-11 13:13:09, Train, Epoch : 4, Step : 2170, Loss : 0.42889, Acc : 0.819, Sensitive_Loss : 0.20283, Sensitive_Acc : 22.400, Run Time : 7.91 sec
INFO:root:2024-04-11 13:13:17, Train, Epoch : 4, Step : 2180, Loss : 0.42521, Acc : 0.822, Sensitive_Loss : 0.17680, Sensitive_Acc : 22.300, Run Time : 7.67 sec
INFO:root:2024-04-11 13:13:26, Train, Epoch : 4, Step : 2190, Loss : 0.43565, Acc : 0.803, Sensitive_Loss : 0.13162, Sensitive_Acc : 23.600, Run Time : 9.47 sec
INFO:root:2024-04-11 13:13:35, Train, Epoch : 4, Step : 2200, Loss : 0.45619, Acc : 0.822, Sensitive_Loss : 0.12657, Sensitive_Acc : 20.700, Run Time : 8.69 sec
INFO:root:2024-04-11 13:15:16, Dev, Step : 2200, Loss : 0.50814, Acc : 0.773, Auc : 0.852, Sensitive_Loss : 0.21511, Sensitive_Acc : 21.150, Sensitive_Auc : 0.997, Mean auc: 0.852, Run Time : 101.05 sec
INFO:root:2024-04-11 13:15:26, Train, Epoch : 4, Step : 2210, Loss : 0.50592, Acc : 0.759, Sensitive_Loss : 0.18832, Sensitive_Acc : 22.000, Run Time : 110.76 sec
INFO:root:2024-04-11 13:15:36, Train, Epoch : 4, Step : 2220, Loss : 0.50507, Acc : 0.775, Sensitive_Loss : 0.17040, Sensitive_Acc : 21.000, Run Time : 10.37 sec
INFO:root:2024-04-11 13:15:45, Train, Epoch : 4, Step : 2230, Loss : 0.46010, Acc : 0.791, Sensitive_Loss : 0.12742, Sensitive_Acc : 18.300, Run Time : 8.99 sec
INFO:root:2024-04-11 13:15:57, Train, Epoch : 4, Step : 2240, Loss : 0.44582, Acc : 0.809, Sensitive_Loss : 0.12038, Sensitive_Acc : 24.500, Run Time : 11.32 sec
INFO:root:2024-04-11 13:16:06, Train, Epoch : 4, Step : 2250, Loss : 0.46377, Acc : 0.766, Sensitive_Loss : 0.25037, Sensitive_Acc : 22.700, Run Time : 9.54 sec
INFO:root:2024-04-11 13:16:15, Train, Epoch : 4, Step : 2260, Loss : 0.42624, Acc : 0.816, Sensitive_Loss : 0.14792, Sensitive_Acc : 22.600, Run Time : 8.60 sec
INFO:root:2024-04-11 13:16:26, Train, Epoch : 4, Step : 2270, Loss : 0.46820, Acc : 0.784, Sensitive_Loss : 0.19564, Sensitive_Acc : 15.700, Run Time : 10.90 sec
INFO:root:2024-04-11 13:16:34, Train, Epoch : 4, Step : 2280, Loss : 0.43850, Acc : 0.787, Sensitive_Loss : 0.17284, Sensitive_Acc : 22.800, Run Time : 8.45 sec
INFO:root:2024-04-11 13:16:42, Train, Epoch : 4, Step : 2290, Loss : 0.47945, Acc : 0.759, Sensitive_Loss : 0.21634, Sensitive_Acc : 23.400, Run Time : 8.32 sec
INFO:root:2024-04-11 13:16:53, Train, Epoch : 4, Step : 2300, Loss : 0.45801, Acc : 0.787, Sensitive_Loss : 0.15683, Sensitive_Acc : 25.500, Run Time : 10.80 sec
INFO:root:2024-04-11 13:18:35, Dev, Step : 2300, Loss : 0.51315, Acc : 0.768, Auc : 0.850, Sensitive_Loss : 0.19351, Sensitive_Acc : 21.406, Sensitive_Auc : 0.997, Mean auc: 0.850, Run Time : 102.11 sec
INFO:root:2024-04-11 13:18:41, Train, Epoch : 4, Step : 2310, Loss : 0.43671, Acc : 0.775, Sensitive_Loss : 0.13788, Sensitive_Acc : 23.300, Run Time : 107.70 sec
INFO:root:2024-04-11 13:18:51, Train, Epoch : 4, Step : 2320, Loss : 0.46865, Acc : 0.816, Sensitive_Loss : 0.14517, Sensitive_Acc : 17.700, Run Time : 10.29 sec
INFO:root:2024-04-11 13:18:59, Train, Epoch : 4, Step : 2330, Loss : 0.49038, Acc : 0.775, Sensitive_Loss : 0.13538, Sensitive_Acc : 18.100, Run Time : 7.59 sec
INFO:root:2024-04-11 13:19:07, Train, Epoch : 4, Step : 2340, Loss : 0.47671, Acc : 0.797, Sensitive_Loss : 0.15674, Sensitive_Acc : 24.100, Run Time : 7.69 sec
INFO:root:2024-04-11 13:19:14, Train, Epoch : 4, Step : 2350, Loss : 0.49970, Acc : 0.800, Sensitive_Loss : 0.22917, Sensitive_Acc : 22.800, Run Time : 7.57 sec
INFO:root:2024-04-11 13:19:25, Train, Epoch : 4, Step : 2360, Loss : 0.40333, Acc : 0.838, Sensitive_Loss : 0.13996, Sensitive_Acc : 26.900, Run Time : 10.44 sec
INFO:root:2024-04-11 13:19:33, Train, Epoch : 4, Step : 2370, Loss : 0.41528, Acc : 0.778, Sensitive_Loss : 0.15380, Sensitive_Acc : 22.600, Run Time : 8.45 sec
INFO:root:2024-04-11 13:19:41, Train, Epoch : 4, Step : 2380, Loss : 0.42902, Acc : 0.806, Sensitive_Loss : 0.17714, Sensitive_Acc : 21.300, Run Time : 7.66 sec
INFO:root:2024-04-11 13:19:51, Train, Epoch : 4, Step : 2390, Loss : 0.42510, Acc : 0.809, Sensitive_Loss : 0.17054, Sensitive_Acc : 22.600, Run Time : 10.56 sec
INFO:root:2024-04-11 13:19:59, Train, Epoch : 4, Step : 2400, Loss : 0.40601, Acc : 0.806, Sensitive_Loss : 0.17105, Sensitive_Acc : 15.200, Run Time : 7.74 sec
INFO:root:2024-04-11 13:21:41, Dev, Step : 2400, Loss : 0.51019, Acc : 0.766, Auc : 0.851, Sensitive_Loss : 0.21774, Sensitive_Acc : 21.120, Sensitive_Auc : 0.996, Mean auc: 0.851, Run Time : 101.90 sec
INFO:root:2024-04-11 13:21:47, Train, Epoch : 4, Step : 2410, Loss : 0.44181, Acc : 0.791, Sensitive_Loss : 0.13282, Sensitive_Acc : 21.700, Run Time : 108.19 sec
INFO:root:2024-04-11 13:21:56, Train, Epoch : 4, Step : 2420, Loss : 0.35720, Acc : 0.781, Sensitive_Loss : 0.16360, Sensitive_Acc : 21.800, Run Time : 9.07 sec
INFO:root:2024-04-11 13:22:04, Train, Epoch : 4, Step : 2430, Loss : 0.45654, Acc : 0.803, Sensitive_Loss : 0.12013, Sensitive_Acc : 25.500, Run Time : 7.55 sec
INFO:root:2024-04-11 13:22:11, Train, Epoch : 4, Step : 2440, Loss : 0.46860, Acc : 0.822, Sensitive_Loss : 0.14959, Sensitive_Acc : 21.800, Run Time : 7.37 sec
INFO:root:2024-04-11 13:22:23, Train, Epoch : 4, Step : 2450, Loss : 0.45490, Acc : 0.803, Sensitive_Loss : 0.13679, Sensitive_Acc : 15.700, Run Time : 11.29 sec
INFO:root:2024-04-11 13:22:34, Train, Epoch : 4, Step : 2460, Loss : 0.44074, Acc : 0.797, Sensitive_Loss : 0.17033, Sensitive_Acc : 24.800, Run Time : 11.18 sec
INFO:root:2024-04-11 13:22:42, Train, Epoch : 4, Step : 2470, Loss : 0.43779, Acc : 0.791, Sensitive_Loss : 0.16783, Sensitive_Acc : 23.400, Run Time : 7.94 sec
INFO:root:2024-04-11 13:22:51, Train, Epoch : 4, Step : 2480, Loss : 0.44087, Acc : 0.803, Sensitive_Loss : 0.14124, Sensitive_Acc : 23.300, Run Time : 9.41 sec
INFO:root:2024-04-11 13:22:59, Train, Epoch : 4, Step : 2490, Loss : 0.48736, Acc : 0.775, Sensitive_Loss : 0.15176, Sensitive_Acc : 19.000, Run Time : 8.05 sec
INFO:root:2024-04-11 13:23:07, Train, Epoch : 4, Step : 2500, Loss : 0.39586, Acc : 0.803, Sensitive_Loss : 0.18523, Sensitive_Acc : 22.200, Run Time : 7.98 sec
INFO:root:2024-04-11 13:24:54, Dev, Step : 2500, Loss : 0.51499, Acc : 0.777, Auc : 0.853, Sensitive_Loss : 0.21790, Sensitive_Acc : 21.211, Sensitive_Auc : 0.996, Mean auc: 0.853, Run Time : 106.64 sec
INFO:root:2024-04-11 13:25:01, Train, Epoch : 4, Step : 2510, Loss : 0.39631, Acc : 0.834, Sensitive_Loss : 0.20221, Sensitive_Acc : 20.000, Run Time : 113.70 sec
INFO:root:2024-04-11 13:25:09, Train, Epoch : 4, Step : 2520, Loss : 0.46042, Acc : 0.800, Sensitive_Loss : 0.15257, Sensitive_Acc : 19.900, Run Time : 7.87 sec
INFO:root:2024-04-11 13:25:16, Train, Epoch : 4, Step : 2530, Loss : 0.49299, Acc : 0.794, Sensitive_Loss : 0.20909, Sensitive_Acc : 23.200, Run Time : 7.68 sec
INFO:root:2024-04-11 13:27:05
INFO:root:y_pred: [0.09741851 0.05825509 0.02593239 ... 0.11569986 0.0531203  0.02864246]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [4.09759814e-04 8.24372109e-04 1.34019216e-03 7.55260512e-02
 7.34665245e-02 4.61741320e-06 4.00235914e-02 5.16674714e-04
 7.20261857e-02 9.97456968e-01 3.16865444e-01 9.00248051e-05
 8.41380097e-05 2.18458798e-08 9.95902121e-01 2.84726266e-02
 5.45115501e-04 9.96757805e-01 9.89552319e-01 5.14237036e-04
 8.01454306e-01 2.74292834e-05 5.31667694e-02 2.01223791e-02
 9.25253406e-02 1.93747893e-01 7.55410201e-06 2.68417993e-03
 2.69944644e-06 1.17341258e-01 3.86772752e-02 9.35815930e-01
 3.96522926e-03 8.86357784e-01 6.08606229e-07 4.30039014e-04
 9.14178599e-05 1.86044142e-01 2.85913169e-01 6.58633858e-02
 7.36516714e-03 8.86573613e-01 8.40651244e-03 1.25183596e-03
 9.78812575e-01 3.84484440e-01 3.40364757e-03 4.04300660e-01
 1.26908913e-01 9.92105126e-01 9.84441578e-01 9.95471597e-01
 8.91755044e-01 9.35229436e-02 1.79722518e-01 2.80261070e-01
 1.11304898e-05 8.82965651e-07 9.91971254e-01 4.51336009e-03
 6.43411768e-04 7.67023712e-02 2.36494150e-02 4.11463843e-05
 9.52938378e-01 6.10611320e-01 1.23972532e-05 6.19541407e-01
 5.31125022e-03 9.81942117e-01 9.96914029e-01 9.98150110e-01
 2.52436603e-05 7.21388638e-01 2.64760864e-04 2.77893782e-01
 5.60318818e-03 4.92395280e-09 2.07482162e-06 3.53600061e-03
 4.26900433e-03 1.74831162e-04 9.51773703e-01 9.79866982e-01
 1.20558776e-01 3.50287110e-02 3.61728802e-04 1.60590548e-03
 3.97826545e-02 9.42050576e-07 1.88697486e-05 5.32530904e-01
 3.43143910e-07 2.33036457e-09 8.35660379e-03 1.87772736e-02
 2.32616061e-04 8.05679202e-01 7.25366990e-05 7.86473829e-05
 1.58830744e-03 1.31832182e-01 3.46889943e-01 6.01212378e-07
 1.35148065e-02 6.14692345e-02 1.16390397e-03 9.08658981e-01
 4.42790985e-02 1.46800712e-01 1.03080478e-02 9.98572946e-01
 9.94960070e-01 1.22413837e-07 5.42578757e-01 9.75590944e-02
 6.67041361e-01 8.23565002e-04 2.03246158e-03 1.37964860e-02
 7.98082503e-04 1.70248234e-06 1.18255254e-03 4.36926051e-08
 9.10365256e-04 9.34852183e-01 1.54959434e-06 9.95837092e-01
 2.15939194e-01 5.86171858e-02 3.54071155e-08 2.05103476e-02
 5.72929730e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 13:27:05, Dev, Step : 2536, Loss : 0.51666, Acc : 0.775, Auc : 0.852, Sensitive_Loss : 0.22782, Sensitive_Acc : 20.940, Sensitive_Auc : 0.996, Mean auc: 0.852, Run Time : 102.40 sec
INFO:root:2024-04-11 13:27:10, Train, Epoch : 5, Step : 2540, Loss : 0.14559, Acc : 0.325, Sensitive_Loss : 0.03497, Sensitive_Acc : 10.000, Run Time : 4.32 sec
INFO:root:2024-04-11 13:27:17, Train, Epoch : 5, Step : 2550, Loss : 0.35846, Acc : 0.856, Sensitive_Loss : 0.13410, Sensitive_Acc : 15.300, Run Time : 7.10 sec
INFO:root:2024-04-11 13:27:28, Train, Epoch : 5, Step : 2560, Loss : 0.36953, Acc : 0.822, Sensitive_Loss : 0.13677, Sensitive_Acc : 23.700, Run Time : 10.29 sec
INFO:root:2024-04-11 13:27:35, Train, Epoch : 5, Step : 2570, Loss : 0.47213, Acc : 0.819, Sensitive_Loss : 0.09179, Sensitive_Acc : 18.600, Run Time : 7.34 sec
INFO:root:2024-04-11 13:27:43, Train, Epoch : 5, Step : 2580, Loss : 0.36479, Acc : 0.822, Sensitive_Loss : 0.15989, Sensitive_Acc : 24.400, Run Time : 7.83 sec
INFO:root:2024-04-11 13:27:50, Train, Epoch : 5, Step : 2590, Loss : 0.43823, Acc : 0.800, Sensitive_Loss : 0.12030, Sensitive_Acc : 26.500, Run Time : 7.54 sec
INFO:root:2024-04-11 13:28:03, Train, Epoch : 5, Step : 2600, Loss : 0.49797, Acc : 0.791, Sensitive_Loss : 0.15438, Sensitive_Acc : 20.200, Run Time : 12.32 sec
INFO:root:2024-04-11 13:29:43, Dev, Step : 2600, Loss : 0.50526, Acc : 0.776, Auc : 0.855, Sensitive_Loss : 0.20506, Sensitive_Acc : 21.075, Sensitive_Auc : 0.995, Mean auc: 0.855, Run Time : 100.26 sec
INFO:root:2024-04-11 13:29:44, Best, Step : 2600, Loss : 0.50526, Acc : 0.776, Auc : 0.855, Sensitive_Loss : 0.20506, Sensitive_Acc : 21.075, Sensitive_Auc : 0.995, Best Auc : 0.855
INFO:root:2024-04-11 13:29:50, Train, Epoch : 5, Step : 2610, Loss : 0.45585, Acc : 0.806, Sensitive_Loss : 0.15180, Sensitive_Acc : 22.500, Run Time : 106.93 sec
INFO:root:2024-04-11 13:30:00, Train, Epoch : 5, Step : 2620, Loss : 0.38254, Acc : 0.859, Sensitive_Loss : 0.18330, Sensitive_Acc : 23.500, Run Time : 10.79 sec
INFO:root:2024-04-11 13:30:08, Train, Epoch : 5, Step : 2630, Loss : 0.44475, Acc : 0.812, Sensitive_Loss : 0.13936, Sensitive_Acc : 25.000, Run Time : 7.51 sec
INFO:root:2024-04-11 13:30:15, Train, Epoch : 5, Step : 2640, Loss : 0.44047, Acc : 0.809, Sensitive_Loss : 0.15620, Sensitive_Acc : 24.300, Run Time : 7.45 sec
INFO:root:2024-04-11 13:30:27, Train, Epoch : 5, Step : 2650, Loss : 0.42299, Acc : 0.806, Sensitive_Loss : 0.19608, Sensitive_Acc : 21.600, Run Time : 11.21 sec
INFO:root:2024-04-11 13:30:34, Train, Epoch : 5, Step : 2660, Loss : 0.41206, Acc : 0.828, Sensitive_Loss : 0.21781, Sensitive_Acc : 21.400, Run Time : 7.02 sec
INFO:root:2024-04-11 13:30:41, Train, Epoch : 5, Step : 2670, Loss : 0.43798, Acc : 0.794, Sensitive_Loss : 0.17919, Sensitive_Acc : 17.900, Run Time : 7.68 sec
INFO:root:2024-04-11 13:30:49, Train, Epoch : 5, Step : 2680, Loss : 0.46916, Acc : 0.806, Sensitive_Loss : 0.16716, Sensitive_Acc : 22.400, Run Time : 7.37 sec
INFO:root:2024-04-11 13:31:01, Train, Epoch : 5, Step : 2690, Loss : 0.45696, Acc : 0.809, Sensitive_Loss : 0.15714, Sensitive_Acc : 19.700, Run Time : 12.86 sec
INFO:root:2024-04-11 13:31:09, Train, Epoch : 5, Step : 2700, Loss : 0.48879, Acc : 0.791, Sensitive_Loss : 0.11882, Sensitive_Acc : 22.500, Run Time : 7.37 sec
INFO:root:2024-04-11 13:32:51, Dev, Step : 2700, Loss : 0.51637, Acc : 0.777, Auc : 0.853, Sensitive_Loss : 0.20481, Sensitive_Acc : 21.436, Sensitive_Auc : 0.997, Mean auc: 0.853, Run Time : 101.70 sec
INFO:root:2024-04-11 13:32:59, Train, Epoch : 5, Step : 2710, Loss : 0.39047, Acc : 0.838, Sensitive_Loss : 0.11561, Sensitive_Acc : 21.000, Run Time : 110.41 sec
INFO:root:2024-04-11 13:33:08, Train, Epoch : 5, Step : 2720, Loss : 0.39445, Acc : 0.794, Sensitive_Loss : 0.14351, Sensitive_Acc : 22.500, Run Time : 8.36 sec
INFO:root:2024-04-11 13:33:15, Train, Epoch : 5, Step : 2730, Loss : 0.41336, Acc : 0.841, Sensitive_Loss : 0.19989, Sensitive_Acc : 21.400, Run Time : 7.39 sec
INFO:root:2024-04-11 13:33:24, Train, Epoch : 5, Step : 2740, Loss : 0.38688, Acc : 0.819, Sensitive_Loss : 0.24023, Sensitive_Acc : 22.900, Run Time : 8.86 sec
INFO:root:2024-04-11 13:33:32, Train, Epoch : 5, Step : 2750, Loss : 0.43770, Acc : 0.828, Sensitive_Loss : 0.12715, Sensitive_Acc : 18.700, Run Time : 8.14 sec
INFO:root:2024-04-11 13:33:39, Train, Epoch : 5, Step : 2760, Loss : 0.45770, Acc : 0.784, Sensitive_Loss : 0.14090, Sensitive_Acc : 18.600, Run Time : 7.42 sec
INFO:root:2024-04-11 13:33:47, Train, Epoch : 5, Step : 2770, Loss : 0.46865, Acc : 0.797, Sensitive_Loss : 0.10695, Sensitive_Acc : 21.300, Run Time : 7.59 sec
INFO:root:2024-04-11 13:33:55, Train, Epoch : 5, Step : 2780, Loss : 0.45351, Acc : 0.778, Sensitive_Loss : 0.10537, Sensitive_Acc : 21.800, Run Time : 8.40 sec
INFO:root:2024-04-11 13:34:03, Train, Epoch : 5, Step : 2790, Loss : 0.39684, Acc : 0.809, Sensitive_Loss : 0.14036, Sensitive_Acc : 24.400, Run Time : 7.44 sec
INFO:root:2024-04-11 13:34:10, Train, Epoch : 5, Step : 2800, Loss : 0.42564, Acc : 0.794, Sensitive_Loss : 0.17834, Sensitive_Acc : 18.600, Run Time : 6.86 sec
INFO:root:2024-04-11 13:35:45, Dev, Step : 2800, Loss : 0.51679, Acc : 0.774, Auc : 0.853, Sensitive_Loss : 0.18104, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.853, Run Time : 95.76 sec
INFO:root:2024-04-11 13:35:51, Train, Epoch : 5, Step : 2810, Loss : 0.42673, Acc : 0.828, Sensitive_Loss : 0.16735, Sensitive_Acc : 17.700, Run Time : 101.00 sec
INFO:root:2024-04-11 13:35:58, Train, Epoch : 5, Step : 2820, Loss : 0.38518, Acc : 0.841, Sensitive_Loss : 0.19314, Sensitive_Acc : 24.200, Run Time : 7.34 sec
INFO:root:2024-04-11 13:36:06, Train, Epoch : 5, Step : 2830, Loss : 0.47257, Acc : 0.809, Sensitive_Loss : 0.17952, Sensitive_Acc : 19.200, Run Time : 7.53 sec
INFO:root:2024-04-11 13:36:13, Train, Epoch : 5, Step : 2840, Loss : 0.42705, Acc : 0.797, Sensitive_Loss : 0.14107, Sensitive_Acc : 20.400, Run Time : 7.34 sec
INFO:root:2024-04-11 13:36:21, Train, Epoch : 5, Step : 2850, Loss : 0.46497, Acc : 0.791, Sensitive_Loss : 0.16963, Sensitive_Acc : 21.100, Run Time : 7.63 sec
INFO:root:2024-04-11 13:36:28, Train, Epoch : 5, Step : 2860, Loss : 0.39477, Acc : 0.834, Sensitive_Loss : 0.08587, Sensitive_Acc : 25.300, Run Time : 7.74 sec
INFO:root:2024-04-11 13:36:36, Train, Epoch : 5, Step : 2870, Loss : 0.38369, Acc : 0.847, Sensitive_Loss : 0.19365, Sensitive_Acc : 18.700, Run Time : 7.33 sec
INFO:root:2024-04-11 13:36:44, Train, Epoch : 5, Step : 2880, Loss : 0.42105, Acc : 0.806, Sensitive_Loss : 0.11228, Sensitive_Acc : 20.800, Run Time : 8.34 sec
INFO:root:2024-04-11 13:36:52, Train, Epoch : 5, Step : 2890, Loss : 0.38544, Acc : 0.828, Sensitive_Loss : 0.17354, Sensitive_Acc : 22.000, Run Time : 7.72 sec
INFO:root:2024-04-11 13:37:00, Train, Epoch : 5, Step : 2900, Loss : 0.43593, Acc : 0.803, Sensitive_Loss : 0.13902, Sensitive_Acc : 24.900, Run Time : 7.92 sec
INFO:root:2024-04-11 13:38:31, Dev, Step : 2900, Loss : 0.51109, Acc : 0.777, Auc : 0.855, Sensitive_Loss : 0.19220, Sensitive_Acc : 21.571, Sensitive_Auc : 0.997, Mean auc: 0.855, Run Time : 91.10 sec
INFO:root:2024-04-11 13:38:36, Train, Epoch : 5, Step : 2910, Loss : 0.42844, Acc : 0.838, Sensitive_Loss : 0.12577, Sensitive_Acc : 19.800, Run Time : 96.85 sec
INFO:root:2024-04-11 13:38:44, Train, Epoch : 5, Step : 2920, Loss : 0.44200, Acc : 0.819, Sensitive_Loss : 0.16472, Sensitive_Acc : 24.400, Run Time : 7.56 sec
INFO:root:2024-04-11 13:38:52, Train, Epoch : 5, Step : 2930, Loss : 0.47474, Acc : 0.787, Sensitive_Loss : 0.16084, Sensitive_Acc : 23.300, Run Time : 8.41 sec
INFO:root:2024-04-11 13:39:01, Train, Epoch : 5, Step : 2940, Loss : 0.44740, Acc : 0.800, Sensitive_Loss : 0.13634, Sensitive_Acc : 18.200, Run Time : 8.23 sec
INFO:root:2024-04-11 13:39:08, Train, Epoch : 5, Step : 2950, Loss : 0.44960, Acc : 0.797, Sensitive_Loss : 0.15793, Sensitive_Acc : 25.100, Run Time : 7.54 sec
INFO:root:2024-04-11 13:39:16, Train, Epoch : 5, Step : 2960, Loss : 0.39215, Acc : 0.803, Sensitive_Loss : 0.14923, Sensitive_Acc : 19.500, Run Time : 7.82 sec
INFO:root:2024-04-11 13:39:25, Train, Epoch : 5, Step : 2970, Loss : 0.49221, Acc : 0.806, Sensitive_Loss : 0.13546, Sensitive_Acc : 24.100, Run Time : 9.43 sec
INFO:root:2024-04-11 13:39:33, Train, Epoch : 5, Step : 2980, Loss : 0.43778, Acc : 0.803, Sensitive_Loss : 0.16921, Sensitive_Acc : 24.600, Run Time : 8.05 sec
INFO:root:2024-04-11 13:39:45, Train, Epoch : 5, Step : 2990, Loss : 0.40635, Acc : 0.819, Sensitive_Loss : 0.14425, Sensitive_Acc : 22.300, Run Time : 11.28 sec
INFO:root:2024-04-11 13:39:52, Train, Epoch : 5, Step : 3000, Loss : 0.50869, Acc : 0.778, Sensitive_Loss : 0.13837, Sensitive_Acc : 19.300, Run Time : 7.33 sec
INFO:root:2024-04-11 13:41:29, Dev, Step : 3000, Loss : 0.50708, Acc : 0.778, Auc : 0.856, Sensitive_Loss : 0.16969, Sensitive_Acc : 21.827, Sensitive_Auc : 0.997, Mean auc: 0.856, Run Time : 97.25 sec
INFO:root:2024-04-11 13:41:30, Best, Step : 3000, Loss : 0.50708, Acc : 0.778, Auc : 0.856, Sensitive_Loss : 0.16969, Sensitive_Acc : 21.827, Sensitive_Auc : 0.997, Best Auc : 0.856
INFO:root:2024-04-11 13:41:36, Train, Epoch : 5, Step : 3010, Loss : 0.46173, Acc : 0.756, Sensitive_Loss : 0.13968, Sensitive_Acc : 18.400, Run Time : 103.78 sec
INFO:root:2024-04-11 13:41:44, Train, Epoch : 5, Step : 3020, Loss : 0.42081, Acc : 0.800, Sensitive_Loss : 0.12845, Sensitive_Acc : 19.800, Run Time : 7.84 sec
INFO:root:2024-04-11 13:41:52, Train, Epoch : 5, Step : 3030, Loss : 0.39161, Acc : 0.794, Sensitive_Loss : 0.11232, Sensitive_Acc : 21.100, Run Time : 8.21 sec
INFO:root:2024-04-11 13:42:00, Train, Epoch : 5, Step : 3040, Loss : 0.40609, Acc : 0.787, Sensitive_Loss : 0.14887, Sensitive_Acc : 20.300, Run Time : 8.33 sec
INFO:root:2024-04-11 13:42:08, Train, Epoch : 5, Step : 3050, Loss : 0.35036, Acc : 0.853, Sensitive_Loss : 0.13064, Sensitive_Acc : 24.200, Run Time : 8.05 sec
INFO:root:2024-04-11 13:42:17, Train, Epoch : 5, Step : 3060, Loss : 0.42744, Acc : 0.791, Sensitive_Loss : 0.15162, Sensitive_Acc : 26.100, Run Time : 9.03 sec
INFO:root:2024-04-11 13:42:26, Train, Epoch : 5, Step : 3070, Loss : 0.39508, Acc : 0.825, Sensitive_Loss : 0.13819, Sensitive_Acc : 24.100, Run Time : 8.98 sec
INFO:root:2024-04-11 13:42:35, Train, Epoch : 5, Step : 3080, Loss : 0.40018, Acc : 0.812, Sensitive_Loss : 0.16721, Sensitive_Acc : 19.300, Run Time : 8.50 sec
INFO:root:2024-04-11 13:42:43, Train, Epoch : 5, Step : 3090, Loss : 0.36428, Acc : 0.822, Sensitive_Loss : 0.11298, Sensitive_Acc : 22.600, Run Time : 8.45 sec
INFO:root:2024-04-11 13:42:51, Train, Epoch : 5, Step : 3100, Loss : 0.49245, Acc : 0.787, Sensitive_Loss : 0.15938, Sensitive_Acc : 23.100, Run Time : 7.90 sec
INFO:root:2024-04-11 13:44:22, Dev, Step : 3100, Loss : 0.51357, Acc : 0.779, Auc : 0.857, Sensitive_Loss : 0.17487, Sensitive_Acc : 21.707, Sensitive_Auc : 0.997, Mean auc: 0.857, Run Time : 90.85 sec
INFO:root:2024-04-11 13:44:23, Best, Step : 3100, Loss : 0.51357, Acc : 0.779, Auc : 0.857, Sensitive_Loss : 0.17487, Sensitive_Acc : 21.707, Sensitive_Auc : 0.997, Best Auc : 0.857
INFO:root:2024-04-11 13:44:29, Train, Epoch : 5, Step : 3110, Loss : 0.41681, Acc : 0.778, Sensitive_Loss : 0.23329, Sensitive_Acc : 22.300, Run Time : 97.73 sec
INFO:root:2024-04-11 13:44:37, Train, Epoch : 5, Step : 3120, Loss : 0.39892, Acc : 0.822, Sensitive_Loss : 0.17895, Sensitive_Acc : 22.300, Run Time : 7.91 sec
INFO:root:2024-04-11 13:44:46, Train, Epoch : 5, Step : 3130, Loss : 0.45933, Acc : 0.797, Sensitive_Loss : 0.14955, Sensitive_Acc : 25.900, Run Time : 9.50 sec
INFO:root:2024-04-11 13:44:54, Train, Epoch : 5, Step : 3140, Loss : 0.44569, Acc : 0.803, Sensitive_Loss : 0.19308, Sensitive_Acc : 23.500, Run Time : 7.90 sec
INFO:root:2024-04-11 13:45:02, Train, Epoch : 5, Step : 3150, Loss : 0.40976, Acc : 0.809, Sensitive_Loss : 0.14931, Sensitive_Acc : 18.800, Run Time : 7.35 sec
INFO:root:2024-04-11 13:45:10, Train, Epoch : 5, Step : 3160, Loss : 0.44083, Acc : 0.794, Sensitive_Loss : 0.12154, Sensitive_Acc : 23.900, Run Time : 8.13 sec
INFO:root:2024-04-11 13:45:17, Train, Epoch : 5, Step : 3170, Loss : 0.43415, Acc : 0.794, Sensitive_Loss : 0.14636, Sensitive_Acc : 23.200, Run Time : 7.46 sec
INFO:root:2024-04-11 13:46:50
INFO:root:y_pred: [0.14446606 0.03817329 0.03709024 ... 0.17201078 0.02162861 0.02001337]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [9.24106396e-04 4.08803113e-04 1.20362230e-02 6.81050941e-02
 2.27425218e-01 8.30716908e-06 2.61085406e-02 1.10223176e-04
 3.94792948e-03 9.96986091e-01 4.99135815e-02 2.94500351e-05
 8.30733825e-07 1.18566410e-08 9.95646656e-01 3.44744287e-02
 7.91695609e-04 9.97899890e-01 9.88320470e-01 6.58064455e-05
 2.52128720e-01 1.01358943e-04 2.17289757e-03 2.90633715e-03
 1.05310930e-02 7.98461773e-03 1.35495893e-05 3.24293668e-03
 5.40948713e-06 6.50389940e-02 3.09923291e-03 9.37463820e-01
 9.89371911e-04 8.89120460e-01 8.62288175e-07 6.58941717e-05
 3.21829998e-06 2.04776838e-01 1.64522544e-01 2.05267183e-02
 3.47500085e-03 8.49444628e-01 3.70885618e-03 1.05246050e-04
 9.65727210e-01 4.66445595e-01 5.08228200e-04 2.05653176e-01
 5.22686280e-02 9.92100060e-01 9.74882245e-01 9.94301677e-01
 8.58143389e-01 1.52543364e-02 4.63020615e-03 7.78007973e-03
 2.80200516e-06 2.46321292e-06 9.90790486e-01 3.01065529e-03
 1.97324960e-04 4.53558937e-02 1.64779164e-02 2.16832905e-05
 9.66552556e-01 5.02285123e-01 1.21448174e-05 6.74570084e-01
 3.28552298e-04 9.75240827e-01 9.97216344e-01 9.98002231e-01
 9.92211085e-07 6.47175074e-01 4.60681331e-05 3.17341238e-01
 5.40911546e-03 2.13878195e-08 8.03340328e-08 1.32278178e-03
 7.06723076e-04 4.24708240e-04 9.58901227e-01 9.76969302e-01
 5.32916896e-02 7.16939056e-03 2.31952500e-03 5.75287959e-05
 2.08589714e-04 3.94622987e-07 2.04923235e-05 5.78691401e-02
 7.87701708e-08 4.20310703e-10 1.00213708e-03 5.10169566e-03
 4.41700213e-05 8.63866627e-01 2.12187952e-05 6.59482612e-04
 6.51118637e-04 8.45733285e-02 3.22319865e-01 3.35843460e-08
 9.63707105e-04 6.17743796e-03 2.31799822e-05 9.24681008e-01
 6.32090541e-03 3.57413888e-02 6.09959709e-03 9.98694599e-01
 9.90694106e-01 4.44766414e-07 5.41032374e-01 7.00719934e-03
 1.98449954e-01 1.70283784e-06 2.77537922e-03 1.97036844e-03
 7.14863418e-04 8.15783210e-07 2.03492746e-04 6.65366295e-09
 1.01403985e-03 7.90150404e-01 5.46222168e-07 9.95602131e-01
 6.49786694e-03 7.64681920e-02 5.55405734e-07 1.19220407e-03
 7.99326585e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 13:46:50, Dev, Step : 3170, Loss : 0.51412, Acc : 0.780, Auc : 0.855, Sensitive_Loss : 0.18309, Sensitive_Acc : 21.391, Sensitive_Auc : 0.995, Mean auc: 0.855, Run Time : 92.50 sec
INFO:root:2024-04-11 13:47:00, Train, Epoch : 6, Step : 3180, Loss : 0.42570, Acc : 0.803, Sensitive_Loss : 0.10969, Sensitive_Acc : 23.900, Run Time : 9.03 sec
INFO:root:2024-04-11 13:47:08, Train, Epoch : 6, Step : 3190, Loss : 0.48565, Acc : 0.809, Sensitive_Loss : 0.14835, Sensitive_Acc : 22.800, Run Time : 7.91 sec
INFO:root:2024-04-11 13:47:18, Train, Epoch : 6, Step : 3200, Loss : 0.33566, Acc : 0.825, Sensitive_Loss : 0.13720, Sensitive_Acc : 21.700, Run Time : 10.61 sec
INFO:root:2024-04-11 13:48:48, Dev, Step : 3200, Loss : 0.50890, Acc : 0.783, Auc : 0.855, Sensitive_Loss : 0.17831, Sensitive_Acc : 21.692, Sensitive_Auc : 0.996, Mean auc: 0.855, Run Time : 90.19 sec
INFO:root:2024-04-11 13:48:54, Train, Epoch : 6, Step : 3210, Loss : 0.40432, Acc : 0.806, Sensitive_Loss : 0.12866, Sensitive_Acc : 21.600, Run Time : 95.74 sec
INFO:root:2024-04-11 13:49:02, Train, Epoch : 6, Step : 3220, Loss : 0.43622, Acc : 0.812, Sensitive_Loss : 0.11212, Sensitive_Acc : 23.300, Run Time : 7.53 sec
INFO:root:2024-04-11 13:49:09, Train, Epoch : 6, Step : 3230, Loss : 0.43240, Acc : 0.825, Sensitive_Loss : 0.17465, Sensitive_Acc : 23.200, Run Time : 7.70 sec
INFO:root:2024-04-11 13:49:17, Train, Epoch : 6, Step : 3240, Loss : 0.36537, Acc : 0.812, Sensitive_Loss : 0.14250, Sensitive_Acc : 24.000, Run Time : 7.70 sec
INFO:root:2024-04-11 13:49:25, Train, Epoch : 6, Step : 3250, Loss : 0.36376, Acc : 0.863, Sensitive_Loss : 0.12622, Sensitive_Acc : 18.200, Run Time : 8.43 sec
INFO:root:2024-04-11 13:49:33, Train, Epoch : 6, Step : 3260, Loss : 0.36535, Acc : 0.844, Sensitive_Loss : 0.14133, Sensitive_Acc : 20.600, Run Time : 7.51 sec
INFO:root:2024-04-11 13:49:41, Train, Epoch : 6, Step : 3270, Loss : 0.32915, Acc : 0.856, Sensitive_Loss : 0.13571, Sensitive_Acc : 20.600, Run Time : 7.74 sec
INFO:root:2024-04-11 13:49:48, Train, Epoch : 6, Step : 3280, Loss : 0.40048, Acc : 0.781, Sensitive_Loss : 0.13348, Sensitive_Acc : 25.700, Run Time : 7.67 sec
INFO:root:2024-04-11 13:49:56, Train, Epoch : 6, Step : 3290, Loss : 0.41900, Acc : 0.800, Sensitive_Loss : 0.11048, Sensitive_Acc : 14.900, Run Time : 7.64 sec
INFO:root:2024-04-11 13:50:04, Train, Epoch : 6, Step : 3300, Loss : 0.38469, Acc : 0.825, Sensitive_Loss : 0.14076, Sensitive_Acc : 23.500, Run Time : 7.97 sec
INFO:root:2024-04-11 13:51:38, Dev, Step : 3300, Loss : 0.56723, Acc : 0.764, Auc : 0.851, Sensitive_Loss : 0.20199, Sensitive_Acc : 21.150, Sensitive_Auc : 0.996, Mean auc: 0.851, Run Time : 94.47 sec
INFO:root:2024-04-11 13:51:44, Train, Epoch : 6, Step : 3310, Loss : 0.40691, Acc : 0.844, Sensitive_Loss : 0.16678, Sensitive_Acc : 18.700, Run Time : 100.59 sec
INFO:root:2024-04-11 13:51:53, Train, Epoch : 6, Step : 3320, Loss : 0.46474, Acc : 0.812, Sensitive_Loss : 0.11482, Sensitive_Acc : 14.500, Run Time : 8.62 sec
INFO:root:2024-04-11 13:52:02, Train, Epoch : 6, Step : 3330, Loss : 0.40299, Acc : 0.794, Sensitive_Loss : 0.13066, Sensitive_Acc : 24.900, Run Time : 9.28 sec
INFO:root:2024-04-11 13:52:12, Train, Epoch : 6, Step : 3340, Loss : 0.45823, Acc : 0.787, Sensitive_Loss : 0.15285, Sensitive_Acc : 21.800, Run Time : 9.18 sec
INFO:root:2024-04-11 13:52:21, Train, Epoch : 6, Step : 3350, Loss : 0.42225, Acc : 0.834, Sensitive_Loss : 0.09921, Sensitive_Acc : 21.900, Run Time : 9.01 sec
INFO:root:2024-04-11 13:52:30, Train, Epoch : 6, Step : 3360, Loss : 0.39799, Acc : 0.812, Sensitive_Loss : 0.19040, Sensitive_Acc : 24.600, Run Time : 8.96 sec
INFO:root:2024-04-11 13:52:39, Train, Epoch : 6, Step : 3370, Loss : 0.38381, Acc : 0.856, Sensitive_Loss : 0.18533, Sensitive_Acc : 23.000, Run Time : 9.95 sec
INFO:root:2024-04-11 13:52:48, Train, Epoch : 6, Step : 3380, Loss : 0.36410, Acc : 0.847, Sensitive_Loss : 0.14027, Sensitive_Acc : 24.300, Run Time : 8.78 sec
INFO:root:2024-04-11 13:52:57, Train, Epoch : 6, Step : 3390, Loss : 0.41621, Acc : 0.819, Sensitive_Loss : 0.10217, Sensitive_Acc : 25.100, Run Time : 8.92 sec
INFO:root:2024-04-11 13:53:07, Train, Epoch : 6, Step : 3400, Loss : 0.38394, Acc : 0.838, Sensitive_Loss : 0.14726, Sensitive_Acc : 22.900, Run Time : 9.64 sec
INFO:root:2024-04-11 13:54:42, Dev, Step : 3400, Loss : 0.51587, Acc : 0.778, Auc : 0.857, Sensitive_Loss : 0.18391, Sensitive_Acc : 21.556, Sensitive_Auc : 0.996, Mean auc: 0.857, Run Time : 94.82 sec
INFO:root:2024-04-11 13:54:48, Train, Epoch : 6, Step : 3410, Loss : 0.44332, Acc : 0.809, Sensitive_Loss : 0.08968, Sensitive_Acc : 26.400, Run Time : 101.52 sec
INFO:root:2024-04-11 13:54:57, Train, Epoch : 6, Step : 3420, Loss : 0.38535, Acc : 0.809, Sensitive_Loss : 0.12655, Sensitive_Acc : 20.300, Run Time : 8.82 sec
INFO:root:2024-04-11 13:55:07, Train, Epoch : 6, Step : 3430, Loss : 0.42699, Acc : 0.825, Sensitive_Loss : 0.14228, Sensitive_Acc : 23.200, Run Time : 9.79 sec
INFO:root:2024-04-11 13:55:16, Train, Epoch : 6, Step : 3440, Loss : 0.34294, Acc : 0.822, Sensitive_Loss : 0.13769, Sensitive_Acc : 25.300, Run Time : 8.56 sec
INFO:root:2024-04-11 13:55:26, Train, Epoch : 6, Step : 3450, Loss : 0.37179, Acc : 0.828, Sensitive_Loss : 0.14163, Sensitive_Acc : 21.800, Run Time : 10.47 sec
INFO:root:2024-04-11 13:55:35, Train, Epoch : 6, Step : 3460, Loss : 0.38342, Acc : 0.819, Sensitive_Loss : 0.17133, Sensitive_Acc : 20.700, Run Time : 8.75 sec
INFO:root:2024-04-11 13:55:44, Train, Epoch : 6, Step : 3470, Loss : 0.45971, Acc : 0.803, Sensitive_Loss : 0.12371, Sensitive_Acc : 20.400, Run Time : 8.96 sec
INFO:root:2024-04-11 13:55:52, Train, Epoch : 6, Step : 3480, Loss : 0.38192, Acc : 0.812, Sensitive_Loss : 0.15133, Sensitive_Acc : 18.700, Run Time : 8.61 sec
INFO:root:2024-04-11 13:56:02, Train, Epoch : 6, Step : 3490, Loss : 0.41858, Acc : 0.828, Sensitive_Loss : 0.18681, Sensitive_Acc : 21.200, Run Time : 9.22 sec
INFO:root:2024-04-11 13:56:10, Train, Epoch : 6, Step : 3500, Loss : 0.41855, Acc : 0.831, Sensitive_Loss : 0.17502, Sensitive_Acc : 22.500, Run Time : 8.45 sec
INFO:root:2024-04-11 13:57:43, Dev, Step : 3500, Loss : 0.51435, Acc : 0.777, Auc : 0.856, Sensitive_Loss : 0.20717, Sensitive_Acc : 20.805, Sensitive_Auc : 0.998, Mean auc: 0.856, Run Time : 93.45 sec
INFO:root:2024-04-11 13:57:50, Train, Epoch : 6, Step : 3510, Loss : 0.40555, Acc : 0.781, Sensitive_Loss : 0.16771, Sensitive_Acc : 20.300, Run Time : 99.66 sec
INFO:root:2024-04-11 13:57:59, Train, Epoch : 6, Step : 3520, Loss : 0.40521, Acc : 0.825, Sensitive_Loss : 0.17225, Sensitive_Acc : 23.000, Run Time : 9.16 sec
INFO:root:2024-04-11 13:58:09, Train, Epoch : 6, Step : 3530, Loss : 0.41862, Acc : 0.819, Sensitive_Loss : 0.09947, Sensitive_Acc : 23.100, Run Time : 9.73 sec
INFO:root:2024-04-11 13:58:20, Train, Epoch : 6, Step : 3540, Loss : 0.40575, Acc : 0.809, Sensitive_Loss : 0.17363, Sensitive_Acc : 19.900, Run Time : 11.40 sec
INFO:root:2024-04-11 13:58:36, Train, Epoch : 6, Step : 3550, Loss : 0.44052, Acc : 0.834, Sensitive_Loss : 0.12997, Sensitive_Acc : 20.000, Run Time : 16.20 sec
INFO:root:2024-04-11 13:58:46, Train, Epoch : 6, Step : 3560, Loss : 0.35848, Acc : 0.819, Sensitive_Loss : 0.11078, Sensitive_Acc : 21.900, Run Time : 10.35 sec
INFO:root:2024-04-11 13:58:55, Train, Epoch : 6, Step : 3570, Loss : 0.42135, Acc : 0.834, Sensitive_Loss : 0.14243, Sensitive_Acc : 20.100, Run Time : 8.58 sec
INFO:root:2024-04-11 13:59:04, Train, Epoch : 6, Step : 3580, Loss : 0.41262, Acc : 0.834, Sensitive_Loss : 0.10730, Sensitive_Acc : 20.200, Run Time : 9.06 sec
INFO:root:2024-04-11 13:59:13, Train, Epoch : 6, Step : 3590, Loss : 0.34261, Acc : 0.875, Sensitive_Loss : 0.12585, Sensitive_Acc : 19.600, Run Time : 8.62 sec
INFO:root:2024-04-11 13:59:22, Train, Epoch : 6, Step : 3600, Loss : 0.41492, Acc : 0.806, Sensitive_Loss : 0.15764, Sensitive_Acc : 24.300, Run Time : 9.06 sec
INFO:root:2024-04-11 14:00:53, Dev, Step : 3600, Loss : 0.52455, Acc : 0.770, Auc : 0.852, Sensitive_Loss : 0.18264, Sensitive_Acc : 21.271, Sensitive_Auc : 0.999, Mean auc: 0.852, Run Time : 91.12 sec
INFO:root:2024-04-11 14:00:59, Train, Epoch : 6, Step : 3610, Loss : 0.37882, Acc : 0.847, Sensitive_Loss : 0.16779, Sensitive_Acc : 21.100, Run Time : 97.30 sec
INFO:root:2024-04-11 14:01:07, Train, Epoch : 6, Step : 3620, Loss : 0.54163, Acc : 0.784, Sensitive_Loss : 0.14271, Sensitive_Acc : 18.600, Run Time : 8.36 sec
INFO:root:2024-04-11 14:01:16, Train, Epoch : 6, Step : 3630, Loss : 0.46925, Acc : 0.797, Sensitive_Loss : 0.09969, Sensitive_Acc : 23.900, Run Time : 8.12 sec
INFO:root:2024-04-11 14:01:24, Train, Epoch : 6, Step : 3640, Loss : 0.43882, Acc : 0.812, Sensitive_Loss : 0.11133, Sensitive_Acc : 16.700, Run Time : 8.71 sec
INFO:root:2024-04-11 14:01:34, Train, Epoch : 6, Step : 3650, Loss : 0.40599, Acc : 0.828, Sensitive_Loss : 0.13855, Sensitive_Acc : 21.800, Run Time : 9.46 sec
INFO:root:2024-04-11 14:01:44, Train, Epoch : 6, Step : 3660, Loss : 0.36249, Acc : 0.856, Sensitive_Loss : 0.12984, Sensitive_Acc : 20.400, Run Time : 10.50 sec
INFO:root:2024-04-11 14:01:52, Train, Epoch : 6, Step : 3670, Loss : 0.38428, Acc : 0.819, Sensitive_Loss : 0.13392, Sensitive_Acc : 16.500, Run Time : 8.20 sec
INFO:root:2024-04-11 14:02:00, Train, Epoch : 6, Step : 3680, Loss : 0.42844, Acc : 0.809, Sensitive_Loss : 0.11795, Sensitive_Acc : 22.100, Run Time : 7.95 sec
INFO:root:2024-04-11 14:02:09, Train, Epoch : 6, Step : 3690, Loss : 0.36399, Acc : 0.831, Sensitive_Loss : 0.14131, Sensitive_Acc : 19.600, Run Time : 8.40 sec
INFO:root:2024-04-11 14:02:18, Train, Epoch : 6, Step : 3700, Loss : 0.36472, Acc : 0.859, Sensitive_Loss : 0.09602, Sensitive_Acc : 20.100, Run Time : 8.89 sec
INFO:root:2024-04-11 14:03:50, Dev, Step : 3700, Loss : 0.50888, Acc : 0.780, Auc : 0.857, Sensitive_Loss : 0.17638, Sensitive_Acc : 21.436, Sensitive_Auc : 0.998, Mean auc: 0.857, Run Time : 92.20 sec
INFO:root:2024-04-11 14:03:51, Best, Step : 3700, Loss : 0.50888, Acc : 0.780, Auc : 0.857, Sensitive_Loss : 0.17638, Sensitive_Acc : 21.436, Sensitive_Auc : 0.998, Best Auc : 0.857
INFO:root:2024-04-11 14:03:56, Train, Epoch : 6, Step : 3710, Loss : 0.38720, Acc : 0.825, Sensitive_Loss : 0.15104, Sensitive_Acc : 21.900, Run Time : 98.79 sec
INFO:root:2024-04-11 14:04:06, Train, Epoch : 6, Step : 3720, Loss : 0.46684, Acc : 0.806, Sensitive_Loss : 0.10684, Sensitive_Acc : 15.300, Run Time : 9.05 sec
INFO:root:2024-04-11 14:04:14, Train, Epoch : 6, Step : 3730, Loss : 0.43188, Acc : 0.841, Sensitive_Loss : 0.13225, Sensitive_Acc : 18.700, Run Time : 8.05 sec
INFO:root:2024-04-11 14:04:22, Train, Epoch : 6, Step : 3740, Loss : 0.39209, Acc : 0.816, Sensitive_Loss : 0.09497, Sensitive_Acc : 16.900, Run Time : 8.80 sec
INFO:root:2024-04-11 14:04:32, Train, Epoch : 6, Step : 3750, Loss : 0.42628, Acc : 0.822, Sensitive_Loss : 0.18683, Sensitive_Acc : 19.100, Run Time : 9.13 sec
INFO:root:2024-04-11 14:04:39, Train, Epoch : 6, Step : 3760, Loss : 0.49243, Acc : 0.784, Sensitive_Loss : 0.17058, Sensitive_Acc : 22.800, Run Time : 7.95 sec
INFO:root:2024-04-11 14:04:48, Train, Epoch : 6, Step : 3770, Loss : 0.44559, Acc : 0.825, Sensitive_Loss : 0.10480, Sensitive_Acc : 21.600, Run Time : 8.26 sec
INFO:root:2024-04-11 14:04:57, Train, Epoch : 6, Step : 3780, Loss : 0.34790, Acc : 0.841, Sensitive_Loss : 0.14377, Sensitive_Acc : 21.000, Run Time : 8.94 sec
INFO:root:2024-04-11 14:05:05, Train, Epoch : 6, Step : 3790, Loss : 0.42227, Acc : 0.825, Sensitive_Loss : 0.12493, Sensitive_Acc : 25.100, Run Time : 8.34 sec
INFO:root:2024-04-11 14:05:14, Train, Epoch : 6, Step : 3800, Loss : 0.35427, Acc : 0.850, Sensitive_Loss : 0.13570, Sensitive_Acc : 17.700, Run Time : 8.83 sec
INFO:root:2024-04-11 14:06:47, Dev, Step : 3800, Loss : 0.54656, Acc : 0.768, Auc : 0.852, Sensitive_Loss : 0.16001, Sensitive_Acc : 21.722, Sensitive_Auc : 0.998, Mean auc: 0.852, Run Time : 92.87 sec
INFO:root:2024-04-11 14:08:17
INFO:root:y_pred: [0.0546515  0.01605668 0.02374006 ... 0.06335231 0.02092052 0.01380271]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.67554783e-04 4.89595986e-05 4.09734016e-03 3.72336917e-02
 1.69850707e-01 4.05741685e-06 1.24488189e-03 4.04536004e-05
 1.24858655e-02 9.97070432e-01 7.77132390e-03 4.45068645e-06
 7.73256303e-08 3.30063821e-09 9.93547380e-01 1.57456212e-02
 5.00486567e-05 9.96298373e-01 9.91330624e-01 8.62472734e-05
 6.61699951e-01 1.85648275e-06 3.48118861e-04 1.70364059e-04
 4.30616830e-03 3.48960701e-03 5.24945472e-06 5.36145060e-04
 7.18876549e-07 1.81741826e-02 1.57209835e-03 9.06968534e-01
 1.32620362e-05 8.90452147e-01 1.91141556e-08 1.54942427e-05
 2.90062410e-08 9.24905315e-02 7.58338273e-02 1.07885106e-02
 1.91905201e-04 8.64308476e-01 1.21314463e-03 2.45795445e-06
 9.65615392e-01 3.08160819e-02 2.73391088e-05 1.76861525e-01
 2.92836446e-02 9.92945373e-01 9.80558813e-01 9.91572022e-01
 7.70860732e-01 4.89093270e-03 8.57627671e-03 6.53641857e-03
 3.69700643e-08 6.04370101e-08 9.85753000e-01 1.12512708e-03
 3.20502331e-05 3.90805714e-02 2.95649678e-03 2.01097100e-06
 9.58384573e-01 2.69355178e-01 2.71365843e-06 4.19286042e-01
 1.48685838e-04 9.81043220e-01 9.96144176e-01 9.97590661e-01
 5.89925264e-08 6.92219436e-01 1.32014543e-06 2.92474777e-01
 4.32828616e-04 5.26064218e-12 4.37898855e-08 1.39475556e-03
 9.00344548e-06 2.96664348e-05 9.61405158e-01 9.75435436e-01
 8.56732856e-03 5.21534879e-04 5.00795468e-05 5.67381994e-06
 7.17141142e-04 1.24754209e-07 1.79941198e-05 4.93349787e-03
 7.36008898e-09 2.75031525e-10 1.61517455e-04 5.62661386e-04
 3.28312552e-07 8.43477309e-01 3.91260841e-07 1.21831050e-07
 2.15204316e-04 2.81122848e-02 2.92580426e-01 1.65983916e-09
 5.49146098e-05 1.74321688e-03 7.00327018e-06 8.70716929e-01
 4.25651888e-05 1.00978911e-02 4.01036395e-03 9.98145819e-01
 9.86333787e-01 1.97064285e-08 5.32504857e-01 1.07343460e-03
 1.68192625e-01 8.80339826e-07 7.09354994e-04 7.54536537e-04
 2.03379350e-05 2.21033332e-07 8.64727554e-05 4.22230517e-09
 1.03248822e-04 2.74050444e-01 4.44055566e-08 9.95935082e-01
 7.27504201e-04 1.57829691e-02 1.18180337e-08 6.48861896e-05
 1.06681534e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 14:08:17, Dev, Step : 3804, Loss : 0.54134, Acc : 0.769, Auc : 0.852, Sensitive_Loss : 0.15963, Sensitive_Acc : 21.722, Sensitive_Auc : 0.998, Mean auc: 0.852, Run Time : 88.53 sec
INFO:root:2024-04-11 14:08:23, Train, Epoch : 7, Step : 3810, Loss : 0.21161, Acc : 0.512, Sensitive_Loss : 0.07889, Sensitive_Acc : 15.600, Run Time : 5.79 sec
INFO:root:2024-04-11 14:08:32, Train, Epoch : 7, Step : 3820, Loss : 0.40223, Acc : 0.819, Sensitive_Loss : 0.12513, Sensitive_Acc : 21.700, Run Time : 9.04 sec
INFO:root:2024-04-11 14:08:41, Train, Epoch : 7, Step : 3830, Loss : 0.39320, Acc : 0.847, Sensitive_Loss : 0.13902, Sensitive_Acc : 20.700, Run Time : 8.62 sec
INFO:root:2024-04-11 14:08:49, Train, Epoch : 7, Step : 3840, Loss : 0.41943, Acc : 0.819, Sensitive_Loss : 0.11474, Sensitive_Acc : 21.400, Run Time : 8.18 sec
INFO:root:2024-04-11 14:08:57, Train, Epoch : 7, Step : 3850, Loss : 0.39394, Acc : 0.844, Sensitive_Loss : 0.17418, Sensitive_Acc : 24.400, Run Time : 8.09 sec
INFO:root:2024-04-11 14:09:06, Train, Epoch : 7, Step : 3860, Loss : 0.35244, Acc : 0.847, Sensitive_Loss : 0.10796, Sensitive_Acc : 18.600, Run Time : 8.52 sec
INFO:root:2024-04-11 14:09:15, Train, Epoch : 7, Step : 3870, Loss : 0.48865, Acc : 0.791, Sensitive_Loss : 0.09399, Sensitive_Acc : 19.400, Run Time : 9.41 sec
INFO:root:2024-04-11 14:09:24, Train, Epoch : 7, Step : 3880, Loss : 0.30164, Acc : 0.894, Sensitive_Loss : 0.11472, Sensitive_Acc : 21.700, Run Time : 8.47 sec
INFO:root:2024-04-11 14:09:32, Train, Epoch : 7, Step : 3890, Loss : 0.41853, Acc : 0.834, Sensitive_Loss : 0.16154, Sensitive_Acc : 23.900, Run Time : 7.99 sec
INFO:root:2024-04-11 14:09:40, Train, Epoch : 7, Step : 3900, Loss : 0.45747, Acc : 0.816, Sensitive_Loss : 0.10651, Sensitive_Acc : 19.500, Run Time : 8.45 sec
INFO:root:2024-04-11 14:11:12, Dev, Step : 3900, Loss : 0.52651, Acc : 0.774, Auc : 0.852, Sensitive_Loss : 0.17757, Sensitive_Acc : 21.316, Sensitive_Auc : 0.998, Mean auc: 0.852, Run Time : 92.04 sec
INFO:root:2024-04-11 14:11:19, Train, Epoch : 7, Step : 3910, Loss : 0.36344, Acc : 0.847, Sensitive_Loss : 0.13344, Sensitive_Acc : 26.200, Run Time : 98.73 sec
INFO:root:2024-04-11 14:11:28, Train, Epoch : 7, Step : 3920, Loss : 0.40577, Acc : 0.816, Sensitive_Loss : 0.13728, Sensitive_Acc : 22.900, Run Time : 9.63 sec
INFO:root:2024-04-11 14:11:37, Train, Epoch : 7, Step : 3930, Loss : 0.38290, Acc : 0.819, Sensitive_Loss : 0.14650, Sensitive_Acc : 20.200, Run Time : 8.49 sec
INFO:root:2024-04-11 14:11:45, Train, Epoch : 7, Step : 3940, Loss : 0.37167, Acc : 0.841, Sensitive_Loss : 0.17758, Sensitive_Acc : 26.500, Run Time : 8.39 sec
INFO:root:2024-04-11 14:11:55, Train, Epoch : 7, Step : 3950, Loss : 0.41335, Acc : 0.828, Sensitive_Loss : 0.16120, Sensitive_Acc : 22.300, Run Time : 9.87 sec
INFO:root:2024-04-11 14:12:05, Train, Epoch : 7, Step : 3960, Loss : 0.34987, Acc : 0.863, Sensitive_Loss : 0.11668, Sensitive_Acc : 18.600, Run Time : 9.49 sec
INFO:root:2024-04-11 14:12:13, Train, Epoch : 7, Step : 3970, Loss : 0.36134, Acc : 0.847, Sensitive_Loss : 0.11386, Sensitive_Acc : 19.200, Run Time : 8.23 sec
INFO:root:2024-04-11 14:12:22, Train, Epoch : 7, Step : 3980, Loss : 0.43380, Acc : 0.812, Sensitive_Loss : 0.13048, Sensitive_Acc : 19.600, Run Time : 8.82 sec
INFO:root:2024-04-11 14:12:30, Train, Epoch : 7, Step : 3990, Loss : 0.40066, Acc : 0.825, Sensitive_Loss : 0.15888, Sensitive_Acc : 18.000, Run Time : 8.74 sec
INFO:root:2024-04-11 14:12:39, Train, Epoch : 7, Step : 4000, Loss : 0.41780, Acc : 0.800, Sensitive_Loss : 0.11166, Sensitive_Acc : 25.000, Run Time : 9.08 sec
INFO:root:2024-04-11 14:14:14, Dev, Step : 4000, Loss : 0.51938, Acc : 0.779, Auc : 0.856, Sensitive_Loss : 0.18139, Sensitive_Acc : 21.316, Sensitive_Auc : 0.999, Mean auc: 0.856, Run Time : 94.41 sec
INFO:root:2024-04-11 14:14:20, Train, Epoch : 7, Step : 4010, Loss : 0.47140, Acc : 0.787, Sensitive_Loss : 0.13973, Sensitive_Acc : 22.900, Run Time : 100.83 sec
INFO:root:2024-04-11 14:14:29, Train, Epoch : 7, Step : 4020, Loss : 0.35883, Acc : 0.844, Sensitive_Loss : 0.11139, Sensitive_Acc : 19.500, Run Time : 8.73 sec
INFO:root:2024-04-11 14:14:37, Train, Epoch : 7, Step : 4030, Loss : 0.38903, Acc : 0.812, Sensitive_Loss : 0.10714, Sensitive_Acc : 23.300, Run Time : 8.44 sec
INFO:root:2024-04-11 14:14:46, Train, Epoch : 7, Step : 4040, Loss : 0.37737, Acc : 0.825, Sensitive_Loss : 0.13175, Sensitive_Acc : 22.300, Run Time : 8.84 sec
INFO:root:2024-04-11 14:14:55, Train, Epoch : 7, Step : 4050, Loss : 0.32198, Acc : 0.834, Sensitive_Loss : 0.12543, Sensitive_Acc : 22.700, Run Time : 8.84 sec
INFO:root:2024-04-11 14:15:04, Train, Epoch : 7, Step : 4060, Loss : 0.37982, Acc : 0.841, Sensitive_Loss : 0.12888, Sensitive_Acc : 22.000, Run Time : 9.18 sec
INFO:root:2024-04-11 14:15:13, Train, Epoch : 7, Step : 4070, Loss : 0.43739, Acc : 0.794, Sensitive_Loss : 0.15022, Sensitive_Acc : 21.500, Run Time : 8.47 sec
INFO:root:2024-04-11 14:15:21, Train, Epoch : 7, Step : 4080, Loss : 0.42195, Acc : 0.784, Sensitive_Loss : 0.12650, Sensitive_Acc : 19.000, Run Time : 8.24 sec
INFO:root:2024-04-11 14:15:30, Train, Epoch : 7, Step : 4090, Loss : 0.41778, Acc : 0.809, Sensitive_Loss : 0.11114, Sensitive_Acc : 20.600, Run Time : 8.49 sec
INFO:root:2024-04-11 14:15:38, Train, Epoch : 7, Step : 4100, Loss : 0.39271, Acc : 0.859, Sensitive_Loss : 0.12429, Sensitive_Acc : 19.400, Run Time : 8.31 sec
INFO:root:2024-04-11 14:17:15, Dev, Step : 4100, Loss : 0.52346, Acc : 0.776, Auc : 0.855, Sensitive_Loss : 0.16948, Sensitive_Acc : 21.586, Sensitive_Auc : 0.999, Mean auc: 0.855, Run Time : 96.82 sec
INFO:root:2024-04-11 14:17:21, Train, Epoch : 7, Step : 4110, Loss : 0.38051, Acc : 0.838, Sensitive_Loss : 0.14697, Sensitive_Acc : 23.500, Run Time : 103.19 sec
INFO:root:2024-04-11 14:17:30, Train, Epoch : 7, Step : 4120, Loss : 0.42191, Acc : 0.838, Sensitive_Loss : 0.13740, Sensitive_Acc : 15.800, Run Time : 8.62 sec
INFO:root:2024-04-11 14:17:39, Train, Epoch : 7, Step : 4130, Loss : 0.41710, Acc : 0.812, Sensitive_Loss : 0.10224, Sensitive_Acc : 16.300, Run Time : 9.44 sec
INFO:root:2024-04-11 14:17:47, Train, Epoch : 7, Step : 4140, Loss : 0.41608, Acc : 0.816, Sensitive_Loss : 0.13107, Sensitive_Acc : 21.800, Run Time : 8.19 sec
INFO:root:2024-04-11 14:17:56, Train, Epoch : 7, Step : 4150, Loss : 0.44924, Acc : 0.819, Sensitive_Loss : 0.11765, Sensitive_Acc : 17.800, Run Time : 8.74 sec
INFO:root:2024-04-11 14:18:05, Train, Epoch : 7, Step : 4160, Loss : 0.37256, Acc : 0.825, Sensitive_Loss : 0.08494, Sensitive_Acc : 23.600, Run Time : 9.24 sec
INFO:root:2024-04-11 14:18:16, Train, Epoch : 7, Step : 4170, Loss : 0.34496, Acc : 0.863, Sensitive_Loss : 0.13823, Sensitive_Acc : 26.600, Run Time : 10.28 sec
INFO:root:2024-04-11 14:18:25, Train, Epoch : 7, Step : 4180, Loss : 0.40939, Acc : 0.844, Sensitive_Loss : 0.16616, Sensitive_Acc : 18.000, Run Time : 9.18 sec
INFO:root:2024-04-11 14:18:33, Train, Epoch : 7, Step : 4190, Loss : 0.40384, Acc : 0.828, Sensitive_Loss : 0.12390, Sensitive_Acc : 25.500, Run Time : 8.41 sec
INFO:root:2024-04-11 14:18:41, Train, Epoch : 7, Step : 4200, Loss : 0.40619, Acc : 0.812, Sensitive_Loss : 0.08485, Sensitive_Acc : 19.800, Run Time : 8.19 sec
INFO:root:2024-04-11 14:20:18, Dev, Step : 4200, Loss : 0.51052, Acc : 0.780, Auc : 0.855, Sensitive_Loss : 0.17109, Sensitive_Acc : 21.436, Sensitive_Auc : 0.998, Mean auc: 0.855, Run Time : 96.86 sec
INFO:root:2024-04-11 14:20:25, Train, Epoch : 7, Step : 4210, Loss : 0.41949, Acc : 0.831, Sensitive_Loss : 0.09201, Sensitive_Acc : 20.600, Run Time : 103.70 sec
INFO:root:2024-04-11 14:20:35, Train, Epoch : 7, Step : 4220, Loss : 0.37155, Acc : 0.841, Sensitive_Loss : 0.12926, Sensitive_Acc : 21.500, Run Time : 9.59 sec
INFO:root:2024-04-11 14:20:43, Train, Epoch : 7, Step : 4230, Loss : 0.34695, Acc : 0.853, Sensitive_Loss : 0.13561, Sensitive_Acc : 22.000, Run Time : 8.32 sec
INFO:root:2024-04-11 14:20:52, Train, Epoch : 7, Step : 4240, Loss : 0.40952, Acc : 0.831, Sensitive_Loss : 0.09870, Sensitive_Acc : 17.700, Run Time : 8.72 sec
INFO:root:2024-04-11 14:21:01, Train, Epoch : 7, Step : 4250, Loss : 0.39179, Acc : 0.834, Sensitive_Loss : 0.11475, Sensitive_Acc : 22.800, Run Time : 9.21 sec
INFO:root:2024-04-11 14:21:09, Train, Epoch : 7, Step : 4260, Loss : 0.38730, Acc : 0.850, Sensitive_Loss : 0.15760, Sensitive_Acc : 20.600, Run Time : 8.36 sec
INFO:root:2024-04-11 14:21:17, Train, Epoch : 7, Step : 4270, Loss : 0.35117, Acc : 0.847, Sensitive_Loss : 0.13896, Sensitive_Acc : 21.300, Run Time : 8.11 sec
INFO:root:2024-04-11 14:21:25, Train, Epoch : 7, Step : 4280, Loss : 0.38361, Acc : 0.841, Sensitive_Loss : 0.11271, Sensitive_Acc : 18.500, Run Time : 7.92 sec
INFO:root:2024-04-11 14:21:33, Train, Epoch : 7, Step : 4290, Loss : 0.42479, Acc : 0.803, Sensitive_Loss : 0.13701, Sensitive_Acc : 16.600, Run Time : 8.19 sec
INFO:root:2024-04-11 14:21:42, Train, Epoch : 7, Step : 4300, Loss : 0.36500, Acc : 0.838, Sensitive_Loss : 0.14195, Sensitive_Acc : 24.600, Run Time : 8.36 sec
INFO:root:2024-04-11 14:23:18, Dev, Step : 4300, Loss : 0.51866, Acc : 0.777, Auc : 0.855, Sensitive_Loss : 0.16315, Sensitive_Acc : 21.842, Sensitive_Auc : 0.998, Mean auc: 0.855, Run Time : 95.72 sec
INFO:root:2024-04-11 14:23:24, Train, Epoch : 7, Step : 4310, Loss : 0.42887, Acc : 0.812, Sensitive_Loss : 0.09832, Sensitive_Acc : 25.200, Run Time : 102.35 sec
INFO:root:2024-04-11 14:23:32, Train, Epoch : 7, Step : 4320, Loss : 0.39838, Acc : 0.797, Sensitive_Loss : 0.09068, Sensitive_Acc : 22.100, Run Time : 8.12 sec
INFO:root:2024-04-11 14:23:41, Train, Epoch : 7, Step : 4330, Loss : 0.37451, Acc : 0.831, Sensitive_Loss : 0.13491, Sensitive_Acc : 21.800, Run Time : 8.50 sec
INFO:root:2024-04-11 14:23:49, Train, Epoch : 7, Step : 4340, Loss : 0.38612, Acc : 0.812, Sensitive_Loss : 0.12327, Sensitive_Acc : 24.600, Run Time : 8.31 sec
INFO:root:2024-04-11 14:23:58, Train, Epoch : 7, Step : 4350, Loss : 0.44887, Acc : 0.809, Sensitive_Loss : 0.11280, Sensitive_Acc : 20.000, Run Time : 8.51 sec
INFO:root:2024-04-11 14:24:06, Train, Epoch : 7, Step : 4360, Loss : 0.36798, Acc : 0.828, Sensitive_Loss : 0.13698, Sensitive_Acc : 23.200, Run Time : 8.85 sec
INFO:root:2024-04-11 14:24:16, Train, Epoch : 7, Step : 4370, Loss : 0.36517, Acc : 0.831, Sensitive_Loss : 0.11073, Sensitive_Acc : 21.700, Run Time : 9.04 sec
INFO:root:2024-04-11 14:24:25, Train, Epoch : 7, Step : 4380, Loss : 0.36085, Acc : 0.847, Sensitive_Loss : 0.12417, Sensitive_Acc : 14.300, Run Time : 9.88 sec
INFO:root:2024-04-11 14:24:34, Train, Epoch : 7, Step : 4390, Loss : 0.35910, Acc : 0.834, Sensitive_Loss : 0.10275, Sensitive_Acc : 22.500, Run Time : 8.79 sec
INFO:root:2024-04-11 14:24:45, Train, Epoch : 7, Step : 4400, Loss : 0.46586, Acc : 0.806, Sensitive_Loss : 0.08757, Sensitive_Acc : 18.400, Run Time : 10.49 sec
INFO:root:2024-04-11 14:26:17, Dev, Step : 4400, Loss : 0.52966, Acc : 0.773, Auc : 0.850, Sensitive_Loss : 0.16772, Sensitive_Acc : 21.571, Sensitive_Auc : 0.998, Mean auc: 0.850, Run Time : 91.96 sec
INFO:root:2024-04-11 14:26:23, Train, Epoch : 7, Step : 4410, Loss : 0.34641, Acc : 0.844, Sensitive_Loss : 0.08130, Sensitive_Acc : 23.600, Run Time : 98.40 sec
INFO:root:2024-04-11 14:26:32, Train, Epoch : 7, Step : 4420, Loss : 0.46275, Acc : 0.784, Sensitive_Loss : 0.10386, Sensitive_Acc : 19.200, Run Time : 9.05 sec
INFO:root:2024-04-11 14:26:41, Train, Epoch : 7, Step : 4430, Loss : 0.40747, Acc : 0.803, Sensitive_Loss : 0.13155, Sensitive_Acc : 24.300, Run Time : 9.26 sec
INFO:root:2024-04-11 14:28:24
INFO:root:y_pred: [0.0740355  0.01871391 0.02881912 ... 0.03862131 0.01925418 0.00901016]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [2.6374739e-05 2.7300323e-06 1.1208011e-03 1.2602631e-02 3.5022311e-02
 1.5819636e-07 8.5473475e-05 4.3405148e-06 2.6526342e-03 9.9844462e-01
 2.7808840e-03 4.6592314e-07 2.8885935e-08 4.7647591e-11 9.9541605e-01
 1.5177818e-03 3.2576897e-06 9.9765599e-01 9.9466562e-01 8.8362631e-06
 4.5464250e-01 4.0655561e-07 2.8324776e-04 3.6372300e-04 1.8847273e-03
 2.0131547e-04 1.8801248e-07 3.7369676e-04 1.6282138e-08 1.3797135e-02
 4.6253681e-05 9.0135258e-01 2.1301323e-04 9.2353255e-01 1.7554148e-09
 1.3258330e-06 1.9554747e-08 6.3680425e-02 6.0603090e-02 9.2581660e-04
 7.3513620e-05 7.7755141e-01 1.9692294e-03 4.2261018e-08 9.8057884e-01
 1.1134808e-01 5.6571512e-06 4.7839809e-02 1.7307280e-02 9.9458295e-01
 9.7833455e-01 9.9181950e-01 8.3347446e-01 5.4786139e-04 6.8795180e-04
 5.3040702e-03 4.9887934e-09 2.2667848e-09 9.8882347e-01 4.1592142e-05
 3.8979699e-05 4.5912741e-03 1.8780713e-03 2.4314292e-07 9.6881223e-01
 1.7315426e-01 2.8506670e-07 1.0515174e-01 1.3055599e-04 9.8262250e-01
 9.9746203e-01 9.9829227e-01 3.1573522e-09 6.3211441e-01 6.0155816e-07
 9.6770458e-02 6.1749211e-05 5.6666698e-12 1.8764306e-08 1.1196858e-05
 2.6725095e-06 3.0766837e-06 9.5406401e-01 9.8575526e-01 1.7180437e-03
 2.1942740e-04 3.7996862e-05 1.1577951e-06 4.2776822e-04 1.6073551e-08
 6.2214986e-06 6.5984521e-03 4.2738427e-10 5.6578674e-12 1.9587408e-04
 5.5404933e-04 2.7606717e-08 7.9829043e-01 9.3937196e-08 4.6904547e-09
 8.5588887e-05 3.4006324e-03 1.1040271e-01 5.8068213e-11 8.1254751e-05
 1.6058650e-03 8.1729308e-07 8.4258056e-01 3.9654688e-04 4.6620155e-03
 4.9740478e-04 9.9879503e-01 9.8964721e-01 1.9725124e-11 4.1200849e-01
 8.5650967e-04 3.9504297e-02 3.2625320e-07 1.4605244e-03 4.6661939e-04
 3.0300678e-06 6.5999957e-09 5.4692659e-06 1.1428841e-10 1.4108716e-04
 3.6652091e-01 3.2119043e-08 9.9708670e-01 7.0516294e-04 9.8659692e-04
 5.4415183e-10 3.3484524e-05 1.2306214e-08]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 14:28:24, Dev, Step : 4438, Loss : 0.53218, Acc : 0.780, Auc : 0.856, Sensitive_Loss : 0.15860, Sensitive_Acc : 21.977, Sensitive_Auc : 0.998, Mean auc: 0.856, Run Time : 94.46 sec
INFO:root:2024-04-11 14:28:27, Train, Epoch : 8, Step : 4440, Loss : 0.07858, Acc : 0.163, Sensitive_Loss : 0.03031, Sensitive_Acc : 5.600, Run Time : 2.78 sec
INFO:root:2024-04-11 14:28:37, Train, Epoch : 8, Step : 4450, Loss : 0.36325, Acc : 0.859, Sensitive_Loss : 0.07395, Sensitive_Acc : 23.500, Run Time : 9.22 sec
INFO:root:2024-04-11 14:28:45, Train, Epoch : 8, Step : 4460, Loss : 0.35292, Acc : 0.850, Sensitive_Loss : 0.11780, Sensitive_Acc : 21.600, Run Time : 8.56 sec
INFO:root:2024-04-11 14:28:53, Train, Epoch : 8, Step : 4470, Loss : 0.35066, Acc : 0.838, Sensitive_Loss : 0.10433, Sensitive_Acc : 20.600, Run Time : 8.10 sec
INFO:root:2024-04-11 14:29:02, Train, Epoch : 8, Step : 4480, Loss : 0.40015, Acc : 0.828, Sensitive_Loss : 0.14470, Sensitive_Acc : 23.900, Run Time : 8.24 sec
INFO:root:2024-04-11 14:29:10, Train, Epoch : 8, Step : 4490, Loss : 0.47919, Acc : 0.800, Sensitive_Loss : 0.10755, Sensitive_Acc : 21.800, Run Time : 8.72 sec
INFO:root:2024-04-11 14:29:19, Train, Epoch : 8, Step : 4500, Loss : 0.38075, Acc : 0.822, Sensitive_Loss : 0.19061, Sensitive_Acc : 17.400, Run Time : 9.10 sec
INFO:root:2024-04-11 14:31:04, Dev, Step : 4500, Loss : 0.52028, Acc : 0.781, Auc : 0.858, Sensitive_Loss : 0.15851, Sensitive_Acc : 21.722, Sensitive_Auc : 1.000, Mean auc: 0.858, Run Time : 104.62 sec
INFO:root:2024-04-11 14:31:05, Best, Step : 4500, Loss : 0.52028, Acc : 0.781, Auc : 0.858, Sensitive_Loss : 0.15851, Sensitive_Acc : 21.722, Sensitive_Auc : 1.000, Best Auc : 0.858
INFO:root:2024-04-11 14:31:12, Train, Epoch : 8, Step : 4510, Loss : 0.29495, Acc : 0.847, Sensitive_Loss : 0.15347, Sensitive_Acc : 25.100, Run Time : 112.59 sec
INFO:root:2024-04-11 14:31:23, Train, Epoch : 8, Step : 4520, Loss : 0.29600, Acc : 0.884, Sensitive_Loss : 0.10091, Sensitive_Acc : 20.000, Run Time : 11.34 sec
INFO:root:2024-04-11 14:31:32, Train, Epoch : 8, Step : 4530, Loss : 0.39752, Acc : 0.863, Sensitive_Loss : 0.13199, Sensitive_Acc : 19.100, Run Time : 9.09 sec
INFO:root:2024-04-11 14:31:41, Train, Epoch : 8, Step : 4540, Loss : 0.38983, Acc : 0.828, Sensitive_Loss : 0.11302, Sensitive_Acc : 19.400, Run Time : 8.35 sec
INFO:root:2024-04-11 14:31:50, Train, Epoch : 8, Step : 4550, Loss : 0.37163, Acc : 0.834, Sensitive_Loss : 0.07018, Sensitive_Acc : 23.100, Run Time : 9.16 sec
INFO:root:2024-04-11 14:31:59, Train, Epoch : 8, Step : 4560, Loss : 0.34585, Acc : 0.847, Sensitive_Loss : 0.09819, Sensitive_Acc : 21.200, Run Time : 9.07 sec
INFO:root:2024-04-11 14:32:08, Train, Epoch : 8, Step : 4570, Loss : 0.34162, Acc : 0.881, Sensitive_Loss : 0.11797, Sensitive_Acc : 25.400, Run Time : 8.75 sec
INFO:root:2024-04-11 14:32:16, Train, Epoch : 8, Step : 4580, Loss : 0.36557, Acc : 0.863, Sensitive_Loss : 0.13174, Sensitive_Acc : 25.400, Run Time : 8.23 sec
INFO:root:2024-04-11 14:32:26, Train, Epoch : 8, Step : 4590, Loss : 0.34281, Acc : 0.828, Sensitive_Loss : 0.08364, Sensitive_Acc : 21.900, Run Time : 9.92 sec
INFO:root:2024-04-11 14:32:36, Train, Epoch : 8, Step : 4600, Loss : 0.39757, Acc : 0.828, Sensitive_Loss : 0.07890, Sensitive_Acc : 22.600, Run Time : 9.91 sec
INFO:root:2024-04-11 14:34:13, Dev, Step : 4600, Loss : 0.51649, Acc : 0.781, Auc : 0.857, Sensitive_Loss : 0.17323, Sensitive_Acc : 21.707, Sensitive_Auc : 1.000, Mean auc: 0.857, Run Time : 96.89 sec
INFO:root:2024-04-11 14:34:19, Train, Epoch : 8, Step : 4610, Loss : 0.42520, Acc : 0.819, Sensitive_Loss : 0.09287, Sensitive_Acc : 22.000, Run Time : 103.03 sec
INFO:root:2024-04-11 14:34:27, Train, Epoch : 8, Step : 4620, Loss : 0.39798, Acc : 0.841, Sensitive_Loss : 0.11128, Sensitive_Acc : 23.100, Run Time : 8.10 sec
INFO:root:2024-04-11 14:34:35, Train, Epoch : 8, Step : 4630, Loss : 0.39134, Acc : 0.812, Sensitive_Loss : 0.09584, Sensitive_Acc : 24.000, Run Time : 8.09 sec
INFO:root:2024-04-11 14:34:44, Train, Epoch : 8, Step : 4640, Loss : 0.31423, Acc : 0.878, Sensitive_Loss : 0.09350, Sensitive_Acc : 19.700, Run Time : 8.76 sec
INFO:root:2024-04-11 14:34:52, Train, Epoch : 8, Step : 4650, Loss : 0.33520, Acc : 0.844, Sensitive_Loss : 0.11131, Sensitive_Acc : 24.200, Run Time : 8.29 sec
INFO:root:2024-04-11 14:35:00, Train, Epoch : 8, Step : 4660, Loss : 0.35768, Acc : 0.841, Sensitive_Loss : 0.09811, Sensitive_Acc : 24.200, Run Time : 8.16 sec
INFO:root:2024-04-11 14:35:08, Train, Epoch : 8, Step : 4670, Loss : 0.31418, Acc : 0.863, Sensitive_Loss : 0.10413, Sensitive_Acc : 19.900, Run Time : 7.84 sec
INFO:root:2024-04-11 14:35:17, Train, Epoch : 8, Step : 4680, Loss : 0.40853, Acc : 0.794, Sensitive_Loss : 0.11913, Sensitive_Acc : 21.000, Run Time : 8.47 sec
INFO:root:2024-04-11 14:35:25, Train, Epoch : 8, Step : 4690, Loss : 0.35954, Acc : 0.853, Sensitive_Loss : 0.11595, Sensitive_Acc : 18.000, Run Time : 8.47 sec
INFO:root:2024-04-11 14:35:33, Train, Epoch : 8, Step : 4700, Loss : 0.37730, Acc : 0.850, Sensitive_Loss : 0.12524, Sensitive_Acc : 20.700, Run Time : 8.06 sec
INFO:root:2024-04-11 14:37:06, Dev, Step : 4700, Loss : 0.53597, Acc : 0.774, Auc : 0.853, Sensitive_Loss : 0.16756, Sensitive_Acc : 21.571, Sensitive_Auc : 1.000, Mean auc: 0.853, Run Time : 92.67 sec
INFO:root:2024-04-11 14:37:12, Train, Epoch : 8, Step : 4710, Loss : 0.41875, Acc : 0.797, Sensitive_Loss : 0.13799, Sensitive_Acc : 22.900, Run Time : 98.87 sec
INFO:root:2024-04-11 14:37:22, Train, Epoch : 8, Step : 4720, Loss : 0.31545, Acc : 0.884, Sensitive_Loss : 0.10815, Sensitive_Acc : 18.800, Run Time : 10.19 sec
INFO:root:2024-04-11 14:37:30, Train, Epoch : 8, Step : 4730, Loss : 0.37158, Acc : 0.838, Sensitive_Loss : 0.12372, Sensitive_Acc : 24.300, Run Time : 8.12 sec
INFO:root:2024-04-11 14:37:39, Train, Epoch : 8, Step : 4740, Loss : 0.33264, Acc : 0.847, Sensitive_Loss : 0.15905, Sensitive_Acc : 21.800, Run Time : 8.62 sec
INFO:root:2024-04-11 14:37:47, Train, Epoch : 8, Step : 4750, Loss : 0.37509, Acc : 0.850, Sensitive_Loss : 0.12502, Sensitive_Acc : 21.200, Run Time : 8.57 sec
INFO:root:2024-04-11 14:37:55, Train, Epoch : 8, Step : 4760, Loss : 0.35697, Acc : 0.838, Sensitive_Loss : 0.08543, Sensitive_Acc : 23.100, Run Time : 7.87 sec
INFO:root:2024-04-11 14:38:03, Train, Epoch : 8, Step : 4770, Loss : 0.38979, Acc : 0.822, Sensitive_Loss : 0.11876, Sensitive_Acc : 18.400, Run Time : 7.81 sec
INFO:root:2024-04-11 14:38:12, Train, Epoch : 8, Step : 4780, Loss : 0.35525, Acc : 0.863, Sensitive_Loss : 0.09458, Sensitive_Acc : 21.400, Run Time : 8.57 sec
INFO:root:2024-04-11 14:38:20, Train, Epoch : 8, Step : 4790, Loss : 0.32175, Acc : 0.853, Sensitive_Loss : 0.08763, Sensitive_Acc : 19.700, Run Time : 8.19 sec
INFO:root:2024-04-11 14:38:28, Train, Epoch : 8, Step : 4800, Loss : 0.40852, Acc : 0.856, Sensitive_Loss : 0.09927, Sensitive_Acc : 22.900, Run Time : 8.42 sec
INFO:root:2024-04-11 14:40:01, Dev, Step : 4800, Loss : 0.51637, Acc : 0.780, Auc : 0.856, Sensitive_Loss : 0.16525, Sensitive_Acc : 21.571, Sensitive_Auc : 1.000, Mean auc: 0.856, Run Time : 92.44 sec
INFO:root:2024-04-11 14:40:07, Train, Epoch : 8, Step : 4810, Loss : 0.41451, Acc : 0.816, Sensitive_Loss : 0.10259, Sensitive_Acc : 22.500, Run Time : 98.24 sec
INFO:root:2024-04-11 14:40:15, Train, Epoch : 8, Step : 4820, Loss : 0.37166, Acc : 0.841, Sensitive_Loss : 0.11524, Sensitive_Acc : 24.000, Run Time : 8.14 sec
INFO:root:2024-04-11 14:40:22, Train, Epoch : 8, Step : 4830, Loss : 0.35546, Acc : 0.828, Sensitive_Loss : 0.08847, Sensitive_Acc : 24.500, Run Time : 7.77 sec
INFO:root:2024-04-11 14:40:31, Train, Epoch : 8, Step : 4840, Loss : 0.42390, Acc : 0.819, Sensitive_Loss : 0.11634, Sensitive_Acc : 24.100, Run Time : 8.65 sec
INFO:root:2024-04-11 14:40:40, Train, Epoch : 8, Step : 4850, Loss : 0.40408, Acc : 0.847, Sensitive_Loss : 0.11751, Sensitive_Acc : 22.200, Run Time : 8.64 sec
INFO:root:2024-04-11 14:40:48, Train, Epoch : 8, Step : 4860, Loss : 0.44920, Acc : 0.809, Sensitive_Loss : 0.10734, Sensitive_Acc : 20.200, Run Time : 8.14 sec
INFO:root:2024-04-11 14:40:56, Train, Epoch : 8, Step : 4870, Loss : 0.42579, Acc : 0.784, Sensitive_Loss : 0.09057, Sensitive_Acc : 20.100, Run Time : 7.82 sec
INFO:root:2024-04-11 14:41:04, Train, Epoch : 8, Step : 4880, Loss : 0.36847, Acc : 0.856, Sensitive_Loss : 0.13537, Sensitive_Acc : 20.000, Run Time : 8.42 sec
INFO:root:2024-04-11 14:41:12, Train, Epoch : 8, Step : 4890, Loss : 0.40108, Acc : 0.822, Sensitive_Loss : 0.12577, Sensitive_Acc : 18.400, Run Time : 8.33 sec
INFO:root:2024-04-11 14:41:21, Train, Epoch : 8, Step : 4900, Loss : 0.36214, Acc : 0.841, Sensitive_Loss : 0.14086, Sensitive_Acc : 23.400, Run Time : 8.82 sec
INFO:root:2024-04-11 14:42:57, Dev, Step : 4900, Loss : 0.51361, Acc : 0.777, Auc : 0.856, Sensitive_Loss : 0.14907, Sensitive_Acc : 21.571, Sensitive_Auc : 1.000, Mean auc: 0.856, Run Time : 95.33 sec
INFO:root:2024-04-11 14:43:03, Train, Epoch : 8, Step : 4910, Loss : 0.31697, Acc : 0.853, Sensitive_Loss : 0.09100, Sensitive_Acc : 20.900, Run Time : 101.26 sec
INFO:root:2024-04-11 14:43:11, Train, Epoch : 8, Step : 4920, Loss : 0.40007, Acc : 0.844, Sensitive_Loss : 0.13628, Sensitive_Acc : 24.300, Run Time : 8.52 sec
INFO:root:2024-04-11 14:43:20, Train, Epoch : 8, Step : 4930, Loss : 0.40844, Acc : 0.841, Sensitive_Loss : 0.11157, Sensitive_Acc : 20.900, Run Time : 9.45 sec
INFO:root:2024-04-11 14:43:29, Train, Epoch : 8, Step : 4940, Loss : 0.37501, Acc : 0.841, Sensitive_Loss : 0.10181, Sensitive_Acc : 23.700, Run Time : 8.41 sec
INFO:root:2024-04-11 14:43:37, Train, Epoch : 8, Step : 4950, Loss : 0.39153, Acc : 0.825, Sensitive_Loss : 0.12705, Sensitive_Acc : 21.900, Run Time : 7.95 sec
INFO:root:2024-04-11 14:43:45, Train, Epoch : 8, Step : 4960, Loss : 0.46669, Acc : 0.784, Sensitive_Loss : 0.13946, Sensitive_Acc : 23.500, Run Time : 8.30 sec
INFO:root:2024-04-11 14:43:54, Train, Epoch : 8, Step : 4970, Loss : 0.34738, Acc : 0.859, Sensitive_Loss : 0.11376, Sensitive_Acc : 23.700, Run Time : 8.84 sec
INFO:root:2024-04-11 14:44:02, Train, Epoch : 8, Step : 4980, Loss : 0.40733, Acc : 0.825, Sensitive_Loss : 0.06979, Sensitive_Acc : 23.600, Run Time : 8.21 sec
INFO:root:2024-04-11 14:44:11, Train, Epoch : 8, Step : 4990, Loss : 0.37923, Acc : 0.856, Sensitive_Loss : 0.13470, Sensitive_Acc : 25.500, Run Time : 8.60 sec
INFO:root:2024-04-11 14:44:20, Train, Epoch : 8, Step : 5000, Loss : 0.39424, Acc : 0.822, Sensitive_Loss : 0.11159, Sensitive_Acc : 22.700, Run Time : 9.08 sec
INFO:root:2024-04-11 14:45:54, Dev, Step : 5000, Loss : 0.53023, Acc : 0.779, Auc : 0.854, Sensitive_Loss : 0.15698, Sensitive_Acc : 21.722, Sensitive_Auc : 0.999, Mean auc: 0.854, Run Time : 94.03 sec
INFO:root:2024-04-11 14:46:00, Train, Epoch : 8, Step : 5010, Loss : 0.38950, Acc : 0.853, Sensitive_Loss : 0.11106, Sensitive_Acc : 24.000, Run Time : 100.28 sec
INFO:root:2024-04-11 14:46:11, Train, Epoch : 8, Step : 5020, Loss : 0.34591, Acc : 0.831, Sensitive_Loss : 0.06423, Sensitive_Acc : 17.400, Run Time : 10.57 sec
INFO:root:2024-04-11 14:46:21, Train, Epoch : 8, Step : 5030, Loss : 0.32598, Acc : 0.863, Sensitive_Loss : 0.20666, Sensitive_Acc : 20.500, Run Time : 10.49 sec
INFO:root:2024-04-11 14:46:30, Train, Epoch : 8, Step : 5040, Loss : 0.41093, Acc : 0.841, Sensitive_Loss : 0.11128, Sensitive_Acc : 19.500, Run Time : 8.39 sec
INFO:root:2024-04-11 14:46:39, Train, Epoch : 8, Step : 5050, Loss : 0.35503, Acc : 0.850, Sensitive_Loss : 0.10362, Sensitive_Acc : 20.400, Run Time : 9.16 sec
INFO:root:2024-04-11 14:46:48, Train, Epoch : 8, Step : 5060, Loss : 0.40514, Acc : 0.828, Sensitive_Loss : 0.13506, Sensitive_Acc : 18.200, Run Time : 8.75 sec
INFO:root:2024-04-11 14:46:56, Train, Epoch : 8, Step : 5070, Loss : 0.34485, Acc : 0.853, Sensitive_Loss : 0.08270, Sensitive_Acc : 20.400, Run Time : 8.39 sec
INFO:root:2024-04-11 14:48:33
INFO:root:y_pred: [0.04427926 0.02398935 0.00861313 ... 0.02704052 0.00805093 0.00861922]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.86398684e-05 1.02602709e-04 3.00372741e-03 6.77799620e-03
 1.75637603e-01 1.96391670e-06 3.12234566e-04 1.82635085e-05
 3.47904228e-02 9.98614669e-01 7.92583264e-03 1.23736913e-06
 2.55764423e-08 9.41768352e-09 9.95937467e-01 5.70387067e-03
 1.55534308e-05 9.98151124e-01 9.95994925e-01 2.84605572e-04
 7.89053738e-01 6.22713151e-06 1.78819479e-04 1.71775333e-04
 1.06054582e-01 1.43479444e-02 7.01581826e-07 1.01279968e-03
 7.86365284e-09 1.06016435e-02 6.05055393e-05 9.54076290e-01
 3.62200692e-04 9.64050472e-01 2.19441851e-08 3.11013446e-06
 7.81521052e-08 9.92492884e-02 1.47685170e-01 1.32847968e-02
 4.88102029e-04 9.12129939e-01 2.03895383e-03 3.08913195e-06
 9.78598177e-01 4.26197685e-02 1.32796149e-05 2.06344649e-01
 1.66864563e-02 9.96901512e-01 9.88491654e-01 9.95296776e-01
 9.30069149e-01 1.06901359e-02 1.38775529e-02 4.10421472e-03
 1.89441867e-08 3.83904641e-08 9.93361115e-01 5.17346816e-05
 6.66413616e-05 1.10452613e-02 5.29209152e-03 7.68380175e-07
 9.71002579e-01 2.19008267e-01 6.49937874e-07 8.34243819e-02
 8.60705448e-04 9.92388308e-01 9.98802543e-01 9.98938382e-01
 4.54600624e-09 7.36971796e-01 7.43986038e-06 3.27756137e-01
 3.14091012e-04 5.92043636e-11 3.57795216e-07 1.36342016e-03
 6.74997500e-05 7.40977193e-05 9.75137889e-01 9.88666177e-01
 1.21336419e-03 4.54685272e-04 1.91003215e-04 2.25749085e-04
 1.71664066e-03 1.11377517e-06 3.64488769e-05 6.29198318e-03
 3.18933435e-09 4.77764564e-11 1.01379723e-04 2.68334290e-03
 7.33211664e-07 9.29352224e-01 7.35965295e-06 1.80597026e-09
 9.94927483e-04 6.74654590e-03 2.57885516e-01 3.21611449e-09
 2.66773510e-04 3.30763077e-03 1.93899200e-06 8.57757568e-01
 5.22081973e-04 4.81551699e-03 1.50223111e-03 9.99206364e-01
 9.93595779e-01 2.08829976e-09 5.83501518e-01 1.63595018e-03
 5.95693067e-02 9.47634908e-06 6.59003435e-03 7.27426261e-04
 3.39319522e-05 6.95441358e-08 1.09827539e-04 7.35510719e-09
 9.98839241e-05 4.22920734e-01 1.17384673e-07 9.98004377e-01
 2.60601053e-04 8.67351703e-03 1.36865843e-08 3.09214141e-04
 3.80043389e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 14:48:33, Dev, Step : 5072, Loss : 0.55156, Acc : 0.771, Auc : 0.852, Sensitive_Loss : 0.15252, Sensitive_Acc : 21.722, Sensitive_Auc : 1.000, Mean auc: 0.852, Run Time : 94.21 sec
INFO:root:2024-04-11 14:48:41, Train, Epoch : 9, Step : 5080, Loss : 0.27098, Acc : 0.691, Sensitive_Loss : 0.05041, Sensitive_Acc : 18.600, Run Time : 7.64 sec
INFO:root:2024-04-11 14:48:50, Train, Epoch : 9, Step : 5090, Loss : 0.40447, Acc : 0.847, Sensitive_Loss : 0.08201, Sensitive_Acc : 23.400, Run Time : 8.24 sec
INFO:root:2024-04-11 14:49:00, Train, Epoch : 9, Step : 5100, Loss : 0.40929, Acc : 0.841, Sensitive_Loss : 0.16636, Sensitive_Acc : 23.200, Run Time : 10.09 sec
INFO:root:2024-04-11 14:50:31, Dev, Step : 5100, Loss : 0.57029, Acc : 0.762, Auc : 0.848, Sensitive_Loss : 0.15778, Sensitive_Acc : 21.722, Sensitive_Auc : 1.000, Mean auc: 0.848, Run Time : 91.53 sec
INFO:root:2024-04-11 14:50:37, Train, Epoch : 9, Step : 5110, Loss : 0.37974, Acc : 0.844, Sensitive_Loss : 0.12553, Sensitive_Acc : 25.800, Run Time : 97.32 sec
INFO:root:2024-04-11 14:50:45, Train, Epoch : 9, Step : 5120, Loss : 0.33527, Acc : 0.875, Sensitive_Loss : 0.12318, Sensitive_Acc : 22.800, Run Time : 8.19 sec
INFO:root:2024-04-11 14:50:53, Train, Epoch : 9, Step : 5130, Loss : 0.40353, Acc : 0.812, Sensitive_Loss : 0.08040, Sensitive_Acc : 13.500, Run Time : 7.70 sec
INFO:root:2024-04-11 14:51:01, Train, Epoch : 9, Step : 5140, Loss : 0.36887, Acc : 0.838, Sensitive_Loss : 0.09804, Sensitive_Acc : 24.200, Run Time : 7.97 sec
INFO:root:2024-04-11 14:51:09, Train, Epoch : 9, Step : 5150, Loss : 0.40794, Acc : 0.819, Sensitive_Loss : 0.11343, Sensitive_Acc : 21.500, Run Time : 8.27 sec
INFO:root:2024-04-11 14:51:18, Train, Epoch : 9, Step : 5160, Loss : 0.31785, Acc : 0.856, Sensitive_Loss : 0.12814, Sensitive_Acc : 20.900, Run Time : 8.73 sec
INFO:root:2024-04-11 14:51:26, Train, Epoch : 9, Step : 5170, Loss : 0.34342, Acc : 0.828, Sensitive_Loss : 0.11701, Sensitive_Acc : 22.900, Run Time : 8.26 sec
INFO:root:2024-04-11 14:51:36, Train, Epoch : 9, Step : 5180, Loss : 0.36219, Acc : 0.838, Sensitive_Loss : 0.07939, Sensitive_Acc : 21.900, Run Time : 9.47 sec
INFO:root:2024-04-11 14:51:44, Train, Epoch : 9, Step : 5190, Loss : 0.31531, Acc : 0.859, Sensitive_Loss : 0.10169, Sensitive_Acc : 22.700, Run Time : 8.23 sec
INFO:root:2024-04-11 14:51:52, Train, Epoch : 9, Step : 5200, Loss : 0.33778, Acc : 0.844, Sensitive_Loss : 0.08786, Sensitive_Acc : 22.800, Run Time : 8.16 sec
INFO:root:2024-04-11 14:53:24, Dev, Step : 5200, Loss : 0.54584, Acc : 0.773, Auc : 0.853, Sensitive_Loss : 0.15330, Sensitive_Acc : 21.571, Sensitive_Auc : 0.999, Mean auc: 0.853, Run Time : 91.81 sec
INFO:root:2024-04-11 14:53:30, Train, Epoch : 9, Step : 5210, Loss : 0.32820, Acc : 0.869, Sensitive_Loss : 0.13501, Sensitive_Acc : 18.800, Run Time : 98.03 sec
INFO:root:2024-04-11 14:53:39, Train, Epoch : 9, Step : 5220, Loss : 0.39083, Acc : 0.853, Sensitive_Loss : 0.09133, Sensitive_Acc : 21.400, Run Time : 9.37 sec
INFO:root:2024-04-11 14:53:48, Train, Epoch : 9, Step : 5230, Loss : 0.35957, Acc : 0.859, Sensitive_Loss : 0.08232, Sensitive_Acc : 23.700, Run Time : 8.77 sec
INFO:root:2024-04-11 14:53:57, Train, Epoch : 9, Step : 5240, Loss : 0.35656, Acc : 0.828, Sensitive_Loss : 0.10239, Sensitive_Acc : 22.000, Run Time : 8.58 sec
INFO:root:2024-04-11 14:54:05, Train, Epoch : 9, Step : 5250, Loss : 0.34245, Acc : 0.847, Sensitive_Loss : 0.09058, Sensitive_Acc : 22.600, Run Time : 8.34 sec
INFO:root:2024-04-11 14:54:13, Train, Epoch : 9, Step : 5260, Loss : 0.38226, Acc : 0.838, Sensitive_Loss : 0.14419, Sensitive_Acc : 23.600, Run Time : 8.45 sec
INFO:root:2024-04-11 14:54:22, Train, Epoch : 9, Step : 5270, Loss : 0.37203, Acc : 0.831, Sensitive_Loss : 0.10713, Sensitive_Acc : 23.300, Run Time : 8.38 sec
INFO:root:2024-04-11 14:54:30, Train, Epoch : 9, Step : 5280, Loss : 0.45073, Acc : 0.772, Sensitive_Loss : 0.11184, Sensitive_Acc : 21.700, Run Time : 8.15 sec
INFO:root:2024-04-11 14:54:38, Train, Epoch : 9, Step : 5290, Loss : 0.41702, Acc : 0.819, Sensitive_Loss : 0.10611, Sensitive_Acc : 19.400, Run Time : 8.06 sec
INFO:root:2024-04-11 14:54:46, Train, Epoch : 9, Step : 5300, Loss : 0.29915, Acc : 0.900, Sensitive_Loss : 0.07566, Sensitive_Acc : 16.800, Run Time : 7.79 sec
INFO:root:2024-04-11 14:56:19, Dev, Step : 5300, Loss : 0.54745, Acc : 0.773, Auc : 0.851, Sensitive_Loss : 0.15267, Sensitive_Acc : 21.707, Sensitive_Auc : 1.000, Mean auc: 0.851, Run Time : 92.85 sec
INFO:root:2024-04-11 14:56:25, Train, Epoch : 9, Step : 5310, Loss : 0.37244, Acc : 0.841, Sensitive_Loss : 0.10011, Sensitive_Acc : 23.500, Run Time : 98.96 sec
INFO:root:2024-04-11 14:56:33, Train, Epoch : 9, Step : 5320, Loss : 0.35884, Acc : 0.853, Sensitive_Loss : 0.08643, Sensitive_Acc : 19.100, Run Time : 8.34 sec
INFO:root:2024-04-11 14:56:41, Train, Epoch : 9, Step : 5330, Loss : 0.29144, Acc : 0.859, Sensitive_Loss : 0.13034, Sensitive_Acc : 21.800, Run Time : 7.99 sec
INFO:root:2024-04-11 14:56:51, Train, Epoch : 9, Step : 5340, Loss : 0.33973, Acc : 0.844, Sensitive_Loss : 0.11462, Sensitive_Acc : 21.600, Run Time : 9.38 sec
INFO:root:2024-04-11 14:56:59, Train, Epoch : 9, Step : 5350, Loss : 0.41034, Acc : 0.850, Sensitive_Loss : 0.08149, Sensitive_Acc : 21.200, Run Time : 8.20 sec
INFO:root:2024-04-11 14:57:07, Train, Epoch : 9, Step : 5360, Loss : 0.33030, Acc : 0.869, Sensitive_Loss : 0.09145, Sensitive_Acc : 20.600, Run Time : 8.69 sec
INFO:root:2024-04-11 14:57:15, Train, Epoch : 9, Step : 5370, Loss : 0.34879, Acc : 0.831, Sensitive_Loss : 0.07526, Sensitive_Acc : 23.300, Run Time : 8.00 sec
INFO:root:2024-04-11 14:57:24, Train, Epoch : 9, Step : 5380, Loss : 0.32690, Acc : 0.853, Sensitive_Loss : 0.10390, Sensitive_Acc : 23.000, Run Time : 8.73 sec
INFO:root:2024-04-11 14:57:33, Train, Epoch : 9, Step : 5390, Loss : 0.39895, Acc : 0.825, Sensitive_Loss : 0.10502, Sensitive_Acc : 26.600, Run Time : 8.70 sec
INFO:root:2024-04-11 14:57:41, Train, Epoch : 9, Step : 5400, Loss : 0.37146, Acc : 0.847, Sensitive_Loss : 0.10213, Sensitive_Acc : 24.000, Run Time : 7.88 sec
INFO:root:2024-04-11 14:59:12, Dev, Step : 5400, Loss : 0.54306, Acc : 0.777, Auc : 0.852, Sensitive_Loss : 0.15896, Sensitive_Acc : 21.707, Sensitive_Auc : 0.999, Mean auc: 0.852, Run Time : 91.13 sec
INFO:root:2024-04-11 14:59:18, Train, Epoch : 9, Step : 5410, Loss : 0.30890, Acc : 0.859, Sensitive_Loss : 0.07027, Sensitive_Acc : 22.600, Run Time : 97.39 sec
INFO:root:2024-04-11 14:59:26, Train, Epoch : 9, Step : 5420, Loss : 0.40446, Acc : 0.841, Sensitive_Loss : 0.17341, Sensitive_Acc : 20.100, Run Time : 7.93 sec
INFO:root:2024-04-11 14:59:35, Train, Epoch : 9, Step : 5430, Loss : 0.35862, Acc : 0.853, Sensitive_Loss : 0.08679, Sensitive_Acc : 17.600, Run Time : 8.45 sec
INFO:root:2024-04-11 14:59:43, Train, Epoch : 9, Step : 5440, Loss : 0.32022, Acc : 0.853, Sensitive_Loss : 0.11949, Sensitive_Acc : 25.500, Run Time : 8.74 sec
INFO:root:2024-04-11 14:59:52, Train, Epoch : 9, Step : 5450, Loss : 0.28767, Acc : 0.878, Sensitive_Loss : 0.12000, Sensitive_Acc : 24.200, Run Time : 8.51 sec
INFO:root:2024-04-11 15:00:00, Train, Epoch : 9, Step : 5460, Loss : 0.32830, Acc : 0.872, Sensitive_Loss : 0.08629, Sensitive_Acc : 21.800, Run Time : 8.31 sec
INFO:root:2024-04-11 15:00:08, Train, Epoch : 9, Step : 5470, Loss : 0.40443, Acc : 0.819, Sensitive_Loss : 0.09976, Sensitive_Acc : 14.100, Run Time : 7.99 sec
INFO:root:2024-04-11 15:00:16, Train, Epoch : 9, Step : 5480, Loss : 0.38017, Acc : 0.828, Sensitive_Loss : 0.11635, Sensitive_Acc : 20.900, Run Time : 8.12 sec
INFO:root:2024-04-11 15:00:24, Train, Epoch : 9, Step : 5490, Loss : 0.37921, Acc : 0.800, Sensitive_Loss : 0.12079, Sensitive_Acc : 22.600, Run Time : 8.02 sec
INFO:root:2024-04-11 15:00:34, Train, Epoch : 9, Step : 5500, Loss : 0.34868, Acc : 0.847, Sensitive_Loss : 0.07455, Sensitive_Acc : 23.300, Run Time : 9.45 sec
INFO:root:2024-04-11 15:02:06, Dev, Step : 5500, Loss : 0.53795, Acc : 0.773, Auc : 0.850, Sensitive_Loss : 0.16001, Sensitive_Acc : 21.722, Sensitive_Auc : 1.000, Mean auc: 0.850, Run Time : 92.17 sec
INFO:root:2024-04-11 15:02:12, Train, Epoch : 9, Step : 5510, Loss : 0.35372, Acc : 0.822, Sensitive_Loss : 0.09715, Sensitive_Acc : 24.000, Run Time : 98.04 sec
INFO:root:2024-04-11 15:02:20, Train, Epoch : 9, Step : 5520, Loss : 0.33158, Acc : 0.859, Sensitive_Loss : 0.11537, Sensitive_Acc : 23.700, Run Time : 8.50 sec
INFO:root:2024-04-11 15:02:30, Train, Epoch : 9, Step : 5530, Loss : 0.34120, Acc : 0.853, Sensitive_Loss : 0.11520, Sensitive_Acc : 23.500, Run Time : 9.56 sec
INFO:root:2024-04-11 15:02:39, Train, Epoch : 9, Step : 5540, Loss : 0.40886, Acc : 0.816, Sensitive_Loss : 0.10696, Sensitive_Acc : 26.000, Run Time : 9.13 sec
INFO:root:2024-04-11 15:02:47, Train, Epoch : 9, Step : 5550, Loss : 0.35760, Acc : 0.866, Sensitive_Loss : 0.11145, Sensitive_Acc : 21.900, Run Time : 7.73 sec
INFO:root:2024-04-11 15:02:55, Train, Epoch : 9, Step : 5560, Loss : 0.33770, Acc : 0.850, Sensitive_Loss : 0.10434, Sensitive_Acc : 22.100, Run Time : 8.35 sec
INFO:root:2024-04-11 15:03:04, Train, Epoch : 9, Step : 5570, Loss : 0.37475, Acc : 0.872, Sensitive_Loss : 0.09267, Sensitive_Acc : 19.900, Run Time : 8.57 sec
INFO:root:2024-04-11 15:03:12, Train, Epoch : 9, Step : 5580, Loss : 0.34527, Acc : 0.863, Sensitive_Loss : 0.12900, Sensitive_Acc : 19.400, Run Time : 8.27 sec
INFO:root:2024-04-11 15:03:20, Train, Epoch : 9, Step : 5590, Loss : 0.36981, Acc : 0.828, Sensitive_Loss : 0.10912, Sensitive_Acc : 22.900, Run Time : 8.42 sec
INFO:root:2024-04-11 15:03:28, Train, Epoch : 9, Step : 5600, Loss : 0.33140, Acc : 0.838, Sensitive_Loss : 0.09466, Sensitive_Acc : 20.700, Run Time : 8.00 sec
INFO:root:2024-04-11 15:05:00, Dev, Step : 5600, Loss : 0.57880, Acc : 0.771, Auc : 0.852, Sensitive_Loss : 0.17615, Sensitive_Acc : 21.571, Sensitive_Auc : 0.999, Mean auc: 0.852, Run Time : 91.95 sec
INFO:root:2024-04-11 15:05:06, Train, Epoch : 9, Step : 5610, Loss : 0.31687, Acc : 0.822, Sensitive_Loss : 0.12813, Sensitive_Acc : 23.600, Run Time : 97.68 sec
INFO:root:2024-04-11 15:05:14, Train, Epoch : 9, Step : 5620, Loss : 0.40734, Acc : 0.819, Sensitive_Loss : 0.06862, Sensitive_Acc : 22.200, Run Time : 8.07 sec
INFO:root:2024-04-11 15:05:22, Train, Epoch : 9, Step : 5630, Loss : 0.37776, Acc : 0.856, Sensitive_Loss : 0.10021, Sensitive_Acc : 20.100, Run Time : 7.68 sec
INFO:root:2024-04-11 15:05:30, Train, Epoch : 9, Step : 5640, Loss : 0.28106, Acc : 0.887, Sensitive_Loss : 0.06696, Sensitive_Acc : 23.900, Run Time : 8.29 sec
INFO:root:2024-04-11 15:05:38, Train, Epoch : 9, Step : 5650, Loss : 0.34865, Acc : 0.869, Sensitive_Loss : 0.07319, Sensitive_Acc : 23.800, Run Time : 8.17 sec
INFO:root:2024-04-11 15:05:46, Train, Epoch : 9, Step : 5660, Loss : 0.35600, Acc : 0.863, Sensitive_Loss : 0.10063, Sensitive_Acc : 21.200, Run Time : 8.17 sec
INFO:root:2024-04-11 15:05:56, Train, Epoch : 9, Step : 5670, Loss : 0.32390, Acc : 0.872, Sensitive_Loss : 0.10306, Sensitive_Acc : 24.000, Run Time : 9.45 sec
INFO:root:2024-04-11 15:06:04, Train, Epoch : 9, Step : 5680, Loss : 0.35482, Acc : 0.838, Sensitive_Loss : 0.08999, Sensitive_Acc : 24.500, Run Time : 8.59 sec
INFO:root:2024-04-11 15:06:12, Train, Epoch : 9, Step : 5690, Loss : 0.30045, Acc : 0.875, Sensitive_Loss : 0.13643, Sensitive_Acc : 23.600, Run Time : 7.82 sec
INFO:root:2024-04-11 15:06:20, Train, Epoch : 9, Step : 5700, Loss : 0.35349, Acc : 0.856, Sensitive_Loss : 0.10621, Sensitive_Acc : 21.100, Run Time : 8.25 sec
INFO:root:2024-04-11 15:07:54, Dev, Step : 5700, Loss : 0.53528, Acc : 0.777, Auc : 0.854, Sensitive_Loss : 0.14985, Sensitive_Acc : 21.857, Sensitive_Auc : 1.000, Mean auc: 0.854, Run Time : 93.42 sec
INFO:root:2024-04-11 15:09:25
INFO:root:y_pred: [0.18662046 0.01072369 0.0655405  ... 0.0384039  0.00577348 0.00468077]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.32861387e-05 1.39903591e-06 1.29505803e-04 8.39573424e-03
 2.23590471e-02 8.49411094e-07 5.12281758e-06 6.53791722e-06
 1.82260526e-03 9.98682082e-01 5.53504389e-04 2.96302915e-07
 2.16617799e-08 3.86097015e-10 9.95039284e-01 3.26293311e-03
 2.28614385e-06 9.97929096e-01 9.95378852e-01 1.14301683e-05
 6.55589640e-01 8.69012240e-07 9.31077448e-05 3.11596559e-05
 3.56358406e-03 3.88685701e-04 9.15424266e-07 5.98451181e-04
 1.11690775e-10 3.32669239e-03 7.90766478e-07 9.47843015e-01
 2.07240693e-04 9.45596278e-01 1.02181341e-09 1.00043383e-06
 1.49367663e-09 7.81806633e-02 9.29056928e-02 4.38842643e-03
 5.66347298e-05 8.16907287e-01 5.53599908e-04 3.68788555e-08
 9.85768139e-01 4.83183516e-03 1.43302725e-06 4.30848449e-02
 5.36621176e-03 9.95584548e-01 9.88838792e-01 9.92635071e-01
 9.21497464e-01 5.35761414e-04 5.78501844e-04 9.89654684e-04
 1.99981892e-10 2.28858466e-09 9.90328491e-01 7.35813683e-06
 6.33668969e-06 2.87813996e-03 5.85077738e-04 2.48546911e-07
 9.63434756e-01 1.37602031e-01 9.50216617e-08 4.63149585e-02
 6.49559661e-05 9.86378193e-01 9.98605907e-01 9.98337984e-01
 2.67821876e-10 5.45853794e-01 3.42045780e-07 5.97283728e-02
 1.12052330e-05 6.60202145e-13 1.88089956e-07 6.19371058e-05
 1.10612846e-05 8.27565145e-06 9.68608439e-01 9.86971438e-01
 4.01706551e-04 9.59958052e-05 6.59511279e-05 6.28886141e-07
 6.43965686e-05 1.93381773e-08 5.83306019e-06 7.37933966e-04
 2.90548252e-10 1.06548331e-12 4.76607383e-05 7.55833695e-04
 3.84936127e-09 9.03824449e-01 2.92900609e-07 9.88062299e-10
 1.18025731e-04 5.49527351e-04 1.70016304e-01 1.38985707e-11
 7.99826012e-05 2.06430952e-04 1.12628084e-06 8.09588015e-01
 5.39837929e-05 2.32969224e-03 6.40445098e-04 9.98976231e-01
 9.91907477e-01 2.19127255e-12 4.39051062e-01 6.04342640e-05
 8.16502143e-03 3.54736045e-07 9.14950389e-03 2.73965532e-04
 2.33630863e-06 7.42390061e-09 2.57083666e-05 4.38835718e-10
 3.62192418e-06 2.36510545e-01 2.04755786e-08 9.97348309e-01
 2.20447073e-05 6.80320896e-03 1.27958035e-08 3.89243178e-05
 3.39084920e-08]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 15:09:25, Dev, Step : 5706, Loss : 0.54275, Acc : 0.777, Auc : 0.853, Sensitive_Loss : 0.15155, Sensitive_Acc : 21.857, Sensitive_Auc : 1.000, Mean auc: 0.853, Run Time : 88.30 sec
INFO:root:2024-04-11 15:09:30, Train, Epoch : 10, Step : 5710, Loss : 0.14557, Acc : 0.328, Sensitive_Loss : 0.04787, Sensitive_Acc : 10.700, Run Time : 4.32 sec
INFO:root:2024-04-11 15:09:38, Train, Epoch : 10, Step : 5720, Loss : 0.36331, Acc : 0.844, Sensitive_Loss : 0.12551, Sensitive_Acc : 21.900, Run Time : 8.21 sec
INFO:root:2024-04-11 15:09:47, Train, Epoch : 10, Step : 5730, Loss : 0.32476, Acc : 0.866, Sensitive_Loss : 0.07606, Sensitive_Acc : 22.000, Run Time : 8.47 sec
INFO:root:2024-04-11 15:09:55, Train, Epoch : 10, Step : 5740, Loss : 0.37259, Acc : 0.838, Sensitive_Loss : 0.12484, Sensitive_Acc : 23.600, Run Time : 8.96 sec
INFO:root:2024-04-11 15:10:04, Train, Epoch : 10, Step : 5750, Loss : 0.35365, Acc : 0.853, Sensitive_Loss : 0.13673, Sensitive_Acc : 16.300, Run Time : 8.82 sec
INFO:root:2024-04-11 15:10:13, Train, Epoch : 10, Step : 5760, Loss : 0.35281, Acc : 0.825, Sensitive_Loss : 0.08294, Sensitive_Acc : 19.100, Run Time : 8.29 sec
INFO:root:2024-04-11 15:10:21, Train, Epoch : 10, Step : 5770, Loss : 0.32805, Acc : 0.875, Sensitive_Loss : 0.07544, Sensitive_Acc : 19.500, Run Time : 8.37 sec
INFO:root:2024-04-11 15:10:29, Train, Epoch : 10, Step : 5780, Loss : 0.32449, Acc : 0.878, Sensitive_Loss : 0.08692, Sensitive_Acc : 24.200, Run Time : 8.21 sec
INFO:root:2024-04-11 15:10:37, Train, Epoch : 10, Step : 5790, Loss : 0.32526, Acc : 0.866, Sensitive_Loss : 0.09186, Sensitive_Acc : 15.500, Run Time : 7.84 sec
INFO:root:2024-04-11 15:10:45, Train, Epoch : 10, Step : 5800, Loss : 0.35222, Acc : 0.847, Sensitive_Loss : 0.08514, Sensitive_Acc : 25.300, Run Time : 8.04 sec
INFO:root:2024-04-11 15:12:16, Dev, Step : 5800, Loss : 0.54041, Acc : 0.779, Auc : 0.852, Sensitive_Loss : 0.15537, Sensitive_Acc : 21.722, Sensitive_Auc : 0.999, Mean auc: 0.852, Run Time : 91.33 sec
INFO:root:2024-04-11 15:12:23, Train, Epoch : 10, Step : 5810, Loss : 0.33862, Acc : 0.838, Sensitive_Loss : 0.11139, Sensitive_Acc : 23.200, Run Time : 97.61 sec
INFO:root:2024-04-11 15:12:32, Train, Epoch : 10, Step : 5820, Loss : 0.32497, Acc : 0.859, Sensitive_Loss : 0.05736, Sensitive_Acc : 20.300, Run Time : 9.42 sec
INFO:root:2024-04-11 15:12:41, Train, Epoch : 10, Step : 5830, Loss : 0.32928, Acc : 0.875, Sensitive_Loss : 0.11322, Sensitive_Acc : 18.500, Run Time : 9.20 sec
INFO:root:2024-04-11 15:12:51, Train, Epoch : 10, Step : 5840, Loss : 0.29960, Acc : 0.869, Sensitive_Loss : 0.13075, Sensitive_Acc : 20.200, Run Time : 9.39 sec
INFO:root:2024-04-11 15:13:00, Train, Epoch : 10, Step : 5850, Loss : 0.36255, Acc : 0.856, Sensitive_Loss : 0.08921, Sensitive_Acc : 21.000, Run Time : 9.29 sec
INFO:root:2024-04-11 15:13:08, Train, Epoch : 10, Step : 5860, Loss : 0.34322, Acc : 0.853, Sensitive_Loss : 0.11727, Sensitive_Acc : 23.800, Run Time : 8.40 sec
INFO:root:2024-04-11 15:13:16, Train, Epoch : 10, Step : 5870, Loss : 0.31237, Acc : 0.872, Sensitive_Loss : 0.12834, Sensitive_Acc : 19.800, Run Time : 7.80 sec
INFO:root:2024-04-11 15:13:25, Train, Epoch : 10, Step : 5880, Loss : 0.33117, Acc : 0.869, Sensitive_Loss : 0.04833, Sensitive_Acc : 23.400, Run Time : 8.75 sec
INFO:root:2024-04-11 15:13:33, Train, Epoch : 10, Step : 5890, Loss : 0.36003, Acc : 0.844, Sensitive_Loss : 0.10577, Sensitive_Acc : 21.900, Run Time : 8.14 sec
INFO:root:2024-04-11 15:13:41, Train, Epoch : 10, Step : 5900, Loss : 0.31964, Acc : 0.869, Sensitive_Loss : 0.11979, Sensitive_Acc : 24.300, Run Time : 8.04 sec
INFO:root:2024-04-11 15:15:13, Dev, Step : 5900, Loss : 0.57719, Acc : 0.775, Auc : 0.853, Sensitive_Loss : 0.15889, Sensitive_Acc : 21.857, Sensitive_Auc : 1.000, Mean auc: 0.853, Run Time : 91.74 sec
INFO:root:2024-04-11 15:15:19, Train, Epoch : 10, Step : 5910, Loss : 0.34337, Acc : 0.853, Sensitive_Loss : 0.05740, Sensitive_Acc : 22.500, Run Time : 97.82 sec
INFO:root:2024-04-11 15:15:27, Train, Epoch : 10, Step : 5920, Loss : 0.39330, Acc : 0.841, Sensitive_Loss : 0.08440, Sensitive_Acc : 22.000, Run Time : 8.01 sec
INFO:root:2024-04-11 15:15:35, Train, Epoch : 10, Step : 5930, Loss : 0.32688, Acc : 0.859, Sensitive_Loss : 0.09250, Sensitive_Acc : 25.100, Run Time : 8.31 sec
INFO:root:2024-04-11 15:15:44, Train, Epoch : 10, Step : 5940, Loss : 0.32711, Acc : 0.863, Sensitive_Loss : 0.15891, Sensitive_Acc : 20.100, Run Time : 8.53 sec
INFO:root:2024-04-11 15:15:52, Train, Epoch : 10, Step : 5950, Loss : 0.37939, Acc : 0.847, Sensitive_Loss : 0.08740, Sensitive_Acc : 23.500, Run Time : 8.11 sec
INFO:root:2024-04-11 15:16:00, Train, Epoch : 10, Step : 5960, Loss : 0.31230, Acc : 0.881, Sensitive_Loss : 0.07642, Sensitive_Acc : 21.100, Run Time : 8.59 sec
INFO:root:2024-04-11 15:16:08, Train, Epoch : 10, Step : 5970, Loss : 0.32956, Acc : 0.869, Sensitive_Loss : 0.05970, Sensitive_Acc : 19.800, Run Time : 7.89 sec
INFO:root:2024-04-11 15:16:16, Train, Epoch : 10, Step : 5980, Loss : 0.35125, Acc : 0.863, Sensitive_Loss : 0.09858, Sensitive_Acc : 16.300, Run Time : 7.75 sec
INFO:root:2024-04-11 15:16:24, Train, Epoch : 10, Step : 5990, Loss : 0.36264, Acc : 0.838, Sensitive_Loss : 0.08641, Sensitive_Acc : 21.300, Run Time : 7.78 sec
INFO:root:2024-04-11 15:16:32, Train, Epoch : 10, Step : 6000, Loss : 0.30968, Acc : 0.878, Sensitive_Loss : 0.11920, Sensitive_Acc : 22.200, Run Time : 8.20 sec
INFO:root:2024-04-11 15:18:04, Dev, Step : 6000, Loss : 0.57265, Acc : 0.773, Auc : 0.850, Sensitive_Loss : 0.15767, Sensitive_Acc : 21.571, Sensitive_Auc : 0.998, Mean auc: 0.850, Run Time : 91.46 sec
INFO:root:2024-04-11 15:18:09, Train, Epoch : 10, Step : 6010, Loss : 0.26897, Acc : 0.894, Sensitive_Loss : 0.07559, Sensitive_Acc : 16.200, Run Time : 97.14 sec
INFO:root:2024-04-11 15:18:17, Train, Epoch : 10, Step : 6020, Loss : 0.36841, Acc : 0.847, Sensitive_Loss : 0.07954, Sensitive_Acc : 16.500, Run Time : 8.17 sec
INFO:root:2024-04-11 15:18:26, Train, Epoch : 10, Step : 6030, Loss : 0.40921, Acc : 0.787, Sensitive_Loss : 0.06422, Sensitive_Acc : 24.200, Run Time : 8.78 sec
INFO:root:2024-04-11 15:18:34, Train, Epoch : 10, Step : 6040, Loss : 0.29033, Acc : 0.881, Sensitive_Loss : 0.09869, Sensitive_Acc : 21.300, Run Time : 7.81 sec
INFO:root:2024-04-11 15:18:42, Train, Epoch : 10, Step : 6050, Loss : 0.35023, Acc : 0.834, Sensitive_Loss : 0.09887, Sensitive_Acc : 27.200, Run Time : 7.94 sec
INFO:root:2024-04-11 15:18:50, Train, Epoch : 10, Step : 6060, Loss : 0.30120, Acc : 0.887, Sensitive_Loss : 0.07910, Sensitive_Acc : 24.100, Run Time : 7.77 sec
INFO:root:2024-04-11 15:18:58, Train, Epoch : 10, Step : 6070, Loss : 0.31781, Acc : 0.887, Sensitive_Loss : 0.12236, Sensitive_Acc : 16.300, Run Time : 8.60 sec
INFO:root:2024-04-11 15:19:06, Train, Epoch : 10, Step : 6080, Loss : 0.31165, Acc : 0.884, Sensitive_Loss : 0.11256, Sensitive_Acc : 23.300, Run Time : 7.89 sec
INFO:root:2024-04-11 15:19:14, Train, Epoch : 10, Step : 6090, Loss : 0.33539, Acc : 0.841, Sensitive_Loss : 0.05270, Sensitive_Acc : 19.500, Run Time : 8.02 sec
INFO:root:2024-04-11 15:19:22, Train, Epoch : 10, Step : 6100, Loss : 0.33905, Acc : 0.822, Sensitive_Loss : 0.08305, Sensitive_Acc : 19.300, Run Time : 8.18 sec
INFO:root:2024-04-11 15:20:54, Dev, Step : 6100, Loss : 0.60213, Acc : 0.767, Auc : 0.849, Sensitive_Loss : 0.15645, Sensitive_Acc : 21.692, Sensitive_Auc : 0.998, Mean auc: 0.849, Run Time : 91.52 sec
INFO:root:2024-04-11 15:21:00, Train, Epoch : 10, Step : 6110, Loss : 0.37527, Acc : 0.859, Sensitive_Loss : 0.12578, Sensitive_Acc : 19.200, Run Time : 97.53 sec
INFO:root:2024-04-11 15:21:08, Train, Epoch : 10, Step : 6120, Loss : 0.37082, Acc : 0.825, Sensitive_Loss : 0.12737, Sensitive_Acc : 20.200, Run Time : 8.46 sec
INFO:root:2024-04-11 15:21:19, Train, Epoch : 10, Step : 6130, Loss : 0.45580, Acc : 0.816, Sensitive_Loss : 0.13442, Sensitive_Acc : 23.000, Run Time : 10.28 sec
INFO:root:2024-04-11 15:21:27, Train, Epoch : 10, Step : 6140, Loss : 0.37225, Acc : 0.850, Sensitive_Loss : 0.13464, Sensitive_Acc : 23.300, Run Time : 8.01 sec
INFO:root:2024-04-11 15:21:36, Train, Epoch : 10, Step : 6150, Loss : 0.30025, Acc : 0.875, Sensitive_Loss : 0.05673, Sensitive_Acc : 20.700, Run Time : 9.15 sec
INFO:root:2024-04-11 15:21:44, Train, Epoch : 10, Step : 6160, Loss : 0.32704, Acc : 0.887, Sensitive_Loss : 0.11632, Sensitive_Acc : 22.000, Run Time : 7.96 sec
INFO:root:2024-04-11 15:21:52, Train, Epoch : 10, Step : 6170, Loss : 0.36557, Acc : 0.847, Sensitive_Loss : 0.05559, Sensitive_Acc : 20.200, Run Time : 8.38 sec
INFO:root:2024-04-11 15:22:00, Train, Epoch : 10, Step : 6180, Loss : 0.30485, Acc : 0.884, Sensitive_Loss : 0.09828, Sensitive_Acc : 20.500, Run Time : 8.07 sec
INFO:root:2024-04-11 15:22:08, Train, Epoch : 10, Step : 6190, Loss : 0.25482, Acc : 0.903, Sensitive_Loss : 0.14064, Sensitive_Acc : 22.600, Run Time : 8.27 sec
INFO:root:2024-04-11 15:22:16, Train, Epoch : 10, Step : 6200, Loss : 0.30485, Acc : 0.891, Sensitive_Loss : 0.11101, Sensitive_Acc : 24.300, Run Time : 7.77 sec
INFO:root:2024-04-11 15:23:49, Dev, Step : 6200, Loss : 0.54044, Acc : 0.777, Auc : 0.849, Sensitive_Loss : 0.15849, Sensitive_Acc : 21.571, Sensitive_Auc : 0.998, Mean auc: 0.849, Run Time : 92.30 sec
INFO:root:2024-04-11 15:23:55, Train, Epoch : 10, Step : 6210, Loss : 0.29169, Acc : 0.856, Sensitive_Loss : 0.10591, Sensitive_Acc : 17.900, Run Time : 98.34 sec
INFO:root:2024-04-11 15:24:03, Train, Epoch : 10, Step : 6220, Loss : 0.31709, Acc : 0.866, Sensitive_Loss : 0.08029, Sensitive_Acc : 22.400, Run Time : 8.18 sec
INFO:root:2024-04-11 15:24:11, Train, Epoch : 10, Step : 6230, Loss : 0.39910, Acc : 0.834, Sensitive_Loss : 0.12035, Sensitive_Acc : 24.900, Run Time : 8.55 sec
INFO:root:2024-04-11 15:24:19, Train, Epoch : 10, Step : 6240, Loss : 0.39567, Acc : 0.847, Sensitive_Loss : 0.07157, Sensitive_Acc : 27.000, Run Time : 8.10 sec
INFO:root:2024-04-11 15:24:28, Train, Epoch : 10, Step : 6250, Loss : 0.32687, Acc : 0.875, Sensitive_Loss : 0.11265, Sensitive_Acc : 26.100, Run Time : 8.51 sec
INFO:root:2024-04-11 15:24:37, Train, Epoch : 10, Step : 6260, Loss : 0.34480, Acc : 0.869, Sensitive_Loss : 0.07800, Sensitive_Acc : 26.000, Run Time : 8.74 sec
INFO:root:2024-04-11 15:24:45, Train, Epoch : 10, Step : 6270, Loss : 0.29529, Acc : 0.863, Sensitive_Loss : 0.08765, Sensitive_Acc : 23.600, Run Time : 8.50 sec
INFO:root:2024-04-11 15:24:53, Train, Epoch : 10, Step : 6280, Loss : 0.33423, Acc : 0.831, Sensitive_Loss : 0.12070, Sensitive_Acc : 21.500, Run Time : 8.14 sec
INFO:root:2024-04-11 15:25:02, Train, Epoch : 10, Step : 6290, Loss : 0.36121, Acc : 0.841, Sensitive_Loss : 0.07373, Sensitive_Acc : 23.800, Run Time : 8.21 sec
INFO:root:2024-04-11 15:25:10, Train, Epoch : 10, Step : 6300, Loss : 0.30852, Acc : 0.869, Sensitive_Loss : 0.10860, Sensitive_Acc : 23.300, Run Time : 8.32 sec
INFO:root:2024-04-11 15:26:41, Dev, Step : 6300, Loss : 0.55160, Acc : 0.773, Auc : 0.849, Sensitive_Loss : 0.16408, Sensitive_Acc : 21.722, Sensitive_Auc : 1.000, Mean auc: 0.849, Run Time : 91.65 sec
INFO:root:2024-04-11 15:26:48, Train, Epoch : 10, Step : 6310, Loss : 0.30040, Acc : 0.884, Sensitive_Loss : 0.10174, Sensitive_Acc : 21.800, Run Time : 97.66 sec
INFO:root:2024-04-11 15:26:57, Train, Epoch : 10, Step : 6320, Loss : 0.38762, Acc : 0.850, Sensitive_Loss : 0.11731, Sensitive_Acc : 18.100, Run Time : 9.44 sec
INFO:root:2024-04-11 15:27:06, Train, Epoch : 10, Step : 6330, Loss : 0.30870, Acc : 0.881, Sensitive_Loss : 0.13289, Sensitive_Acc : 18.200, Run Time : 8.75 sec
INFO:root:2024-04-11 15:27:14, Train, Epoch : 10, Step : 6340, Loss : 0.37408, Acc : 0.853, Sensitive_Loss : 0.09960, Sensitive_Acc : 23.100, Run Time : 8.25 sec
INFO:root:2024-04-11 15:28:45
INFO:root:y_pred: [0.04891323 0.01895632 0.05970644 ... 0.09830501 0.00934093 0.00602841]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [6.9140027e-05 6.6429707e-06 1.1895767e-03 1.2598905e-02 1.4584306e-02
 1.4076508e-06 3.3275479e-05 1.8325514e-05 3.2598097e-02 9.9899763e-01
 3.9599109e-03 4.2443102e-07 1.0290667e-07 3.0291983e-10 9.9546403e-01
 3.3346596e-04 2.2535337e-06 9.9818265e-01 9.9723780e-01 3.6839346e-04
 6.2348503e-01 3.4713257e-06 2.5033040e-04 2.8564411e-06 4.8450921e-03
 6.2800682e-05 1.1085403e-06 9.4635546e-04 4.4317436e-10 6.6950666e-03
 1.2362368e-06 9.6157950e-01 3.1909830e-04 9.6473843e-01 3.3731651e-10
 7.3875020e-07 3.4513352e-09 1.8854011e-02 1.2444708e-01 3.5850105e-03
 1.3237075e-04 9.0707141e-01 1.5457526e-03 2.8416963e-07 9.9309021e-01
 8.1274873e-03 3.3362063e-05 6.3475505e-02 1.8075893e-02 9.9627727e-01
 9.8913991e-01 9.9535191e-01 9.6871907e-01 2.9326769e-04 9.6442178e-04
 2.9849738e-03 4.3315104e-10 8.2088771e-09 9.9063128e-01 1.0457453e-06
 1.1828928e-05 6.2890560e-03 5.8683165e-04 2.3833621e-07 9.7369224e-01
 1.3906269e-01 5.3577015e-08 2.1217559e-02 1.9047724e-04 9.8893541e-01
 9.9921477e-01 9.9926502e-01 5.2142634e-11 6.2734503e-01 3.7438161e-07
 2.5892094e-01 7.7194845e-06 7.1320398e-12 1.3744421e-06 4.1798863e-05
 4.0586950e-05 6.7404821e-06 9.8073924e-01 9.9019402e-01 1.0498316e-03
 8.2606042e-04 1.0655610e-04 2.0652631e-06 1.2382080e-03 3.2912482e-07
 8.0458840e-06 5.9943245e-04 4.4190823e-10 3.2145335e-12 1.3463502e-04
 3.4281821e-04 5.6356758e-10 9.4837111e-01 9.5030060e-07 6.1865894e-11
 3.7967078e-03 6.8573601e-04 2.8885254e-01 2.1186940e-10 6.4218664e-05
 1.5035321e-04 5.8878150e-06 7.6116282e-01 2.9529258e-05 6.7372252e-03
 9.3281723e-04 9.9906188e-01 9.9725866e-01 1.2797070e-12 5.6297225e-01
 2.8530181e-05 1.4527493e-02 5.9563050e-08 3.2381231e-03 2.5537307e-04
 1.0139887e-05 4.6391886e-09 2.2906592e-05 1.6782802e-10 3.1469547e-06
 6.8834919e-01 8.4852239e-08 9.9796921e-01 3.8236898e-04 5.8831540e-03
 7.6371247e-09 7.8031750e-05 1.2372625e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 15:28:45, Dev, Step : 6340, Loss : 0.54079, Acc : 0.771, Auc : 0.849, Sensitive_Loss : 0.15345, Sensitive_Acc : 21.571, Sensitive_Auc : 0.999, Mean auc: 0.849, Run Time : 91.42 sec
