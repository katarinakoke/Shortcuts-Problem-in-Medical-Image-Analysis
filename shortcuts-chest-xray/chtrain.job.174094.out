Running on desktop22:
stdin: is not a tty
/home/pmen/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_pmen.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/balanced_dataset_train.csv",
    "dev_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/balanced_dataset_val.csv",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.05,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-10 12:12:12, Train, Epoch : 1, Step : 10, Loss : 0.72836, Acc : 0.559, Sensitive_Loss : 0.69741, Sensitive_Acc : 15.500, Run Time : 9.74 sec
INFO:root:2024-04-10 12:12:21, Train, Epoch : 1, Step : 20, Loss : 0.66565, Acc : 0.559, Sensitive_Loss : 0.74349, Sensitive_Acc : 15.600, Run Time : 9.70 sec
INFO:root:2024-04-10 12:12:33, Train, Epoch : 1, Step : 30, Loss : 0.70964, Acc : 0.584, Sensitive_Loss : 0.66399, Sensitive_Acc : 16.600, Run Time : 11.07 sec
INFO:root:2024-04-10 12:12:43, Train, Epoch : 1, Step : 40, Loss : 0.62387, Acc : 0.641, Sensitive_Loss : 0.66089, Sensitive_Acc : 16.500, Run Time : 10.44 sec
INFO:root:2024-04-10 12:12:53, Train, Epoch : 1, Step : 50, Loss : 0.69582, Acc : 0.628, Sensitive_Loss : 0.68229, Sensitive_Acc : 16.500, Run Time : 10.02 sec
INFO:root:2024-04-10 12:13:03, Train, Epoch : 1, Step : 60, Loss : 0.57099, Acc : 0.681, Sensitive_Loss : 0.54293, Sensitive_Acc : 16.400, Run Time : 9.97 sec
INFO:root:2024-04-10 12:13:15, Train, Epoch : 1, Step : 70, Loss : 0.61190, Acc : 0.681, Sensitive_Loss : 0.62644, Sensitive_Acc : 16.100, Run Time : 11.63 sec
INFO:root:2024-04-10 12:13:26, Train, Epoch : 1, Step : 80, Loss : 0.57431, Acc : 0.691, Sensitive_Loss : 0.54695, Sensitive_Acc : 15.200, Run Time : 11.02 sec
INFO:root:2024-04-10 12:13:36, Train, Epoch : 1, Step : 90, Loss : 0.61511, Acc : 0.691, Sensitive_Loss : 0.56859, Sensitive_Acc : 16.200, Run Time : 10.92 sec
INFO:root:2024-04-10 12:13:46, Train, Epoch : 1, Step : 100, Loss : 0.55629, Acc : 0.706, Sensitive_Loss : 0.55095, Sensitive_Acc : 15.500, Run Time : 9.74 sec
INFO:root:2024-04-10 12:15:22, Dev, Step : 100, Loss : 0.60965, Acc : 0.690, Auc : 0.757, Sensitive_Loss : 0.51399, Sensitive_Acc : 16.107, Sensitive_Auc : 0.847, Mean auc: 0.757, Run Time : 96.11 sec
INFO:root:2024-04-10 12:15:24, Best, Step : 100, Loss : 0.60965, Acc : 0.690, Auc : 0.757, Sensitive_Loss : 0.51399, Sensitive_Acc : 16.107, Sensitive_Auc : 0.847, Best Auc : 0.757
INFO:root:2024-04-10 12:15:32, Train, Epoch : 1, Step : 110, Loss : 0.64387, Acc : 0.656, Sensitive_Loss : 0.52591, Sensitive_Acc : 16.600, Run Time : 105.29 sec
INFO:root:2024-04-10 12:15:42, Train, Epoch : 1, Step : 120, Loss : 0.63514, Acc : 0.634, Sensitive_Loss : 0.51435, Sensitive_Acc : 15.700, Run Time : 10.48 sec
INFO:root:2024-04-10 12:15:54, Train, Epoch : 1, Step : 130, Loss : 0.57433, Acc : 0.703, Sensitive_Loss : 0.48487, Sensitive_Acc : 16.100, Run Time : 12.07 sec
INFO:root:2024-04-10 12:16:09, Train, Epoch : 1, Step : 140, Loss : 0.65995, Acc : 0.697, Sensitive_Loss : 0.53983, Sensitive_Acc : 16.900, Run Time : 14.75 sec
INFO:root:2024-04-10 12:16:20, Train, Epoch : 1, Step : 150, Loss : 0.58510, Acc : 0.675, Sensitive_Loss : 0.51916, Sensitive_Acc : 16.700, Run Time : 11.05 sec
INFO:root:2024-04-10 12:16:33, Train, Epoch : 1, Step : 160, Loss : 0.63072, Acc : 0.662, Sensitive_Loss : 0.43412, Sensitive_Acc : 15.500, Run Time : 13.09 sec
INFO:root:2024-04-10 12:16:44, Train, Epoch : 1, Step : 170, Loss : 0.57207, Acc : 0.675, Sensitive_Loss : 0.47693, Sensitive_Acc : 16.900, Run Time : 11.41 sec
INFO:root:2024-04-10 12:16:55, Train, Epoch : 1, Step : 180, Loss : 0.59161, Acc : 0.641, Sensitive_Loss : 0.44051, Sensitive_Acc : 16.500, Run Time : 11.12 sec
INFO:root:2024-04-10 12:17:11, Train, Epoch : 1, Step : 190, Loss : 0.52135, Acc : 0.738, Sensitive_Loss : 0.46600, Sensitive_Acc : 15.400, Run Time : 15.01 sec
INFO:root:2024-04-10 12:17:24, Train, Epoch : 1, Step : 200, Loss : 0.64906, Acc : 0.688, Sensitive_Loss : 0.45083, Sensitive_Acc : 16.200, Run Time : 13.00 sec
INFO:root:2024-04-10 12:19:00, Dev, Step : 200, Loss : 0.64557, Acc : 0.661, Auc : 0.759, Sensitive_Loss : 0.49665, Sensitive_Acc : 16.021, Sensitive_Auc : 0.896, Mean auc: 0.759, Run Time : 96.93 sec
INFO:root:2024-04-10 12:19:02, Best, Step : 200, Loss : 0.64557, Acc : 0.661, Auc : 0.759, Sensitive_Loss : 0.49665, Sensitive_Acc : 16.021, Sensitive_Auc : 0.896, Best Auc : 0.759
INFO:root:2024-04-10 12:19:09, Train, Epoch : 1, Step : 210, Loss : 0.67862, Acc : 0.662, Sensitive_Loss : 0.43340, Sensitive_Acc : 17.000, Run Time : 105.66 sec
INFO:root:2024-04-10 12:19:20, Train, Epoch : 1, Step : 220, Loss : 0.54774, Acc : 0.709, Sensitive_Loss : 0.40613, Sensitive_Acc : 16.600, Run Time : 10.86 sec
INFO:root:2024-04-10 12:19:30, Train, Epoch : 1, Step : 230, Loss : 0.48280, Acc : 0.750, Sensitive_Loss : 0.39958, Sensitive_Acc : 17.400, Run Time : 10.36 sec
INFO:root:2024-04-10 12:19:41, Train, Epoch : 1, Step : 240, Loss : 0.59935, Acc : 0.700, Sensitive_Loss : 0.43509, Sensitive_Acc : 15.200, Run Time : 10.69 sec
INFO:root:2024-04-10 12:19:51, Train, Epoch : 1, Step : 250, Loss : 0.59412, Acc : 0.684, Sensitive_Loss : 0.38018, Sensitive_Acc : 16.200, Run Time : 10.07 sec
INFO:root:2024-04-10 12:20:05, Train, Epoch : 1, Step : 260, Loss : 0.58151, Acc : 0.684, Sensitive_Loss : 0.36209, Sensitive_Acc : 16.500, Run Time : 13.43 sec
INFO:root:2024-04-10 12:20:16, Train, Epoch : 1, Step : 270, Loss : 0.56688, Acc : 0.681, Sensitive_Loss : 0.32740, Sensitive_Acc : 16.400, Run Time : 10.99 sec
INFO:root:2024-04-10 12:20:25, Train, Epoch : 1, Step : 280, Loss : 0.53709, Acc : 0.725, Sensitive_Loss : 0.36819, Sensitive_Acc : 15.500, Run Time : 9.16 sec
INFO:root:2024-04-10 12:20:40, Train, Epoch : 1, Step : 290, Loss : 0.59629, Acc : 0.688, Sensitive_Loss : 0.37190, Sensitive_Acc : 16.200, Run Time : 15.04 sec
INFO:root:2024-04-10 12:20:51, Train, Epoch : 1, Step : 300, Loss : 0.62821, Acc : 0.681, Sensitive_Loss : 0.38595, Sensitive_Acc : 15.200, Run Time : 11.28 sec
INFO:root:2024-04-10 12:22:28, Dev, Step : 300, Loss : 0.56767, Acc : 0.721, Auc : 0.795, Sensitive_Loss : 0.35214, Sensitive_Acc : 16.207, Sensitive_Auc : 0.962, Mean auc: 0.795, Run Time : 97.32 sec
INFO:root:2024-04-10 12:22:29, Best, Step : 300, Loss : 0.56767, Acc : 0.721, Auc : 0.795, Sensitive_Loss : 0.35214, Sensitive_Acc : 16.207, Sensitive_Auc : 0.962, Best Auc : 0.795
INFO:root:2024-04-10 12:22:35, Train, Epoch : 1, Step : 310, Loss : 0.58370, Acc : 0.716, Sensitive_Loss : 0.32693, Sensitive_Acc : 17.600, Run Time : 104.36 sec
INFO:root:2024-04-10 12:22:45, Train, Epoch : 1, Step : 320, Loss : 0.61388, Acc : 0.709, Sensitive_Loss : 0.37600, Sensitive_Acc : 15.100, Run Time : 9.63 sec
INFO:root:2024-04-10 12:22:56, Train, Epoch : 1, Step : 330, Loss : 0.48335, Acc : 0.741, Sensitive_Loss : 0.32041, Sensitive_Acc : 15.200, Run Time : 11.11 sec
INFO:root:2024-04-10 12:23:07, Train, Epoch : 1, Step : 340, Loss : 0.67095, Acc : 0.681, Sensitive_Loss : 0.30008, Sensitive_Acc : 15.900, Run Time : 10.92 sec
INFO:root:2024-04-10 12:23:16, Train, Epoch : 1, Step : 350, Loss : 0.53792, Acc : 0.731, Sensitive_Loss : 0.34380, Sensitive_Acc : 15.600, Run Time : 9.10 sec
INFO:root:2024-04-10 12:23:25, Train, Epoch : 1, Step : 360, Loss : 0.55106, Acc : 0.697, Sensitive_Loss : 0.36510, Sensitive_Acc : 14.800, Run Time : 9.25 sec
INFO:root:2024-04-10 12:23:37, Train, Epoch : 1, Step : 370, Loss : 0.60272, Acc : 0.741, Sensitive_Loss : 0.28938, Sensitive_Acc : 16.600, Run Time : 11.34 sec
INFO:root:2024-04-10 12:23:48, Train, Epoch : 1, Step : 380, Loss : 0.51255, Acc : 0.728, Sensitive_Loss : 0.34395, Sensitive_Acc : 17.500, Run Time : 11.07 sec
INFO:root:2024-04-10 12:23:58, Train, Epoch : 1, Step : 390, Loss : 0.50489, Acc : 0.697, Sensitive_Loss : 0.34588, Sensitive_Acc : 16.500, Run Time : 9.92 sec
INFO:root:2024-04-10 12:24:08, Train, Epoch : 1, Step : 400, Loss : 0.48233, Acc : 0.756, Sensitive_Loss : 0.27258, Sensitive_Acc : 15.200, Run Time : 10.57 sec
INFO:root:2024-04-10 12:25:45, Dev, Step : 400, Loss : 0.68231, Acc : 0.665, Auc : 0.796, Sensitive_Loss : 0.33140, Sensitive_Acc : 16.150, Sensitive_Auc : 0.959, Mean auc: 0.796, Run Time : 96.71 sec
INFO:root:2024-04-10 12:25:48, Best, Step : 400, Loss : 0.68231, Acc : 0.665, Auc : 0.796, Sensitive_Loss : 0.33140, Sensitive_Acc : 16.150, Sensitive_Auc : 0.959, Best Auc : 0.796
INFO:root:2024-04-10 12:25:56, Train, Epoch : 1, Step : 410, Loss : 0.55697, Acc : 0.709, Sensitive_Loss : 0.31812, Sensitive_Acc : 17.500, Run Time : 107.60 sec
INFO:root:2024-04-10 12:26:05, Train, Epoch : 1, Step : 420, Loss : 0.51036, Acc : 0.738, Sensitive_Loss : 0.22100, Sensitive_Acc : 17.400, Run Time : 9.54 sec
INFO:root:2024-04-10 12:26:16, Train, Epoch : 1, Step : 430, Loss : 0.53278, Acc : 0.703, Sensitive_Loss : 0.24799, Sensitive_Acc : 16.800, Run Time : 10.51 sec
INFO:root:2024-04-10 12:26:26, Train, Epoch : 1, Step : 440, Loss : 0.57448, Acc : 0.706, Sensitive_Loss : 0.26475, Sensitive_Acc : 15.100, Run Time : 9.90 sec
INFO:root:2024-04-10 12:26:36, Train, Epoch : 1, Step : 450, Loss : 0.57916, Acc : 0.728, Sensitive_Loss : 0.25506, Sensitive_Acc : 16.700, Run Time : 10.10 sec
INFO:root:2024-04-10 12:26:47, Train, Epoch : 1, Step : 460, Loss : 0.58715, Acc : 0.703, Sensitive_Loss : 0.30299, Sensitive_Acc : 14.800, Run Time : 10.92 sec
INFO:root:2024-04-10 12:26:59, Train, Epoch : 1, Step : 470, Loss : 0.61392, Acc : 0.731, Sensitive_Loss : 0.31118, Sensitive_Acc : 15.900, Run Time : 12.49 sec
INFO:root:2024-04-10 12:27:13, Train, Epoch : 1, Step : 480, Loss : 0.53192, Acc : 0.725, Sensitive_Loss : 0.28334, Sensitive_Acc : 16.100, Run Time : 13.84 sec
INFO:root:2024-04-10 12:27:23, Train, Epoch : 1, Step : 490, Loss : 0.52409, Acc : 0.706, Sensitive_Loss : 0.25975, Sensitive_Acc : 17.900, Run Time : 9.70 sec
INFO:root:2024-04-10 12:27:35, Train, Epoch : 1, Step : 500, Loss : 0.65742, Acc : 0.700, Sensitive_Loss : 0.25743, Sensitive_Acc : 15.300, Run Time : 12.29 sec
INFO:root:2024-04-10 12:29:08, Dev, Step : 500, Loss : 0.57609, Acc : 0.715, Auc : 0.801, Sensitive_Loss : 0.34268, Sensitive_Acc : 16.150, Sensitive_Auc : 0.968, Mean auc: 0.801, Run Time : 92.46 sec
INFO:root:2024-04-10 12:29:09, Best, Step : 500, Loss : 0.57609, Acc : 0.715, Auc : 0.801, Sensitive_Loss : 0.34268, Sensitive_Acc : 16.150, Sensitive_Auc : 0.968, Best Auc : 0.801
INFO:root:2024-04-10 12:29:18, Train, Epoch : 1, Step : 510, Loss : 0.62434, Acc : 0.684, Sensitive_Loss : 0.28128, Sensitive_Acc : 16.400, Run Time : 102.53 sec
INFO:root:2024-04-10 12:29:34, Train, Epoch : 1, Step : 520, Loss : 0.55620, Acc : 0.713, Sensitive_Loss : 0.27737, Sensitive_Acc : 15.600, Run Time : 16.13 sec
INFO:root:2024-04-10 12:29:45, Train, Epoch : 1, Step : 530, Loss : 0.60946, Acc : 0.703, Sensitive_Loss : 0.32144, Sensitive_Acc : 16.000, Run Time : 11.23 sec
INFO:root:2024-04-10 12:29:57, Train, Epoch : 1, Step : 540, Loss : 0.61192, Acc : 0.656, Sensitive_Loss : 0.28434, Sensitive_Acc : 14.400, Run Time : 12.00 sec
INFO:root:2024-04-10 12:30:08, Train, Epoch : 1, Step : 550, Loss : 0.58728, Acc : 0.722, Sensitive_Loss : 0.24032, Sensitive_Acc : 17.400, Run Time : 11.19 sec
INFO:root:2024-04-10 12:30:18, Train, Epoch : 1, Step : 560, Loss : 0.61378, Acc : 0.700, Sensitive_Loss : 0.29599, Sensitive_Acc : 14.600, Run Time : 9.69 sec
INFO:root:2024-04-10 12:30:31, Train, Epoch : 1, Step : 570, Loss : 0.53232, Acc : 0.728, Sensitive_Loss : 0.22589, Sensitive_Acc : 15.400, Run Time : 12.83 sec
INFO:root:2024-04-10 12:30:41, Train, Epoch : 1, Step : 580, Loss : 0.53335, Acc : 0.725, Sensitive_Loss : 0.21869, Sensitive_Acc : 17.600, Run Time : 10.44 sec
INFO:root:2024-04-10 12:30:50, Train, Epoch : 1, Step : 590, Loss : 0.50781, Acc : 0.738, Sensitive_Loss : 0.21534, Sensitive_Acc : 15.300, Run Time : 8.52 sec
INFO:root:2024-04-10 12:31:00, Train, Epoch : 1, Step : 600, Loss : 0.52476, Acc : 0.719, Sensitive_Loss : 0.29160, Sensitive_Acc : 18.600, Run Time : 10.41 sec
INFO:root:2024-04-10 12:32:33, Dev, Step : 600, Loss : 0.57081, Acc : 0.714, Auc : 0.806, Sensitive_Loss : 0.22711, Sensitive_Acc : 16.193, Sensitive_Auc : 0.971, Mean auc: 0.806, Run Time : 92.38 sec
INFO:root:2024-04-10 12:32:33, Best, Step : 600, Loss : 0.57081, Acc : 0.714, Auc : 0.806, Sensitive_Loss : 0.22711, Sensitive_Acc : 16.193, Sensitive_Auc : 0.971, Best Auc : 0.806
INFO:root:2024-04-10 12:32:40, Train, Epoch : 1, Step : 610, Loss : 0.54890, Acc : 0.688, Sensitive_Loss : 0.23328, Sensitive_Acc : 17.100, Run Time : 99.98 sec
INFO:root:2024-04-10 12:32:53, Train, Epoch : 1, Step : 620, Loss : 0.50205, Acc : 0.725, Sensitive_Loss : 0.26362, Sensitive_Acc : 16.000, Run Time : 12.46 sec
INFO:root:2024-04-10 12:33:04, Train, Epoch : 1, Step : 630, Loss : 0.54702, Acc : 0.734, Sensitive_Loss : 0.25130, Sensitive_Acc : 15.500, Run Time : 11.84 sec
INFO:root:2024-04-10 12:33:14, Train, Epoch : 1, Step : 640, Loss : 0.52987, Acc : 0.759, Sensitive_Loss : 0.22850, Sensitive_Acc : 15.500, Run Time : 9.54 sec
INFO:root:2024-04-10 12:34:56
INFO:root:y_pred: [0.0930786  0.19564617 0.64047384 ... 0.29303202 0.25489444 0.59451795]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.96984422e-01 6.86379746e-02 3.51518273e-01 9.93317246e-01
 9.98891771e-01 8.12437415e-01 9.94386196e-01 1.52985100e-04
 4.23280269e-01 9.98577952e-01 1.30128443e-01 1.44798890e-01
 6.83323592e-02 8.40560019e-01 9.91249084e-01 9.98778522e-01
 9.98952746e-01 8.55255723e-01 9.10750449e-01 9.95679975e-01
 9.81587768e-01 1.19359165e-01 9.65042770e-01 5.25189877e-01
 8.51281703e-01 2.30298087e-01 6.01607978e-01 5.21532655e-01
 9.96683657e-01 1.37475386e-01 7.61138182e-03 1.42840996e-01
 1.06998428e-03 9.96946394e-01 1.95601699e-03 9.84245360e-01
 2.31249109e-02 9.99717057e-01 2.97424227e-01 9.80127931e-01
 9.97402966e-01 4.70916405e-02 2.14150324e-01 3.37985754e-02
 6.41883552e-01 3.35539073e-01 9.91030812e-01 8.21910143e-01
 9.79571760e-01 9.85004187e-01 1.98393717e-01 4.03123647e-01
 3.46763790e-01 5.85337937e-01 9.95504916e-01 1.15360722e-01
 9.89168525e-01 9.95658755e-01 4.89638150e-01 9.60668921e-02
 5.70320524e-03 9.91073072e-01 2.86436856e-01 9.92346525e-01
 9.68336701e-01 4.79869604e-01 9.95941579e-01 5.91102421e-01
 9.99292016e-01 9.92543101e-01 7.60996103e-01 8.26331735e-01
 9.44161952e-01 9.86626804e-01 9.91813302e-01 4.01513353e-02
 2.65729219e-01 2.84462310e-02 1.82728353e-03 9.87161100e-01
 1.52465180e-01 9.26295221e-01 9.99168873e-01 9.94772971e-01
 3.06383938e-01 9.99784052e-01 1.51686817e-01 6.20383546e-02
 9.89096701e-01 9.74938214e-01 1.62125036e-01 6.99223042e-01
 3.42722870e-02 3.75397086e-01 7.45627403e-01 9.98135090e-01
 2.20168650e-01 9.93939757e-01 8.88261914e-01 2.67309956e-02
 1.31176785e-02 6.50499046e-01 9.75830197e-01 9.70198512e-01
 9.01889801e-01 9.47945535e-01 9.74909306e-01 4.98868600e-02
 1.82090640e-01 9.82028842e-01 9.37454868e-04 1.38912737e-01
 9.19229507e-01 9.96516824e-01 9.83851254e-01 4.15609777e-01
 7.28580832e-01 2.07348308e-03 9.98776615e-01 9.94392216e-01
 9.70382929e-01 9.98212099e-01 1.54127866e-01 2.74998277e-01
 3.45148563e-01 2.26252042e-02 1.27751147e-02 1.71876997e-02
 9.73063707e-01 9.99457777e-01 1.70167312e-02 5.95952198e-02
 2.52245343e-03 7.93743670e-01 9.96789098e-01 9.69797552e-01
 9.79413927e-01 7.62646914e-01 1.12704843e-01 8.79111528e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 12:34:56, Dev, Step : 644, Loss : 0.59419, Acc : 0.702, Auc : 0.807, Sensitive_Loss : 0.27348, Sensitive_Acc : 16.121, Sensitive_Auc : 0.973, Mean auc: 0.807, Run Time : 98.56 sec
INFO:root:2024-04-10 12:34:57, Best, Step : 644, Loss : 0.59419, Acc : 0.702,Auc : 0.807, Best Auc : 0.807, Sensitive_Loss : 0.27348, Sensitive_Acc : 16.121, Sensitive_Auc : 0.973
INFO:root:2024-04-10 12:35:05, Train, Epoch : 2, Step : 650, Loss : 0.36618, Acc : 0.425, Sensitive_Loss : 0.17016, Sensitive_Acc : 9.300, Run Time : 6.95 sec
INFO:root:2024-04-10 12:35:14, Train, Epoch : 2, Step : 660, Loss : 0.51171, Acc : 0.734, Sensitive_Loss : 0.25619, Sensitive_Acc : 16.600, Run Time : 9.67 sec
INFO:root:2024-04-10 12:35:26, Train, Epoch : 2, Step : 670, Loss : 0.57355, Acc : 0.734, Sensitive_Loss : 0.23699, Sensitive_Acc : 16.700, Run Time : 11.51 sec
INFO:root:2024-04-10 12:35:37, Train, Epoch : 2, Step : 680, Loss : 0.49922, Acc : 0.778, Sensitive_Loss : 0.20231, Sensitive_Acc : 15.900, Run Time : 10.69 sec
INFO:root:2024-04-10 12:35:46, Train, Epoch : 2, Step : 690, Loss : 0.50707, Acc : 0.728, Sensitive_Loss : 0.20631, Sensitive_Acc : 16.600, Run Time : 9.21 sec
INFO:root:2024-04-10 12:35:59, Train, Epoch : 2, Step : 700, Loss : 0.46464, Acc : 0.775, Sensitive_Loss : 0.23282, Sensitive_Acc : 16.200, Run Time : 13.47 sec
INFO:root:2024-04-10 12:37:40, Dev, Step : 700, Loss : 0.59291, Acc : 0.719, Auc : 0.819, Sensitive_Loss : 0.30036, Sensitive_Acc : 15.993, Sensitive_Auc : 0.978, Mean auc: 0.819, Run Time : 101.27 sec
INFO:root:2024-04-10 12:37:43, Best, Step : 700, Loss : 0.59291, Acc : 0.719, Auc : 0.819, Sensitive_Loss : 0.30036, Sensitive_Acc : 15.993, Sensitive_Auc : 0.978, Best Auc : 0.819
INFO:root:2024-04-10 12:37:51, Train, Epoch : 2, Step : 710, Loss : 0.50222, Acc : 0.756, Sensitive_Loss : 0.22878, Sensitive_Acc : 15.500, Run Time : 111.56 sec
INFO:root:2024-04-10 12:38:01, Train, Epoch : 2, Step : 720, Loss : 0.52815, Acc : 0.741, Sensitive_Loss : 0.23225, Sensitive_Acc : 15.500, Run Time : 9.74 sec
INFO:root:2024-04-10 12:38:10, Train, Epoch : 2, Step : 730, Loss : 0.52117, Acc : 0.753, Sensitive_Loss : 0.19587, Sensitive_Acc : 16.100, Run Time : 9.73 sec
INFO:root:2024-04-10 12:38:22, Train, Epoch : 2, Step : 740, Loss : 0.52246, Acc : 0.741, Sensitive_Loss : 0.22781, Sensitive_Acc : 16.600, Run Time : 12.17 sec
INFO:root:2024-04-10 12:38:32, Train, Epoch : 2, Step : 750, Loss : 0.53289, Acc : 0.716, Sensitive_Loss : 0.18835, Sensitive_Acc : 15.900, Run Time : 9.11 sec
INFO:root:2024-04-10 12:38:42, Train, Epoch : 2, Step : 760, Loss : 0.47654, Acc : 0.753, Sensitive_Loss : 0.19972, Sensitive_Acc : 15.600, Run Time : 10.35 sec
INFO:root:2024-04-10 12:38:53, Train, Epoch : 2, Step : 770, Loss : 0.50777, Acc : 0.741, Sensitive_Loss : 0.17744, Sensitive_Acc : 17.200, Run Time : 11.32 sec
INFO:root:2024-04-10 12:39:05, Train, Epoch : 2, Step : 780, Loss : 0.51476, Acc : 0.759, Sensitive_Loss : 0.18604, Sensitive_Acc : 15.700, Run Time : 11.86 sec
INFO:root:2024-04-10 12:39:15, Train, Epoch : 2, Step : 790, Loss : 0.47255, Acc : 0.750, Sensitive_Loss : 0.18175, Sensitive_Acc : 15.100, Run Time : 10.10 sec
INFO:root:2024-04-10 12:39:25, Train, Epoch : 2, Step : 800, Loss : 0.54369, Acc : 0.728, Sensitive_Loss : 0.21460, Sensitive_Acc : 17.600, Run Time : 9.90 sec
INFO:root:2024-04-10 12:41:07, Dev, Step : 800, Loss : 0.56869, Acc : 0.738, Auc : 0.816, Sensitive_Loss : 0.24843, Sensitive_Acc : 16.207, Sensitive_Auc : 0.973, Mean auc: 0.816, Run Time : 101.96 sec
INFO:root:2024-04-10 12:41:15, Train, Epoch : 2, Step : 810, Loss : 0.53046, Acc : 0.744, Sensitive_Loss : 0.20805, Sensitive_Acc : 14.100, Run Time : 109.88 sec
INFO:root:2024-04-10 12:41:27, Train, Epoch : 2, Step : 820, Loss : 0.53035, Acc : 0.778, Sensitive_Loss : 0.17926, Sensitive_Acc : 17.100, Run Time : 12.53 sec
INFO:root:2024-04-10 12:41:36, Train, Epoch : 2, Step : 830, Loss : 0.54475, Acc : 0.725, Sensitive_Loss : 0.30238, Sensitive_Acc : 17.600, Run Time : 8.80 sec
INFO:root:2024-04-10 12:41:46, Train, Epoch : 2, Step : 840, Loss : 0.53165, Acc : 0.703, Sensitive_Loss : 0.21675, Sensitive_Acc : 15.500, Run Time : 9.39 sec
INFO:root:2024-04-10 12:42:00, Train, Epoch : 2, Step : 850, Loss : 0.48566, Acc : 0.728, Sensitive_Loss : 0.13773, Sensitive_Acc : 17.100, Run Time : 14.08 sec
INFO:root:2024-04-10 12:42:10, Train, Epoch : 2, Step : 860, Loss : 0.52288, Acc : 0.741, Sensitive_Loss : 0.20792, Sensitive_Acc : 17.100, Run Time : 10.19 sec
INFO:root:2024-04-10 12:42:22, Train, Epoch : 2, Step : 870, Loss : 0.45517, Acc : 0.803, Sensitive_Loss : 0.21122, Sensitive_Acc : 16.200, Run Time : 12.29 sec
INFO:root:2024-04-10 12:42:33, Train, Epoch : 2, Step : 880, Loss : 0.50115, Acc : 0.766, Sensitive_Loss : 0.21086, Sensitive_Acc : 15.300, Run Time : 11.02 sec
INFO:root:2024-04-10 12:42:43, Train, Epoch : 2, Step : 890, Loss : 0.52505, Acc : 0.719, Sensitive_Loss : 0.20442, Sensitive_Acc : 16.500, Run Time : 9.39 sec
INFO:root:2024-04-10 12:42:54, Train, Epoch : 2, Step : 900, Loss : 0.50577, Acc : 0.747, Sensitive_Loss : 0.20586, Sensitive_Acc : 15.400, Run Time : 11.51 sec
INFO:root:2024-04-10 12:44:33, Dev, Step : 900, Loss : 0.61904, Acc : 0.702, Auc : 0.812, Sensitive_Loss : 0.22521, Sensitive_Acc : 16.221, Sensitive_Auc : 0.977, Mean auc: 0.812, Run Time : 99.19 sec
INFO:root:2024-04-10 12:44:42, Train, Epoch : 2, Step : 910, Loss : 0.58387, Acc : 0.709, Sensitive_Loss : 0.21799, Sensitive_Acc : 16.100, Run Time : 108.26 sec
INFO:root:2024-04-10 12:44:52, Train, Epoch : 2, Step : 920, Loss : 0.49641, Acc : 0.728, Sensitive_Loss : 0.23503, Sensitive_Acc : 17.100, Run Time : 9.91 sec
INFO:root:2024-04-10 12:45:01, Train, Epoch : 2, Step : 930, Loss : 0.56163, Acc : 0.741, Sensitive_Loss : 0.18864, Sensitive_Acc : 15.900, Run Time : 9.01 sec
INFO:root:2024-04-10 12:45:11, Train, Epoch : 2, Step : 940, Loss : 0.50927, Acc : 0.762, Sensitive_Loss : 0.22648, Sensitive_Acc : 16.600, Run Time : 9.51 sec
INFO:root:2024-04-10 12:45:21, Train, Epoch : 2, Step : 950, Loss : 0.48947, Acc : 0.791, Sensitive_Loss : 0.18249, Sensitive_Acc : 15.900, Run Time : 10.42 sec
INFO:root:2024-04-10 12:45:30, Train, Epoch : 2, Step : 960, Loss : 0.45732, Acc : 0.756, Sensitive_Loss : 0.24690, Sensitive_Acc : 16.200, Run Time : 9.05 sec
INFO:root:2024-04-10 12:45:39, Train, Epoch : 2, Step : 970, Loss : 0.47099, Acc : 0.769, Sensitive_Loss : 0.14720, Sensitive_Acc : 16.100, Run Time : 8.78 sec
INFO:root:2024-04-10 12:45:50, Train, Epoch : 2, Step : 980, Loss : 0.53043, Acc : 0.747, Sensitive_Loss : 0.17256, Sensitive_Acc : 15.600, Run Time : 10.50 sec
INFO:root:2024-04-10 12:46:00, Train, Epoch : 2, Step : 990, Loss : 0.50844, Acc : 0.741, Sensitive_Loss : 0.23162, Sensitive_Acc : 16.400, Run Time : 9.98 sec
INFO:root:2024-04-10 12:46:08, Train, Epoch : 2, Step : 1000, Loss : 0.55956, Acc : 0.734, Sensitive_Loss : 0.20578, Sensitive_Acc : 16.000, Run Time : 8.34 sec
INFO:root:2024-04-10 12:47:44, Dev, Step : 1000, Loss : 0.57987, Acc : 0.728, Auc : 0.808, Sensitive_Loss : 0.21266, Sensitive_Acc : 16.193, Sensitive_Auc : 0.983, Mean auc: 0.808, Run Time : 96.45 sec
INFO:root:2024-04-10 12:47:51, Train, Epoch : 2, Step : 1010, Loss : 0.49205, Acc : 0.738, Sensitive_Loss : 0.17429, Sensitive_Acc : 15.300, Run Time : 102.96 sec
INFO:root:2024-04-10 12:48:02, Train, Epoch : 2, Step : 1020, Loss : 0.60799, Acc : 0.753, Sensitive_Loss : 0.22360, Sensitive_Acc : 15.300, Run Time : 11.20 sec
INFO:root:2024-04-10 12:48:11, Train, Epoch : 2, Step : 1030, Loss : 0.47615, Acc : 0.747, Sensitive_Loss : 0.18266, Sensitive_Acc : 16.900, Run Time : 9.40 sec
INFO:root:2024-04-10 12:48:23, Train, Epoch : 2, Step : 1040, Loss : 0.55174, Acc : 0.744, Sensitive_Loss : 0.13562, Sensitive_Acc : 15.200, Run Time : 11.45 sec
INFO:root:2024-04-10 12:48:33, Train, Epoch : 2, Step : 1050, Loss : 0.50324, Acc : 0.744, Sensitive_Loss : 0.16297, Sensitive_Acc : 16.800, Run Time : 9.65 sec
INFO:root:2024-04-10 12:48:41, Train, Epoch : 2, Step : 1060, Loss : 0.46893, Acc : 0.750, Sensitive_Loss : 0.22190, Sensitive_Acc : 16.100, Run Time : 8.51 sec
INFO:root:2024-04-10 12:48:51, Train, Epoch : 2, Step : 1070, Loss : 0.46436, Acc : 0.772, Sensitive_Loss : 0.23298, Sensitive_Acc : 15.800, Run Time : 10.03 sec
INFO:root:2024-04-10 12:49:01, Train, Epoch : 2, Step : 1080, Loss : 0.52732, Acc : 0.728, Sensitive_Loss : 0.19804, Sensitive_Acc : 14.900, Run Time : 9.97 sec
INFO:root:2024-04-10 12:49:10, Train, Epoch : 2, Step : 1090, Loss : 0.54676, Acc : 0.750, Sensitive_Loss : 0.22483, Sensitive_Acc : 15.100, Run Time : 8.93 sec
INFO:root:2024-04-10 12:49:19, Train, Epoch : 2, Step : 1100, Loss : 0.49464, Acc : 0.716, Sensitive_Loss : 0.20181, Sensitive_Acc : 15.400, Run Time : 8.87 sec
INFO:root:2024-04-10 12:50:57, Dev, Step : 1100, Loss : 0.53980, Acc : 0.746, Auc : 0.823, Sensitive_Loss : 0.23379, Sensitive_Acc : 16.179, Sensitive_Auc : 0.986, Mean auc: 0.823, Run Time : 97.65 sec
INFO:root:2024-04-10 12:50:57, Best, Step : 1100, Loss : 0.53980, Acc : 0.746, Auc : 0.823, Sensitive_Loss : 0.23379, Sensitive_Acc : 16.179, Sensitive_Auc : 0.986, Best Auc : 0.823
INFO:root:2024-04-10 12:51:04, Train, Epoch : 2, Step : 1110, Loss : 0.46795, Acc : 0.784, Sensitive_Loss : 0.19781, Sensitive_Acc : 16.000, Run Time : 105.06 sec
INFO:root:2024-04-10 12:51:14, Train, Epoch : 2, Step : 1120, Loss : 0.51659, Acc : 0.756, Sensitive_Loss : 0.16722, Sensitive_Acc : 16.300, Run Time : 10.02 sec
INFO:root:2024-04-10 12:51:25, Train, Epoch : 2, Step : 1130, Loss : 0.44269, Acc : 0.750, Sensitive_Loss : 0.17444, Sensitive_Acc : 16.300, Run Time : 11.09 sec
INFO:root:2024-04-10 12:51:34, Train, Epoch : 2, Step : 1140, Loss : 0.56194, Acc : 0.738, Sensitive_Loss : 0.14126, Sensitive_Acc : 18.000, Run Time : 8.86 sec
INFO:root:2024-04-10 12:51:43, Train, Epoch : 2, Step : 1150, Loss : 0.42570, Acc : 0.797, Sensitive_Loss : 0.17783, Sensitive_Acc : 15.800, Run Time : 8.94 sec
INFO:root:2024-04-10 12:51:55, Train, Epoch : 2, Step : 1160, Loss : 0.53821, Acc : 0.750, Sensitive_Loss : 0.17537, Sensitive_Acc : 14.600, Run Time : 11.77 sec
INFO:root:2024-04-10 12:52:04, Train, Epoch : 2, Step : 1170, Loss : 0.50118, Acc : 0.747, Sensitive_Loss : 0.17295, Sensitive_Acc : 15.400, Run Time : 9.28 sec
INFO:root:2024-04-10 12:52:13, Train, Epoch : 2, Step : 1180, Loss : 0.54079, Acc : 0.753, Sensitive_Loss : 0.17882, Sensitive_Acc : 16.200, Run Time : 9.09 sec
INFO:root:2024-04-10 12:52:23, Train, Epoch : 2, Step : 1190, Loss : 0.56953, Acc : 0.725, Sensitive_Loss : 0.14029, Sensitive_Acc : 15.700, Run Time : 9.93 sec
INFO:root:2024-04-10 12:52:34, Train, Epoch : 2, Step : 1200, Loss : 0.52860, Acc : 0.744, Sensitive_Loss : 0.14591, Sensitive_Acc : 15.700, Run Time : 11.51 sec
INFO:root:2024-04-10 12:54:16, Dev, Step : 1200, Loss : 0.56681, Acc : 0.724, Auc : 0.812, Sensitive_Loss : 0.24270, Sensitive_Acc : 16.064, Sensitive_Auc : 0.980, Mean auc: 0.812, Run Time : 101.77 sec
INFO:root:2024-04-10 12:54:22, Train, Epoch : 2, Step : 1210, Loss : 0.54441, Acc : 0.753, Sensitive_Loss : 0.15840, Sensitive_Acc : 18.400, Run Time : 107.82 sec
INFO:root:2024-04-10 12:54:32, Train, Epoch : 2, Step : 1220, Loss : 0.49977, Acc : 0.738, Sensitive_Loss : 0.23701, Sensitive_Acc : 15.400, Run Time : 9.38 sec
INFO:root:2024-04-10 12:54:42, Train, Epoch : 2, Step : 1230, Loss : 0.51122, Acc : 0.772, Sensitive_Loss : 0.18884, Sensitive_Acc : 19.100, Run Time : 10.39 sec
INFO:root:2024-04-10 12:54:52, Train, Epoch : 2, Step : 1240, Loss : 0.47042, Acc : 0.762, Sensitive_Loss : 0.17704, Sensitive_Acc : 16.000, Run Time : 10.00 sec
INFO:root:2024-04-10 12:55:01, Train, Epoch : 2, Step : 1250, Loss : 0.45113, Acc : 0.806, Sensitive_Loss : 0.20560, Sensitive_Acc : 18.200, Run Time : 8.90 sec
INFO:root:2024-04-10 12:55:12, Train, Epoch : 2, Step : 1260, Loss : 0.36876, Acc : 0.797, Sensitive_Loss : 0.18687, Sensitive_Acc : 17.200, Run Time : 11.25 sec
INFO:root:2024-04-10 12:55:22, Train, Epoch : 2, Step : 1270, Loss : 0.46100, Acc : 0.778, Sensitive_Loss : 0.11480, Sensitive_Acc : 14.200, Run Time : 10.21 sec
INFO:root:2024-04-10 12:55:31, Train, Epoch : 2, Step : 1280, Loss : 0.56149, Acc : 0.731, Sensitive_Loss : 0.20800, Sensitive_Acc : 15.500, Run Time : 9.11 sec
INFO:root:2024-04-10 12:57:21
INFO:root:y_pred: [0.07101369 0.05612646 0.78793514 ... 0.37892792 0.22982402 0.3858542 ]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9805248e-01 1.6319707e-02 4.3873510e-01 9.9899071e-01 9.9787307e-01
 9.9103373e-01 9.9037713e-01 2.4723893e-06 8.6690807e-01 9.8983777e-01
 3.0815649e-01 1.1246860e-01 5.3870375e-04 9.5366770e-01 9.9851221e-01
 9.9974567e-01 9.9584621e-01 9.7516191e-01 9.7991782e-01 9.7203213e-01
 9.8863345e-01 2.7900723e-01 9.8413497e-01 7.1528077e-01 5.7645774e-01
 6.7313775e-02 9.5566344e-01 1.7097671e-02 9.9948150e-01 6.9085859e-02
 3.2737132e-02 2.2887915e-01 1.4159641e-02 9.9267322e-01 4.2361866e-05
 8.8592941e-01 8.0014730e-04 9.9982721e-01 1.2565315e-02 9.1755509e-01
 9.7321260e-01 8.7583400e-03 2.4036532e-02 1.7948723e-03 4.1963568e-01
 2.8584033e-01 9.8566878e-01 8.9171952e-01 9.2001331e-01 9.6626318e-01
 1.0759953e-02 6.3028139e-01 2.1535615e-03 2.7465507e-01 9.9774134e-01
 2.7252309e-02 9.6671993e-01 9.9877673e-01 9.1492182e-01 3.7705481e-02
 2.9088405e-04 9.8870742e-01 1.6716550e-01 9.9561125e-01 9.0945369e-01
 4.7960278e-02 9.1812187e-01 4.6335179e-01 9.9731296e-01 9.8492420e-01
 8.7972149e-02 6.5745598e-01 9.3860066e-01 9.9913186e-01 9.4203639e-01
 1.5306894e-02 3.3816513e-01 2.1229699e-02 7.0039934e-04 8.9493728e-01
 2.6215887e-02 9.9136788e-01 9.9856514e-01 9.9775320e-01 1.1158179e-02
 9.9997187e-01 1.6909038e-03 1.2673473e-02 9.9705279e-01 9.9114251e-01
 3.9752968e-02 8.3145791e-01 1.5910601e-02 7.2419339e-01 1.7889865e-01
 9.9831653e-01 4.0587396e-03 9.9799097e-01 6.8190581e-01 6.3739358e-03
 6.3235476e-04 1.2284777e-01 9.9164975e-01 9.9387383e-01 9.5816457e-01
 9.7792017e-01 9.7724462e-01 2.5229440e-03 5.1188864e-02 9.9603528e-01
 6.5499480e-05 1.9486408e-03 2.0279525e-01 9.9261969e-01 9.7453862e-01
 6.3697970e-03 7.6444674e-01 1.0584772e-03 9.9058151e-01 9.5189589e-01
 9.9986863e-01 9.9953997e-01 1.6460760e-01 8.5684061e-02 5.3159207e-01
 1.9761484e-02 1.3817691e-03 1.3244527e-03 9.9795961e-01 9.9977475e-01
 5.6857051e-04 3.7878088e-03 1.5209408e-03 8.4494382e-01 9.9433535e-01
 9.9141568e-01 9.9016142e-01 3.9747161e-01 7.6456899e-03 8.3165359e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 12:57:21, Dev, Step : 1288, Loss : 0.60580, Acc : 0.713, Auc : 0.836, Sensitive_Loss : 0.15524, Sensitive_Acc : 16.207, Sensitive_Auc : 0.986, Mean auc: 0.836, Run Time : 98.68 sec
INFO:root:2024-04-10 12:57:22, Best, Step : 1288, Loss : 0.60580, Acc : 0.713,Auc : 0.836, Best Auc : 0.836, Sensitive_Loss : 0.15524, Sensitive_Acc : 16.207, Sensitive_Auc : 0.986
INFO:root:2024-04-10 12:57:26, Train, Epoch : 3, Step : 1290, Loss : 0.08079, Acc : 0.163, Sensitive_Loss : 0.04152, Sensitive_Acc : 3.000, Run Time : 2.65 sec
INFO:root:2024-04-10 12:57:36, Train, Epoch : 3, Step : 1300, Loss : 0.45893, Acc : 0.794, Sensitive_Loss : 0.14938, Sensitive_Acc : 17.500, Run Time : 10.25 sec
INFO:root:2024-04-10 12:59:09, Dev, Step : 1300, Loss : 0.54459, Acc : 0.748, Auc : 0.838, Sensitive_Loss : 0.16996, Sensitive_Acc : 16.236, Sensitive_Auc : 0.989, Mean auc: 0.838, Run Time : 92.76 sec
INFO:root:2024-04-10 12:59:09, Best, Step : 1300, Loss : 0.54459, Acc : 0.748, Auc : 0.838, Sensitive_Loss : 0.16996, Sensitive_Acc : 16.236, Sensitive_Auc : 0.989, Best Auc : 0.838
INFO:root:2024-04-10 12:59:18, Train, Epoch : 3, Step : 1310, Loss : 0.44792, Acc : 0.772, Sensitive_Loss : 0.15030, Sensitive_Acc : 15.400, Run Time : 102.02 sec
INFO:root:2024-04-10 12:59:31, Train, Epoch : 3, Step : 1320, Loss : 0.41733, Acc : 0.781, Sensitive_Loss : 0.16613, Sensitive_Acc : 16.900, Run Time : 13.24 sec
INFO:root:2024-04-10 12:59:43, Train, Epoch : 3, Step : 1330, Loss : 0.51221, Acc : 0.762, Sensitive_Loss : 0.14328, Sensitive_Acc : 16.600, Run Time : 11.26 sec
INFO:root:2024-04-10 12:59:52, Train, Epoch : 3, Step : 1340, Loss : 0.51430, Acc : 0.797, Sensitive_Loss : 0.17800, Sensitive_Acc : 16.300, Run Time : 9.93 sec
INFO:root:2024-04-10 13:00:05, Train, Epoch : 3, Step : 1350, Loss : 0.39955, Acc : 0.806, Sensitive_Loss : 0.14064, Sensitive_Acc : 15.200, Run Time : 12.58 sec
INFO:root:2024-04-10 13:00:14, Train, Epoch : 3, Step : 1360, Loss : 0.41714, Acc : 0.828, Sensitive_Loss : 0.17028, Sensitive_Acc : 15.500, Run Time : 9.32 sec
INFO:root:2024-04-10 13:00:24, Train, Epoch : 3, Step : 1370, Loss : 0.48086, Acc : 0.781, Sensitive_Loss : 0.18797, Sensitive_Acc : 16.900, Run Time : 9.48 sec
INFO:root:2024-04-10 13:00:36, Train, Epoch : 3, Step : 1380, Loss : 0.41773, Acc : 0.794, Sensitive_Loss : 0.17188, Sensitive_Acc : 15.900, Run Time : 11.96 sec
INFO:root:2024-04-10 13:00:45, Train, Epoch : 3, Step : 1390, Loss : 0.44107, Acc : 0.794, Sensitive_Loss : 0.16607, Sensitive_Acc : 13.100, Run Time : 9.19 sec
INFO:root:2024-04-10 13:00:55, Train, Epoch : 3, Step : 1400, Loss : 0.45751, Acc : 0.806, Sensitive_Loss : 0.19064, Sensitive_Acc : 16.000, Run Time : 9.71 sec
INFO:root:2024-04-10 13:02:28, Dev, Step : 1400, Loss : 0.52010, Acc : 0.768, Auc : 0.849, Sensitive_Loss : 0.17496, Sensitive_Acc : 16.350, Sensitive_Auc : 0.987, Mean auc: 0.849, Run Time : 93.31 sec
INFO:root:2024-04-10 13:02:29, Best, Step : 1400, Loss : 0.52010, Acc : 0.768, Auc : 0.849, Sensitive_Loss : 0.17496, Sensitive_Acc : 16.350, Sensitive_Auc : 0.987, Best Auc : 0.849
INFO:root:2024-04-10 13:02:36, Train, Epoch : 3, Step : 1410, Loss : 0.42472, Acc : 0.759, Sensitive_Loss : 0.14865, Sensitive_Acc : 15.600, Run Time : 101.23 sec
INFO:root:2024-04-10 13:02:48, Train, Epoch : 3, Step : 1420, Loss : 0.45454, Acc : 0.772, Sensitive_Loss : 0.14113, Sensitive_Acc : 18.100, Run Time : 11.80 sec
INFO:root:2024-04-10 13:02:58, Train, Epoch : 3, Step : 1430, Loss : 0.42809, Acc : 0.819, Sensitive_Loss : 0.14037, Sensitive_Acc : 15.600, Run Time : 9.92 sec
INFO:root:2024-04-10 13:03:07, Train, Epoch : 3, Step : 1440, Loss : 0.43830, Acc : 0.766, Sensitive_Loss : 0.15481, Sensitive_Acc : 17.100, Run Time : 8.98 sec
INFO:root:2024-04-10 13:03:17, Train, Epoch : 3, Step : 1450, Loss : 0.42375, Acc : 0.772, Sensitive_Loss : 0.14601, Sensitive_Acc : 17.200, Run Time : 10.04 sec
INFO:root:2024-04-10 13:03:28, Train, Epoch : 3, Step : 1460, Loss : 0.40677, Acc : 0.781, Sensitive_Loss : 0.11651, Sensitive_Acc : 16.200, Run Time : 11.63 sec
INFO:root:2024-04-10 13:03:39, Train, Epoch : 3, Step : 1470, Loss : 0.46109, Acc : 0.803, Sensitive_Loss : 0.16944, Sensitive_Acc : 15.900, Run Time : 10.35 sec
INFO:root:2024-04-10 13:03:49, Train, Epoch : 3, Step : 1480, Loss : 0.45438, Acc : 0.812, Sensitive_Loss : 0.17838, Sensitive_Acc : 15.400, Run Time : 9.86 sec
INFO:root:2024-04-10 13:03:58, Train, Epoch : 3, Step : 1490, Loss : 0.44386, Acc : 0.766, Sensitive_Loss : 0.16055, Sensitive_Acc : 14.900, Run Time : 9.97 sec
INFO:root:2024-04-10 13:04:10, Train, Epoch : 3, Step : 1500, Loss : 0.43995, Acc : 0.800, Sensitive_Loss : 0.13282, Sensitive_Acc : 16.700, Run Time : 11.13 sec
INFO:root:2024-04-10 13:05:45, Dev, Step : 1500, Loss : 0.51631, Acc : 0.767, Auc : 0.851, Sensitive_Loss : 0.16549, Sensitive_Acc : 16.407, Sensitive_Auc : 0.988, Mean auc: 0.851, Run Time : 95.18 sec
INFO:root:2024-04-10 13:05:45, Best, Step : 1500, Loss : 0.51631, Acc : 0.767, Auc : 0.851, Sensitive_Loss : 0.16549, Sensitive_Acc : 16.407, Sensitive_Auc : 0.988, Best Auc : 0.851
INFO:root:2024-04-10 13:05:53, Train, Epoch : 3, Step : 1510, Loss : 0.47146, Acc : 0.787, Sensitive_Loss : 0.12527, Sensitive_Acc : 18.400, Run Time : 102.93 sec
INFO:root:2024-04-10 13:06:04, Train, Epoch : 3, Step : 1520, Loss : 0.41622, Acc : 0.825, Sensitive_Loss : 0.14303, Sensitive_Acc : 15.100, Run Time : 11.67 sec
INFO:root:2024-04-10 13:06:15, Train, Epoch : 3, Step : 1530, Loss : 0.41444, Acc : 0.809, Sensitive_Loss : 0.15026, Sensitive_Acc : 16.300, Run Time : 10.92 sec
INFO:root:2024-04-10 13:06:28, Train, Epoch : 3, Step : 1540, Loss : 0.47882, Acc : 0.778, Sensitive_Loss : 0.11588, Sensitive_Acc : 14.900, Run Time : 13.20 sec
INFO:root:2024-04-10 13:06:39, Train, Epoch : 3, Step : 1550, Loss : 0.42680, Acc : 0.791, Sensitive_Loss : 0.16429, Sensitive_Acc : 17.500, Run Time : 10.55 sec
INFO:root:2024-04-10 13:06:51, Train, Epoch : 3, Step : 1560, Loss : 0.42664, Acc : 0.791, Sensitive_Loss : 0.14630, Sensitive_Acc : 16.300, Run Time : 12.27 sec
INFO:root:2024-04-10 13:07:05, Train, Epoch : 3, Step : 1570, Loss : 0.43434, Acc : 0.797, Sensitive_Loss : 0.13259, Sensitive_Acc : 16.900, Run Time : 14.32 sec
INFO:root:2024-04-10 13:07:15, Train, Epoch : 3, Step : 1580, Loss : 0.44679, Acc : 0.781, Sensitive_Loss : 0.11395, Sensitive_Acc : 16.800, Run Time : 9.52 sec
INFO:root:2024-04-10 13:07:26, Train, Epoch : 3, Step : 1590, Loss : 0.42130, Acc : 0.838, Sensitive_Loss : 0.18313, Sensitive_Acc : 15.800, Run Time : 11.35 sec
INFO:root:2024-04-10 13:07:38, Train, Epoch : 3, Step : 1600, Loss : 0.41575, Acc : 0.806, Sensitive_Loss : 0.14251, Sensitive_Acc : 16.000, Run Time : 11.89 sec
INFO:root:2024-04-10 13:09:13, Dev, Step : 1600, Loss : 0.51505, Acc : 0.768, Auc : 0.853, Sensitive_Loss : 0.15288, Sensitive_Acc : 16.336, Sensitive_Auc : 0.987, Mean auc: 0.853, Run Time : 95.20 sec
INFO:root:2024-04-10 13:09:16, Best, Step : 1600, Loss : 0.51505, Acc : 0.768, Auc : 0.853, Sensitive_Loss : 0.15288, Sensitive_Acc : 16.336, Sensitive_Auc : 0.987, Best Auc : 0.853
INFO:root:2024-04-10 13:09:24, Train, Epoch : 3, Step : 1610, Loss : 0.40574, Acc : 0.797, Sensitive_Loss : 0.11652, Sensitive_Acc : 15.900, Run Time : 106.08 sec
INFO:root:2024-04-10 13:09:35, Train, Epoch : 3, Step : 1620, Loss : 0.43003, Acc : 0.803, Sensitive_Loss : 0.16130, Sensitive_Acc : 15.600, Run Time : 10.55 sec
INFO:root:2024-04-10 13:09:45, Train, Epoch : 3, Step : 1630, Loss : 0.39191, Acc : 0.822, Sensitive_Loss : 0.18550, Sensitive_Acc : 15.400, Run Time : 10.25 sec
INFO:root:2024-04-10 13:09:57, Train, Epoch : 3, Step : 1640, Loss : 0.42207, Acc : 0.791, Sensitive_Loss : 0.15893, Sensitive_Acc : 17.200, Run Time : 12.19 sec
INFO:root:2024-04-10 13:10:06, Train, Epoch : 3, Step : 1650, Loss : 0.42169, Acc : 0.812, Sensitive_Loss : 0.12769, Sensitive_Acc : 14.900, Run Time : 9.10 sec
INFO:root:2024-04-10 13:10:15, Train, Epoch : 3, Step : 1660, Loss : 0.46467, Acc : 0.769, Sensitive_Loss : 0.16219, Sensitive_Acc : 16.900, Run Time : 8.85 sec
INFO:root:2024-04-10 13:10:28, Train, Epoch : 3, Step : 1670, Loss : 0.44615, Acc : 0.800, Sensitive_Loss : 0.13915, Sensitive_Acc : 16.500, Run Time : 12.99 sec
INFO:root:2024-04-10 13:10:38, Train, Epoch : 3, Step : 1680, Loss : 0.41024, Acc : 0.816, Sensitive_Loss : 0.16528, Sensitive_Acc : 16.400, Run Time : 9.67 sec
INFO:root:2024-04-10 13:10:48, Train, Epoch : 3, Step : 1690, Loss : 0.38384, Acc : 0.825, Sensitive_Loss : 0.17463, Sensitive_Acc : 17.800, Run Time : 9.71 sec
INFO:root:2024-04-10 13:10:57, Train, Epoch : 3, Step : 1700, Loss : 0.46653, Acc : 0.803, Sensitive_Loss : 0.13493, Sensitive_Acc : 16.600, Run Time : 9.37 sec
INFO:root:2024-04-10 13:12:35, Dev, Step : 1700, Loss : 0.50382, Acc : 0.771, Auc : 0.855, Sensitive_Loss : 0.15080, Sensitive_Acc : 16.421, Sensitive_Auc : 0.989, Mean auc: 0.855, Run Time : 98.15 sec
INFO:root:2024-04-10 13:12:36, Best, Step : 1700, Loss : 0.50382, Acc : 0.771, Auc : 0.855, Sensitive_Loss : 0.15080, Sensitive_Acc : 16.421, Sensitive_Auc : 0.989, Best Auc : 0.855
INFO:root:2024-04-10 13:12:43, Train, Epoch : 3, Step : 1710, Loss : 0.41573, Acc : 0.800, Sensitive_Loss : 0.14132, Sensitive_Acc : 17.000, Run Time : 106.46 sec
INFO:root:2024-04-10 13:12:53, Train, Epoch : 3, Step : 1720, Loss : 0.41969, Acc : 0.791, Sensitive_Loss : 0.17492, Sensitive_Acc : 16.100, Run Time : 9.54 sec
INFO:root:2024-04-10 13:13:05, Train, Epoch : 3, Step : 1730, Loss : 0.40014, Acc : 0.803, Sensitive_Loss : 0.09237, Sensitive_Acc : 17.200, Run Time : 12.12 sec
INFO:root:2024-04-10 13:13:15, Train, Epoch : 3, Step : 1740, Loss : 0.41538, Acc : 0.828, Sensitive_Loss : 0.14447, Sensitive_Acc : 16.100, Run Time : 9.93 sec
INFO:root:2024-04-10 13:13:25, Train, Epoch : 3, Step : 1750, Loss : 0.36587, Acc : 0.838, Sensitive_Loss : 0.15194, Sensitive_Acc : 16.600, Run Time : 10.09 sec
INFO:root:2024-04-10 13:13:39, Train, Epoch : 3, Step : 1760, Loss : 0.40153, Acc : 0.791, Sensitive_Loss : 0.16511, Sensitive_Acc : 15.800, Run Time : 13.37 sec
INFO:root:2024-04-10 13:13:49, Train, Epoch : 3, Step : 1770, Loss : 0.51897, Acc : 0.744, Sensitive_Loss : 0.14410, Sensitive_Acc : 15.200, Run Time : 10.20 sec
INFO:root:2024-04-10 13:13:59, Train, Epoch : 3, Step : 1780, Loss : 0.43377, Acc : 0.800, Sensitive_Loss : 0.19940, Sensitive_Acc : 16.900, Run Time : 9.98 sec
INFO:root:2024-04-10 13:14:12, Train, Epoch : 3, Step : 1790, Loss : 0.47444, Acc : 0.766, Sensitive_Loss : 0.12370, Sensitive_Acc : 15.500, Run Time : 13.56 sec
INFO:root:2024-04-10 13:14:24, Train, Epoch : 3, Step : 1800, Loss : 0.47486, Acc : 0.800, Sensitive_Loss : 0.14712, Sensitive_Acc : 14.100, Run Time : 11.33 sec
INFO:root:2024-04-10 13:16:06, Dev, Step : 1800, Loss : 0.51089, Acc : 0.773, Auc : 0.854, Sensitive_Loss : 0.14420, Sensitive_Acc : 16.436, Sensitive_Auc : 0.988, Mean auc: 0.854, Run Time : 102.30 sec
INFO:root:2024-04-10 13:16:12, Train, Epoch : 3, Step : 1810, Loss : 0.40258, Acc : 0.800, Sensitive_Loss : 0.14452, Sensitive_Acc : 16.800, Run Time : 108.86 sec
INFO:root:2024-04-10 13:16:21, Train, Epoch : 3, Step : 1820, Loss : 0.44239, Acc : 0.841, Sensitive_Loss : 0.13938, Sensitive_Acc : 16.500, Run Time : 8.85 sec
INFO:root:2024-04-10 13:16:32, Train, Epoch : 3, Step : 1830, Loss : 0.40403, Acc : 0.816, Sensitive_Loss : 0.17236, Sensitive_Acc : 15.100, Run Time : 10.56 sec
INFO:root:2024-04-10 13:16:42, Train, Epoch : 3, Step : 1840, Loss : 0.41099, Acc : 0.809, Sensitive_Loss : 0.16129, Sensitive_Acc : 15.300, Run Time : 10.43 sec
INFO:root:2024-04-10 13:16:51, Train, Epoch : 3, Step : 1850, Loss : 0.43415, Acc : 0.812, Sensitive_Loss : 0.11100, Sensitive_Acc : 14.500, Run Time : 9.14 sec
INFO:root:2024-04-10 13:17:01, Train, Epoch : 3, Step : 1860, Loss : 0.46195, Acc : 0.791, Sensitive_Loss : 0.15560, Sensitive_Acc : 16.800, Run Time : 9.40 sec
INFO:root:2024-04-10 13:17:11, Train, Epoch : 3, Step : 1870, Loss : 0.39979, Acc : 0.800, Sensitive_Loss : 0.15027, Sensitive_Acc : 16.000, Run Time : 10.28 sec
INFO:root:2024-04-10 13:17:22, Train, Epoch : 3, Step : 1880, Loss : 0.32701, Acc : 0.819, Sensitive_Loss : 0.12945, Sensitive_Acc : 16.900, Run Time : 10.79 sec
INFO:root:2024-04-10 13:17:31, Train, Epoch : 3, Step : 1890, Loss : 0.43900, Acc : 0.809, Sensitive_Loss : 0.12279, Sensitive_Acc : 16.000, Run Time : 9.41 sec
INFO:root:2024-04-10 13:17:41, Train, Epoch : 3, Step : 1900, Loss : 0.42215, Acc : 0.803, Sensitive_Loss : 0.13545, Sensitive_Acc : 15.900, Run Time : 9.36 sec
INFO:root:2024-04-10 13:19:19, Dev, Step : 1900, Loss : 0.51457, Acc : 0.773, Auc : 0.860, Sensitive_Loss : 0.17603, Sensitive_Acc : 16.321, Sensitive_Auc : 0.988, Mean auc: 0.860, Run Time : 98.31 sec
INFO:root:2024-04-10 13:19:20, Best, Step : 1900, Loss : 0.51457, Acc : 0.773, Auc : 0.860, Sensitive_Loss : 0.17603, Sensitive_Acc : 16.321, Sensitive_Auc : 0.988, Best Auc : 0.860
INFO:root:2024-04-10 13:19:27, Train, Epoch : 3, Step : 1910, Loss : 0.38647, Acc : 0.816, Sensitive_Loss : 0.14263, Sensitive_Acc : 16.500, Run Time : 106.46 sec
INFO:root:2024-04-10 13:19:39, Train, Epoch : 3, Step : 1920, Loss : 0.48572, Acc : 0.784, Sensitive_Loss : 0.14378, Sensitive_Acc : 13.900, Run Time : 11.74 sec
INFO:root:2024-04-10 13:19:48, Train, Epoch : 3, Step : 1930, Loss : 0.45602, Acc : 0.741, Sensitive_Loss : 0.15303, Sensitive_Acc : 17.700, Run Time : 9.04 sec
INFO:root:2024-04-10 13:21:32
INFO:root:y_pred: [0.10369623 0.10312546 0.86566937 ... 0.6165716  0.7115506  0.43641153]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9940348e-01 4.1323369e-03 7.1141309e-01 9.9997568e-01 9.9912018e-01
 9.8963213e-01 9.9952662e-01 5.9087106e-05 8.9829516e-01 9.9807173e-01
 3.0669901e-01 2.1628517e-01 3.5861405e-04 9.7000039e-01 9.9973732e-01
 9.9998724e-01 9.9935728e-01 9.8748869e-01 9.9823534e-01 9.8970371e-01
 9.9739897e-01 3.7153074e-01 9.9796593e-01 8.6497515e-01 9.0080959e-01
 9.7788736e-02 9.9294174e-01 5.9555698e-02 9.9976403e-01 5.5125114e-02
 1.0459626e-01 3.9319500e-01 7.8751193e-03 9.9875224e-01 6.6103867e-06
 9.8973829e-01 6.8445987e-04 9.9996197e-01 9.9643171e-03 9.7314036e-01
 9.9590945e-01 9.8450817e-03 1.3217727e-02 2.2714119e-03 4.7769168e-01
 6.8047601e-01 9.9925047e-01 9.4099033e-01 9.7194844e-01 9.8852915e-01
 1.1628882e-02 9.3725997e-01 2.5773413e-02 9.7180493e-02 9.9984360e-01
 5.0444547e-02 9.9336421e-01 9.9967098e-01 9.6942490e-01 2.5676476e-02
 2.3843811e-03 9.9892932e-01 2.0665908e-01 9.9937123e-01 9.9130005e-01
 2.9241556e-01 9.5719039e-01 7.6934469e-01 9.9971956e-01 9.9964046e-01
 1.6363597e-01 7.8760904e-01 9.8959363e-01 9.9993098e-01 9.8796332e-01
 6.7600653e-02 4.8511508e-01 1.7522654e-02 1.9153693e-03 9.9027574e-01
 8.8519298e-02 9.9909043e-01 9.9986529e-01 9.9915254e-01 2.2053624e-02
 9.9999774e-01 7.0811477e-03 9.1804331e-03 9.9960655e-01 9.9720484e-01
 1.6819102e-01 9.3769914e-01 2.4096321e-02 9.5201200e-01 1.5628073e-01
 9.9976736e-01 1.1297230e-02 9.9935943e-01 9.5098108e-01 2.4684755e-02
 3.7548941e-04 3.0622214e-01 9.9730492e-01 9.9862111e-01 9.8818910e-01
 9.8799592e-01 9.8531556e-01 1.0439075e-02 1.9411662e-01 9.9921310e-01
 2.1794885e-04 6.9614075e-04 1.6715366e-01 9.9965906e-01 9.9487853e-01
 1.2298492e-02 9.2693818e-01 7.6935417e-04 9.9896550e-01 9.7065836e-01
 9.9998224e-01 9.9996769e-01 1.4090456e-01 2.2589390e-01 3.7795043e-01
 1.8848158e-02 5.9292563e-03 1.5715259e-03 9.9812633e-01 9.9994564e-01
 7.6179435e-03 4.9274527e-03 2.0405836e-03 9.2167264e-01 9.9908555e-01
 9.9673080e-01 9.9824131e-01 8.0183500e-01 3.4900084e-02 9.7848892e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 13:21:32, Dev, Step : 1932, Loss : 0.50607, Acc : 0.778, Auc : 0.857, Sensitive_Loss : 0.16630, Sensitive_Acc : 16.436, Sensitive_Auc : 0.991, Mean auc: 0.857, Run Time : 102.41 sec
INFO:root:2024-04-10 13:21:42, Train, Epoch : 4, Step : 1940, Loss : 0.33819, Acc : 0.622, Sensitive_Loss : 0.13371, Sensitive_Acc : 12.200, Run Time : 9.04 sec
INFO:root:2024-04-10 13:21:51, Train, Epoch : 4, Step : 1950, Loss : 0.43762, Acc : 0.809, Sensitive_Loss : 0.12676, Sensitive_Acc : 16.300, Run Time : 9.36 sec
INFO:root:2024-04-10 13:22:01, Train, Epoch : 4, Step : 1960, Loss : 0.40709, Acc : 0.825, Sensitive_Loss : 0.17297, Sensitive_Acc : 16.200, Run Time : 9.47 sec
INFO:root:2024-04-10 13:22:10, Train, Epoch : 4, Step : 1970, Loss : 0.35378, Acc : 0.825, Sensitive_Loss : 0.18654, Sensitive_Acc : 14.900, Run Time : 9.19 sec
INFO:root:2024-04-10 13:22:20, Train, Epoch : 4, Step : 1980, Loss : 0.35956, Acc : 0.834, Sensitive_Loss : 0.16160, Sensitive_Acc : 17.000, Run Time : 9.95 sec
INFO:root:2024-04-10 13:22:29, Train, Epoch : 4, Step : 1990, Loss : 0.37269, Acc : 0.838, Sensitive_Loss : 0.18368, Sensitive_Acc : 15.700, Run Time : 9.42 sec
INFO:root:2024-04-10 13:22:38, Train, Epoch : 4, Step : 2000, Loss : 0.36584, Acc : 0.856, Sensitive_Loss : 0.12996, Sensitive_Acc : 14.700, Run Time : 9.27 sec
INFO:root:2024-04-10 13:24:16, Dev, Step : 2000, Loss : 0.52871, Acc : 0.761, Auc : 0.857, Sensitive_Loss : 0.16716, Sensitive_Acc : 16.393, Sensitive_Auc : 0.991, Mean auc: 0.857, Run Time : 97.08 sec
INFO:root:2024-04-10 13:24:22, Train, Epoch : 4, Step : 2010, Loss : 0.33763, Acc : 0.822, Sensitive_Loss : 0.13746, Sensitive_Acc : 16.100, Run Time : 103.66 sec
INFO:root:2024-04-10 13:24:31, Train, Epoch : 4, Step : 2020, Loss : 0.45880, Acc : 0.791, Sensitive_Loss : 0.16884, Sensitive_Acc : 14.900, Run Time : 9.33 sec
INFO:root:2024-04-10 13:24:43, Train, Epoch : 4, Step : 2030, Loss : 0.36547, Acc : 0.834, Sensitive_Loss : 0.15951, Sensitive_Acc : 15.600, Run Time : 11.41 sec
INFO:root:2024-04-10 13:24:54, Train, Epoch : 4, Step : 2040, Loss : 0.40606, Acc : 0.822, Sensitive_Loss : 0.12013, Sensitive_Acc : 17.800, Run Time : 11.19 sec
INFO:root:2024-04-10 13:25:03, Train, Epoch : 4, Step : 2050, Loss : 0.37540, Acc : 0.822, Sensitive_Loss : 0.17875, Sensitive_Acc : 16.000, Run Time : 9.19 sec
INFO:root:2024-04-10 13:25:12, Train, Epoch : 4, Step : 2060, Loss : 0.44953, Acc : 0.787, Sensitive_Loss : 0.12720, Sensitive_Acc : 16.100, Run Time : 9.22 sec
INFO:root:2024-04-10 13:25:22, Train, Epoch : 4, Step : 2070, Loss : 0.44340, Acc : 0.787, Sensitive_Loss : 0.12858, Sensitive_Acc : 17.200, Run Time : 9.60 sec
INFO:root:2024-04-10 13:25:31, Train, Epoch : 4, Step : 2080, Loss : 0.36116, Acc : 0.834, Sensitive_Loss : 0.10996, Sensitive_Acc : 18.100, Run Time : 8.88 sec
INFO:root:2024-04-10 13:25:40, Train, Epoch : 4, Step : 2090, Loss : 0.33469, Acc : 0.847, Sensitive_Loss : 0.12300, Sensitive_Acc : 17.400, Run Time : 9.09 sec
INFO:root:2024-04-10 13:25:49, Train, Epoch : 4, Step : 2100, Loss : 0.45054, Acc : 0.816, Sensitive_Loss : 0.17491, Sensitive_Acc : 16.600, Run Time : 9.01 sec
INFO:root:2024-04-10 13:27:26, Dev, Step : 2100, Loss : 0.50838, Acc : 0.780, Auc : 0.859, Sensitive_Loss : 0.16626, Sensitive_Acc : 16.321, Sensitive_Auc : 0.989, Mean auc: 0.859, Run Time : 97.11 sec
INFO:root:2024-04-10 13:27:32, Train, Epoch : 4, Step : 2110, Loss : 0.43016, Acc : 0.819, Sensitive_Loss : 0.12986, Sensitive_Acc : 15.300, Run Time : 103.41 sec
INFO:root:2024-04-10 13:27:46, Train, Epoch : 4, Step : 2120, Loss : 0.47608, Acc : 0.784, Sensitive_Loss : 0.15206, Sensitive_Acc : 15.700, Run Time : 13.73 sec
INFO:root:2024-04-10 13:27:56, Train, Epoch : 4, Step : 2130, Loss : 0.44075, Acc : 0.784, Sensitive_Loss : 0.13125, Sensitive_Acc : 17.100, Run Time : 10.15 sec
INFO:root:2024-04-10 13:28:05, Train, Epoch : 4, Step : 2140, Loss : 0.40346, Acc : 0.791, Sensitive_Loss : 0.13880, Sensitive_Acc : 17.100, Run Time : 8.96 sec
INFO:root:2024-04-10 13:28:15, Train, Epoch : 4, Step : 2150, Loss : 0.42956, Acc : 0.794, Sensitive_Loss : 0.14683, Sensitive_Acc : 17.200, Run Time : 9.30 sec
INFO:root:2024-04-10 13:28:24, Train, Epoch : 4, Step : 2160, Loss : 0.40139, Acc : 0.825, Sensitive_Loss : 0.13980, Sensitive_Acc : 16.300, Run Time : 9.43 sec
INFO:root:2024-04-10 13:28:33, Train, Epoch : 4, Step : 2170, Loss : 0.35933, Acc : 0.869, Sensitive_Loss : 0.12446, Sensitive_Acc : 15.900, Run Time : 8.89 sec
INFO:root:2024-04-10 13:28:42, Train, Epoch : 4, Step : 2180, Loss : 0.37514, Acc : 0.825, Sensitive_Loss : 0.11641, Sensitive_Acc : 15.900, Run Time : 9.31 sec
INFO:root:2024-04-10 13:28:53, Train, Epoch : 4, Step : 2190, Loss : 0.35408, Acc : 0.856, Sensitive_Loss : 0.16564, Sensitive_Acc : 15.400, Run Time : 10.65 sec
INFO:root:2024-04-10 13:29:02, Train, Epoch : 4, Step : 2200, Loss : 0.45037, Acc : 0.800, Sensitive_Loss : 0.12406, Sensitive_Acc : 17.000, Run Time : 8.75 sec
INFO:root:2024-04-10 13:30:40, Dev, Step : 2200, Loss : 0.49537, Acc : 0.781, Auc : 0.859, Sensitive_Loss : 0.17894, Sensitive_Acc : 16.364, Sensitive_Auc : 0.990, Mean auc: 0.859, Run Time : 98.35 sec
INFO:root:2024-04-10 13:30:47, Train, Epoch : 4, Step : 2210, Loss : 0.33352, Acc : 0.844, Sensitive_Loss : 0.11345, Sensitive_Acc : 15.700, Run Time : 105.36 sec
INFO:root:2024-04-10 13:30:57, Train, Epoch : 4, Step : 2220, Loss : 0.40601, Acc : 0.834, Sensitive_Loss : 0.16936, Sensitive_Acc : 16.400, Run Time : 10.41 sec
INFO:root:2024-04-10 13:31:08, Train, Epoch : 4, Step : 2230, Loss : 0.41960, Acc : 0.816, Sensitive_Loss : 0.11681, Sensitive_Acc : 16.800, Run Time : 10.74 sec
INFO:root:2024-04-10 13:31:20, Train, Epoch : 4, Step : 2240, Loss : 0.41497, Acc : 0.812, Sensitive_Loss : 0.15415, Sensitive_Acc : 15.700, Run Time : 11.54 sec
INFO:root:2024-04-10 13:31:29, Train, Epoch : 4, Step : 2250, Loss : 0.38538, Acc : 0.812, Sensitive_Loss : 0.12748, Sensitive_Acc : 15.700, Run Time : 9.31 sec
INFO:root:2024-04-10 13:31:40, Train, Epoch : 4, Step : 2260, Loss : 0.33256, Acc : 0.838, Sensitive_Loss : 0.15423, Sensitive_Acc : 15.600, Run Time : 10.85 sec
INFO:root:2024-04-10 13:31:50, Train, Epoch : 4, Step : 2270, Loss : 0.35895, Acc : 0.869, Sensitive_Loss : 0.18717, Sensitive_Acc : 16.700, Run Time : 9.95 sec
INFO:root:2024-04-10 13:31:59, Train, Epoch : 4, Step : 2280, Loss : 0.45712, Acc : 0.784, Sensitive_Loss : 0.11168, Sensitive_Acc : 16.600, Run Time : 8.91 sec
INFO:root:2024-04-10 13:32:09, Train, Epoch : 4, Step : 2290, Loss : 0.49393, Acc : 0.803, Sensitive_Loss : 0.20760, Sensitive_Acc : 17.600, Run Time : 10.26 sec
INFO:root:2024-04-10 13:32:19, Train, Epoch : 4, Step : 2300, Loss : 0.39387, Acc : 0.800, Sensitive_Loss : 0.15711, Sensitive_Acc : 16.800, Run Time : 10.43 sec
INFO:root:2024-04-10 13:33:56, Dev, Step : 2300, Loss : 0.53412, Acc : 0.766, Auc : 0.859, Sensitive_Loss : 0.16253, Sensitive_Acc : 16.364, Sensitive_Auc : 0.992, Mean auc: 0.859, Run Time : 96.21 sec
INFO:root:2024-04-10 13:34:03, Train, Epoch : 4, Step : 2310, Loss : 0.43562, Acc : 0.744, Sensitive_Loss : 0.08836, Sensitive_Acc : 17.700, Run Time : 103.42 sec
INFO:root:2024-04-10 13:34:14, Train, Epoch : 4, Step : 2320, Loss : 0.45566, Acc : 0.791, Sensitive_Loss : 0.12341, Sensitive_Acc : 17.000, Run Time : 10.89 sec
INFO:root:2024-04-10 13:34:28, Train, Epoch : 4, Step : 2330, Loss : 0.40592, Acc : 0.812, Sensitive_Loss : 0.13048, Sensitive_Acc : 17.300, Run Time : 14.17 sec
INFO:root:2024-04-10 13:34:41, Train, Epoch : 4, Step : 2340, Loss : 0.36798, Acc : 0.859, Sensitive_Loss : 0.14367, Sensitive_Acc : 15.300, Run Time : 12.80 sec
INFO:root:2024-04-10 13:34:51, Train, Epoch : 4, Step : 2350, Loss : 0.40820, Acc : 0.825, Sensitive_Loss : 0.10340, Sensitive_Acc : 16.500, Run Time : 10.68 sec
INFO:root:2024-04-10 13:35:04, Train, Epoch : 4, Step : 2360, Loss : 0.38801, Acc : 0.816, Sensitive_Loss : 0.17293, Sensitive_Acc : 17.300, Run Time : 12.63 sec
INFO:root:2024-04-10 13:35:16, Train, Epoch : 4, Step : 2370, Loss : 0.41368, Acc : 0.838, Sensitive_Loss : 0.11708, Sensitive_Acc : 16.000, Run Time : 11.94 sec
INFO:root:2024-04-10 13:35:29, Train, Epoch : 4, Step : 2380, Loss : 0.40571, Acc : 0.844, Sensitive_Loss : 0.11045, Sensitive_Acc : 15.600, Run Time : 13.51 sec
INFO:root:2024-04-10 13:35:40, Train, Epoch : 4, Step : 2390, Loss : 0.37927, Acc : 0.816, Sensitive_Loss : 0.11510, Sensitive_Acc : 17.000, Run Time : 10.14 sec
INFO:root:2024-04-10 13:35:51, Train, Epoch : 4, Step : 2400, Loss : 0.37989, Acc : 0.825, Sensitive_Loss : 0.12259, Sensitive_Acc : 18.600, Run Time : 11.52 sec
INFO:root:2024-04-10 13:37:24, Dev, Step : 2400, Loss : 0.51739, Acc : 0.771, Auc : 0.860, Sensitive_Loss : 0.15782, Sensitive_Acc : 16.407, Sensitive_Auc : 0.990, Mean auc: 0.860, Run Time : 92.52 sec
INFO:root:2024-04-10 13:37:25, Best, Step : 2400, Loss : 0.51739, Acc : 0.771, Auc : 0.860, Sensitive_Loss : 0.15782, Sensitive_Acc : 16.407, Sensitive_Auc : 0.990, Best Auc : 0.860
INFO:root:2024-04-10 13:37:31, Train, Epoch : 4, Step : 2410, Loss : 0.42170, Acc : 0.791, Sensitive_Loss : 0.13629, Sensitive_Acc : 15.200, Run Time : 100.33 sec
INFO:root:2024-04-10 13:37:41, Train, Epoch : 4, Step : 2420, Loss : 0.44282, Acc : 0.791, Sensitive_Loss : 0.12971, Sensitive_Acc : 16.400, Run Time : 9.66 sec
INFO:root:2024-04-10 13:37:52, Train, Epoch : 4, Step : 2430, Loss : 0.35277, Acc : 0.863, Sensitive_Loss : 0.16617, Sensitive_Acc : 16.800, Run Time : 10.57 sec
INFO:root:2024-04-10 13:38:02, Train, Epoch : 4, Step : 2440, Loss : 0.46044, Acc : 0.766, Sensitive_Loss : 0.08751, Sensitive_Acc : 15.800, Run Time : 10.24 sec
INFO:root:2024-04-10 13:38:13, Train, Epoch : 4, Step : 2450, Loss : 0.43978, Acc : 0.822, Sensitive_Loss : 0.12598, Sensitive_Acc : 16.500, Run Time : 11.64 sec
INFO:root:2024-04-10 13:38:24, Train, Epoch : 4, Step : 2460, Loss : 0.41113, Acc : 0.844, Sensitive_Loss : 0.08443, Sensitive_Acc : 16.700, Run Time : 10.29 sec
INFO:root:2024-04-10 13:38:34, Train, Epoch : 4, Step : 2470, Loss : 0.42370, Acc : 0.800, Sensitive_Loss : 0.15454, Sensitive_Acc : 15.700, Run Time : 9.72 sec
INFO:root:2024-04-10 13:38:42, Train, Epoch : 4, Step : 2480, Loss : 0.42987, Acc : 0.831, Sensitive_Loss : 0.15318, Sensitive_Acc : 15.200, Run Time : 8.86 sec
INFO:root:2024-04-10 13:38:51, Train, Epoch : 4, Step : 2490, Loss : 0.35473, Acc : 0.822, Sensitive_Loss : 0.10555, Sensitive_Acc : 15.500, Run Time : 8.98 sec
INFO:root:2024-04-10 13:39:01, Train, Epoch : 4, Step : 2500, Loss : 0.41740, Acc : 0.794, Sensitive_Loss : 0.14415, Sensitive_Acc : 16.700, Run Time : 9.23 sec
INFO:root:2024-04-10 13:40:33, Dev, Step : 2500, Loss : 0.51727, Acc : 0.775, Auc : 0.861, Sensitive_Loss : 0.15569, Sensitive_Acc : 16.407, Sensitive_Auc : 0.989, Mean auc: 0.861, Run Time : 92.83 sec
INFO:root:2024-04-10 13:40:36, Best, Step : 2500, Loss : 0.51727, Acc : 0.775, Auc : 0.861, Sensitive_Loss : 0.15569, Sensitive_Acc : 16.407, Sensitive_Auc : 0.989, Best Auc : 0.861
INFO:root:2024-04-10 13:40:46, Train, Epoch : 4, Step : 2510, Loss : 0.42073, Acc : 0.803, Sensitive_Loss : 0.11000, Sensitive_Acc : 16.800, Run Time : 105.54 sec
INFO:root:2024-04-10 13:40:57, Train, Epoch : 4, Step : 2520, Loss : 0.42661, Acc : 0.806, Sensitive_Loss : 0.14850, Sensitive_Acc : 16.900, Run Time : 11.34 sec
INFO:root:2024-04-10 13:41:08, Train, Epoch : 4, Step : 2530, Loss : 0.39264, Acc : 0.809, Sensitive_Loss : 0.11295, Sensitive_Acc : 14.300, Run Time : 10.45 sec
INFO:root:2024-04-10 13:41:21, Train, Epoch : 4, Step : 2540, Loss : 0.37024, Acc : 0.834, Sensitive_Loss : 0.10861, Sensitive_Acc : 15.900, Run Time : 13.18 sec
INFO:root:2024-04-10 13:41:32, Train, Epoch : 4, Step : 2550, Loss : 0.39083, Acc : 0.822, Sensitive_Loss : 0.11564, Sensitive_Acc : 16.200, Run Time : 11.09 sec
INFO:root:2024-04-10 13:41:42, Train, Epoch : 4, Step : 2560, Loss : 0.38964, Acc : 0.831, Sensitive_Loss : 0.10291, Sensitive_Acc : 15.700, Run Time : 9.60 sec
INFO:root:2024-04-10 13:41:53, Train, Epoch : 4, Step : 2570, Loss : 0.42079, Acc : 0.816, Sensitive_Loss : 0.11893, Sensitive_Acc : 16.700, Run Time : 10.79 sec
INFO:root:2024-04-10 13:43:53
INFO:root:y_pred: [0.08927381 0.03064769 0.87474304 ... 0.6372096  0.646782   0.31178942]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.99454200e-01 2.25053309e-03 7.68585861e-01 9.99986768e-01
 9.99304175e-01 9.76596534e-01 9.99598444e-01 1.10961693e-04
 9.30294335e-01 9.97794747e-01 3.01364660e-01 1.33141786e-01
 2.24277537e-04 9.62542176e-01 9.99857783e-01 9.99976277e-01
 9.99090433e-01 9.86493051e-01 9.99055088e-01 9.94005024e-01
 9.97499526e-01 4.26231861e-01 9.98797417e-01 9.00498569e-01
 8.47108305e-01 9.16174203e-02 9.95707214e-01 2.23944206e-02
 9.99671817e-01 6.23692051e-02 6.04038201e-02 4.75617349e-01
 1.16501395e-02 9.99066293e-01 5.10437349e-06 9.93178844e-01
 2.89894029e-04 9.99934554e-01 7.89059233e-03 9.74562824e-01
 9.97448206e-01 5.22084115e-03 6.47992967e-03 1.31701620e-03
 4.46105361e-01 6.15557194e-01 9.99334991e-01 9.62736487e-01
 9.59101200e-01 9.94730473e-01 4.69516451e-03 9.52253103e-01
 2.00648680e-02 1.00587904e-01 9.99909759e-01 5.50213270e-02
 9.96516228e-01 9.99499559e-01 9.71911490e-01 1.00250263e-02
 5.02845459e-03 9.99546587e-01 2.13859975e-01 9.99701917e-01
 9.92634952e-01 4.55839574e-01 9.63278413e-01 8.63840401e-01
 9.99813855e-01 9.99600828e-01 1.01541765e-01 7.34124362e-01
 9.92486477e-01 9.99936819e-01 9.87258375e-01 3.04095391e-02
 6.72342241e-01 1.12653356e-02 1.63525797e-03 9.95936990e-01
 4.10079695e-02 9.99499083e-01 9.99886870e-01 9.98394072e-01
 1.86813604e-02 9.99998927e-01 4.67139715e-03 3.21220681e-02
 9.99317646e-01 9.97342885e-01 8.85161459e-02 8.84375155e-01
 1.36342384e-02 9.41074610e-01 8.89734775e-02 9.99921560e-01
 1.44981351e-02 9.98332322e-01 8.75151873e-01 2.60345321e-02
 2.69466545e-04 3.79139483e-01 9.97130215e-01 9.99144316e-01
 9.85154927e-01 9.75591719e-01 9.84036386e-01 8.47307220e-03
 1.34416044e-01 9.99341071e-01 3.35057062e-04 1.75622627e-04
 1.14473611e-01 9.99837875e-01 9.95783687e-01 5.51374070e-03
 9.50877190e-01 5.08563593e-04 9.99391675e-01 9.69437778e-01
 9.99987125e-01 9.99990940e-01 2.45981768e-01 1.81972861e-01
 2.66208619e-01 1.47318589e-02 5.65050216e-03 1.14821165e-03
 9.98978853e-01 9.99979377e-01 9.08544939e-03 3.77400382e-03
 1.15511601e-03 9.05610859e-01 9.99613941e-01 9.95693922e-01
 9.98994410e-01 7.98575699e-01 4.06732000e-02 9.70825493e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 13:43:53, Dev, Step : 2576, Loss : 0.51012, Acc : 0.780, Auc : 0.861, Sensitive_Loss : 0.15906, Sensitive_Acc : 16.407, Sensitive_Auc : 0.990, Mean auc: 0.861, Run Time : 114.90 sec
INFO:root:2024-04-10 13:43:56, Best, Step : 2576, Loss : 0.51012, Acc : 0.780,Auc : 0.861, Best Auc : 0.861, Sensitive_Loss : 0.15906, Sensitive_Acc : 16.407, Sensitive_Auc : 0.990
INFO:root:2024-04-10 13:44:04, Train, Epoch : 5, Step : 2580, Loss : 0.14161, Acc : 0.350, Sensitive_Loss : 0.07030, Sensitive_Acc : 7.200, Run Time : 5.95 sec
INFO:root:2024-04-10 13:44:13, Train, Epoch : 5, Step : 2590, Loss : 0.40552, Acc : 0.822, Sensitive_Loss : 0.15849, Sensitive_Acc : 16.800, Run Time : 9.49 sec
INFO:root:2024-04-10 13:44:25, Train, Epoch : 5, Step : 2600, Loss : 0.39468, Acc : 0.838, Sensitive_Loss : 0.10697, Sensitive_Acc : 15.500, Run Time : 11.75 sec
INFO:root:2024-04-10 13:46:05, Dev, Step : 2600, Loss : 0.51086, Acc : 0.773, Auc : 0.863, Sensitive_Loss : 0.14705, Sensitive_Acc : 16.436, Sensitive_Auc : 0.989, Mean auc: 0.863, Run Time : 99.89 sec
INFO:root:2024-04-10 13:46:07, Best, Step : 2600, Loss : 0.51086, Acc : 0.773, Auc : 0.863, Sensitive_Loss : 0.14705, Sensitive_Acc : 16.436, Sensitive_Auc : 0.989, Best Auc : 0.863
INFO:root:2024-04-10 13:46:16, Train, Epoch : 5, Step : 2610, Loss : 0.38694, Acc : 0.816, Sensitive_Loss : 0.12852, Sensitive_Acc : 17.800, Run Time : 110.87 sec
INFO:root:2024-04-10 13:46:25, Train, Epoch : 5, Step : 2620, Loss : 0.46056, Acc : 0.812, Sensitive_Loss : 0.17084, Sensitive_Acc : 18.000, Run Time : 9.22 sec
INFO:root:2024-04-10 13:46:34, Train, Epoch : 5, Step : 2630, Loss : 0.36612, Acc : 0.838, Sensitive_Loss : 0.12921, Sensitive_Acc : 17.700, Run Time : 9.06 sec
INFO:root:2024-04-10 13:46:44, Train, Epoch : 5, Step : 2640, Loss : 0.36061, Acc : 0.816, Sensitive_Loss : 0.10922, Sensitive_Acc : 16.800, Run Time : 9.39 sec
INFO:root:2024-04-10 13:46:53, Train, Epoch : 5, Step : 2650, Loss : 0.36210, Acc : 0.831, Sensitive_Loss : 0.16270, Sensitive_Acc : 15.900, Run Time : 9.44 sec
INFO:root:2024-04-10 13:47:04, Train, Epoch : 5, Step : 2660, Loss : 0.36348, Acc : 0.822, Sensitive_Loss : 0.10814, Sensitive_Acc : 16.000, Run Time : 10.40 sec
INFO:root:2024-04-10 13:47:16, Train, Epoch : 5, Step : 2670, Loss : 0.39763, Acc : 0.825, Sensitive_Loss : 0.13440, Sensitive_Acc : 18.400, Run Time : 12.28 sec
INFO:root:2024-04-10 13:47:26, Train, Epoch : 5, Step : 2680, Loss : 0.32717, Acc : 0.863, Sensitive_Loss : 0.15832, Sensitive_Acc : 16.200, Run Time : 10.44 sec
INFO:root:2024-04-10 13:47:37, Train, Epoch : 5, Step : 2690, Loss : 0.38440, Acc : 0.831, Sensitive_Loss : 0.09931, Sensitive_Acc : 17.600, Run Time : 10.39 sec
INFO:root:2024-04-10 13:47:50, Train, Epoch : 5, Step : 2700, Loss : 0.47711, Acc : 0.794, Sensitive_Loss : 0.10965, Sensitive_Acc : 15.800, Run Time : 13.01 sec
INFO:root:2024-04-10 13:49:29, Dev, Step : 2700, Loss : 0.52592, Acc : 0.772, Auc : 0.856, Sensitive_Loss : 0.15253, Sensitive_Acc : 16.321, Sensitive_Auc : 0.992, Mean auc: 0.856, Run Time : 99.35 sec
INFO:root:2024-04-10 13:49:37, Train, Epoch : 5, Step : 2710, Loss : 0.44288, Acc : 0.812, Sensitive_Loss : 0.11009, Sensitive_Acc : 17.200, Run Time : 106.86 sec
INFO:root:2024-04-10 13:49:47, Train, Epoch : 5, Step : 2720, Loss : 0.34259, Acc : 0.831, Sensitive_Loss : 0.14773, Sensitive_Acc : 16.500, Run Time : 10.65 sec
INFO:root:2024-04-10 13:49:57, Train, Epoch : 5, Step : 2730, Loss : 0.32766, Acc : 0.828, Sensitive_Loss : 0.20452, Sensitive_Acc : 16.400, Run Time : 9.60 sec
INFO:root:2024-04-10 13:50:12, Train, Epoch : 5, Step : 2740, Loss : 0.35976, Acc : 0.816, Sensitive_Loss : 0.12748, Sensitive_Acc : 16.300, Run Time : 15.51 sec
INFO:root:2024-04-10 13:50:22, Train, Epoch : 5, Step : 2750, Loss : 0.36414, Acc : 0.838, Sensitive_Loss : 0.07383, Sensitive_Acc : 16.400, Run Time : 9.50 sec
INFO:root:2024-04-10 13:50:31, Train, Epoch : 5, Step : 2760, Loss : 0.35983, Acc : 0.831, Sensitive_Loss : 0.11816, Sensitive_Acc : 15.200, Run Time : 9.43 sec
INFO:root:2024-04-10 13:50:43, Train, Epoch : 5, Step : 2770, Loss : 0.38874, Acc : 0.816, Sensitive_Loss : 0.11232, Sensitive_Acc : 15.600, Run Time : 11.31 sec
INFO:root:2024-04-10 13:50:52, Train, Epoch : 5, Step : 2780, Loss : 0.32133, Acc : 0.819, Sensitive_Loss : 0.11505, Sensitive_Acc : 16.200, Run Time : 9.34 sec
INFO:root:2024-04-10 13:51:02, Train, Epoch : 5, Step : 2790, Loss : 0.34905, Acc : 0.831, Sensitive_Loss : 0.09840, Sensitive_Acc : 16.500, Run Time : 10.54 sec
INFO:root:2024-04-10 13:51:15, Train, Epoch : 5, Step : 2800, Loss : 0.33476, Acc : 0.812, Sensitive_Loss : 0.15158, Sensitive_Acc : 16.200, Run Time : 12.11 sec
INFO:root:2024-04-10 13:52:59, Dev, Step : 2800, Loss : 0.52557, Acc : 0.771, Auc : 0.861, Sensitive_Loss : 0.15442, Sensitive_Acc : 16.307, Sensitive_Auc : 0.991, Mean auc: 0.861, Run Time : 104.11 sec
INFO:root:2024-04-10 13:53:09, Train, Epoch : 5, Step : 2810, Loss : 0.43544, Acc : 0.822, Sensitive_Loss : 0.16356, Sensitive_Acc : 16.900, Run Time : 114.56 sec
INFO:root:2024-04-10 13:53:19, Train, Epoch : 5, Step : 2820, Loss : 0.35970, Acc : 0.869, Sensitive_Loss : 0.12419, Sensitive_Acc : 15.700, Run Time : 9.86 sec
INFO:root:2024-04-10 13:53:28, Train, Epoch : 5, Step : 2830, Loss : 0.38799, Acc : 0.834, Sensitive_Loss : 0.17527, Sensitive_Acc : 15.400, Run Time : 9.27 sec
INFO:root:2024-04-10 13:53:39, Train, Epoch : 5, Step : 2840, Loss : 0.36773, Acc : 0.816, Sensitive_Loss : 0.11129, Sensitive_Acc : 16.200, Run Time : 10.28 sec
INFO:root:2024-04-10 13:53:49, Train, Epoch : 5, Step : 2850, Loss : 0.36276, Acc : 0.831, Sensitive_Loss : 0.15530, Sensitive_Acc : 16.600, Run Time : 10.19 sec
INFO:root:2024-04-10 13:53:58, Train, Epoch : 5, Step : 2860, Loss : 0.41810, Acc : 0.838, Sensitive_Loss : 0.09825, Sensitive_Acc : 17.400, Run Time : 9.33 sec
INFO:root:2024-04-10 13:54:09, Train, Epoch : 5, Step : 2870, Loss : 0.37692, Acc : 0.834, Sensitive_Loss : 0.12427, Sensitive_Acc : 16.600, Run Time : 11.16 sec
INFO:root:2024-04-10 13:54:19, Train, Epoch : 5, Step : 2880, Loss : 0.37935, Acc : 0.800, Sensitive_Loss : 0.12793, Sensitive_Acc : 17.400, Run Time : 10.24 sec
INFO:root:2024-04-10 13:54:28, Train, Epoch : 5, Step : 2890, Loss : 0.42977, Acc : 0.800, Sensitive_Loss : 0.11746, Sensitive_Acc : 14.900, Run Time : 8.74 sec
INFO:root:2024-04-10 13:54:37, Train, Epoch : 5, Step : 2900, Loss : 0.37866, Acc : 0.847, Sensitive_Loss : 0.13423, Sensitive_Acc : 15.800, Run Time : 8.51 sec
INFO:root:2024-04-10 13:56:09, Dev, Step : 2900, Loss : 0.50701, Acc : 0.779, Auc : 0.859, Sensitive_Loss : 0.15923, Sensitive_Acc : 16.336, Sensitive_Auc : 0.991, Mean auc: 0.859, Run Time : 92.37 sec
INFO:root:2024-04-10 13:56:16, Train, Epoch : 5, Step : 2910, Loss : 0.42111, Acc : 0.806, Sensitive_Loss : 0.13510, Sensitive_Acc : 15.300, Run Time : 99.45 sec
INFO:root:2024-04-10 13:56:27, Train, Epoch : 5, Step : 2920, Loss : 0.39218, Acc : 0.819, Sensitive_Loss : 0.11999, Sensitive_Acc : 15.500, Run Time : 11.05 sec
INFO:root:2024-04-10 13:56:37, Train, Epoch : 5, Step : 2930, Loss : 0.37127, Acc : 0.834, Sensitive_Loss : 0.11691, Sensitive_Acc : 16.200, Run Time : 9.50 sec
INFO:root:2024-04-10 13:56:49, Train, Epoch : 5, Step : 2940, Loss : 0.36006, Acc : 0.853, Sensitive_Loss : 0.11318, Sensitive_Acc : 16.100, Run Time : 12.45 sec
INFO:root:2024-04-10 13:57:00, Train, Epoch : 5, Step : 2950, Loss : 0.39947, Acc : 0.803, Sensitive_Loss : 0.13009, Sensitive_Acc : 17.300, Run Time : 11.12 sec
INFO:root:2024-04-10 13:57:10, Train, Epoch : 5, Step : 2960, Loss : 0.37324, Acc : 0.834, Sensitive_Loss : 0.13149, Sensitive_Acc : 16.600, Run Time : 9.32 sec
INFO:root:2024-04-10 13:57:19, Train, Epoch : 5, Step : 2970, Loss : 0.41486, Acc : 0.806, Sensitive_Loss : 0.11514, Sensitive_Acc : 15.600, Run Time : 9.35 sec
INFO:root:2024-04-10 13:57:29, Train, Epoch : 5, Step : 2980, Loss : 0.39485, Acc : 0.806, Sensitive_Loss : 0.11702, Sensitive_Acc : 17.400, Run Time : 9.70 sec
INFO:root:2024-04-10 13:57:38, Train, Epoch : 5, Step : 2990, Loss : 0.34158, Acc : 0.841, Sensitive_Loss : 0.09770, Sensitive_Acc : 15.100, Run Time : 9.13 sec
INFO:root:2024-04-10 13:57:47, Train, Epoch : 5, Step : 3000, Loss : 0.35636, Acc : 0.850, Sensitive_Loss : 0.14845, Sensitive_Acc : 16.200, Run Time : 9.53 sec
INFO:root:2024-04-10 13:59:24, Dev, Step : 3000, Loss : 0.54542, Acc : 0.760, Auc : 0.856, Sensitive_Loss : 0.16806, Sensitive_Acc : 16.250, Sensitive_Auc : 0.991, Mean auc: 0.856, Run Time : 96.52 sec
INFO:root:2024-04-10 13:59:32, Train, Epoch : 5, Step : 3010, Loss : 0.36737, Acc : 0.828, Sensitive_Loss : 0.13121, Sensitive_Acc : 16.600, Run Time : 105.06 sec
INFO:root:2024-04-10 13:59:46, Train, Epoch : 5, Step : 3020, Loss : 0.36539, Acc : 0.825, Sensitive_Loss : 0.11975, Sensitive_Acc : 15.400, Run Time : 14.12 sec
INFO:root:2024-04-10 13:59:56, Train, Epoch : 5, Step : 3030, Loss : 0.41465, Acc : 0.816, Sensitive_Loss : 0.11078, Sensitive_Acc : 16.500, Run Time : 9.89 sec
INFO:root:2024-04-10 14:00:08, Train, Epoch : 5, Step : 3040, Loss : 0.34390, Acc : 0.825, Sensitive_Loss : 0.10573, Sensitive_Acc : 16.400, Run Time : 11.58 sec
INFO:root:2024-04-10 14:00:21, Train, Epoch : 5, Step : 3050, Loss : 0.40768, Acc : 0.825, Sensitive_Loss : 0.14564, Sensitive_Acc : 16.100, Run Time : 13.01 sec
INFO:root:2024-04-10 14:00:30, Train, Epoch : 5, Step : 3060, Loss : 0.42194, Acc : 0.816, Sensitive_Loss : 0.16282, Sensitive_Acc : 16.600, Run Time : 9.30 sec
INFO:root:2024-04-10 14:00:40, Train, Epoch : 5, Step : 3070, Loss : 0.44609, Acc : 0.825, Sensitive_Loss : 0.15771, Sensitive_Acc : 16.300, Run Time : 10.22 sec
INFO:root:2024-04-10 14:00:50, Train, Epoch : 5, Step : 3080, Loss : 0.35517, Acc : 0.838, Sensitive_Loss : 0.11030, Sensitive_Acc : 15.700, Run Time : 9.65 sec
INFO:root:2024-04-10 14:01:00, Train, Epoch : 5, Step : 3090, Loss : 0.36346, Acc : 0.838, Sensitive_Loss : 0.16221, Sensitive_Acc : 16.800, Run Time : 9.44 sec
INFO:root:2024-04-10 14:01:09, Train, Epoch : 5, Step : 3100, Loss : 0.33579, Acc : 0.856, Sensitive_Loss : 0.12851, Sensitive_Acc : 16.900, Run Time : 9.27 sec
INFO:root:2024-04-10 14:02:50, Dev, Step : 3100, Loss : 0.52972, Acc : 0.774, Auc : 0.860, Sensitive_Loss : 0.18433, Sensitive_Acc : 16.307, Sensitive_Auc : 0.993, Mean auc: 0.860, Run Time : 100.92 sec
INFO:root:2024-04-10 14:02:58, Train, Epoch : 5, Step : 3110, Loss : 0.44430, Acc : 0.800, Sensitive_Loss : 0.11663, Sensitive_Acc : 17.100, Run Time : 108.68 sec
INFO:root:2024-04-10 14:03:10, Train, Epoch : 5, Step : 3120, Loss : 0.36295, Acc : 0.822, Sensitive_Loss : 0.11482, Sensitive_Acc : 15.400, Run Time : 12.03 sec
INFO:root:2024-04-10 14:03:19, Train, Epoch : 5, Step : 3130, Loss : 0.45243, Acc : 0.797, Sensitive_Loss : 0.13571, Sensitive_Acc : 16.900, Run Time : 9.43 sec
INFO:root:2024-04-10 14:03:28, Train, Epoch : 5, Step : 3140, Loss : 0.42002, Acc : 0.816, Sensitive_Loss : 0.12420, Sensitive_Acc : 15.100, Run Time : 9.10 sec
INFO:root:2024-04-10 14:03:39, Train, Epoch : 5, Step : 3150, Loss : 0.36420, Acc : 0.853, Sensitive_Loss : 0.10347, Sensitive_Acc : 16.200, Run Time : 10.98 sec
INFO:root:2024-04-10 14:03:49, Train, Epoch : 5, Step : 3160, Loss : 0.50786, Acc : 0.794, Sensitive_Loss : 0.09690, Sensitive_Acc : 16.600, Run Time : 9.78 sec
INFO:root:2024-04-10 14:03:58, Train, Epoch : 5, Step : 3170, Loss : 0.32397, Acc : 0.866, Sensitive_Loss : 0.13844, Sensitive_Acc : 16.000, Run Time : 9.25 sec
INFO:root:2024-04-10 14:04:09, Train, Epoch : 5, Step : 3180, Loss : 0.39372, Acc : 0.856, Sensitive_Loss : 0.12488, Sensitive_Acc : 16.900, Run Time : 10.52 sec
INFO:root:2024-04-10 14:04:19, Train, Epoch : 5, Step : 3190, Loss : 0.35783, Acc : 0.819, Sensitive_Loss : 0.09900, Sensitive_Acc : 15.400, Run Time : 10.71 sec
INFO:root:2024-04-10 14:04:28, Train, Epoch : 5, Step : 3200, Loss : 0.34703, Acc : 0.853, Sensitive_Loss : 0.14728, Sensitive_Acc : 15.900, Run Time : 8.99 sec
INFO:root:2024-04-10 14:06:07, Dev, Step : 3200, Loss : 0.50476, Acc : 0.779, Auc : 0.861, Sensitive_Loss : 0.15460, Sensitive_Acc : 16.364, Sensitive_Auc : 0.990, Mean auc: 0.861, Run Time : 98.68 sec
INFO:root:2024-04-10 14:06:14, Train, Epoch : 5, Step : 3210, Loss : 0.38320, Acc : 0.809, Sensitive_Loss : 0.09970, Sensitive_Acc : 16.800, Run Time : 105.71 sec
INFO:root:2024-04-10 14:06:25, Train, Epoch : 5, Step : 3220, Loss : 0.34974, Acc : 0.828, Sensitive_Loss : 0.13668, Sensitive_Acc : 15.700, Run Time : 11.30 sec
INFO:root:2024-04-10 14:07:56
INFO:root:y_pred: [0.12183134 0.02288426 0.87911254 ... 0.5620288  0.40469334 0.32600784]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.99097347e-01 2.55758548e-03 8.36575806e-01 9.99993443e-01
 9.99618292e-01 9.79201555e-01 9.99837756e-01 6.10998468e-05
 9.09498930e-01 9.98265445e-01 4.65210974e-01 1.25778720e-01
 1.22624173e-04 9.61566627e-01 9.99882936e-01 9.99989986e-01
 9.98485386e-01 9.93872285e-01 9.99274790e-01 9.96529996e-01
 9.97112155e-01 4.92311627e-01 9.98427868e-01 8.27103198e-01
 8.49524796e-01 1.08703285e-01 9.98887241e-01 6.77904580e-03
 9.99738157e-01 6.06846623e-02 5.37174456e-02 4.24270004e-01
 2.91374745e-03 9.98142481e-01 3.56774149e-06 9.91094172e-01
 2.08722631e-04 9.99939442e-01 8.04716721e-03 9.48996186e-01
 9.92740571e-01 2.27697403e-03 6.83690980e-03 1.13558467e-03
 4.80360478e-01 6.86962605e-01 9.99279559e-01 9.70030248e-01
 9.48834598e-01 9.96877193e-01 2.23745685e-03 9.39293861e-01
 7.97474291e-03 1.09328456e-01 9.99972224e-01 5.91216683e-02
 9.93399501e-01 9.99713004e-01 9.82945144e-01 9.81173012e-03
 3.96325253e-03 9.99572814e-01 1.69845819e-01 9.99521613e-01
 9.89544451e-01 5.10297179e-01 9.64798808e-01 8.81130457e-01
 9.99891162e-01 9.99800265e-01 7.59326965e-02 5.49382329e-01
 9.94276226e-01 9.99913573e-01 9.88804042e-01 2.27305274e-02
 6.12248600e-01 1.21628223e-02 1.01592566e-03 9.96248543e-01
 2.53167171e-02 9.99502897e-01 9.99906659e-01 9.97236848e-01
 2.45038066e-02 9.99999642e-01 4.88310168e-03 1.91271007e-02
 9.99356329e-01 9.98295724e-01 4.40831073e-02 8.06554675e-01
 6.78354502e-03 9.78898048e-01 8.30169395e-02 9.99942303e-01
 8.12282600e-03 9.98645842e-01 7.80616701e-01 2.14601196e-02
 1.12677728e-04 3.17802757e-01 9.94959235e-01 9.99562800e-01
 9.80658054e-01 9.81808603e-01 9.73737895e-01 3.36806849e-03
 1.01499334e-01 9.99145508e-01 1.36776565e-04 1.76292378e-04
 7.66820833e-02 9.99938369e-01 9.95268762e-01 3.16289277e-03
 9.28354681e-01 4.58251976e-04 9.98763919e-01 9.69164133e-01
 9.99994040e-01 9.99994278e-01 1.87011421e-01 1.64319053e-01
 1.27519011e-01 1.69815514e-02 3.36818560e-03 4.84746008e-04
 9.98811245e-01 9.99984026e-01 5.24226204e-03 2.68922164e-03
 1.07273331e-03 8.88010025e-01 9.99660134e-01 9.97070909e-01
 9.99273121e-01 8.49808991e-01 4.49335538e-02 9.67568398e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 14:07:56, Dev, Step : 3220, Loss : 0.52189, Acc : 0.774, Auc : 0.861, Sensitive_Loss : 0.15158, Sensitive_Acc : 16.321, Sensitive_Auc : 0.988, Mean auc: 0.861, Run Time : 90.40 sec
INFO:root:2024-04-10 14:08:12, Train, Epoch : 6, Step : 3230, Loss : 0.40007, Acc : 0.809, Sensitive_Loss : 0.15922, Sensitive_Acc : 15.000, Run Time : 15.54 sec
INFO:root:2024-04-10 14:08:25, Train, Epoch : 6, Step : 3240, Loss : 0.33696, Acc : 0.831, Sensitive_Loss : 0.15463, Sensitive_Acc : 17.100, Run Time : 12.72 sec
INFO:root:2024-04-10 14:08:35, Train, Epoch : 6, Step : 3250, Loss : 0.31030, Acc : 0.856, Sensitive_Loss : 0.09455, Sensitive_Acc : 16.100, Run Time : 9.76 sec
INFO:root:2024-04-10 14:08:47, Train, Epoch : 6, Step : 3260, Loss : 0.30792, Acc : 0.869, Sensitive_Loss : 0.14089, Sensitive_Acc : 17.000, Run Time : 11.64 sec
INFO:root:2024-04-10 14:09:01, Train, Epoch : 6, Step : 3270, Loss : 0.32236, Acc : 0.844, Sensitive_Loss : 0.10099, Sensitive_Acc : 15.600, Run Time : 14.36 sec
INFO:root:2024-04-10 14:09:11, Train, Epoch : 6, Step : 3280, Loss : 0.33478, Acc : 0.853, Sensitive_Loss : 0.14441, Sensitive_Acc : 15.600, Run Time : 9.80 sec
INFO:root:2024-04-10 14:09:23, Train, Epoch : 6, Step : 3290, Loss : 0.41597, Acc : 0.825, Sensitive_Loss : 0.08694, Sensitive_Acc : 17.200, Run Time : 12.24 sec
INFO:root:2024-04-10 14:09:35, Train, Epoch : 6, Step : 3300, Loss : 0.40341, Acc : 0.838, Sensitive_Loss : 0.10886, Sensitive_Acc : 17.200, Run Time : 12.26 sec
INFO:root:2024-04-10 14:11:30, Dev, Step : 3300, Loss : 0.52790, Acc : 0.773, Auc : 0.859, Sensitive_Loss : 0.16677, Sensitive_Acc : 16.264, Sensitive_Auc : 0.990, Mean auc: 0.859, Run Time : 114.53 sec
INFO:root:2024-04-10 14:11:36, Train, Epoch : 6, Step : 3310, Loss : 0.38347, Acc : 0.825, Sensitive_Loss : 0.13187, Sensitive_Acc : 15.600, Run Time : 121.28 sec
INFO:root:2024-04-10 14:11:47, Train, Epoch : 6, Step : 3320, Loss : 0.37325, Acc : 0.831, Sensitive_Loss : 0.11924, Sensitive_Acc : 17.200, Run Time : 10.06 sec
INFO:root:2024-04-10 14:11:56, Train, Epoch : 6, Step : 3330, Loss : 0.39611, Acc : 0.838, Sensitive_Loss : 0.09934, Sensitive_Acc : 17.500, Run Time : 9.07 sec
INFO:root:2024-04-10 14:12:04, Train, Epoch : 6, Step : 3340, Loss : 0.40032, Acc : 0.841, Sensitive_Loss : 0.11653, Sensitive_Acc : 16.100, Run Time : 8.77 sec
INFO:root:2024-04-10 14:12:15, Train, Epoch : 6, Step : 3350, Loss : 0.33127, Acc : 0.847, Sensitive_Loss : 0.14139, Sensitive_Acc : 16.200, Run Time : 10.68 sec
INFO:root:2024-04-10 14:12:27, Train, Epoch : 6, Step : 3360, Loss : 0.28733, Acc : 0.872, Sensitive_Loss : 0.15119, Sensitive_Acc : 14.200, Run Time : 11.46 sec
INFO:root:2024-04-10 14:12:35, Train, Epoch : 6, Step : 3370, Loss : 0.38924, Acc : 0.834, Sensitive_Loss : 0.15092, Sensitive_Acc : 15.800, Run Time : 8.95 sec
INFO:root:2024-04-10 14:12:44, Train, Epoch : 6, Step : 3380, Loss : 0.37156, Acc : 0.847, Sensitive_Loss : 0.13200, Sensitive_Acc : 16.500, Run Time : 9.01 sec
INFO:root:2024-04-10 14:12:56, Train, Epoch : 6, Step : 3390, Loss : 0.41348, Acc : 0.809, Sensitive_Loss : 0.09317, Sensitive_Acc : 17.200, Run Time : 11.36 sec
INFO:root:2024-04-10 14:13:05, Train, Epoch : 6, Step : 3400, Loss : 0.32948, Acc : 0.819, Sensitive_Loss : 0.17275, Sensitive_Acc : 16.000, Run Time : 8.80 sec
INFO:root:2024-04-10 14:14:37, Dev, Step : 3400, Loss : 0.54240, Acc : 0.770, Auc : 0.860, Sensitive_Loss : 0.15917, Sensitive_Acc : 16.264, Sensitive_Auc : 0.990, Mean auc: 0.860, Run Time : 92.69 sec
INFO:root:2024-04-10 14:14:44, Train, Epoch : 6, Step : 3410, Loss : 0.28330, Acc : 0.863, Sensitive_Loss : 0.12087, Sensitive_Acc : 18.100, Run Time : 99.01 sec
INFO:root:2024-04-10 14:14:53, Train, Epoch : 6, Step : 3420, Loss : 0.33586, Acc : 0.834, Sensitive_Loss : 0.11516, Sensitive_Acc : 15.900, Run Time : 9.01 sec
INFO:root:2024-04-10 14:15:03, Train, Epoch : 6, Step : 3430, Loss : 0.34372, Acc : 0.834, Sensitive_Loss : 0.15476, Sensitive_Acc : 16.400, Run Time : 10.13 sec
INFO:root:2024-04-10 14:15:14, Train, Epoch : 6, Step : 3440, Loss : 0.35248, Acc : 0.869, Sensitive_Loss : 0.13042, Sensitive_Acc : 17.100, Run Time : 11.48 sec
INFO:root:2024-04-10 14:15:25, Train, Epoch : 6, Step : 3450, Loss : 0.38055, Acc : 0.841, Sensitive_Loss : 0.11162, Sensitive_Acc : 17.800, Run Time : 10.95 sec
INFO:root:2024-04-10 14:15:34, Train, Epoch : 6, Step : 3460, Loss : 0.40121, Acc : 0.834, Sensitive_Loss : 0.14212, Sensitive_Acc : 17.900, Run Time : 8.75 sec
INFO:root:2024-04-10 14:15:44, Train, Epoch : 6, Step : 3470, Loss : 0.32663, Acc : 0.859, Sensitive_Loss : 0.10645, Sensitive_Acc : 16.900, Run Time : 10.44 sec
INFO:root:2024-04-10 14:15:56, Train, Epoch : 6, Step : 3480, Loss : 0.41054, Acc : 0.847, Sensitive_Loss : 0.17873, Sensitive_Acc : 18.100, Run Time : 11.84 sec
INFO:root:2024-04-10 14:16:05, Train, Epoch : 6, Step : 3490, Loss : 0.37595, Acc : 0.850, Sensitive_Loss : 0.12432, Sensitive_Acc : 15.100, Run Time : 8.80 sec
INFO:root:2024-04-10 14:16:14, Train, Epoch : 6, Step : 3500, Loss : 0.39203, Acc : 0.856, Sensitive_Loss : 0.09693, Sensitive_Acc : 17.500, Run Time : 8.60 sec
INFO:root:2024-04-10 14:17:45, Dev, Step : 3500, Loss : 0.49729, Acc : 0.785, Auc : 0.861, Sensitive_Loss : 0.16323, Sensitive_Acc : 16.364, Sensitive_Auc : 0.990, Mean auc: 0.861, Run Time : 91.33 sec
INFO:root:2024-04-10 14:17:52, Train, Epoch : 6, Step : 3510, Loss : 0.33546, Acc : 0.847, Sensitive_Loss : 0.11058, Sensitive_Acc : 15.400, Run Time : 98.03 sec
INFO:root:2024-04-10 14:18:01, Train, Epoch : 6, Step : 3520, Loss : 0.36188, Acc : 0.831, Sensitive_Loss : 0.12179, Sensitive_Acc : 17.600, Run Time : 9.49 sec
INFO:root:2024-04-10 14:18:11, Train, Epoch : 6, Step : 3530, Loss : 0.38762, Acc : 0.828, Sensitive_Loss : 0.09773, Sensitive_Acc : 15.600, Run Time : 9.49 sec
INFO:root:2024-04-10 14:18:20, Train, Epoch : 6, Step : 3540, Loss : 0.27524, Acc : 0.825, Sensitive_Loss : 0.10841, Sensitive_Acc : 16.400, Run Time : 9.32 sec
INFO:root:2024-04-10 14:18:29, Train, Epoch : 6, Step : 3550, Loss : 0.37096, Acc : 0.822, Sensitive_Loss : 0.14413, Sensitive_Acc : 16.300, Run Time : 9.38 sec
INFO:root:2024-04-10 14:18:41, Train, Epoch : 6, Step : 3560, Loss : 0.40179, Acc : 0.809, Sensitive_Loss : 0.09897, Sensitive_Acc : 16.700, Run Time : 11.81 sec
INFO:root:2024-04-10 14:18:52, Train, Epoch : 6, Step : 3570, Loss : 0.36395, Acc : 0.844, Sensitive_Loss : 0.17950, Sensitive_Acc : 14.800, Run Time : 11.13 sec
INFO:root:2024-04-10 14:19:02, Train, Epoch : 6, Step : 3580, Loss : 0.36348, Acc : 0.822, Sensitive_Loss : 0.12758, Sensitive_Acc : 16.400, Run Time : 9.22 sec
INFO:root:2024-04-10 14:19:12, Train, Epoch : 6, Step : 3590, Loss : 0.34999, Acc : 0.809, Sensitive_Loss : 0.12705, Sensitive_Acc : 17.600, Run Time : 10.54 sec
INFO:root:2024-04-10 14:19:22, Train, Epoch : 6, Step : 3600, Loss : 0.43680, Acc : 0.834, Sensitive_Loss : 0.14692, Sensitive_Acc : 17.600, Run Time : 10.21 sec
INFO:root:2024-04-10 14:21:00, Dev, Step : 3600, Loss : 0.51277, Acc : 0.776, Auc : 0.856, Sensitive_Loss : 0.16444, Sensitive_Acc : 16.364, Sensitive_Auc : 0.990, Mean auc: 0.856, Run Time : 98.08 sec
INFO:root:2024-04-10 14:21:07, Train, Epoch : 6, Step : 3610, Loss : 0.35658, Acc : 0.878, Sensitive_Loss : 0.12665, Sensitive_Acc : 15.900, Run Time : 104.30 sec
INFO:root:2024-04-10 14:21:18, Train, Epoch : 6, Step : 3620, Loss : 0.42324, Acc : 0.816, Sensitive_Loss : 0.15399, Sensitive_Acc : 16.300, Run Time : 11.09 sec
INFO:root:2024-04-10 14:21:28, Train, Epoch : 6, Step : 3630, Loss : 0.37203, Acc : 0.844, Sensitive_Loss : 0.16031, Sensitive_Acc : 15.300, Run Time : 10.23 sec
INFO:root:2024-04-10 14:21:37, Train, Epoch : 6, Step : 3640, Loss : 0.29714, Acc : 0.863, Sensitive_Loss : 0.11095, Sensitive_Acc : 15.800, Run Time : 8.97 sec
INFO:root:2024-04-10 14:21:46, Train, Epoch : 6, Step : 3650, Loss : 0.32768, Acc : 0.866, Sensitive_Loss : 0.12076, Sensitive_Acc : 17.000, Run Time : 9.41 sec
INFO:root:2024-04-10 14:21:58, Train, Epoch : 6, Step : 3660, Loss : 0.40882, Acc : 0.816, Sensitive_Loss : 0.08703, Sensitive_Acc : 16.300, Run Time : 12.15 sec
INFO:root:2024-04-10 14:22:08, Train, Epoch : 6, Step : 3670, Loss : 0.38639, Acc : 0.822, Sensitive_Loss : 0.12904, Sensitive_Acc : 16.000, Run Time : 9.30 sec
INFO:root:2024-04-10 14:22:17, Train, Epoch : 6, Step : 3680, Loss : 0.36677, Acc : 0.809, Sensitive_Loss : 0.16126, Sensitive_Acc : 15.500, Run Time : 9.28 sec
INFO:root:2024-04-10 14:22:30, Train, Epoch : 6, Step : 3690, Loss : 0.33200, Acc : 0.819, Sensitive_Loss : 0.09699, Sensitive_Acc : 15.500, Run Time : 13.08 sec
INFO:root:2024-04-10 14:22:41, Train, Epoch : 6, Step : 3700, Loss : 0.31402, Acc : 0.869, Sensitive_Loss : 0.10753, Sensitive_Acc : 16.500, Run Time : 10.46 sec
INFO:root:2024-04-10 14:24:25, Dev, Step : 3700, Loss : 0.54487, Acc : 0.771, Auc : 0.857, Sensitive_Loss : 0.16032, Sensitive_Acc : 16.307, Sensitive_Auc : 0.988, Mean auc: 0.857, Run Time : 104.69 sec
INFO:root:2024-04-10 14:24:32, Train, Epoch : 6, Step : 3710, Loss : 0.34763, Acc : 0.856, Sensitive_Loss : 0.10841, Sensitive_Acc : 16.800, Run Time : 111.18 sec
INFO:root:2024-04-10 14:24:41, Train, Epoch : 6, Step : 3720, Loss : 0.36998, Acc : 0.847, Sensitive_Loss : 0.11606, Sensitive_Acc : 13.900, Run Time : 9.68 sec
INFO:root:2024-04-10 14:24:50, Train, Epoch : 6, Step : 3730, Loss : 0.38239, Acc : 0.809, Sensitive_Loss : 0.11111, Sensitive_Acc : 16.400, Run Time : 8.78 sec
INFO:root:2024-04-10 14:25:01, Train, Epoch : 6, Step : 3740, Loss : 0.36766, Acc : 0.831, Sensitive_Loss : 0.11304, Sensitive_Acc : 18.600, Run Time : 10.57 sec
INFO:root:2024-04-10 14:25:11, Train, Epoch : 6, Step : 3750, Loss : 0.33649, Acc : 0.844, Sensitive_Loss : 0.12842, Sensitive_Acc : 18.900, Run Time : 10.06 sec
INFO:root:2024-04-10 14:25:20, Train, Epoch : 6, Step : 3760, Loss : 0.34960, Acc : 0.847, Sensitive_Loss : 0.11264, Sensitive_Acc : 16.500, Run Time : 9.41 sec
INFO:root:2024-04-10 14:25:31, Train, Epoch : 6, Step : 3770, Loss : 0.33539, Acc : 0.859, Sensitive_Loss : 0.11667, Sensitive_Acc : 15.800, Run Time : 11.15 sec
INFO:root:2024-04-10 14:25:43, Train, Epoch : 6, Step : 3780, Loss : 0.37898, Acc : 0.822, Sensitive_Loss : 0.09383, Sensitive_Acc : 15.700, Run Time : 11.52 sec
INFO:root:2024-04-10 14:25:52, Train, Epoch : 6, Step : 3790, Loss : 0.31640, Acc : 0.872, Sensitive_Loss : 0.08913, Sensitive_Acc : 16.600, Run Time : 9.39 sec
INFO:root:2024-04-10 14:26:01, Train, Epoch : 6, Step : 3800, Loss : 0.33895, Acc : 0.869, Sensitive_Loss : 0.12291, Sensitive_Acc : 14.700, Run Time : 8.49 sec
INFO:root:2024-04-10 14:27:40, Dev, Step : 3800, Loss : 0.50817, Acc : 0.783, Auc : 0.859, Sensitive_Loss : 0.14952, Sensitive_Acc : 16.364, Sensitive_Auc : 0.990, Mean auc: 0.859, Run Time : 98.81 sec
INFO:root:2024-04-10 14:27:47, Train, Epoch : 6, Step : 3810, Loss : 0.35938, Acc : 0.825, Sensitive_Loss : 0.11931, Sensitive_Acc : 16.000, Run Time : 105.70 sec
INFO:root:2024-04-10 14:27:59, Train, Epoch : 6, Step : 3820, Loss : 0.40962, Acc : 0.825, Sensitive_Loss : 0.08790, Sensitive_Acc : 17.300, Run Time : 12.07 sec
INFO:root:2024-04-10 14:28:09, Train, Epoch : 6, Step : 3830, Loss : 0.36119, Acc : 0.863, Sensitive_Loss : 0.13378, Sensitive_Acc : 17.900, Run Time : 10.58 sec
INFO:root:2024-04-10 14:28:19, Train, Epoch : 6, Step : 3840, Loss : 0.38849, Acc : 0.831, Sensitive_Loss : 0.15372, Sensitive_Acc : 16.800, Run Time : 9.35 sec
INFO:root:2024-04-10 14:28:31, Train, Epoch : 6, Step : 3850, Loss : 0.38592, Acc : 0.856, Sensitive_Loss : 0.14460, Sensitive_Acc : 15.900, Run Time : 12.57 sec
INFO:root:2024-04-10 14:28:40, Train, Epoch : 6, Step : 3860, Loss : 0.41976, Acc : 0.822, Sensitive_Loss : 0.09591, Sensitive_Acc : 17.300, Run Time : 9.27 sec
INFO:root:2024-04-10 14:30:24
INFO:root:y_pred: [0.128301   0.02343873 0.9134031  ... 0.58280975 0.4166742  0.28640276]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.99065101e-01 4.05413006e-03 8.12132955e-01 9.99993801e-01
 9.99407768e-01 9.85240221e-01 9.99681115e-01 9.21657265e-05
 8.49388540e-01 9.99208033e-01 5.24409115e-01 1.20838799e-01
 1.82443982e-04 9.57056642e-01 9.99978423e-01 9.99967456e-01
 9.97278631e-01 9.95931685e-01 9.98949230e-01 9.94713128e-01
 9.95142937e-01 6.19373024e-01 9.98068511e-01 7.43295968e-01
 8.93416643e-01 1.23991705e-01 9.97943103e-01 1.33790066e-02
 9.99499798e-01 4.30008471e-02 7.26742893e-02 3.10442775e-01
 8.42570420e-03 9.99174416e-01 4.94795768e-06 9.88592386e-01
 1.67905353e-04 9.99940515e-01 1.09244967e-02 9.77378368e-01
 9.96009231e-01 4.35481267e-03 6.35867286e-03 1.03461801e-03
 6.19296730e-01 7.38569140e-01 9.99213576e-01 9.63813663e-01
 9.72490430e-01 9.94441807e-01 2.52305320e-03 9.72771347e-01
 1.47388233e-02 1.80288404e-01 9.99841213e-01 3.43115665e-02
 9.92538989e-01 9.98141766e-01 9.82401788e-01 1.75838750e-02
 8.66138935e-03 9.99705851e-01 1.52210429e-01 9.99165297e-01
 9.87338662e-01 4.23816830e-01 9.79373693e-01 9.03146207e-01
 9.99879003e-01 9.99418974e-01 1.40723482e-01 7.02004671e-01
 9.93338645e-01 9.99691129e-01 9.89723444e-01 2.18576025e-02
 5.21744370e-01 8.28626379e-03 1.18552032e-03 9.92764294e-01
 2.04336978e-02 9.99705255e-01 9.99841213e-01 9.97170269e-01
 1.30044883e-02 9.99998331e-01 3.52709531e-03 2.32254826e-02
 9.99659181e-01 9.94386196e-01 3.74426879e-02 9.17239785e-01
 9.94488038e-03 9.55707073e-01 4.65502888e-02 9.99906778e-01
 1.70397889e-02 9.97462630e-01 8.63329828e-01 2.26159617e-02
 1.46267732e-04 3.55580449e-01 9.96941149e-01 9.99825895e-01
 9.73270059e-01 9.67715979e-01 9.81410027e-01 1.88323529e-03
 9.84890088e-02 9.99234080e-01 9.57705051e-05 3.35775141e-04
 2.29615167e-01 9.99877691e-01 9.97275174e-01 5.15290862e-03
 8.98260415e-01 6.65582716e-04 9.97232378e-01 9.71026421e-01
 9.99988437e-01 9.99997854e-01 2.55484015e-01 1.03200309e-01
 2.93131024e-01 2.29255725e-02 6.05235947e-03 9.28411377e-04
 9.99239087e-01 9.99983430e-01 8.25091172e-03 3.34041053e-03
 4.23783931e-04 8.44542742e-01 9.99037623e-01 9.96431589e-01
 9.98506963e-01 8.36492717e-01 8.76982585e-02 9.69565928e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 14:30:24, Dev, Step : 3864, Loss : 0.52312, Acc : 0.779, Auc : 0.861, Sensitive_Loss : 0.15310, Sensitive_Acc : 16.350, Sensitive_Auc : 0.991, Mean auc: 0.861, Run Time : 100.49 sec
INFO:root:2024-04-10 14:30:31, Train, Epoch : 7, Step : 3870, Loss : 0.20625, Acc : 0.494, Sensitive_Loss : 0.06566, Sensitive_Acc : 11.100, Run Time : 5.75 sec
INFO:root:2024-04-10 14:30:43, Train, Epoch : 7, Step : 3880, Loss : 0.39148, Acc : 0.841, Sensitive_Loss : 0.10817, Sensitive_Acc : 14.200, Run Time : 11.36 sec
INFO:root:2024-04-10 14:30:52, Train, Epoch : 7, Step : 3890, Loss : 0.37440, Acc : 0.844, Sensitive_Loss : 0.10513, Sensitive_Acc : 15.600, Run Time : 9.58 sec
INFO:root:2024-04-10 14:31:01, Train, Epoch : 7, Step : 3900, Loss : 0.34793, Acc : 0.863, Sensitive_Loss : 0.12215, Sensitive_Acc : 15.300, Run Time : 8.97 sec
INFO:root:2024-04-10 14:32:40, Dev, Step : 3900, Loss : 0.52130, Acc : 0.777, Auc : 0.860, Sensitive_Loss : 0.15597, Sensitive_Acc : 16.307, Sensitive_Auc : 0.991, Mean auc: 0.860, Run Time : 98.65 sec
INFO:root:2024-04-10 14:32:46, Train, Epoch : 7, Step : 3910, Loss : 0.32645, Acc : 0.866, Sensitive_Loss : 0.12690, Sensitive_Acc : 17.400, Run Time : 104.90 sec
INFO:root:2024-04-10 14:32:58, Train, Epoch : 7, Step : 3920, Loss : 0.35754, Acc : 0.806, Sensitive_Loss : 0.10208, Sensitive_Acc : 15.500, Run Time : 12.09 sec
INFO:root:2024-04-10 14:33:07, Train, Epoch : 7, Step : 3930, Loss : 0.37360, Acc : 0.844, Sensitive_Loss : 0.12779, Sensitive_Acc : 16.600, Run Time : 9.18 sec
INFO:root:2024-04-10 14:33:17, Train, Epoch : 7, Step : 3940, Loss : 0.34877, Acc : 0.841, Sensitive_Loss : 0.13044, Sensitive_Acc : 16.800, Run Time : 9.20 sec
INFO:root:2024-04-10 14:33:27, Train, Epoch : 7, Step : 3950, Loss : 0.40714, Acc : 0.803, Sensitive_Loss : 0.13518, Sensitive_Acc : 16.400, Run Time : 10.78 sec
INFO:root:2024-04-10 14:33:38, Train, Epoch : 7, Step : 3960, Loss : 0.31260, Acc : 0.884, Sensitive_Loss : 0.11360, Sensitive_Acc : 15.200, Run Time : 10.53 sec
INFO:root:2024-04-10 14:33:47, Train, Epoch : 7, Step : 3970, Loss : 0.37010, Acc : 0.819, Sensitive_Loss : 0.12864, Sensitive_Acc : 16.600, Run Time : 9.24 sec
INFO:root:2024-04-10 14:33:58, Train, Epoch : 7, Step : 3980, Loss : 0.28694, Acc : 0.872, Sensitive_Loss : 0.12581, Sensitive_Acc : 16.600, Run Time : 10.59 sec
INFO:root:2024-04-10 14:34:10, Train, Epoch : 7, Step : 3990, Loss : 0.34763, Acc : 0.850, Sensitive_Loss : 0.07830, Sensitive_Acc : 16.900, Run Time : 12.37 sec
INFO:root:2024-04-10 14:34:20, Train, Epoch : 7, Step : 4000, Loss : 0.27767, Acc : 0.872, Sensitive_Loss : 0.08304, Sensitive_Acc : 15.200, Run Time : 9.53 sec
INFO:root:2024-04-10 14:35:57, Dev, Step : 4000, Loss : 0.54616, Acc : 0.767, Auc : 0.859, Sensitive_Loss : 0.16069, Sensitive_Acc : 16.350, Sensitive_Auc : 0.991, Mean auc: 0.859, Run Time : 97.67 sec
INFO:root:2024-04-10 14:36:04, Train, Epoch : 7, Step : 4010, Loss : 0.33326, Acc : 0.850, Sensitive_Loss : 0.10977, Sensitive_Acc : 17.500, Run Time : 104.42 sec
INFO:root:2024-04-10 14:36:14, Train, Epoch : 7, Step : 4020, Loss : 0.35138, Acc : 0.844, Sensitive_Loss : 0.13221, Sensitive_Acc : 16.500, Run Time : 10.32 sec
INFO:root:2024-04-10 14:36:26, Train, Epoch : 7, Step : 4030, Loss : 0.39247, Acc : 0.819, Sensitive_Loss : 0.10122, Sensitive_Acc : 15.000, Run Time : 12.07 sec
INFO:root:2024-04-10 14:36:36, Train, Epoch : 7, Step : 4040, Loss : 0.37619, Acc : 0.831, Sensitive_Loss : 0.11701, Sensitive_Acc : 14.500, Run Time : 9.92 sec
INFO:root:2024-04-10 14:36:46, Train, Epoch : 7, Step : 4050, Loss : 0.31979, Acc : 0.850, Sensitive_Loss : 0.11983, Sensitive_Acc : 16.600, Run Time : 9.45 sec
INFO:root:2024-04-10 14:36:55, Train, Epoch : 7, Step : 4060, Loss : 0.39909, Acc : 0.822, Sensitive_Loss : 0.10837, Sensitive_Acc : 17.100, Run Time : 9.23 sec
INFO:root:2024-04-10 14:37:04, Train, Epoch : 7, Step : 4070, Loss : 0.32807, Acc : 0.875, Sensitive_Loss : 0.11303, Sensitive_Acc : 15.700, Run Time : 9.11 sec
INFO:root:2024-04-10 14:37:14, Train, Epoch : 7, Step : 4080, Loss : 0.31948, Acc : 0.869, Sensitive_Loss : 0.08780, Sensitive_Acc : 15.800, Run Time : 9.97 sec
INFO:root:2024-04-10 14:37:24, Train, Epoch : 7, Step : 4090, Loss : 0.40885, Acc : 0.819, Sensitive_Loss : 0.11469, Sensitive_Acc : 18.500, Run Time : 10.13 sec
INFO:root:2024-04-10 14:37:36, Train, Epoch : 7, Step : 4100, Loss : 0.37125, Acc : 0.856, Sensitive_Loss : 0.13597, Sensitive_Acc : 15.400, Run Time : 11.28 sec
INFO:root:2024-04-10 14:39:13, Dev, Step : 4100, Loss : 0.53838, Acc : 0.769, Auc : 0.854, Sensitive_Loss : 0.14562, Sensitive_Acc : 16.350, Sensitive_Auc : 0.992, Mean auc: 0.854, Run Time : 97.31 sec
INFO:root:2024-04-10 14:39:20, Train, Epoch : 7, Step : 4110, Loss : 0.37115, Acc : 0.825, Sensitive_Loss : 0.11062, Sensitive_Acc : 14.800, Run Time : 104.17 sec
INFO:root:2024-04-10 14:39:32, Train, Epoch : 7, Step : 4120, Loss : 0.32671, Acc : 0.866, Sensitive_Loss : 0.16590, Sensitive_Acc : 15.300, Run Time : 12.48 sec
INFO:root:2024-04-10 14:39:42, Train, Epoch : 7, Step : 4130, Loss : 0.38159, Acc : 0.834, Sensitive_Loss : 0.14666, Sensitive_Acc : 16.800, Run Time : 9.91 sec
INFO:root:2024-04-10 14:39:52, Train, Epoch : 7, Step : 4140, Loss : 0.31937, Acc : 0.878, Sensitive_Loss : 0.10882, Sensitive_Acc : 17.000, Run Time : 10.03 sec
INFO:root:2024-04-10 14:40:08, Train, Epoch : 7, Step : 4150, Loss : 0.35972, Acc : 0.828, Sensitive_Loss : 0.15064, Sensitive_Acc : 16.300, Run Time : 16.31 sec
INFO:root:2024-04-10 14:40:18, Train, Epoch : 7, Step : 4160, Loss : 0.35898, Acc : 0.838, Sensitive_Loss : 0.16623, Sensitive_Acc : 16.700, Run Time : 9.98 sec
INFO:root:2024-04-10 14:40:28, Train, Epoch : 7, Step : 4170, Loss : 0.37867, Acc : 0.812, Sensitive_Loss : 0.13614, Sensitive_Acc : 17.100, Run Time : 9.98 sec
INFO:root:2024-04-10 14:40:42, Train, Epoch : 7, Step : 4180, Loss : 0.39229, Acc : 0.828, Sensitive_Loss : 0.07096, Sensitive_Acc : 15.800, Run Time : 13.64 sec
INFO:root:2024-04-10 14:40:51, Train, Epoch : 7, Step : 4190, Loss : 0.32023, Acc : 0.856, Sensitive_Loss : 0.14053, Sensitive_Acc : 15.100, Run Time : 9.45 sec
INFO:root:2024-04-10 14:41:01, Train, Epoch : 7, Step : 4200, Loss : 0.32512, Acc : 0.844, Sensitive_Loss : 0.11832, Sensitive_Acc : 16.100, Run Time : 9.68 sec
INFO:root:2024-04-10 14:42:38, Dev, Step : 4200, Loss : 0.51268, Acc : 0.783, Auc : 0.859, Sensitive_Loss : 0.14048, Sensitive_Acc : 16.321, Sensitive_Auc : 0.993, Mean auc: 0.859, Run Time : 97.28 sec
INFO:root:2024-04-10 14:42:46, Train, Epoch : 7, Step : 4210, Loss : 0.31041, Acc : 0.881, Sensitive_Loss : 0.10131, Sensitive_Acc : 15.100, Run Time : 105.02 sec
INFO:root:2024-04-10 14:42:58, Train, Epoch : 7, Step : 4220, Loss : 0.37089, Acc : 0.850, Sensitive_Loss : 0.15918, Sensitive_Acc : 16.900, Run Time : 11.71 sec
INFO:root:2024-04-10 14:43:10, Train, Epoch : 7, Step : 4230, Loss : 0.32839, Acc : 0.866, Sensitive_Loss : 0.10274, Sensitive_Acc : 15.800, Run Time : 12.18 sec
INFO:root:2024-04-10 14:43:21, Train, Epoch : 7, Step : 4240, Loss : 0.35967, Acc : 0.853, Sensitive_Loss : 0.13267, Sensitive_Acc : 17.200, Run Time : 10.88 sec
INFO:root:2024-04-10 14:43:34, Train, Epoch : 7, Step : 4250, Loss : 0.35723, Acc : 0.825, Sensitive_Loss : 0.15838, Sensitive_Acc : 15.900, Run Time : 12.60 sec
INFO:root:2024-04-10 14:43:45, Train, Epoch : 7, Step : 4260, Loss : 0.27919, Acc : 0.872, Sensitive_Loss : 0.10491, Sensitive_Acc : 16.500, Run Time : 11.34 sec
INFO:root:2024-04-10 14:43:55, Train, Epoch : 7, Step : 4270, Loss : 0.29132, Acc : 0.866, Sensitive_Loss : 0.11602, Sensitive_Acc : 15.200, Run Time : 10.01 sec
INFO:root:2024-04-10 14:44:07, Train, Epoch : 7, Step : 4280, Loss : 0.32044, Acc : 0.838, Sensitive_Loss : 0.11968, Sensitive_Acc : 16.800, Run Time : 12.03 sec
INFO:root:2024-04-10 14:44:18, Train, Epoch : 7, Step : 4290, Loss : 0.29790, Acc : 0.844, Sensitive_Loss : 0.12800, Sensitive_Acc : 16.900, Run Time : 10.75 sec
INFO:root:2024-04-10 14:44:27, Train, Epoch : 7, Step : 4300, Loss : 0.35779, Acc : 0.838, Sensitive_Loss : 0.10337, Sensitive_Acc : 15.700, Run Time : 9.69 sec
INFO:root:2024-04-10 14:46:02, Dev, Step : 4300, Loss : 0.56280, Acc : 0.769, Auc : 0.858, Sensitive_Loss : 0.15252, Sensitive_Acc : 16.221, Sensitive_Auc : 0.993, Mean auc: 0.858, Run Time : 94.56 sec
INFO:root:2024-04-10 14:46:08, Train, Epoch : 7, Step : 4310, Loss : 0.28620, Acc : 0.841, Sensitive_Loss : 0.11564, Sensitive_Acc : 16.700, Run Time : 100.73 sec
INFO:root:2024-04-10 14:46:18, Train, Epoch : 7, Step : 4320, Loss : 0.34083, Acc : 0.850, Sensitive_Loss : 0.09815, Sensitive_Acc : 16.200, Run Time : 9.44 sec
INFO:root:2024-04-10 14:46:29, Train, Epoch : 7, Step : 4330, Loss : 0.31085, Acc : 0.825, Sensitive_Loss : 0.13191, Sensitive_Acc : 16.600, Run Time : 11.02 sec
INFO:root:2024-04-10 14:46:38, Train, Epoch : 7, Step : 4340, Loss : 0.31445, Acc : 0.875, Sensitive_Loss : 0.08970, Sensitive_Acc : 15.500, Run Time : 9.07 sec
INFO:root:2024-04-10 14:46:47, Train, Epoch : 7, Step : 4350, Loss : 0.35759, Acc : 0.847, Sensitive_Loss : 0.12382, Sensitive_Acc : 16.100, Run Time : 9.01 sec
INFO:root:2024-04-10 14:46:59, Train, Epoch : 7, Step : 4360, Loss : 0.41370, Acc : 0.812, Sensitive_Loss : 0.10860, Sensitive_Acc : 15.300, Run Time : 12.06 sec
INFO:root:2024-04-10 14:47:08, Train, Epoch : 7, Step : 4370, Loss : 0.37370, Acc : 0.844, Sensitive_Loss : 0.15008, Sensitive_Acc : 17.500, Run Time : 9.81 sec
INFO:root:2024-04-10 14:47:17, Train, Epoch : 7, Step : 4380, Loss : 0.27920, Acc : 0.881, Sensitive_Loss : 0.09457, Sensitive_Acc : 15.200, Run Time : 8.71 sec
INFO:root:2024-04-10 14:47:28, Train, Epoch : 7, Step : 4390, Loss : 0.32176, Acc : 0.834, Sensitive_Loss : 0.11620, Sensitive_Acc : 16.500, Run Time : 10.65 sec
INFO:root:2024-04-10 14:47:38, Train, Epoch : 7, Step : 4400, Loss : 0.29111, Acc : 0.878, Sensitive_Loss : 0.11799, Sensitive_Acc : 17.100, Run Time : 9.99 sec
INFO:root:2024-04-10 14:49:14, Dev, Step : 4400, Loss : 0.53811, Acc : 0.773, Auc : 0.860, Sensitive_Loss : 0.16972, Sensitive_Acc : 16.293, Sensitive_Auc : 0.991, Mean auc: 0.860, Run Time : 96.25 sec
INFO:root:2024-04-10 14:49:22, Train, Epoch : 7, Step : 4410, Loss : 0.36911, Acc : 0.856, Sensitive_Loss : 0.10273, Sensitive_Acc : 17.200, Run Time : 103.83 sec
INFO:root:2024-04-10 14:49:31, Train, Epoch : 7, Step : 4420, Loss : 0.31057, Acc : 0.847, Sensitive_Loss : 0.10846, Sensitive_Acc : 16.500, Run Time : 8.96 sec
INFO:root:2024-04-10 14:49:40, Train, Epoch : 7, Step : 4430, Loss : 0.39598, Acc : 0.831, Sensitive_Loss : 0.09357, Sensitive_Acc : 16.000, Run Time : 9.65 sec
INFO:root:2024-04-10 14:49:52, Train, Epoch : 7, Step : 4440, Loss : 0.38343, Acc : 0.828, Sensitive_Loss : 0.13608, Sensitive_Acc : 14.900, Run Time : 11.51 sec
INFO:root:2024-04-10 14:50:02, Train, Epoch : 7, Step : 4450, Loss : 0.31212, Acc : 0.841, Sensitive_Loss : 0.12189, Sensitive_Acc : 16.700, Run Time : 10.29 sec
INFO:root:2024-04-10 14:50:11, Train, Epoch : 7, Step : 4460, Loss : 0.31617, Acc : 0.881, Sensitive_Loss : 0.09938, Sensitive_Acc : 16.100, Run Time : 8.55 sec
INFO:root:2024-04-10 14:50:21, Train, Epoch : 7, Step : 4470, Loss : 0.31558, Acc : 0.856, Sensitive_Loss : 0.15374, Sensitive_Acc : 15.800, Run Time : 10.52 sec
INFO:root:2024-04-10 14:50:32, Train, Epoch : 7, Step : 4480, Loss : 0.35887, Acc : 0.850, Sensitive_Loss : 0.08944, Sensitive_Acc : 16.300, Run Time : 10.62 sec
INFO:root:2024-04-10 14:50:40, Train, Epoch : 7, Step : 4490, Loss : 0.35079, Acc : 0.869, Sensitive_Loss : 0.11520, Sensitive_Acc : 15.400, Run Time : 8.70 sec
INFO:root:2024-04-10 14:50:51, Train, Epoch : 7, Step : 4500, Loss : 0.33203, Acc : 0.831, Sensitive_Loss : 0.10252, Sensitive_Acc : 17.900, Run Time : 10.63 sec
INFO:root:2024-04-10 14:52:26, Dev, Step : 4500, Loss : 0.53134, Acc : 0.776, Auc : 0.863, Sensitive_Loss : 0.17784, Sensitive_Acc : 16.293, Sensitive_Auc : 0.992, Mean auc: 0.863, Run Time : 94.51 sec
INFO:root:2024-04-10 14:54:18
INFO:root:y_pred: [0.14871489 0.01555361 0.9345064  ... 0.6198079  0.31144467 0.14939745]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9935800e-01 6.7332499e-03 8.3007431e-01 9.9999774e-01 9.9950147e-01
 9.8775429e-01 9.9979347e-01 1.0826470e-04 9.0913671e-01 9.9945432e-01
 5.6780541e-01 2.2639368e-01 4.3187125e-04 9.8098606e-01 9.9996293e-01
 9.9998403e-01 9.9755943e-01 9.9788636e-01 9.9942845e-01 9.9861491e-01
 9.9724859e-01 8.0548161e-01 9.9913567e-01 7.9656518e-01 9.3276846e-01
 2.5289130e-01 9.9923372e-01 3.8326968e-02 9.9967098e-01 5.4420527e-02
 8.4340230e-02 3.5936770e-01 1.9010013e-02 9.9941385e-01 6.6145426e-06
 9.9141508e-01 3.5722577e-04 9.9993491e-01 2.2291714e-02 9.8399955e-01
 9.9859136e-01 3.7926536e-03 1.2348795e-02 1.6437036e-03 6.3811374e-01
 7.7725148e-01 9.9963582e-01 9.7839785e-01 9.9348205e-01 9.9780554e-01
 2.1066843e-03 9.5929021e-01 6.5995120e-03 3.4437531e-01 9.9993908e-01
 8.0039822e-02 9.9476177e-01 9.9943596e-01 9.9022305e-01 3.6074996e-02
 7.2448393e-03 9.9973089e-01 1.7495672e-01 9.9932575e-01 9.9165571e-01
 5.6586766e-01 9.9464959e-01 9.1503680e-01 9.9989903e-01 9.9960226e-01
 1.8121836e-01 6.4440906e-01 9.9467057e-01 9.9966061e-01 9.9584633e-01
 2.7634470e-02 7.9925144e-01 1.5470365e-02 1.1752745e-03 9.9558133e-01
 5.0899439e-02 9.9975604e-01 9.9991035e-01 9.9906486e-01 2.0784231e-02
 9.9999928e-01 5.7041380e-03 5.4704335e-02 9.9952745e-01 9.9837303e-01
 3.4001321e-02 9.6735173e-01 7.9367738e-03 9.5878595e-01 9.2100665e-02
 9.9999344e-01 1.4822357e-02 9.9813277e-01 9.0534496e-01 9.5925722e-03
 1.6018831e-04 5.0767016e-01 9.9776030e-01 9.9964166e-01 9.8575968e-01
 9.7854257e-01 9.8645079e-01 1.6616937e-03 1.5161791e-01 9.9967134e-01
 2.5695792e-04 1.0821156e-03 2.3278041e-01 9.9991441e-01 9.9844724e-01
 1.5519629e-03 9.3420804e-01 8.6534210e-04 9.9758184e-01 9.8616648e-01
 9.9999273e-01 9.9999917e-01 3.3125716e-01 1.7550109e-01 2.0493037e-01
 2.4539342e-02 1.4056463e-02 1.2645689e-03 9.9970043e-01 9.9998963e-01
 5.6016371e-03 4.5978529e-03 9.4323413e-04 9.2363429e-01 9.9975187e-01
 9.9889296e-01 9.9929965e-01 8.8986605e-01 6.0418040e-02 9.7113287e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 14:54:18, Dev, Step : 4508, Loss : 0.52989, Acc : 0.774, Auc : 0.862, Sensitive_Loss : 0.17130, Sensitive_Acc : 16.293, Sensitive_Auc : 0.991, Mean auc: 0.862, Run Time : 105.68 sec
INFO:root:2024-04-10 14:54:22, Train, Epoch : 8, Step : 4510, Loss : 0.05638, Acc : 0.175, Sensitive_Loss : 0.01234, Sensitive_Acc : 3.100, Run Time : 2.69 sec
INFO:root:2024-04-10 14:54:31, Train, Epoch : 8, Step : 4520, Loss : 0.35136, Acc : 0.853, Sensitive_Loss : 0.09854, Sensitive_Acc : 15.700, Run Time : 9.04 sec
INFO:root:2024-04-10 14:54:41, Train, Epoch : 8, Step : 4530, Loss : 0.33126, Acc : 0.866, Sensitive_Loss : 0.14548, Sensitive_Acc : 17.100, Run Time : 10.23 sec
INFO:root:2024-04-10 14:54:52, Train, Epoch : 8, Step : 4540, Loss : 0.39870, Acc : 0.800, Sensitive_Loss : 0.19049, Sensitive_Acc : 15.700, Run Time : 10.48 sec
INFO:root:2024-04-10 14:55:02, Train, Epoch : 8, Step : 4550, Loss : 0.29114, Acc : 0.834, Sensitive_Loss : 0.10743, Sensitive_Acc : 15.900, Run Time : 10.05 sec
INFO:root:2024-04-10 14:55:14, Train, Epoch : 8, Step : 4560, Loss : 0.32434, Acc : 0.844, Sensitive_Loss : 0.10980, Sensitive_Acc : 16.800, Run Time : 11.84 sec
INFO:root:2024-04-10 14:55:27, Train, Epoch : 8, Step : 4570, Loss : 0.29804, Acc : 0.872, Sensitive_Loss : 0.11513, Sensitive_Acc : 16.000, Run Time : 13.48 sec
INFO:root:2024-04-10 14:55:39, Train, Epoch : 8, Step : 4580, Loss : 0.32415, Acc : 0.856, Sensitive_Loss : 0.11214, Sensitive_Acc : 16.000, Run Time : 11.87 sec
INFO:root:2024-04-10 14:55:49, Train, Epoch : 8, Step : 4590, Loss : 0.32389, Acc : 0.856, Sensitive_Loss : 0.12972, Sensitive_Acc : 18.200, Run Time : 10.41 sec
INFO:root:2024-04-10 14:56:01, Train, Epoch : 8, Step : 4600, Loss : 0.30278, Acc : 0.881, Sensitive_Loss : 0.10592, Sensitive_Acc : 16.400, Run Time : 11.56 sec
INFO:root:2024-04-10 14:57:40, Dev, Step : 4600, Loss : 0.54240, Acc : 0.772, Auc : 0.858, Sensitive_Loss : 0.14345, Sensitive_Acc : 16.350, Sensitive_Auc : 0.991, Mean auc: 0.858, Run Time : 98.93 sec
INFO:root:2024-04-10 14:57:48, Train, Epoch : 8, Step : 4610, Loss : 0.31356, Acc : 0.866, Sensitive_Loss : 0.13234, Sensitive_Acc : 16.000, Run Time : 107.37 sec
INFO:root:2024-04-10 14:58:01, Train, Epoch : 8, Step : 4620, Loss : 0.28198, Acc : 0.866, Sensitive_Loss : 0.12731, Sensitive_Acc : 15.500, Run Time : 12.91 sec
INFO:root:2024-04-10 14:58:12, Train, Epoch : 8, Step : 4630, Loss : 0.33402, Acc : 0.859, Sensitive_Loss : 0.09787, Sensitive_Acc : 15.500, Run Time : 10.43 sec
INFO:root:2024-04-10 14:58:21, Train, Epoch : 8, Step : 4640, Loss : 0.40318, Acc : 0.834, Sensitive_Loss : 0.10923, Sensitive_Acc : 16.200, Run Time : 9.59 sec
INFO:root:2024-04-10 14:58:30, Train, Epoch : 8, Step : 4650, Loss : 0.34557, Acc : 0.844, Sensitive_Loss : 0.10941, Sensitive_Acc : 16.700, Run Time : 9.12 sec
INFO:root:2024-04-10 14:58:40, Train, Epoch : 8, Step : 4660, Loss : 0.32467, Acc : 0.841, Sensitive_Loss : 0.07169, Sensitive_Acc : 16.100, Run Time : 9.82 sec
INFO:root:2024-04-10 14:58:49, Train, Epoch : 8, Step : 4670, Loss : 0.31135, Acc : 0.863, Sensitive_Loss : 0.11572, Sensitive_Acc : 16.300, Run Time : 9.06 sec
INFO:root:2024-04-10 14:59:02, Train, Epoch : 8, Step : 4680, Loss : 0.27463, Acc : 0.875, Sensitive_Loss : 0.11860, Sensitive_Acc : 16.500, Run Time : 12.75 sec
INFO:root:2024-04-10 14:59:14, Train, Epoch : 8, Step : 4690, Loss : 0.35609, Acc : 0.844, Sensitive_Loss : 0.11683, Sensitive_Acc : 15.300, Run Time : 11.84 sec
INFO:root:2024-04-10 14:59:23, Train, Epoch : 8, Step : 4700, Loss : 0.34849, Acc : 0.825, Sensitive_Loss : 0.10415, Sensitive_Acc : 15.800, Run Time : 9.62 sec
INFO:root:2024-04-10 15:01:01, Dev, Step : 4700, Loss : 0.53803, Acc : 0.777, Auc : 0.859, Sensitive_Loss : 0.16616, Sensitive_Acc : 16.293, Sensitive_Auc : 0.989, Mean auc: 0.859, Run Time : 97.98 sec
INFO:root:2024-04-10 15:01:09, Train, Epoch : 8, Step : 4710, Loss : 0.36384, Acc : 0.850, Sensitive_Loss : 0.08663, Sensitive_Acc : 15.500, Run Time : 105.78 sec
INFO:root:2024-04-10 15:01:19, Train, Epoch : 8, Step : 4720, Loss : 0.30512, Acc : 0.884, Sensitive_Loss : 0.11673, Sensitive_Acc : 17.700, Run Time : 9.33 sec
INFO:root:2024-04-10 15:01:31, Train, Epoch : 8, Step : 4730, Loss : 0.32031, Acc : 0.838, Sensitive_Loss : 0.12013, Sensitive_Acc : 15.700, Run Time : 12.10 sec
INFO:root:2024-04-10 15:01:41, Train, Epoch : 8, Step : 4740, Loss : 0.32696, Acc : 0.859, Sensitive_Loss : 0.12169, Sensitive_Acc : 15.600, Run Time : 10.76 sec
INFO:root:2024-04-10 15:01:51, Train, Epoch : 8, Step : 4750, Loss : 0.27292, Acc : 0.894, Sensitive_Loss : 0.09632, Sensitive_Acc : 15.900, Run Time : 9.38 sec
INFO:root:2024-04-10 15:02:04, Train, Epoch : 8, Step : 4760, Loss : 0.31137, Acc : 0.863, Sensitive_Loss : 0.11052, Sensitive_Acc : 16.200, Run Time : 13.15 sec
INFO:root:2024-04-10 15:02:15, Train, Epoch : 8, Step : 4770, Loss : 0.28992, Acc : 0.887, Sensitive_Loss : 0.09975, Sensitive_Acc : 15.700, Run Time : 11.55 sec
INFO:root:2024-04-10 15:02:25, Train, Epoch : 8, Step : 4780, Loss : 0.35203, Acc : 0.866, Sensitive_Loss : 0.10835, Sensitive_Acc : 16.400, Run Time : 9.75 sec
INFO:root:2024-04-10 15:02:37, Train, Epoch : 8, Step : 4790, Loss : 0.26575, Acc : 0.887, Sensitive_Loss : 0.07624, Sensitive_Acc : 17.000, Run Time : 12.20 sec
INFO:root:2024-04-10 15:02:51, Train, Epoch : 8, Step : 4800, Loss : 0.33636, Acc : 0.859, Sensitive_Loss : 0.10050, Sensitive_Acc : 16.700, Run Time : 13.94 sec
INFO:root:2024-04-10 15:04:28, Dev, Step : 4800, Loss : 0.52240, Acc : 0.780, Auc : 0.861, Sensitive_Loss : 0.15649, Sensitive_Acc : 16.293, Sensitive_Auc : 0.990, Mean auc: 0.861, Run Time : 97.08 sec
INFO:root:2024-04-10 15:04:35, Train, Epoch : 8, Step : 4810, Loss : 0.35385, Acc : 0.841, Sensitive_Loss : 0.07759, Sensitive_Acc : 17.000, Run Time : 103.78 sec
INFO:root:2024-04-10 15:04:45, Train, Epoch : 8, Step : 4820, Loss : 0.26551, Acc : 0.894, Sensitive_Loss : 0.12180, Sensitive_Acc : 16.900, Run Time : 9.49 sec
INFO:root:2024-04-10 15:04:56, Train, Epoch : 8, Step : 4830, Loss : 0.33054, Acc : 0.869, Sensitive_Loss : 0.10369, Sensitive_Acc : 16.600, Run Time : 10.99 sec
INFO:root:2024-04-10 15:05:07, Train, Epoch : 8, Step : 4840, Loss : 0.24956, Acc : 0.881, Sensitive_Loss : 0.12563, Sensitive_Acc : 17.200, Run Time : 11.21 sec
INFO:root:2024-04-10 15:05:16, Train, Epoch : 8, Step : 4850, Loss : 0.35532, Acc : 0.841, Sensitive_Loss : 0.14892, Sensitive_Acc : 16.800, Run Time : 9.39 sec
INFO:root:2024-04-10 15:05:25, Train, Epoch : 8, Step : 4860, Loss : 0.21607, Acc : 0.884, Sensitive_Loss : 0.09349, Sensitive_Acc : 16.700, Run Time : 9.18 sec
INFO:root:2024-04-10 15:05:37, Train, Epoch : 8, Step : 4870, Loss : 0.30569, Acc : 0.875, Sensitive_Loss : 0.07715, Sensitive_Acc : 14.400, Run Time : 11.74 sec
INFO:root:2024-04-10 15:05:47, Train, Epoch : 8, Step : 4880, Loss : 0.27950, Acc : 0.881, Sensitive_Loss : 0.07826, Sensitive_Acc : 15.200, Run Time : 10.00 sec
INFO:root:2024-04-10 15:05:58, Train, Epoch : 8, Step : 4890, Loss : 0.28080, Acc : 0.878, Sensitive_Loss : 0.15890, Sensitive_Acc : 16.500, Run Time : 10.57 sec
INFO:root:2024-04-10 15:06:07, Train, Epoch : 8, Step : 4900, Loss : 0.32388, Acc : 0.859, Sensitive_Loss : 0.11435, Sensitive_Acc : 17.100, Run Time : 9.06 sec
INFO:root:2024-04-10 15:07:46, Dev, Step : 4900, Loss : 0.55200, Acc : 0.775, Auc : 0.857, Sensitive_Loss : 0.16862, Sensitive_Acc : 16.321, Sensitive_Auc : 0.993, Mean auc: 0.857, Run Time : 99.14 sec
INFO:root:2024-04-10 15:07:52, Train, Epoch : 8, Step : 4910, Loss : 0.28613, Acc : 0.844, Sensitive_Loss : 0.11043, Sensitive_Acc : 16.100, Run Time : 105.53 sec
INFO:root:2024-04-10 15:08:04, Train, Epoch : 8, Step : 4920, Loss : 0.33173, Acc : 0.850, Sensitive_Loss : 0.18835, Sensitive_Acc : 14.800, Run Time : 11.47 sec
INFO:root:2024-04-10 15:08:14, Train, Epoch : 8, Step : 4930, Loss : 0.23045, Acc : 0.894, Sensitive_Loss : 0.11797, Sensitive_Acc : 16.100, Run Time : 10.15 sec
INFO:root:2024-04-10 15:08:24, Train, Epoch : 8, Step : 4940, Loss : 0.28501, Acc : 0.875, Sensitive_Loss : 0.14834, Sensitive_Acc : 14.600, Run Time : 9.67 sec
INFO:root:2024-04-10 15:08:32, Train, Epoch : 8, Step : 4950, Loss : 0.32503, Acc : 0.831, Sensitive_Loss : 0.10075, Sensitive_Acc : 14.100, Run Time : 8.85 sec
INFO:root:2024-04-10 15:08:43, Train, Epoch : 8, Step : 4960, Loss : 0.30528, Acc : 0.881, Sensitive_Loss : 0.14479, Sensitive_Acc : 16.700, Run Time : 10.40 sec
INFO:root:2024-04-10 15:08:52, Train, Epoch : 8, Step : 4970, Loss : 0.32880, Acc : 0.866, Sensitive_Loss : 0.08340, Sensitive_Acc : 15.600, Run Time : 9.15 sec
INFO:root:2024-04-10 15:09:01, Train, Epoch : 8, Step : 4980, Loss : 0.39125, Acc : 0.831, Sensitive_Loss : 0.13756, Sensitive_Acc : 19.000, Run Time : 9.44 sec
INFO:root:2024-04-10 15:09:12, Train, Epoch : 8, Step : 4990, Loss : 0.28149, Acc : 0.866, Sensitive_Loss : 0.11372, Sensitive_Acc : 15.400, Run Time : 10.23 sec
INFO:root:2024-04-10 15:09:24, Train, Epoch : 8, Step : 5000, Loss : 0.29556, Acc : 0.850, Sensitive_Loss : 0.10542, Sensitive_Acc : 14.800, Run Time : 12.65 sec
INFO:root:2024-04-10 15:11:04, Dev, Step : 5000, Loss : 0.53118, Acc : 0.783, Auc : 0.858, Sensitive_Loss : 0.14350, Sensitive_Acc : 16.236, Sensitive_Auc : 0.992, Mean auc: 0.858, Run Time : 99.62 sec
INFO:root:2024-04-10 15:11:14, Train, Epoch : 8, Step : 5010, Loss : 0.33650, Acc : 0.838, Sensitive_Loss : 0.13055, Sensitive_Acc : 16.700, Run Time : 109.91 sec
INFO:root:2024-04-10 15:11:26, Train, Epoch : 8, Step : 5020, Loss : 0.36128, Acc : 0.838, Sensitive_Loss : 0.13111, Sensitive_Acc : 17.100, Run Time : 11.57 sec
INFO:root:2024-04-10 15:11:36, Train, Epoch : 8, Step : 5030, Loss : 0.36634, Acc : 0.828, Sensitive_Loss : 0.12491, Sensitive_Acc : 16.600, Run Time : 10.00 sec
INFO:root:2024-04-10 15:11:49, Train, Epoch : 8, Step : 5040, Loss : 0.40320, Acc : 0.803, Sensitive_Loss : 0.12175, Sensitive_Acc : 16.800, Run Time : 13.24 sec
INFO:root:2024-04-10 15:12:00, Train, Epoch : 8, Step : 5050, Loss : 0.31947, Acc : 0.847, Sensitive_Loss : 0.10573, Sensitive_Acc : 15.900, Run Time : 11.37 sec
INFO:root:2024-04-10 15:12:11, Train, Epoch : 8, Step : 5060, Loss : 0.33378, Acc : 0.856, Sensitive_Loss : 0.11172, Sensitive_Acc : 15.900, Run Time : 10.72 sec
INFO:root:2024-04-10 15:12:23, Train, Epoch : 8, Step : 5070, Loss : 0.32598, Acc : 0.853, Sensitive_Loss : 0.11370, Sensitive_Acc : 16.000, Run Time : 12.31 sec
INFO:root:2024-04-10 15:12:34, Train, Epoch : 8, Step : 5080, Loss : 0.25914, Acc : 0.891, Sensitive_Loss : 0.11012, Sensitive_Acc : 18.000, Run Time : 10.88 sec
INFO:root:2024-04-10 15:12:45, Train, Epoch : 8, Step : 5090, Loss : 0.34664, Acc : 0.844, Sensitive_Loss : 0.15063, Sensitive_Acc : 15.900, Run Time : 10.58 sec
INFO:root:2024-04-10 15:12:57, Train, Epoch : 8, Step : 5100, Loss : 0.35402, Acc : 0.844, Sensitive_Loss : 0.11213, Sensitive_Acc : 16.000, Run Time : 11.65 sec
INFO:root:2024-04-10 15:14:34, Dev, Step : 5100, Loss : 0.54275, Acc : 0.782, Auc : 0.862, Sensitive_Loss : 0.15383, Sensitive_Acc : 16.379, Sensitive_Auc : 0.993, Mean auc: 0.862, Run Time : 96.96 sec
INFO:root:2024-04-10 15:14:40, Train, Epoch : 8, Step : 5110, Loss : 0.36625, Acc : 0.844, Sensitive_Loss : 0.13202, Sensitive_Acc : 17.900, Run Time : 103.91 sec
INFO:root:2024-04-10 15:14:53, Train, Epoch : 8, Step : 5120, Loss : 0.33432, Acc : 0.878, Sensitive_Loss : 0.12089, Sensitive_Acc : 18.900, Run Time : 12.26 sec
INFO:root:2024-04-10 15:15:05, Train, Epoch : 8, Step : 5130, Loss : 0.32883, Acc : 0.834, Sensitive_Loss : 0.16949, Sensitive_Acc : 15.400, Run Time : 12.17 sec
INFO:root:2024-04-10 15:15:15, Train, Epoch : 8, Step : 5140, Loss : 0.26750, Acc : 0.878, Sensitive_Loss : 0.14757, Sensitive_Acc : 16.000, Run Time : 10.36 sec
INFO:root:2024-04-10 15:15:27, Train, Epoch : 8, Step : 5150, Loss : 0.30446, Acc : 0.856, Sensitive_Loss : 0.08471, Sensitive_Acc : 14.800, Run Time : 11.29 sec
INFO:root:2024-04-10 15:17:20
INFO:root:y_pred: [0.08257791 0.01632553 0.93822944 ... 0.5189411  0.34195486 0.07761902]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9918133e-01 2.1488010e-03 7.7627444e-01 9.9999607e-01 9.9899238e-01
 9.7660542e-01 9.9965990e-01 7.0771828e-05 8.6758810e-01 9.9812084e-01
 5.8279794e-01 1.1488673e-01 2.1496273e-04 9.6640521e-01 9.9994326e-01
 9.9998879e-01 9.9760842e-01 9.9783462e-01 9.9832314e-01 9.9887031e-01
 9.9490416e-01 7.1274775e-01 9.9925047e-01 7.7600777e-01 9.1314507e-01
 1.2866002e-01 9.9874604e-01 1.7440809e-02 9.9967480e-01 3.5341688e-02
 5.9250426e-02 4.7179204e-01 1.4726379e-02 9.9959987e-01 1.6256081e-06
 9.8917013e-01 1.7961119e-04 9.9993777e-01 7.9278843e-03 9.8585826e-01
 9.9660599e-01 2.4516212e-03 1.3743587e-02 1.0252745e-03 4.5892784e-01
 6.9830292e-01 9.9899203e-01 9.7146398e-01 9.7580034e-01 9.9692172e-01
 2.2132769e-03 9.3863285e-01 5.5211815e-03 2.8656992e-01 9.9991262e-01
 4.6021264e-02 9.9725217e-01 9.9915075e-01 9.6863919e-01 1.4863525e-02
 3.1270699e-03 9.9970680e-01 1.8446487e-01 9.9951470e-01 9.8203570e-01
 3.2766721e-01 9.8679322e-01 9.1803646e-01 9.9993610e-01 9.9808997e-01
 1.2628315e-01 6.7341864e-01 9.9041724e-01 9.9973732e-01 9.9670178e-01
 3.3578578e-02 8.5698879e-01 1.4252969e-02 4.1264406e-04 9.9153805e-01
 6.5592244e-02 9.9978393e-01 9.9991071e-01 9.9744439e-01 1.8885395e-02
 9.9999881e-01 2.7923526e-03 2.6211789e-02 9.9824238e-01 9.9727684e-01
 1.2836957e-02 9.2623925e-01 6.9953306e-03 9.6893466e-01 8.5110910e-02
 9.9998510e-01 7.4739675e-03 9.9693799e-01 7.9882705e-01 1.4699455e-02
 9.6945798e-05 3.8676360e-01 9.9555969e-01 9.9921119e-01 9.7623092e-01
 9.7064173e-01 9.7925848e-01 9.9328859e-04 1.2676862e-01 9.9941111e-01
 4.3333457e-05 6.6819845e-04 2.1979159e-01 9.9981266e-01 9.9784040e-01
 8.2684320e-04 9.2575842e-01 4.9713010e-04 9.9675828e-01 9.7416919e-01
 9.9999046e-01 9.9999714e-01 3.3953723e-01 2.6588455e-01 5.4150335e-02
 1.9073365e-02 4.2897253e-03 5.9855328e-04 9.9902964e-01 9.9996209e-01
 5.2724755e-03 2.3639188e-03 6.4987229e-04 8.6120069e-01 9.9924988e-01
 9.9846518e-01 9.9877828e-01 8.3744925e-01 6.1528228e-02 9.6717703e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 15:17:20, Dev, Step : 5152, Loss : 0.59696, Acc : 0.762, Auc : 0.860, Sensitive_Loss : 0.15000, Sensitive_Acc : 16.436, Sensitive_Auc : 0.993, Mean auc: 0.860, Run Time : 109.56 sec
INFO:root:2024-04-10 15:17:31, Train, Epoch : 9, Step : 5160, Loss : 0.22881, Acc : 0.688, Sensitive_Loss : 0.11535, Sensitive_Acc : 12.700, Run Time : 8.57 sec
INFO:root:2024-04-10 15:17:40, Train, Epoch : 9, Step : 5170, Loss : 0.34585, Acc : 0.859, Sensitive_Loss : 0.14628, Sensitive_Acc : 16.700, Run Time : 9.70 sec
INFO:root:2024-04-10 15:17:53, Train, Epoch : 9, Step : 5180, Loss : 0.27957, Acc : 0.887, Sensitive_Loss : 0.12389, Sensitive_Acc : 16.900, Run Time : 12.67 sec
INFO:root:2024-04-10 15:18:04, Train, Epoch : 9, Step : 5190, Loss : 0.30365, Acc : 0.853, Sensitive_Loss : 0.11357, Sensitive_Acc : 15.000, Run Time : 10.58 sec
INFO:root:2024-04-10 15:18:13, Train, Epoch : 9, Step : 5200, Loss : 0.28667, Acc : 0.884, Sensitive_Loss : 0.13184, Sensitive_Acc : 15.500, Run Time : 9.33 sec
INFO:root:2024-04-10 15:19:54, Dev, Step : 5200, Loss : 0.59100, Acc : 0.765, Auc : 0.858, Sensitive_Loss : 0.16891, Sensitive_Acc : 16.379, Sensitive_Auc : 0.992, Mean auc: 0.858, Run Time : 101.09 sec
INFO:root:2024-04-10 15:20:01, Train, Epoch : 9, Step : 5210, Loss : 0.27755, Acc : 0.900, Sensitive_Loss : 0.08157, Sensitive_Acc : 16.900, Run Time : 108.11 sec
INFO:root:2024-04-10 15:20:13, Train, Epoch : 9, Step : 5220, Loss : 0.34282, Acc : 0.866, Sensitive_Loss : 0.07916, Sensitive_Acc : 17.400, Run Time : 11.52 sec
INFO:root:2024-04-10 15:20:26, Train, Epoch : 9, Step : 5230, Loss : 0.28315, Acc : 0.872, Sensitive_Loss : 0.10451, Sensitive_Acc : 15.400, Run Time : 13.18 sec
INFO:root:2024-04-10 15:20:35, Train, Epoch : 9, Step : 5240, Loss : 0.33406, Acc : 0.834, Sensitive_Loss : 0.10657, Sensitive_Acc : 16.500, Run Time : 9.23 sec
INFO:root:2024-04-10 15:20:44, Train, Epoch : 9, Step : 5250, Loss : 0.27952, Acc : 0.894, Sensitive_Loss : 0.09043, Sensitive_Acc : 17.200, Run Time : 9.29 sec
INFO:root:2024-04-10 15:20:58, Train, Epoch : 9, Step : 5260, Loss : 0.29674, Acc : 0.863, Sensitive_Loss : 0.12291, Sensitive_Acc : 16.200, Run Time : 13.51 sec
INFO:root:2024-04-10 15:21:07, Train, Epoch : 9, Step : 5270, Loss : 0.31521, Acc : 0.887, Sensitive_Loss : 0.07722, Sensitive_Acc : 15.400, Run Time : 9.49 sec
INFO:root:2024-04-10 15:21:17, Train, Epoch : 9, Step : 5280, Loss : 0.27229, Acc : 0.881, Sensitive_Loss : 0.09589, Sensitive_Acc : 17.900, Run Time : 9.52 sec
INFO:root:2024-04-10 15:21:26, Train, Epoch : 9, Step : 5290, Loss : 0.28101, Acc : 0.872, Sensitive_Loss : 0.12168, Sensitive_Acc : 15.400, Run Time : 9.34 sec
INFO:root:2024-04-10 15:21:38, Train, Epoch : 9, Step : 5300, Loss : 0.31700, Acc : 0.866, Sensitive_Loss : 0.14436, Sensitive_Acc : 17.200, Run Time : 11.62 sec
INFO:root:2024-04-10 15:23:14, Dev, Step : 5300, Loss : 0.54864, Acc : 0.775, Auc : 0.857, Sensitive_Loss : 0.15615, Sensitive_Acc : 16.379, Sensitive_Auc : 0.989, Mean auc: 0.857, Run Time : 96.22 sec
INFO:root:2024-04-10 15:23:21, Train, Epoch : 9, Step : 5310, Loss : 0.27673, Acc : 0.878, Sensitive_Loss : 0.08505, Sensitive_Acc : 16.500, Run Time : 103.34 sec
INFO:root:2024-04-10 15:23:32, Train, Epoch : 9, Step : 5320, Loss : 0.29147, Acc : 0.869, Sensitive_Loss : 0.12123, Sensitive_Acc : 17.200, Run Time : 10.71 sec
INFO:root:2024-04-10 15:23:42, Train, Epoch : 9, Step : 5330, Loss : 0.24652, Acc : 0.894, Sensitive_Loss : 0.11666, Sensitive_Acc : 17.300, Run Time : 10.49 sec
INFO:root:2024-04-10 15:23:54, Train, Epoch : 9, Step : 5340, Loss : 0.30255, Acc : 0.856, Sensitive_Loss : 0.13330, Sensitive_Acc : 15.800, Run Time : 12.13 sec
INFO:root:2024-04-10 15:24:04, Train, Epoch : 9, Step : 5350, Loss : 0.26889, Acc : 0.859, Sensitive_Loss : 0.16023, Sensitive_Acc : 16.700, Run Time : 10.02 sec
INFO:root:2024-04-10 15:24:17, Train, Epoch : 9, Step : 5360, Loss : 0.26345, Acc : 0.891, Sensitive_Loss : 0.10881, Sensitive_Acc : 18.100, Run Time : 12.61 sec
INFO:root:2024-04-10 15:24:30, Train, Epoch : 9, Step : 5370, Loss : 0.27067, Acc : 0.891, Sensitive_Loss : 0.17127, Sensitive_Acc : 16.600, Run Time : 12.59 sec
INFO:root:2024-04-10 15:24:42, Train, Epoch : 9, Step : 5380, Loss : 0.27418, Acc : 0.863, Sensitive_Loss : 0.11676, Sensitive_Acc : 15.300, Run Time : 12.25 sec
INFO:root:2024-04-10 15:24:58, Train, Epoch : 9, Step : 5390, Loss : 0.32058, Acc : 0.856, Sensitive_Loss : 0.10647, Sensitive_Acc : 15.300, Run Time : 15.79 sec
INFO:root:2024-04-10 15:25:08, Train, Epoch : 9, Step : 5400, Loss : 0.27532, Acc : 0.900, Sensitive_Loss : 0.12512, Sensitive_Acc : 17.700, Run Time : 10.55 sec
INFO:root:2024-04-10 15:27:32, Dev, Step : 5400, Loss : 0.53248, Acc : 0.782, Auc : 0.861, Sensitive_Loss : 0.14443, Sensitive_Acc : 16.236, Sensitive_Auc : 0.992, Mean auc: 0.861, Run Time : 143.54 sec
INFO:root:2024-04-10 15:27:42, Train, Epoch : 9, Step : 5410, Loss : 0.29820, Acc : 0.869, Sensitive_Loss : 0.11206, Sensitive_Acc : 15.800, Run Time : 153.96 sec
INFO:root:2024-04-10 15:27:54, Train, Epoch : 9, Step : 5420, Loss : 0.29782, Acc : 0.869, Sensitive_Loss : 0.09465, Sensitive_Acc : 16.600, Run Time : 11.53 sec
INFO:root:2024-04-10 15:28:06, Train, Epoch : 9, Step : 5430, Loss : 0.25173, Acc : 0.884, Sensitive_Loss : 0.11906, Sensitive_Acc : 16.500, Run Time : 12.00 sec
INFO:root:2024-04-10 15:28:20, Train, Epoch : 9, Step : 5440, Loss : 0.31127, Acc : 0.866, Sensitive_Loss : 0.12421, Sensitive_Acc : 15.400, Run Time : 13.86 sec
INFO:root:2024-04-10 15:28:29, Train, Epoch : 9, Step : 5450, Loss : 0.25424, Acc : 0.884, Sensitive_Loss : 0.10137, Sensitive_Acc : 15.500, Run Time : 9.50 sec
INFO:root:2024-04-10 15:28:40, Train, Epoch : 9, Step : 5460, Loss : 0.36920, Acc : 0.834, Sensitive_Loss : 0.09776, Sensitive_Acc : 16.600, Run Time : 10.58 sec
INFO:root:2024-04-10 15:28:52, Train, Epoch : 9, Step : 5470, Loss : 0.25602, Acc : 0.897, Sensitive_Loss : 0.11625, Sensitive_Acc : 15.100, Run Time : 12.27 sec
INFO:root:2024-04-10 15:29:02, Train, Epoch : 9, Step : 5480, Loss : 0.39076, Acc : 0.831, Sensitive_Loss : 0.11493, Sensitive_Acc : 17.000, Run Time : 10.11 sec
INFO:root:2024-04-10 15:29:12, Train, Epoch : 9, Step : 5490, Loss : 0.26848, Acc : 0.875, Sensitive_Loss : 0.10029, Sensitive_Acc : 15.400, Run Time : 10.07 sec
INFO:root:2024-04-10 15:29:23, Train, Epoch : 9, Step : 5500, Loss : 0.33183, Acc : 0.866, Sensitive_Loss : 0.11525, Sensitive_Acc : 16.400, Run Time : 10.65 sec
INFO:root:2024-04-10 15:31:45, Dev, Step : 5500, Loss : 0.55408, Acc : 0.776, Auc : 0.856, Sensitive_Loss : 0.12847, Sensitive_Acc : 16.307, Sensitive_Auc : 0.989, Mean auc: 0.856, Run Time : 142.17 sec
INFO:root:2024-04-10 15:31:51, Train, Epoch : 9, Step : 5510, Loss : 0.27939, Acc : 0.878, Sensitive_Loss : 0.10606, Sensitive_Acc : 15.400, Run Time : 147.90 sec
INFO:root:2024-04-10 15:31:59, Train, Epoch : 9, Step : 5520, Loss : 0.28419, Acc : 0.838, Sensitive_Loss : 0.09417, Sensitive_Acc : 15.400, Run Time : 8.34 sec
INFO:root:2024-04-10 15:32:09, Train, Epoch : 9, Step : 5530, Loss : 0.28469, Acc : 0.887, Sensitive_Loss : 0.12122, Sensitive_Acc : 15.700, Run Time : 9.96 sec
INFO:root:2024-04-10 15:32:18, Train, Epoch : 9, Step : 5540, Loss : 0.28806, Acc : 0.894, Sensitive_Loss : 0.08757, Sensitive_Acc : 15.100, Run Time : 8.71 sec
INFO:root:2024-04-10 15:32:26, Train, Epoch : 9, Step : 5550, Loss : 0.25744, Acc : 0.884, Sensitive_Loss : 0.13537, Sensitive_Acc : 16.600, Run Time : 8.50 sec
INFO:root:2024-04-10 15:32:35, Train, Epoch : 9, Step : 5560, Loss : 0.33354, Acc : 0.853, Sensitive_Loss : 0.13635, Sensitive_Acc : 16.000, Run Time : 8.43 sec
INFO:root:2024-04-10 15:32:44, Train, Epoch : 9, Step : 5570, Loss : 0.33416, Acc : 0.853, Sensitive_Loss : 0.12009, Sensitive_Acc : 18.100, Run Time : 9.58 sec
INFO:root:2024-04-10 15:32:52, Train, Epoch : 9, Step : 5580, Loss : 0.29831, Acc : 0.847, Sensitive_Loss : 0.09853, Sensitive_Acc : 16.000, Run Time : 8.14 sec
INFO:root:2024-04-10 15:33:01, Train, Epoch : 9, Step : 5590, Loss : 0.29225, Acc : 0.859, Sensitive_Loss : 0.10681, Sensitive_Acc : 16.800, Run Time : 8.12 sec
INFO:root:2024-04-10 15:33:08, Train, Epoch : 9, Step : 5600, Loss : 0.29726, Acc : 0.887, Sensitive_Loss : 0.09143, Sensitive_Acc : 15.700, Run Time : 7.86 sec
INFO:root:2024-04-10 15:34:54, Dev, Step : 5600, Loss : 0.53415, Acc : 0.780, Auc : 0.858, Sensitive_Loss : 0.16129, Sensitive_Acc : 16.293, Sensitive_Auc : 0.990, Mean auc: 0.858, Run Time : 105.12 sec
INFO:root:2024-04-10 15:35:00, Train, Epoch : 9, Step : 5610, Loss : 0.34875, Acc : 0.853, Sensitive_Loss : 0.11400, Sensitive_Acc : 14.500, Run Time : 111.22 sec
INFO:root:2024-04-10 15:35:08, Train, Epoch : 9, Step : 5620, Loss : 0.35397, Acc : 0.834, Sensitive_Loss : 0.07345, Sensitive_Acc : 16.300, Run Time : 8.11 sec
INFO:root:2024-04-10 15:35:16, Train, Epoch : 9, Step : 5630, Loss : 0.33852, Acc : 0.878, Sensitive_Loss : 0.11959, Sensitive_Acc : 16.200, Run Time : 8.28 sec
INFO:root:2024-04-10 15:35:24, Train, Epoch : 9, Step : 5640, Loss : 0.29659, Acc : 0.856, Sensitive_Loss : 0.11965, Sensitive_Acc : 15.000, Run Time : 8.03 sec
INFO:root:2024-04-10 15:35:33, Train, Epoch : 9, Step : 5650, Loss : 0.30906, Acc : 0.866, Sensitive_Loss : 0.08982, Sensitive_Acc : 15.300, Run Time : 8.47 sec
INFO:root:2024-04-10 15:35:42, Train, Epoch : 9, Step : 5660, Loss : 0.34046, Acc : 0.875, Sensitive_Loss : 0.13232, Sensitive_Acc : 16.800, Run Time : 9.24 sec
INFO:root:2024-04-10 15:35:50, Train, Epoch : 9, Step : 5670, Loss : 0.26875, Acc : 0.887, Sensitive_Loss : 0.10662, Sensitive_Acc : 16.800, Run Time : 8.55 sec
INFO:root:2024-04-10 15:36:01, Train, Epoch : 9, Step : 5680, Loss : 0.31625, Acc : 0.853, Sensitive_Loss : 0.10886, Sensitive_Acc : 15.900, Run Time : 10.84 sec
INFO:root:2024-04-10 15:36:10, Train, Epoch : 9, Step : 5690, Loss : 0.27744, Acc : 0.884, Sensitive_Loss : 0.11752, Sensitive_Acc : 16.200, Run Time : 8.63 sec
INFO:root:2024-04-10 15:36:19, Train, Epoch : 9, Step : 5700, Loss : 0.31947, Acc : 0.897, Sensitive_Loss : 0.10566, Sensitive_Acc : 16.700, Run Time : 9.14 sec
INFO:root:2024-04-10 15:37:55, Dev, Step : 5700, Loss : 0.54940, Acc : 0.781, Auc : 0.858, Sensitive_Loss : 0.15395, Sensitive_Acc : 16.221, Sensitive_Auc : 0.991, Mean auc: 0.858, Run Time : 95.84 sec
INFO:root:2024-04-10 15:38:01, Train, Epoch : 9, Step : 5710, Loss : 0.34604, Acc : 0.850, Sensitive_Loss : 0.08873, Sensitive_Acc : 16.800, Run Time : 101.79 sec
INFO:root:2024-04-10 15:38:09, Train, Epoch : 9, Step : 5720, Loss : 0.30316, Acc : 0.856, Sensitive_Loss : 0.12570, Sensitive_Acc : 16.500, Run Time : 8.30 sec
INFO:root:2024-04-10 15:38:19, Train, Epoch : 9, Step : 5730, Loss : 0.31694, Acc : 0.863, Sensitive_Loss : 0.11191, Sensitive_Acc : 17.000, Run Time : 9.48 sec
INFO:root:2024-04-10 15:38:29, Train, Epoch : 9, Step : 5740, Loss : 0.29947, Acc : 0.859, Sensitive_Loss : 0.07732, Sensitive_Acc : 17.800, Run Time : 10.76 sec
INFO:root:2024-04-10 15:38:38, Train, Epoch : 9, Step : 5750, Loss : 0.33971, Acc : 0.853, Sensitive_Loss : 0.10748, Sensitive_Acc : 17.900, Run Time : 9.10 sec
INFO:root:2024-04-10 15:38:47, Train, Epoch : 9, Step : 5760, Loss : 0.27336, Acc : 0.875, Sensitive_Loss : 0.08992, Sensitive_Acc : 14.600, Run Time : 9.06 sec
INFO:root:2024-04-10 15:39:00, Train, Epoch : 9, Step : 5770, Loss : 0.24196, Acc : 0.909, Sensitive_Loss : 0.09005, Sensitive_Acc : 17.000, Run Time : 12.18 sec
INFO:root:2024-04-10 15:39:10, Train, Epoch : 9, Step : 5780, Loss : 0.24478, Acc : 0.887, Sensitive_Loss : 0.10074, Sensitive_Acc : 16.000, Run Time : 10.81 sec
INFO:root:2024-04-10 15:39:20, Train, Epoch : 9, Step : 5790, Loss : 0.27999, Acc : 0.872, Sensitive_Loss : 0.10503, Sensitive_Acc : 16.300, Run Time : 9.90 sec
INFO:root:2024-04-10 15:41:03
INFO:root:y_pred: [0.10796818 0.01201414 0.97030264 ... 0.64332765 0.7475746  0.11009057]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9918228e-01 2.0909421e-03 8.9251459e-01 9.9999678e-01 9.9948359e-01
 9.8179334e-01 9.9982327e-01 4.3460179e-05 8.3698881e-01 9.9950051e-01
 4.5265228e-01 9.9371165e-02 9.6956166e-05 9.6803498e-01 9.9999225e-01
 9.9997759e-01 9.9791151e-01 9.9825102e-01 9.9693054e-01 9.9825984e-01
 9.9723512e-01 5.8010393e-01 9.9894935e-01 8.2639152e-01 9.1503507e-01
 1.4822046e-01 9.9914849e-01 1.3828443e-02 9.9969745e-01 4.7827307e-02
 4.3219078e-02 4.0508512e-01 1.7483180e-02 9.9967694e-01 2.5211500e-06
 9.9651563e-01 1.4874512e-04 9.9993992e-01 1.1382080e-02 9.7551775e-01
 9.9837351e-01 1.6646064e-03 1.8888924e-02 8.6208893e-04 5.9660214e-01
 7.8170723e-01 9.9950528e-01 9.8771894e-01 9.8780572e-01 9.9635917e-01
 1.7489484e-03 9.4935340e-01 2.6066194e-03 2.2100498e-01 9.9997044e-01
 2.1214247e-02 9.9681693e-01 9.9955565e-01 9.6451455e-01 1.0632323e-02
 9.0093017e-03 9.9979728e-01 2.1725161e-01 9.9959797e-01 9.8465192e-01
 6.4048386e-01 9.9258000e-01 9.3931741e-01 9.9992096e-01 9.9951875e-01
 1.1346349e-01 4.5777839e-01 9.9490118e-01 9.9960059e-01 9.9509537e-01
 3.0138128e-02 8.5806972e-01 8.3412267e-03 1.6048139e-04 9.9552345e-01
 4.6485391e-02 9.9974829e-01 9.9991369e-01 9.9910176e-01 1.5348930e-02
 9.9999976e-01 3.1031820e-03 2.6302453e-02 9.9962044e-01 9.9690253e-01
 1.4829872e-02 9.5499301e-01 1.0365138e-02 9.8090243e-01 5.6009073e-02
 9.9999702e-01 9.5827049e-03 9.9641883e-01 8.8954246e-01 4.9171811e-03
 1.3516303e-04 4.3991584e-01 9.9747556e-01 9.9948162e-01 9.6049762e-01
 9.5974696e-01 9.8402506e-01 9.5672533e-04 9.8112769e-02 9.9965739e-01
 3.6048135e-05 1.8179700e-03 1.2690541e-01 9.9998081e-01 9.9783665e-01
 9.8459132e-04 9.5594895e-01 2.3893391e-04 9.9610329e-01 9.8152578e-01
 9.9999404e-01 9.9999881e-01 2.0810939e-01 2.0368740e-01 1.0445798e-01
 6.0831849e-03 6.7973742e-03 1.0847414e-03 9.9956554e-01 9.9999273e-01
 7.8209080e-03 1.6647958e-03 4.4403292e-04 8.7672478e-01 9.9876785e-01
 9.9824905e-01 9.9889261e-01 8.8653117e-01 1.2424746e-01 9.7719860e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 15:41:03, Dev, Step : 5796, Loss : 0.57401, Acc : 0.776, Auc : 0.857, Sensitive_Loss : 0.15066, Sensitive_Acc : 16.279, Sensitive_Auc : 0.993, Mean auc: 0.857, Run Time : 95.94 sec
INFO:root:2024-04-10 15:41:09, Train, Epoch : 10, Step : 5800, Loss : 0.09553, Acc : 0.353, Sensitive_Loss : 0.06337, Sensitive_Acc : 6.900, Run Time : 4.67 sec
INFO:root:2024-04-10 15:42:50, Dev, Step : 5800, Loss : 0.59008, Acc : 0.772, Auc : 0.856, Sensitive_Loss : 0.15294, Sensitive_Acc : 16.279, Sensitive_Auc : 0.993, Mean auc: 0.856, Run Time : 100.41 sec
INFO:root:2024-04-10 15:42:58, Train, Epoch : 10, Step : 5810, Loss : 0.28875, Acc : 0.872, Sensitive_Loss : 0.11984, Sensitive_Acc : 15.700, Run Time : 108.51 sec
INFO:root:2024-04-10 15:43:10, Train, Epoch : 10, Step : 5820, Loss : 0.26770, Acc : 0.900, Sensitive_Loss : 0.10175, Sensitive_Acc : 17.200, Run Time : 12.09 sec
INFO:root:2024-04-10 15:43:22, Train, Epoch : 10, Step : 5830, Loss : 0.23943, Acc : 0.894, Sensitive_Loss : 0.11643, Sensitive_Acc : 16.200, Run Time : 12.37 sec
INFO:root:2024-04-10 15:43:33, Train, Epoch : 10, Step : 5840, Loss : 0.25852, Acc : 0.869, Sensitive_Loss : 0.10923, Sensitive_Acc : 15.600, Run Time : 11.00 sec
INFO:root:2024-04-10 15:43:47, Train, Epoch : 10, Step : 5850, Loss : 0.25397, Acc : 0.900, Sensitive_Loss : 0.11421, Sensitive_Acc : 15.900, Run Time : 13.82 sec
INFO:root:2024-04-10 15:43:57, Train, Epoch : 10, Step : 5860, Loss : 0.19573, Acc : 0.894, Sensitive_Loss : 0.09771, Sensitive_Acc : 16.600, Run Time : 9.97 sec
INFO:root:2024-04-10 15:44:08, Train, Epoch : 10, Step : 5870, Loss : 0.26637, Acc : 0.863, Sensitive_Loss : 0.11015, Sensitive_Acc : 16.300, Run Time : 10.97 sec
INFO:root:2024-04-10 15:44:19, Train, Epoch : 10, Step : 5880, Loss : 0.26224, Acc : 0.884, Sensitive_Loss : 0.09994, Sensitive_Acc : 16.000, Run Time : 11.44 sec
INFO:root:2024-04-10 15:44:30, Train, Epoch : 10, Step : 5890, Loss : 0.27485, Acc : 0.887, Sensitive_Loss : 0.10280, Sensitive_Acc : 17.400, Run Time : 11.09 sec
INFO:root:2024-04-10 15:44:41, Train, Epoch : 10, Step : 5900, Loss : 0.31468, Acc : 0.859, Sensitive_Loss : 0.08998, Sensitive_Acc : 15.500, Run Time : 10.67 sec
INFO:root:2024-04-10 15:46:35, Dev, Step : 5900, Loss : 0.58839, Acc : 0.769, Auc : 0.856, Sensitive_Loss : 0.14746, Sensitive_Acc : 16.264, Sensitive_Auc : 0.994, Mean auc: 0.856, Run Time : 113.55 sec
INFO:root:2024-04-10 15:46:42, Train, Epoch : 10, Step : 5910, Loss : 0.24504, Acc : 0.875, Sensitive_Loss : 0.11425, Sensitive_Acc : 14.900, Run Time : 120.54 sec
INFO:root:2024-04-10 15:46:51, Train, Epoch : 10, Step : 5920, Loss : 0.26525, Acc : 0.887, Sensitive_Loss : 0.12939, Sensitive_Acc : 17.100, Run Time : 9.50 sec
INFO:root:2024-04-10 15:47:05, Train, Epoch : 10, Step : 5930, Loss : 0.27691, Acc : 0.891, Sensitive_Loss : 0.08886, Sensitive_Acc : 16.900, Run Time : 13.67 sec
INFO:root:2024-04-10 15:47:14, Train, Epoch : 10, Step : 5940, Loss : 0.31302, Acc : 0.853, Sensitive_Loss : 0.10317, Sensitive_Acc : 16.100, Run Time : 9.19 sec
INFO:root:2024-04-10 15:47:23, Train, Epoch : 10, Step : 5950, Loss : 0.26473, Acc : 0.872, Sensitive_Loss : 0.09110, Sensitive_Acc : 17.400, Run Time : 9.26 sec
INFO:root:2024-04-10 15:47:35, Train, Epoch : 10, Step : 5960, Loss : 0.33351, Acc : 0.859, Sensitive_Loss : 0.12437, Sensitive_Acc : 15.900, Run Time : 11.64 sec
INFO:root:2024-04-10 15:47:46, Train, Epoch : 10, Step : 5970, Loss : 0.26329, Acc : 0.881, Sensitive_Loss : 0.14018, Sensitive_Acc : 15.100, Run Time : 10.66 sec
INFO:root:2024-04-10 15:47:54, Train, Epoch : 10, Step : 5980, Loss : 0.25720, Acc : 0.887, Sensitive_Loss : 0.12856, Sensitive_Acc : 17.500, Run Time : 8.79 sec
INFO:root:2024-04-10 15:48:05, Train, Epoch : 10, Step : 5990, Loss : 0.24375, Acc : 0.891, Sensitive_Loss : 0.09514, Sensitive_Acc : 15.400, Run Time : 10.52 sec
INFO:root:2024-04-10 15:48:16, Train, Epoch : 10, Step : 6000, Loss : 0.32458, Acc : 0.863, Sensitive_Loss : 0.08647, Sensitive_Acc : 16.800, Run Time : 11.35 sec
INFO:root:2024-04-10 15:50:28, Dev, Step : 6000, Loss : 0.56718, Acc : 0.772, Auc : 0.856, Sensitive_Loss : 0.14812, Sensitive_Acc : 16.350, Sensitive_Auc : 0.990, Mean auc: 0.856, Run Time : 131.81 sec
INFO:root:2024-04-10 15:50:35, Train, Epoch : 10, Step : 6010, Loss : 0.30712, Acc : 0.872, Sensitive_Loss : 0.14133, Sensitive_Acc : 18.400, Run Time : 138.89 sec
INFO:root:2024-04-10 15:50:45, Train, Epoch : 10, Step : 6020, Loss : 0.23481, Acc : 0.909, Sensitive_Loss : 0.09292, Sensitive_Acc : 15.800, Run Time : 10.24 sec
INFO:root:2024-04-10 15:50:58, Train, Epoch : 10, Step : 6030, Loss : 0.24468, Acc : 0.900, Sensitive_Loss : 0.10686, Sensitive_Acc : 18.400, Run Time : 12.66 sec
INFO:root:2024-04-10 15:51:10, Train, Epoch : 10, Step : 6040, Loss : 0.32167, Acc : 0.881, Sensitive_Loss : 0.12943, Sensitive_Acc : 16.600, Run Time : 11.88 sec
INFO:root:2024-04-10 15:51:21, Train, Epoch : 10, Step : 6050, Loss : 0.23975, Acc : 0.919, Sensitive_Loss : 0.09729, Sensitive_Acc : 16.600, Run Time : 11.12 sec
INFO:root:2024-04-10 15:51:31, Train, Epoch : 10, Step : 6060, Loss : 0.28385, Acc : 0.881, Sensitive_Loss : 0.12413, Sensitive_Acc : 16.400, Run Time : 9.93 sec
INFO:root:2024-04-10 15:51:40, Train, Epoch : 10, Step : 6070, Loss : 0.29680, Acc : 0.856, Sensitive_Loss : 0.07716, Sensitive_Acc : 14.900, Run Time : 8.85 sec
INFO:root:2024-04-10 15:51:49, Train, Epoch : 10, Step : 6080, Loss : 0.31011, Acc : 0.887, Sensitive_Loss : 0.09488, Sensitive_Acc : 14.900, Run Time : 9.12 sec
INFO:root:2024-04-10 15:51:58, Train, Epoch : 10, Step : 6090, Loss : 0.22726, Acc : 0.891, Sensitive_Loss : 0.12045, Sensitive_Acc : 17.000, Run Time : 8.96 sec
INFO:root:2024-04-10 15:52:08, Train, Epoch : 10, Step : 6100, Loss : 0.30974, Acc : 0.878, Sensitive_Loss : 0.10531, Sensitive_Acc : 16.700, Run Time : 9.89 sec
INFO:root:2024-04-10 15:54:16, Dev, Step : 6100, Loss : 0.56640, Acc : 0.780, Auc : 0.855, Sensitive_Loss : 0.14229, Sensitive_Acc : 16.350, Sensitive_Auc : 0.991, Mean auc: 0.855, Run Time : 128.16 sec
INFO:root:2024-04-10 15:54:24, Train, Epoch : 10, Step : 6110, Loss : 0.27508, Acc : 0.859, Sensitive_Loss : 0.12629, Sensitive_Acc : 17.100, Run Time : 136.35 sec
INFO:root:2024-04-10 15:54:35, Train, Epoch : 10, Step : 6120, Loss : 0.32228, Acc : 0.872, Sensitive_Loss : 0.11560, Sensitive_Acc : 16.400, Run Time : 10.49 sec
INFO:root:2024-04-10 15:54:43, Train, Epoch : 10, Step : 6130, Loss : 0.28489, Acc : 0.884, Sensitive_Loss : 0.11759, Sensitive_Acc : 17.800, Run Time : 8.85 sec
INFO:root:2024-04-10 15:54:52, Train, Epoch : 10, Step : 6140, Loss : 0.28207, Acc : 0.887, Sensitive_Loss : 0.11125, Sensitive_Acc : 15.000, Run Time : 8.81 sec
INFO:root:2024-04-10 15:55:02, Train, Epoch : 10, Step : 6150, Loss : 0.24029, Acc : 0.878, Sensitive_Loss : 0.06899, Sensitive_Acc : 16.400, Run Time : 9.67 sec
INFO:root:2024-04-10 15:55:11, Train, Epoch : 10, Step : 6160, Loss : 0.27071, Acc : 0.872, Sensitive_Loss : 0.08901, Sensitive_Acc : 16.800, Run Time : 9.26 sec
INFO:root:2024-04-10 15:55:22, Train, Epoch : 10, Step : 6170, Loss : 0.30343, Acc : 0.847, Sensitive_Loss : 0.12639, Sensitive_Acc : 16.700, Run Time : 10.74 sec
INFO:root:2024-04-10 15:55:33, Train, Epoch : 10, Step : 6180, Loss : 0.29243, Acc : 0.887, Sensitive_Loss : 0.13896, Sensitive_Acc : 16.800, Run Time : 10.74 sec
INFO:root:2024-04-10 15:55:43, Train, Epoch : 10, Step : 6190, Loss : 0.28336, Acc : 0.869, Sensitive_Loss : 0.10460, Sensitive_Acc : 16.300, Run Time : 10.06 sec
INFO:root:2024-04-10 15:55:52, Train, Epoch : 10, Step : 6200, Loss : 0.27811, Acc : 0.894, Sensitive_Loss : 0.14882, Sensitive_Acc : 15.300, Run Time : 8.83 sec
INFO:root:2024-04-10 15:57:44, Dev, Step : 6200, Loss : 0.59928, Acc : 0.768, Auc : 0.846, Sensitive_Loss : 0.14437, Sensitive_Acc : 16.350, Sensitive_Auc : 0.989, Mean auc: 0.846, Run Time : 112.16 sec
INFO:root:2024-04-10 15:57:51, Train, Epoch : 10, Step : 6210, Loss : 0.31058, Acc : 0.863, Sensitive_Loss : 0.07219, Sensitive_Acc : 16.300, Run Time : 119.44 sec
INFO:root:2024-04-10 15:58:02, Train, Epoch : 10, Step : 6220, Loss : 0.29984, Acc : 0.859, Sensitive_Loss : 0.10668, Sensitive_Acc : 15.400, Run Time : 11.08 sec
INFO:root:2024-04-10 15:58:11, Train, Epoch : 10, Step : 6230, Loss : 0.30006, Acc : 0.856, Sensitive_Loss : 0.10852, Sensitive_Acc : 16.800, Run Time : 8.63 sec
INFO:root:2024-04-10 15:58:21, Train, Epoch : 10, Step : 6240, Loss : 0.34120, Acc : 0.853, Sensitive_Loss : 0.16671, Sensitive_Acc : 16.800, Run Time : 10.38 sec
INFO:root:2024-04-10 15:58:29, Train, Epoch : 10, Step : 6250, Loss : 0.23866, Acc : 0.900, Sensitive_Loss : 0.10648, Sensitive_Acc : 15.800, Run Time : 8.32 sec
INFO:root:2024-04-10 15:58:38, Train, Epoch : 10, Step : 6260, Loss : 0.33294, Acc : 0.844, Sensitive_Loss : 0.07950, Sensitive_Acc : 15.800, Run Time : 9.03 sec
INFO:root:2024-04-10 15:58:46, Train, Epoch : 10, Step : 6270, Loss : 0.27400, Acc : 0.891, Sensitive_Loss : 0.08162, Sensitive_Acc : 17.000, Run Time : 8.04 sec
INFO:root:2024-04-10 15:58:57, Train, Epoch : 10, Step : 6280, Loss : 0.30103, Acc : 0.863, Sensitive_Loss : 0.12247, Sensitive_Acc : 17.100, Run Time : 10.07 sec
INFO:root:2024-04-10 15:59:06, Train, Epoch : 10, Step : 6290, Loss : 0.34017, Acc : 0.863, Sensitive_Loss : 0.09296, Sensitive_Acc : 17.100, Run Time : 9.23 sec
INFO:root:2024-04-10 15:59:15, Train, Epoch : 10, Step : 6300, Loss : 0.28242, Acc : 0.875, Sensitive_Loss : 0.13846, Sensitive_Acc : 15.000, Run Time : 9.10 sec
INFO:root:2024-04-10 16:00:49, Dev, Step : 6300, Loss : 0.56342, Acc : 0.779, Auc : 0.855, Sensitive_Loss : 0.13975, Sensitive_Acc : 16.307, Sensitive_Auc : 0.993, Mean auc: 0.855, Run Time : 93.71 sec
INFO:root:2024-04-10 16:00:57, Train, Epoch : 10, Step : 6310, Loss : 0.28486, Acc : 0.878, Sensitive_Loss : 0.09323, Sensitive_Acc : 16.000, Run Time : 102.08 sec
INFO:root:2024-04-10 16:01:10, Train, Epoch : 10, Step : 6320, Loss : 0.28653, Acc : 0.900, Sensitive_Loss : 0.09897, Sensitive_Acc : 15.500, Run Time : 13.55 sec
INFO:root:2024-04-10 16:01:20, Train, Epoch : 10, Step : 6330, Loss : 0.29902, Acc : 0.878, Sensitive_Loss : 0.13270, Sensitive_Acc : 17.000, Run Time : 9.90 sec
INFO:root:2024-04-10 16:01:29, Train, Epoch : 10, Step : 6340, Loss : 0.26994, Acc : 0.894, Sensitive_Loss : 0.12263, Sensitive_Acc : 16.200, Run Time : 9.03 sec
INFO:root:2024-04-10 16:01:40, Train, Epoch : 10, Step : 6350, Loss : 0.25030, Acc : 0.906, Sensitive_Loss : 0.08873, Sensitive_Acc : 16.600, Run Time : 11.06 sec
INFO:root:2024-04-10 16:01:52, Train, Epoch : 10, Step : 6360, Loss : 0.24003, Acc : 0.900, Sensitive_Loss : 0.10516, Sensitive_Acc : 15.800, Run Time : 11.05 sec
INFO:root:2024-04-10 16:02:02, Train, Epoch : 10, Step : 6370, Loss : 0.25089, Acc : 0.900, Sensitive_Loss : 0.11029, Sensitive_Acc : 17.800, Run Time : 10.08 sec
INFO:root:2024-04-10 16:02:12, Train, Epoch : 10, Step : 6380, Loss : 0.25545, Acc : 0.875, Sensitive_Loss : 0.10621, Sensitive_Acc : 15.900, Run Time : 10.07 sec
INFO:root:2024-04-10 16:02:23, Train, Epoch : 10, Step : 6390, Loss : 0.33816, Acc : 0.841, Sensitive_Loss : 0.10077, Sensitive_Acc : 16.700, Run Time : 11.35 sec
INFO:root:2024-04-10 16:02:32, Train, Epoch : 10, Step : 6400, Loss : 0.29463, Acc : 0.856, Sensitive_Loss : 0.09611, Sensitive_Acc : 15.300, Run Time : 8.65 sec
INFO:root:2024-04-10 16:04:07, Dev, Step : 6400, Loss : 0.58736, Acc : 0.771, Auc : 0.855, Sensitive_Loss : 0.14838, Sensitive_Acc : 16.350, Sensitive_Auc : 0.995, Mean auc: 0.855, Run Time : 95.23 sec
INFO:root:2024-04-10 16:04:13, Train, Epoch : 10, Step : 6410, Loss : 0.26346, Acc : 0.891, Sensitive_Loss : 0.10147, Sensitive_Acc : 14.900, Run Time : 101.66 sec
INFO:root:2024-04-10 16:04:22, Train, Epoch : 10, Step : 6420, Loss : 0.32722, Acc : 0.841, Sensitive_Loss : 0.08938, Sensitive_Acc : 17.600, Run Time : 9.01 sec
INFO:root:2024-04-10 16:04:32, Train, Epoch : 10, Step : 6430, Loss : 0.26588, Acc : 0.909, Sensitive_Loss : 0.11299, Sensitive_Acc : 16.300, Run Time : 9.56 sec
INFO:root:2024-04-10 16:04:43, Train, Epoch : 10, Step : 6440, Loss : 0.23657, Acc : 0.884, Sensitive_Loss : 0.17018, Sensitive_Acc : 16.100, Run Time : 11.30 sec
INFO:root:2024-04-10 16:06:15
INFO:root:y_pred: [0.04017846 0.01692061 0.94768596 ... 0.812783   0.70243686 0.06976341]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9902904e-01 1.1064211e-03 8.3986425e-01 9.9999607e-01 9.9856550e-01
 9.8193133e-01 9.9979252e-01 8.9032772e-05 6.2256527e-01 9.9748129e-01
 5.0650883e-01 2.4423346e-02 8.2486615e-05 9.5221806e-01 9.9999011e-01
 9.9995470e-01 9.9559480e-01 9.9708301e-01 9.9273574e-01 9.9682558e-01
 9.9302411e-01 4.3367785e-01 9.9823833e-01 4.7624642e-01 9.2098480e-01
 5.9316784e-02 9.9809259e-01 3.3481235e-03 9.9906415e-01 2.8409217e-02
 1.0347027e-02 2.8824207e-01 7.3906938e-03 9.9900848e-01 8.6160912e-07
 9.8509467e-01 1.4027317e-04 9.9992418e-01 1.1654856e-02 9.5049793e-01
 9.9597174e-01 2.4929154e-03 1.2894040e-02 6.8471726e-04 4.1229579e-01
 6.4089566e-01 9.9934548e-01 9.9124712e-01 9.7909087e-01 9.9511307e-01
 1.1015250e-03 8.9125985e-01 2.1837146e-03 1.3221709e-01 9.9997139e-01
 4.5565661e-02 9.9532795e-01 9.9930274e-01 9.2365277e-01 5.9644291e-03
 1.3297803e-03 9.9962533e-01 7.8870356e-02 9.9899787e-01 9.4679385e-01
 2.8307772e-01 9.7407871e-01 9.2170733e-01 9.9988699e-01 9.9948275e-01
 4.9619865e-02 5.1094788e-01 9.9160320e-01 9.9952483e-01 9.9295396e-01
 1.0274260e-02 7.3913980e-01 9.5178131e-03 8.9152309e-05 9.9201798e-01
 2.2266049e-02 9.9976510e-01 9.9988437e-01 9.9802375e-01 1.3802802e-02
 9.9999928e-01 2.1320069e-03 2.3369221e-02 9.9517488e-01 9.9691832e-01
 5.0511095e-03 9.0093863e-01 4.1903779e-03 9.6854514e-01 2.6390597e-02
 9.9998283e-01 5.2285786e-03 9.9734384e-01 7.8891343e-01 2.2269215e-03
 2.7497956e-05 1.9585915e-01 9.9612170e-01 9.9902678e-01 9.6452254e-01
 9.1816580e-01 9.7390467e-01 7.5351150e-04 2.7449565e-02 9.9939823e-01
 4.1671810e-06 7.4032217e-04 1.0307330e-01 9.9984694e-01 9.9674159e-01
 3.9267112e-04 9.3803155e-01 1.8385082e-04 9.9474066e-01 9.4354534e-01
 9.9999022e-01 9.9999964e-01 1.5485640e-01 3.9916229e-01 3.9603226e-02
 1.6537923e-02 5.3252801e-03 1.3368661e-03 9.9919611e-01 9.9996459e-01
 8.3868476e-03 1.1072441e-03 2.4516511e-04 6.9919014e-01 9.9753654e-01
 9.9780577e-01 9.9854678e-01 7.3532230e-01 4.6308260e-02 9.4646686e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 16:06:15, Dev, Step : 6440, Loss : 0.60332, Acc : 0.772, Auc : 0.854, Sensitive_Loss : 0.12972, Sensitive_Acc : 16.464, Sensitive_Auc : 0.996, Mean auc: 0.854, Run Time : 92.11 sec
