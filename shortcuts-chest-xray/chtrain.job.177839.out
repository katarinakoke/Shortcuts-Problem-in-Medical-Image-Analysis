Running on desktop25:
stdin: is not a tty
Activating chexpert environment...
0
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/ChestX-ray14/images/",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/chest_drains_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/chest_drains_dataset_val.csv",
    "pred_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Biased_Sex_1_pos01.csv",
    "pred_model": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Biased_Sex_1_pos011.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Drain",
    "lambda_val": -0.1,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-25 12:57:16, Train, Epoch : 1, Step : 10, Loss : 0.75171, Acc : 0.619, Sensitive_Loss : 1.18855, Sensitive_Acc : 9.500, Run Time : 23.31 sec
INFO:root:2024-04-25 12:57:37, Train, Epoch : 1, Step : 20, Loss : 0.65101, Acc : 0.703, Sensitive_Loss : 1.94918, Sensitive_Acc : 17.600, Run Time : 21.52 sec
INFO:root:2024-04-25 12:57:58, Train, Epoch : 1, Step : 30, Loss : 0.63106, Acc : 0.753, Sensitive_Loss : 3.53719, Sensitive_Acc : 15.600, Run Time : 21.06 sec
INFO:root:2024-04-25 12:58:19, Train, Epoch : 1, Step : 40, Loss : 0.63165, Acc : 0.728, Sensitive_Loss : 6.57714, Sensitive_Acc : 10.300, Run Time : 20.36 sec
libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG
INFO:root:2024-04-25 12:58:41, Train, Epoch : 1, Step : 50, Loss : 0.61296, Acc : 0.703, Sensitive_Loss : 6.37986, Sensitive_Acc : 11.500, Run Time : 21.92 sec
INFO:root:2024-04-25 12:59:03, Train, Epoch : 1, Step : 60, Loss : 0.61069, Acc : 0.700, Sensitive_Loss : 5.93371, Sensitive_Acc : 12.200, Run Time : 22.03 sec
INFO:root:2024-04-25 12:59:25, Train, Epoch : 1, Step : 70, Loss : 0.58926, Acc : 0.722, Sensitive_Loss : 4.87588, Sensitive_Acc : 13.100, Run Time : 22.00 sec
INFO:root:2024-04-25 12:59:47, Train, Epoch : 1, Step : 80, Loss : 0.68468, Acc : 0.700, Sensitive_Loss : 5.14702, Sensitive_Acc : 16.100, Run Time : 22.45 sec
INFO:root:2024-04-25 13:00:08, Train, Epoch : 1, Step : 90, Loss : 0.70475, Acc : 0.653, Sensitive_Loss : 6.60384, Sensitive_Acc : 17.800, Run Time : 21.44 sec
INFO:root:2024-04-25 13:00:31, Train, Epoch : 1, Step : 100, Loss : 0.78515, Acc : 0.625, Sensitive_Loss : 6.63660, Sensitive_Acc : 12.300, Run Time : 22.18 sec
INFO:root:2024-04-25 13:01:56, Dev, Step : 100, Loss : 0.64055, Acc : 0.687, Auc : 0.774, Sensitive_Loss : 4.13506, Sensitive_Acc : 11.500, Sensitive_Auc : 0.030, Mean auc: 0.774, Run Time : 85.75 sec
INFO:root:2024-04-25 13:01:57, Best, Step : 100, Loss : 0.64055, Acc : 0.687, Auc : 0.774, Sensitive_Loss : 4.13506, Sensitive_Acc : 11.500, Sensitive_Auc : 0.030, Best Auc : 0.774
INFO:root:2024-04-25 13:02:12, Train, Epoch : 1, Step : 110, Loss : 0.60821, Acc : 0.688, Sensitive_Loss : 4.79551, Sensitive_Acc : 11.000, Run Time : 101.03 sec
INFO:root:2024-04-25 13:02:34, Train, Epoch : 1, Step : 120, Loss : 0.61296, Acc : 0.694, Sensitive_Loss : 5.34119, Sensitive_Acc : 13.000, Run Time : 21.91 sec
INFO:root:2024-04-25 13:02:55, Train, Epoch : 1, Step : 130, Loss : 0.54297, Acc : 0.731, Sensitive_Loss : 5.38959, Sensitive_Acc : 9.400, Run Time : 21.53 sec
INFO:root:2024-04-25 13:03:17, Train, Epoch : 1, Step : 140, Loss : 0.64919, Acc : 0.719, Sensitive_Loss : 5.24246, Sensitive_Acc : 12.500, Run Time : 22.21 sec
INFO:root:2024-04-25 13:03:39, Train, Epoch : 1, Step : 150, Loss : 0.72480, Acc : 0.675, Sensitive_Loss : 5.15929, Sensitive_Acc : 10.700, Run Time : 21.43 sec
INFO:root:2024-04-25 13:04:01, Train, Epoch : 1, Step : 160, Loss : 0.61431, Acc : 0.722, Sensitive_Loss : 4.34525, Sensitive_Acc : 10.900, Run Time : 22.00 sec
INFO:root:2024-04-25 13:04:23, Train, Epoch : 1, Step : 170, Loss : 0.75532, Acc : 0.700, Sensitive_Loss : 3.45093, Sensitive_Acc : 15.100, Run Time : 21.77 sec
INFO:root:2024-04-25 13:04:44, Train, Epoch : 1, Step : 180, Loss : 0.77278, Acc : 0.669, Sensitive_Loss : 3.26358, Sensitive_Acc : 15.100, Run Time : 21.74 sec
INFO:root:2024-04-25 13:06:07
INFO:root:y_pred: [0.10483529 0.04854368 0.5569873  ... 0.9194006  0.8165107  0.10478603]
INFO:root:y_true: [0. 0. 0. ... 1. 1. 0.]
INFO:root:sensitive_y_pred: [0.9242636  0.44395068 0.8700121  0.11179137 0.92550623 0.434922
 0.88422453 0.6827688  0.6303065  0.91114724 0.7889806  0.30625755
 0.7486754  0.9452612  0.12667985 0.12964165 0.9531335  0.77654064
 0.89122206 0.9068142  0.8538895  0.5162752  0.9806333  0.34777838
 0.79165596 0.9217103  0.17880988 0.16878445 0.5184722  0.5493904
 0.9449885  0.6527863  0.937808   0.08339839 0.06838561 0.96466684
 0.86765957 0.43437263]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
INFO:root:2024-04-25 13:06:07, Dev, Step : 184, Loss : 0.68189, Acc : 0.724, Auc : 0.780, Sensitive_Loss : 2.14129, Sensitive_Acc : 11.921, Sensitive_Auc : 0.036, Mean auc: 0.780, Run Time : 74.50 sec
INFO:root:2024-04-25 13:06:08, Best, Step : 184, Loss : 0.68189, Acc : 0.724,Auc : 0.780, Best Auc : 0.780, Sensitive_Loss : 2.14129, Sensitive_Acc : 11.921, Sensitive_Auc : 0.036
INFO:root:2024-04-25 13:06:23, Train, Epoch : 2, Step : 190, Loss : 0.37259, Acc : 0.456, Sensitive_Loss : 1.64263, Sensitive_Acc : 11.600, Run Time : 14.09 sec
INFO:root:2024-04-25 13:06:43, Train, Epoch : 2, Step : 200, Loss : 0.56247, Acc : 0.781, Sensitive_Loss : 2.36317, Sensitive_Acc : 13.200, Run Time : 20.39 sec
INFO:root:2024-04-25 13:08:03, Dev, Step : 200, Loss : 0.65164, Acc : 0.729, Auc : 0.816, Sensitive_Loss : 1.70587, Sensitive_Acc : 7.289, Sensitive_Auc : 0.127, Mean auc: 0.816, Run Time : 80.10 sec
INFO:root:2024-04-25 13:08:04, Best, Step : 200, Loss : 0.65164, Acc : 0.729, Auc : 0.816, Sensitive_Loss : 1.70587, Sensitive_Acc : 7.289, Sensitive_Auc : 0.127, Best Auc : 0.816
libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG
INFO:root:2024-04-25 13:08:19, Train, Epoch : 2, Step : 210, Loss : 0.52031, Acc : 0.778, Sensitive_Loss : 1.71800, Sensitive_Acc : 8.600, Run Time : 95.42 sec
INFO:root:2024-04-25 13:08:40, Train, Epoch : 2, Step : 220, Loss : 0.52348, Acc : 0.784, Sensitive_Loss : 1.38644, Sensitive_Acc : 9.700, Run Time : 21.12 sec
INFO:root:2024-04-25 13:09:00, Train, Epoch : 2, Step : 230, Loss : 0.52858, Acc : 0.753, Sensitive_Loss : 1.44518, Sensitive_Acc : 13.000, Run Time : 20.24 sec
INFO:root:2024-04-25 13:09:21, Train, Epoch : 2, Step : 240, Loss : 0.67762, Acc : 0.756, Sensitive_Loss : 1.28350, Sensitive_Acc : 9.000, Run Time : 20.79 sec
INFO:root:2024-04-25 13:09:43, Train, Epoch : 2, Step : 250, Loss : 0.50287, Acc : 0.797, Sensitive_Loss : 0.97681, Sensitive_Acc : 9.400, Run Time : 21.98 sec
INFO:root:2024-04-25 13:10:03, Train, Epoch : 2, Step : 260, Loss : 0.65371, Acc : 0.719, Sensitive_Loss : 0.85006, Sensitive_Acc : 10.100, Run Time : 19.99 sec
INFO:root:2024-04-25 13:10:23, Train, Epoch : 2, Step : 270, Loss : 0.53417, Acc : 0.772, Sensitive_Loss : 0.86208, Sensitive_Acc : 21.200, Run Time : 19.81 sec
INFO:root:2024-04-25 13:10:44, Train, Epoch : 2, Step : 280, Loss : 0.53048, Acc : 0.762, Sensitive_Loss : 0.75591, Sensitive_Acc : 23.800, Run Time : 21.25 sec
INFO:root:2024-04-25 13:11:05, Train, Epoch : 2, Step : 290, Loss : 0.45526, Acc : 0.797, Sensitive_Loss : 0.68755, Sensitive_Acc : 19.900, Run Time : 20.78 sec
INFO:root:2024-04-25 13:11:25, Train, Epoch : 2, Step : 300, Loss : 0.51843, Acc : 0.775, Sensitive_Loss : 0.71876, Sensitive_Acc : 20.600, Run Time : 20.12 sec
INFO:root:2024-04-25 13:12:42, Dev, Step : 300, Loss : 0.70538, Acc : 0.758, Auc : 0.809, Sensitive_Loss : 0.60374, Sensitive_Acc : 23.237, Sensitive_Auc : 0.976, Mean auc: 0.809, Run Time : 77.53 sec
INFO:root:2024-04-25 13:12:57, Train, Epoch : 2, Step : 310, Loss : 0.50239, Acc : 0.809, Sensitive_Loss : 0.67696, Sensitive_Acc : 19.300, Run Time : 92.07 sec
INFO:root:2024-04-25 13:13:18, Train, Epoch : 2, Step : 320, Loss : 0.54681, Acc : 0.775, Sensitive_Loss : 0.73825, Sensitive_Acc : 22.700, Run Time : 20.71 sec
INFO:root:2024-04-25 13:13:38, Train, Epoch : 2, Step : 330, Loss : 0.59315, Acc : 0.738, Sensitive_Loss : 0.67493, Sensitive_Acc : 17.600, Run Time : 20.46 sec
INFO:root:2024-04-25 13:13:57, Train, Epoch : 2, Step : 340, Loss : 0.52874, Acc : 0.753, Sensitive_Loss : 0.67279, Sensitive_Acc : 20.200, Run Time : 18.83 sec
INFO:root:2024-04-25 13:14:20, Train, Epoch : 2, Step : 350, Loss : 0.65853, Acc : 0.741, Sensitive_Loss : 0.66231, Sensitive_Acc : 21.900, Run Time : 23.11 sec
INFO:root:2024-04-25 13:14:41, Train, Epoch : 2, Step : 360, Loss : 0.61041, Acc : 0.766, Sensitive_Loss : 0.62422, Sensitive_Acc : 21.900, Run Time : 20.86 sec
INFO:root:2024-04-25 13:16:11
INFO:root:y_pred: [0.0907357  0.08047332 0.4985617  ... 0.5583503  0.47491547 0.15425603]
INFO:root:y_true: [0. 0. 0. ... 1. 1. 0.]
INFO:root:sensitive_y_pred: [0.2561904  0.36700004 0.2710264  0.35371518 0.3498897  0.37341496
 0.27354705 0.31776756 0.2590938  0.23796989 0.23298013 0.3204703
 0.30084467 0.37138084 0.31614995 0.3777024  0.29448113 0.30090982
 0.28324655 0.2711566  0.23766504 0.40736142 0.27455512 0.665791
 0.28616002 0.25729722 0.29046634 0.4446908  0.21611416 0.44509014
 0.24891418 0.38252744 0.3047893  0.7174075  0.83133984 0.2786314
 0.29850927 0.27907786]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
INFO:root:2024-04-25 13:16:11, Dev, Step : 368, Loss : 0.56637, Acc : 0.772, Auc : 0.849, Sensitive_Loss : 0.68498, Sensitive_Acc : 25.395, Sensitive_Auc : 0.909, Mean auc: 0.849, Run Time : 74.43 sec
INFO:root:2024-04-25 13:16:11, Best, Step : 368, Loss : 0.56637, Acc : 0.772,Auc : 0.849, Best Auc : 0.849, Sensitive_Loss : 0.68498, Sensitive_Acc : 25.395, Sensitive_Auc : 0.909
INFO:root:2024-04-25 13:16:18, Train, Epoch : 3, Step : 370, Loss : 0.11746, Acc : 0.147, Sensitive_Loss : 0.13569, Sensitive_Acc : 5.000, Run Time : 5.84 sec
INFO:root:2024-04-25 13:16:39, Train, Epoch : 3, Step : 380, Loss : 0.50813, Acc : 0.794, Sensitive_Loss : 0.64145, Sensitive_Acc : 17.400, Run Time : 20.28 sec
INFO:root:2024-04-25 13:16:59, Train, Epoch : 3, Step : 390, Loss : 0.48105, Acc : 0.784, Sensitive_Loss : 0.69852, Sensitive_Acc : 19.100, Run Time : 20.50 sec
INFO:root:2024-04-25 13:17:21, Train, Epoch : 3, Step : 400, Loss : 0.46361, Acc : 0.816, Sensitive_Loss : 0.66185, Sensitive_Acc : 23.800, Run Time : 22.06 sec
INFO:root:2024-04-25 13:18:40, Dev, Step : 400, Loss : 0.57441, Acc : 0.779, Auc : 0.852, Sensitive_Loss : 0.64245, Sensitive_Acc : 25.395, Sensitive_Auc : 0.861, Mean auc: 0.852, Run Time : 78.92 sec
INFO:root:2024-04-25 13:18:41, Best, Step : 400, Loss : 0.57441, Acc : 0.779, Auc : 0.852, Sensitive_Loss : 0.64245, Sensitive_Acc : 25.395, Sensitive_Auc : 0.861, Best Auc : 0.852
INFO:root:2024-04-25 13:18:55, Train, Epoch : 3, Step : 410, Loss : 0.50489, Acc : 0.784, Sensitive_Loss : 0.65077, Sensitive_Acc : 23.300, Run Time : 93.74 sec
INFO:root:2024-04-25 13:19:15, Train, Epoch : 3, Step : 420, Loss : 0.50109, Acc : 0.819, Sensitive_Loss : 0.73103, Sensitive_Acc : 23.200, Run Time : 20.04 sec
INFO:root:2024-04-25 13:19:36, Train, Epoch : 3, Step : 430, Loss : 0.44966, Acc : 0.803, Sensitive_Loss : 0.74258, Sensitive_Acc : 21.300, Run Time : 20.61 sec
INFO:root:2024-04-25 13:19:56, Train, Epoch : 3, Step : 440, Loss : 0.58087, Acc : 0.769, Sensitive_Loss : 0.70235, Sensitive_Acc : 20.800, Run Time : 20.15 sec
INFO:root:2024-04-25 13:20:17, Train, Epoch : 3, Step : 450, Loss : 0.46638, Acc : 0.834, Sensitive_Loss : 0.67125, Sensitive_Acc : 25.000, Run Time : 21.70 sec
INFO:root:2024-04-25 13:20:39, Train, Epoch : 3, Step : 460, Loss : 0.46656, Acc : 0.816, Sensitive_Loss : 0.62438, Sensitive_Acc : 19.500, Run Time : 21.18 sec
INFO:root:2024-04-25 13:20:59, Train, Epoch : 3, Step : 470, Loss : 0.39839, Acc : 0.847, Sensitive_Loss : 0.64943, Sensitive_Acc : 22.600, Run Time : 20.39 sec
INFO:root:2024-04-25 13:21:19, Train, Epoch : 3, Step : 480, Loss : 0.46678, Acc : 0.812, Sensitive_Loss : 0.69239, Sensitive_Acc : 20.100, Run Time : 19.69 sec
INFO:root:2024-04-25 13:21:40, Train, Epoch : 3, Step : 490, Loss : 0.46741, Acc : 0.816, Sensitive_Loss : 0.65817, Sensitive_Acc : 24.100, Run Time : 21.06 sec
INFO:root:2024-04-25 13:22:00, Train, Epoch : 3, Step : 500, Loss : 0.46366, Acc : 0.787, Sensitive_Loss : 0.65873, Sensitive_Acc : 20.700, Run Time : 19.69 sec
INFO:root:2024-04-25 13:23:19, Dev, Step : 500, Loss : 0.57934, Acc : 0.784, Auc : 0.849, Sensitive_Loss : 0.71013, Sensitive_Acc : 25.395, Sensitive_Auc : 0.733, Mean auc: 0.849, Run Time : 79.86 sec
INFO:root:2024-04-25 13:23:33, Train, Epoch : 3, Step : 510, Loss : 0.46568, Acc : 0.816, Sensitive_Loss : 0.68580, Sensitive_Acc : 19.500, Run Time : 93.52 sec
INFO:root:2024-04-25 13:23:55, Train, Epoch : 3, Step : 520, Loss : 0.42011, Acc : 0.819, Sensitive_Loss : 0.76443, Sensitive_Acc : 23.500, Run Time : 21.75 sec
INFO:root:2024-04-25 13:24:14, Train, Epoch : 3, Step : 530, Loss : 0.41897, Acc : 0.841, Sensitive_Loss : 0.70651, Sensitive_Acc : 22.500, Run Time : 19.57 sec
INFO:root:2024-04-25 13:24:36, Train, Epoch : 3, Step : 540, Loss : 0.41485, Acc : 0.838, Sensitive_Loss : 0.75825, Sensitive_Acc : 23.300, Run Time : 22.08 sec
libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG
INFO:root:2024-04-25 13:24:57, Train, Epoch : 3, Step : 550, Loss : 0.46531, Acc : 0.809, Sensitive_Loss : 0.76646, Sensitive_Acc : 18.400, Run Time : 20.15 sec
INFO:root:2024-04-25 13:26:15
INFO:root:y_pred: [0.03236671 0.02744285 0.35263896 ... 0.41194957 0.23794715 0.06067744]
INFO:root:y_true: [0. 0. 0. ... 1. 1. 0.]
INFO:root:sensitive_y_pred: [0.23411249 0.27815938 0.27647585 0.3340789  0.37439245 0.39155635
 0.33811378 0.3193451  0.38191354 0.25401658 0.26243764 0.27401608
 0.29489687 0.40253592 0.24437666 0.21228878 0.2879328  0.28087118
 0.27415392 0.2877119  0.27115947 0.30879992 0.24591433 0.6714318
 0.26379383 0.2723666  0.3204224  0.48286894 0.17883578 0.3623181
 0.23505242 0.37096986 0.2722467  0.9115379  0.95034176 0.25734642
 0.26878217 0.23875551]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
INFO:root:2024-04-25 13:26:15, Dev, Step : 552, Loss : 0.59360, Acc : 0.780, Auc : 0.850, Sensitive_Loss : 0.69910, Sensitive_Acc : 25.395, Sensitive_Auc : 0.630, Mean auc: 0.850, Run Time : 76.19 sec
INFO:root:2024-04-25 13:26:34, Train, Epoch : 4, Step : 560, Loss : 0.35863, Acc : 0.678, Sensitive_Loss : 0.56381, Sensitive_Acc : 14.900, Run Time : 17.54 sec
INFO:root:2024-04-25 13:26:55, Train, Epoch : 4, Step : 570, Loss : 0.38579, Acc : 0.831, Sensitive_Loss : 0.77718, Sensitive_Acc : 22.700, Run Time : 20.76 sec
INFO:root:2024-04-25 13:27:15, Train, Epoch : 4, Step : 580, Loss : 0.45344, Acc : 0.806, Sensitive_Loss : 0.75133, Sensitive_Acc : 22.300, Run Time : 20.08 sec
INFO:root:2024-04-25 13:27:36, Train, Epoch : 4, Step : 590, Loss : 0.43232, Acc : 0.844, Sensitive_Loss : 0.76764, Sensitive_Acc : 22.400, Run Time : 21.15 sec
INFO:root:2024-04-25 13:27:56, Train, Epoch : 4, Step : 600, Loss : 0.50132, Acc : 0.800, Sensitive_Loss : 0.72652, Sensitive_Acc : 23.000, Run Time : 19.95 sec
INFO:root:2024-04-25 13:29:16, Dev, Step : 600, Loss : 0.59931, Acc : 0.782, Auc : 0.851, Sensitive_Loss : 0.78373, Sensitive_Acc : 25.395, Sensitive_Auc : 0.618, Mean auc: 0.851, Run Time : 80.25 sec
INFO:root:2024-04-25 13:29:31, Train, Epoch : 4, Step : 610, Loss : 0.40940, Acc : 0.831, Sensitive_Loss : 0.79682, Sensitive_Acc : 24.500, Run Time : 94.85 sec
INFO:root:2024-04-25 13:29:50, Train, Epoch : 4, Step : 620, Loss : 0.46105, Acc : 0.816, Sensitive_Loss : 0.77011, Sensitive_Acc : 24.500, Run Time : 19.76 sec
INFO:root:2024-04-25 13:30:11, Train, Epoch : 4, Step : 630, Loss : 0.38590, Acc : 0.844, Sensitive_Loss : 0.81953, Sensitive_Acc : 20.600, Run Time : 20.76 sec
INFO:root:2024-04-25 13:30:33, Train, Epoch : 4, Step : 640, Loss : 0.47937, Acc : 0.838, Sensitive_Loss : 0.79441, Sensitive_Acc : 23.400, Run Time : 21.98 sec
libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG
INFO:root:2024-04-25 13:30:54, Train, Epoch : 4, Step : 650, Loss : 0.45887, Acc : 0.816, Sensitive_Loss : 0.81337, Sensitive_Acc : 16.500, Run Time : 20.42 sec
INFO:root:2024-04-25 13:31:15, Train, Epoch : 4, Step : 660, Loss : 0.37122, Acc : 0.878, Sensitive_Loss : 0.81459, Sensitive_Acc : 18.500, Run Time : 21.53 sec
INFO:root:2024-04-25 13:31:35, Train, Epoch : 4, Step : 670, Loss : 0.42477, Acc : 0.834, Sensitive_Loss : 0.85038, Sensitive_Acc : 22.600, Run Time : 20.24 sec
INFO:root:2024-04-25 13:31:56, Train, Epoch : 4, Step : 680, Loss : 0.41005, Acc : 0.850, Sensitive_Loss : 0.86372, Sensitive_Acc : 22.400, Run Time : 20.70 sec
INFO:root:2024-04-25 13:32:16, Train, Epoch : 4, Step : 690, Loss : 0.47817, Acc : 0.819, Sensitive_Loss : 0.89002, Sensitive_Acc : 21.600, Run Time : 20.13 sec
INFO:root:2024-04-25 13:32:36, Train, Epoch : 4, Step : 700, Loss : 0.50587, Acc : 0.778, Sensitive_Loss : 0.82615, Sensitive_Acc : 19.400, Run Time : 19.88 sec
INFO:root:2024-04-25 13:33:55, Dev, Step : 700, Loss : 0.57288, Acc : 0.784, Auc : 0.846, Sensitive_Loss : 0.86292, Sensitive_Acc : 24.079, Sensitive_Auc : 0.606, Mean auc: 0.846, Run Time : 79.05 sec
INFO:root:2024-04-25 13:34:09, Train, Epoch : 4, Step : 710, Loss : 0.42513, Acc : 0.844, Sensitive_Loss : 0.94296, Sensitive_Acc : 22.600, Run Time : 93.26 sec
INFO:root:2024-04-25 13:34:30, Train, Epoch : 4, Step : 720, Loss : 0.37112, Acc : 0.856, Sensitive_Loss : 0.96532, Sensitive_Acc : 19.600, Run Time : 20.70 sec
INFO:root:2024-04-25 13:34:49, Train, Epoch : 4, Step : 730, Loss : 0.42144, Acc : 0.850, Sensitive_Loss : 0.93550, Sensitive_Acc : 21.100, Run Time : 19.50 sec
INFO:root:2024-04-25 13:36:16
INFO:root:y_pred: [0.05366056 0.03560549 0.34801286 ... 0.58113647 0.2885741  0.15496951]
INFO:root:y_true: [0. 0. 0. ... 1. 1. 0.]
INFO:root:sensitive_y_pred: [0.3613191  0.28622892 0.39724806 0.22956942 0.49749574 0.523247
 0.48701826 0.42782134 0.57718945 0.39102566 0.40889737 0.28933218
 0.35932755 0.5977428  0.19445674 0.11478677 0.3851987  0.3740973
 0.32731134 0.45893165 0.3399605  0.3397767  0.2968293  0.6572946
 0.38859645 0.38947144 0.23657618 0.43901873 0.19713527 0.44216287
 0.348122   0.52045196 0.34152406 0.7805278  0.943856   0.33238828
 0.36741915 0.2514673 ]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
INFO:root:2024-04-25 13:36:16, Dev, Step : 736, Loss : 0.56776, Acc : 0.786, Auc : 0.843, Sensitive_Loss : 0.90288, Sensitive_Acc : 23.500, Sensitive_Auc : 0.600, Mean auc: 0.843, Run Time : 74.27 sec
INFO:root:2024-04-25 13:36:27, Train, Epoch : 5, Step : 740, Loss : 0.17684, Acc : 0.328, Sensitive_Loss : 0.43019, Sensitive_Acc : 9.600, Run Time : 9.71 sec
INFO:root:2024-04-25 13:36:48, Train, Epoch : 5, Step : 750, Loss : 0.49117, Acc : 0.809, Sensitive_Loss : 1.02507, Sensitive_Acc : 19.900, Run Time : 21.29 sec
INFO:root:2024-04-25 13:37:09, Train, Epoch : 5, Step : 760, Loss : 0.49398, Acc : 0.809, Sensitive_Loss : 0.92945, Sensitive_Acc : 21.200, Run Time : 21.53 sec
INFO:root:2024-04-25 13:37:30, Train, Epoch : 5, Step : 770, Loss : 0.37091, Acc : 0.853, Sensitive_Loss : 1.03421, Sensitive_Acc : 17.600, Run Time : 20.75 sec
INFO:root:2024-04-25 13:37:50, Train, Epoch : 5, Step : 780, Loss : 0.50387, Acc : 0.784, Sensitive_Loss : 0.85672, Sensitive_Acc : 19.400, Run Time : 19.87 sec
INFO:root:2024-04-25 13:38:10, Train, Epoch : 5, Step : 790, Loss : 0.36612, Acc : 0.859, Sensitive_Loss : 0.89349, Sensitive_Acc : 19.600, Run Time : 20.11 sec
INFO:root:2024-04-25 13:38:30, Train, Epoch : 5, Step : 800, Loss : 0.42615, Acc : 0.806, Sensitive_Loss : 0.95521, Sensitive_Acc : 13.200, Run Time : 20.19 sec
INFO:root:2024-04-25 13:39:50, Dev, Step : 800, Loss : 0.59314, Acc : 0.775, Auc : 0.840, Sensitive_Loss : 0.97692, Sensitive_Acc : 22.395, Sensitive_Auc : 0.564, Mean auc: 0.840, Run Time : 79.28 sec
INFO:root:2024-04-25 13:40:04, Train, Epoch : 5, Step : 810, Loss : 0.49463, Acc : 0.784, Sensitive_Loss : 1.14132, Sensitive_Acc : 15.600, Run Time : 93.26 sec
INFO:root:2024-04-25 13:40:24, Train, Epoch : 5, Step : 820, Loss : 0.43491, Acc : 0.819, Sensitive_Loss : 1.02122, Sensitive_Acc : 15.100, Run Time : 20.57 sec
INFO:root:2024-04-25 13:40:45, Train, Epoch : 5, Step : 830, Loss : 0.47004, Acc : 0.828, Sensitive_Loss : 1.27062, Sensitive_Acc : 11.200, Run Time : 20.58 sec
INFO:root:2024-04-25 13:41:05, Train, Epoch : 5, Step : 840, Loss : 0.49670, Acc : 0.800, Sensitive_Loss : 1.16130, Sensitive_Acc : 16.500, Run Time : 20.49 sec
libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG
INFO:root:2024-04-25 13:41:27, Train, Epoch : 5, Step : 850, Loss : 0.45240, Acc : 0.825, Sensitive_Loss : 1.07908, Sensitive_Acc : 16.500, Run Time : 21.94 sec
INFO:root:2024-04-25 13:41:49, Train, Epoch : 5, Step : 860, Loss : 0.42166, Acc : 0.809, Sensitive_Loss : 1.18672, Sensitive_Acc : 10.600, Run Time : 21.97 sec
INFO:root:2024-04-25 13:42:10, Train, Epoch : 5, Step : 870, Loss : 0.48936, Acc : 0.797, Sensitive_Loss : 1.33804, Sensitive_Acc : 13.900, Run Time : 20.89 sec
INFO:root:2024-04-25 13:42:32, Train, Epoch : 5, Step : 880, Loss : 0.38729, Acc : 0.853, Sensitive_Loss : 1.46700, Sensitive_Acc : 17.100, Run Time : 22.32 sec
INFO:root:2024-04-25 13:42:53, Train, Epoch : 5, Step : 890, Loss : 0.54185, Acc : 0.781, Sensitive_Loss : 1.65979, Sensitive_Acc : 10.300, Run Time : 20.53 sec
INFO:root:2024-04-25 13:43:13, Train, Epoch : 5, Step : 900, Loss : 0.42653, Acc : 0.784, Sensitive_Loss : 1.87578, Sensitive_Acc : 14.600, Run Time : 20.35 sec
INFO:root:2024-04-25 13:44:31, Dev, Step : 900, Loss : 0.60505, Acc : 0.769, Auc : 0.829, Sensitive_Loss : 1.58583, Sensitive_Acc : 13.289, Sensitive_Auc : 0.030, Mean auc: 0.829, Run Time : 77.85 sec
INFO:root:2024-04-25 13:44:45, Train, Epoch : 5, Step : 910, Loss : 0.51831, Acc : 0.806, Sensitive_Loss : 1.94560, Sensitive_Acc : 14.400, Run Time : 91.89 sec
INFO:root:2024-04-25 13:45:05, Train, Epoch : 5, Step : 920, Loss : 0.54042, Acc : 0.787, Sensitive_Loss : 1.76605, Sensitive_Acc : 15.400, Run Time : 19.57 sec
INFO:root:2024-04-25 13:46:19
INFO:root:y_pred: [0.18432578 0.18803988 0.46659908 ... 0.6713362  0.70180446 0.32375947]
INFO:root:y_true: [0. 0. 0. ... 1. 1. 0.]
INFO:root:sensitive_y_pred: [0.8100746  0.38405222 0.75062263 0.5313458  0.84943277 0.88160765
 0.8686067  0.7453942  0.8434658  0.83217615 0.86765265 0.4739575
 0.72167766 0.8947545  0.23658079 0.18273324 0.8247368  0.79561365
 0.6651566  0.87934035 0.7476534  0.318967   0.6751348  0.4473949
 0.8770765  0.78651595 0.29659194 0.55471814 0.56226474 0.53161913
 0.7987975  0.77472734 0.6570372  0.2815105  0.50277174 0.6895966
 0.69273734 0.5036304 ]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
INFO:root:2024-04-25 13:46:19, Dev, Step : 920, Loss : 0.57888, Acc : 0.713, Auc : 0.830, Sensitive_Loss : 1.41298, Sensitive_Acc : 8.868, Sensitive_Auc : 0.042, Mean auc: 0.830, Run Time : 74.14 sec
INFO:root:2024-04-25 13:46:43, Train, Epoch : 6, Step : 930, Loss : 0.49976, Acc : 0.794, Sensitive_Loss : 1.53889, Sensitive_Acc : 15.400, Run Time : 22.55 sec
INFO:root:2024-04-25 13:47:04, Train, Epoch : 6, Step : 940, Loss : 0.45906, Acc : 0.831, Sensitive_Loss : 1.42697, Sensitive_Acc : 10.600, Run Time : 21.15 sec
INFO:root:2024-04-25 13:47:24, Train, Epoch : 6, Step : 950, Loss : 0.50453, Acc : 0.809, Sensitive_Loss : 1.30927, Sensitive_Acc : 9.000, Run Time : 20.54 sec
INFO:root:2024-04-25 13:47:45, Train, Epoch : 6, Step : 960, Loss : 0.51239, Acc : 0.766, Sensitive_Loss : 1.33517, Sensitive_Acc : 8.000, Run Time : 20.98 sec
INFO:root:2024-04-25 13:48:06, Train, Epoch : 6, Step : 970, Loss : 0.49667, Acc : 0.766, Sensitive_Loss : 1.27538, Sensitive_Acc : 14.700, Run Time : 20.40 sec
INFO:root:2024-04-25 13:48:25, Train, Epoch : 6, Step : 980, Loss : 0.50349, Acc : 0.812, Sensitive_Loss : 1.33365, Sensitive_Acc : 8.800, Run Time : 19.48 sec
INFO:root:2024-04-25 13:48:45, Train, Epoch : 6, Step : 990, Loss : 0.40341, Acc : 0.831, Sensitive_Loss : 1.25983, Sensitive_Acc : 10.200, Run Time : 19.92 sec
INFO:root:2024-04-25 13:49:06, Train, Epoch : 6, Step : 1000, Loss : 0.41810, Acc : 0.847, Sensitive_Loss : 1.25685, Sensitive_Acc : 10.900, Run Time : 20.69 sec
INFO:root:2024-04-25 13:50:26, Dev, Step : 1000, Loss : 0.58052, Acc : 0.761, Auc : 0.825, Sensitive_Loss : 1.19454, Sensitive_Acc : 8.974, Sensitive_Auc : 0.424, Mean auc: 0.825, Run Time : 79.97 sec
libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG
INFO:root:2024-04-25 13:50:40, Train, Epoch : 6, Step : 1010, Loss : 0.46261, Acc : 0.825, Sensitive_Loss : 1.15390, Sensitive_Acc : 11.900, Run Time : 93.94 sec
INFO:root:2024-04-25 13:51:01, Train, Epoch : 6, Step : 1020, Loss : 0.44395, Acc : 0.844, Sensitive_Loss : 1.20773, Sensitive_Acc : 9.900, Run Time : 21.43 sec
INFO:root:2024-04-25 13:51:24, Train, Epoch : 6, Step : 1030, Loss : 0.44545, Acc : 0.853, Sensitive_Loss : 1.23961, Sensitive_Acc : 8.900, Run Time : 23.21 sec
INFO:root:2024-04-25 13:51:45, Train, Epoch : 6, Step : 1040, Loss : 0.47679, Acc : 0.853, Sensitive_Loss : 1.12885, Sensitive_Acc : 11.600, Run Time : 20.68 sec
INFO:root:2024-04-25 13:52:04, Train, Epoch : 6, Step : 1050, Loss : 0.46152, Acc : 0.825, Sensitive_Loss : 1.25908, Sensitive_Acc : 13.300, Run Time : 19.31 sec
INFO:root:2024-04-25 13:52:26, Train, Epoch : 6, Step : 1060, Loss : 0.40773, Acc : 0.859, Sensitive_Loss : 1.13013, Sensitive_Acc : 10.600, Run Time : 21.18 sec
INFO:root:2024-04-25 13:52:45, Train, Epoch : 6, Step : 1070, Loss : 0.48666, Acc : 0.803, Sensitive_Loss : 1.14923, Sensitive_Acc : 8.700, Run Time : 19.52 sec
INFO:root:2024-04-25 13:53:06, Train, Epoch : 6, Step : 1080, Loss : 0.45526, Acc : 0.806, Sensitive_Loss : 1.11051, Sensitive_Acc : 9.600, Run Time : 20.42 sec
INFO:root:2024-04-25 13:53:26, Train, Epoch : 6, Step : 1090, Loss : 0.47767, Acc : 0.803, Sensitive_Loss : 1.06492, Sensitive_Acc : 10.000, Run Time : 20.43 sec
INFO:root:2024-04-25 13:53:46, Train, Epoch : 6, Step : 1100, Loss : 0.44949, Acc : 0.847, Sensitive_Loss : 1.03293, Sensitive_Acc : 12.800, Run Time : 20.40 sec
INFO:root:2024-04-25 13:55:07, Dev, Step : 1100, Loss : 0.56310, Acc : 0.788, Auc : 0.846, Sensitive_Loss : 0.97021, Sensitive_Acc : 9.237, Sensitive_Auc : 0.594, Mean auc: 0.846, Run Time : 80.55 sec
INFO:root:2024-04-25 13:56:23
INFO:root:y_pred: [0.03040868 0.03909477 0.36580953 ... 0.39623162 0.40409967 0.07442462]
INFO:root:y_true: [0. 0. 0. ... 1. 1. 0.]
INFO:root:sensitive_y_pred: [0.6333767  0.3584322  0.767911   0.886836   0.79735196 0.65833515
 0.82202023 0.7080007  0.5136728  0.7677796  0.78410035 0.3744148
 0.71223533 0.84830916 0.39515138 0.42257726 0.71526575 0.6470538
 0.6265614  0.7037415  0.68412286 0.64832455 0.64123076 0.9724025
 0.69666404 0.6713989  0.34370887 0.6754129  0.37196603 0.7161189
 0.69404876 0.83832693 0.67526966 0.77881265 0.8861431  0.6708105
 0.7374772  0.6127803 ]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
INFO:root:2024-04-25 13:56:23, Dev, Step : 1104, Loss : 0.56560, Acc : 0.790, Auc : 0.848, Sensitive_Loss : 0.93916, Sensitive_Acc : 9.237, Sensitive_Auc : 0.606, Mean auc: 0.848, Run Time : 74.08 sec
INFO:root:2024-04-25 13:56:38, Train, Epoch : 7, Step : 1110, Loss : 0.26389, Acc : 0.487, Sensitive_Loss : 0.59321, Sensitive_Acc : 5.300, Run Time : 13.59 sec
libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG
INFO:root:2024-04-25 13:56:58, Train, Epoch : 7, Step : 1120, Loss : 0.37474, Acc : 0.878, Sensitive_Loss : 1.00648, Sensitive_Acc : 8.900, Run Time : 20.42 sec
INFO:root:2024-04-25 13:57:19, Train, Epoch : 7, Step : 1130, Loss : 0.44290, Acc : 0.819, Sensitive_Loss : 1.06787, Sensitive_Acc : 15.200, Run Time : 21.17 sec
INFO:root:2024-04-25 13:57:39, Train, Epoch : 7, Step : 1140, Loss : 0.42752, Acc : 0.844, Sensitive_Loss : 1.09326, Sensitive_Acc : 8.900, Run Time : 19.72 sec
INFO:root:2024-04-25 13:58:01, Train, Epoch : 7, Step : 1150, Loss : 0.48311, Acc : 0.812, Sensitive_Loss : 1.06016, Sensitive_Acc : 9.800, Run Time : 21.42 sec
INFO:root:2024-04-25 13:58:21, Train, Epoch : 7, Step : 1160, Loss : 0.43530, Acc : 0.853, Sensitive_Loss : 1.06237, Sensitive_Acc : 12.000, Run Time : 20.65 sec
INFO:root:2024-04-25 13:58:43, Train, Epoch : 7, Step : 1170, Loss : 0.37898, Acc : 0.863, Sensitive_Loss : 0.99343, Sensitive_Acc : 11.600, Run Time : 21.38 sec
INFO:root:2024-04-25 13:59:03, Train, Epoch : 7, Step : 1180, Loss : 0.46576, Acc : 0.850, Sensitive_Loss : 1.01025, Sensitive_Acc : 7.200, Run Time : 20.42 sec
INFO:root:2024-04-25 13:59:23, Train, Epoch : 7, Step : 1190, Loss : 0.47814, Acc : 0.841, Sensitive_Loss : 1.03918, Sensitive_Acc : 8.300, Run Time : 19.65 sec
INFO:root:2024-04-25 13:59:43, Train, Epoch : 7, Step : 1200, Loss : 0.40953, Acc : 0.841, Sensitive_Loss : 1.05043, Sensitive_Acc : 13.400, Run Time : 20.13 sec
INFO:root:2024-04-25 14:01:04, Dev, Step : 1200, Loss : 0.59259, Acc : 0.782, Auc : 0.844, Sensitive_Loss : 0.89012, Sensitive_Acc : 9.974, Sensitive_Auc : 0.436, Mean auc: 0.844, Run Time : 80.93 sec
INFO:root:2024-04-25 14:01:17, Train, Epoch : 7, Step : 1210, Loss : 0.43520, Acc : 0.838, Sensitive_Loss : 1.08460, Sensitive_Acc : 13.000, Run Time : 94.63 sec
INFO:root:2024-04-25 14:01:38, Train, Epoch : 7, Step : 1220, Loss : 0.41150, Acc : 0.834, Sensitive_Loss : 1.08119, Sensitive_Acc : 11.100, Run Time : 20.88 sec
INFO:root:2024-04-25 14:01:59, Train, Epoch : 7, Step : 1230, Loss : 0.44401, Acc : 0.850, Sensitive_Loss : 1.07522, Sensitive_Acc : 11.300, Run Time : 20.33 sec
INFO:root:2024-04-25 14:02:19, Train, Epoch : 7, Step : 1240, Loss : 0.41871, Acc : 0.847, Sensitive_Loss : 0.96470, Sensitive_Acc : 12.300, Run Time : 20.25 sec
INFO:root:2024-04-25 14:02:40, Train, Epoch : 7, Step : 1250, Loss : 0.38446, Acc : 0.844, Sensitive_Loss : 0.96522, Sensitive_Acc : 15.000, Run Time : 20.86 sec
INFO:root:2024-04-25 14:03:00, Train, Epoch : 7, Step : 1260, Loss : 0.43158, Acc : 0.838, Sensitive_Loss : 1.07105, Sensitive_Acc : 9.500, Run Time : 20.37 sec
INFO:root:2024-04-25 14:03:20, Train, Epoch : 7, Step : 1270, Loss : 0.39127, Acc : 0.831, Sensitive_Loss : 1.01699, Sensitive_Acc : 11.400, Run Time : 20.10 sec
INFO:root:2024-04-25 14:03:40, Train, Epoch : 7, Step : 1280, Loss : 0.43651, Acc : 0.844, Sensitive_Loss : 1.00758, Sensitive_Acc : 9.900, Run Time : 19.35 sec
INFO:root:2024-04-25 14:05:24
INFO:root:y_pred: [0.01638383 0.02945687 0.31579235 ... 0.49495453 0.5484902  0.08185589]
INFO:root:y_true: [0. 0. 0. ... 1. 1. 0.]
INFO:root:sensitive_y_pred: [0.51784426 0.39692566 0.74859595 0.8372645  0.7673341  0.47311202
 0.7763513  0.6370476  0.41070557 0.7043475  0.7299457  0.30944857
 0.736896   0.8543532  0.6054392  0.7549362  0.64393145 0.5582962
 0.5385788  0.5650648  0.577457   0.64757234 0.5566178  0.9574326
 0.60733235 0.5909559  0.17627183 0.48586056 0.21056354 0.74332523
 0.67474383 0.8529646  0.68953    0.76449555 0.89326465 0.58194363
 0.7368766  0.6520277 ]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
INFO:root:2024-04-25 14:05:24, Dev, Step : 1288, Loss : 0.56386, Acc : 0.785, Auc : 0.849, Sensitive_Loss : 0.82113, Sensitive_Acc : 10.605, Sensitive_Auc : 0.830, Mean auc: 0.849, Run Time : 88.26 sec
INFO:root:2024-04-25 14:05:31, Train, Epoch : 8, Step : 1290, Loss : 0.06895, Acc : 0.169, Sensitive_Loss : 0.18836, Sensitive_Acc : 3.500, Run Time : 5.75 sec
INFO:root:2024-04-25 14:05:55, Train, Epoch : 8, Step : 1300, Loss : 0.43206, Acc : 0.816, Sensitive_Loss : 0.99801, Sensitive_Acc : 9.200, Run Time : 24.04 sec
INFO:root:2024-04-25 14:07:22, Dev, Step : 1300, Loss : 0.54894, Acc : 0.772, Auc : 0.850, Sensitive_Loss : 0.86473, Sensitive_Acc : 10.605, Sensitive_Auc : 0.861, Mean auc: 0.850, Run Time : 86.82 sec
INFO:root:2024-04-25 14:07:36, Train, Epoch : 8, Step : 1310, Loss : 0.42428, Acc : 0.822, Sensitive_Loss : 0.94643, Sensitive_Acc : 7.900, Run Time : 101.03 sec
INFO:root:2024-04-25 14:07:58, Train, Epoch : 8, Step : 1320, Loss : 0.47425, Acc : 0.838, Sensitive_Loss : 1.02727, Sensitive_Acc : 14.800, Run Time : 21.24 sec
INFO:root:2024-04-25 14:08:18, Train, Epoch : 8, Step : 1330, Loss : 0.40928, Acc : 0.838, Sensitive_Loss : 0.97265, Sensitive_Acc : 11.400, Run Time : 19.89 sec
INFO:root:2024-04-25 14:08:38, Train, Epoch : 8, Step : 1340, Loss : 0.43877, Acc : 0.828, Sensitive_Loss : 0.91110, Sensitive_Acc : 11.400, Run Time : 20.27 sec
INFO:root:2024-04-25 14:08:58, Train, Epoch : 8, Step : 1350, Loss : 0.40395, Acc : 0.853, Sensitive_Loss : 0.84295, Sensitive_Acc : 19.900, Run Time : 20.28 sec
libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG
INFO:root:2024-04-25 14:09:18, Train, Epoch : 8, Step : 1360, Loss : 0.43182, Acc : 0.834, Sensitive_Loss : 0.83107, Sensitive_Acc : 16.400, Run Time : 20.00 sec
INFO:root:2024-04-25 14:09:39, Train, Epoch : 8, Step : 1370, Loss : 0.42658, Acc : 0.822, Sensitive_Loss : 0.82815, Sensitive_Acc : 10.900, Run Time : 21.04 sec
INFO:root:2024-04-25 14:09:59, Train, Epoch : 8, Step : 1380, Loss : 0.37321, Acc : 0.897, Sensitive_Loss : 0.85710, Sensitive_Acc : 8.900, Run Time : 20.24 sec
INFO:root:2024-04-25 14:10:20, Train, Epoch : 8, Step : 1390, Loss : 0.41993, Acc : 0.844, Sensitive_Loss : 0.83975, Sensitive_Acc : 14.900, Run Time : 20.89 sec
INFO:root:2024-04-25 14:10:42, Train, Epoch : 8, Step : 1400, Loss : 0.36056, Acc : 0.847, Sensitive_Loss : 0.80109, Sensitive_Acc : 15.000, Run Time : 21.26 sec
INFO:root:2024-04-25 14:12:00, Dev, Step : 1400, Loss : 0.61258, Acc : 0.803, Auc : 0.852, Sensitive_Loss : 0.71833, Sensitive_Acc : 13.395, Sensitive_Auc : 0.939, Mean auc: 0.852, Run Time : 78.68 sec
INFO:root:2024-04-25 14:12:01, Best, Step : 1400, Loss : 0.61258, Acc : 0.803, Auc : 0.852, Sensitive_Loss : 0.71833, Sensitive_Acc : 13.395, Sensitive_Auc : 0.939, Best Auc : 0.852
INFO:root:2024-04-25 14:12:16, Train, Epoch : 8, Step : 1410, Loss : 0.37008, Acc : 0.856, Sensitive_Loss : 0.84815, Sensitive_Acc : 17.900, Run Time : 94.23 sec
INFO:root:2024-04-25 14:12:36, Train, Epoch : 8, Step : 1420, Loss : 0.37617, Acc : 0.853, Sensitive_Loss : 0.74891, Sensitive_Acc : 7.900, Run Time : 19.77 sec
INFO:root:2024-04-25 14:12:56, Train, Epoch : 8, Step : 1430, Loss : 0.35341, Acc : 0.844, Sensitive_Loss : 0.75296, Sensitive_Acc : 12.300, Run Time : 20.40 sec
INFO:root:2024-04-25 14:13:16, Train, Epoch : 8, Step : 1440, Loss : 0.45863, Acc : 0.838, Sensitive_Loss : 0.74978, Sensitive_Acc : 16.800, Run Time : 20.35 sec
INFO:root:2024-04-25 14:13:37, Train, Epoch : 8, Step : 1450, Loss : 0.40106, Acc : 0.841, Sensitive_Loss : 0.72411, Sensitive_Acc : 14.600, Run Time : 20.93 sec
INFO:root:2024-04-25 14:13:58, Train, Epoch : 8, Step : 1460, Loss : 0.43912, Acc : 0.838, Sensitive_Loss : 0.78190, Sensitive_Acc : 18.600, Run Time : 21.17 sec
INFO:root:2024-04-25 14:14:19, Train, Epoch : 8, Step : 1470, Loss : 0.35126, Acc : 0.834, Sensitive_Loss : 0.69272, Sensitive_Acc : 14.700, Run Time : 20.35 sec
INFO:root:2024-04-25 14:15:36
INFO:root:y_pred: [0.00826793 0.02024497 0.22622906 ... 0.4802972  0.45503464 0.02352621]
INFO:root:y_true: [0. 0. 0. ... 1. 1. 0.]
INFO:root:sensitive_y_pred: [0.3205814  0.36124223 0.6169635  0.9683091  0.6457194  0.3492257
 0.60923576 0.5018638  0.33816227 0.55598205 0.5711536  0.24412115
 0.67862666 0.76872087 0.8631995  0.9321217  0.49957907 0.42908537
 0.3514785  0.41315603 0.39898124 0.6477948  0.3542777  0.9933664
 0.38913995 0.41290942 0.18771796 0.5269209  0.1385089  0.71708363
 0.48254162 0.7747066  0.5357782  0.9061527  0.9700903  0.38411868
 0.4971799  0.57735693]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
INFO:root:2024-04-25 14:15:36, Dev, Step : 1472, Loss : 0.61206, Acc : 0.790, Auc : 0.845, Sensitive_Loss : 0.61560, Sensitive_Acc : 16.605, Sensitive_Auc : 0.982, Mean auc: 0.845, Run Time : 74.53 sec
INFO:root:2024-04-25 14:15:54, Train, Epoch : 9, Step : 1480, Loss : 0.25738, Acc : 0.700, Sensitive_Loss : 0.52148, Sensitive_Acc : 12.200, Run Time : 17.67 sec
INFO:root:2024-04-25 14:16:15, Train, Epoch : 9, Step : 1490, Loss : 0.37016, Acc : 0.869, Sensitive_Loss : 0.69166, Sensitive_Acc : 19.700, Run Time : 20.92 sec
INFO:root:2024-04-25 14:16:35, Train, Epoch : 9, Step : 1500, Loss : 0.36423, Acc : 0.859, Sensitive_Loss : 0.63919, Sensitive_Acc : 18.200, Run Time : 19.77 sec
INFO:root:2024-04-25 14:17:56, Dev, Step : 1500, Loss : 0.58736, Acc : 0.800, Auc : 0.851, Sensitive_Loss : 0.63272, Sensitive_Acc : 17.184, Sensitive_Auc : 0.982, Mean auc: 0.851, Run Time : 80.36 sec
INFO:root:2024-04-25 14:18:10, Train, Epoch : 9, Step : 1510, Loss : 0.39105, Acc : 0.850, Sensitive_Loss : 0.67380, Sensitive_Acc : 14.200, Run Time : 94.36 sec
INFO:root:2024-04-25 14:18:30, Train, Epoch : 9, Step : 1520, Loss : 0.39831, Acc : 0.834, Sensitive_Loss : 0.70975, Sensitive_Acc : 17.500, Run Time : 20.85 sec
INFO:root:2024-04-25 14:18:51, Train, Epoch : 9, Step : 1530, Loss : 0.45333, Acc : 0.831, Sensitive_Loss : 0.68299, Sensitive_Acc : 17.500, Run Time : 20.74 sec
INFO:root:2024-04-25 14:19:12, Train, Epoch : 9, Step : 1540, Loss : 0.40381, Acc : 0.856, Sensitive_Loss : 0.60735, Sensitive_Acc : 16.800, Run Time : 20.96 sec
INFO:root:2024-04-25 14:19:33, Train, Epoch : 9, Step : 1550, Loss : 0.38017, Acc : 0.863, Sensitive_Loss : 0.67000, Sensitive_Acc : 14.900, Run Time : 20.76 sec
INFO:root:2024-04-25 14:19:53, Train, Epoch : 9, Step : 1560, Loss : 0.44371, Acc : 0.831, Sensitive_Loss : 0.70329, Sensitive_Acc : 17.600, Run Time : 20.09 sec
INFO:root:2024-04-25 14:20:14, Train, Epoch : 9, Step : 1570, Loss : 0.35052, Acc : 0.875, Sensitive_Loss : 0.80349, Sensitive_Acc : 13.900, Run Time : 20.85 sec
INFO:root:2024-04-25 14:20:34, Train, Epoch : 9, Step : 1580, Loss : 0.30866, Acc : 0.878, Sensitive_Loss : 0.55657, Sensitive_Acc : 17.700, Run Time : 20.56 sec
INFO:root:2024-04-25 14:20:55, Train, Epoch : 9, Step : 1590, Loss : 0.36146, Acc : 0.828, Sensitive_Loss : 0.68178, Sensitive_Acc : 18.100, Run Time : 20.55 sec
INFO:root:2024-04-25 14:21:14, Train, Epoch : 9, Step : 1600, Loss : 0.37831, Acc : 0.856, Sensitive_Loss : 0.63620, Sensitive_Acc : 18.600, Run Time : 19.55 sec
INFO:root:2024-04-25 14:22:34, Dev, Step : 1600, Loss : 0.64572, Acc : 0.784, Auc : 0.843, Sensitive_Loss : 0.53680, Sensitive_Acc : 19.816, Sensitive_Auc : 0.982, Mean auc: 0.843, Run Time : 79.30 sec
INFO:root:2024-04-25 14:22:48, Train, Epoch : 9, Step : 1610, Loss : 0.46373, Acc : 0.816, Sensitive_Loss : 0.59460, Sensitive_Acc : 14.000, Run Time : 93.20 sec
libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG
INFO:root:2024-04-25 14:23:08, Train, Epoch : 9, Step : 1620, Loss : 0.38873, Acc : 0.856, Sensitive_Loss : 0.72566, Sensitive_Acc : 19.700, Run Time : 20.76 sec
INFO:root:2024-04-25 14:23:28, Train, Epoch : 9, Step : 1630, Loss : 0.38230, Acc : 0.850, Sensitive_Loss : 0.60131, Sensitive_Acc : 18.100, Run Time : 19.96 sec
INFO:root:2024-04-25 14:23:49, Train, Epoch : 9, Step : 1640, Loss : 0.40630, Acc : 0.825, Sensitive_Loss : 0.64116, Sensitive_Acc : 17.300, Run Time : 20.55 sec
INFO:root:2024-04-25 14:24:09, Train, Epoch : 9, Step : 1650, Loss : 0.44860, Acc : 0.844, Sensitive_Loss : 0.54891, Sensitive_Acc : 21.200, Run Time : 20.37 sec
INFO:root:2024-04-25 14:25:35
INFO:root:y_pred: [0.00325505 0.01151812 0.18321854 ... 0.5217378  0.5832262  0.05573953]
INFO:root:y_true: [0. 0. 0. ... 1. 1. 0.]
INFO:root:sensitive_y_pred: [0.18287024 0.25798392 0.5225827  0.98220575 0.6070204  0.29661363
 0.50433856 0.51620483 0.25272086 0.40035316 0.4124541  0.24544999
 0.6728071  0.6941262  0.83118844 0.88036287 0.39171162 0.413364
 0.2703804  0.24728134 0.3130099  0.57597935 0.28814384 0.9960193
 0.23736687 0.31015486 0.15472648 0.33916706 0.0738899  0.7080342
 0.32952338 0.68219316 0.49668187 0.920656   0.997413   0.27934888
 0.4017118  0.43035796]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
INFO:root:2024-04-25 14:25:35, Dev, Step : 1656, Loss : 0.60180, Acc : 0.788, Auc : 0.852, Sensitive_Loss : 0.54244, Sensitive_Acc : 19.395, Sensitive_Auc : 0.982, Mean auc: 0.852, Run Time : 74.23 sec
INFO:root:2024-04-25 14:25:36, Best, Step : 1656, Loss : 0.60180, Acc : 0.788,Auc : 0.852, Best Auc : 0.852, Sensitive_Loss : 0.54244, Sensitive_Acc : 19.395, Sensitive_Auc : 0.982
INFO:root:2024-04-25 14:25:47, Train, Epoch : 10, Step : 1660, Loss : 0.13522, Acc : 0.347, Sensitive_Loss : 0.22552, Sensitive_Acc : 6.700, Run Time : 10.32 sec
INFO:root:2024-04-25 14:26:08, Train, Epoch : 10, Step : 1670, Loss : 0.37242, Acc : 0.863, Sensitive_Loss : 0.55955, Sensitive_Acc : 23.800, Run Time : 20.93 sec
INFO:root:2024-04-25 14:26:31, Train, Epoch : 10, Step : 1680, Loss : 0.30004, Acc : 0.900, Sensitive_Loss : 0.57407, Sensitive_Acc : 19.700, Run Time : 22.67 sec
INFO:root:2024-04-25 14:26:51, Train, Epoch : 10, Step : 1690, Loss : 0.34960, Acc : 0.866, Sensitive_Loss : 0.54108, Sensitive_Acc : 18.700, Run Time : 19.58 sec
INFO:root:2024-04-25 14:27:10, Train, Epoch : 10, Step : 1700, Loss : 0.42632, Acc : 0.844, Sensitive_Loss : 0.55323, Sensitive_Acc : 14.000, Run Time : 19.96 sec
INFO:root:2024-04-25 14:28:31, Dev, Step : 1700, Loss : 0.62646, Acc : 0.791, Auc : 0.853, Sensitive_Loss : 0.50955, Sensitive_Acc : 22.605, Sensitive_Auc : 0.982, Mean auc: 0.853, Run Time : 80.23 sec
INFO:root:2024-04-25 14:28:31, Best, Step : 1700, Loss : 0.62646, Acc : 0.791, Auc : 0.853, Sensitive_Loss : 0.50955, Sensitive_Acc : 22.605, Sensitive_Auc : 0.982, Best Auc : 0.853
INFO:root:2024-04-25 14:28:45, Train, Epoch : 10, Step : 1710, Loss : 0.35461, Acc : 0.875, Sensitive_Loss : 0.56066, Sensitive_Acc : 22.900, Run Time : 94.87 sec
INFO:root:2024-04-25 14:29:06, Train, Epoch : 10, Step : 1720, Loss : 0.31281, Acc : 0.884, Sensitive_Loss : 0.56779, Sensitive_Acc : 18.400, Run Time : 20.62 sec
INFO:root:2024-04-25 14:29:27, Train, Epoch : 10, Step : 1730, Loss : 0.37044, Acc : 0.853, Sensitive_Loss : 0.65353, Sensitive_Acc : 11.400, Run Time : 20.97 sec
INFO:root:2024-04-25 14:29:47, Train, Epoch : 10, Step : 1740, Loss : 0.40498, Acc : 0.844, Sensitive_Loss : 0.52884, Sensitive_Acc : 15.200, Run Time : 20.36 sec
INFO:root:2024-04-25 14:30:07, Train, Epoch : 10, Step : 1750, Loss : 0.36629, Acc : 0.853, Sensitive_Loss : 0.59162, Sensitive_Acc : 13.500, Run Time : 20.14 sec
INFO:root:2024-04-25 14:30:29, Train, Epoch : 10, Step : 1760, Loss : 0.32173, Acc : 0.863, Sensitive_Loss : 0.56166, Sensitive_Acc : 17.900, Run Time : 21.19 sec
INFO:root:2024-04-25 14:30:49, Train, Epoch : 10, Step : 1770, Loss : 0.35221, Acc : 0.863, Sensitive_Loss : 0.58858, Sensitive_Acc : 19.400, Run Time : 20.34 sec
INFO:root:2024-04-25 14:31:09, Train, Epoch : 10, Step : 1780, Loss : 0.38795, Acc : 0.841, Sensitive_Loss : 0.60611, Sensitive_Acc : 16.500, Run Time : 19.77 sec
libpng warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG
INFO:root:2024-04-25 14:31:30, Train, Epoch : 10, Step : 1790, Loss : 0.33062, Acc : 0.891, Sensitive_Loss : 0.52275, Sensitive_Acc : 16.900, Run Time : 21.59 sec
INFO:root:2024-04-25 14:31:50, Train, Epoch : 10, Step : 1800, Loss : 0.35743, Acc : 0.869, Sensitive_Loss : 0.55913, Sensitive_Acc : 15.000, Run Time : 20.16 sec
INFO:root:2024-04-25 14:33:10, Dev, Step : 1800, Loss : 0.59912, Acc : 0.789, Auc : 0.856, Sensitive_Loss : 0.51435, Sensitive_Acc : 21.079, Sensitive_Auc : 0.982, Mean auc: 0.856, Run Time : 79.42 sec
INFO:root:2024-04-25 14:33:11, Best, Step : 1800, Loss : 0.59912, Acc : 0.789, Auc : 0.856, Sensitive_Loss : 0.51435, Sensitive_Acc : 21.079, Sensitive_Auc : 0.982, Best Auc : 0.856
INFO:root:2024-04-25 14:33:24, Train, Epoch : 10, Step : 1810, Loss : 0.35638, Acc : 0.850, Sensitive_Loss : 0.47389, Sensitive_Acc : 19.600, Run Time : 93.95 sec
INFO:root:2024-04-25 14:33:46, Train, Epoch : 10, Step : 1820, Loss : 0.36693, Acc : 0.831, Sensitive_Loss : 0.53557, Sensitive_Acc : 18.900, Run Time : 21.23 sec
INFO:root:2024-04-25 14:34:06, Train, Epoch : 10, Step : 1830, Loss : 0.35605, Acc : 0.869, Sensitive_Loss : 0.47681, Sensitive_Acc : 21.900, Run Time : 20.08 sec
INFO:root:2024-04-25 14:34:25, Train, Epoch : 10, Step : 1840, Loss : 0.35934, Acc : 0.875, Sensitive_Loss : 0.52953, Sensitive_Acc : 22.600, Run Time : 19.61 sec
INFO:root:2024-04-25 14:35:40
INFO:root:y_pred: [0.00344454 0.01284537 0.2336752  ... 0.39698404 0.5177266  0.04374678]
INFO:root:y_true: [0. 0. 0. ... 1. 1. 0.]
INFO:root:sensitive_y_pred: [0.13669796 0.17959195 0.35103273 0.97788006 0.44240797 0.17921203
 0.36507446 0.34498358 0.16687462 0.21619838 0.28876063 0.15871838
 0.5308616  0.6306895  0.9584481  0.97351295 0.27174616 0.30204627
 0.1800256  0.18327956 0.15097563 0.3687585  0.16825227 0.99575126
 0.15103884 0.21700504 0.10942823 0.20975724 0.13737994 0.5975839
 0.21183568 0.56763035 0.35888118 0.9616022  0.9971814  0.15301831
 0.24894169 0.29713994]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]
INFO:root:2024-04-25 14:35:40, Dev, Step : 1840, Loss : 0.62605, Acc : 0.792, Auc : 0.855, Sensitive_Loss : 0.47764, Sensitive_Acc : 22.605, Sensitive_Auc : 0.982, Mean auc: 0.855, Run Time : 74.80 sec
