Running on desktop22:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/random_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/random_dataset_val.csv",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.1,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-03-28 21:34:28, Train, Epoch : 1, Step : 10, Loss : 1.34659, Acc : 0.500, Sensitive_Loss : 0.83796, Sensitive_Acc : 15.600, Run Time : 20.22 sec
INFO:root:2024-03-28 21:34:40, Train, Epoch : 1, Step : 20, Loss : 1.29489, Acc : 0.503, Sensitive_Loss : 0.77876, Sensitive_Acc : 14.400, Run Time : 12.12 sec
INFO:root:2024-03-28 21:34:54, Train, Epoch : 1, Step : 30, Loss : 1.15060, Acc : 0.506, Sensitive_Loss : 0.81432, Sensitive_Acc : 15.800, Run Time : 14.30 sec
INFO:root:2024-03-28 21:35:06, Train, Epoch : 1, Step : 40, Loss : 1.17397, Acc : 0.519, Sensitive_Loss : 0.73078, Sensitive_Acc : 16.100, Run Time : 12.42 sec
INFO:root:2024-03-28 21:35:19, Train, Epoch : 1, Step : 50, Loss : 1.22262, Acc : 0.503, Sensitive_Loss : 0.68052, Sensitive_Acc : 16.000, Run Time : 13.01 sec
INFO:root:2024-03-28 21:35:36, Train, Epoch : 1, Step : 60, Loss : 1.19825, Acc : 0.469, Sensitive_Loss : 0.64077, Sensitive_Acc : 15.100, Run Time : 16.18 sec
INFO:root:2024-03-28 21:35:50, Train, Epoch : 1, Step : 70, Loss : 1.08123, Acc : 0.491, Sensitive_Loss : 0.64651, Sensitive_Acc : 16.100, Run Time : 13.99 sec
INFO:root:2024-03-28 21:36:03, Train, Epoch : 1, Step : 80, Loss : 1.41528, Acc : 0.509, Sensitive_Loss : 0.69523, Sensitive_Acc : 14.600, Run Time : 13.08 sec
INFO:root:2024-03-28 21:36:15, Train, Epoch : 1, Step : 90, Loss : 1.27530, Acc : 0.534, Sensitive_Loss : 0.68467, Sensitive_Acc : 14.500, Run Time : 12.57 sec
INFO:root:2024-03-28 21:36:29, Train, Epoch : 1, Step : 100, Loss : 0.99123, Acc : 0.534, Sensitive_Loss : 0.71440, Sensitive_Acc : 16.300, Run Time : 13.65 sec
INFO:root:2024-03-28 21:39:42, Dev, Step : 100, Loss : 1.27901, Acc : 0.794, Auc : 0.651, Sensitive_Loss : 0.67236, Sensitive_Acc : 16.667, Sensitive_Auc : 0.799, Mean auc: 0.651, Run Time : 193.15 sec
INFO:root:2024-03-28 21:39:44, Best, Step : 100, Loss : 1.27901, Acc : 0.794, Auc : 0.651, Sensitive_Loss : 0.67236, Sensitive_Acc : 16.667, Sensitive_Auc : 0.799, Best Auc : 0.651
INFO:root:2024-03-28 21:39:52, Train, Epoch : 1, Step : 110, Loss : 1.34273, Acc : 0.547, Sensitive_Loss : 0.63639, Sensitive_Acc : 16.600, Run Time : 203.31 sec
INFO:root:2024-03-28 21:40:06, Train, Epoch : 1, Step : 120, Loss : 1.20826, Acc : 0.544, Sensitive_Loss : 0.62568, Sensitive_Acc : 16.500, Run Time : 13.57 sec
INFO:root:2024-03-28 21:40:18, Train, Epoch : 1, Step : 130, Loss : 1.17415, Acc : 0.516, Sensitive_Loss : 0.64593, Sensitive_Acc : 16.400, Run Time : 12.38 sec
INFO:root:2024-03-28 21:40:34, Train, Epoch : 1, Step : 140, Loss : 1.21307, Acc : 0.528, Sensitive_Loss : 0.57933, Sensitive_Acc : 17.500, Run Time : 16.38 sec
INFO:root:2024-03-28 21:40:47, Train, Epoch : 1, Step : 150, Loss : 1.29251, Acc : 0.550, Sensitive_Loss : 0.60428, Sensitive_Acc : 16.900, Run Time : 12.99 sec
INFO:root:2024-03-28 21:41:02, Train, Epoch : 1, Step : 160, Loss : 0.89515, Acc : 0.516, Sensitive_Loss : 0.62884, Sensitive_Acc : 15.800, Run Time : 14.50 sec
INFO:root:2024-03-28 21:41:15, Train, Epoch : 1, Step : 170, Loss : 1.30427, Acc : 0.494, Sensitive_Loss : 0.57740, Sensitive_Acc : 17.100, Run Time : 12.84 sec
INFO:root:2024-03-28 21:41:28, Train, Epoch : 1, Step : 180, Loss : 1.14387, Acc : 0.516, Sensitive_Loss : 0.58327, Sensitive_Acc : 15.300, Run Time : 12.89 sec
INFO:root:2024-03-28 21:41:42, Train, Epoch : 1, Step : 190, Loss : 1.05736, Acc : 0.559, Sensitive_Loss : 0.66048, Sensitive_Acc : 16.700, Run Time : 14.74 sec
INFO:root:2024-03-28 21:41:56, Train, Epoch : 1, Step : 200, Loss : 1.13766, Acc : 0.544, Sensitive_Loss : 0.59301, Sensitive_Acc : 17.700, Run Time : 13.38 sec
INFO:root:2024-03-28 21:44:32, Dev, Step : 200, Loss : 1.09411, Acc : 0.523, Auc : 0.658, Sensitive_Loss : 0.62776, Sensitive_Acc : 15.674, Sensitive_Auc : 0.856, Mean auc: 0.658, Run Time : 156.03 sec
INFO:root:2024-03-28 21:44:33, Best, Step : 200, Loss : 1.09411, Acc : 0.523, Auc : 0.658, Sensitive_Loss : 0.62776, Sensitive_Acc : 15.674, Sensitive_Auc : 0.856, Best Auc : 0.658
INFO:root:2024-03-28 21:44:41, Train, Epoch : 1, Step : 210, Loss : 0.95155, Acc : 0.622, Sensitive_Loss : 0.62812, Sensitive_Acc : 15.700, Run Time : 165.01 sec
INFO:root:2024-03-28 21:44:53, Train, Epoch : 1, Step : 220, Loss : 1.09488, Acc : 0.603, Sensitive_Loss : 0.57361, Sensitive_Acc : 15.800, Run Time : 11.91 sec
INFO:root:2024-03-28 21:45:06, Train, Epoch : 1, Step : 230, Loss : 1.02697, Acc : 0.606, Sensitive_Loss : 0.62403, Sensitive_Acc : 17.600, Run Time : 13.51 sec
INFO:root:2024-03-28 21:45:20, Train, Epoch : 1, Step : 240, Loss : 0.90431, Acc : 0.594, Sensitive_Loss : 0.51594, Sensitive_Acc : 16.300, Run Time : 13.91 sec
INFO:root:2024-03-28 21:45:32, Train, Epoch : 1, Step : 250, Loss : 1.11406, Acc : 0.578, Sensitive_Loss : 0.56393, Sensitive_Acc : 18.300, Run Time : 11.35 sec
INFO:root:2024-03-28 21:45:48, Train, Epoch : 1, Step : 260, Loss : 1.02307, Acc : 0.584, Sensitive_Loss : 0.57642, Sensitive_Acc : 16.200, Run Time : 16.46 sec
INFO:root:2024-03-28 21:46:00, Train, Epoch : 1, Step : 270, Loss : 1.12928, Acc : 0.575, Sensitive_Loss : 0.56181, Sensitive_Acc : 15.700, Run Time : 11.89 sec
INFO:root:2024-03-28 21:46:12, Train, Epoch : 1, Step : 280, Loss : 1.15756, Acc : 0.581, Sensitive_Loss : 0.52980, Sensitive_Acc : 15.000, Run Time : 12.17 sec
INFO:root:2024-03-28 21:46:24, Train, Epoch : 1, Step : 290, Loss : 1.17492, Acc : 0.556, Sensitive_Loss : 0.58355, Sensitive_Acc : 14.800, Run Time : 11.53 sec
INFO:root:2024-03-28 21:46:36, Train, Epoch : 1, Step : 300, Loss : 1.16587, Acc : 0.556, Sensitive_Loss : 0.57598, Sensitive_Acc : 17.400, Run Time : 11.95 sec
INFO:root:2024-03-28 21:49:09, Dev, Step : 300, Loss : 1.09618, Acc : 0.471, Auc : 0.690, Sensitive_Loss : 0.56417, Sensitive_Acc : 16.312, Sensitive_Auc : 0.877, Mean auc: 0.690, Run Time : 153.20 sec
INFO:root:2024-03-28 21:49:10, Best, Step : 300, Loss : 1.09618, Acc : 0.471, Auc : 0.690, Sensitive_Loss : 0.56417, Sensitive_Acc : 16.312, Sensitive_Auc : 0.877, Best Auc : 0.690
INFO:root:2024-03-28 21:49:19, Train, Epoch : 1, Step : 310, Loss : 1.12609, Acc : 0.588, Sensitive_Loss : 0.58250, Sensitive_Acc : 18.000, Run Time : 163.85 sec
INFO:root:2024-03-28 21:49:34, Train, Epoch : 1, Step : 320, Loss : 1.20566, Acc : 0.556, Sensitive_Loss : 0.56080, Sensitive_Acc : 17.300, Run Time : 14.35 sec
INFO:root:2024-03-28 21:49:46, Train, Epoch : 1, Step : 330, Loss : 1.04761, Acc : 0.600, Sensitive_Loss : 0.54233, Sensitive_Acc : 17.800, Run Time : 11.98 sec
INFO:root:2024-03-28 21:49:59, Train, Epoch : 1, Step : 340, Loss : 0.99252, Acc : 0.594, Sensitive_Loss : 0.53211, Sensitive_Acc : 15.500, Run Time : 12.97 sec
INFO:root:2024-03-28 21:50:10, Train, Epoch : 1, Step : 350, Loss : 1.14534, Acc : 0.581, Sensitive_Loss : 0.50337, Sensitive_Acc : 16.700, Run Time : 11.49 sec
INFO:root:2024-03-28 21:50:22, Train, Epoch : 1, Step : 360, Loss : 1.03140, Acc : 0.575, Sensitive_Loss : 0.52073, Sensitive_Acc : 17.100, Run Time : 12.30 sec
INFO:root:2024-03-28 21:50:36, Train, Epoch : 1, Step : 370, Loss : 1.10768, Acc : 0.647, Sensitive_Loss : 0.46424, Sensitive_Acc : 15.600, Run Time : 13.99 sec
INFO:root:2024-03-28 21:50:47, Train, Epoch : 1, Step : 380, Loss : 0.99778, Acc : 0.613, Sensitive_Loss : 0.48112, Sensitive_Acc : 16.000, Run Time : 10.98 sec
INFO:root:2024-03-28 21:50:59, Train, Epoch : 1, Step : 390, Loss : 1.11315, Acc : 0.634, Sensitive_Loss : 0.46465, Sensitive_Acc : 15.900, Run Time : 11.53 sec
INFO:root:2024-03-28 21:51:15, Train, Epoch : 1, Step : 400, Loss : 1.14042, Acc : 0.609, Sensitive_Loss : 0.53400, Sensitive_Acc : 16.900, Run Time : 16.05 sec
INFO:root:2024-03-28 21:53:42, Dev, Step : 400, Loss : 0.99751, Acc : 0.664, Auc : 0.738, Sensitive_Loss : 0.54826, Sensitive_Acc : 15.759, Sensitive_Auc : 0.932, Mean auc: 0.738, Run Time : 147.19 sec
INFO:root:2024-03-28 21:53:43, Best, Step : 400, Loss : 0.99751, Acc : 0.664, Auc : 0.738, Sensitive_Loss : 0.54826, Sensitive_Acc : 15.759, Sensitive_Auc : 0.932, Best Auc : 0.738
INFO:root:2024-03-28 21:53:51, Train, Epoch : 1, Step : 410, Loss : 1.02334, Acc : 0.600, Sensitive_Loss : 0.50465, Sensitive_Acc : 16.000, Run Time : 155.55 sec
INFO:root:2024-03-28 21:54:03, Train, Epoch : 1, Step : 420, Loss : 1.32196, Acc : 0.603, Sensitive_Loss : 0.45276, Sensitive_Acc : 15.800, Run Time : 12.07 sec
INFO:root:2024-03-28 21:54:17, Train, Epoch : 1, Step : 430, Loss : 0.80106, Acc : 0.581, Sensitive_Loss : 0.50257, Sensitive_Acc : 15.100, Run Time : 14.07 sec
INFO:root:2024-03-28 21:54:29, Train, Epoch : 1, Step : 440, Loss : 0.98994, Acc : 0.575, Sensitive_Loss : 0.42003, Sensitive_Acc : 14.400, Run Time : 12.75 sec
INFO:root:2024-03-28 21:54:41, Train, Epoch : 1, Step : 450, Loss : 1.09517, Acc : 0.591, Sensitive_Loss : 0.51419, Sensitive_Acc : 15.600, Run Time : 11.24 sec
INFO:root:2024-03-28 21:54:53, Train, Epoch : 1, Step : 460, Loss : 1.14812, Acc : 0.575, Sensitive_Loss : 0.53223, Sensitive_Acc : 14.400, Run Time : 12.19 sec
INFO:root:2024-03-28 21:55:06, Train, Epoch : 1, Step : 470, Loss : 1.22779, Acc : 0.569, Sensitive_Loss : 0.45536, Sensitive_Acc : 16.900, Run Time : 13.40 sec
INFO:root:2024-03-28 21:55:18, Train, Epoch : 1, Step : 480, Loss : 1.08710, Acc : 0.591, Sensitive_Loss : 0.45372, Sensitive_Acc : 17.300, Run Time : 11.91 sec
INFO:root:2024-03-28 21:55:31, Train, Epoch : 1, Step : 490, Loss : 1.18345, Acc : 0.622, Sensitive_Loss : 0.40469, Sensitive_Acc : 17.900, Run Time : 12.85 sec
INFO:root:2024-03-28 21:57:58
INFO:root:y_pred: [0.28215155 0.09811145 0.40181538 ... 0.47187465 0.3015064  0.5240805 ]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [7.2326906e-02 3.1050146e-01 8.7991941e-01 4.8598170e-01 7.1930957e-01
 3.3315048e-01 7.5403827e-01 5.5609530e-01 1.3072291e-02 1.6517054e-01
 3.3369145e-01 1.8261756e-01 9.7863376e-01 4.1856870e-02 9.9132907e-01
 9.7202522e-01 3.8394138e-01 8.9104617e-01 8.7447256e-01 9.5730633e-01
 2.4064209e-01 7.2155178e-01 9.9988914e-01 3.3905727e-01 4.9703190e-01
 9.4162577e-01 2.9002696e-01 9.9700564e-01 1.0246949e-01 4.9310037e-01
 9.9406505e-01 3.3854514e-01 1.3366236e-02 2.7602255e-01 2.1737452e-01
 4.8625679e-04 3.8198136e-02 2.9987738e-01 9.2352718e-01 4.8354965e-01
 1.5695822e-01 5.3148019e-01 8.0802464e-01 9.9965477e-01 4.6416864e-01
 4.0142021e-01 2.0863509e-01 2.0313489e-01 9.8280752e-01 8.5009944e-01
 9.8340386e-01 9.0457392e-01 9.9834526e-01 2.8771931e-01 9.9879563e-01
 6.8040830e-01 8.3836180e-01 1.8721379e-01 9.8412919e-01 3.5713008e-01
 4.0049586e-02 5.1939446e-01 8.2147557e-01 9.8660386e-01 9.6053898e-01
 9.9273950e-01 7.7975595e-01 8.4117746e-01 9.9338949e-01 9.7527540e-01
 4.1607732e-01 4.0159081e-03 7.4683917e-01 5.1925099e-01 5.9722281e-01
 7.1203187e-02 6.1249620e-01 8.8673942e-03 5.7285696e-01 8.7238681e-01
 9.5573211e-01 3.5087639e-01 4.7181976e-01 9.9299252e-01 9.4844103e-01
 8.9742476e-01 9.3177819e-01 6.4286095e-01 7.0222116e-01 5.9764701e-01
 9.3839690e-03 5.7769042e-01 3.0991912e-01 1.6772810e-01 5.5371755e-01
 8.6186039e-01 1.6609690e-01 5.5627458e-02 9.6439660e-01 8.7102515e-01
 9.4436234e-01 5.2494343e-02 9.9260074e-01 4.3867344e-01 5.7885188e-01
 6.6500920e-01 9.1031438e-01 5.2967513e-01 1.8700562e-01 3.9656836e-01
 1.2070309e-01 9.5351607e-01 4.8740190e-01 9.2192650e-02 3.9000586e-01
 4.5648843e-04 9.6161741e-01 1.6199946e-01 9.0797895e-01 1.3194771e-01
 8.3220285e-01 4.0827304e-01 9.9331397e-01 1.0518524e-01 1.2332150e-01
 9.4728988e-01 6.4930910e-01 7.3347998e-01 4.3040496e-01 8.7699711e-01
 8.5049748e-02 9.3423986e-01 3.5480194e-02 9.8432434e-01 8.9803916e-01
 8.9118522e-01 8.5284370e-01 4.7726804e-01 2.0712178e-02 9.6341550e-01
 9.5991480e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-28 21:57:58, Dev, Step : 492, Loss : 1.04994, Acc : 0.572, Auc : 0.710, Sensitive_Loss : 0.55824, Sensitive_Acc : 16.482, Sensitive_Auc : 0.879, Mean auc: 0.710, Run Time : 144.19 sec
INFO:root:2024-03-28 21:58:08, Train, Epoch : 2, Step : 500, Loss : 0.95449, Acc : 0.494, Sensitive_Loss : 0.40169, Sensitive_Acc : 13.100, Run Time : 8.61 sec
INFO:root:2024-03-28 21:59:45, Dev, Step : 500, Loss : 1.06209, Acc : 0.724, Auc : 0.695, Sensitive_Loss : 0.63414, Sensitive_Acc : 15.972, Sensitive_Auc : 0.890, Mean auc: 0.695, Run Time : 96.87 sec
INFO:root:2024-03-28 21:59:52, Train, Epoch : 2, Step : 510, Loss : 1.16188, Acc : 0.544, Sensitive_Loss : 0.42978, Sensitive_Acc : 17.200, Run Time : 103.88 sec
INFO:root:2024-03-28 22:00:06, Train, Epoch : 2, Step : 520, Loss : 1.07188, Acc : 0.616, Sensitive_Loss : 0.46489, Sensitive_Acc : 16.100, Run Time : 13.85 sec
INFO:root:2024-03-28 22:00:18, Train, Epoch : 2, Step : 530, Loss : 1.07183, Acc : 0.613, Sensitive_Loss : 0.47697, Sensitive_Acc : 17.200, Run Time : 12.55 sec
INFO:root:2024-03-28 22:00:29, Train, Epoch : 2, Step : 540, Loss : 1.04997, Acc : 0.641, Sensitive_Loss : 0.43687, Sensitive_Acc : 17.000, Run Time : 10.54 sec
INFO:root:2024-03-28 22:00:40, Train, Epoch : 2, Step : 550, Loss : 1.06372, Acc : 0.647, Sensitive_Loss : 0.45831, Sensitive_Acc : 14.300, Run Time : 11.00 sec
INFO:root:2024-03-28 22:00:50, Train, Epoch : 2, Step : 560, Loss : 1.33632, Acc : 0.628, Sensitive_Loss : 0.48027, Sensitive_Acc : 18.900, Run Time : 10.31 sec
INFO:root:2024-03-28 22:01:01, Train, Epoch : 2, Step : 570, Loss : 1.29383, Acc : 0.584, Sensitive_Loss : 0.41950, Sensitive_Acc : 16.200, Run Time : 10.55 sec
INFO:root:2024-03-28 22:01:12, Train, Epoch : 2, Step : 580, Loss : 1.14789, Acc : 0.641, Sensitive_Loss : 0.49036, Sensitive_Acc : 17.600, Run Time : 11.00 sec
INFO:root:2024-03-28 22:01:24, Train, Epoch : 2, Step : 590, Loss : 1.06479, Acc : 0.634, Sensitive_Loss : 0.43034, Sensitive_Acc : 17.800, Run Time : 12.03 sec
INFO:root:2024-03-28 22:01:34, Train, Epoch : 2, Step : 600, Loss : 1.07738, Acc : 0.619, Sensitive_Loss : 0.40322, Sensitive_Acc : 16.300, Run Time : 10.80 sec
INFO:root:2024-03-28 22:03:50, Dev, Step : 600, Loss : 1.05002, Acc : 0.564, Auc : 0.714, Sensitive_Loss : 0.47451, Sensitive_Acc : 16.468, Sensitive_Auc : 0.933, Mean auc: 0.714, Run Time : 135.97 sec
INFO:root:2024-03-28 22:03:57, Train, Epoch : 2, Step : 610, Loss : 1.12498, Acc : 0.625, Sensitive_Loss : 0.50647, Sensitive_Acc : 14.900, Run Time : 142.82 sec
INFO:root:2024-03-28 22:04:07, Train, Epoch : 2, Step : 620, Loss : 1.11969, Acc : 0.588, Sensitive_Loss : 0.40910, Sensitive_Acc : 16.700, Run Time : 9.91 sec
INFO:root:2024-03-28 22:04:19, Train, Epoch : 2, Step : 630, Loss : 1.19854, Acc : 0.553, Sensitive_Loss : 0.44454, Sensitive_Acc : 16.900, Run Time : 11.91 sec
INFO:root:2024-03-28 22:04:29, Train, Epoch : 2, Step : 640, Loss : 1.13539, Acc : 0.616, Sensitive_Loss : 0.46780, Sensitive_Acc : 15.000, Run Time : 10.29 sec
INFO:root:2024-03-28 22:04:40, Train, Epoch : 2, Step : 650, Loss : 0.95505, Acc : 0.647, Sensitive_Loss : 0.45726, Sensitive_Acc : 15.400, Run Time : 10.10 sec
INFO:root:2024-03-28 22:04:49, Train, Epoch : 2, Step : 660, Loss : 0.95507, Acc : 0.637, Sensitive_Loss : 0.33586, Sensitive_Acc : 17.200, Run Time : 9.71 sec
INFO:root:2024-03-28 22:04:58, Train, Epoch : 2, Step : 670, Loss : 1.13561, Acc : 0.603, Sensitive_Loss : 0.46225, Sensitive_Acc : 14.700, Run Time : 9.18 sec
INFO:root:2024-03-28 22:05:09, Train, Epoch : 2, Step : 680, Loss : 1.06543, Acc : 0.631, Sensitive_Loss : 0.38459, Sensitive_Acc : 15.900, Run Time : 10.20 sec
INFO:root:2024-03-28 22:05:21, Train, Epoch : 2, Step : 690, Loss : 1.02284, Acc : 0.644, Sensitive_Loss : 0.43216, Sensitive_Acc : 16.300, Run Time : 12.19 sec
INFO:root:2024-03-28 22:05:31, Train, Epoch : 2, Step : 700, Loss : 0.93267, Acc : 0.662, Sensitive_Loss : 0.47098, Sensitive_Acc : 15.500, Run Time : 9.78 sec
INFO:root:2024-03-28 22:07:49, Dev, Step : 700, Loss : 1.02700, Acc : 0.705, Auc : 0.743, Sensitive_Loss : 0.40186, Sensitive_Acc : 16.794, Sensitive_Auc : 0.948, Mean auc: 0.743, Run Time : 138.78 sec
INFO:root:2024-03-28 22:07:50, Best, Step : 700, Loss : 1.02700, Acc : 0.705, Auc : 0.743, Sensitive_Loss : 0.40186, Sensitive_Acc : 16.794, Sensitive_Auc : 0.948, Best Auc : 0.743
INFO:root:2024-03-28 22:07:57, Train, Epoch : 2, Step : 710, Loss : 0.93295, Acc : 0.641, Sensitive_Loss : 0.38326, Sensitive_Acc : 17.000, Run Time : 146.23 sec
INFO:root:2024-03-28 22:08:09, Train, Epoch : 2, Step : 720, Loss : 1.03811, Acc : 0.619, Sensitive_Loss : 0.44684, Sensitive_Acc : 16.500, Run Time : 12.58 sec
INFO:root:2024-03-28 22:08:19, Train, Epoch : 2, Step : 730, Loss : 1.17808, Acc : 0.647, Sensitive_Loss : 0.43540, Sensitive_Acc : 16.900, Run Time : 9.85 sec
INFO:root:2024-03-28 22:08:31, Train, Epoch : 2, Step : 740, Loss : 1.02108, Acc : 0.631, Sensitive_Loss : 0.38459, Sensitive_Acc : 15.300, Run Time : 12.26 sec
INFO:root:2024-03-28 22:08:45, Train, Epoch : 2, Step : 750, Loss : 0.95640, Acc : 0.675, Sensitive_Loss : 0.34073, Sensitive_Acc : 16.200, Run Time : 13.54 sec
INFO:root:2024-03-28 22:08:58, Train, Epoch : 2, Step : 760, Loss : 0.99152, Acc : 0.641, Sensitive_Loss : 0.38038, Sensitive_Acc : 15.800, Run Time : 13.03 sec
INFO:root:2024-03-28 22:09:08, Train, Epoch : 2, Step : 770, Loss : 0.94870, Acc : 0.656, Sensitive_Loss : 0.37458, Sensitive_Acc : 17.200, Run Time : 10.03 sec
INFO:root:2024-03-28 22:09:18, Train, Epoch : 2, Step : 780, Loss : 1.02917, Acc : 0.653, Sensitive_Loss : 0.40902, Sensitive_Acc : 17.600, Run Time : 10.00 sec
INFO:root:2024-03-28 22:09:33, Train, Epoch : 2, Step : 790, Loss : 1.08378, Acc : 0.613, Sensitive_Loss : 0.40129, Sensitive_Acc : 16.400, Run Time : 14.67 sec
INFO:root:2024-03-28 22:09:43, Train, Epoch : 2, Step : 800, Loss : 1.04535, Acc : 0.625, Sensitive_Loss : 0.37496, Sensitive_Acc : 15.200, Run Time : 10.72 sec
INFO:root:2024-03-28 22:12:06, Dev, Step : 800, Loss : 1.04695, Acc : 0.801, Auc : 0.718, Sensitive_Loss : 0.50822, Sensitive_Acc : 15.887, Sensitive_Auc : 0.968, Mean auc: 0.718, Run Time : 142.95 sec
INFO:root:2024-03-28 22:12:13, Train, Epoch : 2, Step : 810, Loss : 0.96851, Acc : 0.678, Sensitive_Loss : 0.34063, Sensitive_Acc : 15.500, Run Time : 149.19 sec
INFO:root:2024-03-28 22:12:23, Train, Epoch : 2, Step : 820, Loss : 1.15956, Acc : 0.669, Sensitive_Loss : 0.31221, Sensitive_Acc : 17.100, Run Time : 9.97 sec
INFO:root:2024-03-28 22:12:32, Train, Epoch : 2, Step : 830, Loss : 1.07808, Acc : 0.659, Sensitive_Loss : 0.37366, Sensitive_Acc : 15.700, Run Time : 9.62 sec
INFO:root:2024-03-28 22:12:42, Train, Epoch : 2, Step : 840, Loss : 0.89444, Acc : 0.691, Sensitive_Loss : 0.40500, Sensitive_Acc : 15.900, Run Time : 9.75 sec
INFO:root:2024-03-28 22:12:52, Train, Epoch : 2, Step : 850, Loss : 0.97978, Acc : 0.684, Sensitive_Loss : 0.41584, Sensitive_Acc : 14.800, Run Time : 9.73 sec
INFO:root:2024-03-28 22:13:01, Train, Epoch : 2, Step : 860, Loss : 1.05649, Acc : 0.628, Sensitive_Loss : 0.33392, Sensitive_Acc : 16.700, Run Time : 9.63 sec
INFO:root:2024-03-28 22:13:11, Train, Epoch : 2, Step : 870, Loss : 1.04955, Acc : 0.662, Sensitive_Loss : 0.33590, Sensitive_Acc : 16.200, Run Time : 9.61 sec
INFO:root:2024-03-28 22:13:21, Train, Epoch : 2, Step : 880, Loss : 1.13808, Acc : 0.656, Sensitive_Loss : 0.33155, Sensitive_Acc : 15.600, Run Time : 9.68 sec
INFO:root:2024-03-28 22:13:30, Train, Epoch : 2, Step : 890, Loss : 1.14704, Acc : 0.650, Sensitive_Loss : 0.38942, Sensitive_Acc : 15.600, Run Time : 9.71 sec
INFO:root:2024-03-28 22:13:42, Train, Epoch : 2, Step : 900, Loss : 1.05279, Acc : 0.641, Sensitive_Loss : 0.32044, Sensitive_Acc : 17.400, Run Time : 11.96 sec
INFO:root:2024-03-28 22:15:23, Dev, Step : 900, Loss : 1.18364, Acc : 0.668, Auc : 0.633, Sensitive_Loss : 0.37820, Sensitive_Acc : 17.092, Sensitive_Auc : 0.961, Mean auc: 0.633, Run Time : 100.94 sec
INFO:root:2024-03-28 22:15:31, Train, Epoch : 2, Step : 910, Loss : 1.21315, Acc : 0.675, Sensitive_Loss : 0.37499, Sensitive_Acc : 16.800, Run Time : 108.21 sec
INFO:root:2024-03-28 22:15:41, Train, Epoch : 2, Step : 920, Loss : 0.80560, Acc : 0.672, Sensitive_Loss : 0.38312, Sensitive_Acc : 17.100, Run Time : 10.58 sec
INFO:root:2024-03-28 22:15:52, Train, Epoch : 2, Step : 930, Loss : 0.98429, Acc : 0.719, Sensitive_Loss : 0.31880, Sensitive_Acc : 18.400, Run Time : 10.99 sec
INFO:root:2024-03-28 22:16:04, Train, Epoch : 2, Step : 940, Loss : 0.96904, Acc : 0.650, Sensitive_Loss : 0.27788, Sensitive_Acc : 18.300, Run Time : 11.92 sec
INFO:root:2024-03-28 22:16:14, Train, Epoch : 2, Step : 950, Loss : 0.93638, Acc : 0.656, Sensitive_Loss : 0.34636, Sensitive_Acc : 17.600, Run Time : 10.20 sec
INFO:root:2024-03-28 22:16:25, Train, Epoch : 2, Step : 960, Loss : 1.23530, Acc : 0.613, Sensitive_Loss : 0.33811, Sensitive_Acc : 17.400, Run Time : 10.49 sec
INFO:root:2024-03-28 22:16:36, Train, Epoch : 2, Step : 970, Loss : 1.02514, Acc : 0.684, Sensitive_Loss : 0.36054, Sensitive_Acc : 16.700, Run Time : 11.18 sec
INFO:root:2024-03-28 22:16:47, Train, Epoch : 2, Step : 980, Loss : 1.03058, Acc : 0.681, Sensitive_Loss : 0.34291, Sensitive_Acc : 15.800, Run Time : 11.17 sec
INFO:root:2024-03-28 22:18:27
INFO:root:y_pred: [0.19718306 0.08798097 0.1757987  ... 0.20110247 0.4138249  0.3669709 ]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [1.19553348e-02 1.81919634e-01 9.98690665e-01 1.31265640e-01
 9.77533400e-01 7.74351582e-02 5.64500690e-01 3.63330871e-01
 2.89510190e-02 8.20869952e-02 4.43507731e-02 1.83691949e-01
 9.82305944e-01 3.84133197e-02 9.87475336e-01 9.94789839e-01
 5.36361150e-03 3.79331976e-01 9.34326530e-01 7.76806235e-01
 2.84289211e-01 6.62019849e-02 9.99955177e-01 9.72175837e-01
 2.79980689e-01 9.96542394e-01 7.37611577e-02 9.98560250e-01
 3.01592518e-02 7.47491479e-01 9.96808469e-01 4.57856268e-01
 3.20177563e-02 8.53770645e-04 3.14905256e-01 3.81159782e-02
 3.20771813e-01 2.44772673e-01 8.39174330e-01 5.15012980e-01
 4.79363576e-02 4.66920584e-02 8.67469490e-01 9.98219788e-01
 3.05540413e-01 5.35084009e-02 1.40405431e-01 5.38636707e-02
 9.22929347e-01 8.04655671e-01 9.44982409e-01 3.28040689e-01
 9.97307062e-01 7.74089336e-01 9.25824106e-01 7.86511898e-01
 7.61149287e-01 4.71521914e-02 9.94575024e-01 5.17085850e-01
 6.69789268e-03 6.89325333e-01 9.21590209e-01 9.65258837e-01
 9.58503306e-01 9.94198680e-01 1.69110313e-01 9.84566629e-01
 9.78680015e-01 9.54107463e-01 9.66677189e-01 8.35739523e-02
 4.73703332e-02 5.19464016e-01 5.27623117e-01 1.95865203e-02
 9.43818748e-01 8.20404943e-03 9.76377547e-01 5.28257847e-01
 7.00332165e-01 4.10243243e-01 1.13775171e-01 1.96971446e-01
 9.57456231e-01 6.76509261e-01 7.64875889e-01 8.37672889e-01
 5.76931685e-02 5.57059824e-01 3.24741565e-02 1.91418171e-01
 1.27257453e-02 6.47796094e-02 8.61022830e-01 5.64418495e-01
 4.39058334e-01 4.61768806e-02 7.76230335e-01 9.44799364e-01
 8.64061654e-01 1.56700775e-01 9.96885240e-01 3.52994710e-01
 2.14881495e-01 9.07569051e-01 1.34576544e-01 1.99198630e-02
 6.21657399e-03 1.12511955e-01 3.89497466e-02 9.91578102e-01
 1.59899428e-01 1.21594220e-02 7.70542383e-01 8.19545414e-04
 9.68940020e-01 6.61531985e-02 6.13297999e-01 3.58394487e-03
 4.12260741e-01 1.20225802e-01 9.94144738e-01 2.91752238e-02
 1.37658715e-01 9.98013616e-01 3.67932431e-02 4.81012493e-01
 3.98797810e-01 9.61214364e-01 3.32548469e-03 9.59017098e-01
 8.18107277e-02 9.98105407e-01 8.44771862e-01 7.23101556e-01
 2.91350663e-01 9.59559560e-01 6.26630262e-02 9.46970463e-01
 9.65138257e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-28 22:18:27, Dev, Step : 984, Loss : 1.00168, Acc : 0.757, Auc : 0.749, Sensitive_Loss : 0.36683, Sensitive_Acc : 16.454, Sensitive_Auc : 0.968, Mean auc: 0.749, Run Time : 96.06 sec
INFO:root:2024-03-28 22:18:28, Best, Step : 984, Loss : 1.00168, Acc : 0.757,Auc : 0.749, Best Auc : 0.749, Sensitive_Loss : 0.36683, Sensitive_Acc : 16.454, Sensitive_Auc : 0.968
INFO:root:2024-03-28 22:18:36, Train, Epoch : 3, Step : 990, Loss : 0.56023, Acc : 0.412, Sensitive_Loss : 0.24994, Sensitive_Acc : 7.900, Run Time : 7.10 sec
INFO:root:2024-03-28 22:18:48, Train, Epoch : 3, Step : 1000, Loss : 1.08480, Acc : 0.653, Sensitive_Loss : 0.30673, Sensitive_Acc : 16.800, Run Time : 12.69 sec
INFO:root:2024-03-28 22:20:26, Dev, Step : 1000, Loss : 0.99756, Acc : 0.746, Auc : 0.752, Sensitive_Loss : 0.36302, Sensitive_Acc : 16.355, Sensitive_Auc : 0.974, Mean auc: 0.752, Run Time : 97.47 sec
INFO:root:2024-03-28 22:20:27, Best, Step : 1000, Loss : 0.99756, Acc : 0.746, Auc : 0.752, Sensitive_Loss : 0.36302, Sensitive_Acc : 16.355, Sensitive_Auc : 0.974, Best Auc : 0.752
INFO:root:2024-03-28 22:20:35, Train, Epoch : 3, Step : 1010, Loss : 0.94617, Acc : 0.669, Sensitive_Loss : 0.31737, Sensitive_Acc : 18.500, Run Time : 106.81 sec
INFO:root:2024-03-28 22:20:47, Train, Epoch : 3, Step : 1020, Loss : 0.92469, Acc : 0.728, Sensitive_Loss : 0.31125, Sensitive_Acc : 14.300, Run Time : 12.14 sec
INFO:root:2024-03-28 22:21:00, Train, Epoch : 3, Step : 1030, Loss : 0.93193, Acc : 0.653, Sensitive_Loss : 0.33160, Sensitive_Acc : 15.100, Run Time : 12.17 sec
INFO:root:2024-03-28 22:21:12, Train, Epoch : 3, Step : 1040, Loss : 0.89099, Acc : 0.694, Sensitive_Loss : 0.26025, Sensitive_Acc : 16.500, Run Time : 12.91 sec
INFO:root:2024-03-28 22:21:23, Train, Epoch : 3, Step : 1050, Loss : 0.94069, Acc : 0.666, Sensitive_Loss : 0.29376, Sensitive_Acc : 17.200, Run Time : 10.95 sec
INFO:root:2024-03-28 22:21:35, Train, Epoch : 3, Step : 1060, Loss : 0.96679, Acc : 0.700, Sensitive_Loss : 0.28080, Sensitive_Acc : 17.900, Run Time : 11.14 sec
INFO:root:2024-03-28 22:21:45, Train, Epoch : 3, Step : 1070, Loss : 0.81098, Acc : 0.700, Sensitive_Loss : 0.30102, Sensitive_Acc : 16.000, Run Time : 10.73 sec
INFO:root:2024-03-28 22:21:56, Train, Epoch : 3, Step : 1080, Loss : 0.69631, Acc : 0.681, Sensitive_Loss : 0.32290, Sensitive_Acc : 15.500, Run Time : 10.56 sec
INFO:root:2024-03-28 22:22:06, Train, Epoch : 3, Step : 1090, Loss : 0.78656, Acc : 0.706, Sensitive_Loss : 0.35668, Sensitive_Acc : 15.900, Run Time : 10.57 sec
INFO:root:2024-03-28 22:22:20, Train, Epoch : 3, Step : 1100, Loss : 0.77474, Acc : 0.697, Sensitive_Loss : 0.34158, Sensitive_Acc : 17.300, Run Time : 14.00 sec
INFO:root:2024-03-28 22:24:41, Dev, Step : 1100, Loss : 0.98595, Acc : 0.773, Auc : 0.760, Sensitive_Loss : 0.38593, Sensitive_Acc : 16.468, Sensitive_Auc : 0.980, Mean auc: 0.760, Run Time : 141.03 sec
INFO:root:2024-03-28 22:24:43, Best, Step : 1100, Loss : 0.98595, Acc : 0.773, Auc : 0.760, Sensitive_Loss : 0.38593, Sensitive_Acc : 16.468, Sensitive_Auc : 0.980, Best Auc : 0.760
INFO:root:2024-03-28 22:24:52, Train, Epoch : 3, Step : 1110, Loss : 0.96085, Acc : 0.697, Sensitive_Loss : 0.30174, Sensitive_Acc : 16.400, Run Time : 151.31 sec
INFO:root:2024-03-28 22:25:02, Train, Epoch : 3, Step : 1120, Loss : 0.95142, Acc : 0.688, Sensitive_Loss : 0.32479, Sensitive_Acc : 16.900, Run Time : 10.02 sec
INFO:root:2024-03-28 22:25:11, Train, Epoch : 3, Step : 1130, Loss : 0.95178, Acc : 0.694, Sensitive_Loss : 0.30741, Sensitive_Acc : 15.500, Run Time : 9.59 sec
INFO:root:2024-03-28 22:25:22, Train, Epoch : 3, Step : 1140, Loss : 0.82508, Acc : 0.703, Sensitive_Loss : 0.29437, Sensitive_Acc : 15.800, Run Time : 10.36 sec
INFO:root:2024-03-28 22:25:32, Train, Epoch : 3, Step : 1150, Loss : 0.97681, Acc : 0.662, Sensitive_Loss : 0.35616, Sensitive_Acc : 18.000, Run Time : 10.67 sec
INFO:root:2024-03-28 22:25:42, Train, Epoch : 3, Step : 1160, Loss : 0.86121, Acc : 0.725, Sensitive_Loss : 0.31636, Sensitive_Acc : 16.200, Run Time : 9.71 sec
INFO:root:2024-03-28 22:25:53, Train, Epoch : 3, Step : 1170, Loss : 0.91148, Acc : 0.719, Sensitive_Loss : 0.36203, Sensitive_Acc : 15.100, Run Time : 11.02 sec
INFO:root:2024-03-28 22:26:06, Train, Epoch : 3, Step : 1180, Loss : 0.96693, Acc : 0.694, Sensitive_Loss : 0.29481, Sensitive_Acc : 17.600, Run Time : 12.49 sec
INFO:root:2024-03-28 22:26:15, Train, Epoch : 3, Step : 1190, Loss : 0.86708, Acc : 0.728, Sensitive_Loss : 0.28166, Sensitive_Acc : 17.200, Run Time : 9.53 sec
INFO:root:2024-03-28 22:26:25, Train, Epoch : 3, Step : 1200, Loss : 1.14023, Acc : 0.681, Sensitive_Loss : 0.30118, Sensitive_Acc : 18.500, Run Time : 10.26 sec
INFO:root:2024-03-28 22:28:43, Dev, Step : 1200, Loss : 0.95721, Acc : 0.746, Auc : 0.774, Sensitive_Loss : 0.34530, Sensitive_Acc : 16.383, Sensitive_Auc : 0.981, Mean auc: 0.774, Run Time : 137.85 sec
INFO:root:2024-03-28 22:28:44, Best, Step : 1200, Loss : 0.95721, Acc : 0.746, Auc : 0.774, Sensitive_Loss : 0.34530, Sensitive_Acc : 16.383, Sensitive_Auc : 0.981, Best Auc : 0.774
INFO:root:2024-03-28 22:28:51, Train, Epoch : 3, Step : 1210, Loss : 0.81457, Acc : 0.669, Sensitive_Loss : 0.30468, Sensitive_Acc : 17.400, Run Time : 145.34 sec
INFO:root:2024-03-28 22:29:01, Train, Epoch : 3, Step : 1220, Loss : 0.87257, Acc : 0.691, Sensitive_Loss : 0.31865, Sensitive_Acc : 16.700, Run Time : 10.50 sec
INFO:root:2024-03-28 22:29:14, Train, Epoch : 3, Step : 1230, Loss : 0.92182, Acc : 0.684, Sensitive_Loss : 0.30891, Sensitive_Acc : 16.200, Run Time : 12.62 sec
INFO:root:2024-03-28 22:29:23, Train, Epoch : 3, Step : 1240, Loss : 0.84297, Acc : 0.725, Sensitive_Loss : 0.25826, Sensitive_Acc : 18.100, Run Time : 9.39 sec
INFO:root:2024-03-28 22:29:33, Train, Epoch : 3, Step : 1250, Loss : 1.11901, Acc : 0.703, Sensitive_Loss : 0.31581, Sensitive_Acc : 16.600, Run Time : 9.59 sec
INFO:root:2024-03-28 22:29:43, Train, Epoch : 3, Step : 1260, Loss : 0.85812, Acc : 0.688, Sensitive_Loss : 0.25919, Sensitive_Acc : 16.700, Run Time : 10.32 sec
INFO:root:2024-03-28 22:29:53, Train, Epoch : 3, Step : 1270, Loss : 0.90296, Acc : 0.688, Sensitive_Loss : 0.25660, Sensitive_Acc : 15.500, Run Time : 9.71 sec
INFO:root:2024-03-28 22:30:02, Train, Epoch : 3, Step : 1280, Loss : 0.83952, Acc : 0.647, Sensitive_Loss : 0.28960, Sensitive_Acc : 18.700, Run Time : 9.56 sec
INFO:root:2024-03-28 22:30:16, Train, Epoch : 3, Step : 1290, Loss : 1.07608, Acc : 0.688, Sensitive_Loss : 0.29305, Sensitive_Acc : 15.900, Run Time : 13.28 sec
INFO:root:2024-03-28 22:30:25, Train, Epoch : 3, Step : 1300, Loss : 0.84000, Acc : 0.703, Sensitive_Loss : 0.29496, Sensitive_Acc : 15.900, Run Time : 9.75 sec
INFO:root:2024-03-28 22:32:40, Dev, Step : 1300, Loss : 0.95414, Acc : 0.736, Auc : 0.770, Sensitive_Loss : 0.32734, Sensitive_Acc : 16.723, Sensitive_Auc : 0.984, Mean auc: 0.770, Run Time : 134.64 sec
INFO:root:2024-03-28 22:32:48, Train, Epoch : 3, Step : 1310, Loss : 0.86460, Acc : 0.713, Sensitive_Loss : 0.31728, Sensitive_Acc : 16.700, Run Time : 142.41 sec
INFO:root:2024-03-28 22:32:59, Train, Epoch : 3, Step : 1320, Loss : 1.01889, Acc : 0.681, Sensitive_Loss : 0.30315, Sensitive_Acc : 17.300, Run Time : 11.36 sec
INFO:root:2024-03-28 22:33:09, Train, Epoch : 3, Step : 1330, Loss : 0.88957, Acc : 0.741, Sensitive_Loss : 0.29580, Sensitive_Acc : 16.300, Run Time : 9.85 sec
INFO:root:2024-03-28 22:33:19, Train, Epoch : 3, Step : 1340, Loss : 0.87175, Acc : 0.688, Sensitive_Loss : 0.33160, Sensitive_Acc : 16.900, Run Time : 9.46 sec
INFO:root:2024-03-28 22:33:28, Train, Epoch : 3, Step : 1350, Loss : 0.87601, Acc : 0.700, Sensitive_Loss : 0.25522, Sensitive_Acc : 17.500, Run Time : 9.70 sec
INFO:root:2024-03-28 22:33:38, Train, Epoch : 3, Step : 1360, Loss : 0.84057, Acc : 0.719, Sensitive_Loss : 0.31002, Sensitive_Acc : 15.600, Run Time : 9.51 sec
INFO:root:2024-03-28 22:33:47, Train, Epoch : 3, Step : 1370, Loss : 0.93504, Acc : 0.697, Sensitive_Loss : 0.27332, Sensitive_Acc : 19.000, Run Time : 9.25 sec
INFO:root:2024-03-28 22:33:57, Train, Epoch : 3, Step : 1380, Loss : 0.74061, Acc : 0.697, Sensitive_Loss : 0.27557, Sensitive_Acc : 16.900, Run Time : 10.27 sec
INFO:root:2024-03-28 22:34:07, Train, Epoch : 3, Step : 1390, Loss : 0.89973, Acc : 0.719, Sensitive_Loss : 0.27260, Sensitive_Acc : 15.600, Run Time : 10.14 sec
INFO:root:2024-03-28 22:34:17, Train, Epoch : 3, Step : 1400, Loss : 0.94941, Acc : 0.666, Sensitive_Loss : 0.31124, Sensitive_Acc : 16.200, Run Time : 9.89 sec
INFO:root:2024-03-28 22:36:34, Dev, Step : 1400, Loss : 0.94978, Acc : 0.761, Auc : 0.775, Sensitive_Loss : 0.33198, Sensitive_Acc : 16.468, Sensitive_Auc : 0.983, Mean auc: 0.775, Run Time : 137.08 sec
INFO:root:2024-03-28 22:36:35, Best, Step : 1400, Loss : 0.94978, Acc : 0.761, Auc : 0.775, Sensitive_Loss : 0.33198, Sensitive_Acc : 16.468, Sensitive_Auc : 0.983, Best Auc : 0.775
INFO:root:2024-03-28 22:36:42, Train, Epoch : 3, Step : 1410, Loss : 0.81225, Acc : 0.703, Sensitive_Loss : 0.29767, Sensitive_Acc : 14.300, Run Time : 144.86 sec
INFO:root:2024-03-28 22:36:52, Train, Epoch : 3, Step : 1420, Loss : 0.96620, Acc : 0.738, Sensitive_Loss : 0.34920, Sensitive_Acc : 17.200, Run Time : 9.91 sec
INFO:root:2024-03-28 22:37:02, Train, Epoch : 3, Step : 1430, Loss : 0.73492, Acc : 0.713, Sensitive_Loss : 0.30522, Sensitive_Acc : 17.100, Run Time : 9.98 sec
INFO:root:2024-03-28 22:37:12, Train, Epoch : 3, Step : 1440, Loss : 0.93874, Acc : 0.713, Sensitive_Loss : 0.27728, Sensitive_Acc : 16.200, Run Time : 9.96 sec
INFO:root:2024-03-28 22:37:21, Train, Epoch : 3, Step : 1450, Loss : 0.72446, Acc : 0.716, Sensitive_Loss : 0.26571, Sensitive_Acc : 14.400, Run Time : 9.38 sec
INFO:root:2024-03-28 22:37:32, Train, Epoch : 3, Step : 1460, Loss : 0.76963, Acc : 0.725, Sensitive_Loss : 0.27883, Sensitive_Acc : 15.800, Run Time : 10.97 sec
INFO:root:2024-03-28 22:37:43, Train, Epoch : 3, Step : 1470, Loss : 0.82575, Acc : 0.722, Sensitive_Loss : 0.32465, Sensitive_Acc : 15.400, Run Time : 10.34 sec
INFO:root:2024-03-28 22:39:29
INFO:root:y_pred: [0.15169957 0.09134612 0.22943676 ... 0.25747156 0.418607   0.31627777]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [4.72768908e-03 2.06830412e-01 9.96608496e-01 6.84647635e-02
 9.60997760e-01 3.01727131e-02 3.80530387e-01 2.44694009e-01
 2.11778861e-02 3.95229831e-02 5.13861440e-02 5.93435168e-02
 9.71488595e-01 4.85883579e-02 9.92398798e-01 9.97282028e-01
 1.06554618e-03 5.87471783e-01 9.74212050e-01 7.71122754e-01
 1.11576930e-01 1.38938241e-02 9.99999166e-01 9.55243766e-01
 2.56722510e-01 9.80018318e-01 3.27008516e-02 9.99469697e-01
 3.01392581e-02 7.71584928e-01 9.98031080e-01 6.89308584e-01
 1.40953325e-02 4.18161391e-04 3.41545641e-01 5.14599122e-02
 9.80136395e-02 2.88415939e-01 8.41810524e-01 7.33910799e-01
 2.25801542e-02 5.01048043e-02 9.43100691e-01 9.99172628e-01
 4.49191004e-01 3.80495489e-02 9.02142227e-02 2.85734069e-02
 9.53333378e-01 8.39759588e-01 8.85234356e-01 2.67009914e-01
 9.99456704e-01 9.40452337e-01 9.34378564e-01 8.73893142e-01
 7.38924623e-01 4.12081033e-02 9.98598874e-01 1.51091799e-01
 2.21430813e-03 4.67710793e-01 9.01777565e-01 9.48338866e-01
 9.63777244e-01 9.96145010e-01 6.99599832e-02 9.95552957e-01
 9.88585293e-01 9.52251673e-01 9.99161839e-01 4.68888469e-02
 1.72269307e-02 6.81252718e-01 8.48566711e-01 1.46556171e-02
 9.51274514e-01 4.04767366e-03 9.92963374e-01 1.76224634e-01
 5.99358559e-01 2.65193224e-01 1.02910884e-01 1.90787062e-01
 9.45502639e-01 7.11941242e-01 7.01387167e-01 6.72717512e-01
 3.63393165e-02 3.80479127e-01 3.44654918e-03 1.31246835e-01
 6.26729801e-03 3.34534980e-02 9.34308887e-01 7.57150412e-01
 5.14252841e-01 9.76887345e-03 6.76499307e-01 9.55017388e-01
 9.31057990e-01 1.33479387e-01 9.99071717e-01 1.93794250e-01
 4.01918054e-01 9.69229519e-01 1.24176495e-01 1.26046659e-02
 2.88440194e-03 5.77458404e-02 1.30983889e-02 9.91062880e-01
 7.91659206e-02 6.86482154e-03 3.60544175e-01 5.71566517e-04
 9.75512862e-01 1.26400320e-02 3.77271086e-01 2.13924097e-03
 2.51599312e-01 6.58091754e-02 9.94910657e-01 4.66400664e-03
 2.28325561e-01 9.99148846e-01 1.79927051e-02 8.45924616e-01
 4.18313116e-01 9.76076007e-01 2.83084251e-03 9.84666824e-01
 5.29838689e-02 9.99267280e-01 9.71996665e-01 6.22923493e-01
 1.76759183e-01 9.70998228e-01 1.49194980e-02 9.44712758e-01
 9.85238671e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-28 22:39:29, Dev, Step : 1476, Loss : 0.93804, Acc : 0.745, Auc : 0.781, Sensitive_Loss : 0.30823, Sensitive_Acc : 16.837, Sensitive_Auc : 0.985, Mean auc: 0.781, Run Time : 100.67 sec
INFO:root:2024-03-28 22:39:30, Best, Step : 1476, Loss : 0.93804, Acc : 0.745,Auc : 0.781, Best Auc : 0.781, Sensitive_Loss : 0.30823, Sensitive_Acc : 16.837, Sensitive_Auc : 0.985
INFO:root:2024-03-28 22:39:35, Train, Epoch : 4, Step : 1480, Loss : 0.29903, Acc : 0.294, Sensitive_Loss : 0.14342, Sensitive_Acc : 7.800, Run Time : 4.49 sec
INFO:root:2024-03-28 22:39:46, Train, Epoch : 4, Step : 1490, Loss : 0.67864, Acc : 0.719, Sensitive_Loss : 0.26905, Sensitive_Acc : 16.300, Run Time : 10.52 sec
INFO:root:2024-03-28 22:39:55, Train, Epoch : 4, Step : 1500, Loss : 0.94757, Acc : 0.731, Sensitive_Loss : 0.23222, Sensitive_Acc : 16.900, Run Time : 9.70 sec
INFO:root:2024-03-28 22:41:32, Dev, Step : 1500, Loss : 0.93751, Acc : 0.778, Auc : 0.784, Sensitive_Loss : 0.31210, Sensitive_Acc : 16.837, Sensitive_Auc : 0.984, Mean auc: 0.784, Run Time : 97.03 sec
INFO:root:2024-03-28 22:41:33, Best, Step : 1500, Loss : 0.93751, Acc : 0.778, Auc : 0.784, Sensitive_Loss : 0.31210, Sensitive_Acc : 16.837, Sensitive_Auc : 0.984, Best Auc : 0.784
INFO:root:2024-03-28 22:41:41, Train, Epoch : 4, Step : 1510, Loss : 0.88287, Acc : 0.747, Sensitive_Loss : 0.24589, Sensitive_Acc : 16.900, Run Time : 105.59 sec
INFO:root:2024-03-28 22:41:52, Train, Epoch : 4, Step : 1520, Loss : 0.84079, Acc : 0.747, Sensitive_Loss : 0.25275, Sensitive_Acc : 16.000, Run Time : 11.01 sec
INFO:root:2024-03-28 22:42:05, Train, Epoch : 4, Step : 1530, Loss : 0.79713, Acc : 0.713, Sensitive_Loss : 0.29865, Sensitive_Acc : 16.700, Run Time : 13.23 sec
INFO:root:2024-03-28 22:42:17, Train, Epoch : 4, Step : 1540, Loss : 0.76830, Acc : 0.728, Sensitive_Loss : 0.23911, Sensitive_Acc : 16.200, Run Time : 12.06 sec
INFO:root:2024-03-28 22:42:28, Train, Epoch : 4, Step : 1550, Loss : 0.97550, Acc : 0.669, Sensitive_Loss : 0.31569, Sensitive_Acc : 17.800, Run Time : 11.15 sec
INFO:root:2024-03-28 22:42:39, Train, Epoch : 4, Step : 1560, Loss : 0.72604, Acc : 0.734, Sensitive_Loss : 0.26293, Sensitive_Acc : 16.200, Run Time : 10.69 sec
INFO:root:2024-03-28 22:42:52, Train, Epoch : 4, Step : 1570, Loss : 0.95589, Acc : 0.694, Sensitive_Loss : 0.29092, Sensitive_Acc : 17.400, Run Time : 13.05 sec
INFO:root:2024-03-28 22:43:03, Train, Epoch : 4, Step : 1580, Loss : 0.89835, Acc : 0.709, Sensitive_Loss : 0.26279, Sensitive_Acc : 17.300, Run Time : 10.65 sec
INFO:root:2024-03-28 22:43:14, Train, Epoch : 4, Step : 1590, Loss : 0.83510, Acc : 0.725, Sensitive_Loss : 0.27811, Sensitive_Acc : 18.500, Run Time : 11.31 sec
INFO:root:2024-03-28 22:43:27, Train, Epoch : 4, Step : 1600, Loss : 0.99355, Acc : 0.691, Sensitive_Loss : 0.29106, Sensitive_Acc : 18.300, Run Time : 12.77 sec
INFO:root:2024-03-28 22:45:43, Dev, Step : 1600, Loss : 0.92849, Acc : 0.732, Auc : 0.786, Sensitive_Loss : 0.32097, Sensitive_Acc : 16.567, Sensitive_Auc : 0.986, Mean auc: 0.786, Run Time : 136.09 sec
INFO:root:2024-03-28 22:45:44, Best, Step : 1600, Loss : 0.92849, Acc : 0.732, Auc : 0.786, Sensitive_Loss : 0.32097, Sensitive_Acc : 16.567, Sensitive_Auc : 0.986, Best Auc : 0.786
INFO:root:2024-03-28 22:45:52, Train, Epoch : 4, Step : 1610, Loss : 0.94476, Acc : 0.694, Sensitive_Loss : 0.27916, Sensitive_Acc : 16.700, Run Time : 145.17 sec
INFO:root:2024-03-28 22:46:02, Train, Epoch : 4, Step : 1620, Loss : 0.83146, Acc : 0.688, Sensitive_Loss : 0.28460, Sensitive_Acc : 17.200, Run Time : 9.95 sec
INFO:root:2024-03-28 22:46:12, Train, Epoch : 4, Step : 1630, Loss : 0.71206, Acc : 0.694, Sensitive_Loss : 0.30018, Sensitive_Acc : 16.200, Run Time : 9.91 sec
INFO:root:2024-03-28 22:46:22, Train, Epoch : 4, Step : 1640, Loss : 0.80973, Acc : 0.716, Sensitive_Loss : 0.27752, Sensitive_Acc : 15.500, Run Time : 10.27 sec
INFO:root:2024-03-28 22:46:32, Train, Epoch : 4, Step : 1650, Loss : 0.79710, Acc : 0.694, Sensitive_Loss : 0.25946, Sensitive_Acc : 16.200, Run Time : 9.43 sec
INFO:root:2024-03-28 22:46:41, Train, Epoch : 4, Step : 1660, Loss : 0.85684, Acc : 0.731, Sensitive_Loss : 0.24653, Sensitive_Acc : 16.400, Run Time : 9.93 sec
INFO:root:2024-03-28 22:46:51, Train, Epoch : 4, Step : 1670, Loss : 0.73044, Acc : 0.706, Sensitive_Loss : 0.24837, Sensitive_Acc : 16.600, Run Time : 9.92 sec
INFO:root:2024-03-28 22:47:01, Train, Epoch : 4, Step : 1680, Loss : 0.75307, Acc : 0.681, Sensitive_Loss : 0.29291, Sensitive_Acc : 16.100, Run Time : 10.04 sec
INFO:root:2024-03-28 22:47:12, Train, Epoch : 4, Step : 1690, Loss : 0.99333, Acc : 0.703, Sensitive_Loss : 0.29148, Sensitive_Acc : 15.700, Run Time : 10.37 sec
INFO:root:2024-03-28 22:47:24, Train, Epoch : 4, Step : 1700, Loss : 0.89657, Acc : 0.716, Sensitive_Loss : 0.33718, Sensitive_Acc : 17.700, Run Time : 12.37 sec
INFO:root:2024-03-28 22:49:43, Dev, Step : 1700, Loss : 0.96564, Acc : 0.765, Auc : 0.767, Sensitive_Loss : 0.33053, Sensitive_Acc : 16.638, Sensitive_Auc : 0.990, Mean auc: 0.767, Run Time : 138.54 sec
INFO:root:2024-03-28 22:49:49, Train, Epoch : 4, Step : 1710, Loss : 0.74437, Acc : 0.697, Sensitive_Loss : 0.23076, Sensitive_Acc : 14.400, Run Time : 145.24 sec
INFO:root:2024-03-28 22:49:59, Train, Epoch : 4, Step : 1720, Loss : 0.73839, Acc : 0.747, Sensitive_Loss : 0.28598, Sensitive_Acc : 16.000, Run Time : 9.45 sec
INFO:root:2024-03-28 22:50:09, Train, Epoch : 4, Step : 1730, Loss : 0.95981, Acc : 0.713, Sensitive_Loss : 0.28196, Sensitive_Acc : 15.100, Run Time : 10.40 sec
INFO:root:2024-03-28 22:50:19, Train, Epoch : 4, Step : 1740, Loss : 0.66203, Acc : 0.722, Sensitive_Loss : 0.24850, Sensitive_Acc : 17.500, Run Time : 9.48 sec
INFO:root:2024-03-28 22:50:28, Train, Epoch : 4, Step : 1750, Loss : 0.92312, Acc : 0.713, Sensitive_Loss : 0.27185, Sensitive_Acc : 18.400, Run Time : 9.73 sec
INFO:root:2024-03-28 22:50:38, Train, Epoch : 4, Step : 1760, Loss : 0.86989, Acc : 0.722, Sensitive_Loss : 0.28867, Sensitive_Acc : 17.000, Run Time : 9.62 sec
INFO:root:2024-03-28 22:50:48, Train, Epoch : 4, Step : 1770, Loss : 0.74664, Acc : 0.725, Sensitive_Loss : 0.30566, Sensitive_Acc : 17.400, Run Time : 10.43 sec
INFO:root:2024-03-28 22:50:58, Train, Epoch : 4, Step : 1780, Loss : 1.10447, Acc : 0.691, Sensitive_Loss : 0.28838, Sensitive_Acc : 17.400, Run Time : 9.16 sec
INFO:root:2024-03-28 22:51:07, Train, Epoch : 4, Step : 1790, Loss : 0.75347, Acc : 0.744, Sensitive_Loss : 0.35623, Sensitive_Acc : 19.700, Run Time : 9.70 sec
INFO:root:2024-03-28 22:51:17, Train, Epoch : 4, Step : 1800, Loss : 0.99722, Acc : 0.691, Sensitive_Loss : 0.24508, Sensitive_Acc : 15.500, Run Time : 9.38 sec
INFO:root:2024-03-28 22:53:31, Dev, Step : 1800, Loss : 0.94472, Acc : 0.771, Auc : 0.777, Sensitive_Loss : 0.30292, Sensitive_Acc : 16.766, Sensitive_Auc : 0.988, Mean auc: 0.777, Run Time : 133.80 sec
INFO:root:2024-03-28 22:53:37, Train, Epoch : 4, Step : 1810, Loss : 0.83939, Acc : 0.666, Sensitive_Loss : 0.24780, Sensitive_Acc : 15.900, Run Time : 140.46 sec
INFO:root:2024-03-28 22:53:47, Train, Epoch : 4, Step : 1820, Loss : 0.56606, Acc : 0.719, Sensitive_Loss : 0.23334, Sensitive_Acc : 16.200, Run Time : 10.27 sec
INFO:root:2024-03-28 22:53:59, Train, Epoch : 4, Step : 1830, Loss : 0.84512, Acc : 0.756, Sensitive_Loss : 0.36776, Sensitive_Acc : 14.900, Run Time : 11.64 sec
INFO:root:2024-03-28 22:54:09, Train, Epoch : 4, Step : 1840, Loss : 0.91466, Acc : 0.747, Sensitive_Loss : 0.24697, Sensitive_Acc : 17.800, Run Time : 9.67 sec
INFO:root:2024-03-28 22:54:18, Train, Epoch : 4, Step : 1850, Loss : 0.81898, Acc : 0.753, Sensitive_Loss : 0.30988, Sensitive_Acc : 14.800, Run Time : 9.66 sec
INFO:root:2024-03-28 22:54:29, Train, Epoch : 4, Step : 1860, Loss : 0.97420, Acc : 0.744, Sensitive_Loss : 0.30126, Sensitive_Acc : 16.600, Run Time : 10.61 sec
INFO:root:2024-03-28 22:54:39, Train, Epoch : 4, Step : 1870, Loss : 0.71860, Acc : 0.744, Sensitive_Loss : 0.25299, Sensitive_Acc : 16.500, Run Time : 9.69 sec
INFO:root:2024-03-28 22:54:48, Train, Epoch : 4, Step : 1880, Loss : 0.78728, Acc : 0.691, Sensitive_Loss : 0.31325, Sensitive_Acc : 17.800, Run Time : 9.67 sec
INFO:root:2024-03-28 22:55:00, Train, Epoch : 4, Step : 1890, Loss : 0.70366, Acc : 0.750, Sensitive_Loss : 0.25240, Sensitive_Acc : 17.200, Run Time : 11.94 sec
INFO:root:2024-03-28 22:55:11, Train, Epoch : 4, Step : 1900, Loss : 0.88035, Acc : 0.716, Sensitive_Loss : 0.30384, Sensitive_Acc : 17.200, Run Time : 10.59 sec
INFO:root:2024-03-28 22:57:23, Dev, Step : 1900, Loss : 0.93197, Acc : 0.694, Auc : 0.791, Sensitive_Loss : 0.29301, Sensitive_Acc : 16.851, Sensitive_Auc : 0.992, Mean auc: 0.791, Run Time : 131.61 sec
INFO:root:2024-03-28 22:57:23, Best, Step : 1900, Loss : 0.93197, Acc : 0.694, Auc : 0.791, Sensitive_Loss : 0.29301, Sensitive_Acc : 16.851, Sensitive_Auc : 0.992, Best Auc : 0.791
INFO:root:2024-03-28 22:57:30, Train, Epoch : 4, Step : 1910, Loss : 0.58625, Acc : 0.678, Sensitive_Loss : 0.23867, Sensitive_Acc : 15.100, Run Time : 139.10 sec
INFO:root:2024-03-28 22:57:41, Train, Epoch : 4, Step : 1920, Loss : 0.96680, Acc : 0.697, Sensitive_Loss : 0.26764, Sensitive_Acc : 15.600, Run Time : 10.62 sec
INFO:root:2024-03-28 22:57:53, Train, Epoch : 4, Step : 1930, Loss : 0.80355, Acc : 0.688, Sensitive_Loss : 0.31977, Sensitive_Acc : 18.800, Run Time : 12.21 sec
INFO:root:2024-03-28 22:58:02, Train, Epoch : 4, Step : 1940, Loss : 1.06341, Acc : 0.719, Sensitive_Loss : 0.26312, Sensitive_Acc : 17.300, Run Time : 9.41 sec
INFO:root:2024-03-28 22:58:13, Train, Epoch : 4, Step : 1950, Loss : 1.07062, Acc : 0.697, Sensitive_Loss : 0.21599, Sensitive_Acc : 16.600, Run Time : 10.50 sec
INFO:root:2024-03-28 22:58:25, Train, Epoch : 4, Step : 1960, Loss : 0.77697, Acc : 0.731, Sensitive_Loss : 0.24410, Sensitive_Acc : 15.500, Run Time : 12.35 sec
INFO:root:2024-03-28 23:00:08
INFO:root:y_pred: [0.22016442 0.09835811 0.36437064 ... 0.37603807 0.4863614  0.23645638]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [1.75478146e-03 1.36859775e-01 9.92942691e-01 5.58951534e-02
 9.71286356e-01 2.17018630e-02 2.50199795e-01 1.06274635e-01
 2.17887424e-02 7.87746012e-02 4.02111039e-02 8.61703977e-02
 9.81179774e-01 4.27550003e-02 9.93699908e-01 9.97033477e-01
 6.08069822e-04 6.38068616e-01 9.88906622e-01 7.41926432e-01
 3.79277244e-02 1.00684967e-02 9.99996305e-01 9.85106468e-01
 4.88162637e-01 9.58276510e-01 1.08412867e-02 9.99764144e-01
 2.86254808e-02 8.68363440e-01 9.98774111e-01 7.36344814e-01
 7.97462557e-03 2.50401063e-04 3.18202764e-01 1.63219739e-02
 4.57225256e-02 3.04599375e-01 8.75912070e-01 8.55989695e-01
 4.02053595e-02 2.18643583e-02 9.74749923e-01 9.99702632e-01
 4.82421041e-01 2.97887046e-02 3.47495824e-02 3.78962085e-02
 9.89595473e-01 8.39927912e-01 9.57624555e-01 5.64490795e-01
 9.99352753e-01 9.68178570e-01 9.68788862e-01 8.40154827e-01
 6.72883868e-01 3.30222212e-02 9.99623060e-01 1.62745163e-01
 1.88499631e-03 5.23338020e-01 9.17004347e-01 9.59987044e-01
 9.74346519e-01 9.96512234e-01 1.00834370e-01 9.94514823e-01
 9.94420230e-01 9.77297187e-01 9.96196032e-01 3.54139879e-02
 1.09289074e-02 7.17514634e-01 8.81881356e-01 9.92635638e-03
 9.23046947e-01 4.44340287e-03 9.95789587e-01 1.92753956e-01
 6.00682735e-01 3.37288201e-01 4.87803034e-02 1.19810991e-01
 9.73764718e-01 6.27469659e-01 8.10653985e-01 5.50016403e-01
 2.63235867e-02 3.33314985e-01 2.46131350e-03 1.56683147e-01
 1.13602681e-02 3.23479213e-02 9.52244997e-01 8.24049652e-01
 3.80522549e-01 1.17999520e-02 7.95923591e-01 9.50688601e-01
 9.53061819e-01 1.24778204e-01 9.99453723e-01 2.31362954e-01
 3.81202191e-01 9.86859858e-01 1.32547706e-01 6.15676399e-03
 5.17137256e-03 2.90718190e-02 1.99562162e-02 9.89144146e-01
 7.22795203e-02 5.82648534e-03 5.01956284e-01 5.88970724e-04
 9.83597100e-01 3.56913172e-02 2.98204988e-01 1.07245753e-03
 2.51232386e-01 4.97575849e-02 9.97068942e-01 3.51213058e-03
 1.66249692e-01 9.99675393e-01 3.36878300e-02 8.55372667e-01
 3.53913397e-01 9.80940640e-01 3.08313826e-03 9.89130855e-01
 4.02267277e-02 9.99591410e-01 9.64009583e-01 4.79807109e-01
 1.22043073e-01 9.23737824e-01 7.72009650e-03 9.60396051e-01
 9.91625726e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-28 23:00:08, Dev, Step : 1968, Loss : 0.91882, Acc : 0.749, Auc : 0.788, Sensitive_Loss : 0.29281, Sensitive_Acc : 16.823, Sensitive_Auc : 0.992, Mean auc: 0.788, Run Time : 95.50 sec
INFO:root:2024-03-28 23:00:13, Train, Epoch : 5, Step : 1970, Loss : 0.15899, Acc : 0.156, Sensitive_Loss : 0.05846, Sensitive_Acc : 3.900, Run Time : 3.48 sec
INFO:root:2024-03-28 23:00:25, Train, Epoch : 5, Step : 1980, Loss : 0.88750, Acc : 0.719, Sensitive_Loss : 0.22900, Sensitive_Acc : 15.300, Run Time : 12.31 sec
INFO:root:2024-03-28 23:00:35, Train, Epoch : 5, Step : 1990, Loss : 0.79506, Acc : 0.738, Sensitive_Loss : 0.30378, Sensitive_Acc : 17.500, Run Time : 9.80 sec
INFO:root:2024-03-28 23:00:45, Train, Epoch : 5, Step : 2000, Loss : 0.84595, Acc : 0.738, Sensitive_Loss : 0.28850, Sensitive_Acc : 16.600, Run Time : 10.55 sec
INFO:root:2024-03-28 23:02:22, Dev, Step : 2000, Loss : 0.92763, Acc : 0.755, Auc : 0.783, Sensitive_Loss : 0.28793, Sensitive_Acc : 16.993, Sensitive_Auc : 0.992, Mean auc: 0.783, Run Time : 97.09 sec
INFO:root:2024-03-28 23:02:31, Train, Epoch : 5, Step : 2010, Loss : 0.63690, Acc : 0.719, Sensitive_Loss : 0.30255, Sensitive_Acc : 14.600, Run Time : 105.19 sec
INFO:root:2024-03-28 23:02:42, Train, Epoch : 5, Step : 2020, Loss : 0.71955, Acc : 0.753, Sensitive_Loss : 0.23803, Sensitive_Acc : 17.900, Run Time : 11.93 sec
INFO:root:2024-03-28 23:02:54, Train, Epoch : 5, Step : 2030, Loss : 0.78868, Acc : 0.734, Sensitive_Loss : 0.30194, Sensitive_Acc : 16.900, Run Time : 11.20 sec
INFO:root:2024-03-28 23:03:04, Train, Epoch : 5, Step : 2040, Loss : 0.88011, Acc : 0.709, Sensitive_Loss : 0.27074, Sensitive_Acc : 17.000, Run Time : 10.69 sec
INFO:root:2024-03-28 23:03:19, Train, Epoch : 5, Step : 2050, Loss : 0.71332, Acc : 0.728, Sensitive_Loss : 0.23445, Sensitive_Acc : 17.500, Run Time : 14.88 sec
INFO:root:2024-03-28 23:03:30, Train, Epoch : 5, Step : 2060, Loss : 0.66521, Acc : 0.741, Sensitive_Loss : 0.27205, Sensitive_Acc : 15.700, Run Time : 10.48 sec
INFO:root:2024-03-28 23:03:40, Train, Epoch : 5, Step : 2070, Loss : 0.69749, Acc : 0.741, Sensitive_Loss : 0.25380, Sensitive_Acc : 15.200, Run Time : 10.20 sec
INFO:root:2024-03-28 23:03:53, Train, Epoch : 5, Step : 2080, Loss : 0.72956, Acc : 0.722, Sensitive_Loss : 0.27420, Sensitive_Acc : 17.800, Run Time : 12.81 sec
INFO:root:2024-03-28 23:04:03, Train, Epoch : 5, Step : 2090, Loss : 0.95933, Acc : 0.709, Sensitive_Loss : 0.20356, Sensitive_Acc : 15.300, Run Time : 10.54 sec
INFO:root:2024-03-28 23:04:13, Train, Epoch : 5, Step : 2100, Loss : 0.65472, Acc : 0.738, Sensitive_Loss : 0.26456, Sensitive_Acc : 18.000, Run Time : 10.14 sec
INFO:root:2024-03-28 23:06:22, Dev, Step : 2100, Loss : 0.92413, Acc : 0.762, Auc : 0.786, Sensitive_Loss : 0.26666, Sensitive_Acc : 17.007, Sensitive_Auc : 0.992, Mean auc: 0.786, Run Time : 128.73 sec
INFO:root:2024-03-28 23:06:29, Train, Epoch : 5, Step : 2110, Loss : 0.66169, Acc : 0.734, Sensitive_Loss : 0.31810, Sensitive_Acc : 15.600, Run Time : 135.78 sec
INFO:root:2024-03-28 23:06:39, Train, Epoch : 5, Step : 2120, Loss : 0.78686, Acc : 0.762, Sensitive_Loss : 0.24811, Sensitive_Acc : 17.300, Run Time : 10.13 sec
INFO:root:2024-03-28 23:06:50, Train, Epoch : 5, Step : 2130, Loss : 0.73533, Acc : 0.728, Sensitive_Loss : 0.24076, Sensitive_Acc : 16.200, Run Time : 10.26 sec
INFO:root:2024-03-28 23:06:59, Train, Epoch : 5, Step : 2140, Loss : 0.75747, Acc : 0.719, Sensitive_Loss : 0.26140, Sensitive_Acc : 16.800, Run Time : 9.79 sec
INFO:root:2024-03-28 23:07:09, Train, Epoch : 5, Step : 2150, Loss : 0.82095, Acc : 0.719, Sensitive_Loss : 0.35317, Sensitive_Acc : 13.000, Run Time : 10.04 sec
INFO:root:2024-03-28 23:07:19, Train, Epoch : 5, Step : 2160, Loss : 0.80384, Acc : 0.728, Sensitive_Loss : 0.26566, Sensitive_Acc : 18.100, Run Time : 9.77 sec
INFO:root:2024-03-28 23:07:32, Train, Epoch : 5, Step : 2170, Loss : 0.91198, Acc : 0.734, Sensitive_Loss : 0.27186, Sensitive_Acc : 15.500, Run Time : 13.03 sec
INFO:root:2024-03-28 23:07:42, Train, Epoch : 5, Step : 2180, Loss : 0.67356, Acc : 0.700, Sensitive_Loss : 0.27117, Sensitive_Acc : 16.600, Run Time : 9.56 sec
INFO:root:2024-03-28 23:07:51, Train, Epoch : 5, Step : 2190, Loss : 0.76290, Acc : 0.703, Sensitive_Loss : 0.24911, Sensitive_Acc : 14.800, Run Time : 9.62 sec
INFO:root:2024-03-28 23:08:02, Train, Epoch : 5, Step : 2200, Loss : 0.71144, Acc : 0.750, Sensitive_Loss : 0.26485, Sensitive_Acc : 15.700, Run Time : 10.43 sec
INFO:root:2024-03-28 23:10:12, Dev, Step : 2200, Loss : 0.93787, Acc : 0.745, Auc : 0.784, Sensitive_Loss : 0.28881, Sensitive_Acc : 16.908, Sensitive_Auc : 0.991, Mean auc: 0.784, Run Time : 129.84 sec
INFO:root:2024-03-28 23:10:18, Train, Epoch : 5, Step : 2210, Loss : 0.68766, Acc : 0.769, Sensitive_Loss : 0.22925, Sensitive_Acc : 16.300, Run Time : 136.38 sec
INFO:root:2024-03-28 23:10:28, Train, Epoch : 5, Step : 2220, Loss : 0.89326, Acc : 0.703, Sensitive_Loss : 0.32884, Sensitive_Acc : 16.100, Run Time : 9.47 sec
INFO:root:2024-03-28 23:10:41, Train, Epoch : 5, Step : 2230, Loss : 0.76212, Acc : 0.681, Sensitive_Loss : 0.18106, Sensitive_Acc : 17.200, Run Time : 13.08 sec
INFO:root:2024-03-28 23:10:51, Train, Epoch : 5, Step : 2240, Loss : 0.66105, Acc : 0.750, Sensitive_Loss : 0.28024, Sensitive_Acc : 17.500, Run Time : 9.81 sec
INFO:root:2024-03-28 23:11:00, Train, Epoch : 5, Step : 2250, Loss : 0.90093, Acc : 0.703, Sensitive_Loss : 0.28712, Sensitive_Acc : 18.600, Run Time : 9.64 sec
INFO:root:2024-03-28 23:11:10, Train, Epoch : 5, Step : 2260, Loss : 0.81762, Acc : 0.719, Sensitive_Loss : 0.29796, Sensitive_Acc : 16.800, Run Time : 9.48 sec
INFO:root:2024-03-28 23:11:20, Train, Epoch : 5, Step : 2270, Loss : 0.68598, Acc : 0.744, Sensitive_Loss : 0.28549, Sensitive_Acc : 16.500, Run Time : 9.93 sec
INFO:root:2024-03-28 23:11:29, Train, Epoch : 5, Step : 2280, Loss : 0.96779, Acc : 0.700, Sensitive_Loss : 0.20554, Sensitive_Acc : 19.200, Run Time : 9.59 sec
INFO:root:2024-03-28 23:11:39, Train, Epoch : 5, Step : 2290, Loss : 0.58825, Acc : 0.747, Sensitive_Loss : 0.26055, Sensitive_Acc : 17.100, Run Time : 9.97 sec
INFO:root:2024-03-28 23:11:49, Train, Epoch : 5, Step : 2300, Loss : 0.87286, Acc : 0.728, Sensitive_Loss : 0.25610, Sensitive_Acc : 17.600, Run Time : 9.59 sec
INFO:root:2024-03-28 23:13:57, Dev, Step : 2300, Loss : 0.92725, Acc : 0.760, Auc : 0.784, Sensitive_Loss : 0.26699, Sensitive_Acc : 17.092, Sensitive_Auc : 0.995, Mean auc: 0.784, Run Time : 128.21 sec
INFO:root:2024-03-28 23:14:03, Train, Epoch : 5, Step : 2310, Loss : 0.68166, Acc : 0.750, Sensitive_Loss : 0.24313, Sensitive_Acc : 15.400, Run Time : 134.62 sec
INFO:root:2024-03-28 23:14:14, Train, Epoch : 5, Step : 2320, Loss : 0.96531, Acc : 0.741, Sensitive_Loss : 0.23941, Sensitive_Acc : 16.400, Run Time : 10.37 sec
INFO:root:2024-03-28 23:14:24, Train, Epoch : 5, Step : 2330, Loss : 0.79562, Acc : 0.772, Sensitive_Loss : 0.29117, Sensitive_Acc : 16.100, Run Time : 10.07 sec
INFO:root:2024-03-28 23:14:34, Train, Epoch : 5, Step : 2340, Loss : 0.73318, Acc : 0.713, Sensitive_Loss : 0.29875, Sensitive_Acc : 17.400, Run Time : 10.00 sec
INFO:root:2024-03-28 23:14:44, Train, Epoch : 5, Step : 2350, Loss : 0.82432, Acc : 0.713, Sensitive_Loss : 0.26018, Sensitive_Acc : 16.400, Run Time : 9.72 sec
INFO:root:2024-03-28 23:14:52, Train, Epoch : 5, Step : 2360, Loss : 0.89913, Acc : 0.719, Sensitive_Loss : 0.29423, Sensitive_Acc : 17.500, Run Time : 8.73 sec
INFO:root:2024-03-28 23:15:02, Train, Epoch : 5, Step : 2370, Loss : 1.05964, Acc : 0.716, Sensitive_Loss : 0.22015, Sensitive_Acc : 17.500, Run Time : 9.72 sec
INFO:root:2024-03-28 23:15:12, Train, Epoch : 5, Step : 2380, Loss : 0.71477, Acc : 0.744, Sensitive_Loss : 0.21506, Sensitive_Acc : 16.600, Run Time : 9.67 sec
INFO:root:2024-03-28 23:15:25, Train, Epoch : 5, Step : 2390, Loss : 0.85808, Acc : 0.734, Sensitive_Loss : 0.38075, Sensitive_Acc : 16.900, Run Time : 13.35 sec
INFO:root:2024-03-28 23:15:35, Train, Epoch : 5, Step : 2400, Loss : 0.95380, Acc : 0.734, Sensitive_Loss : 0.22394, Sensitive_Acc : 14.800, Run Time : 10.40 sec
INFO:root:2024-03-28 23:17:42, Dev, Step : 2400, Loss : 0.93623, Acc : 0.790, Auc : 0.782, Sensitive_Loss : 0.26402, Sensitive_Acc : 16.993, Sensitive_Auc : 0.993, Mean auc: 0.782, Run Time : 126.68 sec
INFO:root:2024-03-28 23:17:49, Train, Epoch : 5, Step : 2410, Loss : 0.67107, Acc : 0.741, Sensitive_Loss : 0.24540, Sensitive_Acc : 16.300, Run Time : 133.79 sec
INFO:root:2024-03-28 23:17:59, Train, Epoch : 5, Step : 2420, Loss : 0.78138, Acc : 0.722, Sensitive_Loss : 0.24494, Sensitive_Acc : 16.400, Run Time : 9.32 sec
INFO:root:2024-03-28 23:18:10, Train, Epoch : 5, Step : 2430, Loss : 0.67553, Acc : 0.716, Sensitive_Loss : 0.24990, Sensitive_Acc : 15.700, Run Time : 11.17 sec
INFO:root:2024-03-28 23:18:22, Train, Epoch : 5, Step : 2440, Loss : 0.68542, Acc : 0.747, Sensitive_Loss : 0.19828, Sensitive_Acc : 15.800, Run Time : 11.92 sec
INFO:root:2024-03-28 23:18:31, Train, Epoch : 5, Step : 2450, Loss : 0.86376, Acc : 0.716, Sensitive_Loss : 0.26129, Sensitive_Acc : 15.800, Run Time : 9.05 sec
INFO:root:2024-03-28 23:18:40, Train, Epoch : 5, Step : 2460, Loss : 0.81675, Acc : 0.744, Sensitive_Loss : 0.22539, Sensitive_Acc : 15.900, Run Time : 9.35 sec
INFO:root:2024-03-28 23:20:15
INFO:root:y_pred: [0.20112143 0.08708452 0.3719293  ... 0.3583182  0.41205686 0.24385613]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [1.23281695e-03 1.11558184e-01 9.89662409e-01 2.67346948e-02
 9.82858539e-01 1.83946546e-02 2.09480703e-01 7.45848492e-02
 5.70082664e-03 7.87931308e-02 2.71147881e-02 9.26363617e-02
 9.65553284e-01 3.33769917e-02 9.96087074e-01 9.92666543e-01
 3.69055488e-04 5.32196999e-01 9.78956163e-01 7.28172898e-01
 2.34557614e-02 6.08142372e-03 9.99996305e-01 9.78163421e-01
 2.50023067e-01 9.21964049e-01 6.54840330e-03 9.99782622e-01
 8.97166412e-03 9.12421525e-01 9.98813272e-01 8.09389472e-01
 4.62430064e-03 9.89000400e-05 3.47089797e-01 1.01562599e-02
 3.71713862e-02 2.40584776e-01 8.74801934e-01 8.80394280e-01
 2.23793294e-02 2.70713288e-02 9.82161164e-01 9.99723256e-01
 5.83779931e-01 1.99412815e-02 1.83566287e-02 2.57123858e-02
 9.90325868e-01 8.82999957e-01 9.63082552e-01 3.83865714e-01
 9.99201596e-01 9.70772564e-01 9.62278366e-01 8.14042926e-01
 7.33780086e-01 4.40664627e-02 9.99611557e-01 1.05385467e-01
 1.58771779e-03 4.07754660e-01 9.05420244e-01 9.54220653e-01
 9.54336345e-01 9.96766925e-01 5.80151342e-02 9.96955514e-01
 9.94887531e-01 9.80032742e-01 9.98011589e-01 9.27782990e-03
 1.58632994e-02 6.58031106e-01 8.81690860e-01 9.80103947e-03
 9.47049141e-01 1.22636382e-03 9.92563784e-01 9.73034725e-02
 4.88900304e-01 2.11914212e-01 1.78452786e-02 1.00698709e-01
 9.77237880e-01 5.27906239e-01 8.48774314e-01 5.29646516e-01
 1.30464854e-02 2.48478636e-01 1.47403777e-03 1.65631309e-01
 9.67907999e-03 3.51498574e-02 9.49902296e-01 7.73972273e-01
 5.24617016e-01 6.73534255e-03 7.24905074e-01 9.42417622e-01
 9.62182045e-01 6.50218725e-02 9.99789894e-01 1.24273434e-01
 3.42175364e-01 9.91802275e-01 1.58907354e-01 5.77649288e-03
 2.18152557e-03 1.10386694e-02 1.37859862e-02 9.89345968e-01
 2.90719680e-02 3.89411580e-03 2.89044738e-01 3.27709568e-04
 9.80269909e-01 2.74086930e-02 2.23451495e-01 7.05966435e-04
 1.22805156e-01 5.71510717e-02 9.96277988e-01 1.22605613e-03
 8.35685506e-02 9.99759376e-01 1.94057208e-02 8.45447838e-01
 3.75330359e-01 9.90409136e-01 1.29234092e-03 9.95252371e-01
 2.52540614e-02 9.99547422e-01 9.87641573e-01 3.31234604e-01
 1.22169763e-01 9.35320497e-01 3.63594876e-03 9.68432605e-01
 9.85972643e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-28 23:20:15, Dev, Step : 2460, Loss : 0.91126, Acc : 0.758, Auc : 0.792, Sensitive_Loss : 0.26978, Sensitive_Acc : 16.865, Sensitive_Auc : 0.993, Mean auc: 0.792, Run Time : 94.86 sec
INFO:root:2024-03-28 23:20:16, Best, Step : 2460, Loss : 0.91126, Acc : 0.758,Auc : 0.792, Best Auc : 0.792, Sensitive_Loss : 0.26978, Sensitive_Acc : 16.865, Sensitive_Auc : 0.993
INFO:root:2024-03-28 23:20:29, Train, Epoch : 6, Step : 2470, Loss : 0.79103, Acc : 0.728, Sensitive_Loss : 0.23332, Sensitive_Acc : 18.600, Run Time : 12.54 sec
INFO:root:2024-03-28 23:20:40, Train, Epoch : 6, Step : 2480, Loss : 0.87733, Acc : 0.731, Sensitive_Loss : 0.24811, Sensitive_Acc : 19.000, Run Time : 10.95 sec
INFO:root:2024-03-28 23:20:50, Train, Epoch : 6, Step : 2490, Loss : 0.92892, Acc : 0.722, Sensitive_Loss : 0.20659, Sensitive_Acc : 16.900, Run Time : 10.07 sec
INFO:root:2024-03-28 23:21:00, Train, Epoch : 6, Step : 2500, Loss : 0.68329, Acc : 0.719, Sensitive_Loss : 0.23235, Sensitive_Acc : 16.500, Run Time : 10.32 sec
INFO:root:2024-03-28 23:22:39, Dev, Step : 2500, Loss : 0.91539, Acc : 0.781, Auc : 0.792, Sensitive_Loss : 0.27873, Sensitive_Acc : 16.823, Sensitive_Auc : 0.993, Mean auc: 0.792, Run Time : 98.24 sec
INFO:root:2024-03-28 23:22:49, Train, Epoch : 6, Step : 2510, Loss : 0.71104, Acc : 0.778, Sensitive_Loss : 0.25347, Sensitive_Acc : 15.600, Run Time : 108.33 sec
INFO:root:2024-03-28 23:23:03, Train, Epoch : 6, Step : 2520, Loss : 0.74274, Acc : 0.716, Sensitive_Loss : 0.26245, Sensitive_Acc : 15.200, Run Time : 13.92 sec
INFO:root:2024-03-28 23:23:14, Train, Epoch : 6, Step : 2530, Loss : 0.89127, Acc : 0.734, Sensitive_Loss : 0.26204, Sensitive_Acc : 16.800, Run Time : 11.16 sec
INFO:root:2024-03-28 23:23:25, Train, Epoch : 6, Step : 2540, Loss : 0.66157, Acc : 0.738, Sensitive_Loss : 0.20157, Sensitive_Acc : 17.600, Run Time : 11.29 sec
INFO:root:2024-03-28 23:23:37, Train, Epoch : 6, Step : 2550, Loss : 0.78081, Acc : 0.744, Sensitive_Loss : 0.25614, Sensitive_Acc : 16.800, Run Time : 11.49 sec
INFO:root:2024-03-28 23:23:48, Train, Epoch : 6, Step : 2560, Loss : 0.81002, Acc : 0.741, Sensitive_Loss : 0.24417, Sensitive_Acc : 16.300, Run Time : 11.12 sec
INFO:root:2024-03-28 23:24:00, Train, Epoch : 6, Step : 2570, Loss : 0.73023, Acc : 0.756, Sensitive_Loss : 0.23293, Sensitive_Acc : 15.800, Run Time : 12.71 sec
INFO:root:2024-03-28 23:24:13, Train, Epoch : 6, Step : 2580, Loss : 0.73059, Acc : 0.738, Sensitive_Loss : 0.31757, Sensitive_Acc : 17.900, Run Time : 12.94 sec
INFO:root:2024-03-28 23:24:24, Train, Epoch : 6, Step : 2590, Loss : 0.78697, Acc : 0.744, Sensitive_Loss : 0.23872, Sensitive_Acc : 14.500, Run Time : 11.08 sec
INFO:root:2024-03-28 23:24:36, Train, Epoch : 6, Step : 2600, Loss : 0.70109, Acc : 0.778, Sensitive_Loss : 0.28787, Sensitive_Acc : 17.700, Run Time : 11.55 sec
INFO:root:2024-03-28 23:26:56, Dev, Step : 2600, Loss : 0.93683, Acc : 0.775, Auc : 0.780, Sensitive_Loss : 0.26602, Sensitive_Acc : 17.064, Sensitive_Auc : 0.995, Mean auc: 0.780, Run Time : 139.92 sec
INFO:root:2024-03-28 23:27:03, Train, Epoch : 6, Step : 2610, Loss : 0.59757, Acc : 0.744, Sensitive_Loss : 0.24070, Sensitive_Acc : 15.300, Run Time : 146.49 sec
INFO:root:2024-03-28 23:27:12, Train, Epoch : 6, Step : 2620, Loss : 0.78868, Acc : 0.716, Sensitive_Loss : 0.26091, Sensitive_Acc : 18.700, Run Time : 9.88 sec
INFO:root:2024-03-28 23:27:22, Train, Epoch : 6, Step : 2630, Loss : 0.84076, Acc : 0.703, Sensitive_Loss : 0.18510, Sensitive_Acc : 16.100, Run Time : 9.93 sec
INFO:root:2024-03-28 23:27:32, Train, Epoch : 6, Step : 2640, Loss : 0.92197, Acc : 0.753, Sensitive_Loss : 0.25365, Sensitive_Acc : 16.300, Run Time : 9.64 sec
INFO:root:2024-03-28 23:27:46, Train, Epoch : 6, Step : 2650, Loss : 0.69667, Acc : 0.762, Sensitive_Loss : 0.24622, Sensitive_Acc : 14.700, Run Time : 14.51 sec
INFO:root:2024-03-28 23:27:56, Train, Epoch : 6, Step : 2660, Loss : 0.85139, Acc : 0.709, Sensitive_Loss : 0.26374, Sensitive_Acc : 16.200, Run Time : 9.58 sec
INFO:root:2024-03-28 23:28:06, Train, Epoch : 6, Step : 2670, Loss : 0.61651, Acc : 0.753, Sensitive_Loss : 0.18626, Sensitive_Acc : 16.900, Run Time : 9.98 sec
INFO:root:2024-03-28 23:28:18, Train, Epoch : 6, Step : 2680, Loss : 0.72973, Acc : 0.759, Sensitive_Loss : 0.28563, Sensitive_Acc : 15.300, Run Time : 12.39 sec
INFO:root:2024-03-28 23:28:28, Train, Epoch : 6, Step : 2690, Loss : 0.85874, Acc : 0.703, Sensitive_Loss : 0.27760, Sensitive_Acc : 16.600, Run Time : 9.64 sec
INFO:root:2024-03-28 23:28:38, Train, Epoch : 6, Step : 2700, Loss : 0.64303, Acc : 0.741, Sensitive_Loss : 0.22624, Sensitive_Acc : 15.300, Run Time : 9.80 sec
INFO:root:2024-03-28 23:30:33, Dev, Step : 2700, Loss : 0.92527, Acc : 0.767, Auc : 0.784, Sensitive_Loss : 0.26382, Sensitive_Acc : 17.064, Sensitive_Auc : 0.996, Mean auc: 0.784, Run Time : 115.06 sec
INFO:root:2024-03-28 23:30:41, Train, Epoch : 6, Step : 2710, Loss : 0.85933, Acc : 0.728, Sensitive_Loss : 0.25287, Sensitive_Acc : 16.800, Run Time : 123.45 sec
INFO:root:2024-03-28 23:30:53, Train, Epoch : 6, Step : 2720, Loss : 0.67082, Acc : 0.778, Sensitive_Loss : 0.23159, Sensitive_Acc : 17.000, Run Time : 11.27 sec
INFO:root:2024-03-28 23:31:02, Train, Epoch : 6, Step : 2730, Loss : 0.72347, Acc : 0.741, Sensitive_Loss : 0.28031, Sensitive_Acc : 14.900, Run Time : 9.86 sec
INFO:root:2024-03-28 23:31:13, Train, Epoch : 6, Step : 2740, Loss : 0.75369, Acc : 0.750, Sensitive_Loss : 0.22739, Sensitive_Acc : 16.000, Run Time : 10.03 sec
INFO:root:2024-03-28 23:31:25, Train, Epoch : 6, Step : 2750, Loss : 0.69971, Acc : 0.797, Sensitive_Loss : 0.27902, Sensitive_Acc : 14.200, Run Time : 12.38 sec
INFO:root:2024-03-28 23:31:36, Train, Epoch : 6, Step : 2760, Loss : 0.66950, Acc : 0.719, Sensitive_Loss : 0.24551, Sensitive_Acc : 16.500, Run Time : 11.34 sec
INFO:root:2024-03-28 23:31:46, Train, Epoch : 6, Step : 2770, Loss : 0.70793, Acc : 0.744, Sensitive_Loss : 0.28244, Sensitive_Acc : 16.100, Run Time : 10.06 sec
INFO:root:2024-03-28 23:31:57, Train, Epoch : 6, Step : 2780, Loss : 0.72350, Acc : 0.719, Sensitive_Loss : 0.28403, Sensitive_Acc : 16.500, Run Time : 10.49 sec
INFO:root:2024-03-28 23:32:08, Train, Epoch : 6, Step : 2790, Loss : 0.81873, Acc : 0.762, Sensitive_Loss : 0.27810, Sensitive_Acc : 16.400, Run Time : 11.00 sec
INFO:root:2024-03-28 23:32:18, Train, Epoch : 6, Step : 2800, Loss : 0.82140, Acc : 0.747, Sensitive_Loss : 0.23971, Sensitive_Acc : 14.800, Run Time : 9.97 sec
INFO:root:2024-03-28 23:33:54, Dev, Step : 2800, Loss : 0.95153, Acc : 0.782, Auc : 0.775, Sensitive_Loss : 0.26256, Sensitive_Acc : 17.064, Sensitive_Auc : 0.996, Mean auc: 0.775, Run Time : 96.66 sec
INFO:root:2024-03-28 23:34:01, Train, Epoch : 6, Step : 2810, Loss : 0.71724, Acc : 0.747, Sensitive_Loss : 0.23501, Sensitive_Acc : 14.500, Run Time : 103.62 sec
INFO:root:2024-03-28 23:34:13, Train, Epoch : 6, Step : 2820, Loss : 0.51621, Acc : 0.750, Sensitive_Loss : 0.25909, Sensitive_Acc : 16.600, Run Time : 11.62 sec
INFO:root:2024-03-28 23:34:27, Train, Epoch : 6, Step : 2830, Loss : 0.86445, Acc : 0.747, Sensitive_Loss : 0.22781, Sensitive_Acc : 17.100, Run Time : 13.74 sec
INFO:root:2024-03-28 23:34:38, Train, Epoch : 6, Step : 2840, Loss : 0.90984, Acc : 0.753, Sensitive_Loss : 0.25382, Sensitive_Acc : 16.300, Run Time : 11.09 sec
INFO:root:2024-03-28 23:34:49, Train, Epoch : 6, Step : 2850, Loss : 0.73401, Acc : 0.750, Sensitive_Loss : 0.22186, Sensitive_Acc : 15.700, Run Time : 10.76 sec
INFO:root:2024-03-28 23:34:59, Train, Epoch : 6, Step : 2860, Loss : 0.96384, Acc : 0.747, Sensitive_Loss : 0.22302, Sensitive_Acc : 16.600, Run Time : 10.37 sec
INFO:root:2024-03-28 23:35:09, Train, Epoch : 6, Step : 2870, Loss : 0.61244, Acc : 0.722, Sensitive_Loss : 0.29541, Sensitive_Acc : 15.100, Run Time : 9.83 sec
INFO:root:2024-03-28 23:35:19, Train, Epoch : 6, Step : 2880, Loss : 0.72318, Acc : 0.747, Sensitive_Loss : 0.21126, Sensitive_Acc : 14.300, Run Time : 10.19 sec
INFO:root:2024-03-28 23:35:31, Train, Epoch : 6, Step : 2890, Loss : 0.61841, Acc : 0.791, Sensitive_Loss : 0.26358, Sensitive_Acc : 15.700, Run Time : 11.76 sec
INFO:root:2024-03-28 23:35:43, Train, Epoch : 6, Step : 2900, Loss : 0.68196, Acc : 0.769, Sensitive_Loss : 0.24119, Sensitive_Acc : 17.400, Run Time : 12.33 sec
INFO:root:2024-03-28 23:37:40, Dev, Step : 2900, Loss : 0.93815, Acc : 0.754, Auc : 0.786, Sensitive_Loss : 0.25686, Sensitive_Acc : 17.106, Sensitive_Auc : 0.993, Mean auc: 0.786, Run Time : 117.06 sec
INFO:root:2024-03-28 23:37:47, Train, Epoch : 6, Step : 2910, Loss : 0.51485, Acc : 0.778, Sensitive_Loss : 0.27077, Sensitive_Acc : 14.700, Run Time : 124.06 sec
INFO:root:2024-03-28 23:37:58, Train, Epoch : 6, Step : 2920, Loss : 0.91180, Acc : 0.722, Sensitive_Loss : 0.28629, Sensitive_Acc : 14.800, Run Time : 11.13 sec
INFO:root:2024-03-28 23:38:09, Train, Epoch : 6, Step : 2930, Loss : 0.67570, Acc : 0.756, Sensitive_Loss : 0.21715, Sensitive_Acc : 18.300, Run Time : 10.64 sec
INFO:root:2024-03-28 23:38:19, Train, Epoch : 6, Step : 2940, Loss : 0.77617, Acc : 0.775, Sensitive_Loss : 0.24079, Sensitive_Acc : 14.700, Run Time : 10.42 sec
INFO:root:2024-03-28 23:38:32, Train, Epoch : 6, Step : 2950, Loss : 0.72551, Acc : 0.766, Sensitive_Loss : 0.20648, Sensitive_Acc : 16.600, Run Time : 13.00 sec
INFO:root:2024-03-28 23:40:11
INFO:root:y_pred: [0.23724343 0.11117687 0.2581745  ... 0.43819797 0.5501143  0.1646832 ]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [5.53567719e-04 4.98695858e-02 9.55906272e-01 1.99033841e-02
 9.84390557e-01 1.02472184e-02 1.18953347e-01 1.27134845e-01
 5.96704474e-03 5.65311164e-02 1.07280556e-02 5.52305579e-02
 9.81720686e-01 1.31440070e-02 9.96707559e-01 9.88751948e-01
 3.44464992e-04 4.33524460e-01 9.64955032e-01 7.50043750e-01
 3.59560512e-02 9.51379538e-03 9.99999642e-01 9.60664868e-01
 1.68274626e-01 9.41662490e-01 2.70704529e-03 9.99897838e-01
 7.12039415e-03 8.82439733e-01 9.99537945e-01 8.12049687e-01
 4.40558931e-03 5.41832305e-05 3.56698155e-01 9.71230119e-03
 5.70554510e-02 1.96900636e-01 8.72600079e-01 8.87837470e-01
 1.00731207e-02 1.51999015e-02 9.58335638e-01 9.99748647e-01
 2.28417501e-01 2.70048454e-02 2.17920393e-02 1.53208785e-02
 9.88495231e-01 8.69513333e-01 9.47190821e-01 4.12763596e-01
 9.99497652e-01 9.48942721e-01 9.78111148e-01 8.02812576e-01
 6.96226120e-01 2.67684180e-02 9.99736130e-01 1.62612021e-01
 1.06455444e-03 5.74545622e-01 9.29247379e-01 9.79576230e-01
 9.82878208e-01 9.96786952e-01 6.15642071e-02 9.95903194e-01
 9.96936560e-01 9.84027386e-01 9.97781575e-01 8.67243484e-03
 2.86807902e-02 6.45225286e-01 7.50902236e-01 7.11553451e-03
 9.47052479e-01 1.32295373e-03 9.97104228e-01 1.09260425e-01
 4.21725035e-01 1.06406659e-01 1.66644156e-02 9.87610668e-02
 9.86237526e-01 4.24624503e-01 9.11497116e-01 4.08858061e-01
 1.75659098e-02 2.35095188e-01 2.82221939e-03 1.38736174e-01
 8.71969759e-03 2.90299952e-02 8.94136608e-01 7.45644093e-01
 2.13082284e-01 2.13065241e-02 8.86767745e-01 9.04671609e-01
 9.62246299e-01 9.87486690e-02 9.99809921e-01 1.88435718e-01
 2.26458624e-01 9.92015481e-01 1.17197432e-01 9.89911985e-03
 3.38523369e-03 1.64436940e-02 4.64486098e-03 9.95372713e-01
 4.77355011e-02 2.63619586e-03 5.76852560e-01 2.44665367e-04
 9.78152454e-01 3.10176499e-02 1.76839888e-01 5.95296733e-04
 1.73880070e-01 4.51781489e-02 9.95321095e-01 1.86577940e-03
 5.19267730e-02 9.99683261e-01 3.55604440e-02 8.71971130e-01
 1.99792042e-01 9.93919492e-01 1.35536096e-03 9.96521592e-01
 1.85232311e-02 9.99769628e-01 9.88875270e-01 1.92170784e-01
 1.89420417e-01 8.30469429e-01 3.45158763e-03 9.71713364e-01
 9.94475901e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-28 23:40:11, Dev, Step : 2952, Loss : 0.92827, Acc : 0.766, Auc : 0.787, Sensitive_Loss : 0.25650, Sensitive_Acc : 16.950, Sensitive_Auc : 0.993, Mean auc: 0.787, Run Time : 95.45 sec
INFO:root:2024-03-28 23:40:25, Train, Epoch : 7, Step : 2960, Loss : 0.56727, Acc : 0.603, Sensitive_Loss : 0.18630, Sensitive_Acc : 12.800, Run Time : 13.36 sec
INFO:root:2024-03-28 23:40:36, Train, Epoch : 7, Step : 2970, Loss : 0.80686, Acc : 0.762, Sensitive_Loss : 0.21688, Sensitive_Acc : 14.600, Run Time : 10.82 sec
INFO:root:2024-03-28 23:40:47, Train, Epoch : 7, Step : 2980, Loss : 0.72648, Acc : 0.778, Sensitive_Loss : 0.22958, Sensitive_Acc : 15.000, Run Time : 10.53 sec
INFO:root:2024-03-28 23:40:57, Train, Epoch : 7, Step : 2990, Loss : 0.73075, Acc : 0.734, Sensitive_Loss : 0.21628, Sensitive_Acc : 17.800, Run Time : 10.49 sec
INFO:root:2024-03-28 23:41:10, Train, Epoch : 7, Step : 3000, Loss : 0.78791, Acc : 0.722, Sensitive_Loss : 0.30043, Sensitive_Acc : 16.400, Run Time : 12.96 sec
INFO:root:2024-03-28 23:42:47, Dev, Step : 3000, Loss : 0.95940, Acc : 0.741, Auc : 0.773, Sensitive_Loss : 0.26137, Sensitive_Acc : 16.908, Sensitive_Auc : 0.994, Mean auc: 0.773, Run Time : 97.26 sec
INFO:root:2024-03-28 23:42:56, Train, Epoch : 7, Step : 3010, Loss : 0.65995, Acc : 0.753, Sensitive_Loss : 0.24997, Sensitive_Acc : 16.900, Run Time : 105.88 sec
INFO:root:2024-03-28 23:43:07, Train, Epoch : 7, Step : 3020, Loss : 0.83193, Acc : 0.756, Sensitive_Loss : 0.23255, Sensitive_Acc : 16.000, Run Time : 11.19 sec
INFO:root:2024-03-28 23:43:21, Train, Epoch : 7, Step : 3030, Loss : 0.65977, Acc : 0.787, Sensitive_Loss : 0.19945, Sensitive_Acc : 16.100, Run Time : 13.74 sec
INFO:root:2024-03-28 23:43:34, Train, Epoch : 7, Step : 3040, Loss : 0.73656, Acc : 0.769, Sensitive_Loss : 0.23747, Sensitive_Acc : 17.200, Run Time : 12.70 sec
INFO:root:2024-03-28 23:43:44, Train, Epoch : 7, Step : 3050, Loss : 0.73577, Acc : 0.791, Sensitive_Loss : 0.25369, Sensitive_Acc : 16.300, Run Time : 10.56 sec
INFO:root:2024-03-28 23:43:55, Train, Epoch : 7, Step : 3060, Loss : 0.66436, Acc : 0.766, Sensitive_Loss : 0.20438, Sensitive_Acc : 15.500, Run Time : 10.96 sec
INFO:root:2024-03-28 23:44:07, Train, Epoch : 7, Step : 3070, Loss : 0.81786, Acc : 0.722, Sensitive_Loss : 0.23243, Sensitive_Acc : 16.000, Run Time : 11.96 sec
INFO:root:2024-03-28 23:44:17, Train, Epoch : 7, Step : 3080, Loss : 0.64096, Acc : 0.734, Sensitive_Loss : 0.22690, Sensitive_Acc : 16.600, Run Time : 10.41 sec
INFO:root:2024-03-28 23:44:29, Train, Epoch : 7, Step : 3090, Loss : 0.72843, Acc : 0.759, Sensitive_Loss : 0.24021, Sensitive_Acc : 18.100, Run Time : 11.34 sec
INFO:root:2024-03-28 23:44:40, Train, Epoch : 7, Step : 3100, Loss : 0.71714, Acc : 0.744, Sensitive_Loss : 0.21470, Sensitive_Acc : 17.400, Run Time : 10.96 sec
INFO:root:2024-03-28 23:46:48, Dev, Step : 3100, Loss : 0.94232, Acc : 0.753, Auc : 0.782, Sensitive_Loss : 0.25379, Sensitive_Acc : 17.035, Sensitive_Auc : 0.993, Mean auc: 0.782, Run Time : 128.13 sec
INFO:root:2024-03-28 23:46:55, Train, Epoch : 7, Step : 3110, Loss : 0.66867, Acc : 0.769, Sensitive_Loss : 0.27827, Sensitive_Acc : 17.500, Run Time : 134.78 sec
INFO:root:2024-03-28 23:47:04, Train, Epoch : 7, Step : 3120, Loss : 0.65521, Acc : 0.762, Sensitive_Loss : 0.20070, Sensitive_Acc : 15.300, Run Time : 9.87 sec
INFO:root:2024-03-28 23:47:14, Train, Epoch : 7, Step : 3130, Loss : 0.71569, Acc : 0.694, Sensitive_Loss : 0.23524, Sensitive_Acc : 18.700, Run Time : 9.96 sec
INFO:root:2024-03-28 23:47:25, Train, Epoch : 7, Step : 3140, Loss : 0.66967, Acc : 0.741, Sensitive_Loss : 0.25364, Sensitive_Acc : 16.000, Run Time : 10.31 sec
INFO:root:2024-03-28 23:47:35, Train, Epoch : 7, Step : 3150, Loss : 0.79223, Acc : 0.741, Sensitive_Loss : 0.20145, Sensitive_Acc : 17.000, Run Time : 9.92 sec
INFO:root:2024-03-28 23:47:44, Train, Epoch : 7, Step : 3160, Loss : 0.72511, Acc : 0.775, Sensitive_Loss : 0.18979, Sensitive_Acc : 15.900, Run Time : 9.69 sec
INFO:root:2024-03-28 23:47:54, Train, Epoch : 7, Step : 3170, Loss : 0.93659, Acc : 0.759, Sensitive_Loss : 0.25932, Sensitive_Acc : 17.500, Run Time : 9.85 sec
INFO:root:2024-03-28 23:48:04, Train, Epoch : 7, Step : 3180, Loss : 0.68358, Acc : 0.759, Sensitive_Loss : 0.23893, Sensitive_Acc : 17.200, Run Time : 9.52 sec
INFO:root:2024-03-28 23:48:14, Train, Epoch : 7, Step : 3190, Loss : 0.74979, Acc : 0.766, Sensitive_Loss : 0.22469, Sensitive_Acc : 17.700, Run Time : 10.34 sec
INFO:root:2024-03-28 23:48:23, Train, Epoch : 7, Step : 3200, Loss : 0.71176, Acc : 0.722, Sensitive_Loss : 0.23917, Sensitive_Acc : 14.700, Run Time : 9.51 sec
INFO:root:2024-03-28 23:50:34, Dev, Step : 3200, Loss : 0.94104, Acc : 0.785, Auc : 0.783, Sensitive_Loss : 0.26160, Sensitive_Acc : 16.851, Sensitive_Auc : 0.993, Mean auc: 0.783, Run Time : 130.24 sec
INFO:root:2024-03-28 23:50:41, Train, Epoch : 7, Step : 3210, Loss : 0.68830, Acc : 0.728, Sensitive_Loss : 0.22430, Sensitive_Acc : 17.100, Run Time : 137.03 sec
INFO:root:2024-03-28 23:50:51, Train, Epoch : 7, Step : 3220, Loss : 0.70917, Acc : 0.741, Sensitive_Loss : 0.21538, Sensitive_Acc : 17.300, Run Time : 10.14 sec
INFO:root:2024-03-28 23:51:00, Train, Epoch : 7, Step : 3230, Loss : 0.80666, Acc : 0.753, Sensitive_Loss : 0.35151, Sensitive_Acc : 16.900, Run Time : 9.82 sec
INFO:root:2024-03-28 23:51:10, Train, Epoch : 7, Step : 3240, Loss : 0.71367, Acc : 0.756, Sensitive_Loss : 0.21955, Sensitive_Acc : 17.100, Run Time : 10.01 sec
INFO:root:2024-03-28 23:51:20, Train, Epoch : 7, Step : 3250, Loss : 0.82732, Acc : 0.762, Sensitive_Loss : 0.23413, Sensitive_Acc : 17.100, Run Time : 9.27 sec
INFO:root:2024-03-28 23:51:30, Train, Epoch : 7, Step : 3260, Loss : 0.73139, Acc : 0.762, Sensitive_Loss : 0.23552, Sensitive_Acc : 17.800, Run Time : 10.05 sec
INFO:root:2024-03-28 23:51:39, Train, Epoch : 7, Step : 3270, Loss : 0.62404, Acc : 0.778, Sensitive_Loss : 0.23033, Sensitive_Acc : 15.400, Run Time : 9.34 sec
INFO:root:2024-03-28 23:51:49, Train, Epoch : 7, Step : 3280, Loss : 0.64689, Acc : 0.769, Sensitive_Loss : 0.20965, Sensitive_Acc : 16.800, Run Time : 9.69 sec
INFO:root:2024-03-28 23:51:58, Train, Epoch : 7, Step : 3290, Loss : 0.77837, Acc : 0.741, Sensitive_Loss : 0.24965, Sensitive_Acc : 16.900, Run Time : 9.61 sec
INFO:root:2024-03-28 23:52:08, Train, Epoch : 7, Step : 3300, Loss : 0.80668, Acc : 0.741, Sensitive_Loss : 0.22751, Sensitive_Acc : 15.300, Run Time : 9.40 sec
INFO:root:2024-03-28 23:54:16, Dev, Step : 3300, Loss : 0.95955, Acc : 0.799, Auc : 0.777, Sensitive_Loss : 0.24659, Sensitive_Acc : 17.149, Sensitive_Auc : 0.994, Mean auc: 0.777, Run Time : 127.69 sec
INFO:root:2024-03-28 23:54:22, Train, Epoch : 7, Step : 3310, Loss : 0.77621, Acc : 0.759, Sensitive_Loss : 0.24768, Sensitive_Acc : 15.900, Run Time : 134.28 sec
INFO:root:2024-03-28 23:54:32, Train, Epoch : 7, Step : 3320, Loss : 0.67270, Acc : 0.772, Sensitive_Loss : 0.25858, Sensitive_Acc : 16.600, Run Time : 9.84 sec
INFO:root:2024-03-28 23:54:41, Train, Epoch : 7, Step : 3330, Loss : 0.81468, Acc : 0.769, Sensitive_Loss : 0.23670, Sensitive_Acc : 15.000, Run Time : 9.42 sec
INFO:root:2024-03-28 23:54:51, Train, Epoch : 7, Step : 3340, Loss : 0.58685, Acc : 0.769, Sensitive_Loss : 0.16806, Sensitive_Acc : 17.600, Run Time : 9.66 sec
INFO:root:2024-03-28 23:55:01, Train, Epoch : 7, Step : 3350, Loss : 0.81377, Acc : 0.722, Sensitive_Loss : 0.28016, Sensitive_Acc : 16.300, Run Time : 9.98 sec
INFO:root:2024-03-28 23:55:11, Train, Epoch : 7, Step : 3360, Loss : 0.68794, Acc : 0.803, Sensitive_Loss : 0.22651, Sensitive_Acc : 16.900, Run Time : 10.32 sec
INFO:root:2024-03-28 23:55:21, Train, Epoch : 7, Step : 3370, Loss : 0.68063, Acc : 0.791, Sensitive_Loss : 0.29646, Sensitive_Acc : 17.400, Run Time : 9.74 sec
INFO:root:2024-03-28 23:55:30, Train, Epoch : 7, Step : 3380, Loss : 0.76674, Acc : 0.738, Sensitive_Loss : 0.21029, Sensitive_Acc : 15.400, Run Time : 9.30 sec
INFO:root:2024-03-28 23:55:41, Train, Epoch : 7, Step : 3390, Loss : 0.69114, Acc : 0.759, Sensitive_Loss : 0.21196, Sensitive_Acc : 17.100, Run Time : 10.68 sec
INFO:root:2024-03-28 23:55:53, Train, Epoch : 7, Step : 3400, Loss : 0.65042, Acc : 0.781, Sensitive_Loss : 0.21512, Sensitive_Acc : 16.100, Run Time : 11.90 sec
INFO:root:2024-03-28 23:58:06, Dev, Step : 3400, Loss : 0.93088, Acc : 0.771, Auc : 0.788, Sensitive_Loss : 0.23880, Sensitive_Acc : 17.078, Sensitive_Auc : 0.994, Mean auc: 0.788, Run Time : 132.92 sec
INFO:root:2024-03-28 23:58:13, Train, Epoch : 7, Step : 3410, Loss : 0.70038, Acc : 0.741, Sensitive_Loss : 0.24746, Sensitive_Acc : 16.900, Run Time : 139.57 sec
INFO:root:2024-03-28 23:58:22, Train, Epoch : 7, Step : 3420, Loss : 0.63096, Acc : 0.731, Sensitive_Loss : 0.21179, Sensitive_Acc : 16.500, Run Time : 9.63 sec
INFO:root:2024-03-28 23:58:31, Train, Epoch : 7, Step : 3430, Loss : 0.76391, Acc : 0.734, Sensitive_Loss : 0.19105, Sensitive_Acc : 15.300, Run Time : 9.28 sec
INFO:root:2024-03-28 23:58:42, Train, Epoch : 7, Step : 3440, Loss : 0.59372, Acc : 0.741, Sensitive_Loss : 0.21883, Sensitive_Acc : 15.800, Run Time : 11.05 sec
INFO:root:2024-03-29 00:00:22
INFO:root:y_pred: [0.21164028 0.13454019 0.2801674  ... 0.33649945 0.36122274 0.16531391]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [6.2600715e-04 4.8227623e-02 9.6129608e-01 2.7293559e-02 9.8322368e-01
 1.1617046e-02 1.6826887e-01 5.8435529e-02 6.7021134e-03 7.1465872e-02
 1.4491998e-02 4.0571939e-02 9.8342913e-01 1.1959762e-02 9.9686688e-01
 9.8601621e-01 2.0056538e-04 5.5941790e-01 9.6215630e-01 7.8607804e-01
 1.4841420e-02 1.3896648e-02 1.0000000e+00 9.8001468e-01 7.5837635e-02
 9.2244166e-01 1.9061128e-03 9.9990773e-01 1.2352885e-02 9.2297500e-01
 9.9960929e-01 8.6092103e-01 3.9152419e-03 1.0385798e-04 2.7187532e-01
 1.2532387e-02 6.1565865e-02 1.6716264e-01 8.8275450e-01 8.2713360e-01
 6.1228336e-03 1.4469311e-02 9.7807127e-01 9.9968910e-01 3.6551273e-01
 2.2471603e-02 2.1416470e-02 2.7730074e-02 9.9220043e-01 8.6182845e-01
 9.7164333e-01 6.7163235e-01 9.9961144e-01 9.7848594e-01 9.8345941e-01
 8.4000951e-01 6.1536658e-01 3.1749461e-02 9.9973327e-01 2.2521964e-01
 8.7688444e-04 6.2756544e-01 9.3216234e-01 9.7966343e-01 9.7915637e-01
 9.9673128e-01 6.2585115e-02 9.9690020e-01 9.9396604e-01 9.7474200e-01
 9.9959165e-01 8.3560357e-03 1.4742910e-02 5.5212140e-01 8.7291867e-01
 8.9393817e-03 9.6967524e-01 5.5714545e-04 9.9592823e-01 1.5898828e-01
 4.2551035e-01 1.3671444e-01 1.7554501e-02 7.9950944e-02 9.7390062e-01
 3.5810187e-01 8.6364132e-01 5.3620338e-01 1.2133702e-02 1.9000871e-01
 2.4684996e-03 1.2516490e-01 7.0595760e-03 2.3839034e-02 9.7240925e-01
 7.0521033e-01 2.7826634e-01 1.4086806e-02 9.0379667e-01 9.3572789e-01
 9.6781063e-01 6.2652208e-02 9.9992061e-01 1.3243499e-01 1.1665172e-01
 9.9635988e-01 2.0168689e-01 7.8879902e-03 2.0201518e-03 4.0297057e-03
 4.2728488e-03 9.9402857e-01 2.6266590e-02 1.9340237e-03 5.5176473e-01
 2.1764378e-04 9.8208249e-01 3.3159334e-02 1.1925906e-01 7.6735020e-04
 2.3432997e-01 5.0298072e-02 9.9331141e-01 9.8761730e-04 6.4350545e-02
 9.9976951e-01 2.0103469e-02 7.9951775e-01 9.9460527e-02 9.9320227e-01
 1.2730015e-03 9.9745458e-01 1.0767006e-02 9.9990225e-01 9.9467468e-01
 1.7826484e-01 1.2228882e-01 9.2648774e-01 2.7528126e-03 9.4889623e-01
 9.9038714e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 00:00:22, Dev, Step : 3444, Loss : 0.93303, Acc : 0.791, Auc : 0.786, Sensitive_Loss : 0.24686, Sensitive_Acc : 16.922, Sensitive_Auc : 0.993, Mean auc: 0.786, Run Time : 95.41 sec
INFO:root:2024-03-29 00:00:33, Train, Epoch : 8, Step : 3450, Loss : 0.43602, Acc : 0.463, Sensitive_Loss : 0.13480, Sensitive_Acc : 11.400, Run Time : 9.84 sec
INFO:root:2024-03-29 00:00:44, Train, Epoch : 8, Step : 3460, Loss : 0.69773, Acc : 0.775, Sensitive_Loss : 0.27104, Sensitive_Acc : 16.900, Run Time : 10.63 sec
INFO:root:2024-03-29 00:00:54, Train, Epoch : 8, Step : 3470, Loss : 0.76518, Acc : 0.762, Sensitive_Loss : 0.23449, Sensitive_Acc : 16.600, Run Time : 9.67 sec
INFO:root:2024-03-29 00:01:06, Train, Epoch : 8, Step : 3480, Loss : 0.63401, Acc : 0.759, Sensitive_Loss : 0.25854, Sensitive_Acc : 17.500, Run Time : 12.54 sec
INFO:root:2024-03-29 00:01:16, Train, Epoch : 8, Step : 3490, Loss : 0.64042, Acc : 0.753, Sensitive_Loss : 0.27702, Sensitive_Acc : 14.900, Run Time : 10.08 sec
INFO:root:2024-03-29 00:01:27, Train, Epoch : 8, Step : 3500, Loss : 0.73761, Acc : 0.731, Sensitive_Loss : 0.27776, Sensitive_Acc : 16.800, Run Time : 10.44 sec
INFO:root:2024-03-29 00:03:03, Dev, Step : 3500, Loss : 0.95966, Acc : 0.801, Auc : 0.778, Sensitive_Loss : 0.23949, Sensitive_Acc : 17.177, Sensitive_Auc : 0.992, Mean auc: 0.778, Run Time : 96.73 sec
INFO:root:2024-03-29 00:03:11, Train, Epoch : 8, Step : 3510, Loss : 0.83772, Acc : 0.747, Sensitive_Loss : 0.16912, Sensitive_Acc : 17.800, Run Time : 104.11 sec
INFO:root:2024-03-29 00:03:22, Train, Epoch : 8, Step : 3520, Loss : 0.65680, Acc : 0.775, Sensitive_Loss : 0.19319, Sensitive_Acc : 16.000, Run Time : 11.18 sec
INFO:root:2024-03-29 00:03:33, Train, Epoch : 8, Step : 3530, Loss : 0.60795, Acc : 0.759, Sensitive_Loss : 0.16935, Sensitive_Acc : 16.600, Run Time : 10.75 sec
INFO:root:2024-03-29 00:03:43, Train, Epoch : 8, Step : 3540, Loss : 0.68256, Acc : 0.744, Sensitive_Loss : 0.25193, Sensitive_Acc : 16.800, Run Time : 10.82 sec
INFO:root:2024-03-29 00:03:54, Train, Epoch : 8, Step : 3550, Loss : 0.82548, Acc : 0.766, Sensitive_Loss : 0.23401, Sensitive_Acc : 16.300, Run Time : 10.73 sec
INFO:root:2024-03-29 00:04:05, Train, Epoch : 8, Step : 3560, Loss : 0.70957, Acc : 0.750, Sensitive_Loss : 0.20289, Sensitive_Acc : 16.700, Run Time : 10.41 sec
INFO:root:2024-03-29 00:04:15, Train, Epoch : 8, Step : 3570, Loss : 0.74869, Acc : 0.766, Sensitive_Loss : 0.21556, Sensitive_Acc : 17.500, Run Time : 10.83 sec
INFO:root:2024-03-29 00:04:27, Train, Epoch : 8, Step : 3580, Loss : 0.69117, Acc : 0.787, Sensitive_Loss : 0.26217, Sensitive_Acc : 14.800, Run Time : 11.50 sec
INFO:root:2024-03-29 00:04:40, Train, Epoch : 8, Step : 3590, Loss : 0.75628, Acc : 0.719, Sensitive_Loss : 0.24854, Sensitive_Acc : 16.100, Run Time : 12.93 sec
INFO:root:2024-03-29 00:04:50, Train, Epoch : 8, Step : 3600, Loss : 0.59175, Acc : 0.791, Sensitive_Loss : 0.24134, Sensitive_Acc : 13.300, Run Time : 10.10 sec
INFO:root:2024-03-29 00:06:57, Dev, Step : 3600, Loss : 0.95031, Acc : 0.806, Auc : 0.779, Sensitive_Loss : 0.26741, Sensitive_Acc : 16.922, Sensitive_Auc : 0.993, Mean auc: 0.779, Run Time : 127.06 sec
INFO:root:2024-03-29 00:07:04, Train, Epoch : 8, Step : 3610, Loss : 0.65656, Acc : 0.797, Sensitive_Loss : 0.21207, Sensitive_Acc : 14.500, Run Time : 133.65 sec
INFO:root:2024-03-29 00:07:13, Train, Epoch : 8, Step : 3620, Loss : 0.64380, Acc : 0.784, Sensitive_Loss : 0.27711, Sensitive_Acc : 18.500, Run Time : 9.50 sec
INFO:root:2024-03-29 00:07:23, Train, Epoch : 8, Step : 3630, Loss : 0.75064, Acc : 0.744, Sensitive_Loss : 0.22896, Sensitive_Acc : 15.400, Run Time : 10.40 sec
INFO:root:2024-03-29 00:07:34, Train, Epoch : 8, Step : 3640, Loss : 0.67738, Acc : 0.725, Sensitive_Loss : 0.28365, Sensitive_Acc : 15.700, Run Time : 10.98 sec
INFO:root:2024-03-29 00:07:44, Train, Epoch : 8, Step : 3650, Loss : 0.50757, Acc : 0.738, Sensitive_Loss : 0.26854, Sensitive_Acc : 15.400, Run Time : 9.60 sec
INFO:root:2024-03-29 00:07:54, Train, Epoch : 8, Step : 3660, Loss : 0.46762, Acc : 0.784, Sensitive_Loss : 0.23338, Sensitive_Acc : 17.200, Run Time : 9.53 sec
INFO:root:2024-03-29 00:08:03, Train, Epoch : 8, Step : 3670, Loss : 0.66657, Acc : 0.747, Sensitive_Loss : 0.21473, Sensitive_Acc : 16.000, Run Time : 9.33 sec
INFO:root:2024-03-29 00:08:13, Train, Epoch : 8, Step : 3680, Loss : 0.66984, Acc : 0.781, Sensitive_Loss : 0.22734, Sensitive_Acc : 17.000, Run Time : 10.23 sec
INFO:root:2024-03-29 00:08:23, Train, Epoch : 8, Step : 3690, Loss : 0.58226, Acc : 0.772, Sensitive_Loss : 0.24080, Sensitive_Acc : 14.900, Run Time : 9.63 sec
INFO:root:2024-03-29 00:08:33, Train, Epoch : 8, Step : 3700, Loss : 0.65137, Acc : 0.797, Sensitive_Loss : 0.24302, Sensitive_Acc : 16.100, Run Time : 9.93 sec
INFO:root:2024-03-29 00:10:48, Dev, Step : 3700, Loss : 0.95517, Acc : 0.801, Auc : 0.783, Sensitive_Loss : 0.25727, Sensitive_Acc : 16.922, Sensitive_Auc : 0.994, Mean auc: 0.783, Run Time : 135.14 sec
INFO:root:2024-03-29 00:10:56, Train, Epoch : 8, Step : 3710, Loss : 0.61037, Acc : 0.794, Sensitive_Loss : 0.20726, Sensitive_Acc : 17.000, Run Time : 143.57 sec
INFO:root:2024-03-29 00:11:06, Train, Epoch : 8, Step : 3720, Loss : 0.63779, Acc : 0.775, Sensitive_Loss : 0.22341, Sensitive_Acc : 15.800, Run Time : 9.84 sec
INFO:root:2024-03-29 00:11:16, Train, Epoch : 8, Step : 3730, Loss : 0.67110, Acc : 0.766, Sensitive_Loss : 0.23350, Sensitive_Acc : 16.300, Run Time : 9.98 sec
INFO:root:2024-03-29 00:11:27, Train, Epoch : 8, Step : 3740, Loss : 0.63178, Acc : 0.744, Sensitive_Loss : 0.23024, Sensitive_Acc : 15.600, Run Time : 10.47 sec
INFO:root:2024-03-29 00:11:37, Train, Epoch : 8, Step : 3750, Loss : 0.71973, Acc : 0.762, Sensitive_Loss : 0.21163, Sensitive_Acc : 16.600, Run Time : 10.76 sec
INFO:root:2024-03-29 00:11:47, Train, Epoch : 8, Step : 3760, Loss : 0.72176, Acc : 0.775, Sensitive_Loss : 0.20594, Sensitive_Acc : 15.300, Run Time : 9.73 sec
INFO:root:2024-03-29 00:11:57, Train, Epoch : 8, Step : 3770, Loss : 0.61501, Acc : 0.797, Sensitive_Loss : 0.29972, Sensitive_Acc : 16.400, Run Time : 10.39 sec
INFO:root:2024-03-29 00:12:07, Train, Epoch : 8, Step : 3780, Loss : 0.63565, Acc : 0.772, Sensitive_Loss : 0.23962, Sensitive_Acc : 16.200, Run Time : 9.92 sec
INFO:root:2024-03-29 00:12:17, Train, Epoch : 8, Step : 3790, Loss : 0.81117, Acc : 0.791, Sensitive_Loss : 0.19765, Sensitive_Acc : 18.200, Run Time : 9.66 sec
INFO:root:2024-03-29 00:12:27, Train, Epoch : 8, Step : 3800, Loss : 0.53934, Acc : 0.781, Sensitive_Loss : 0.21405, Sensitive_Acc : 14.400, Run Time : 10.42 sec
INFO:root:2024-03-29 00:14:53, Dev, Step : 3800, Loss : 0.94293, Acc : 0.773, Auc : 0.779, Sensitive_Loss : 0.24153, Sensitive_Acc : 16.894, Sensitive_Auc : 0.993, Mean auc: 0.779, Run Time : 145.25 sec
INFO:root:2024-03-29 00:14:59, Train, Epoch : 8, Step : 3810, Loss : 0.65054, Acc : 0.812, Sensitive_Loss : 0.20417, Sensitive_Acc : 15.100, Run Time : 151.76 sec
INFO:root:2024-03-29 00:15:11, Train, Epoch : 8, Step : 3820, Loss : 0.79519, Acc : 0.747, Sensitive_Loss : 0.23337, Sensitive_Acc : 18.300, Run Time : 11.38 sec
INFO:root:2024-03-29 00:15:20, Train, Epoch : 8, Step : 3830, Loss : 0.67086, Acc : 0.741, Sensitive_Loss : 0.23768, Sensitive_Acc : 15.500, Run Time : 9.85 sec
INFO:root:2024-03-29 00:15:30, Train, Epoch : 8, Step : 3840, Loss : 0.72359, Acc : 0.784, Sensitive_Loss : 0.22000, Sensitive_Acc : 16.300, Run Time : 9.90 sec
INFO:root:2024-03-29 00:15:41, Train, Epoch : 8, Step : 3850, Loss : 0.66896, Acc : 0.781, Sensitive_Loss : 0.18769, Sensitive_Acc : 15.200, Run Time : 10.48 sec
INFO:root:2024-03-29 00:15:52, Train, Epoch : 8, Step : 3860, Loss : 0.70681, Acc : 0.791, Sensitive_Loss : 0.25201, Sensitive_Acc : 15.400, Run Time : 11.43 sec
INFO:root:2024-03-29 00:16:02, Train, Epoch : 8, Step : 3870, Loss : 0.70280, Acc : 0.738, Sensitive_Loss : 0.22117, Sensitive_Acc : 15.500, Run Time : 10.14 sec
INFO:root:2024-03-29 00:16:13, Train, Epoch : 8, Step : 3880, Loss : 0.71309, Acc : 0.806, Sensitive_Loss : 0.25235, Sensitive_Acc : 15.000, Run Time : 10.89 sec
INFO:root:2024-03-29 00:16:25, Train, Epoch : 8, Step : 3890, Loss : 0.66514, Acc : 0.800, Sensitive_Loss : 0.20011, Sensitive_Acc : 15.800, Run Time : 11.87 sec
INFO:root:2024-03-29 00:16:35, Train, Epoch : 8, Step : 3900, Loss : 0.68701, Acc : 0.806, Sensitive_Loss : 0.17611, Sensitive_Acc : 16.900, Run Time : 10.00 sec
INFO:root:2024-03-29 00:19:01, Dev, Step : 3900, Loss : 0.93837, Acc : 0.755, Auc : 0.780, Sensitive_Loss : 0.24144, Sensitive_Acc : 16.894, Sensitive_Auc : 0.995, Mean auc: 0.780, Run Time : 145.79 sec
INFO:root:2024-03-29 00:19:08, Train, Epoch : 8, Step : 3910, Loss : 0.64072, Acc : 0.831, Sensitive_Loss : 0.17872, Sensitive_Acc : 17.000, Run Time : 152.94 sec
INFO:root:2024-03-29 00:19:18, Train, Epoch : 8, Step : 3920, Loss : 0.52458, Acc : 0.762, Sensitive_Loss : 0.28785, Sensitive_Acc : 18.700, Run Time : 10.32 sec
INFO:root:2024-03-29 00:19:29, Train, Epoch : 8, Step : 3930, Loss : 0.52001, Acc : 0.822, Sensitive_Loss : 0.24544, Sensitive_Acc : 16.100, Run Time : 10.40 sec
INFO:root:2024-03-29 00:21:11
INFO:root:y_pred: [0.26153365 0.16961154 0.446529   ... 0.50977814 0.6468822  0.20173922]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [5.9085229e-04 7.4157916e-02 9.5124918e-01 2.7653111e-02 9.9162382e-01
 9.2296237e-03 1.4001268e-01 8.0080435e-02 4.2612585e-03 8.6235039e-02
 2.1498976e-02 1.3413486e-01 9.9064797e-01 1.6474959e-02 9.9691522e-01
 9.9073046e-01 2.6863493e-04 5.7029939e-01 9.7704834e-01 7.7244896e-01
 2.2795429e-02 1.4968900e-02 9.9999988e-01 9.8732620e-01 9.8452583e-02
 9.2846596e-01 2.2296165e-03 9.9993253e-01 9.8277675e-03 9.5113903e-01
 9.9965358e-01 9.3332297e-01 3.6172038e-03 6.5836386e-05 3.5737240e-01
 1.8584380e-02 5.1151730e-02 2.8710911e-01 9.3723506e-01 9.3627989e-01
 8.2030911e-03 1.7300736e-02 9.7810388e-01 9.9985266e-01 3.5610309e-01
 2.3313781e-02 4.1777495e-02 3.3999559e-02 9.9351585e-01 9.1927153e-01
 9.6555036e-01 6.0220593e-01 9.9955708e-01 9.9319977e-01 9.8625213e-01
 8.7202334e-01 7.8290540e-01 6.0166694e-02 9.9988353e-01 1.7723669e-01
 8.5452310e-04 6.4327884e-01 9.5498008e-01 9.8037982e-01 9.8471522e-01
 9.9665219e-01 7.8157827e-02 9.9858284e-01 9.9734062e-01 9.8860884e-01
 9.9931574e-01 1.2735023e-02 1.1776372e-02 6.1998498e-01 9.0558982e-01
 8.1322249e-03 9.7888809e-01 7.8894931e-04 9.9677485e-01 9.1778420e-02
 4.3339732e-01 1.3114537e-01 1.9391932e-02 3.6912888e-02 9.9168605e-01
 3.6756241e-01 8.9693356e-01 6.1803520e-01 1.4723245e-02 1.9356595e-01
 5.1146317e-03 1.7040980e-01 6.4225355e-03 2.6643207e-02 9.6688896e-01
 8.0996114e-01 3.7435687e-01 1.4810498e-02 9.2303151e-01 9.4603127e-01
 9.6585631e-01 7.6568611e-02 9.9991298e-01 2.1927913e-01 1.2473897e-01
 9.9723095e-01 1.5339978e-01 8.9813704e-03 2.1176089e-03 6.5994244e-03
 3.2200913e-03 9.9596357e-01 3.7510928e-02 1.6978201e-03 7.7460355e-01
 4.0585981e-04 9.9085170e-01 6.0887408e-02 1.0138526e-01 3.7377528e-04
 1.9812173e-01 2.6711205e-02 9.9593484e-01 1.0798214e-03 1.0007433e-01
 9.9990845e-01 2.8994394e-02 9.5249140e-01 1.0599380e-01 9.9420542e-01
 1.2485851e-03 9.9779308e-01 9.3105622e-03 9.9992871e-01 9.9508733e-01
 3.5082480e-01 1.0464869e-01 9.4262379e-01 2.5340961e-03 9.7990900e-01
 9.9657804e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 00:21:11, Dev, Step : 3936, Loss : 0.94446, Acc : 0.715, Auc : 0.788, Sensitive_Loss : 0.25679, Sensitive_Acc : 16.922, Sensitive_Auc : 0.996, Mean auc: 0.788, Run Time : 95.87 sec
INFO:root:2024-03-29 00:21:19, Train, Epoch : 9, Step : 3940, Loss : 0.24698, Acc : 0.322, Sensitive_Loss : 0.05870, Sensitive_Acc : 7.000, Run Time : 5.11 sec
INFO:root:2024-03-29 00:21:30, Train, Epoch : 9, Step : 3950, Loss : 0.58374, Acc : 0.794, Sensitive_Loss : 0.17486, Sensitive_Acc : 16.500, Run Time : 10.87 sec
INFO:root:2024-03-29 00:21:40, Train, Epoch : 9, Step : 3960, Loss : 0.66424, Acc : 0.762, Sensitive_Loss : 0.19512, Sensitive_Acc : 15.100, Run Time : 10.49 sec
INFO:root:2024-03-29 00:21:51, Train, Epoch : 9, Step : 3970, Loss : 0.74201, Acc : 0.812, Sensitive_Loss : 0.20669, Sensitive_Acc : 14.700, Run Time : 10.90 sec
INFO:root:2024-03-29 00:22:02, Train, Epoch : 9, Step : 3980, Loss : 0.83493, Acc : 0.747, Sensitive_Loss : 0.22081, Sensitive_Acc : 17.400, Run Time : 10.90 sec
INFO:root:2024-03-29 00:22:13, Train, Epoch : 9, Step : 3990, Loss : 0.75253, Acc : 0.762, Sensitive_Loss : 0.28669, Sensitive_Acc : 17.000, Run Time : 10.95 sec
INFO:root:2024-03-29 00:22:23, Train, Epoch : 9, Step : 4000, Loss : 0.61538, Acc : 0.819, Sensitive_Loss : 0.20232, Sensitive_Acc : 15.400, Run Time : 10.35 sec
INFO:root:2024-03-29 00:24:00, Dev, Step : 4000, Loss : 0.99137, Acc : 0.746, Auc : 0.757, Sensitive_Loss : 0.24181, Sensitive_Acc : 16.950, Sensitive_Auc : 0.996, Mean auc: 0.757, Run Time : 96.74 sec
INFO:root:2024-03-29 00:24:08, Train, Epoch : 9, Step : 4010, Loss : 0.61038, Acc : 0.769, Sensitive_Loss : 0.29766, Sensitive_Acc : 16.800, Run Time : 104.47 sec
INFO:root:2024-03-29 00:24:19, Train, Epoch : 9, Step : 4020, Loss : 0.68578, Acc : 0.769, Sensitive_Loss : 0.21889, Sensitive_Acc : 17.500, Run Time : 11.31 sec
INFO:root:2024-03-29 00:24:30, Train, Epoch : 9, Step : 4030, Loss : 0.64757, Acc : 0.772, Sensitive_Loss : 0.23611, Sensitive_Acc : 16.400, Run Time : 10.62 sec
INFO:root:2024-03-29 00:24:43, Train, Epoch : 9, Step : 4040, Loss : 0.75530, Acc : 0.797, Sensitive_Loss : 0.19172, Sensitive_Acc : 17.200, Run Time : 12.77 sec
INFO:root:2024-03-29 00:24:54, Train, Epoch : 9, Step : 4050, Loss : 0.74735, Acc : 0.753, Sensitive_Loss : 0.26245, Sensitive_Acc : 18.000, Run Time : 11.09 sec
INFO:root:2024-03-29 00:25:05, Train, Epoch : 9, Step : 4060, Loss : 0.49179, Acc : 0.766, Sensitive_Loss : 0.32187, Sensitive_Acc : 15.600, Run Time : 11.24 sec
INFO:root:2024-03-29 00:25:17, Train, Epoch : 9, Step : 4070, Loss : 0.61171, Acc : 0.812, Sensitive_Loss : 0.23431, Sensitive_Acc : 18.300, Run Time : 12.03 sec
INFO:root:2024-03-29 00:25:31, Train, Epoch : 9, Step : 4080, Loss : 0.76512, Acc : 0.781, Sensitive_Loss : 0.20256, Sensitive_Acc : 15.400, Run Time : 14.36 sec
INFO:root:2024-03-29 00:25:42, Train, Epoch : 9, Step : 4090, Loss : 0.94215, Acc : 0.731, Sensitive_Loss : 0.24441, Sensitive_Acc : 20.200, Run Time : 10.27 sec
INFO:root:2024-03-29 00:25:53, Train, Epoch : 9, Step : 4100, Loss : 0.70659, Acc : 0.797, Sensitive_Loss : 0.23097, Sensitive_Acc : 15.100, Run Time : 11.11 sec
INFO:root:2024-03-29 00:28:01, Dev, Step : 4100, Loss : 1.02830, Acc : 0.780, Auc : 0.748, Sensitive_Loss : 0.24592, Sensitive_Acc : 16.936, Sensitive_Auc : 0.997, Mean auc: 0.748, Run Time : 128.48 sec
INFO:root:2024-03-29 00:28:08, Train, Epoch : 9, Step : 4110, Loss : 0.63856, Acc : 0.762, Sensitive_Loss : 0.17652, Sensitive_Acc : 17.100, Run Time : 135.20 sec
INFO:root:2024-03-29 00:28:17, Train, Epoch : 9, Step : 4120, Loss : 0.57299, Acc : 0.766, Sensitive_Loss : 0.21439, Sensitive_Acc : 15.500, Run Time : 9.51 sec
INFO:root:2024-03-29 00:28:28, Train, Epoch : 9, Step : 4130, Loss : 0.54060, Acc : 0.806, Sensitive_Loss : 0.22036, Sensitive_Acc : 15.400, Run Time : 10.16 sec
INFO:root:2024-03-29 00:28:37, Train, Epoch : 9, Step : 4140, Loss : 0.60591, Acc : 0.775, Sensitive_Loss : 0.17925, Sensitive_Acc : 17.500, Run Time : 9.64 sec
INFO:root:2024-03-29 00:28:46, Train, Epoch : 9, Step : 4150, Loss : 0.59732, Acc : 0.744, Sensitive_Loss : 0.25519, Sensitive_Acc : 17.200, Run Time : 9.32 sec
INFO:root:2024-03-29 00:28:57, Train, Epoch : 9, Step : 4160, Loss : 0.50153, Acc : 0.778, Sensitive_Loss : 0.21583, Sensitive_Acc : 18.000, Run Time : 10.20 sec
INFO:root:2024-03-29 00:29:06, Train, Epoch : 9, Step : 4170, Loss : 0.66994, Acc : 0.762, Sensitive_Loss : 0.26081, Sensitive_Acc : 18.600, Run Time : 9.81 sec
INFO:root:2024-03-29 00:29:16, Train, Epoch : 9, Step : 4180, Loss : 0.61941, Acc : 0.747, Sensitive_Loss : 0.25237, Sensitive_Acc : 13.800, Run Time : 9.81 sec
INFO:root:2024-03-29 00:29:26, Train, Epoch : 9, Step : 4190, Loss : 0.75238, Acc : 0.778, Sensitive_Loss : 0.18500, Sensitive_Acc : 15.400, Run Time : 9.98 sec
INFO:root:2024-03-29 00:29:37, Train, Epoch : 9, Step : 4200, Loss : 0.51775, Acc : 0.772, Sensitive_Loss : 0.21924, Sensitive_Acc : 16.400, Run Time : 10.54 sec
INFO:root:2024-03-29 00:31:46, Dev, Step : 4200, Loss : 0.97337, Acc : 0.763, Auc : 0.772, Sensitive_Loss : 0.24747, Sensitive_Acc : 16.965, Sensitive_Auc : 0.995, Mean auc: 0.772, Run Time : 128.73 sec
INFO:root:2024-03-29 00:31:52, Train, Epoch : 9, Step : 4210, Loss : 0.55721, Acc : 0.781, Sensitive_Loss : 0.24733, Sensitive_Acc : 17.100, Run Time : 135.31 sec
INFO:root:2024-03-29 00:32:02, Train, Epoch : 9, Step : 4220, Loss : 0.54523, Acc : 0.762, Sensitive_Loss : 0.23415, Sensitive_Acc : 16.000, Run Time : 9.86 sec
INFO:root:2024-03-29 00:32:12, Train, Epoch : 9, Step : 4230, Loss : 0.78606, Acc : 0.797, Sensitive_Loss : 0.22354, Sensitive_Acc : 15.800, Run Time : 9.88 sec
INFO:root:2024-03-29 00:32:24, Train, Epoch : 9, Step : 4240, Loss : 0.67193, Acc : 0.772, Sensitive_Loss : 0.26956, Sensitive_Acc : 16.000, Run Time : 11.74 sec
INFO:root:2024-03-29 00:32:34, Train, Epoch : 9, Step : 4250, Loss : 0.67955, Acc : 0.769, Sensitive_Loss : 0.15007, Sensitive_Acc : 17.900, Run Time : 10.07 sec
INFO:root:2024-03-29 00:32:43, Train, Epoch : 9, Step : 4260, Loss : 0.41855, Acc : 0.809, Sensitive_Loss : 0.18605, Sensitive_Acc : 17.600, Run Time : 9.27 sec
INFO:root:2024-03-29 00:32:53, Train, Epoch : 9, Step : 4270, Loss : 0.58527, Acc : 0.800, Sensitive_Loss : 0.20925, Sensitive_Acc : 16.200, Run Time : 10.03 sec
INFO:root:2024-03-29 00:33:06, Train, Epoch : 9, Step : 4280, Loss : 0.65288, Acc : 0.794, Sensitive_Loss : 0.23274, Sensitive_Acc : 17.000, Run Time : 13.39 sec
INFO:root:2024-03-29 00:33:16, Train, Epoch : 9, Step : 4290, Loss : 0.60651, Acc : 0.759, Sensitive_Loss : 0.17336, Sensitive_Acc : 17.800, Run Time : 9.30 sec
INFO:root:2024-03-29 00:33:25, Train, Epoch : 9, Step : 4300, Loss : 0.74301, Acc : 0.781, Sensitive_Loss : 0.25061, Sensitive_Acc : 17.000, Run Time : 9.40 sec
INFO:root:2024-03-29 00:35:28, Dev, Step : 4300, Loss : 0.98256, Acc : 0.830, Auc : 0.771, Sensitive_Loss : 0.23419, Sensitive_Acc : 17.064, Sensitive_Auc : 0.993, Mean auc: 0.771, Run Time : 123.00 sec
INFO:root:2024-03-29 00:35:35, Train, Epoch : 9, Step : 4310, Loss : 0.62349, Acc : 0.766, Sensitive_Loss : 0.22122, Sensitive_Acc : 15.500, Run Time : 129.55 sec
INFO:root:2024-03-29 00:35:45, Train, Epoch : 9, Step : 4320, Loss : 0.55261, Acc : 0.750, Sensitive_Loss : 0.26968, Sensitive_Acc : 17.300, Run Time : 10.79 sec
INFO:root:2024-03-29 00:35:55, Train, Epoch : 9, Step : 4330, Loss : 0.45978, Acc : 0.781, Sensitive_Loss : 0.24935, Sensitive_Acc : 16.700, Run Time : 9.78 sec
INFO:root:2024-03-29 00:36:05, Train, Epoch : 9, Step : 4340, Loss : 0.54074, Acc : 0.806, Sensitive_Loss : 0.19534, Sensitive_Acc : 15.800, Run Time : 9.58 sec
INFO:root:2024-03-29 00:36:14, Train, Epoch : 9, Step : 4350, Loss : 0.66412, Acc : 0.812, Sensitive_Loss : 0.20741, Sensitive_Acc : 15.800, Run Time : 9.25 sec
INFO:root:2024-03-29 00:36:24, Train, Epoch : 9, Step : 4360, Loss : 0.61288, Acc : 0.806, Sensitive_Loss : 0.19499, Sensitive_Acc : 16.500, Run Time : 10.40 sec
INFO:root:2024-03-29 00:36:34, Train, Epoch : 9, Step : 4370, Loss : 0.71190, Acc : 0.822, Sensitive_Loss : 0.25720, Sensitive_Acc : 17.700, Run Time : 9.64 sec
INFO:root:2024-03-29 00:36:44, Train, Epoch : 9, Step : 4380, Loss : 0.57812, Acc : 0.772, Sensitive_Loss : 0.22666, Sensitive_Acc : 15.600, Run Time : 9.91 sec
INFO:root:2024-03-29 00:36:54, Train, Epoch : 9, Step : 4390, Loss : 0.77287, Acc : 0.791, Sensitive_Loss : 0.22541, Sensitive_Acc : 14.500, Run Time : 10.44 sec
INFO:root:2024-03-29 00:37:06, Train, Epoch : 9, Step : 4400, Loss : 0.64753, Acc : 0.803, Sensitive_Loss : 0.16996, Sensitive_Acc : 17.400, Run Time : 11.45 sec
INFO:root:2024-03-29 00:39:18, Dev, Step : 4400, Loss : 0.96056, Acc : 0.718, Auc : 0.783, Sensitive_Loss : 0.23211, Sensitive_Acc : 17.050, Sensitive_Auc : 0.994, Mean auc: 0.783, Run Time : 131.66 sec
INFO:root:2024-03-29 00:39:24, Train, Epoch : 9, Step : 4410, Loss : 0.56612, Acc : 0.778, Sensitive_Loss : 0.21810, Sensitive_Acc : 15.200, Run Time : 138.24 sec
INFO:root:2024-03-29 00:39:34, Train, Epoch : 9, Step : 4420, Loss : 0.81799, Acc : 0.775, Sensitive_Loss : 0.23006, Sensitive_Acc : 16.400, Run Time : 9.96 sec
INFO:root:2024-03-29 00:41:18
INFO:root:y_pred: [0.2112931  0.06301902 0.26759696 ... 0.23298863 0.348519   0.1395713 ]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [4.64099110e-04 9.25545320e-02 9.65518057e-01 1.64235178e-02
 9.88862455e-01 7.22884480e-03 1.97255999e-01 6.14678562e-02
 2.67217215e-03 8.08597207e-02 1.68700144e-02 5.16695008e-02
 9.91865695e-01 8.62343609e-03 9.98262942e-01 9.80054975e-01
 2.51688791e-04 6.03861272e-01 9.56487298e-01 7.56894708e-01
 1.54511230e-02 8.10812507e-03 1.00000000e+00 9.90786612e-01
 5.31172082e-02 9.76414680e-01 2.62452336e-03 9.99934673e-01
 3.91272502e-03 9.17612433e-01 9.99738276e-01 9.61583138e-01
 2.51125684e-03 4.65858648e-05 2.48085603e-01 5.77294221e-03
 3.10575962e-02 2.02639312e-01 9.62589920e-01 9.09603596e-01
 2.06600479e-03 2.96332035e-02 9.56906855e-01 9.99755442e-01
 4.30339456e-01 2.48199068e-02 1.45030068e-02 3.14775072e-02
 9.87766981e-01 9.27331567e-01 9.56451118e-01 5.10557353e-01
 9.99365032e-01 9.93784308e-01 9.82298374e-01 8.87090385e-01
 8.17662001e-01 4.22482938e-02 9.99901533e-01 3.07569116e-01
 2.57022359e-04 5.57654738e-01 9.25578535e-01 9.82832968e-01
 9.85652089e-01 9.97488856e-01 3.52607630e-02 9.97911394e-01
 9.97479022e-01 9.89905059e-01 9.98981893e-01 5.48454374e-03
 1.08903628e-02 4.56617922e-01 9.55121636e-01 6.72192220e-03
 9.58459020e-01 4.76157875e-04 9.94248509e-01 1.30489111e-01
 4.84177411e-01 7.74617642e-02 2.06016935e-02 3.12741250e-02
 9.91520584e-01 2.24953696e-01 9.50676858e-01 5.49775839e-01
 1.84124131e-02 1.53684065e-01 6.16453588e-03 7.78764263e-02
 6.34059776e-03 3.14420015e-02 9.66362596e-01 8.27739894e-01
 3.27837169e-01 1.19542032e-02 9.02830303e-01 9.59995449e-01
 9.78182912e-01 9.92109478e-02 9.99932647e-01 1.19617410e-01
 1.24559470e-01 9.98130381e-01 1.56413659e-01 1.27391359e-02
 2.31507770e-03 3.87385441e-03 1.31000229e-03 9.96658683e-01
 2.83658281e-02 1.56203995e-03 7.67114699e-01 9.62967024e-05
 9.89871323e-01 3.53506729e-02 1.15774624e-01 2.99708336e-04
 2.39625663e-01 2.88368929e-02 9.97049034e-01 8.97917082e-04
 7.37566873e-02 9.99932647e-01 1.96803641e-02 9.54787016e-01
 1.38005897e-01 9.95062768e-01 8.02576775e-04 9.96902287e-01
 6.14089752e-03 9.99939919e-01 9.95793581e-01 3.10859978e-01
 8.35569203e-02 8.68852198e-01 4.60209791e-03 9.74302590e-01
 9.98878777e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 00:41:18, Dev, Step : 4428, Loss : 0.94569, Acc : 0.787, Auc : 0.781, Sensitive_Loss : 0.24281, Sensitive_Acc : 16.950, Sensitive_Auc : 0.994, Mean auc: 0.781, Run Time : 95.86 sec
INFO:root:2024-03-29 00:41:22, Train, Epoch : 10, Step : 4430, Loss : 0.09309, Acc : 0.166, Sensitive_Loss : 0.02308, Sensitive_Acc : 2.800, Run Time : 3.09 sec
INFO:root:2024-03-29 00:41:33, Train, Epoch : 10, Step : 4440, Loss : 0.61862, Acc : 0.831, Sensitive_Loss : 0.19267, Sensitive_Acc : 16.700, Run Time : 11.13 sec
INFO:root:2024-03-29 00:41:45, Train, Epoch : 10, Step : 4450, Loss : 0.48382, Acc : 0.800, Sensitive_Loss : 0.18201, Sensitive_Acc : 16.800, Run Time : 11.89 sec
INFO:root:2024-03-29 00:41:55, Train, Epoch : 10, Step : 4460, Loss : 0.51064, Acc : 0.781, Sensitive_Loss : 0.19350, Sensitive_Acc : 18.400, Run Time : 10.18 sec
INFO:root:2024-03-29 00:42:06, Train, Epoch : 10, Step : 4470, Loss : 0.62700, Acc : 0.772, Sensitive_Loss : 0.24513, Sensitive_Acc : 16.400, Run Time : 11.48 sec
INFO:root:2024-03-29 00:42:16, Train, Epoch : 10, Step : 4480, Loss : 0.58256, Acc : 0.822, Sensitive_Loss : 0.16295, Sensitive_Acc : 17.000, Run Time : 10.00 sec
INFO:root:2024-03-29 00:42:27, Train, Epoch : 10, Step : 4490, Loss : 0.55077, Acc : 0.806, Sensitive_Loss : 0.21696, Sensitive_Acc : 17.200, Run Time : 10.55 sec
INFO:root:2024-03-29 00:42:40, Train, Epoch : 10, Step : 4500, Loss : 0.54208, Acc : 0.787, Sensitive_Loss : 0.21649, Sensitive_Acc : 15.600, Run Time : 12.62 sec
INFO:root:2024-03-29 00:44:19, Dev, Step : 4500, Loss : 0.98469, Acc : 0.803, Auc : 0.765, Sensitive_Loss : 0.25571, Sensitive_Acc : 16.922, Sensitive_Auc : 0.993, Mean auc: 0.765, Run Time : 99.87 sec
INFO:root:2024-03-29 00:44:30, Train, Epoch : 10, Step : 4510, Loss : 0.55724, Acc : 0.803, Sensitive_Loss : 0.21020, Sensitive_Acc : 17.500, Run Time : 110.36 sec
INFO:root:2024-03-29 00:44:42, Train, Epoch : 10, Step : 4520, Loss : 0.49532, Acc : 0.834, Sensitive_Loss : 0.22501, Sensitive_Acc : 15.500, Run Time : 11.70 sec
INFO:root:2024-03-29 00:44:53, Train, Epoch : 10, Step : 4530, Loss : 0.57130, Acc : 0.787, Sensitive_Loss : 0.27117, Sensitive_Acc : 18.000, Run Time : 11.01 sec
INFO:root:2024-03-29 00:45:05, Train, Epoch : 10, Step : 4540, Loss : 0.49846, Acc : 0.819, Sensitive_Loss : 0.25406, Sensitive_Acc : 16.700, Run Time : 11.99 sec
INFO:root:2024-03-29 00:45:18, Train, Epoch : 10, Step : 4550, Loss : 0.53811, Acc : 0.803, Sensitive_Loss : 0.20759, Sensitive_Acc : 16.500, Run Time : 13.90 sec
INFO:root:2024-03-29 00:45:29, Train, Epoch : 10, Step : 4560, Loss : 0.58645, Acc : 0.812, Sensitive_Loss : 0.22815, Sensitive_Acc : 16.500, Run Time : 10.31 sec
INFO:root:2024-03-29 00:45:40, Train, Epoch : 10, Step : 4570, Loss : 0.48996, Acc : 0.769, Sensitive_Loss : 0.15700, Sensitive_Acc : 16.200, Run Time : 11.35 sec
INFO:root:2024-03-29 00:45:50, Train, Epoch : 10, Step : 4580, Loss : 0.60360, Acc : 0.791, Sensitive_Loss : 0.23720, Sensitive_Acc : 16.300, Run Time : 10.27 sec
INFO:root:2024-03-29 00:46:01, Train, Epoch : 10, Step : 4590, Loss : 0.52290, Acc : 0.806, Sensitive_Loss : 0.18964, Sensitive_Acc : 17.100, Run Time : 10.99 sec
INFO:root:2024-03-29 00:46:12, Train, Epoch : 10, Step : 4600, Loss : 0.56163, Acc : 0.794, Sensitive_Loss : 0.18536, Sensitive_Acc : 16.800, Run Time : 11.00 sec
INFO:root:2024-03-29 00:48:41, Dev, Step : 4600, Loss : 0.96302, Acc : 0.778, Auc : 0.775, Sensitive_Loss : 0.23200, Sensitive_Acc : 16.965, Sensitive_Auc : 0.994, Mean auc: 0.775, Run Time : 148.14 sec
INFO:root:2024-03-29 00:48:48, Train, Epoch : 10, Step : 4610, Loss : 0.66339, Acc : 0.809, Sensitive_Loss : 0.20343, Sensitive_Acc : 15.800, Run Time : 155.67 sec
INFO:root:2024-03-29 00:48:58, Train, Epoch : 10, Step : 4620, Loss : 0.54909, Acc : 0.797, Sensitive_Loss : 0.17380, Sensitive_Acc : 15.900, Run Time : 9.47 sec
INFO:root:2024-03-29 00:49:10, Train, Epoch : 10, Step : 4630, Loss : 0.69560, Acc : 0.797, Sensitive_Loss : 0.21978, Sensitive_Acc : 16.900, Run Time : 12.10 sec
INFO:root:2024-03-29 00:49:21, Train, Epoch : 10, Step : 4640, Loss : 0.57477, Acc : 0.794, Sensitive_Loss : 0.21356, Sensitive_Acc : 16.600, Run Time : 11.68 sec
INFO:root:2024-03-29 00:49:32, Train, Epoch : 10, Step : 4650, Loss : 0.67610, Acc : 0.797, Sensitive_Loss : 0.19059, Sensitive_Acc : 17.800, Run Time : 10.65 sec
INFO:root:2024-03-29 00:49:43, Train, Epoch : 10, Step : 4660, Loss : 0.69943, Acc : 0.775, Sensitive_Loss : 0.20898, Sensitive_Acc : 17.000, Run Time : 11.41 sec
INFO:root:2024-03-29 00:49:54, Train, Epoch : 10, Step : 4670, Loss : 0.48643, Acc : 0.800, Sensitive_Loss : 0.26485, Sensitive_Acc : 16.700, Run Time : 10.66 sec
INFO:root:2024-03-29 00:50:04, Train, Epoch : 10, Step : 4680, Loss : 0.59850, Acc : 0.791, Sensitive_Loss : 0.19664, Sensitive_Acc : 15.800, Run Time : 9.87 sec
INFO:root:2024-03-29 00:50:14, Train, Epoch : 10, Step : 4690, Loss : 0.52776, Acc : 0.797, Sensitive_Loss : 0.26476, Sensitive_Acc : 15.700, Run Time : 10.30 sec
INFO:root:2024-03-29 00:50:25, Train, Epoch : 10, Step : 4700, Loss : 0.62345, Acc : 0.800, Sensitive_Loss : 0.20982, Sensitive_Acc : 17.100, Run Time : 10.45 sec
INFO:root:2024-03-29 00:52:45, Dev, Step : 4700, Loss : 0.98410, Acc : 0.732, Auc : 0.768, Sensitive_Loss : 0.25455, Sensitive_Acc : 16.950, Sensitive_Auc : 0.993, Mean auc: 0.768, Run Time : 140.07 sec
INFO:root:2024-03-29 00:52:51, Train, Epoch : 10, Step : 4710, Loss : 0.65972, Acc : 0.806, Sensitive_Loss : 0.23117, Sensitive_Acc : 15.700, Run Time : 146.78 sec
INFO:root:2024-03-29 00:53:01, Train, Epoch : 10, Step : 4720, Loss : 0.48435, Acc : 0.781, Sensitive_Loss : 0.22750, Sensitive_Acc : 18.700, Run Time : 9.85 sec
INFO:root:2024-03-29 00:53:11, Train, Epoch : 10, Step : 4730, Loss : 0.56070, Acc : 0.794, Sensitive_Loss : 0.19938, Sensitive_Acc : 18.600, Run Time : 9.91 sec
INFO:root:2024-03-29 00:53:21, Train, Epoch : 10, Step : 4740, Loss : 0.67022, Acc : 0.797, Sensitive_Loss : 0.24614, Sensitive_Acc : 16.100, Run Time : 10.13 sec
INFO:root:2024-03-29 00:53:31, Train, Epoch : 10, Step : 4750, Loss : 0.57151, Acc : 0.797, Sensitive_Loss : 0.23672, Sensitive_Acc : 17.100, Run Time : 9.95 sec
INFO:root:2024-03-29 00:53:41, Train, Epoch : 10, Step : 4760, Loss : 0.65692, Acc : 0.781, Sensitive_Loss : 0.16298, Sensitive_Acc : 15.100, Run Time : 9.47 sec
INFO:root:2024-03-29 00:53:51, Train, Epoch : 10, Step : 4770, Loss : 0.58416, Acc : 0.772, Sensitive_Loss : 0.20180, Sensitive_Acc : 17.100, Run Time : 9.97 sec
INFO:root:2024-03-29 00:54:03, Train, Epoch : 10, Step : 4780, Loss : 0.62721, Acc : 0.797, Sensitive_Loss : 0.22295, Sensitive_Acc : 16.000, Run Time : 12.33 sec
INFO:root:2024-03-29 00:54:13, Train, Epoch : 10, Step : 4790, Loss : 0.49121, Acc : 0.787, Sensitive_Loss : 0.21362, Sensitive_Acc : 17.400, Run Time : 9.58 sec
INFO:root:2024-03-29 00:54:22, Train, Epoch : 10, Step : 4800, Loss : 0.59795, Acc : 0.787, Sensitive_Loss : 0.22673, Sensitive_Acc : 16.900, Run Time : 9.79 sec
INFO:root:2024-03-29 00:56:41, Dev, Step : 4800, Loss : 0.97920, Acc : 0.772, Auc : 0.773, Sensitive_Loss : 0.25718, Sensitive_Acc : 16.922, Sensitive_Auc : 0.994, Mean auc: 0.773, Run Time : 138.76 sec
INFO:root:2024-03-29 00:56:48, Train, Epoch : 10, Step : 4810, Loss : 0.59084, Acc : 0.816, Sensitive_Loss : 0.19810, Sensitive_Acc : 15.500, Run Time : 145.31 sec
INFO:root:2024-03-29 00:56:57, Train, Epoch : 10, Step : 4820, Loss : 0.58302, Acc : 0.812, Sensitive_Loss : 0.18031, Sensitive_Acc : 18.500, Run Time : 9.62 sec
INFO:root:2024-03-29 00:57:09, Train, Epoch : 10, Step : 4830, Loss : 0.61438, Acc : 0.803, Sensitive_Loss : 0.21285, Sensitive_Acc : 18.000, Run Time : 11.71 sec
INFO:root:2024-03-29 00:57:20, Train, Epoch : 10, Step : 4840, Loss : 0.64274, Acc : 0.809, Sensitive_Loss : 0.20337, Sensitive_Acc : 17.000, Run Time : 10.82 sec
INFO:root:2024-03-29 00:57:30, Train, Epoch : 10, Step : 4850, Loss : 0.53659, Acc : 0.834, Sensitive_Loss : 0.24968, Sensitive_Acc : 13.400, Run Time : 9.91 sec
INFO:root:2024-03-29 00:57:39, Train, Epoch : 10, Step : 4860, Loss : 0.53035, Acc : 0.784, Sensitive_Loss : 0.30853, Sensitive_Acc : 15.900, Run Time : 9.45 sec
INFO:root:2024-03-29 00:57:50, Train, Epoch : 10, Step : 4870, Loss : 0.59761, Acc : 0.838, Sensitive_Loss : 0.25052, Sensitive_Acc : 18.600, Run Time : 10.59 sec
INFO:root:2024-03-29 00:58:00, Train, Epoch : 10, Step : 4880, Loss : 0.46075, Acc : 0.809, Sensitive_Loss : 0.19293, Sensitive_Acc : 15.800, Run Time : 9.69 sec
INFO:root:2024-03-29 00:58:09, Train, Epoch : 10, Step : 4890, Loss : 0.62970, Acc : 0.816, Sensitive_Loss : 0.16329, Sensitive_Acc : 15.500, Run Time : 9.69 sec
INFO:root:2024-03-29 00:58:21, Train, Epoch : 10, Step : 4900, Loss : 0.49251, Acc : 0.806, Sensitive_Loss : 0.21698, Sensitive_Acc : 15.600, Run Time : 11.52 sec
INFO:root:2024-03-29 01:00:16, Dev, Step : 4900, Loss : 0.98233, Acc : 0.811, Auc : 0.778, Sensitive_Loss : 0.23489, Sensitive_Acc : 17.007, Sensitive_Auc : 0.993, Mean auc: 0.778, Run Time : 115.68 sec
INFO:root:2024-03-29 01:00:23, Train, Epoch : 10, Step : 4910, Loss : 0.59170, Acc : 0.816, Sensitive_Loss : 0.16954, Sensitive_Acc : 16.400, Run Time : 122.56 sec
INFO:root:2024-03-29 01:00:33, Train, Epoch : 10, Step : 4920, Loss : 0.59419, Acc : 0.841, Sensitive_Loss : 0.16191, Sensitive_Acc : 16.200, Run Time : 9.78 sec
INFO:root:2024-03-29 01:02:09
INFO:root:y_pred: [0.20662552 0.11166074 0.19713776 ... 0.24876079 0.16611621 0.09712813]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [1.86511854e-04 5.38641997e-02 9.32117760e-01 1.34184789e-02
 9.86522734e-01 3.07989493e-03 1.41529560e-01 4.72202450e-02
 1.68373180e-03 4.27523963e-02 1.54495789e-02 3.84530909e-02
 9.83134389e-01 3.42597300e-03 9.97277796e-01 9.81556773e-01
 1.99967879e-04 4.62550402e-01 9.33343768e-01 5.78517199e-01
 7.33796880e-03 5.30402269e-03 1.00000000e+00 9.91533816e-01
 4.45529297e-02 9.39040542e-01 1.28828862e-03 9.99898791e-01
 2.21358333e-03 9.12662446e-01 9.99810398e-01 9.26077604e-01
 1.14364130e-03 1.92407024e-05 2.34036610e-01 5.33311348e-03
 6.37316480e-02 1.22801155e-01 9.31893587e-01 8.67133856e-01
 2.43375450e-03 1.15980124e-02 9.52888250e-01 9.99695182e-01
 2.40452513e-01 1.81448571e-02 1.22878337e-02 3.76475751e-02
 9.91405725e-01 8.79281521e-01 9.56565082e-01 3.14383209e-01
 9.99403477e-01 9.84464228e-01 9.72538769e-01 8.28332067e-01
 7.28013575e-01 2.64089741e-02 9.99886751e-01 2.13891000e-01
 2.13086780e-04 4.33187395e-01 8.85215461e-01 9.83915508e-01
 9.87012804e-01 9.96947348e-01 5.59844077e-02 9.97725904e-01
 9.97392297e-01 9.89418328e-01 9.98986185e-01 2.63677654e-03
 5.47946151e-03 3.94818068e-01 9.14757073e-01 3.44067160e-03
 9.33574855e-01 2.61607755e-04 9.97897148e-01 6.18062243e-02
 1.99226052e-01 5.84718734e-02 2.82561127e-02 1.79103650e-02
 9.78461742e-01 2.69863307e-01 9.22331095e-01 4.89970058e-01
 3.72571424e-02 7.01191276e-02 4.89624590e-03 8.18034708e-02
 3.71093745e-03 2.32872274e-02 9.57450628e-01 6.76415563e-01
 1.46303087e-01 6.31198986e-03 7.62255073e-01 9.35306549e-01
 9.52128708e-01 8.76090378e-02 9.99907851e-01 1.59150288e-01
 8.01921263e-02 9.97373581e-01 9.94761288e-02 9.02640633e-03
 8.74672609e-04 1.70740159e-03 8.95100413e-04 9.95674312e-01
 2.23319270e-02 2.09447811e-03 5.62167108e-01 1.33977257e-04
 9.88140821e-01 2.17743367e-02 8.99145529e-02 1.81199037e-04
 1.54274434e-01 2.30074711e-02 9.96039867e-01 5.02855110e-04
 7.95763433e-02 9.99944806e-01 9.27636214e-03 9.21332419e-01
 6.45553991e-02 9.95070457e-01 4.90322651e-04 9.92479682e-01
 4.52243024e-03 9.99916315e-01 9.94300961e-01 2.63809323e-01
 4.51179817e-02 8.71075511e-01 1.95246143e-03 9.78960395e-01
 9.98936713e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 01:02:09, Dev, Step : 4920, Loss : 0.99823, Acc : 0.818, Auc : 0.773, Sensitive_Loss : 0.22305, Sensitive_Acc : 17.064, Sensitive_Auc : 0.993, Mean auc: 0.773, Run Time : 95.51 sec
