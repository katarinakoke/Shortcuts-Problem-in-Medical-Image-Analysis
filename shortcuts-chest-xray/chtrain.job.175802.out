Running on desktop25:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
3
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_val.csv",
    "pred_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Balanced_Sex_0_0.csv",
    "pred_model": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Balanced_Sex_0_01.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.1,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-17 22:25:11, Train, Epoch : 1, Step : 10, Loss : 0.70148, Acc : 0.519, Sensitive_Loss : 1.13784, Sensitive_Acc : 13.000, Run Time : 12.94 sec
INFO:root:2024-04-17 22:25:22, Train, Epoch : 1, Step : 20, Loss : 0.67609, Acc : 0.594, Sensitive_Loss : 1.02193, Sensitive_Acc : 19.800, Run Time : 11.86 sec
INFO:root:2024-04-17 22:25:35, Train, Epoch : 1, Step : 30, Loss : 0.65527, Acc : 0.594, Sensitive_Loss : 0.98680, Sensitive_Acc : 19.200, Run Time : 12.22 sec
INFO:root:2024-04-17 22:25:46, Train, Epoch : 1, Step : 40, Loss : 0.66733, Acc : 0.631, Sensitive_Loss : 0.87089, Sensitive_Acc : 23.500, Run Time : 11.36 sec
INFO:root:2024-04-17 22:25:58, Train, Epoch : 1, Step : 50, Loss : 0.66815, Acc : 0.562, Sensitive_Loss : 0.95151, Sensitive_Acc : 18.300, Run Time : 11.89 sec
INFO:root:2024-04-17 22:26:11, Train, Epoch : 1, Step : 60, Loss : 0.66776, Acc : 0.647, Sensitive_Loss : 0.90109, Sensitive_Acc : 18.600, Run Time : 12.84 sec
INFO:root:2024-04-17 22:26:23, Train, Epoch : 1, Step : 70, Loss : 0.64807, Acc : 0.653, Sensitive_Loss : 0.80135, Sensitive_Acc : 20.100, Run Time : 11.97 sec
INFO:root:2024-04-17 22:26:35, Train, Epoch : 1, Step : 80, Loss : 0.64335, Acc : 0.659, Sensitive_Loss : 0.71030, Sensitive_Acc : 13.300, Run Time : 12.02 sec
INFO:root:2024-04-17 22:26:47, Train, Epoch : 1, Step : 90, Loss : 0.69654, Acc : 0.650, Sensitive_Loss : 0.69835, Sensitive_Acc : 20.000, Run Time : 12.31 sec
INFO:root:2024-04-17 22:26:59, Train, Epoch : 1, Step : 100, Loss : 0.66202, Acc : 0.669, Sensitive_Loss : 0.70151, Sensitive_Acc : 20.200, Run Time : 11.74 sec
INFO:root:2024-04-17 22:29:39, Dev, Step : 100, Loss : 0.67666, Acc : 0.657, Auc : 0.730, Sensitive_Loss : 0.59542, Sensitive_Acc : 18.955, Sensitive_Auc : 0.907, Mean auc: 0.730, Run Time : 160.01 sec
INFO:root:2024-04-17 22:29:40, Best, Step : 100, Loss : 0.67666, Acc : 0.657, Auc : 0.730, Sensitive_Loss : 0.59542, Sensitive_Acc : 18.955, Sensitive_Auc : 0.907, Best Auc : 0.730
INFO:root:2024-04-17 22:29:48, Train, Epoch : 1, Step : 110, Loss : 0.61777, Acc : 0.681, Sensitive_Loss : 0.59823, Sensitive_Acc : 22.700, Run Time : 169.32 sec
INFO:root:2024-04-17 22:30:00, Train, Epoch : 1, Step : 120, Loss : 0.56650, Acc : 0.697, Sensitive_Loss : 0.50537, Sensitive_Acc : 19.400, Run Time : 11.62 sec
INFO:root:2024-04-17 22:30:11, Train, Epoch : 1, Step : 130, Loss : 0.63813, Acc : 0.703, Sensitive_Loss : 0.72419, Sensitive_Acc : 20.100, Run Time : 11.09 sec
INFO:root:2024-04-17 22:30:23, Train, Epoch : 1, Step : 140, Loss : 0.69984, Acc : 0.653, Sensitive_Loss : 0.58309, Sensitive_Acc : 18.600, Run Time : 11.74 sec
INFO:root:2024-04-17 22:30:34, Train, Epoch : 1, Step : 150, Loss : 0.58121, Acc : 0.662, Sensitive_Loss : 0.49296, Sensitive_Acc : 22.200, Run Time : 11.79 sec
INFO:root:2024-04-17 22:30:46, Train, Epoch : 1, Step : 160, Loss : 0.57823, Acc : 0.728, Sensitive_Loss : 0.45817, Sensitive_Acc : 14.600, Run Time : 11.25 sec
INFO:root:2024-04-17 22:30:57, Train, Epoch : 1, Step : 170, Loss : 0.62177, Acc : 0.700, Sensitive_Loss : 0.64517, Sensitive_Acc : 18.100, Run Time : 11.40 sec
INFO:root:2024-04-17 22:31:09, Train, Epoch : 1, Step : 180, Loss : 0.53747, Acc : 0.681, Sensitive_Loss : 0.39335, Sensitive_Acc : 18.100, Run Time : 12.26 sec
INFO:root:2024-04-17 22:31:21, Train, Epoch : 1, Step : 190, Loss : 0.66632, Acc : 0.644, Sensitive_Loss : 0.48424, Sensitive_Acc : 20.400, Run Time : 11.48 sec
INFO:root:2024-04-17 22:31:32, Train, Epoch : 1, Step : 200, Loss : 0.62632, Acc : 0.691, Sensitive_Loss : 0.39978, Sensitive_Acc : 22.200, Run Time : 11.38 sec
INFO:root:2024-04-17 22:34:03, Dev, Step : 200, Loss : 0.63063, Acc : 0.687, Auc : 0.752, Sensitive_Loss : 0.36488, Sensitive_Acc : 21.060, Sensitive_Auc : 0.951, Mean auc: 0.752, Run Time : 151.31 sec
INFO:root:2024-04-17 22:34:04, Best, Step : 200, Loss : 0.63063, Acc : 0.687, Auc : 0.752, Sensitive_Loss : 0.36488, Sensitive_Acc : 21.060, Sensitive_Auc : 0.951, Best Auc : 0.752
INFO:root:2024-04-17 22:34:13, Train, Epoch : 1, Step : 210, Loss : 0.57459, Acc : 0.688, Sensitive_Loss : 0.46008, Sensitive_Acc : 19.900, Run Time : 160.48 sec
INFO:root:2024-04-17 22:34:24, Train, Epoch : 1, Step : 220, Loss : 0.71458, Acc : 0.628, Sensitive_Loss : 0.57245, Sensitive_Acc : 20.300, Run Time : 11.05 sec
INFO:root:2024-04-17 22:34:36, Train, Epoch : 1, Step : 230, Loss : 0.66877, Acc : 0.672, Sensitive_Loss : 0.44903, Sensitive_Acc : 18.400, Run Time : 12.30 sec
INFO:root:2024-04-17 22:34:48, Train, Epoch : 1, Step : 240, Loss : 0.63114, Acc : 0.684, Sensitive_Loss : 0.46802, Sensitive_Acc : 22.900, Run Time : 11.72 sec
INFO:root:2024-04-17 22:34:59, Train, Epoch : 1, Step : 250, Loss : 0.56634, Acc : 0.697, Sensitive_Loss : 0.34709, Sensitive_Acc : 18.500, Run Time : 11.37 sec
INFO:root:2024-04-17 22:35:11, Train, Epoch : 1, Step : 260, Loss : 0.57711, Acc : 0.700, Sensitive_Loss : 0.34444, Sensitive_Acc : 23.700, Run Time : 11.93 sec
INFO:root:2024-04-17 22:35:23, Train, Epoch : 1, Step : 270, Loss : 0.57727, Acc : 0.688, Sensitive_Loss : 0.34760, Sensitive_Acc : 21.200, Run Time : 11.57 sec
INFO:root:2024-04-17 22:35:35, Train, Epoch : 1, Step : 280, Loss : 0.53877, Acc : 0.684, Sensitive_Loss : 0.37356, Sensitive_Acc : 18.900, Run Time : 12.20 sec
INFO:root:2024-04-17 22:35:47, Train, Epoch : 1, Step : 290, Loss : 0.56134, Acc : 0.725, Sensitive_Loss : 0.32296, Sensitive_Acc : 20.900, Run Time : 11.69 sec
INFO:root:2024-04-17 22:35:58, Train, Epoch : 1, Step : 300, Loss : 0.58214, Acc : 0.700, Sensitive_Loss : 0.33893, Sensitive_Acc : 19.200, Run Time : 11.85 sec
INFO:root:2024-04-17 22:38:26, Dev, Step : 300, Loss : 0.59022, Acc : 0.719, Auc : 0.794, Sensitive_Loss : 0.36496, Sensitive_Acc : 20.414, Sensitive_Auc : 0.937, Mean auc: 0.794, Run Time : 147.15 sec
INFO:root:2024-04-17 22:38:26, Best, Step : 300, Loss : 0.59022, Acc : 0.719, Auc : 0.794, Sensitive_Loss : 0.36496, Sensitive_Acc : 20.414, Sensitive_Auc : 0.937, Best Auc : 0.794
INFO:root:2024-04-17 22:38:35, Train, Epoch : 1, Step : 310, Loss : 0.56390, Acc : 0.744, Sensitive_Loss : 0.37943, Sensitive_Acc : 20.800, Run Time : 156.40 sec
INFO:root:2024-04-17 22:38:46, Train, Epoch : 1, Step : 320, Loss : 0.53697, Acc : 0.684, Sensitive_Loss : 0.28861, Sensitive_Acc : 19.100, Run Time : 11.68 sec
INFO:root:2024-04-17 22:38:58, Train, Epoch : 1, Step : 330, Loss : 0.65795, Acc : 0.669, Sensitive_Loss : 0.28910, Sensitive_Acc : 19.800, Run Time : 12.00 sec
INFO:root:2024-04-17 22:39:10, Train, Epoch : 1, Step : 340, Loss : 0.61040, Acc : 0.681, Sensitive_Loss : 0.36645, Sensitive_Acc : 23.400, Run Time : 11.97 sec
INFO:root:2024-04-17 22:39:22, Train, Epoch : 1, Step : 350, Loss : 0.62464, Acc : 0.744, Sensitive_Loss : 0.34568, Sensitive_Acc : 22.300, Run Time : 11.14 sec
INFO:root:2024-04-17 22:39:33, Train, Epoch : 1, Step : 360, Loss : 0.64840, Acc : 0.716, Sensitive_Loss : 0.37442, Sensitive_Acc : 16.700, Run Time : 11.31 sec
INFO:root:2024-04-17 22:39:46, Train, Epoch : 1, Step : 370, Loss : 0.59814, Acc : 0.691, Sensitive_Loss : 0.30868, Sensitive_Acc : 24.200, Run Time : 12.93 sec
INFO:root:2024-04-17 22:39:57, Train, Epoch : 1, Step : 380, Loss : 0.57325, Acc : 0.675, Sensitive_Loss : 0.38601, Sensitive_Acc : 18.700, Run Time : 11.26 sec
INFO:root:2024-04-17 22:40:09, Train, Epoch : 1, Step : 390, Loss : 0.53369, Acc : 0.713, Sensitive_Loss : 0.25071, Sensitive_Acc : 21.600, Run Time : 12.05 sec
INFO:root:2024-04-17 22:40:20, Train, Epoch : 1, Step : 400, Loss : 0.55833, Acc : 0.731, Sensitive_Loss : 0.33987, Sensitive_Acc : 22.000, Run Time : 11.27 sec
INFO:root:2024-04-17 22:42:47, Dev, Step : 400, Loss : 0.58261, Acc : 0.724, Auc : 0.794, Sensitive_Loss : 0.32878, Sensitive_Acc : 20.895, Sensitive_Auc : 0.951, Mean auc: 0.794, Run Time : 147.05 sec
INFO:root:2024-04-17 22:42:55, Train, Epoch : 1, Step : 410, Loss : 0.63001, Acc : 0.713, Sensitive_Loss : 0.31534, Sensitive_Acc : 20.000, Run Time : 155.00 sec
INFO:root:2024-04-17 22:43:08, Train, Epoch : 1, Step : 420, Loss : 0.64766, Acc : 0.703, Sensitive_Loss : 0.34081, Sensitive_Acc : 23.100, Run Time : 12.62 sec
INFO:root:2024-04-17 22:43:18, Train, Epoch : 1, Step : 430, Loss : 0.57166, Acc : 0.694, Sensitive_Loss : 0.53280, Sensitive_Acc : 19.800, Run Time : 10.46 sec
INFO:root:2024-04-17 22:43:30, Train, Epoch : 1, Step : 440, Loss : 0.50078, Acc : 0.753, Sensitive_Loss : 0.35801, Sensitive_Acc : 11.800, Run Time : 11.91 sec
INFO:root:2024-04-17 22:43:42, Train, Epoch : 1, Step : 450, Loss : 0.54119, Acc : 0.738, Sensitive_Loss : 0.49813, Sensitive_Acc : 21.400, Run Time : 11.72 sec
INFO:root:2024-04-17 22:43:54, Train, Epoch : 1, Step : 460, Loss : 0.52834, Acc : 0.728, Sensitive_Loss : 0.39842, Sensitive_Acc : 22.200, Run Time : 12.14 sec
INFO:root:2024-04-17 22:44:07, Train, Epoch : 1, Step : 470, Loss : 0.52568, Acc : 0.741, Sensitive_Loss : 0.29031, Sensitive_Acc : 19.400, Run Time : 12.48 sec
INFO:root:2024-04-17 22:44:19, Train, Epoch : 1, Step : 480, Loss : 0.63040, Acc : 0.666, Sensitive_Loss : 0.24071, Sensitive_Acc : 20.400, Run Time : 12.00 sec
INFO:root:2024-04-17 22:44:30, Train, Epoch : 1, Step : 490, Loss : 0.64946, Acc : 0.669, Sensitive_Loss : 0.36259, Sensitive_Acc : 22.000, Run Time : 11.72 sec
INFO:root:2024-04-17 22:44:42, Train, Epoch : 1, Step : 500, Loss : 0.54250, Acc : 0.716, Sensitive_Loss : 0.28539, Sensitive_Acc : 18.800, Run Time : 11.22 sec
INFO:root:2024-04-17 22:47:09, Dev, Step : 500, Loss : 0.58049, Acc : 0.727, Auc : 0.800, Sensitive_Loss : 0.30800, Sensitive_Acc : 21.000, Sensitive_Auc : 0.972, Mean auc: 0.800, Run Time : 147.42 sec
INFO:root:2024-04-17 22:47:10, Best, Step : 500, Loss : 0.58049, Acc : 0.727, Auc : 0.800, Sensitive_Loss : 0.30800, Sensitive_Acc : 21.000, Sensitive_Auc : 0.972, Best Auc : 0.800
INFO:root:2024-04-17 22:47:18, Train, Epoch : 1, Step : 510, Loss : 0.56501, Acc : 0.719, Sensitive_Loss : 0.28686, Sensitive_Acc : 21.600, Run Time : 156.68 sec
INFO:root:2024-04-17 22:47:30, Train, Epoch : 1, Step : 520, Loss : 0.55174, Acc : 0.728, Sensitive_Loss : 0.23559, Sensitive_Acc : 18.400, Run Time : 11.79 sec
INFO:root:2024-04-17 22:47:43, Train, Epoch : 1, Step : 530, Loss : 0.56706, Acc : 0.728, Sensitive_Loss : 0.26538, Sensitive_Acc : 17.400, Run Time : 12.85 sec
INFO:root:2024-04-17 22:47:53, Train, Epoch : 1, Step : 540, Loss : 0.56587, Acc : 0.709, Sensitive_Loss : 0.19828, Sensitive_Acc : 18.500, Run Time : 10.47 sec
INFO:root:2024-04-17 22:48:06, Train, Epoch : 1, Step : 550, Loss : 0.57705, Acc : 0.703, Sensitive_Loss : 0.30918, Sensitive_Acc : 25.700, Run Time : 12.18 sec
INFO:root:2024-04-17 22:48:17, Train, Epoch : 1, Step : 560, Loss : 0.52915, Acc : 0.719, Sensitive_Loss : 0.20648, Sensitive_Acc : 19.900, Run Time : 11.62 sec
INFO:root:2024-04-17 22:48:29, Train, Epoch : 1, Step : 570, Loss : 0.61165, Acc : 0.744, Sensitive_Loss : 0.21840, Sensitive_Acc : 20.900, Run Time : 11.91 sec
INFO:root:2024-04-17 22:48:41, Train, Epoch : 1, Step : 580, Loss : 0.48932, Acc : 0.772, Sensitive_Loss : 0.32335, Sensitive_Acc : 22.000, Run Time : 12.05 sec
INFO:root:2024-04-17 22:48:53, Train, Epoch : 1, Step : 590, Loss : 0.53045, Acc : 0.728, Sensitive_Loss : 0.37309, Sensitive_Acc : 16.300, Run Time : 12.04 sec
INFO:root:2024-04-17 22:49:04, Train, Epoch : 1, Step : 600, Loss : 0.61173, Acc : 0.728, Sensitive_Loss : 0.33037, Sensitive_Acc : 17.700, Run Time : 11.03 sec
INFO:root:2024-04-17 22:51:31, Dev, Step : 600, Loss : 0.58098, Acc : 0.730, Auc : 0.803, Sensitive_Loss : 0.28129, Sensitive_Acc : 20.714, Sensitive_Auc : 0.969, Mean auc: 0.803, Run Time : 146.72 sec
INFO:root:2024-04-17 22:51:32, Best, Step : 600, Loss : 0.58098, Acc : 0.730, Auc : 0.803, Sensitive_Loss : 0.28129, Sensitive_Acc : 20.714, Sensitive_Auc : 0.969, Best Auc : 0.803
INFO:root:2024-04-17 22:51:40, Train, Epoch : 1, Step : 610, Loss : 0.53172, Acc : 0.738, Sensitive_Loss : 0.26509, Sensitive_Acc : 22.100, Run Time : 156.11 sec
INFO:root:2024-04-17 22:51:52, Train, Epoch : 1, Step : 620, Loss : 0.59809, Acc : 0.747, Sensitive_Loss : 0.33997, Sensitive_Acc : 21.500, Run Time : 12.13 sec
INFO:root:2024-04-17 22:52:04, Train, Epoch : 1, Step : 630, Loss : 0.57164, Acc : 0.731, Sensitive_Loss : 0.26675, Sensitive_Acc : 14.200, Run Time : 11.54 sec
INFO:root:2024-04-17 22:54:33
INFO:root:y_pred: [0.03038671 0.01954376 0.12618056 ... 0.142445   0.21217425 0.14707157]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [3.35726619e-01 6.70269411e-03 1.35116484e-02 6.74642622e-02
 3.66271771e-02 8.66832398e-03 3.06179468e-02 6.15822244e-03
 7.37386286e-01 9.99461591e-01 1.55450627e-01 1.95526215e-03
 4.52723093e-02 8.88164970e-04 9.98572826e-01 2.32758522e-01
 3.35417092e-02 9.98760104e-01 9.99248207e-01 3.55547629e-02
 7.82647371e-01 8.83159861e-02 1.56060323e-01 3.35484684e-01
 8.14848989e-02 8.30242038e-01 7.43855126e-05 4.75022430e-03
 1.17586448e-03 7.41223916e-02 5.17888032e-02 9.98266578e-01
 6.21410966e-01 2.81229168e-01 1.27359212e-03 5.97733166e-03
 1.88826814e-01 8.08300748e-02 7.82430917e-02 1.40515104e-01
 3.98448944e-01 9.97126281e-01 3.10117662e-01 6.03788756e-02
 9.07914340e-01 7.55385041e-01 9.13188279e-01 3.45027894e-01
 5.16339503e-02 9.96052861e-01 8.92361939e-01 9.98283923e-01
 9.92169559e-01 8.07698369e-02 1.41189426e-01 3.19229871e-01
 1.47716608e-02 2.45024785e-01 9.98300850e-01 7.08301514e-02
 1.58011063e-03 1.20804156e-03 2.99745351e-02 1.45441405e-02
 9.95635569e-01 2.68132627e-01 3.81989195e-03 4.66123894e-02
 2.00525252e-03 8.63223732e-01 9.99250352e-01 9.99294639e-01
 2.13195886e-02 3.68106067e-01 1.85096823e-02 8.19735527e-01
 2.55619064e-02 2.62435246e-03 4.20422573e-03 1.78386029e-02
 5.61569929e-01 1.90093964e-02 9.74646866e-01 9.83586967e-01
 2.59218603e-01 1.60199717e-01 4.50059325e-01 1.28136249e-02
 7.56581724e-02 1.20291542e-02 1.96853489e-01 6.72832489e-01
 5.71866333e-03 8.81740532e-04 1.05538771e-01 1.54942617e-01
 1.87592041e-02 2.61274844e-01 1.04246726e-02 5.45798168e-02
 6.73426911e-02 2.81374939e-02 3.73048276e-01 3.75556550e-03
 1.72146052e-01 1.98241454e-02 2.88879067e-01 7.44126081e-01
 9.77179468e-01 1.88462660e-01 2.79696076e-04 9.99665022e-01
 9.99721229e-01 3.44056025e-04 1.93816885e-01 7.02822030e-01
 4.25519854e-01 2.31099036e-02 1.04471855e-01 7.02628940e-02
 1.91970244e-02 1.29274977e-03 1.52413800e-01 9.02339583e-04
 4.63779718e-02 9.05747950e-01 3.67383026e-02 9.87852216e-01
 2.02040836e-01 3.95402014e-02 1.88392848e-01 4.54876363e-01
 2.14452579e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-17 22:54:33, Dev, Step : 634, Loss : 0.58554, Acc : 0.729, Auc : 0.817, Sensitive_Loss : 0.30318, Sensitive_Acc : 20.729, Sensitive_Auc : 0.976, Mean auc: 0.817, Run Time : 144.53 sec
INFO:root:2024-04-17 22:54:34, Best, Step : 634, Loss : 0.58554, Acc : 0.729,Auc : 0.817, Best Auc : 0.817, Sensitive_Loss : 0.30318, Sensitive_Acc : 20.729, Sensitive_Auc : 0.976
INFO:root:2024-04-17 22:54:43, Train, Epoch : 2, Step : 640, Loss : 0.31121, Acc : 0.447, Sensitive_Loss : 0.08928, Sensitive_Acc : 11.400, Run Time : 8.36 sec
INFO:root:2024-04-17 22:54:55, Train, Epoch : 2, Step : 650, Loss : 0.60424, Acc : 0.725, Sensitive_Loss : 0.20140, Sensitive_Acc : 17.600, Run Time : 12.01 sec
INFO:root:2024-04-17 22:55:06, Train, Epoch : 2, Step : 660, Loss : 0.51475, Acc : 0.753, Sensitive_Loss : 0.23429, Sensitive_Acc : 20.500, Run Time : 11.59 sec
INFO:root:2024-04-17 22:55:18, Train, Epoch : 2, Step : 670, Loss : 0.54302, Acc : 0.725, Sensitive_Loss : 0.27334, Sensitive_Acc : 20.100, Run Time : 11.87 sec
INFO:root:2024-04-17 22:55:30, Train, Epoch : 2, Step : 680, Loss : 0.53584, Acc : 0.747, Sensitive_Loss : 0.28455, Sensitive_Acc : 18.300, Run Time : 11.29 sec
INFO:root:2024-04-17 22:55:41, Train, Epoch : 2, Step : 690, Loss : 0.46614, Acc : 0.769, Sensitive_Loss : 0.23047, Sensitive_Acc : 20.500, Run Time : 11.01 sec
INFO:root:2024-04-17 22:55:51, Train, Epoch : 2, Step : 700, Loss : 0.54331, Acc : 0.750, Sensitive_Loss : 0.22954, Sensitive_Acc : 25.600, Run Time : 10.31 sec
INFO:root:2024-04-17 22:58:18, Dev, Step : 700, Loss : 0.56092, Acc : 0.745, Auc : 0.815, Sensitive_Loss : 0.27052, Sensitive_Acc : 20.729, Sensitive_Auc : 0.974, Mean auc: 0.815, Run Time : 147.25 sec
INFO:root:2024-04-17 22:58:27, Train, Epoch : 2, Step : 710, Loss : 0.50715, Acc : 0.753, Sensitive_Loss : 0.16180, Sensitive_Acc : 20.900, Run Time : 155.60 sec
INFO:root:2024-04-17 22:58:38, Train, Epoch : 2, Step : 720, Loss : 0.57812, Acc : 0.756, Sensitive_Loss : 0.31575, Sensitive_Acc : 15.800, Run Time : 11.95 sec
INFO:root:2024-04-17 22:58:51, Train, Epoch : 2, Step : 730, Loss : 0.46500, Acc : 0.787, Sensitive_Loss : 0.25267, Sensitive_Acc : 24.800, Run Time : 12.06 sec
INFO:root:2024-04-17 22:59:02, Train, Epoch : 2, Step : 740, Loss : 0.60578, Acc : 0.709, Sensitive_Loss : 0.21954, Sensitive_Acc : 15.800, Run Time : 11.29 sec
INFO:root:2024-04-17 22:59:13, Train, Epoch : 2, Step : 750, Loss : 0.50918, Acc : 0.741, Sensitive_Loss : 0.16602, Sensitive_Acc : 22.300, Run Time : 11.36 sec
INFO:root:2024-04-17 22:59:24, Train, Epoch : 2, Step : 760, Loss : 0.54703, Acc : 0.747, Sensitive_Loss : 0.26886, Sensitive_Acc : 20.900, Run Time : 10.45 sec
INFO:root:2024-04-17 22:59:35, Train, Epoch : 2, Step : 770, Loss : 0.60089, Acc : 0.709, Sensitive_Loss : 0.30516, Sensitive_Acc : 24.000, Run Time : 11.70 sec
INFO:root:2024-04-17 22:59:47, Train, Epoch : 2, Step : 780, Loss : 0.47489, Acc : 0.784, Sensitive_Loss : 0.35363, Sensitive_Acc : 21.100, Run Time : 11.93 sec
INFO:root:2024-04-17 22:59:58, Train, Epoch : 2, Step : 790, Loss : 0.54505, Acc : 0.772, Sensitive_Loss : 0.23068, Sensitive_Acc : 21.900, Run Time : 11.23 sec
INFO:root:2024-04-17 23:00:09, Train, Epoch : 2, Step : 800, Loss : 0.54031, Acc : 0.738, Sensitive_Loss : 0.25194, Sensitive_Acc : 19.600, Run Time : 10.62 sec
INFO:root:2024-04-17 23:03:13, Dev, Step : 800, Loss : 0.56859, Acc : 0.725, Auc : 0.806, Sensitive_Loss : 0.25511, Sensitive_Acc : 21.286, Sensitive_Auc : 0.977, Mean auc: 0.806, Run Time : 183.52 sec
INFO:root:2024-04-17 23:03:24, Train, Epoch : 2, Step : 810, Loss : 0.53414, Acc : 0.781, Sensitive_Loss : 0.24462, Sensitive_Acc : 20.400, Run Time : 194.46 sec
INFO:root:2024-04-17 23:03:37, Train, Epoch : 2, Step : 820, Loss : 0.60372, Acc : 0.747, Sensitive_Loss : 0.16093, Sensitive_Acc : 24.200, Run Time : 13.21 sec
INFO:root:2024-04-17 23:03:49, Train, Epoch : 2, Step : 830, Loss : 0.58812, Acc : 0.734, Sensitive_Loss : 0.18263, Sensitive_Acc : 25.700, Run Time : 11.90 sec
INFO:root:2024-04-17 23:04:02, Train, Epoch : 2, Step : 840, Loss : 0.58967, Acc : 0.725, Sensitive_Loss : 0.21933, Sensitive_Acc : 22.100, Run Time : 13.59 sec
INFO:root:2024-04-17 23:04:16, Train, Epoch : 2, Step : 850, Loss : 0.49663, Acc : 0.744, Sensitive_Loss : 0.25832, Sensitive_Acc : 23.600, Run Time : 13.55 sec
INFO:root:2024-04-17 23:04:30, Train, Epoch : 2, Step : 860, Loss : 0.49574, Acc : 0.744, Sensitive_Loss : 0.26026, Sensitive_Acc : 21.600, Run Time : 13.76 sec
INFO:root:2024-04-17 23:04:43, Train, Epoch : 2, Step : 870, Loss : 0.55025, Acc : 0.750, Sensitive_Loss : 0.22278, Sensitive_Acc : 25.000, Run Time : 13.71 sec
INFO:root:2024-04-17 23:04:58, Train, Epoch : 2, Step : 880, Loss : 0.56021, Acc : 0.741, Sensitive_Loss : 0.18047, Sensitive_Acc : 24.000, Run Time : 14.28 sec
INFO:root:2024-04-17 23:05:10, Train, Epoch : 2, Step : 890, Loss : 0.49945, Acc : 0.766, Sensitive_Loss : 0.22269, Sensitive_Acc : 22.300, Run Time : 12.36 sec
INFO:root:2024-04-17 23:05:23, Train, Epoch : 2, Step : 900, Loss : 0.52624, Acc : 0.728, Sensitive_Loss : 0.22071, Sensitive_Acc : 22.700, Run Time : 13.23 sec
INFO:root:2024-04-17 23:08:05, Dev, Step : 900, Loss : 0.55878, Acc : 0.754, Auc : 0.827, Sensitive_Loss : 0.30856, Sensitive_Acc : 20.639, Sensitive_Auc : 0.991, Mean auc: 0.827, Run Time : 161.57 sec
INFO:root:2024-04-17 23:08:06, Best, Step : 900, Loss : 0.55878, Acc : 0.754, Auc : 0.827, Sensitive_Loss : 0.30856, Sensitive_Acc : 20.639, Sensitive_Auc : 0.991, Best Auc : 0.827
INFO:root:2024-04-17 23:08:15, Train, Epoch : 2, Step : 910, Loss : 0.51663, Acc : 0.762, Sensitive_Loss : 0.17086, Sensitive_Acc : 19.000, Run Time : 171.76 sec
INFO:root:2024-04-17 23:08:26, Train, Epoch : 2, Step : 920, Loss : 0.46606, Acc : 0.741, Sensitive_Loss : 0.21070, Sensitive_Acc : 18.100, Run Time : 11.57 sec
INFO:root:2024-04-17 23:08:37, Train, Epoch : 2, Step : 930, Loss : 0.50223, Acc : 0.741, Sensitive_Loss : 0.29949, Sensitive_Acc : 23.600, Run Time : 10.66 sec
INFO:root:2024-04-17 23:08:49, Train, Epoch : 2, Step : 940, Loss : 0.55337, Acc : 0.725, Sensitive_Loss : 0.31317, Sensitive_Acc : 25.300, Run Time : 11.80 sec
INFO:root:2024-04-17 23:09:00, Train, Epoch : 2, Step : 950, Loss : 0.57157, Acc : 0.738, Sensitive_Loss : 0.21348, Sensitive_Acc : 20.400, Run Time : 11.32 sec
INFO:root:2024-04-17 23:09:11, Train, Epoch : 2, Step : 960, Loss : 0.51832, Acc : 0.734, Sensitive_Loss : 0.25869, Sensitive_Acc : 25.000, Run Time : 10.91 sec
INFO:root:2024-04-17 23:09:23, Train, Epoch : 2, Step : 970, Loss : 0.46833, Acc : 0.750, Sensitive_Loss : 0.19536, Sensitive_Acc : 20.100, Run Time : 11.78 sec
INFO:root:2024-04-17 23:09:34, Train, Epoch : 2, Step : 980, Loss : 0.55180, Acc : 0.747, Sensitive_Loss : 0.16608, Sensitive_Acc : 21.000, Run Time : 11.16 sec
INFO:root:2024-04-17 23:09:46, Train, Epoch : 2, Step : 990, Loss : 0.43147, Acc : 0.772, Sensitive_Loss : 0.17088, Sensitive_Acc : 23.300, Run Time : 11.43 sec
INFO:root:2024-04-17 23:09:57, Train, Epoch : 2, Step : 1000, Loss : 0.55129, Acc : 0.725, Sensitive_Loss : 0.21151, Sensitive_Acc : 18.700, Run Time : 11.39 sec
INFO:root:2024-04-17 23:12:24, Dev, Step : 1000, Loss : 0.54996, Acc : 0.750, Auc : 0.826, Sensitive_Loss : 0.20651, Sensitive_Acc : 21.647, Sensitive_Auc : 0.984, Mean auc: 0.826, Run Time : 147.16 sec
INFO:root:2024-04-17 23:12:32, Train, Epoch : 2, Step : 1010, Loss : 0.47903, Acc : 0.800, Sensitive_Loss : 0.15369, Sensitive_Acc : 23.600, Run Time : 155.27 sec
INFO:root:2024-04-17 23:12:44, Train, Epoch : 2, Step : 1020, Loss : 0.47270, Acc : 0.762, Sensitive_Loss : 0.32031, Sensitive_Acc : 24.800, Run Time : 11.49 sec
INFO:root:2024-04-17 23:12:57, Train, Epoch : 2, Step : 1030, Loss : 0.49125, Acc : 0.753, Sensitive_Loss : 0.29715, Sensitive_Acc : 18.000, Run Time : 12.90 sec
INFO:root:2024-04-17 23:13:08, Train, Epoch : 2, Step : 1040, Loss : 0.50383, Acc : 0.750, Sensitive_Loss : 0.30575, Sensitive_Acc : 17.400, Run Time : 11.02 sec
INFO:root:2024-04-17 23:13:18, Train, Epoch : 2, Step : 1050, Loss : 0.57044, Acc : 0.728, Sensitive_Loss : 0.18436, Sensitive_Acc : 21.500, Run Time : 10.59 sec
INFO:root:2024-04-17 23:13:30, Train, Epoch : 2, Step : 1060, Loss : 0.52821, Acc : 0.722, Sensitive_Loss : 0.23793, Sensitive_Acc : 22.600, Run Time : 11.43 sec
INFO:root:2024-04-17 23:13:40, Train, Epoch : 2, Step : 1070, Loss : 0.56183, Acc : 0.731, Sensitive_Loss : 0.19009, Sensitive_Acc : 21.500, Run Time : 10.68 sec
INFO:root:2024-04-17 23:13:52, Train, Epoch : 2, Step : 1080, Loss : 0.52316, Acc : 0.759, Sensitive_Loss : 0.13793, Sensitive_Acc : 24.100, Run Time : 11.65 sec
INFO:root:2024-04-17 23:14:04, Train, Epoch : 2, Step : 1090, Loss : 0.51015, Acc : 0.731, Sensitive_Loss : 0.23057, Sensitive_Acc : 22.800, Run Time : 11.61 sec
INFO:root:2024-04-17 23:14:15, Train, Epoch : 2, Step : 1100, Loss : 0.51237, Acc : 0.734, Sensitive_Loss : 0.25409, Sensitive_Acc : 22.300, Run Time : 11.72 sec
INFO:root:2024-04-17 23:16:43, Dev, Step : 1100, Loss : 0.56573, Acc : 0.742, Auc : 0.833, Sensitive_Loss : 0.20863, Sensitive_Acc : 21.707, Sensitive_Auc : 0.990, Mean auc: 0.833, Run Time : 147.61 sec
INFO:root:2024-04-17 23:16:44, Best, Step : 1100, Loss : 0.56573, Acc : 0.742, Auc : 0.833, Sensitive_Loss : 0.20863, Sensitive_Acc : 21.707, Sensitive_Auc : 0.990, Best Auc : 0.833
INFO:root:2024-04-17 23:16:52, Train, Epoch : 2, Step : 1110, Loss : 0.56933, Acc : 0.762, Sensitive_Loss : 0.19317, Sensitive_Acc : 22.200, Run Time : 156.65 sec
INFO:root:2024-04-17 23:17:03, Train, Epoch : 2, Step : 1120, Loss : 0.50243, Acc : 0.766, Sensitive_Loss : 0.26943, Sensitive_Acc : 24.200, Run Time : 11.40 sec
INFO:root:2024-04-17 23:17:15, Train, Epoch : 2, Step : 1130, Loss : 0.44926, Acc : 0.787, Sensitive_Loss : 0.20905, Sensitive_Acc : 19.100, Run Time : 12.00 sec
INFO:root:2024-04-17 23:17:26, Train, Epoch : 2, Step : 1140, Loss : 0.54362, Acc : 0.741, Sensitive_Loss : 0.16375, Sensitive_Acc : 19.700, Run Time : 10.96 sec
INFO:root:2024-04-17 23:17:38, Train, Epoch : 2, Step : 1150, Loss : 0.51053, Acc : 0.784, Sensitive_Loss : 0.29060, Sensitive_Acc : 16.100, Run Time : 11.47 sec
INFO:root:2024-04-17 23:17:49, Train, Epoch : 2, Step : 1160, Loss : 0.46269, Acc : 0.816, Sensitive_Loss : 0.21766, Sensitive_Acc : 17.600, Run Time : 11.09 sec
INFO:root:2024-04-17 23:18:01, Train, Epoch : 2, Step : 1170, Loss : 0.53706, Acc : 0.759, Sensitive_Loss : 0.17494, Sensitive_Acc : 13.800, Run Time : 11.78 sec
INFO:root:2024-04-17 23:18:12, Train, Epoch : 2, Step : 1180, Loss : 0.48862, Acc : 0.756, Sensitive_Loss : 0.17154, Sensitive_Acc : 16.800, Run Time : 11.48 sec
INFO:root:2024-04-17 23:18:24, Train, Epoch : 2, Step : 1190, Loss : 0.52521, Acc : 0.728, Sensitive_Loss : 0.17066, Sensitive_Acc : 21.500, Run Time : 11.49 sec
INFO:root:2024-04-17 23:18:35, Train, Epoch : 2, Step : 1200, Loss : 0.52356, Acc : 0.741, Sensitive_Loss : 0.23182, Sensitive_Acc : 21.000, Run Time : 11.49 sec
INFO:root:2024-04-17 23:21:02, Dev, Step : 1200, Loss : 0.53723, Acc : 0.753, Auc : 0.830, Sensitive_Loss : 0.21556, Sensitive_Acc : 21.782, Sensitive_Auc : 0.995, Mean auc: 0.830, Run Time : 146.41 sec
INFO:root:2024-04-17 23:21:10, Train, Epoch : 2, Step : 1210, Loss : 0.57401, Acc : 0.734, Sensitive_Loss : 0.14885, Sensitive_Acc : 18.700, Run Time : 155.01 sec
INFO:root:2024-04-17 23:21:21, Train, Epoch : 2, Step : 1220, Loss : 0.52420, Acc : 0.734, Sensitive_Loss : 0.20069, Sensitive_Acc : 23.200, Run Time : 11.20 sec
INFO:root:2024-04-17 23:21:33, Train, Epoch : 2, Step : 1230, Loss : 0.60200, Acc : 0.759, Sensitive_Loss : 0.18665, Sensitive_Acc : 22.700, Run Time : 11.31 sec
INFO:root:2024-04-17 23:21:45, Train, Epoch : 2, Step : 1240, Loss : 0.56184, Acc : 0.775, Sensitive_Loss : 0.22752, Sensitive_Acc : 25.900, Run Time : 12.16 sec
INFO:root:2024-04-17 23:21:56, Train, Epoch : 2, Step : 1250, Loss : 0.51448, Acc : 0.731, Sensitive_Loss : 0.24601, Sensitive_Acc : 23.700, Run Time : 11.38 sec
INFO:root:2024-04-17 23:22:07, Train, Epoch : 2, Step : 1260, Loss : 0.47476, Acc : 0.716, Sensitive_Loss : 0.19812, Sensitive_Acc : 19.600, Run Time : 10.44 sec
INFO:root:2024-04-17 23:24:39
INFO:root:y_pred: [0.41219002 0.01621957 0.26798242 ... 0.32882366 0.1106341  0.2139431 ]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [9.82849821e-02 4.77138767e-03 7.08654895e-02 6.71566129e-02
 1.50549427e-01 5.34993655e-04 4.78566019e-03 4.05201083e-03
 6.60684168e-01 9.99997616e-01 8.79348457e-01 7.12281791e-03
 9.53141134e-03 7.54016591e-03 9.99984860e-01 4.99382019e-01
 4.97480296e-02 9.99985456e-01 9.99756873e-01 3.36838067e-02
 9.96417284e-01 2.85073482e-02 3.11132055e-02 5.84172197e-02
 5.54238915e-01 5.93974590e-01 9.98360192e-05 3.42181826e-04
 6.43873354e-04 1.50345266e-01 2.12554485e-02 9.96775448e-01
 6.88541889e-01 9.66344774e-01 2.14781356e-03 1.70470786e-03
 1.57434672e-01 1.58258811e-01 4.40292880e-02 4.96872999e-02
 1.77837595e-01 9.98678982e-01 2.97539476e-02 9.82693676e-03
 9.98039424e-01 9.28849101e-01 2.52380550e-01 5.04383504e-01
 1.46071404e-01 9.99682188e-01 9.92992520e-01 9.99920964e-01
 9.99359906e-01 1.50954593e-02 4.78398085e-01 3.02379072e-01
 1.43621981e-01 3.68431285e-02 9.96890128e-01 3.87774445e-02
 1.56664348e-03 3.20547522e-04 1.44281745e-01 4.20340709e-02
 9.99925256e-01 3.15366387e-01 2.36203242e-03 1.22596480e-01
 3.37163180e-01 9.99397755e-01 9.99956250e-01 9.99983549e-01
 1.00156842e-02 6.75124884e-01 1.15087815e-03 9.24953818e-01
 2.71159172e-01 1.69558945e-04 3.98810348e-03 5.38459383e-02
 8.06313574e-01 6.28245855e-03 9.97379959e-01 9.99501705e-01
 1.33281142e-01 2.03115772e-02 4.67764348e-01 3.88431782e-03
 4.53429997e-01 2.17490457e-02 3.33391055e-02 8.47077608e-01
 6.20873063e-04 1.20662584e-03 4.05529328e-02 2.45936647e-01
 1.42030325e-02 5.82202792e-01 8.81711766e-02 8.27613398e-02
 1.58990562e-01 1.70137715e-02 3.45853508e-01 2.11440288e-02
 5.66157140e-02 5.37384709e-04 2.92841464e-01 6.57331467e-01
 9.73249614e-01 7.27956653e-01 3.46545363e-04 9.99995232e-01
 9.99997139e-01 2.19164998e-04 2.39493653e-01 5.47740385e-02
 2.93009788e-01 8.70255567e-03 1.79589570e-01 1.50851518e-01
 6.49342686e-02 6.82716258e-04 5.98932356e-02 3.45966849e-03
 4.02835086e-02 9.76064980e-01 1.11167028e-03 9.99415636e-01
 5.78853011e-01 1.54506624e-01 7.90225491e-02 6.78142309e-01
 1.43138604e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-17 23:24:39, Dev, Step : 1268, Loss : 0.54139, Acc : 0.753, Auc : 0.828, Sensitive_Loss : 0.42908, Sensitive_Acc : 19.932, Sensitive_Auc : 0.994, Mean auc: 0.828, Run Time : 144.21 sec
INFO:root:2024-04-17 23:24:44, Train, Epoch : 3, Step : 1270, Loss : 0.10234, Acc : 0.159, Sensitive_Loss : 0.04452, Sensitive_Acc : 5.400, Run Time : 3.82 sec
INFO:root:2024-04-17 23:24:56, Train, Epoch : 3, Step : 1280, Loss : 0.40724, Acc : 0.828, Sensitive_Loss : 0.24543, Sensitive_Acc : 23.300, Run Time : 11.45 sec
INFO:root:2024-04-17 23:25:07, Train, Epoch : 3, Step : 1290, Loss : 0.45570, Acc : 0.794, Sensitive_Loss : 0.14304, Sensitive_Acc : 21.000, Run Time : 11.07 sec
INFO:root:2024-04-17 23:25:19, Train, Epoch : 3, Step : 1300, Loss : 0.47521, Acc : 0.800, Sensitive_Loss : 0.12308, Sensitive_Acc : 17.200, Run Time : 11.87 sec
INFO:root:2024-04-17 23:27:46, Dev, Step : 1300, Loss : 0.52707, Acc : 0.769, Auc : 0.841, Sensitive_Loss : 0.23550, Sensitive_Acc : 21.496, Sensitive_Auc : 0.995, Mean auc: 0.841, Run Time : 146.77 sec
INFO:root:2024-04-17 23:27:47, Best, Step : 1300, Loss : 0.52707, Acc : 0.769, Auc : 0.841, Sensitive_Loss : 0.23550, Sensitive_Acc : 21.496, Sensitive_Auc : 0.995, Best Auc : 0.841
INFO:root:2024-04-17 23:27:55, Train, Epoch : 3, Step : 1310, Loss : 0.43869, Acc : 0.791, Sensitive_Loss : 0.15309, Sensitive_Acc : 24.500, Run Time : 156.16 sec
INFO:root:2024-04-17 23:28:08, Train, Epoch : 3, Step : 1320, Loss : 0.41437, Acc : 0.828, Sensitive_Loss : 0.11210, Sensitive_Acc : 20.500, Run Time : 12.61 sec
INFO:root:2024-04-17 23:28:19, Train, Epoch : 3, Step : 1330, Loss : 0.49588, Acc : 0.784, Sensitive_Loss : 0.13851, Sensitive_Acc : 20.800, Run Time : 11.64 sec
INFO:root:2024-04-17 23:28:31, Train, Epoch : 3, Step : 1340, Loss : 0.50059, Acc : 0.772, Sensitive_Loss : 0.16425, Sensitive_Acc : 20.600, Run Time : 11.38 sec
INFO:root:2024-04-17 23:28:41, Train, Epoch : 3, Step : 1350, Loss : 0.46115, Acc : 0.787, Sensitive_Loss : 0.15561, Sensitive_Acc : 20.300, Run Time : 10.71 sec
INFO:root:2024-04-17 23:28:53, Train, Epoch : 3, Step : 1360, Loss : 0.51042, Acc : 0.794, Sensitive_Loss : 0.12342, Sensitive_Acc : 20.400, Run Time : 11.31 sec
INFO:root:2024-04-17 23:29:04, Train, Epoch : 3, Step : 1370, Loss : 0.42685, Acc : 0.838, Sensitive_Loss : 0.28747, Sensitive_Acc : 18.000, Run Time : 11.26 sec
INFO:root:2024-04-17 23:29:16, Train, Epoch : 3, Step : 1380, Loss : 0.45101, Acc : 0.781, Sensitive_Loss : 0.14143, Sensitive_Acc : 20.000, Run Time : 12.01 sec
INFO:root:2024-04-17 23:29:29, Train, Epoch : 3, Step : 1390, Loss : 0.44235, Acc : 0.794, Sensitive_Loss : 0.11880, Sensitive_Acc : 20.700, Run Time : 12.89 sec
INFO:root:2024-04-17 23:29:46, Train, Epoch : 3, Step : 1400, Loss : 0.45055, Acc : 0.831, Sensitive_Loss : 0.13490, Sensitive_Acc : 19.400, Run Time : 17.05 sec
INFO:root:2024-04-17 23:32:53, Dev, Step : 1400, Loss : 0.51412, Acc : 0.773, Auc : 0.848, Sensitive_Loss : 0.19354, Sensitive_Acc : 21.782, Sensitive_Auc : 0.997, Mean auc: 0.848, Run Time : 187.23 sec
INFO:root:2024-04-17 23:32:54, Best, Step : 1400, Loss : 0.51412, Acc : 0.773, Auc : 0.848, Sensitive_Loss : 0.19354, Sensitive_Acc : 21.782, Sensitive_Auc : 0.997, Best Auc : 0.848
INFO:root:2024-04-17 23:33:03, Train, Epoch : 3, Step : 1410, Loss : 0.45704, Acc : 0.766, Sensitive_Loss : 0.14972, Sensitive_Acc : 23.800, Run Time : 196.89 sec
INFO:root:2024-04-17 23:33:14, Train, Epoch : 3, Step : 1420, Loss : 0.46142, Acc : 0.800, Sensitive_Loss : 0.09802, Sensitive_Acc : 22.400, Run Time : 11.24 sec
INFO:root:2024-04-17 23:33:25, Train, Epoch : 3, Step : 1430, Loss : 0.46189, Acc : 0.781, Sensitive_Loss : 0.14240, Sensitive_Acc : 21.600, Run Time : 11.29 sec
INFO:root:2024-04-17 23:33:37, Train, Epoch : 3, Step : 1440, Loss : 0.44143, Acc : 0.819, Sensitive_Loss : 0.19132, Sensitive_Acc : 20.400, Run Time : 11.64 sec
INFO:root:2024-04-17 23:33:48, Train, Epoch : 3, Step : 1450, Loss : 0.41791, Acc : 0.812, Sensitive_Loss : 0.21374, Sensitive_Acc : 20.800, Run Time : 11.39 sec
INFO:root:2024-04-17 23:34:00, Train, Epoch : 3, Step : 1460, Loss : 0.42013, Acc : 0.844, Sensitive_Loss : 0.13171, Sensitive_Acc : 16.700, Run Time : 11.64 sec
INFO:root:2024-04-17 23:34:12, Train, Epoch : 3, Step : 1470, Loss : 0.44386, Acc : 0.828, Sensitive_Loss : 0.12000, Sensitive_Acc : 22.400, Run Time : 11.79 sec
INFO:root:2024-04-17 23:34:23, Train, Epoch : 3, Step : 1480, Loss : 0.37634, Acc : 0.844, Sensitive_Loss : 0.13218, Sensitive_Acc : 21.300, Run Time : 11.01 sec
INFO:root:2024-04-17 23:34:34, Train, Epoch : 3, Step : 1490, Loss : 0.49293, Acc : 0.766, Sensitive_Loss : 0.16930, Sensitive_Acc : 21.400, Run Time : 10.77 sec
INFO:root:2024-04-17 23:34:45, Train, Epoch : 3, Step : 1500, Loss : 0.42669, Acc : 0.803, Sensitive_Loss : 0.13199, Sensitive_Acc : 23.500, Run Time : 11.73 sec
INFO:root:2024-04-17 23:37:13, Dev, Step : 1500, Loss : 0.51668, Acc : 0.771, Auc : 0.850, Sensitive_Loss : 0.20230, Sensitive_Acc : 21.782, Sensitive_Auc : 0.998, Mean auc: 0.850, Run Time : 147.68 sec
INFO:root:2024-04-17 23:37:14, Best, Step : 1500, Loss : 0.51668, Acc : 0.771, Auc : 0.850, Sensitive_Loss : 0.20230, Sensitive_Acc : 21.782, Sensitive_Auc : 0.998, Best Auc : 0.850
INFO:root:2024-04-17 23:37:23, Train, Epoch : 3, Step : 1510, Loss : 0.49919, Acc : 0.803, Sensitive_Loss : 0.11845, Sensitive_Acc : 22.400, Run Time : 157.49 sec
INFO:root:2024-04-17 23:37:34, Train, Epoch : 3, Step : 1520, Loss : 0.46854, Acc : 0.753, Sensitive_Loss : 0.15489, Sensitive_Acc : 21.100, Run Time : 11.17 sec
INFO:root:2024-04-17 23:37:45, Train, Epoch : 3, Step : 1530, Loss : 0.47167, Acc : 0.803, Sensitive_Loss : 0.20239, Sensitive_Acc : 21.900, Run Time : 11.36 sec
INFO:root:2024-04-17 23:37:56, Train, Epoch : 3, Step : 1540, Loss : 0.45837, Acc : 0.787, Sensitive_Loss : 0.13095, Sensitive_Acc : 25.100, Run Time : 10.61 sec
INFO:root:2024-04-17 23:38:08, Train, Epoch : 3, Step : 1550, Loss : 0.48486, Acc : 0.787, Sensitive_Loss : 0.15018, Sensitive_Acc : 21.900, Run Time : 12.21 sec
INFO:root:2024-04-17 23:38:19, Train, Epoch : 3, Step : 1560, Loss : 0.41485, Acc : 0.841, Sensitive_Loss : 0.15146, Sensitive_Acc : 22.700, Run Time : 11.12 sec
INFO:root:2024-04-17 23:38:31, Train, Epoch : 3, Step : 1570, Loss : 0.40504, Acc : 0.791, Sensitive_Loss : 0.15925, Sensitive_Acc : 25.300, Run Time : 11.51 sec
INFO:root:2024-04-17 23:38:42, Train, Epoch : 3, Step : 1580, Loss : 0.47991, Acc : 0.778, Sensitive_Loss : 0.18902, Sensitive_Acc : 20.900, Run Time : 11.24 sec
INFO:root:2024-04-17 23:38:54, Train, Epoch : 3, Step : 1590, Loss : 0.48296, Acc : 0.797, Sensitive_Loss : 0.17168, Sensitive_Acc : 24.600, Run Time : 11.89 sec
INFO:root:2024-04-17 23:39:04, Train, Epoch : 3, Step : 1600, Loss : 0.46557, Acc : 0.787, Sensitive_Loss : 0.12389, Sensitive_Acc : 15.600, Run Time : 10.61 sec
INFO:root:2024-04-17 23:41:32, Dev, Step : 1600, Loss : 0.51345, Acc : 0.773, Auc : 0.851, Sensitive_Loss : 0.19317, Sensitive_Acc : 21.647, Sensitive_Auc : 0.998, Mean auc: 0.851, Run Time : 147.53 sec
INFO:root:2024-04-17 23:41:33, Best, Step : 1600, Loss : 0.51345, Acc : 0.773, Auc : 0.851, Sensitive_Loss : 0.19317, Sensitive_Acc : 21.647, Sensitive_Auc : 0.998, Best Auc : 0.851
INFO:root:2024-04-17 23:41:42, Train, Epoch : 3, Step : 1610, Loss : 0.40258, Acc : 0.787, Sensitive_Loss : 0.15351, Sensitive_Acc : 23.500, Run Time : 157.41 sec
INFO:root:2024-04-17 23:41:53, Train, Epoch : 3, Step : 1620, Loss : 0.47125, Acc : 0.800, Sensitive_Loss : 0.11717, Sensitive_Acc : 22.100, Run Time : 11.25 sec
INFO:root:2024-04-17 23:42:06, Train, Epoch : 3, Step : 1630, Loss : 0.49125, Acc : 0.759, Sensitive_Loss : 0.18504, Sensitive_Acc : 24.500, Run Time : 12.82 sec
INFO:root:2024-04-17 23:42:17, Train, Epoch : 3, Step : 1640, Loss : 0.44420, Acc : 0.794, Sensitive_Loss : 0.12920, Sensitive_Acc : 21.500, Run Time : 10.78 sec
INFO:root:2024-04-17 23:42:28, Train, Epoch : 3, Step : 1650, Loss : 0.48303, Acc : 0.766, Sensitive_Loss : 0.10707, Sensitive_Acc : 22.200, Run Time : 10.81 sec
INFO:root:2024-04-17 23:42:39, Train, Epoch : 3, Step : 1660, Loss : 0.46674, Acc : 0.797, Sensitive_Loss : 0.11235, Sensitive_Acc : 24.700, Run Time : 11.45 sec
INFO:root:2024-04-17 23:42:51, Train, Epoch : 3, Step : 1670, Loss : 0.44939, Acc : 0.822, Sensitive_Loss : 0.09147, Sensitive_Acc : 20.000, Run Time : 11.54 sec
INFO:root:2024-04-17 23:43:02, Train, Epoch : 3, Step : 1680, Loss : 0.38433, Acc : 0.831, Sensitive_Loss : 0.07480, Sensitive_Acc : 22.700, Run Time : 11.08 sec
INFO:root:2024-04-17 23:43:14, Train, Epoch : 3, Step : 1690, Loss : 0.39679, Acc : 0.838, Sensitive_Loss : 0.15171, Sensitive_Acc : 20.400, Run Time : 11.88 sec
INFO:root:2024-04-17 23:43:25, Train, Epoch : 3, Step : 1700, Loss : 0.45969, Acc : 0.784, Sensitive_Loss : 0.17058, Sensitive_Acc : 18.600, Run Time : 11.10 sec
INFO:root:2024-04-17 23:45:52, Dev, Step : 1700, Loss : 0.51505, Acc : 0.771, Auc : 0.851, Sensitive_Loss : 0.17841, Sensitive_Acc : 21.887, Sensitive_Auc : 0.998, Mean auc: 0.851, Run Time : 147.77 sec
INFO:root:2024-04-17 23:45:53, Best, Step : 1700, Loss : 0.51505, Acc : 0.771, Auc : 0.851, Sensitive_Loss : 0.17841, Sensitive_Acc : 21.887, Sensitive_Auc : 0.998, Best Auc : 0.851
INFO:root:2024-04-17 23:46:02, Train, Epoch : 3, Step : 1710, Loss : 0.48848, Acc : 0.769, Sensitive_Loss : 0.18370, Sensitive_Acc : 17.500, Run Time : 156.95 sec
INFO:root:2024-04-17 23:46:13, Train, Epoch : 3, Step : 1720, Loss : 0.37570, Acc : 0.812, Sensitive_Loss : 0.17461, Sensitive_Acc : 21.200, Run Time : 11.58 sec
INFO:root:2024-04-17 23:46:24, Train, Epoch : 3, Step : 1730, Loss : 0.45495, Acc : 0.800, Sensitive_Loss : 0.17846, Sensitive_Acc : 22.100, Run Time : 11.26 sec
INFO:root:2024-04-17 23:46:36, Train, Epoch : 3, Step : 1740, Loss : 0.46398, Acc : 0.781, Sensitive_Loss : 0.15853, Sensitive_Acc : 23.800, Run Time : 11.66 sec
INFO:root:2024-04-17 23:46:47, Train, Epoch : 3, Step : 1750, Loss : 0.44543, Acc : 0.797, Sensitive_Loss : 0.12568, Sensitive_Acc : 22.600, Run Time : 11.12 sec
INFO:root:2024-04-17 23:46:59, Train, Epoch : 3, Step : 1760, Loss : 0.53390, Acc : 0.772, Sensitive_Loss : 0.21948, Sensitive_Acc : 19.900, Run Time : 11.30 sec
INFO:root:2024-04-17 23:47:11, Train, Epoch : 3, Step : 1770, Loss : 0.46493, Acc : 0.772, Sensitive_Loss : 0.12246, Sensitive_Acc : 25.600, Run Time : 12.21 sec
INFO:root:2024-04-17 23:47:21, Train, Epoch : 3, Step : 1780, Loss : 0.41843, Acc : 0.787, Sensitive_Loss : 0.21851, Sensitive_Acc : 20.500, Run Time : 10.31 sec
INFO:root:2024-04-17 23:47:33, Train, Epoch : 3, Step : 1790, Loss : 0.42575, Acc : 0.803, Sensitive_Loss : 0.13111, Sensitive_Acc : 22.800, Run Time : 12.34 sec
INFO:root:2024-04-17 23:47:44, Train, Epoch : 3, Step : 1800, Loss : 0.43368, Acc : 0.822, Sensitive_Loss : 0.16608, Sensitive_Acc : 23.600, Run Time : 11.09 sec
INFO:root:2024-04-17 23:50:12, Dev, Step : 1800, Loss : 0.50620, Acc : 0.776, Auc : 0.854, Sensitive_Loss : 0.19546, Sensitive_Acc : 21.647, Sensitive_Auc : 0.998, Mean auc: 0.854, Run Time : 147.36 sec
INFO:root:2024-04-17 23:50:12, Best, Step : 1800, Loss : 0.50620, Acc : 0.776, Auc : 0.854, Sensitive_Loss : 0.19546, Sensitive_Acc : 21.647, Sensitive_Auc : 0.998, Best Auc : 0.854
INFO:root:2024-04-17 23:50:21, Train, Epoch : 3, Step : 1810, Loss : 0.39670, Acc : 0.806, Sensitive_Loss : 0.15686, Sensitive_Acc : 21.900, Run Time : 156.66 sec
INFO:root:2024-04-17 23:50:33, Train, Epoch : 3, Step : 1820, Loss : 0.44929, Acc : 0.791, Sensitive_Loss : 0.11778, Sensitive_Acc : 21.600, Run Time : 12.12 sec
INFO:root:2024-04-17 23:50:44, Train, Epoch : 3, Step : 1830, Loss : 0.43701, Acc : 0.775, Sensitive_Loss : 0.15881, Sensitive_Acc : 17.200, Run Time : 10.92 sec
INFO:root:2024-04-17 23:50:56, Train, Epoch : 3, Step : 1840, Loss : 0.41484, Acc : 0.809, Sensitive_Loss : 0.14711, Sensitive_Acc : 22.400, Run Time : 11.89 sec
INFO:root:2024-04-17 23:51:07, Train, Epoch : 3, Step : 1850, Loss : 0.44068, Acc : 0.800, Sensitive_Loss : 0.20479, Sensitive_Acc : 25.100, Run Time : 11.17 sec
INFO:root:2024-04-17 23:51:19, Train, Epoch : 3, Step : 1860, Loss : 0.39228, Acc : 0.841, Sensitive_Loss : 0.11303, Sensitive_Acc : 18.600, Run Time : 11.53 sec
INFO:root:2024-04-17 23:51:30, Train, Epoch : 3, Step : 1870, Loss : 0.38299, Acc : 0.838, Sensitive_Loss : 0.11204, Sensitive_Acc : 22.200, Run Time : 11.29 sec
INFO:root:2024-04-17 23:51:41, Train, Epoch : 3, Step : 1880, Loss : 0.40569, Acc : 0.778, Sensitive_Loss : 0.12024, Sensitive_Acc : 22.500, Run Time : 11.02 sec
INFO:root:2024-04-17 23:51:53, Train, Epoch : 3, Step : 1890, Loss : 0.45514, Acc : 0.797, Sensitive_Loss : 0.19056, Sensitive_Acc : 25.600, Run Time : 11.68 sec
INFO:root:2024-04-17 23:52:04, Train, Epoch : 3, Step : 1900, Loss : 0.45868, Acc : 0.794, Sensitive_Loss : 0.15202, Sensitive_Acc : 19.000, Run Time : 11.44 sec
INFO:root:2024-04-17 23:54:30, Dev, Step : 1900, Loss : 0.50840, Acc : 0.780, Auc : 0.856, Sensitive_Loss : 0.19144, Sensitive_Acc : 21.752, Sensitive_Auc : 0.998, Mean auc: 0.856, Run Time : 145.63 sec
INFO:root:2024-04-17 23:54:31, Best, Step : 1900, Loss : 0.50840, Acc : 0.780, Auc : 0.856, Sensitive_Loss : 0.19144, Sensitive_Acc : 21.752, Sensitive_Auc : 0.998, Best Auc : 0.856
INFO:root:2024-04-17 23:56:56
INFO:root:y_pred: [0.14174145 0.00772644 0.16198356 ... 0.14633521 0.08270266 0.06298672]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.54680328e-03 2.31408485e-04 1.02338549e-02 1.16149755e-02
 3.00160665e-02 4.14924216e-05 2.00929062e-04 1.85499981e-03
 3.52885932e-01 9.99973416e-01 5.39409101e-01 1.79849740e-03
 2.03587767e-03 8.19950423e-04 9.99866962e-01 1.57086983e-01
 1.27504403e-02 9.99921441e-01 9.99444544e-01 1.38746891e-02
 9.89703119e-01 1.53939473e-03 4.84529184e-03 2.41925148e-03
 1.05974421e-01 1.59977585e-01 8.53228994e-06 2.07117628e-04
 1.76494214e-04 1.14764692e-02 1.21594896e-03 9.96043086e-01
 3.53750139e-02 7.84663737e-01 1.45420534e-04 1.75930280e-03
 1.04213372e-01 4.56978530e-02 1.21937878e-02 8.75005219e-03
 6.14060387e-02 9.98439014e-01 2.83766584e-03 6.49933238e-03
 9.92759109e-01 7.56435171e-02 1.70037434e-01 2.84100175e-01
 5.92352115e-02 9.97823477e-01 9.80773389e-01 9.99668121e-01
 9.96891677e-01 7.56658847e-03 9.98013839e-02 1.48764672e-02
 5.09719644e-03 4.76548774e-03 9.97167528e-01 4.41900361e-03
 3.51558010e-05 6.15675177e-04 8.86366051e-03 1.58701581e-03
 9.99788463e-01 2.43472353e-01 1.40308359e-04 4.02201200e-03
 2.75303107e-02 9.97996211e-01 9.99497056e-01 9.99969244e-01
 5.78393228e-04 1.42632663e-01 2.17426437e-04 8.76809001e-01
 2.74522323e-02 5.64230231e-06 1.15084415e-03 1.11640254e-02
 2.20669061e-01 2.73837039e-04 9.98530388e-01 9.95588422e-01
 4.02968824e-02 5.71643875e-04 1.01356506e-01 6.25945104e-04
 3.66678722e-02 4.85007156e-04 1.85960252e-02 7.22198337e-02
 1.61040720e-04 1.18636512e-04 6.41070190e-04 1.20714232e-02
 1.82627328e-03 4.11794096e-01 8.65883101e-03 1.50008546e-03
 3.42349615e-03 2.33538356e-03 6.30079880e-02 1.48042641e-03
 1.22344680e-02 1.01294980e-04 1.48212006e-02 1.81110427e-01
 4.38426733e-01 3.37061673e-01 1.14969975e-04 9.99975204e-01
 9.99952793e-01 8.37076550e-06 8.38477388e-02 2.09132675e-02
 2.02525422e-01 7.04592268e-04 9.63951126e-02 3.27355154e-02
 7.98321737e-04 2.25277749e-04 6.32202858e-03 2.54028244e-04
 5.74454572e-03 8.47538173e-01 4.78147296e-04 9.81686950e-01
 1.98035389e-02 4.62473556e-02 2.88912263e-02 3.87468822e-02
 2.98536597e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-17 23:56:56, Dev, Step : 1902, Loss : 0.50914, Acc : 0.781, Auc : 0.856, Sensitive_Loss : 0.19271, Sensitive_Acc : 21.752, Sensitive_Auc : 0.998, Mean auc: 0.856, Run Time : 144.29 sec
INFO:root:2024-04-17 23:56:57, Best, Step : 1902, Loss : 0.50914, Acc : 0.781,Auc : 0.856, Best Auc : 0.856, Sensitive_Loss : 0.19271, Sensitive_Acc : 21.752, Sensitive_Auc : 0.998
INFO:root:2024-04-17 23:57:09, Train, Epoch : 4, Step : 1910, Loss : 0.33537, Acc : 0.631, Sensitive_Loss : 0.06917, Sensitive_Acc : 18.300, Run Time : 10.89 sec
INFO:root:2024-04-17 23:57:20, Train, Epoch : 4, Step : 1920, Loss : 0.39783, Acc : 0.856, Sensitive_Loss : 0.10878, Sensitive_Acc : 22.900, Run Time : 10.88 sec
INFO:root:2024-04-17 23:57:32, Train, Epoch : 4, Step : 1930, Loss : 0.39341, Acc : 0.838, Sensitive_Loss : 0.14271, Sensitive_Acc : 22.200, Run Time : 12.13 sec
INFO:root:2024-04-17 23:57:43, Train, Epoch : 4, Step : 1940, Loss : 0.42690, Acc : 0.812, Sensitive_Loss : 0.11218, Sensitive_Acc : 23.500, Run Time : 11.09 sec
INFO:root:2024-04-17 23:57:54, Train, Epoch : 4, Step : 1950, Loss : 0.41164, Acc : 0.812, Sensitive_Loss : 0.17657, Sensitive_Acc : 18.400, Run Time : 11.42 sec
INFO:root:2024-04-17 23:58:05, Train, Epoch : 4, Step : 1960, Loss : 0.50613, Acc : 0.791, Sensitive_Loss : 0.14701, Sensitive_Acc : 20.900, Run Time : 10.79 sec
INFO:root:2024-04-17 23:58:16, Train, Epoch : 4, Step : 1970, Loss : 0.45436, Acc : 0.781, Sensitive_Loss : 0.09407, Sensitive_Acc : 23.300, Run Time : 10.91 sec
INFO:root:2024-04-17 23:58:28, Train, Epoch : 4, Step : 1980, Loss : 0.41477, Acc : 0.803, Sensitive_Loss : 0.12016, Sensitive_Acc : 22.300, Run Time : 12.60 sec
INFO:root:2024-04-17 23:58:39, Train, Epoch : 4, Step : 1990, Loss : 0.46756, Acc : 0.787, Sensitive_Loss : 0.13514, Sensitive_Acc : 23.500, Run Time : 11.04 sec
INFO:root:2024-04-17 23:58:50, Train, Epoch : 4, Step : 2000, Loss : 0.42195, Acc : 0.828, Sensitive_Loss : 0.10272, Sensitive_Acc : 23.200, Run Time : 10.57 sec
INFO:root:2024-04-18 00:01:17, Dev, Step : 2000, Loss : 0.51293, Acc : 0.774, Auc : 0.854, Sensitive_Loss : 0.18631, Sensitive_Acc : 21.887, Sensitive_Auc : 0.998, Mean auc: 0.854, Run Time : 146.60 sec
INFO:root:2024-04-18 00:01:26, Train, Epoch : 4, Step : 2010, Loss : 0.48786, Acc : 0.784, Sensitive_Loss : 0.09137, Sensitive_Acc : 20.900, Run Time : 155.47 sec
INFO:root:2024-04-18 00:01:36, Train, Epoch : 4, Step : 2020, Loss : 0.41552, Acc : 0.800, Sensitive_Loss : 0.11634, Sensitive_Acc : 25.800, Run Time : 10.91 sec
INFO:root:2024-04-18 00:01:48, Train, Epoch : 4, Step : 2030, Loss : 0.50073, Acc : 0.800, Sensitive_Loss : 0.13199, Sensitive_Acc : 22.300, Run Time : 11.52 sec
INFO:root:2024-04-18 00:01:59, Train, Epoch : 4, Step : 2040, Loss : 0.45905, Acc : 0.778, Sensitive_Loss : 0.15544, Sensitive_Acc : 19.900, Run Time : 11.46 sec
INFO:root:2024-04-18 00:02:11, Train, Epoch : 4, Step : 2050, Loss : 0.45262, Acc : 0.806, Sensitive_Loss : 0.09438, Sensitive_Acc : 21.500, Run Time : 11.87 sec
INFO:root:2024-04-18 00:02:22, Train, Epoch : 4, Step : 2060, Loss : 0.36951, Acc : 0.834, Sensitive_Loss : 0.14127, Sensitive_Acc : 20.300, Run Time : 10.82 sec
INFO:root:2024-04-18 00:02:34, Train, Epoch : 4, Step : 2070, Loss : 0.39312, Acc : 0.825, Sensitive_Loss : 0.12714, Sensitive_Acc : 24.100, Run Time : 11.74 sec
INFO:root:2024-04-18 00:02:45, Train, Epoch : 4, Step : 2080, Loss : 0.44017, Acc : 0.787, Sensitive_Loss : 0.12096, Sensitive_Acc : 21.300, Run Time : 11.21 sec
INFO:root:2024-04-18 00:02:56, Train, Epoch : 4, Step : 2090, Loss : 0.37870, Acc : 0.825, Sensitive_Loss : 0.12628, Sensitive_Acc : 24.900, Run Time : 10.99 sec
INFO:root:2024-04-18 00:03:07, Train, Epoch : 4, Step : 2100, Loss : 0.42481, Acc : 0.794, Sensitive_Loss : 0.09695, Sensitive_Acc : 21.800, Run Time : 11.24 sec
INFO:root:2024-04-18 00:05:35, Dev, Step : 2100, Loss : 0.51035, Acc : 0.780, Auc : 0.857, Sensitive_Loss : 0.18412, Sensitive_Acc : 21.782, Sensitive_Auc : 0.997, Mean auc: 0.857, Run Time : 147.50 sec
INFO:root:2024-04-18 00:05:36, Best, Step : 2100, Loss : 0.51035, Acc : 0.780, Auc : 0.857, Sensitive_Loss : 0.18412, Sensitive_Acc : 21.782, Sensitive_Auc : 0.997, Best Auc : 0.857
INFO:root:2024-04-18 00:05:44, Train, Epoch : 4, Step : 2110, Loss : 0.36855, Acc : 0.834, Sensitive_Loss : 0.17075, Sensitive_Acc : 20.100, Run Time : 156.58 sec
INFO:root:2024-04-18 00:05:55, Train, Epoch : 4, Step : 2120, Loss : 0.45097, Acc : 0.797, Sensitive_Loss : 0.10691, Sensitive_Acc : 18.400, Run Time : 11.38 sec
INFO:root:2024-04-18 00:06:08, Train, Epoch : 4, Step : 2130, Loss : 0.42612, Acc : 0.809, Sensitive_Loss : 0.18303, Sensitive_Acc : 21.500, Run Time : 12.77 sec
INFO:root:2024-04-18 00:06:19, Train, Epoch : 4, Step : 2140, Loss : 0.53138, Acc : 0.766, Sensitive_Loss : 0.09367, Sensitive_Acc : 24.500, Run Time : 11.33 sec
INFO:root:2024-04-18 00:06:30, Train, Epoch : 4, Step : 2150, Loss : 0.41140, Acc : 0.838, Sensitive_Loss : 0.11205, Sensitive_Acc : 22.100, Run Time : 10.28 sec
INFO:root:2024-04-18 00:06:42, Train, Epoch : 4, Step : 2160, Loss : 0.43507, Acc : 0.784, Sensitive_Loss : 0.07636, Sensitive_Acc : 20.900, Run Time : 12.08 sec
INFO:root:2024-04-18 00:06:53, Train, Epoch : 4, Step : 2170, Loss : 0.39787, Acc : 0.850, Sensitive_Loss : 0.12409, Sensitive_Acc : 19.300, Run Time : 11.14 sec
INFO:root:2024-04-18 00:07:04, Train, Epoch : 4, Step : 2180, Loss : 0.34043, Acc : 0.844, Sensitive_Loss : 0.19993, Sensitive_Acc : 17.900, Run Time : 10.75 sec
INFO:root:2024-04-18 00:07:16, Train, Epoch : 4, Step : 2190, Loss : 0.46588, Acc : 0.791, Sensitive_Loss : 0.12432, Sensitive_Acc : 16.800, Run Time : 12.29 sec
INFO:root:2024-04-18 00:07:27, Train, Epoch : 4, Step : 2200, Loss : 0.46100, Acc : 0.806, Sensitive_Loss : 0.16058, Sensitive_Acc : 24.300, Run Time : 10.64 sec
INFO:root:2024-04-18 00:09:54, Dev, Step : 2200, Loss : 0.51187, Acc : 0.778, Auc : 0.856, Sensitive_Loss : 0.18542, Sensitive_Acc : 21.782, Sensitive_Auc : 0.999, Mean auc: 0.856, Run Time : 147.44 sec
INFO:root:2024-04-18 00:10:02, Train, Epoch : 4, Step : 2210, Loss : 0.39617, Acc : 0.822, Sensitive_Loss : 0.10861, Sensitive_Acc : 21.100, Run Time : 155.70 sec
INFO:root:2024-04-18 00:10:14, Train, Epoch : 4, Step : 2220, Loss : 0.43765, Acc : 0.812, Sensitive_Loss : 0.08829, Sensitive_Acc : 20.300, Run Time : 11.82 sec
INFO:root:2024-04-18 00:10:25, Train, Epoch : 4, Step : 2230, Loss : 0.37568, Acc : 0.834, Sensitive_Loss : 0.10819, Sensitive_Acc : 21.400, Run Time : 11.32 sec
INFO:root:2024-04-18 00:10:36, Train, Epoch : 4, Step : 2240, Loss : 0.42236, Acc : 0.809, Sensitive_Loss : 0.11325, Sensitive_Acc : 18.100, Run Time : 10.92 sec
INFO:root:2024-04-18 00:10:48, Train, Epoch : 4, Step : 2250, Loss : 0.35645, Acc : 0.838, Sensitive_Loss : 0.14664, Sensitive_Acc : 16.400, Run Time : 11.83 sec
INFO:root:2024-04-18 00:10:59, Train, Epoch : 4, Step : 2260, Loss : 0.43389, Acc : 0.794, Sensitive_Loss : 0.10651, Sensitive_Acc : 18.900, Run Time : 11.20 sec
INFO:root:2024-04-18 00:11:12, Train, Epoch : 4, Step : 2270, Loss : 0.43248, Acc : 0.834, Sensitive_Loss : 0.12642, Sensitive_Acc : 19.700, Run Time : 12.27 sec
INFO:root:2024-04-18 00:11:23, Train, Epoch : 4, Step : 2280, Loss : 0.35943, Acc : 0.825, Sensitive_Loss : 0.13006, Sensitive_Acc : 21.600, Run Time : 11.46 sec
INFO:root:2024-04-18 00:11:35, Train, Epoch : 4, Step : 2290, Loss : 0.38260, Acc : 0.812, Sensitive_Loss : 0.13298, Sensitive_Acc : 19.000, Run Time : 12.02 sec
INFO:root:2024-04-18 00:11:47, Train, Epoch : 4, Step : 2300, Loss : 0.39831, Acc : 0.828, Sensitive_Loss : 0.11492, Sensitive_Acc : 21.800, Run Time : 11.50 sec
INFO:root:2024-04-18 00:14:13, Dev, Step : 2300, Loss : 0.50581, Acc : 0.780, Auc : 0.857, Sensitive_Loss : 0.17633, Sensitive_Acc : 21.782, Sensitive_Auc : 0.998, Mean auc: 0.857, Run Time : 146.30 sec
INFO:root:2024-04-18 00:14:22, Train, Epoch : 4, Step : 2310, Loss : 0.41037, Acc : 0.800, Sensitive_Loss : 0.15315, Sensitive_Acc : 20.200, Run Time : 155.39 sec
INFO:root:2024-04-18 00:14:33, Train, Epoch : 4, Step : 2320, Loss : 0.43807, Acc : 0.803, Sensitive_Loss : 0.08481, Sensitive_Acc : 22.000, Run Time : 11.08 sec
INFO:root:2024-04-18 00:14:44, Train, Epoch : 4, Step : 2330, Loss : 0.41593, Acc : 0.806, Sensitive_Loss : 0.11658, Sensitive_Acc : 23.900, Run Time : 11.33 sec
INFO:root:2024-04-18 00:14:56, Train, Epoch : 4, Step : 2340, Loss : 0.33677, Acc : 0.863, Sensitive_Loss : 0.10804, Sensitive_Acc : 18.500, Run Time : 11.28 sec
INFO:root:2024-04-18 00:15:08, Train, Epoch : 4, Step : 2350, Loss : 0.42018, Acc : 0.806, Sensitive_Loss : 0.14865, Sensitive_Acc : 22.600, Run Time : 12.07 sec
INFO:root:2024-04-18 00:15:19, Train, Epoch : 4, Step : 2360, Loss : 0.44602, Acc : 0.775, Sensitive_Loss : 0.11109, Sensitive_Acc : 23.000, Run Time : 11.58 sec
INFO:root:2024-04-18 00:15:30, Train, Epoch : 4, Step : 2370, Loss : 0.43814, Acc : 0.816, Sensitive_Loss : 0.12478, Sensitive_Acc : 24.200, Run Time : 10.64 sec
INFO:root:2024-04-18 00:15:42, Train, Epoch : 4, Step : 2380, Loss : 0.38160, Acc : 0.806, Sensitive_Loss : 0.16421, Sensitive_Acc : 19.900, Run Time : 11.99 sec
INFO:root:2024-04-18 00:15:53, Train, Epoch : 4, Step : 2390, Loss : 0.42806, Acc : 0.778, Sensitive_Loss : 0.18179, Sensitive_Acc : 21.800, Run Time : 10.91 sec
INFO:root:2024-04-18 00:16:04, Train, Epoch : 4, Step : 2400, Loss : 0.39643, Acc : 0.816, Sensitive_Loss : 0.17662, Sensitive_Acc : 19.200, Run Time : 11.28 sec
INFO:root:2024-04-18 00:18:31, Dev, Step : 2400, Loss : 0.51067, Acc : 0.778, Auc : 0.858, Sensitive_Loss : 0.20708, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.858, Run Time : 147.03 sec
INFO:root:2024-04-18 00:18:32, Best, Step : 2400, Loss : 0.51067, Acc : 0.778, Auc : 0.858, Sensitive_Loss : 0.20708, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Best Auc : 0.858
INFO:root:2024-04-18 00:18:41, Train, Epoch : 4, Step : 2410, Loss : 0.37200, Acc : 0.841, Sensitive_Loss : 0.10318, Sensitive_Acc : 21.500, Run Time : 157.21 sec
INFO:root:2024-04-18 00:18:53, Train, Epoch : 4, Step : 2420, Loss : 0.40326, Acc : 0.825, Sensitive_Loss : 0.12684, Sensitive_Acc : 21.300, Run Time : 12.08 sec
INFO:root:2024-04-18 00:19:05, Train, Epoch : 4, Step : 2430, Loss : 0.43548, Acc : 0.806, Sensitive_Loss : 0.11666, Sensitive_Acc : 19.500, Run Time : 11.50 sec
INFO:root:2024-04-18 00:19:15, Train, Epoch : 4, Step : 2440, Loss : 0.43688, Acc : 0.816, Sensitive_Loss : 0.12399, Sensitive_Acc : 20.400, Run Time : 9.92 sec
INFO:root:2024-04-18 00:19:27, Train, Epoch : 4, Step : 2450, Loss : 0.40969, Acc : 0.822, Sensitive_Loss : 0.18129, Sensitive_Acc : 25.100, Run Time : 12.01 sec
INFO:root:2024-04-18 00:19:39, Train, Epoch : 4, Step : 2460, Loss : 0.41343, Acc : 0.812, Sensitive_Loss : 0.09070, Sensitive_Acc : 19.500, Run Time : 11.60 sec
INFO:root:2024-04-18 00:19:50, Train, Epoch : 4, Step : 2470, Loss : 0.35852, Acc : 0.863, Sensitive_Loss : 0.16698, Sensitive_Acc : 22.000, Run Time : 11.75 sec
INFO:root:2024-04-18 00:20:02, Train, Epoch : 4, Step : 2480, Loss : 0.40009, Acc : 0.816, Sensitive_Loss : 0.12263, Sensitive_Acc : 24.200, Run Time : 11.72 sec
INFO:root:2024-04-18 00:20:14, Train, Epoch : 4, Step : 2490, Loss : 0.35418, Acc : 0.863, Sensitive_Loss : 0.06395, Sensitive_Acc : 20.700, Run Time : 12.31 sec
INFO:root:2024-04-18 00:20:26, Train, Epoch : 4, Step : 2500, Loss : 0.38116, Acc : 0.834, Sensitive_Loss : 0.17514, Sensitive_Acc : 22.500, Run Time : 11.24 sec
INFO:root:2024-04-18 00:22:51, Dev, Step : 2500, Loss : 0.52253, Acc : 0.777, Auc : 0.858, Sensitive_Loss : 0.16988, Sensitive_Acc : 21.782, Sensitive_Auc : 0.998, Mean auc: 0.858, Run Time : 145.38 sec
INFO:root:2024-04-18 00:22:59, Train, Epoch : 4, Step : 2510, Loss : 0.39910, Acc : 0.800, Sensitive_Loss : 0.10458, Sensitive_Acc : 20.900, Run Time : 153.59 sec
INFO:root:2024-04-18 00:23:11, Train, Epoch : 4, Step : 2520, Loss : 0.45975, Acc : 0.816, Sensitive_Loss : 0.15184, Sensitive_Acc : 16.000, Run Time : 11.70 sec
INFO:root:2024-04-18 00:23:22, Train, Epoch : 4, Step : 2530, Loss : 0.37387, Acc : 0.822, Sensitive_Loss : 0.11944, Sensitive_Acc : 19.600, Run Time : 11.22 sec
INFO:root:2024-04-18 00:25:53
INFO:root:y_pred: [0.12848084 0.00370337 0.14494422 ... 0.10563906 0.08155827 0.03487229]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.11865113e-03 4.67812097e-05 4.81734984e-03 3.51611292e-03
 1.06736179e-02 2.25863314e-05 7.03306869e-05 9.81597812e-04
 1.55442700e-01 9.99954820e-01 2.70494550e-01 5.36782201e-04
 1.05330534e-03 2.32845516e-04 9.99794185e-01 5.22733405e-02
 6.37868792e-03 9.99921918e-01 9.99361098e-01 5.64148836e-03
 9.93015528e-01 1.25253119e-03 2.66217347e-03 1.98276993e-03
 6.70662820e-02 9.53834802e-02 2.63567699e-06 7.42716002e-05
 4.02548358e-05 3.72403418e-03 2.39968125e-04 9.94124591e-01
 1.64040867e-02 6.57376230e-01 3.64268490e-05 6.30141352e-04
 3.01571656e-02 5.42682260e-02 9.15504713e-03 2.19165743e-03
 4.22227159e-02 9.96290207e-01 1.25394459e-03 3.83642339e-03
 9.84738827e-01 5.01902588e-02 1.31333113e-01 1.79467887e-01
 3.74660119e-02 9.94361997e-01 9.70497429e-01 9.99383450e-01
 9.93380785e-01 3.76846478e-03 1.09824771e-02 8.26191436e-03
 1.19228347e-03 3.01834685e-03 9.96873975e-01 1.20072532e-03
 2.51786259e-05 9.05959983e-04 2.57912371e-03 4.18123906e-04
 9.99668837e-01 1.70765355e-01 5.40729779e-05 2.07621604e-03
 3.72537747e-02 9.97194529e-01 9.99340951e-01 9.99960661e-01
 3.36578931e-04 1.41117871e-01 7.60175390e-05 8.13227475e-01
 2.75406390e-02 1.74779552e-06 7.73138774e-04 6.29925868e-03
 2.04029605e-01 1.61775926e-04 9.99305248e-01 9.89327848e-01
 1.46840550e-02 2.36956301e-04 3.78097296e-02 2.42890790e-04
 2.80252006e-02 3.37865524e-04 1.58984587e-02 1.72573291e-02
 1.00548285e-04 5.14576677e-05 9.97867246e-05 5.16815251e-03
 8.09694931e-04 5.69695532e-01 2.00162083e-03 4.05774132e-04
 1.92334608e-03 4.44303907e-04 2.18326543e-02 2.37110449e-04
 7.64123769e-03 5.21288384e-05 4.35429532e-03 1.58330470e-01
 2.49503434e-01 1.65440112e-01 2.58952969e-05 9.99971867e-01
 9.99838352e-01 3.62821629e-06 4.13122699e-02 9.82986577e-03
 6.30021989e-02 3.06091591e-04 8.01689550e-02 1.44312130e-02
 8.46523733e-04 8.60395303e-05 8.96537956e-03 4.99945127e-05
 3.62298288e-03 7.06458330e-01 1.75325797e-04 9.78588343e-01
 1.87904835e-02 1.90913193e-02 1.12026762e-02 7.43826572e-03
 8.52997073e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 00:25:53, Dev, Step : 2536, Loss : 0.51042, Acc : 0.775, Auc : 0.857, Sensitive_Loss : 0.16871, Sensitive_Acc : 21.782, Sensitive_Auc : 0.998, Mean auc: 0.857, Run Time : 144.68 sec
INFO:root:2024-04-18 00:26:01, Train, Epoch : 5, Step : 2540, Loss : 0.12157, Acc : 0.350, Sensitive_Loss : 0.03669, Sensitive_Acc : 8.700, Run Time : 6.59 sec
INFO:root:2024-04-18 00:26:13, Train, Epoch : 5, Step : 2550, Loss : 0.39963, Acc : 0.812, Sensitive_Loss : 0.14598, Sensitive_Acc : 19.300, Run Time : 12.23 sec
INFO:root:2024-04-18 00:26:24, Train, Epoch : 5, Step : 2560, Loss : 0.42663, Acc : 0.809, Sensitive_Loss : 0.08586, Sensitive_Acc : 22.200, Run Time : 11.39 sec
INFO:root:2024-04-18 00:26:35, Train, Epoch : 5, Step : 2570, Loss : 0.38182, Acc : 0.838, Sensitive_Loss : 0.13146, Sensitive_Acc : 21.600, Run Time : 10.60 sec
INFO:root:2024-04-18 00:26:46, Train, Epoch : 5, Step : 2580, Loss : 0.42767, Acc : 0.791, Sensitive_Loss : 0.09691, Sensitive_Acc : 20.500, Run Time : 11.27 sec
INFO:root:2024-04-18 00:26:57, Train, Epoch : 5, Step : 2590, Loss : 0.34788, Acc : 0.834, Sensitive_Loss : 0.11405, Sensitive_Acc : 24.000, Run Time : 11.13 sec
INFO:root:2024-04-18 00:27:09, Train, Epoch : 5, Step : 2600, Loss : 0.43352, Acc : 0.806, Sensitive_Loss : 0.11291, Sensitive_Acc : 25.600, Run Time : 11.85 sec
INFO:root:2024-04-18 00:29:36, Dev, Step : 2600, Loss : 0.51124, Acc : 0.775, Auc : 0.856, Sensitive_Loss : 0.16396, Sensitive_Acc : 21.887, Sensitive_Auc : 0.998, Mean auc: 0.856, Run Time : 146.77 sec
INFO:root:2024-04-18 00:29:45, Train, Epoch : 5, Step : 2610, Loss : 0.42373, Acc : 0.831, Sensitive_Loss : 0.11181, Sensitive_Acc : 21.000, Run Time : 155.33 sec
INFO:root:2024-04-18 00:29:56, Train, Epoch : 5, Step : 2620, Loss : 0.44207, Acc : 0.822, Sensitive_Loss : 0.14628, Sensitive_Acc : 24.700, Run Time : 11.59 sec
INFO:root:2024-04-18 00:30:07, Train, Epoch : 5, Step : 2630, Loss : 0.38896, Acc : 0.834, Sensitive_Loss : 0.08494, Sensitive_Acc : 19.700, Run Time : 11.27 sec
INFO:root:2024-04-18 00:30:19, Train, Epoch : 5, Step : 2640, Loss : 0.44453, Acc : 0.816, Sensitive_Loss : 0.12424, Sensitive_Acc : 18.500, Run Time : 11.84 sec
INFO:root:2024-04-18 00:30:30, Train, Epoch : 5, Step : 2650, Loss : 0.39051, Acc : 0.819, Sensitive_Loss : 0.09958, Sensitive_Acc : 22.800, Run Time : 10.94 sec
INFO:root:2024-04-18 00:30:41, Train, Epoch : 5, Step : 2660, Loss : 0.36091, Acc : 0.822, Sensitive_Loss : 0.12748, Sensitive_Acc : 25.300, Run Time : 11.01 sec
INFO:root:2024-04-18 00:30:53, Train, Epoch : 5, Step : 2670, Loss : 0.45556, Acc : 0.822, Sensitive_Loss : 0.10290, Sensitive_Acc : 18.200, Run Time : 11.73 sec
INFO:root:2024-04-18 00:31:04, Train, Epoch : 5, Step : 2680, Loss : 0.38777, Acc : 0.844, Sensitive_Loss : 0.09967, Sensitive_Acc : 11.700, Run Time : 10.95 sec
INFO:root:2024-04-18 00:31:16, Train, Epoch : 5, Step : 2690, Loss : 0.35064, Acc : 0.847, Sensitive_Loss : 0.11204, Sensitive_Acc : 23.000, Run Time : 12.43 sec
INFO:root:2024-04-18 00:31:27, Train, Epoch : 5, Step : 2700, Loss : 0.41440, Acc : 0.806, Sensitive_Loss : 0.14789, Sensitive_Acc : 21.800, Run Time : 10.29 sec
INFO:root:2024-04-18 00:33:53, Dev, Step : 2700, Loss : 0.51225, Acc : 0.781, Auc : 0.857, Sensitive_Loss : 0.16960, Sensitive_Acc : 21.782, Sensitive_Auc : 0.999, Mean auc: 0.857, Run Time : 146.34 sec
INFO:root:2024-04-18 00:34:01, Train, Epoch : 5, Step : 2710, Loss : 0.39360, Acc : 0.816, Sensitive_Loss : 0.10756, Sensitive_Acc : 20.600, Run Time : 154.70 sec
INFO:root:2024-04-18 00:34:14, Train, Epoch : 5, Step : 2720, Loss : 0.40324, Acc : 0.825, Sensitive_Loss : 0.14570, Sensitive_Acc : 23.400, Run Time : 12.60 sec
INFO:root:2024-04-18 00:34:25, Train, Epoch : 5, Step : 2730, Loss : 0.39969, Acc : 0.834, Sensitive_Loss : 0.12123, Sensitive_Acc : 21.000, Run Time : 10.99 sec
INFO:root:2024-04-18 00:34:37, Train, Epoch : 5, Step : 2740, Loss : 0.39187, Acc : 0.816, Sensitive_Loss : 0.13837, Sensitive_Acc : 17.600, Run Time : 11.74 sec
INFO:root:2024-04-18 00:34:48, Train, Epoch : 5, Step : 2750, Loss : 0.37081, Acc : 0.866, Sensitive_Loss : 0.14641, Sensitive_Acc : 19.900, Run Time : 11.43 sec
INFO:root:2024-04-18 00:34:59, Train, Epoch : 5, Step : 2760, Loss : 0.40857, Acc : 0.822, Sensitive_Loss : 0.10153, Sensitive_Acc : 19.600, Run Time : 11.23 sec
INFO:root:2024-04-18 00:35:11, Train, Epoch : 5, Step : 2770, Loss : 0.31663, Acc : 0.853, Sensitive_Loss : 0.13715, Sensitive_Acc : 22.300, Run Time : 11.62 sec
INFO:root:2024-04-18 00:35:22, Train, Epoch : 5, Step : 2780, Loss : 0.38885, Acc : 0.856, Sensitive_Loss : 0.12894, Sensitive_Acc : 25.200, Run Time : 10.96 sec
INFO:root:2024-04-18 00:35:34, Train, Epoch : 5, Step : 2790, Loss : 0.40414, Acc : 0.812, Sensitive_Loss : 0.10504, Sensitive_Acc : 17.700, Run Time : 11.69 sec
INFO:root:2024-04-18 00:35:45, Train, Epoch : 5, Step : 2800, Loss : 0.38760, Acc : 0.812, Sensitive_Loss : 0.18182, Sensitive_Acc : 21.700, Run Time : 11.88 sec
INFO:root:2024-04-18 00:38:11, Dev, Step : 2800, Loss : 0.52395, Acc : 0.775, Auc : 0.856, Sensitive_Loss : 0.17527, Sensitive_Acc : 21.782, Sensitive_Auc : 0.999, Mean auc: 0.856, Run Time : 145.97 sec
INFO:root:2024-04-18 00:38:20, Train, Epoch : 5, Step : 2810, Loss : 0.39834, Acc : 0.841, Sensitive_Loss : 0.15999, Sensitive_Acc : 22.000, Run Time : 154.79 sec
INFO:root:2024-04-18 00:38:31, Train, Epoch : 5, Step : 2820, Loss : 0.42658, Acc : 0.812, Sensitive_Loss : 0.16511, Sensitive_Acc : 20.800, Run Time : 10.91 sec
INFO:root:2024-04-18 00:38:43, Train, Epoch : 5, Step : 2830, Loss : 0.45464, Acc : 0.778, Sensitive_Loss : 0.12192, Sensitive_Acc : 21.300, Run Time : 11.43 sec
INFO:root:2024-04-18 00:38:54, Train, Epoch : 5, Step : 2840, Loss : 0.40505, Acc : 0.819, Sensitive_Loss : 0.20056, Sensitive_Acc : 22.200, Run Time : 11.52 sec
INFO:root:2024-04-18 00:39:05, Train, Epoch : 5, Step : 2850, Loss : 0.34503, Acc : 0.841, Sensitive_Loss : 0.17632, Sensitive_Acc : 21.500, Run Time : 11.10 sec
INFO:root:2024-04-18 00:39:17, Train, Epoch : 5, Step : 2860, Loss : 0.39053, Acc : 0.850, Sensitive_Loss : 0.08085, Sensitive_Acc : 24.300, Run Time : 12.19 sec
INFO:root:2024-04-18 00:39:28, Train, Epoch : 5, Step : 2870, Loss : 0.36736, Acc : 0.841, Sensitive_Loss : 0.11398, Sensitive_Acc : 25.600, Run Time : 11.13 sec
INFO:root:2024-04-18 00:39:40, Train, Epoch : 5, Step : 2880, Loss : 0.37116, Acc : 0.834, Sensitive_Loss : 0.12676, Sensitive_Acc : 17.800, Run Time : 11.72 sec
INFO:root:2024-04-18 00:39:51, Train, Epoch : 5, Step : 2890, Loss : 0.34913, Acc : 0.822, Sensitive_Loss : 0.17015, Sensitive_Acc : 27.000, Run Time : 10.83 sec
INFO:root:2024-04-18 00:40:03, Train, Epoch : 5, Step : 2900, Loss : 0.42794, Acc : 0.787, Sensitive_Loss : 0.13821, Sensitive_Acc : 20.500, Run Time : 11.65 sec
INFO:root:2024-04-18 00:42:29, Dev, Step : 2900, Loss : 0.51950, Acc : 0.773, Auc : 0.856, Sensitive_Loss : 0.19745, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.856, Run Time : 146.55 sec
INFO:root:2024-04-18 00:42:39, Train, Epoch : 5, Step : 2910, Loss : 0.44261, Acc : 0.803, Sensitive_Loss : 0.09149, Sensitive_Acc : 18.900, Run Time : 156.63 sec
INFO:root:2024-04-18 00:42:50, Train, Epoch : 5, Step : 2920, Loss : 0.42635, Acc : 0.822, Sensitive_Loss : 0.11759, Sensitive_Acc : 23.900, Run Time : 10.80 sec
INFO:root:2024-04-18 00:43:01, Train, Epoch : 5, Step : 2930, Loss : 0.37289, Acc : 0.822, Sensitive_Loss : 0.16423, Sensitive_Acc : 22.800, Run Time : 10.62 sec
INFO:root:2024-04-18 00:43:13, Train, Epoch : 5, Step : 2940, Loss : 0.37764, Acc : 0.819, Sensitive_Loss : 0.10207, Sensitive_Acc : 23.600, Run Time : 12.14 sec
INFO:root:2024-04-18 00:43:25, Train, Epoch : 5, Step : 2950, Loss : 0.39471, Acc : 0.803, Sensitive_Loss : 0.12691, Sensitive_Acc : 21.600, Run Time : 11.76 sec
INFO:root:2024-04-18 00:43:36, Train, Epoch : 5, Step : 2960, Loss : 0.54800, Acc : 0.772, Sensitive_Loss : 0.15784, Sensitive_Acc : 21.700, Run Time : 11.09 sec
INFO:root:2024-04-18 00:43:47, Train, Epoch : 5, Step : 2970, Loss : 0.37589, Acc : 0.847, Sensitive_Loss : 0.10303, Sensitive_Acc : 17.800, Run Time : 11.28 sec
INFO:root:2024-04-18 00:43:58, Train, Epoch : 5, Step : 2980, Loss : 0.44488, Acc : 0.806, Sensitive_Loss : 0.14880, Sensitive_Acc : 21.800, Run Time : 10.72 sec
INFO:root:2024-04-18 00:44:10, Train, Epoch : 5, Step : 2990, Loss : 0.39636, Acc : 0.822, Sensitive_Loss : 0.16711, Sensitive_Acc : 23.300, Run Time : 12.61 sec
INFO:root:2024-04-18 00:44:21, Train, Epoch : 5, Step : 3000, Loss : 0.41750, Acc : 0.809, Sensitive_Loss : 0.11855, Sensitive_Acc : 22.100, Run Time : 10.54 sec
INFO:root:2024-04-18 00:46:47, Dev, Step : 3000, Loss : 0.51510, Acc : 0.772, Auc : 0.856, Sensitive_Loss : 0.17228, Sensitive_Acc : 21.782, Sensitive_Auc : 0.999, Mean auc: 0.856, Run Time : 146.46 sec
INFO:root:2024-04-18 00:46:56, Train, Epoch : 5, Step : 3010, Loss : 0.36067, Acc : 0.831, Sensitive_Loss : 0.10011, Sensitive_Acc : 21.900, Run Time : 154.77 sec
INFO:root:2024-04-18 00:47:07, Train, Epoch : 5, Step : 3020, Loss : 0.39564, Acc : 0.809, Sensitive_Loss : 0.11829, Sensitive_Acc : 23.300, Run Time : 11.54 sec
INFO:root:2024-04-18 00:47:18, Train, Epoch : 5, Step : 3030, Loss : 0.36786, Acc : 0.834, Sensitive_Loss : 0.10023, Sensitive_Acc : 24.900, Run Time : 11.14 sec
INFO:root:2024-04-18 00:47:30, Train, Epoch : 5, Step : 3040, Loss : 0.34248, Acc : 0.844, Sensitive_Loss : 0.12050, Sensitive_Acc : 21.700, Run Time : 11.93 sec
INFO:root:2024-04-18 00:47:42, Train, Epoch : 5, Step : 3050, Loss : 0.37198, Acc : 0.853, Sensitive_Loss : 0.15889, Sensitive_Acc : 20.500, Run Time : 11.30 sec
INFO:root:2024-04-18 00:47:53, Train, Epoch : 5, Step : 3060, Loss : 0.40769, Acc : 0.819, Sensitive_Loss : 0.11453, Sensitive_Acc : 21.900, Run Time : 11.08 sec
INFO:root:2024-04-18 00:48:04, Train, Epoch : 5, Step : 3070, Loss : 0.41888, Acc : 0.847, Sensitive_Loss : 0.13528, Sensitive_Acc : 22.400, Run Time : 11.47 sec
INFO:root:2024-04-18 00:48:16, Train, Epoch : 5, Step : 3080, Loss : 0.42094, Acc : 0.828, Sensitive_Loss : 0.10961, Sensitive_Acc : 20.500, Run Time : 11.37 sec
INFO:root:2024-04-18 00:48:27, Train, Epoch : 5, Step : 3090, Loss : 0.40385, Acc : 0.825, Sensitive_Loss : 0.08220, Sensitive_Acc : 19.800, Run Time : 11.44 sec
INFO:root:2024-04-18 00:48:38, Train, Epoch : 5, Step : 3100, Loss : 0.45353, Acc : 0.797, Sensitive_Loss : 0.10069, Sensitive_Acc : 21.700, Run Time : 11.23 sec
INFO:root:2024-04-18 00:51:05, Dev, Step : 3100, Loss : 0.51619, Acc : 0.776, Auc : 0.856, Sensitive_Loss : 0.16778, Sensitive_Acc : 21.782, Sensitive_Auc : 0.999, Mean auc: 0.856, Run Time : 147.07 sec
INFO:root:2024-04-18 00:51:14, Train, Epoch : 5, Step : 3110, Loss : 0.40302, Acc : 0.816, Sensitive_Loss : 0.08730, Sensitive_Acc : 19.800, Run Time : 155.85 sec
INFO:root:2024-04-18 00:51:25, Train, Epoch : 5, Step : 3120, Loss : 0.31996, Acc : 0.878, Sensitive_Loss : 0.15544, Sensitive_Acc : 22.000, Run Time : 10.73 sec
INFO:root:2024-04-18 00:51:37, Train, Epoch : 5, Step : 3130, Loss : 0.42276, Acc : 0.812, Sensitive_Loss : 0.12479, Sensitive_Acc : 21.900, Run Time : 12.64 sec
INFO:root:2024-04-18 00:51:49, Train, Epoch : 5, Step : 3140, Loss : 0.33327, Acc : 0.859, Sensitive_Loss : 0.12250, Sensitive_Acc : 19.900, Run Time : 11.14 sec
INFO:root:2024-04-18 00:52:00, Train, Epoch : 5, Step : 3150, Loss : 0.34419, Acc : 0.844, Sensitive_Loss : 0.16599, Sensitive_Acc : 24.000, Run Time : 11.27 sec
INFO:root:2024-04-18 00:52:11, Train, Epoch : 5, Step : 3160, Loss : 0.39004, Acc : 0.834, Sensitive_Loss : 0.11903, Sensitive_Acc : 22.600, Run Time : 10.71 sec
INFO:root:2024-04-18 00:52:22, Train, Epoch : 5, Step : 3170, Loss : 0.42410, Acc : 0.816, Sensitive_Loss : 0.09706, Sensitive_Acc : 15.900, Run Time : 11.14 sec
INFO:root:2024-04-18 00:54:46
INFO:root:y_pred: [0.11660364 0.00329898 0.15605405 ... 0.14112134 0.05891902 0.03495683]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [2.98665720e-03 7.74480941e-05 1.47415651e-02 8.50774162e-03
 2.24838834e-02 5.21401744e-05 5.06373763e-04 7.85778277e-04
 1.47236869e-01 9.99992132e-01 3.08577508e-01 8.56129860e-04
 1.43316353e-03 4.11346235e-04 9.99887705e-01 8.32390189e-02
 1.01382565e-02 9.99958396e-01 9.99782264e-01 7.44442176e-03
 9.97184098e-01 1.90175348e-03 1.56052585e-03 3.48465913e-03
 1.86754480e-01 2.33738407e-01 6.51952587e-06 1.49549203e-04
 6.14121454e-05 1.37750804e-02 7.39668088e-04 9.97568786e-01
 2.16613095e-02 8.78520370e-01 4.62956486e-05 7.67537742e-04
 7.12906644e-02 8.27720836e-02 1.91269796e-02 3.28881899e-03
 9.66503397e-02 9.98932064e-01 2.94014858e-03 7.76406610e-03
 9.95476544e-01 1.24432422e-01 1.86547667e-01 2.61980325e-01
 8.32420215e-02 9.96125400e-01 9.88836884e-01 9.99748170e-01
 9.96450186e-01 3.54389753e-03 3.53781544e-02 1.30080748e-02
 1.80088403e-03 8.22892599e-03 9.98718739e-01 9.21693107e-04
 1.75681616e-05 3.97920189e-03 2.61171581e-03 5.00421971e-04
 9.99795258e-01 3.56770217e-01 1.12096663e-04 5.88057796e-03
 7.56868497e-02 9.99525309e-01 9.99711454e-01 9.99980807e-01
 3.89982917e-04 3.13460767e-01 8.24719100e-05 8.99319947e-01
 4.67969999e-02 1.48237348e-06 1.07400701e-03 7.26134423e-03
 2.18191952e-01 3.09711846e-04 9.99732196e-01 9.93808210e-01
 2.07085833e-02 4.52160020e-04 1.30099073e-01 4.44650941e-04
 8.21579918e-02 5.24940726e-04 3.30112502e-02 1.13481497e-02
 1.11203524e-04 5.11503276e-05 1.51900866e-04 8.42741318e-03
 1.21599343e-03 7.31132030e-01 3.55782825e-03 1.54672877e-03
 4.24685469e-03 9.52748931e-04 6.35610297e-02 1.99230795e-04
 1.15345689e-02 9.16606150e-05 1.23119950e-02 3.32654417e-01
 5.92593908e-01 2.07196608e-01 3.34000724e-05 9.99986768e-01
 9.99877810e-01 6.05327523e-06 1.43082768e-01 1.13909915e-02
 1.01138622e-01 3.25624802e-04 1.86410084e-01 2.97669917e-02
 3.67311109e-03 1.85237761e-04 2.53076684e-02 8.78721839e-05
 5.36566181e-03 7.99884677e-01 1.13731083e-04 9.80870426e-01
 2.45331451e-02 4.13260385e-02 1.44124329e-02 5.87376347e-03
 2.35146908e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 00:54:46, Dev, Step : 3170, Loss : 0.51709, Acc : 0.776, Auc : 0.854, Sensitive_Loss : 0.19423, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.854, Run Time : 144.68 sec
INFO:root:2024-04-18 00:55:01, Train, Epoch : 6, Step : 3180, Loss : 0.37667, Acc : 0.853, Sensitive_Loss : 0.15317, Sensitive_Acc : 17.700, Run Time : 13.68 sec
INFO:root:2024-04-18 00:55:13, Train, Epoch : 6, Step : 3190, Loss : 0.33513, Acc : 0.863, Sensitive_Loss : 0.10352, Sensitive_Acc : 16.900, Run Time : 12.15 sec
INFO:root:2024-04-18 00:55:24, Train, Epoch : 6, Step : 3200, Loss : 0.35746, Acc : 0.863, Sensitive_Loss : 0.10617, Sensitive_Acc : 21.800, Run Time : 10.43 sec
INFO:root:2024-04-18 00:57:50, Dev, Step : 3200, Loss : 0.51205, Acc : 0.779, Auc : 0.857, Sensitive_Loss : 0.16909, Sensitive_Acc : 21.782, Sensitive_Auc : 0.999, Mean auc: 0.857, Run Time : 146.39 sec
INFO:root:2024-04-18 00:57:59, Train, Epoch : 6, Step : 3210, Loss : 0.37817, Acc : 0.859, Sensitive_Loss : 0.08853, Sensitive_Acc : 19.800, Run Time : 155.49 sec
INFO:root:2024-04-18 00:58:11, Train, Epoch : 6, Step : 3220, Loss : 0.36880, Acc : 0.806, Sensitive_Loss : 0.14044, Sensitive_Acc : 21.400, Run Time : 11.67 sec
INFO:root:2024-04-18 00:58:22, Train, Epoch : 6, Step : 3230, Loss : 0.38954, Acc : 0.809, Sensitive_Loss : 0.12480, Sensitive_Acc : 23.500, Run Time : 10.93 sec
INFO:root:2024-04-18 00:58:33, Train, Epoch : 6, Step : 3240, Loss : 0.30838, Acc : 0.869, Sensitive_Loss : 0.11653, Sensitive_Acc : 18.700, Run Time : 11.20 sec
INFO:root:2024-04-18 00:58:45, Train, Epoch : 6, Step : 3250, Loss : 0.40818, Acc : 0.847, Sensitive_Loss : 0.12313, Sensitive_Acc : 16.500, Run Time : 12.07 sec
INFO:root:2024-04-18 00:58:57, Train, Epoch : 6, Step : 3260, Loss : 0.33524, Acc : 0.863, Sensitive_Loss : 0.08199, Sensitive_Acc : 19.800, Run Time : 11.90 sec
INFO:root:2024-04-18 00:59:08, Train, Epoch : 6, Step : 3270, Loss : 0.35485, Acc : 0.853, Sensitive_Loss : 0.11700, Sensitive_Acc : 20.900, Run Time : 11.17 sec
INFO:root:2024-04-18 00:59:19, Train, Epoch : 6, Step : 3280, Loss : 0.43479, Acc : 0.816, Sensitive_Loss : 0.09438, Sensitive_Acc : 25.000, Run Time : 11.07 sec
INFO:root:2024-04-18 00:59:31, Train, Epoch : 6, Step : 3290, Loss : 0.37387, Acc : 0.866, Sensitive_Loss : 0.10058, Sensitive_Acc : 22.400, Run Time : 11.41 sec
INFO:root:2024-04-18 00:59:42, Train, Epoch : 6, Step : 3300, Loss : 0.35189, Acc : 0.819, Sensitive_Loss : 0.19496, Sensitive_Acc : 21.900, Run Time : 10.79 sec
INFO:root:2024-04-18 01:02:08, Dev, Step : 3300, Loss : 0.51194, Acc : 0.777, Auc : 0.856, Sensitive_Loss : 0.16204, Sensitive_Acc : 21.782, Sensitive_Auc : 0.999, Mean auc: 0.856, Run Time : 146.73 sec
INFO:root:2024-04-18 01:02:17, Train, Epoch : 6, Step : 3310, Loss : 0.36898, Acc : 0.825, Sensitive_Loss : 0.09169, Sensitive_Acc : 20.100, Run Time : 155.78 sec
INFO:root:2024-04-18 01:02:29, Train, Epoch : 6, Step : 3320, Loss : 0.39365, Acc : 0.859, Sensitive_Loss : 0.13059, Sensitive_Acc : 24.000, Run Time : 11.82 sec
INFO:root:2024-04-18 01:02:40, Train, Epoch : 6, Step : 3330, Loss : 0.34568, Acc : 0.859, Sensitive_Loss : 0.14254, Sensitive_Acc : 15.500, Run Time : 11.31 sec
INFO:root:2024-04-18 01:02:52, Train, Epoch : 6, Step : 3340, Loss : 0.33460, Acc : 0.825, Sensitive_Loss : 0.13523, Sensitive_Acc : 19.600, Run Time : 11.26 sec
INFO:root:2024-04-18 01:03:03, Train, Epoch : 6, Step : 3350, Loss : 0.39655, Acc : 0.841, Sensitive_Loss : 0.17373, Sensitive_Acc : 21.200, Run Time : 11.52 sec
INFO:root:2024-04-18 01:03:15, Train, Epoch : 6, Step : 3360, Loss : 0.40458, Acc : 0.825, Sensitive_Loss : 0.11714, Sensitive_Acc : 19.300, Run Time : 11.66 sec
INFO:root:2024-04-18 01:03:26, Train, Epoch : 6, Step : 3370, Loss : 0.44608, Acc : 0.816, Sensitive_Loss : 0.07745, Sensitive_Acc : 22.400, Run Time : 10.75 sec
INFO:root:2024-04-18 01:03:36, Train, Epoch : 6, Step : 3380, Loss : 0.34176, Acc : 0.825, Sensitive_Loss : 0.13550, Sensitive_Acc : 20.400, Run Time : 10.79 sec
INFO:root:2024-04-18 01:03:48, Train, Epoch : 6, Step : 3390, Loss : 0.40328, Acc : 0.809, Sensitive_Loss : 0.10572, Sensitive_Acc : 25.800, Run Time : 11.27 sec
INFO:root:2024-04-18 01:04:00, Train, Epoch : 6, Step : 3400, Loss : 0.43899, Acc : 0.794, Sensitive_Loss : 0.12513, Sensitive_Acc : 21.600, Run Time : 12.54 sec
INFO:root:2024-04-18 01:06:26, Dev, Step : 3400, Loss : 0.53369, Acc : 0.777, Auc : 0.856, Sensitive_Loss : 0.17146, Sensitive_Acc : 21.782, Sensitive_Auc : 0.999, Mean auc: 0.856, Run Time : 146.24 sec
INFO:root:2024-04-18 01:06:35, Train, Epoch : 6, Step : 3410, Loss : 0.38804, Acc : 0.822, Sensitive_Loss : 0.10559, Sensitive_Acc : 21.600, Run Time : 154.88 sec
INFO:root:2024-04-18 01:06:47, Train, Epoch : 6, Step : 3420, Loss : 0.36800, Acc : 0.831, Sensitive_Loss : 0.13249, Sensitive_Acc : 22.200, Run Time : 11.99 sec
INFO:root:2024-04-18 01:06:59, Train, Epoch : 6, Step : 3430, Loss : 0.49672, Acc : 0.831, Sensitive_Loss : 0.12964, Sensitive_Acc : 23.100, Run Time : 11.81 sec
INFO:root:2024-04-18 01:07:10, Train, Epoch : 6, Step : 3440, Loss : 0.40373, Acc : 0.834, Sensitive_Loss : 0.11078, Sensitive_Acc : 23.500, Run Time : 11.24 sec
INFO:root:2024-04-18 01:07:21, Train, Epoch : 6, Step : 3450, Loss : 0.41048, Acc : 0.859, Sensitive_Loss : 0.09915, Sensitive_Acc : 20.100, Run Time : 10.95 sec
INFO:root:2024-04-18 01:07:33, Train, Epoch : 6, Step : 3460, Loss : 0.35816, Acc : 0.828, Sensitive_Loss : 0.10286, Sensitive_Acc : 22.100, Run Time : 11.51 sec
INFO:root:2024-04-18 01:07:44, Train, Epoch : 6, Step : 3470, Loss : 0.40794, Acc : 0.816, Sensitive_Loss : 0.15289, Sensitive_Acc : 21.100, Run Time : 11.14 sec
INFO:root:2024-04-18 01:07:55, Train, Epoch : 6, Step : 3480, Loss : 0.39295, Acc : 0.816, Sensitive_Loss : 0.10668, Sensitive_Acc : 20.300, Run Time : 10.90 sec
INFO:root:2024-04-18 01:08:06, Train, Epoch : 6, Step : 3490, Loss : 0.35881, Acc : 0.831, Sensitive_Loss : 0.13553, Sensitive_Acc : 22.000, Run Time : 11.70 sec
INFO:root:2024-04-18 01:08:18, Train, Epoch : 6, Step : 3500, Loss : 0.35635, Acc : 0.809, Sensitive_Loss : 0.09934, Sensitive_Acc : 21.200, Run Time : 11.70 sec
INFO:root:2024-04-18 01:11:03, Dev, Step : 3500, Loss : 0.52890, Acc : 0.777, Auc : 0.855, Sensitive_Loss : 0.16938, Sensitive_Acc : 21.782, Sensitive_Auc : 1.000, Mean auc: 0.855, Run Time : 165.40 sec
INFO:root:2024-04-18 01:11:13, Train, Epoch : 6, Step : 3510, Loss : 0.41792, Acc : 0.797, Sensitive_Loss : 0.10480, Sensitive_Acc : 23.500, Run Time : 174.97 sec
INFO:root:2024-04-18 01:11:24, Train, Epoch : 6, Step : 3520, Loss : 0.38983, Acc : 0.841, Sensitive_Loss : 0.13513, Sensitive_Acc : 23.900, Run Time : 11.30 sec
INFO:root:2024-04-18 01:11:36, Train, Epoch : 6, Step : 3530, Loss : 0.33700, Acc : 0.856, Sensitive_Loss : 0.09853, Sensitive_Acc : 26.200, Run Time : 11.75 sec
INFO:root:2024-04-18 01:11:47, Train, Epoch : 6, Step : 3540, Loss : 0.39617, Acc : 0.825, Sensitive_Loss : 0.11213, Sensitive_Acc : 19.600, Run Time : 10.74 sec
INFO:root:2024-04-18 01:11:58, Train, Epoch : 6, Step : 3550, Loss : 0.34005, Acc : 0.863, Sensitive_Loss : 0.20779, Sensitive_Acc : 25.100, Run Time : 11.59 sec
INFO:root:2024-04-18 01:12:10, Train, Epoch : 6, Step : 3560, Loss : 0.36529, Acc : 0.831, Sensitive_Loss : 0.08461, Sensitive_Acc : 21.900, Run Time : 11.41 sec
INFO:root:2024-04-18 01:12:22, Train, Epoch : 6, Step : 3570, Loss : 0.27752, Acc : 0.866, Sensitive_Loss : 0.11367, Sensitive_Acc : 18.100, Run Time : 11.71 sec
INFO:root:2024-04-18 01:12:32, Train, Epoch : 6, Step : 3580, Loss : 0.33023, Acc : 0.866, Sensitive_Loss : 0.13341, Sensitive_Acc : 20.700, Run Time : 10.31 sec
INFO:root:2024-04-18 01:12:43, Train, Epoch : 6, Step : 3590, Loss : 0.38269, Acc : 0.831, Sensitive_Loss : 0.09149, Sensitive_Acc : 19.800, Run Time : 10.94 sec
INFO:root:2024-04-18 01:12:54, Train, Epoch : 6, Step : 3600, Loss : 0.39185, Acc : 0.816, Sensitive_Loss : 0.10711, Sensitive_Acc : 23.000, Run Time : 11.63 sec
INFO:root:2024-04-18 01:15:21, Dev, Step : 3600, Loss : 0.51567, Acc : 0.779, Auc : 0.857, Sensitive_Loss : 0.16822, Sensitive_Acc : 21.782, Sensitive_Auc : 1.000, Mean auc: 0.857, Run Time : 146.92 sec
INFO:root:2024-04-18 01:15:30, Train, Epoch : 6, Step : 3610, Loss : 0.44167, Acc : 0.828, Sensitive_Loss : 0.07443, Sensitive_Acc : 23.700, Run Time : 155.42 sec
INFO:root:2024-04-18 01:15:42, Train, Epoch : 6, Step : 3620, Loss : 0.38169, Acc : 0.828, Sensitive_Loss : 0.09924, Sensitive_Acc : 16.200, Run Time : 11.72 sec
INFO:root:2024-04-18 01:15:52, Train, Epoch : 6, Step : 3630, Loss : 0.29030, Acc : 0.866, Sensitive_Loss : 0.13961, Sensitive_Acc : 23.800, Run Time : 10.91 sec
INFO:root:2024-04-18 01:16:04, Train, Epoch : 6, Step : 3640, Loss : 0.38992, Acc : 0.825, Sensitive_Loss : 0.13269, Sensitive_Acc : 20.600, Run Time : 11.46 sec
INFO:root:2024-04-18 01:16:16, Train, Epoch : 6, Step : 3650, Loss : 0.32665, Acc : 0.878, Sensitive_Loss : 0.04626, Sensitive_Acc : 20.000, Run Time : 11.80 sec
INFO:root:2024-04-18 01:16:27, Train, Epoch : 6, Step : 3660, Loss : 0.42630, Acc : 0.831, Sensitive_Loss : 0.11940, Sensitive_Acc : 23.900, Run Time : 11.67 sec
INFO:root:2024-04-18 01:16:40, Train, Epoch : 6, Step : 3670, Loss : 0.37748, Acc : 0.834, Sensitive_Loss : 0.11262, Sensitive_Acc : 19.700, Run Time : 12.16 sec
INFO:root:2024-04-18 01:16:51, Train, Epoch : 6, Step : 3680, Loss : 0.38042, Acc : 0.841, Sensitive_Loss : 0.07745, Sensitive_Acc : 21.400, Run Time : 11.63 sec
INFO:root:2024-04-18 01:17:02, Train, Epoch : 6, Step : 3690, Loss : 0.32868, Acc : 0.819, Sensitive_Loss : 0.07689, Sensitive_Acc : 23.200, Run Time : 10.43 sec
INFO:root:2024-04-18 01:17:13, Train, Epoch : 6, Step : 3700, Loss : 0.36631, Acc : 0.834, Sensitive_Loss : 0.09208, Sensitive_Acc : 21.200, Run Time : 11.85 sec
INFO:root:2024-04-18 01:19:40, Dev, Step : 3700, Loss : 0.54317, Acc : 0.774, Auc : 0.855, Sensitive_Loss : 0.17612, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.855, Run Time : 146.32 sec
INFO:root:2024-04-18 01:19:48, Train, Epoch : 6, Step : 3710, Loss : 0.35217, Acc : 0.844, Sensitive_Loss : 0.08300, Sensitive_Acc : 24.900, Run Time : 154.70 sec
INFO:root:2024-04-18 01:20:00, Train, Epoch : 6, Step : 3720, Loss : 0.36506, Acc : 0.859, Sensitive_Loss : 0.08309, Sensitive_Acc : 23.800, Run Time : 11.50 sec
INFO:root:2024-04-18 01:20:11, Train, Epoch : 6, Step : 3730, Loss : 0.32444, Acc : 0.884, Sensitive_Loss : 0.13392, Sensitive_Acc : 19.700, Run Time : 11.34 sec
INFO:root:2024-04-18 01:20:22, Train, Epoch : 6, Step : 3740, Loss : 0.40512, Acc : 0.806, Sensitive_Loss : 0.07315, Sensitive_Acc : 22.100, Run Time : 11.22 sec
INFO:root:2024-04-18 01:20:34, Train, Epoch : 6, Step : 3750, Loss : 0.37840, Acc : 0.819, Sensitive_Loss : 0.12494, Sensitive_Acc : 22.800, Run Time : 11.63 sec
INFO:root:2024-04-18 01:20:46, Train, Epoch : 6, Step : 3760, Loss : 0.37354, Acc : 0.825, Sensitive_Loss : 0.10071, Sensitive_Acc : 22.600, Run Time : 12.22 sec
INFO:root:2024-04-18 01:20:57, Train, Epoch : 6, Step : 3770, Loss : 0.44086, Acc : 0.806, Sensitive_Loss : 0.11385, Sensitive_Acc : 21.500, Run Time : 11.34 sec
INFO:root:2024-04-18 01:21:08, Train, Epoch : 6, Step : 3780, Loss : 0.34098, Acc : 0.853, Sensitive_Loss : 0.12551, Sensitive_Acc : 21.300, Run Time : 10.12 sec
INFO:root:2024-04-18 01:21:20, Train, Epoch : 6, Step : 3790, Loss : 0.42663, Acc : 0.841, Sensitive_Loss : 0.22505, Sensitive_Acc : 23.300, Run Time : 12.17 sec
INFO:root:2024-04-18 01:21:32, Train, Epoch : 6, Step : 3800, Loss : 0.36967, Acc : 0.853, Sensitive_Loss : 0.14101, Sensitive_Acc : 23.300, Run Time : 11.79 sec
INFO:root:2024-04-18 01:23:58, Dev, Step : 3800, Loss : 0.52980, Acc : 0.774, Auc : 0.853, Sensitive_Loss : 0.17817, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.853, Run Time : 146.20 sec
INFO:root:2024-04-18 01:26:25
INFO:root:y_pred: [0.03854181 0.00269428 0.13588277 ... 0.15960374 0.03500455 0.02827394]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.9516653e-03 5.3024334e-05 9.5340861e-03 3.9510396e-03 2.7926544e-02
 1.6679089e-05 9.5492811e-04 3.8583160e-04 5.7601936e-02 9.9998248e-01
 1.9731544e-01 5.5876240e-04 1.0328045e-03 8.0020144e-04 9.9985158e-01
 4.5257770e-02 6.5045008e-03 9.9992931e-01 9.9962974e-01 9.5492788e-03
 9.9333680e-01 1.4609820e-03 9.4807695e-04 1.2055736e-03 9.3226895e-02
 9.3364596e-02 6.3226071e-06 9.0588081e-05 1.0768082e-04 6.8843216e-03
 5.3120752e-05 9.9836904e-01 2.9628366e-02 8.7332207e-01 5.8061531e-05
 4.4173966e-04 1.1257481e-01 8.9913145e-02 1.4464707e-02 3.6865255e-04
 6.4102724e-02 9.9765199e-01 1.4009773e-03 3.1003968e-03 9.9616385e-01
 2.6139917e-02 2.4942984e-01 1.4467968e-01 5.9864230e-02 9.9341714e-01
 9.8924190e-01 9.9967730e-01 9.8870778e-01 3.4067675e-03 8.8555794e-03
 8.9480970e-03 9.7927230e-04 6.8719448e-03 9.9661177e-01 3.5584834e-04
 2.1439102e-05 2.6875816e-03 3.2926914e-03 6.7280536e-04 9.9973983e-01
 3.8202801e-01 1.1723060e-04 3.7639884e-03 2.4993848e-02 9.9913067e-01
 9.9960393e-01 9.9997044e-01 2.5571929e-04 2.3419170e-01 5.0859071e-05
 8.9559370e-01 1.8766636e-02 1.4048036e-06 5.3431431e-04 7.3295790e-03
 1.4628302e-01 2.9968977e-04 9.9948835e-01 9.8017246e-01 1.1504303e-02
 4.4240104e-04 1.1056331e-01 1.3391186e-04 3.9414693e-02 2.2096591e-04
 1.0110119e-02 3.3441766e-03 1.3164307e-04 3.3216285e-05 2.8779999e-05
 8.5911695e-03 8.4478204e-04 7.0001549e-01 3.8316008e-03 7.9381035e-04
 1.4335996e-03 6.8656949e-04 4.7644280e-02 1.4899822e-04 9.4639333e-03
 8.2798622e-05 3.0031835e-03 2.3015328e-01 5.9752208e-01 1.8973231e-01
 5.5807330e-05 9.9997628e-01 9.9985170e-01 4.9569876e-06 3.9748222e-02
 4.0709148e-03 4.3580424e-02 1.5571724e-04 8.4988311e-02 8.9902328e-03
 3.5721415e-03 2.0093251e-04 1.2968043e-02 1.2745304e-04 2.1295587e-03
 6.9307107e-01 8.5113970e-05 9.7587186e-01 1.8114941e-02 8.1279263e-02
 6.5852376e-03 1.0767217e-03 1.0321110e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 01:26:25, Dev, Step : 3804, Loss : 0.53657, Acc : 0.774, Auc : 0.852, Sensitive_Loss : 0.17710, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.852, Run Time : 144.87 sec
INFO:root:2024-04-18 01:26:33, Train, Epoch : 7, Step : 3810, Loss : 0.27213, Acc : 0.475, Sensitive_Loss : 0.06969, Sensitive_Acc : 10.100, Run Time : 7.90 sec
INFO:root:2024-04-18 01:26:46, Train, Epoch : 7, Step : 3820, Loss : 0.36346, Acc : 0.859, Sensitive_Loss : 0.09796, Sensitive_Acc : 18.700, Run Time : 12.36 sec
INFO:root:2024-04-18 01:26:57, Train, Epoch : 7, Step : 3830, Loss : 0.33583, Acc : 0.856, Sensitive_Loss : 0.12070, Sensitive_Acc : 21.800, Run Time : 11.35 sec
INFO:root:2024-04-18 01:27:09, Train, Epoch : 7, Step : 3840, Loss : 0.30463, Acc : 0.872, Sensitive_Loss : 0.07554, Sensitive_Acc : 21.200, Run Time : 11.40 sec
INFO:root:2024-04-18 01:27:19, Train, Epoch : 7, Step : 3850, Loss : 0.37473, Acc : 0.850, Sensitive_Loss : 0.09745, Sensitive_Acc : 19.900, Run Time : 10.76 sec
INFO:root:2024-04-18 01:27:31, Train, Epoch : 7, Step : 3860, Loss : 0.41166, Acc : 0.838, Sensitive_Loss : 0.14541, Sensitive_Acc : 24.000, Run Time : 11.68 sec
INFO:root:2024-04-18 01:27:43, Train, Epoch : 7, Step : 3870, Loss : 0.38851, Acc : 0.844, Sensitive_Loss : 0.09704, Sensitive_Acc : 23.900, Run Time : 11.79 sec
INFO:root:2024-04-18 01:27:55, Train, Epoch : 7, Step : 3880, Loss : 0.29781, Acc : 0.884, Sensitive_Loss : 0.10099, Sensitive_Acc : 20.700, Run Time : 12.11 sec
INFO:root:2024-04-18 01:28:06, Train, Epoch : 7, Step : 3890, Loss : 0.41590, Acc : 0.844, Sensitive_Loss : 0.06831, Sensitive_Acc : 17.600, Run Time : 10.71 sec
INFO:root:2024-04-18 01:28:17, Train, Epoch : 7, Step : 3900, Loss : 0.32689, Acc : 0.856, Sensitive_Loss : 0.07779, Sensitive_Acc : 22.200, Run Time : 11.24 sec
INFO:root:2024-04-18 01:30:44, Dev, Step : 3900, Loss : 0.53244, Acc : 0.773, Auc : 0.854, Sensitive_Loss : 0.16710, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.854, Run Time : 147.18 sec
INFO:root:2024-04-18 01:30:52, Train, Epoch : 7, Step : 3910, Loss : 0.38101, Acc : 0.834, Sensitive_Loss : 0.12830, Sensitive_Acc : 23.900, Run Time : 155.60 sec
INFO:root:2024-04-18 01:31:05, Train, Epoch : 7, Step : 3920, Loss : 0.34267, Acc : 0.884, Sensitive_Loss : 0.12061, Sensitive_Acc : 23.700, Run Time : 12.16 sec
INFO:root:2024-04-18 01:31:16, Train, Epoch : 7, Step : 3930, Loss : 0.30717, Acc : 0.872, Sensitive_Loss : 0.11363, Sensitive_Acc : 22.700, Run Time : 11.24 sec
INFO:root:2024-04-18 01:31:28, Train, Epoch : 7, Step : 3940, Loss : 0.39736, Acc : 0.812, Sensitive_Loss : 0.09529, Sensitive_Acc : 24.900, Run Time : 12.00 sec
INFO:root:2024-04-18 01:31:38, Train, Epoch : 7, Step : 3950, Loss : 0.34835, Acc : 0.844, Sensitive_Loss : 0.09585, Sensitive_Acc : 19.800, Run Time : 10.01 sec
INFO:root:2024-04-18 01:31:50, Train, Epoch : 7, Step : 3960, Loss : 0.34184, Acc : 0.816, Sensitive_Loss : 0.09672, Sensitive_Acc : 25.500, Run Time : 12.25 sec
INFO:root:2024-04-18 01:32:01, Train, Epoch : 7, Step : 3970, Loss : 0.34209, Acc : 0.856, Sensitive_Loss : 0.06025, Sensitive_Acc : 23.100, Run Time : 10.65 sec
INFO:root:2024-04-18 01:32:13, Train, Epoch : 7, Step : 3980, Loss : 0.33548, Acc : 0.841, Sensitive_Loss : 0.08906, Sensitive_Acc : 17.900, Run Time : 12.44 sec
INFO:root:2024-04-18 01:32:25, Train, Epoch : 7, Step : 3990, Loss : 0.35766, Acc : 0.844, Sensitive_Loss : 0.17026, Sensitive_Acc : 16.200, Run Time : 11.90 sec
INFO:root:2024-04-18 01:32:36, Train, Epoch : 7, Step : 4000, Loss : 0.41393, Acc : 0.812, Sensitive_Loss : 0.12567, Sensitive_Acc : 21.800, Run Time : 11.08 sec
INFO:root:2024-04-18 01:35:03, Dev, Step : 4000, Loss : 0.53346, Acc : 0.770, Auc : 0.851, Sensitive_Loss : 0.18053, Sensitive_Acc : 21.677, Sensitive_Auc : 0.998, Mean auc: 0.851, Run Time : 147.00 sec
INFO:root:2024-04-18 01:35:12, Train, Epoch : 7, Step : 4010, Loss : 0.33861, Acc : 0.853, Sensitive_Loss : 0.10603, Sensitive_Acc : 23.700, Run Time : 155.82 sec
INFO:root:2024-04-18 01:35:23, Train, Epoch : 7, Step : 4020, Loss : 0.38022, Acc : 0.838, Sensitive_Loss : 0.09066, Sensitive_Acc : 24.700, Run Time : 11.38 sec
INFO:root:2024-04-18 01:35:36, Train, Epoch : 7, Step : 4030, Loss : 0.40727, Acc : 0.825, Sensitive_Loss : 0.08781, Sensitive_Acc : 19.500, Run Time : 13.01 sec
INFO:root:2024-04-18 01:35:47, Train, Epoch : 7, Step : 4040, Loss : 0.35093, Acc : 0.850, Sensitive_Loss : 0.11267, Sensitive_Acc : 21.600, Run Time : 10.84 sec
INFO:root:2024-04-18 01:35:58, Train, Epoch : 7, Step : 4050, Loss : 0.34815, Acc : 0.847, Sensitive_Loss : 0.10620, Sensitive_Acc : 22.100, Run Time : 10.85 sec
INFO:root:2024-04-18 01:36:10, Train, Epoch : 7, Step : 4060, Loss : 0.36659, Acc : 0.856, Sensitive_Loss : 0.15069, Sensitive_Acc : 18.200, Run Time : 11.87 sec
INFO:root:2024-04-18 01:36:21, Train, Epoch : 7, Step : 4070, Loss : 0.32933, Acc : 0.853, Sensitive_Loss : 0.13096, Sensitive_Acc : 17.700, Run Time : 11.45 sec
INFO:root:2024-04-18 01:36:33, Train, Epoch : 7, Step : 4080, Loss : 0.30792, Acc : 0.881, Sensitive_Loss : 0.11066, Sensitive_Acc : 21.600, Run Time : 11.45 sec
INFO:root:2024-04-18 01:36:44, Train, Epoch : 7, Step : 4090, Loss : 0.36971, Acc : 0.859, Sensitive_Loss : 0.08343, Sensitive_Acc : 22.100, Run Time : 11.53 sec
INFO:root:2024-04-18 01:36:55, Train, Epoch : 7, Step : 4100, Loss : 0.33619, Acc : 0.856, Sensitive_Loss : 0.08719, Sensitive_Acc : 20.400, Run Time : 10.78 sec
INFO:root:2024-04-18 01:39:22, Dev, Step : 4100, Loss : 0.51858, Acc : 0.781, Auc : 0.855, Sensitive_Loss : 0.17074, Sensitive_Acc : 21.677, Sensitive_Auc : 0.998, Mean auc: 0.855, Run Time : 146.48 sec
INFO:root:2024-04-18 01:39:30, Train, Epoch : 7, Step : 4110, Loss : 0.34813, Acc : 0.853, Sensitive_Loss : 0.10179, Sensitive_Acc : 21.900, Run Time : 154.81 sec
INFO:root:2024-04-18 01:39:42, Train, Epoch : 7, Step : 4120, Loss : 0.37456, Acc : 0.822, Sensitive_Loss : 0.09126, Sensitive_Acc : 19.300, Run Time : 12.32 sec
INFO:root:2024-04-18 01:39:53, Train, Epoch : 7, Step : 4130, Loss : 0.33408, Acc : 0.831, Sensitive_Loss : 0.12760, Sensitive_Acc : 23.300, Run Time : 11.03 sec
INFO:root:2024-04-18 01:40:05, Train, Epoch : 7, Step : 4140, Loss : 0.38943, Acc : 0.834, Sensitive_Loss : 0.13764, Sensitive_Acc : 24.600, Run Time : 12.09 sec
INFO:root:2024-04-18 01:40:17, Train, Epoch : 7, Step : 4150, Loss : 0.33807, Acc : 0.856, Sensitive_Loss : 0.11213, Sensitive_Acc : 18.300, Run Time : 11.68 sec
INFO:root:2024-04-18 01:40:28, Train, Epoch : 7, Step : 4160, Loss : 0.32203, Acc : 0.869, Sensitive_Loss : 0.09477, Sensitive_Acc : 22.900, Run Time : 11.29 sec
INFO:root:2024-04-18 01:40:40, Train, Epoch : 7, Step : 4170, Loss : 0.32421, Acc : 0.875, Sensitive_Loss : 0.09736, Sensitive_Acc : 24.300, Run Time : 11.40 sec
INFO:root:2024-04-18 01:40:51, Train, Epoch : 7, Step : 4180, Loss : 0.43163, Acc : 0.834, Sensitive_Loss : 0.10920, Sensitive_Acc : 16.200, Run Time : 11.17 sec
INFO:root:2024-04-18 01:41:02, Train, Epoch : 7, Step : 4190, Loss : 0.42602, Acc : 0.828, Sensitive_Loss : 0.09217, Sensitive_Acc : 21.700, Run Time : 11.53 sec
INFO:root:2024-04-18 01:41:14, Train, Epoch : 7, Step : 4200, Loss : 0.37373, Acc : 0.847, Sensitive_Loss : 0.10517, Sensitive_Acc : 22.900, Run Time : 11.26 sec
INFO:root:2024-04-18 01:43:39, Dev, Step : 4200, Loss : 0.54221, Acc : 0.776, Auc : 0.855, Sensitive_Loss : 0.16371, Sensitive_Acc : 21.782, Sensitive_Auc : 0.999, Mean auc: 0.855, Run Time : 145.60 sec
INFO:root:2024-04-18 01:43:48, Train, Epoch : 7, Step : 4210, Loss : 0.31378, Acc : 0.853, Sensitive_Loss : 0.06832, Sensitive_Acc : 23.000, Run Time : 153.76 sec
INFO:root:2024-04-18 01:43:59, Train, Epoch : 7, Step : 4220, Loss : 0.32668, Acc : 0.881, Sensitive_Loss : 0.11926, Sensitive_Acc : 24.300, Run Time : 11.79 sec
INFO:root:2024-04-18 01:44:11, Train, Epoch : 7, Step : 4230, Loss : 0.31559, Acc : 0.878, Sensitive_Loss : 0.12662, Sensitive_Acc : 23.200, Run Time : 11.72 sec
INFO:root:2024-04-18 01:44:23, Train, Epoch : 7, Step : 4240, Loss : 0.37191, Acc : 0.853, Sensitive_Loss : 0.13947, Sensitive_Acc : 16.100, Run Time : 12.30 sec
INFO:root:2024-04-18 01:44:33, Train, Epoch : 7, Step : 4250, Loss : 0.35248, Acc : 0.841, Sensitive_Loss : 0.10710, Sensitive_Acc : 21.100, Run Time : 9.92 sec
INFO:root:2024-04-18 01:44:46, Train, Epoch : 7, Step : 4260, Loss : 0.36010, Acc : 0.853, Sensitive_Loss : 0.08898, Sensitive_Acc : 21.900, Run Time : 12.56 sec
INFO:root:2024-04-18 01:44:57, Train, Epoch : 7, Step : 4270, Loss : 0.32906, Acc : 0.847, Sensitive_Loss : 0.10017, Sensitive_Acc : 15.900, Run Time : 11.07 sec
INFO:root:2024-04-18 01:45:08, Train, Epoch : 7, Step : 4280, Loss : 0.33426, Acc : 0.897, Sensitive_Loss : 0.06781, Sensitive_Acc : 22.300, Run Time : 11.14 sec
INFO:root:2024-04-18 01:45:19, Train, Epoch : 7, Step : 4290, Loss : 0.29820, Acc : 0.866, Sensitive_Loss : 0.12044, Sensitive_Acc : 22.600, Run Time : 11.48 sec
INFO:root:2024-04-18 01:45:31, Train, Epoch : 7, Step : 4300, Loss : 0.35002, Acc : 0.856, Sensitive_Loss : 0.10355, Sensitive_Acc : 21.600, Run Time : 11.36 sec
INFO:root:2024-04-18 01:47:58, Dev, Step : 4300, Loss : 0.53065, Acc : 0.775, Auc : 0.856, Sensitive_Loss : 0.18848, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.856, Run Time : 146.94 sec
INFO:root:2024-04-18 01:48:07, Train, Epoch : 7, Step : 4310, Loss : 0.34107, Acc : 0.856, Sensitive_Loss : 0.13467, Sensitive_Acc : 19.400, Run Time : 156.03 sec
INFO:root:2024-04-18 01:48:19, Train, Epoch : 7, Step : 4320, Loss : 0.30727, Acc : 0.859, Sensitive_Loss : 0.12760, Sensitive_Acc : 22.600, Run Time : 12.23 sec
INFO:root:2024-04-18 01:48:30, Train, Epoch : 7, Step : 4330, Loss : 0.33456, Acc : 0.859, Sensitive_Loss : 0.06462, Sensitive_Acc : 20.200, Run Time : 11.20 sec
INFO:root:2024-04-18 01:48:41, Train, Epoch : 7, Step : 4340, Loss : 0.36562, Acc : 0.838, Sensitive_Loss : 0.10497, Sensitive_Acc : 18.100, Run Time : 11.09 sec
INFO:root:2024-04-18 01:48:53, Train, Epoch : 7, Step : 4350, Loss : 0.39901, Acc : 0.812, Sensitive_Loss : 0.10110, Sensitive_Acc : 19.900, Run Time : 11.35 sec
INFO:root:2024-04-18 01:49:04, Train, Epoch : 7, Step : 4360, Loss : 0.37870, Acc : 0.828, Sensitive_Loss : 0.12407, Sensitive_Acc : 22.300, Run Time : 11.25 sec
INFO:root:2024-04-18 01:49:15, Train, Epoch : 7, Step : 4370, Loss : 0.37654, Acc : 0.834, Sensitive_Loss : 0.12976, Sensitive_Acc : 25.400, Run Time : 11.36 sec
INFO:root:2024-04-18 01:49:27, Train, Epoch : 7, Step : 4380, Loss : 0.35372, Acc : 0.850, Sensitive_Loss : 0.09345, Sensitive_Acc : 14.700, Run Time : 11.74 sec
INFO:root:2024-04-18 01:49:39, Train, Epoch : 7, Step : 4390, Loss : 0.34632, Acc : 0.856, Sensitive_Loss : 0.09707, Sensitive_Acc : 23.900, Run Time : 12.21 sec
INFO:root:2024-04-18 01:49:50, Train, Epoch : 7, Step : 4400, Loss : 0.37433, Acc : 0.825, Sensitive_Loss : 0.07997, Sensitive_Acc : 18.200, Run Time : 11.00 sec
INFO:root:2024-04-18 01:52:17, Dev, Step : 4400, Loss : 0.55530, Acc : 0.771, Auc : 0.849, Sensitive_Loss : 0.16448, Sensitive_Acc : 21.932, Sensitive_Auc : 0.999, Mean auc: 0.849, Run Time : 147.17 sec
INFO:root:2024-04-18 01:52:26, Train, Epoch : 7, Step : 4410, Loss : 0.34221, Acc : 0.866, Sensitive_Loss : 0.09188, Sensitive_Acc : 22.900, Run Time : 155.98 sec
INFO:root:2024-04-18 01:52:38, Train, Epoch : 7, Step : 4420, Loss : 0.34801, Acc : 0.847, Sensitive_Loss : 0.08406, Sensitive_Acc : 22.200, Run Time : 12.03 sec
INFO:root:2024-04-18 01:52:50, Train, Epoch : 7, Step : 4430, Loss : 0.33404, Acc : 0.859, Sensitive_Loss : 0.08646, Sensitive_Acc : 20.000, Run Time : 11.62 sec
INFO:root:2024-04-18 01:55:22
INFO:root:y_pred: [0.04867178 0.00205404 0.21027015 ... 0.11350537 0.02019796 0.01460791]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.26916484e-03 7.66280846e-06 9.77475196e-03 3.39668500e-03
 1.41450167e-02 1.51005206e-05 3.46078858e-04 2.58019223e-04
 1.14067487e-01 9.99982476e-01 5.22494391e-02 2.91926990e-04
 1.34936790e-03 3.90755100e-04 9.99895096e-01 2.23245248e-02
 2.25212891e-03 9.99947429e-01 9.99629617e-01 3.73350759e-03
 9.98074055e-01 1.14671595e-03 8.24380433e-04 1.28126098e-03
 1.45422250e-01 1.59452572e-01 1.56094836e-06 6.58349381e-05
 3.83133847e-05 1.23042967e-02 4.82326832e-05 9.98180985e-01
 3.84188853e-02 7.80443490e-01 2.44003022e-05 1.06945474e-04
 1.71511516e-01 8.59807208e-02 1.53061310e-02 2.79751577e-04
 4.12423648e-02 9.95891094e-01 1.54461525e-03 1.58219214e-03
 9.94166613e-01 2.03168895e-02 1.70818239e-01 1.31035537e-01
 4.64158878e-02 9.94657695e-01 9.87766385e-01 9.99448121e-01
 9.83424246e-01 1.98282255e-03 2.23832252e-03 1.50703639e-02
 4.62207332e-04 4.92732693e-03 9.96747851e-01 3.31881747e-04
 2.00586383e-05 1.32087269e-03 2.84701237e-03 3.35646124e-04
 9.99804318e-01 3.48504275e-01 1.24977218e-04 2.65930500e-03
 4.36010212e-02 9.98871744e-01 9.99735415e-01 9.99965072e-01
 2.44753348e-04 2.12124303e-01 2.85019523e-05 7.76267409e-01
 1.10313054e-02 3.61608045e-07 6.75647752e-04 1.79996062e-02
 1.48125008e-01 1.39819138e-04 9.99822438e-01 9.92416739e-01
 6.33379072e-03 3.40183906e-04 4.69373278e-02 1.26758910e-04
 8.14203471e-02 5.50450000e-04 8.24894570e-03 3.30547290e-03
 1.53444285e-04 2.20563161e-05 7.41966005e-06 6.11901004e-03
 6.58383360e-04 7.25430965e-01 8.89848452e-04 6.36978366e-04
 1.26686494e-03 1.93855536e-04 9.12244692e-02 1.14188246e-04
 9.46287811e-03 2.73490714e-05 2.49046437e-03 1.33008376e-01
 5.82282841e-01 1.60184234e-01 2.29908055e-05 9.99984980e-01
 9.99794424e-01 3.29627574e-06 2.78037358e-02 4.03830549e-03
 6.71544718e-03 2.89302734e-05 1.29146025e-01 9.92862973e-03
 3.05787334e-03 1.71228981e-04 1.48363803e-02 8.65991751e-05
 1.80708512e-03 5.68615735e-01 2.02904703e-05 9.76286709e-01
 2.51293238e-02 2.33683828e-02 2.56008771e-03 1.95641682e-04
 9.01158910e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 01:55:22, Dev, Step : 4438, Loss : 0.53499, Acc : 0.774, Auc : 0.852, Sensitive_Loss : 0.17470, Sensitive_Acc : 21.677, Sensitive_Auc : 1.000, Mean auc: 0.852, Run Time : 144.93 sec
INFO:root:2024-04-18 01:55:28, Train, Epoch : 8, Step : 4440, Loss : 0.05678, Acc : 0.178, Sensitive_Loss : 0.02301, Sensitive_Acc : 3.800, Run Time : 4.31 sec
INFO:root:2024-04-18 01:55:39, Train, Epoch : 8, Step : 4450, Loss : 0.29113, Acc : 0.866, Sensitive_Loss : 0.09577, Sensitive_Acc : 19.700, Run Time : 10.91 sec
INFO:root:2024-04-18 01:55:50, Train, Epoch : 8, Step : 4460, Loss : 0.30744, Acc : 0.866, Sensitive_Loss : 0.09641, Sensitive_Acc : 21.200, Run Time : 11.31 sec
INFO:root:2024-04-18 01:56:02, Train, Epoch : 8, Step : 4470, Loss : 0.36650, Acc : 0.838, Sensitive_Loss : 0.07000, Sensitive_Acc : 22.900, Run Time : 12.28 sec
INFO:root:2024-04-18 01:56:14, Train, Epoch : 8, Step : 4480, Loss : 0.35746, Acc : 0.853, Sensitive_Loss : 0.13425, Sensitive_Acc : 23.500, Run Time : 11.52 sec
INFO:root:2024-04-18 01:56:25, Train, Epoch : 8, Step : 4490, Loss : 0.32984, Acc : 0.844, Sensitive_Loss : 0.08270, Sensitive_Acc : 22.100, Run Time : 11.32 sec
INFO:root:2024-04-18 01:56:37, Train, Epoch : 8, Step : 4500, Loss : 0.36044, Acc : 0.875, Sensitive_Loss : 0.07163, Sensitive_Acc : 19.200, Run Time : 12.40 sec
INFO:root:2024-04-18 01:59:03, Dev, Step : 4500, Loss : 0.53084, Acc : 0.775, Auc : 0.852, Sensitive_Loss : 0.16438, Sensitive_Acc : 21.782, Sensitive_Auc : 1.000, Mean auc: 0.852, Run Time : 145.75 sec
INFO:root:2024-04-18 01:59:13, Train, Epoch : 8, Step : 4510, Loss : 0.28315, Acc : 0.887, Sensitive_Loss : 0.10539, Sensitive_Acc : 23.200, Run Time : 155.14 sec
INFO:root:2024-04-18 01:59:23, Train, Epoch : 8, Step : 4520, Loss : 0.30712, Acc : 0.859, Sensitive_Loss : 0.08647, Sensitive_Acc : 18.500, Run Time : 10.85 sec
INFO:root:2024-04-18 01:59:35, Train, Epoch : 8, Step : 4530, Loss : 0.34200, Acc : 0.838, Sensitive_Loss : 0.13058, Sensitive_Acc : 19.600, Run Time : 11.50 sec
INFO:root:2024-04-18 01:59:46, Train, Epoch : 8, Step : 4540, Loss : 0.32630, Acc : 0.853, Sensitive_Loss : 0.08430, Sensitive_Acc : 22.300, Run Time : 11.50 sec
INFO:root:2024-04-18 01:59:58, Train, Epoch : 8, Step : 4550, Loss : 0.33064, Acc : 0.869, Sensitive_Loss : 0.12435, Sensitive_Acc : 24.800, Run Time : 11.83 sec
INFO:root:2024-04-18 02:00:09, Train, Epoch : 8, Step : 4560, Loss : 0.39468, Acc : 0.812, Sensitive_Loss : 0.13711, Sensitive_Acc : 19.400, Run Time : 10.97 sec
INFO:root:2024-04-18 02:00:21, Train, Epoch : 8, Step : 4570, Loss : 0.27319, Acc : 0.878, Sensitive_Loss : 0.11288, Sensitive_Acc : 20.200, Run Time : 11.46 sec
INFO:root:2024-04-18 02:00:32, Train, Epoch : 8, Step : 4580, Loss : 0.25874, Acc : 0.903, Sensitive_Loss : 0.07905, Sensitive_Acc : 20.000, Run Time : 11.61 sec
INFO:root:2024-04-18 02:00:44, Train, Epoch : 8, Step : 4590, Loss : 0.34005, Acc : 0.850, Sensitive_Loss : 0.09028, Sensitive_Acc : 17.800, Run Time : 11.22 sec
INFO:root:2024-04-18 02:00:55, Train, Epoch : 8, Step : 4600, Loss : 0.29178, Acc : 0.856, Sensitive_Loss : 0.12406, Sensitive_Acc : 22.000, Run Time : 11.36 sec
INFO:root:2024-04-18 02:03:22, Dev, Step : 4600, Loss : 0.55035, Acc : 0.773, Auc : 0.849, Sensitive_Loss : 0.17717, Sensitive_Acc : 21.782, Sensitive_Auc : 0.999, Mean auc: 0.849, Run Time : 146.62 sec
INFO:root:2024-04-18 02:03:30, Train, Epoch : 8, Step : 4610, Loss : 0.35341, Acc : 0.847, Sensitive_Loss : 0.12501, Sensitive_Acc : 16.300, Run Time : 155.43 sec
INFO:root:2024-04-18 02:03:42, Train, Epoch : 8, Step : 4620, Loss : 0.37053, Acc : 0.853, Sensitive_Loss : 0.07634, Sensitive_Acc : 25.700, Run Time : 11.90 sec
INFO:root:2024-04-18 02:03:53, Train, Epoch : 8, Step : 4630, Loss : 0.31999, Acc : 0.841, Sensitive_Loss : 0.08493, Sensitive_Acc : 25.800, Run Time : 11.17 sec
INFO:root:2024-04-18 02:04:05, Train, Epoch : 8, Step : 4640, Loss : 0.32953, Acc : 0.869, Sensitive_Loss : 0.09482, Sensitive_Acc : 24.000, Run Time : 11.99 sec
INFO:root:2024-04-18 02:04:17, Train, Epoch : 8, Step : 4650, Loss : 0.29459, Acc : 0.856, Sensitive_Loss : 0.10147, Sensitive_Acc : 23.100, Run Time : 11.37 sec
INFO:root:2024-04-18 02:04:28, Train, Epoch : 8, Step : 4660, Loss : 0.33316, Acc : 0.881, Sensitive_Loss : 0.08965, Sensitive_Acc : 21.300, Run Time : 11.55 sec
INFO:root:2024-04-18 02:04:39, Train, Epoch : 8, Step : 4670, Loss : 0.30454, Acc : 0.866, Sensitive_Loss : 0.08358, Sensitive_Acc : 23.600, Run Time : 10.74 sec
INFO:root:2024-04-18 02:04:50, Train, Epoch : 8, Step : 4680, Loss : 0.39226, Acc : 0.853, Sensitive_Loss : 0.07986, Sensitive_Acc : 21.800, Run Time : 11.11 sec
INFO:root:2024-04-18 02:05:01, Train, Epoch : 8, Step : 4690, Loss : 0.34612, Acc : 0.869, Sensitive_Loss : 0.07346, Sensitive_Acc : 18.600, Run Time : 11.07 sec
INFO:root:2024-04-18 02:05:14, Train, Epoch : 8, Step : 4700, Loss : 0.33218, Acc : 0.850, Sensitive_Loss : 0.11616, Sensitive_Acc : 18.200, Run Time : 12.59 sec
INFO:root:2024-04-18 02:07:40, Dev, Step : 4700, Loss : 0.53712, Acc : 0.777, Auc : 0.854, Sensitive_Loss : 0.16132, Sensitive_Acc : 21.782, Sensitive_Auc : 1.000, Mean auc: 0.854, Run Time : 146.45 sec
INFO:root:2024-04-18 02:07:48, Train, Epoch : 8, Step : 4710, Loss : 0.35515, Acc : 0.859, Sensitive_Loss : 0.09463, Sensitive_Acc : 23.400, Run Time : 154.62 sec
INFO:root:2024-04-18 02:08:00, Train, Epoch : 8, Step : 4720, Loss : 0.40411, Acc : 0.825, Sensitive_Loss : 0.11487, Sensitive_Acc : 18.300, Run Time : 11.97 sec
INFO:root:2024-04-18 02:08:12, Train, Epoch : 8, Step : 4730, Loss : 0.37958, Acc : 0.853, Sensitive_Loss : 0.09374, Sensitive_Acc : 18.300, Run Time : 11.22 sec
INFO:root:2024-04-18 02:08:24, Train, Epoch : 8, Step : 4740, Loss : 0.33582, Acc : 0.872, Sensitive_Loss : 0.07743, Sensitive_Acc : 24.900, Run Time : 11.91 sec
INFO:root:2024-04-18 02:08:34, Train, Epoch : 8, Step : 4750, Loss : 0.29340, Acc : 0.887, Sensitive_Loss : 0.08530, Sensitive_Acc : 25.100, Run Time : 10.76 sec
INFO:root:2024-04-18 02:08:47, Train, Epoch : 8, Step : 4760, Loss : 0.31267, Acc : 0.853, Sensitive_Loss : 0.08389, Sensitive_Acc : 23.800, Run Time : 12.24 sec
INFO:root:2024-04-18 02:08:58, Train, Epoch : 8, Step : 4770, Loss : 0.32995, Acc : 0.853, Sensitive_Loss : 0.05878, Sensitive_Acc : 21.800, Run Time : 11.15 sec
INFO:root:2024-04-18 02:09:09, Train, Epoch : 8, Step : 4780, Loss : 0.32630, Acc : 0.866, Sensitive_Loss : 0.06910, Sensitive_Acc : 22.700, Run Time : 11.41 sec
INFO:root:2024-04-18 02:09:21, Train, Epoch : 8, Step : 4790, Loss : 0.44156, Acc : 0.816, Sensitive_Loss : 0.10047, Sensitive_Acc : 22.400, Run Time : 12.18 sec
INFO:root:2024-04-18 02:09:33, Train, Epoch : 8, Step : 4800, Loss : 0.31836, Acc : 0.884, Sensitive_Loss : 0.09115, Sensitive_Acc : 22.300, Run Time : 11.68 sec
INFO:root:2024-04-18 02:11:59, Dev, Step : 4800, Loss : 0.53660, Acc : 0.772, Auc : 0.850, Sensitive_Loss : 0.15703, Sensitive_Acc : 21.782, Sensitive_Auc : 1.000, Mean auc: 0.850, Run Time : 146.15 sec
INFO:root:2024-04-18 02:12:07, Train, Epoch : 8, Step : 4810, Loss : 0.29248, Acc : 0.863, Sensitive_Loss : 0.11653, Sensitive_Acc : 21.100, Run Time : 154.31 sec
INFO:root:2024-04-18 02:12:19, Train, Epoch : 8, Step : 4820, Loss : 0.31671, Acc : 0.853, Sensitive_Loss : 0.07800, Sensitive_Acc : 13.100, Run Time : 11.90 sec
INFO:root:2024-04-18 02:12:30, Train, Epoch : 8, Step : 4830, Loss : 0.29910, Acc : 0.844, Sensitive_Loss : 0.07440, Sensitive_Acc : 22.700, Run Time : 11.31 sec
INFO:root:2024-04-18 02:12:42, Train, Epoch : 8, Step : 4840, Loss : 0.34866, Acc : 0.866, Sensitive_Loss : 0.06971, Sensitive_Acc : 20.600, Run Time : 11.52 sec
INFO:root:2024-04-18 02:12:53, Train, Epoch : 8, Step : 4850, Loss : 0.29006, Acc : 0.869, Sensitive_Loss : 0.07197, Sensitive_Acc : 19.600, Run Time : 10.71 sec
INFO:root:2024-04-18 02:13:04, Train, Epoch : 8, Step : 4860, Loss : 0.29819, Acc : 0.838, Sensitive_Loss : 0.07868, Sensitive_Acc : 21.300, Run Time : 11.76 sec
INFO:root:2024-04-18 02:13:16, Train, Epoch : 8, Step : 4870, Loss : 0.28802, Acc : 0.863, Sensitive_Loss : 0.08745, Sensitive_Acc : 19.300, Run Time : 11.26 sec
INFO:root:2024-04-18 02:13:28, Train, Epoch : 8, Step : 4880, Loss : 0.32619, Acc : 0.841, Sensitive_Loss : 0.07296, Sensitive_Acc : 19.900, Run Time : 12.74 sec
INFO:root:2024-04-18 02:13:39, Train, Epoch : 8, Step : 4890, Loss : 0.29567, Acc : 0.856, Sensitive_Loss : 0.07513, Sensitive_Acc : 22.700, Run Time : 10.20 sec
INFO:root:2024-04-18 02:13:50, Train, Epoch : 8, Step : 4900, Loss : 0.33743, Acc : 0.838, Sensitive_Loss : 0.12223, Sensitive_Acc : 24.200, Run Time : 11.28 sec
INFO:root:2024-04-18 02:16:16, Dev, Step : 4900, Loss : 0.55342, Acc : 0.773, Auc : 0.850, Sensitive_Loss : 0.16543, Sensitive_Acc : 21.782, Sensitive_Auc : 1.000, Mean auc: 0.850, Run Time : 146.53 sec
INFO:root:2024-04-18 02:16:24, Train, Epoch : 8, Step : 4910, Loss : 0.32293, Acc : 0.869, Sensitive_Loss : 0.17370, Sensitive_Acc : 25.100, Run Time : 154.47 sec
INFO:root:2024-04-18 02:16:37, Train, Epoch : 8, Step : 4920, Loss : 0.36778, Acc : 0.847, Sensitive_Loss : 0.14244, Sensitive_Acc : 16.500, Run Time : 12.15 sec
INFO:root:2024-04-18 02:16:48, Train, Epoch : 8, Step : 4930, Loss : 0.36099, Acc : 0.856, Sensitive_Loss : 0.09747, Sensitive_Acc : 25.800, Run Time : 11.51 sec
INFO:root:2024-04-18 02:17:00, Train, Epoch : 8, Step : 4940, Loss : 0.39603, Acc : 0.831, Sensitive_Loss : 0.07917, Sensitive_Acc : 22.800, Run Time : 11.83 sec
INFO:root:2024-04-18 02:17:11, Train, Epoch : 8, Step : 4950, Loss : 0.41659, Acc : 0.812, Sensitive_Loss : 0.10774, Sensitive_Acc : 18.800, Run Time : 11.22 sec
INFO:root:2024-04-18 02:17:23, Train, Epoch : 8, Step : 4960, Loss : 0.34430, Acc : 0.841, Sensitive_Loss : 0.12275, Sensitive_Acc : 16.300, Run Time : 11.68 sec
INFO:root:2024-04-18 02:17:34, Train, Epoch : 8, Step : 4970, Loss : 0.29980, Acc : 0.819, Sensitive_Loss : 0.13028, Sensitive_Acc : 22.100, Run Time : 10.87 sec
INFO:root:2024-04-18 02:17:46, Train, Epoch : 8, Step : 4980, Loss : 0.25213, Acc : 0.900, Sensitive_Loss : 0.10059, Sensitive_Acc : 19.700, Run Time : 11.85 sec
INFO:root:2024-04-18 02:17:56, Train, Epoch : 8, Step : 4990, Loss : 0.39126, Acc : 0.847, Sensitive_Loss : 0.12065, Sensitive_Acc : 16.800, Run Time : 10.79 sec
INFO:root:2024-04-18 02:18:08, Train, Epoch : 8, Step : 5000, Loss : 0.31893, Acc : 0.863, Sensitive_Loss : 0.08800, Sensitive_Acc : 23.400, Run Time : 11.58 sec
INFO:root:2024-04-18 02:20:34, Dev, Step : 5000, Loss : 0.54803, Acc : 0.776, Auc : 0.851, Sensitive_Loss : 0.17993, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.851, Run Time : 146.44 sec
INFO:root:2024-04-18 02:20:44, Train, Epoch : 8, Step : 5010, Loss : 0.32271, Acc : 0.869, Sensitive_Loss : 0.08447, Sensitive_Acc : 20.500, Run Time : 155.77 sec
INFO:root:2024-04-18 02:20:55, Train, Epoch : 8, Step : 5020, Loss : 0.27710, Acc : 0.894, Sensitive_Loss : 0.09188, Sensitive_Acc : 19.400, Run Time : 11.11 sec
INFO:root:2024-04-18 02:21:06, Train, Epoch : 8, Step : 5030, Loss : 0.35114, Acc : 0.853, Sensitive_Loss : 0.08865, Sensitive_Acc : 20.600, Run Time : 11.47 sec
INFO:root:2024-04-18 02:21:17, Train, Epoch : 8, Step : 5040, Loss : 0.32794, Acc : 0.828, Sensitive_Loss : 0.05833, Sensitive_Acc : 22.100, Run Time : 11.04 sec
INFO:root:2024-04-18 02:21:29, Train, Epoch : 8, Step : 5050, Loss : 0.32038, Acc : 0.856, Sensitive_Loss : 0.08942, Sensitive_Acc : 24.500, Run Time : 11.52 sec
INFO:root:2024-04-18 02:21:40, Train, Epoch : 8, Step : 5060, Loss : 0.27278, Acc : 0.875, Sensitive_Loss : 0.11171, Sensitive_Acc : 22.300, Run Time : 11.25 sec
INFO:root:2024-04-18 02:21:52, Train, Epoch : 8, Step : 5070, Loss : 0.31622, Acc : 0.881, Sensitive_Loss : 0.08024, Sensitive_Acc : 23.800, Run Time : 11.90 sec
INFO:root:2024-04-18 02:24:24
INFO:root:y_pred: [0.05371807 0.00118928 0.1214451  ... 0.08168858 0.01331603 0.02461969]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.79516606e-03 1.15326202e-05 8.22349917e-03 1.96906156e-03
 2.06012651e-02 2.79157975e-05 3.76490178e-04 2.31706974e-04
 6.26255795e-02 9.99983668e-01 6.80847019e-02 5.24424424e-04
 8.10426311e-04 5.62044967e-04 9.99943495e-01 3.60149704e-02
 4.71037952e-03 9.99941468e-01 9.99882698e-01 4.27374383e-03
 9.97952461e-01 7.36009621e-04 1.19051873e-03 2.28971429e-03
 1.17926337e-01 1.53536737e-01 2.93731910e-06 3.68246328e-05
 4.77777794e-05 1.55983279e-02 3.72492068e-05 9.99293208e-01
 6.27431720e-02 9.06721950e-01 5.41203917e-05 1.89738770e-04
 1.63326070e-01 1.69908151e-01 1.99294910e-02 1.92414009e-04
 9.38941836e-02 9.98564422e-01 3.00859078e-03 1.30851974e-03
 9.94519293e-01 6.94064349e-02 3.04425269e-01 3.32615912e-01
 7.07850605e-02 9.96356726e-01 9.97062504e-01 9.99786794e-01
 9.90560472e-01 2.30515818e-03 1.10920761e-02 1.04174344e-02
 5.10977290e-04 6.00476237e-03 9.98485506e-01 3.90078640e-04
 1.76688391e-05 3.01994127e-03 4.69939783e-03 5.18256566e-04
 9.99852657e-01 3.95120144e-01 1.10612025e-04 6.61506737e-03
 3.33010107e-02 9.99557555e-01 9.99874234e-01 9.99974012e-01
 4.12863243e-04 4.03081059e-01 2.66148236e-05 8.42810810e-01
 5.91273047e-03 1.14842237e-06 4.98313806e-04 8.25142674e-03
 1.43678784e-01 1.29393840e-04 9.99851823e-01 9.83150959e-01
 1.27699915e-02 6.12139935e-04 9.68547910e-02 1.57554299e-04
 9.28699747e-02 1.67921418e-03 1.03284298e-02 3.15355649e-03
 2.01480434e-04 3.04848381e-05 6.72225633e-06 1.05502252e-02
 9.55197902e-04 7.16750264e-01 1.38192135e-03 5.60232380e-04
 9.44081927e-04 4.81013587e-04 5.75948060e-02 4.17777810e-05
 1.20047489e-02 3.50928603e-05 1.10217752e-02 2.69261271e-01
 6.77369654e-01 1.24373227e-01 1.81905361e-05 9.99989867e-01
 9.99892950e-01 5.06100150e-06 3.51877101e-02 2.60217511e-03
 4.55743121e-03 1.51462504e-04 8.60949606e-02 5.60126733e-03
 2.19522044e-03 2.12233324e-04 3.06421835e-02 1.29007982e-04
 1.68410945e-03 6.88863695e-01 4.64402438e-05 9.85090137e-01
 1.02290414e-01 1.09963849e-01 3.85109545e-03 2.06693847e-04
 2.20706443e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 02:24:24, Dev, Step : 5072, Loss : 0.55058, Acc : 0.774, Auc : 0.852, Sensitive_Loss : 0.18745, Sensitive_Acc : 21.677, Sensitive_Auc : 1.000, Mean auc: 0.852, Run Time : 150.35 sec
INFO:root:2024-04-18 02:24:39, Train, Epoch : 9, Step : 5080, Loss : 0.23556, Acc : 0.691, Sensitive_Loss : 0.04006, Sensitive_Acc : 19.300, Run Time : 13.75 sec
INFO:root:2024-04-18 02:24:53, Train, Epoch : 9, Step : 5090, Loss : 0.29570, Acc : 0.884, Sensitive_Loss : 0.05747, Sensitive_Acc : 21.700, Run Time : 14.33 sec
INFO:root:2024-04-18 02:25:06, Train, Epoch : 9, Step : 5100, Loss : 0.27878, Acc : 0.878, Sensitive_Loss : 0.09804, Sensitive_Acc : 21.200, Run Time : 12.97 sec
INFO:root:2024-04-18 02:28:21, Dev, Step : 5100, Loss : 0.53450, Acc : 0.778, Auc : 0.854, Sensitive_Loss : 0.17229, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.854, Run Time : 194.64 sec
INFO:root:2024-04-18 02:28:30, Train, Epoch : 9, Step : 5110, Loss : 0.32808, Acc : 0.872, Sensitive_Loss : 0.11650, Sensitive_Acc : 26.600, Run Time : 203.22 sec
INFO:root:2024-04-18 02:28:41, Train, Epoch : 9, Step : 5120, Loss : 0.30829, Acc : 0.844, Sensitive_Loss : 0.10427, Sensitive_Acc : 23.300, Run Time : 11.94 sec
INFO:root:2024-04-18 02:28:52, Train, Epoch : 9, Step : 5130, Loss : 0.31433, Acc : 0.863, Sensitive_Loss : 0.10038, Sensitive_Acc : 22.000, Run Time : 11.03 sec
INFO:root:2024-04-18 02:29:05, Train, Epoch : 9, Step : 5140, Loss : 0.28860, Acc : 0.872, Sensitive_Loss : 0.10911, Sensitive_Acc : 20.500, Run Time : 12.47 sec
INFO:root:2024-04-18 02:29:16, Train, Epoch : 9, Step : 5150, Loss : 0.25328, Acc : 0.903, Sensitive_Loss : 0.09966, Sensitive_Acc : 22.700, Run Time : 11.51 sec
INFO:root:2024-04-18 02:29:28, Train, Epoch : 9, Step : 5160, Loss : 0.29484, Acc : 0.878, Sensitive_Loss : 0.08789, Sensitive_Acc : 22.800, Run Time : 11.06 sec
INFO:root:2024-04-18 02:29:39, Train, Epoch : 9, Step : 5170, Loss : 0.25424, Acc : 0.863, Sensitive_Loss : 0.12621, Sensitive_Acc : 25.700, Run Time : 11.69 sec
INFO:root:2024-04-18 02:29:51, Train, Epoch : 9, Step : 5180, Loss : 0.29850, Acc : 0.887, Sensitive_Loss : 0.08902, Sensitive_Acc : 19.300, Run Time : 11.33 sec
INFO:root:2024-04-18 02:30:02, Train, Epoch : 9, Step : 5190, Loss : 0.29528, Acc : 0.884, Sensitive_Loss : 0.07136, Sensitive_Acc : 18.600, Run Time : 11.88 sec
INFO:root:2024-04-18 02:30:14, Train, Epoch : 9, Step : 5200, Loss : 0.32811, Acc : 0.847, Sensitive_Loss : 0.07993, Sensitive_Acc : 22.900, Run Time : 11.45 sec
INFO:root:2024-04-18 02:32:39, Dev, Step : 5200, Loss : 0.54976, Acc : 0.773, Auc : 0.850, Sensitive_Loss : 0.16496, Sensitive_Acc : 21.932, Sensitive_Auc : 1.000, Mean auc: 0.850, Run Time : 145.55 sec
INFO:root:2024-04-18 02:32:48, Train, Epoch : 9, Step : 5210, Loss : 0.25595, Acc : 0.897, Sensitive_Loss : 0.05705, Sensitive_Acc : 21.000, Run Time : 153.76 sec
INFO:root:2024-04-18 02:32:59, Train, Epoch : 9, Step : 5220, Loss : 0.26986, Acc : 0.891, Sensitive_Loss : 0.07766, Sensitive_Acc : 22.500, Run Time : 11.86 sec
INFO:root:2024-04-18 02:33:11, Train, Epoch : 9, Step : 5230, Loss : 0.28409, Acc : 0.878, Sensitive_Loss : 0.06795, Sensitive_Acc : 18.900, Run Time : 11.23 sec
INFO:root:2024-04-18 02:33:23, Train, Epoch : 9, Step : 5240, Loss : 0.30111, Acc : 0.847, Sensitive_Loss : 0.08491, Sensitive_Acc : 23.300, Run Time : 12.09 sec
INFO:root:2024-04-18 02:33:35, Train, Epoch : 9, Step : 5250, Loss : 0.29584, Acc : 0.897, Sensitive_Loss : 0.10816, Sensitive_Acc : 19.200, Run Time : 12.05 sec
INFO:root:2024-04-18 02:33:47, Train, Epoch : 9, Step : 5260, Loss : 0.30424, Acc : 0.875, Sensitive_Loss : 0.09436, Sensitive_Acc : 15.700, Run Time : 11.79 sec
INFO:root:2024-04-18 02:33:57, Train, Epoch : 9, Step : 5270, Loss : 0.40969, Acc : 0.850, Sensitive_Loss : 0.10790, Sensitive_Acc : 18.700, Run Time : 10.83 sec
INFO:root:2024-04-18 02:34:09, Train, Epoch : 9, Step : 5280, Loss : 0.26129, Acc : 0.884, Sensitive_Loss : 0.07301, Sensitive_Acc : 23.000, Run Time : 11.17 sec
INFO:root:2024-04-18 02:34:20, Train, Epoch : 9, Step : 5290, Loss : 0.29735, Acc : 0.859, Sensitive_Loss : 0.10350, Sensitive_Acc : 24.900, Run Time : 11.64 sec
INFO:root:2024-04-18 02:34:32, Train, Epoch : 9, Step : 5300, Loss : 0.28978, Acc : 0.869, Sensitive_Loss : 0.03885, Sensitive_Acc : 21.700, Run Time : 11.35 sec
INFO:root:2024-04-18 02:36:58, Dev, Step : 5300, Loss : 0.55243, Acc : 0.771, Auc : 0.847, Sensitive_Loss : 0.18271, Sensitive_Acc : 21.677, Sensitive_Auc : 1.000, Mean auc: 0.847, Run Time : 146.10 sec
INFO:root:2024-04-18 02:37:07, Train, Epoch : 9, Step : 5310, Loss : 0.33759, Acc : 0.844, Sensitive_Loss : 0.09097, Sensitive_Acc : 21.900, Run Time : 155.23 sec
INFO:root:2024-04-18 02:37:18, Train, Epoch : 9, Step : 5320, Loss : 0.25927, Acc : 0.881, Sensitive_Loss : 0.11853, Sensitive_Acc : 24.700, Run Time : 11.08 sec
INFO:root:2024-04-18 02:37:30, Train, Epoch : 9, Step : 5330, Loss : 0.36406, Acc : 0.847, Sensitive_Loss : 0.09090, Sensitive_Acc : 18.100, Run Time : 11.73 sec
INFO:root:2024-04-18 02:37:41, Train, Epoch : 9, Step : 5340, Loss : 0.34929, Acc : 0.881, Sensitive_Loss : 0.05593, Sensitive_Acc : 23.600, Run Time : 11.14 sec
INFO:root:2024-04-18 02:37:52, Train, Epoch : 9, Step : 5350, Loss : 0.28044, Acc : 0.875, Sensitive_Loss : 0.10512, Sensitive_Acc : 18.900, Run Time : 11.48 sec
INFO:root:2024-04-18 02:38:04, Train, Epoch : 9, Step : 5360, Loss : 0.33989, Acc : 0.869, Sensitive_Loss : 0.09200, Sensitive_Acc : 21.700, Run Time : 11.68 sec
INFO:root:2024-04-18 02:38:16, Train, Epoch : 9, Step : 5370, Loss : 0.35428, Acc : 0.847, Sensitive_Loss : 0.11642, Sensitive_Acc : 22.200, Run Time : 11.93 sec
INFO:root:2024-04-18 02:38:27, Train, Epoch : 9, Step : 5380, Loss : 0.29464, Acc : 0.875, Sensitive_Loss : 0.09575, Sensitive_Acc : 23.500, Run Time : 11.59 sec
INFO:root:2024-04-18 02:38:39, Train, Epoch : 9, Step : 5390, Loss : 0.35626, Acc : 0.847, Sensitive_Loss : 0.05242, Sensitive_Acc : 23.700, Run Time : 11.22 sec
INFO:root:2024-04-18 02:38:50, Train, Epoch : 9, Step : 5400, Loss : 0.32802, Acc : 0.869, Sensitive_Loss : 0.05958, Sensitive_Acc : 16.700, Run Time : 11.38 sec
INFO:root:2024-04-18 02:41:15, Dev, Step : 5400, Loss : 0.57551, Acc : 0.766, Auc : 0.845, Sensitive_Loss : 0.17251, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.845, Run Time : 145.37 sec
INFO:root:2024-04-18 02:41:24, Train, Epoch : 9, Step : 5410, Loss : 0.36989, Acc : 0.847, Sensitive_Loss : 0.09826, Sensitive_Acc : 19.300, Run Time : 153.93 sec
INFO:root:2024-04-18 02:41:35, Train, Epoch : 9, Step : 5420, Loss : 0.37919, Acc : 0.853, Sensitive_Loss : 0.10934, Sensitive_Acc : 21.900, Run Time : 11.30 sec
INFO:root:2024-04-18 02:41:47, Train, Epoch : 9, Step : 5430, Loss : 0.33774, Acc : 0.878, Sensitive_Loss : 0.08311, Sensitive_Acc : 20.700, Run Time : 11.84 sec
INFO:root:2024-04-18 02:41:58, Train, Epoch : 9, Step : 5440, Loss : 0.36836, Acc : 0.863, Sensitive_Loss : 0.12247, Sensitive_Acc : 18.600, Run Time : 11.07 sec
INFO:root:2024-04-18 02:42:09, Train, Epoch : 9, Step : 5450, Loss : 0.27497, Acc : 0.881, Sensitive_Loss : 0.10562, Sensitive_Acc : 25.000, Run Time : 10.92 sec
INFO:root:2024-04-18 02:42:21, Train, Epoch : 9, Step : 5460, Loss : 0.25443, Acc : 0.884, Sensitive_Loss : 0.10846, Sensitive_Acc : 25.000, Run Time : 11.47 sec
INFO:root:2024-04-18 02:42:32, Train, Epoch : 9, Step : 5470, Loss : 0.29958, Acc : 0.881, Sensitive_Loss : 0.09891, Sensitive_Acc : 16.700, Run Time : 11.38 sec
INFO:root:2024-04-18 02:42:43, Train, Epoch : 9, Step : 5480, Loss : 0.38352, Acc : 0.850, Sensitive_Loss : 0.16515, Sensitive_Acc : 21.900, Run Time : 11.42 sec
INFO:root:2024-04-18 02:42:55, Train, Epoch : 9, Step : 5490, Loss : 0.32692, Acc : 0.844, Sensitive_Loss : 0.07182, Sensitive_Acc : 19.500, Run Time : 11.46 sec
INFO:root:2024-04-18 02:43:06, Train, Epoch : 9, Step : 5500, Loss : 0.28054, Acc : 0.887, Sensitive_Loss : 0.07739, Sensitive_Acc : 18.300, Run Time : 11.40 sec
INFO:root:2024-04-18 02:45:33, Dev, Step : 5500, Loss : 0.56104, Acc : 0.775, Auc : 0.847, Sensitive_Loss : 0.18242, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.847, Run Time : 146.92 sec
INFO:root:2024-04-18 02:45:43, Train, Epoch : 9, Step : 5510, Loss : 0.33326, Acc : 0.850, Sensitive_Loss : 0.12252, Sensitive_Acc : 24.400, Run Time : 156.33 sec
INFO:root:2024-04-18 02:45:53, Train, Epoch : 9, Step : 5520, Loss : 0.36257, Acc : 0.834, Sensitive_Loss : 0.07740, Sensitive_Acc : 18.700, Run Time : 10.61 sec
INFO:root:2024-04-18 02:46:05, Train, Epoch : 9, Step : 5530, Loss : 0.27273, Acc : 0.881, Sensitive_Loss : 0.06657, Sensitive_Acc : 15.400, Run Time : 11.82 sec
INFO:root:2024-04-18 02:46:16, Train, Epoch : 9, Step : 5540, Loss : 0.30972, Acc : 0.878, Sensitive_Loss : 0.08819, Sensitive_Acc : 20.900, Run Time : 10.86 sec
INFO:root:2024-04-18 02:46:27, Train, Epoch : 9, Step : 5550, Loss : 0.32257, Acc : 0.866, Sensitive_Loss : 0.13790, Sensitive_Acc : 23.700, Run Time : 11.37 sec
INFO:root:2024-04-18 02:46:39, Train, Epoch : 9, Step : 5560, Loss : 0.33226, Acc : 0.853, Sensitive_Loss : 0.11783, Sensitive_Acc : 16.800, Run Time : 11.52 sec
INFO:root:2024-04-18 02:46:51, Train, Epoch : 9, Step : 5570, Loss : 0.32341, Acc : 0.897, Sensitive_Loss : 0.13475, Sensitive_Acc : 25.300, Run Time : 12.24 sec
INFO:root:2024-04-18 02:47:08, Train, Epoch : 9, Step : 5580, Loss : 0.36869, Acc : 0.856, Sensitive_Loss : 0.10497, Sensitive_Acc : 24.300, Run Time : 16.47 sec
INFO:root:2024-04-18 02:47:23, Train, Epoch : 9, Step : 5590, Loss : 0.36784, Acc : 0.856, Sensitive_Loss : 0.13037, Sensitive_Acc : 23.400, Run Time : 15.69 sec
INFO:root:2024-04-18 02:47:38, Train, Epoch : 9, Step : 5600, Loss : 0.33496, Acc : 0.863, Sensitive_Loss : 0.05875, Sensitive_Acc : 25.500, Run Time : 14.55 sec
INFO:root:2024-04-18 02:50:25, Dev, Step : 5600, Loss : 0.55904, Acc : 0.766, Auc : 0.843, Sensitive_Loss : 0.16739, Sensitive_Acc : 21.782, Sensitive_Auc : 0.999, Mean auc: 0.843, Run Time : 167.03 sec
INFO:root:2024-04-18 02:50:33, Train, Epoch : 9, Step : 5610, Loss : 0.27449, Acc : 0.909, Sensitive_Loss : 0.08743, Sensitive_Acc : 21.000, Run Time : 175.53 sec
INFO:root:2024-04-18 02:50:45, Train, Epoch : 9, Step : 5620, Loss : 0.30680, Acc : 0.872, Sensitive_Loss : 0.03848, Sensitive_Acc : 24.400, Run Time : 11.15 sec
INFO:root:2024-04-18 02:50:57, Train, Epoch : 9, Step : 5630, Loss : 0.31949, Acc : 0.853, Sensitive_Loss : 0.14394, Sensitive_Acc : 21.900, Run Time : 11.97 sec
INFO:root:2024-04-18 02:51:08, Train, Epoch : 9, Step : 5640, Loss : 0.31968, Acc : 0.887, Sensitive_Loss : 0.10287, Sensitive_Acc : 20.000, Run Time : 11.65 sec
INFO:root:2024-04-18 02:51:19, Train, Epoch : 9, Step : 5650, Loss : 0.32842, Acc : 0.872, Sensitive_Loss : 0.09444, Sensitive_Acc : 23.400, Run Time : 10.56 sec
INFO:root:2024-04-18 02:51:30, Train, Epoch : 9, Step : 5660, Loss : 0.29490, Acc : 0.884, Sensitive_Loss : 0.07614, Sensitive_Acc : 20.200, Run Time : 11.23 sec
INFO:root:2024-04-18 02:51:42, Train, Epoch : 9, Step : 5670, Loss : 0.25933, Acc : 0.891, Sensitive_Loss : 0.06709, Sensitive_Acc : 18.100, Run Time : 11.51 sec
INFO:root:2024-04-18 02:51:53, Train, Epoch : 9, Step : 5680, Loss : 0.28442, Acc : 0.856, Sensitive_Loss : 0.08786, Sensitive_Acc : 16.700, Run Time : 11.09 sec
INFO:root:2024-04-18 02:52:06, Train, Epoch : 9, Step : 5690, Loss : 0.36580, Acc : 0.841, Sensitive_Loss : 0.06697, Sensitive_Acc : 23.800, Run Time : 12.90 sec
INFO:root:2024-04-18 02:52:17, Train, Epoch : 9, Step : 5700, Loss : 0.29430, Acc : 0.878, Sensitive_Loss : 0.08555, Sensitive_Acc : 20.200, Run Time : 11.06 sec
INFO:root:2024-04-18 02:54:43, Dev, Step : 5700, Loss : 0.58068, Acc : 0.768, Auc : 0.848, Sensitive_Loss : 0.17317, Sensitive_Acc : 21.677, Sensitive_Auc : 1.000, Mean auc: 0.848, Run Time : 146.35 sec
INFO:root:2024-04-18 02:57:12
INFO:root:y_pred: [0.05656341 0.00060674 0.0798968  ... 0.05826635 0.01832042 0.01260952]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [8.63537076e-04 7.21013657e-06 2.61105895e-02 6.71341142e-04
 5.07945847e-03 8.00809812e-06 1.04862382e-04 1.35097449e-04
 5.81505783e-02 9.99988675e-01 5.92147522e-02 5.16549393e-04
 2.77611136e-04 3.26326262e-04 9.99948621e-01 1.36955632e-02
 2.13217526e-03 9.99889851e-01 9.99833941e-01 3.79270595e-03
 9.98353481e-01 5.80358843e-04 1.08326727e-03 2.32850225e-03
 1.40867993e-01 9.68627483e-02 2.16707826e-06 6.97831347e-05
 2.51644615e-05 5.39588742e-03 8.88813593e-05 9.99126494e-01
 1.61992162e-02 9.16874230e-01 9.47246463e-06 1.21030687e-04
 1.17279150e-01 1.08776391e-01 1.19806901e-02 3.86232219e-04
 5.35043553e-02 9.99305725e-01 4.13953699e-03 1.20204524e-03
 9.97690678e-01 4.49800938e-02 3.19763631e-01 1.14772961e-01
 5.68269081e-02 9.97726738e-01 9.94549811e-01 9.99209225e-01
 9.93707001e-01 6.54795207e-04 7.20690994e-04 1.04119545e-02
 8.13127335e-05 9.21141077e-03 9.98978734e-01 3.66095745e-04
 7.71791656e-06 1.40908314e-03 1.32448995e-03 1.59260337e-04
 9.99812782e-01 1.64355233e-01 4.08474589e-05 4.51440597e-03
 4.86150803e-03 9.99259651e-01 9.99603689e-01 9.99977469e-01
 1.07749947e-04 1.97810158e-01 2.21324863e-05 8.05469751e-01
 8.13743006e-03 6.68142718e-07 3.06002592e-04 9.35852434e-03
 4.89326306e-02 4.47694911e-05 9.99863386e-01 9.92004097e-01
 6.30699424e-03 4.34063433e-04 9.59540680e-02 1.43932848e-04
 5.08062057e-02 3.41669569e-04 4.87941457e-03 2.74460413e-03
 6.37424018e-05 1.90862211e-05 2.18607947e-05 3.09984852e-03
 2.31348458e-04 8.01966548e-01 4.95951565e-04 5.16076980e-04
 1.57893694e-03 1.32071698e-04 6.41191155e-02 2.67121322e-05
 1.04401736e-02 2.67974810e-05 4.26970795e-03 3.41338664e-01
 6.52046800e-01 8.73176455e-02 1.03554594e-05 9.99979854e-01
 9.99698758e-01 1.54023132e-06 1.03995964e-01 8.81164242e-03
 5.86312544e-03 2.84455291e-05 2.20701039e-01 9.40120965e-03
 2.20807572e-03 1.19931588e-04 6.61289599e-03 3.21103798e-05
 3.35906079e-04 8.14300656e-01 2.77233212e-05 9.85776365e-01
 3.81786153e-02 2.30777040e-02 1.51541317e-03 1.11611362e-03
 4.92931576e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 02:57:12, Dev, Step : 5706, Loss : 0.57952, Acc : 0.768, Auc : 0.848, Sensitive_Loss : 0.16668, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.848, Run Time : 144.80 sec
INFO:root:2024-04-18 02:57:18, Train, Epoch : 10, Step : 5710, Loss : 0.10701, Acc : 0.362, Sensitive_Loss : 0.02653, Sensitive_Acc : 8.500, Run Time : 5.77 sec
INFO:root:2024-04-18 02:57:29, Train, Epoch : 10, Step : 5720, Loss : 0.35772, Acc : 0.859, Sensitive_Loss : 0.07921, Sensitive_Acc : 22.200, Run Time : 11.20 sec
INFO:root:2024-04-18 02:57:42, Train, Epoch : 10, Step : 5730, Loss : 0.38403, Acc : 0.847, Sensitive_Loss : 0.08574, Sensitive_Acc : 20.600, Run Time : 12.18 sec
INFO:root:2024-04-18 02:57:53, Train, Epoch : 10, Step : 5740, Loss : 0.25456, Acc : 0.894, Sensitive_Loss : 0.07711, Sensitive_Acc : 15.300, Run Time : 10.92 sec
INFO:root:2024-04-18 02:58:04, Train, Epoch : 10, Step : 5750, Loss : 0.27263, Acc : 0.887, Sensitive_Loss : 0.09457, Sensitive_Acc : 20.900, Run Time : 11.35 sec
INFO:root:2024-04-18 02:58:15, Train, Epoch : 10, Step : 5760, Loss : 0.27720, Acc : 0.881, Sensitive_Loss : 0.07988, Sensitive_Acc : 21.400, Run Time : 11.38 sec
INFO:root:2024-04-18 02:58:27, Train, Epoch : 10, Step : 5770, Loss : 0.27370, Acc : 0.894, Sensitive_Loss : 0.13119, Sensitive_Acc : 21.800, Run Time : 12.13 sec
INFO:root:2024-04-18 02:58:38, Train, Epoch : 10, Step : 5780, Loss : 0.34162, Acc : 0.850, Sensitive_Loss : 0.09521, Sensitive_Acc : 23.300, Run Time : 11.03 sec
INFO:root:2024-04-18 02:58:50, Train, Epoch : 10, Step : 5790, Loss : 0.28621, Acc : 0.881, Sensitive_Loss : 0.08828, Sensitive_Acc : 19.400, Run Time : 11.73 sec
INFO:root:2024-04-18 02:59:02, Train, Epoch : 10, Step : 5800, Loss : 0.27520, Acc : 0.894, Sensitive_Loss : 0.12909, Sensitive_Acc : 23.000, Run Time : 11.59 sec
INFO:root:2024-04-18 03:01:29, Dev, Step : 5800, Loss : 0.57613, Acc : 0.770, Auc : 0.846, Sensitive_Loss : 0.16990, Sensitive_Acc : 21.677, Sensitive_Auc : 1.000, Mean auc: 0.846, Run Time : 146.77 sec
INFO:root:2024-04-18 03:01:38, Train, Epoch : 10, Step : 5810, Loss : 0.28244, Acc : 0.863, Sensitive_Loss : 0.10407, Sensitive_Acc : 18.800, Run Time : 156.48 sec
INFO:root:2024-04-18 03:01:49, Train, Epoch : 10, Step : 5820, Loss : 0.29657, Acc : 0.894, Sensitive_Loss : 0.08084, Sensitive_Acc : 25.900, Run Time : 10.74 sec
INFO:root:2024-04-18 03:02:00, Train, Epoch : 10, Step : 5830, Loss : 0.26869, Acc : 0.866, Sensitive_Loss : 0.09563, Sensitive_Acc : 21.700, Run Time : 11.34 sec
INFO:root:2024-04-18 03:02:12, Train, Epoch : 10, Step : 5840, Loss : 0.27885, Acc : 0.884, Sensitive_Loss : 0.18553, Sensitive_Acc : 22.200, Run Time : 11.17 sec
INFO:root:2024-04-18 03:02:23, Train, Epoch : 10, Step : 5850, Loss : 0.30152, Acc : 0.894, Sensitive_Loss : 0.10344, Sensitive_Acc : 21.600, Run Time : 11.78 sec
INFO:root:2024-04-18 03:02:35, Train, Epoch : 10, Step : 5860, Loss : 0.26734, Acc : 0.906, Sensitive_Loss : 0.08024, Sensitive_Acc : 22.400, Run Time : 11.61 sec
INFO:root:2024-04-18 03:02:46, Train, Epoch : 10, Step : 5870, Loss : 0.30990, Acc : 0.869, Sensitive_Loss : 0.11035, Sensitive_Acc : 24.500, Run Time : 10.74 sec
INFO:root:2024-04-18 03:02:58, Train, Epoch : 10, Step : 5880, Loss : 0.32975, Acc : 0.856, Sensitive_Loss : 0.13135, Sensitive_Acc : 26.700, Run Time : 12.26 sec
INFO:root:2024-04-18 03:03:08, Train, Epoch : 10, Step : 5890, Loss : 0.22896, Acc : 0.900, Sensitive_Loss : 0.08593, Sensitive_Acc : 21.000, Run Time : 10.37 sec
INFO:root:2024-04-18 03:03:19, Train, Epoch : 10, Step : 5900, Loss : 0.28255, Acc : 0.878, Sensitive_Loss : 0.07070, Sensitive_Acc : 23.600, Run Time : 10.80 sec
INFO:root:2024-04-18 03:05:47, Dev, Step : 5900, Loss : 0.56377, Acc : 0.775, Auc : 0.846, Sensitive_Loss : 0.15660, Sensitive_Acc : 21.782, Sensitive_Auc : 1.000, Mean auc: 0.846, Run Time : 147.60 sec
INFO:root:2024-04-18 03:05:56, Train, Epoch : 10, Step : 5910, Loss : 0.31238, Acc : 0.863, Sensitive_Loss : 0.06464, Sensitive_Acc : 20.800, Run Time : 156.64 sec
INFO:root:2024-04-18 03:06:07, Train, Epoch : 10, Step : 5920, Loss : 0.29903, Acc : 0.863, Sensitive_Loss : 0.05719, Sensitive_Acc : 23.500, Run Time : 11.41 sec
INFO:root:2024-04-18 03:06:20, Train, Epoch : 10, Step : 5930, Loss : 0.27665, Acc : 0.875, Sensitive_Loss : 0.07897, Sensitive_Acc : 21.600, Run Time : 12.32 sec
INFO:root:2024-04-18 03:06:31, Train, Epoch : 10, Step : 5940, Loss : 0.26484, Acc : 0.891, Sensitive_Loss : 0.08188, Sensitive_Acc : 24.400, Run Time : 11.27 sec
INFO:root:2024-04-18 03:06:41, Train, Epoch : 10, Step : 5950, Loss : 0.28216, Acc : 0.866, Sensitive_Loss : 0.12021, Sensitive_Acc : 22.700, Run Time : 10.60 sec
INFO:root:2024-04-18 03:06:53, Train, Epoch : 10, Step : 5960, Loss : 0.26541, Acc : 0.869, Sensitive_Loss : 0.05392, Sensitive_Acc : 19.600, Run Time : 11.35 sec
INFO:root:2024-04-18 03:07:04, Train, Epoch : 10, Step : 5970, Loss : 0.27253, Acc : 0.903, Sensitive_Loss : 0.09490, Sensitive_Acc : 23.100, Run Time : 11.07 sec
INFO:root:2024-04-18 03:07:16, Train, Epoch : 10, Step : 5980, Loss : 0.36732, Acc : 0.863, Sensitive_Loss : 0.10725, Sensitive_Acc : 24.900, Run Time : 12.37 sec
INFO:root:2024-04-18 03:07:28, Train, Epoch : 10, Step : 5990, Loss : 0.29551, Acc : 0.881, Sensitive_Loss : 0.08963, Sensitive_Acc : 21.900, Run Time : 12.15 sec
INFO:root:2024-04-18 03:07:39, Train, Epoch : 10, Step : 6000, Loss : 0.27745, Acc : 0.891, Sensitive_Loss : 0.06198, Sensitive_Acc : 22.600, Run Time : 10.75 sec
INFO:root:2024-04-18 03:10:05, Dev, Step : 6000, Loss : 0.56813, Acc : 0.770, Auc : 0.843, Sensitive_Loss : 0.15868, Sensitive_Acc : 21.932, Sensitive_Auc : 1.000, Mean auc: 0.843, Run Time : 146.13 sec
INFO:root:2024-04-18 03:10:14, Train, Epoch : 10, Step : 6010, Loss : 0.27277, Acc : 0.881, Sensitive_Loss : 0.13626, Sensitive_Acc : 18.300, Run Time : 154.71 sec
INFO:root:2024-04-18 03:10:25, Train, Epoch : 10, Step : 6020, Loss : 0.30322, Acc : 0.859, Sensitive_Loss : 0.07547, Sensitive_Acc : 21.200, Run Time : 11.44 sec
INFO:root:2024-04-18 03:10:38, Train, Epoch : 10, Step : 6030, Loss : 0.35810, Acc : 0.866, Sensitive_Loss : 0.07312, Sensitive_Acc : 21.000, Run Time : 12.67 sec
INFO:root:2024-04-18 03:10:48, Train, Epoch : 10, Step : 6040, Loss : 0.29705, Acc : 0.866, Sensitive_Loss : 0.09465, Sensitive_Acc : 22.700, Run Time : 10.21 sec
INFO:root:2024-04-18 03:11:00, Train, Epoch : 10, Step : 6050, Loss : 0.28604, Acc : 0.869, Sensitive_Loss : 0.11581, Sensitive_Acc : 22.700, Run Time : 11.80 sec
INFO:root:2024-04-18 03:11:12, Train, Epoch : 10, Step : 6060, Loss : 0.26258, Acc : 0.881, Sensitive_Loss : 0.10474, Sensitive_Acc : 21.200, Run Time : 11.96 sec
INFO:root:2024-04-18 03:11:22, Train, Epoch : 10, Step : 6070, Loss : 0.27821, Acc : 0.891, Sensitive_Loss : 0.06417, Sensitive_Acc : 22.900, Run Time : 10.49 sec
INFO:root:2024-04-18 03:11:33, Train, Epoch : 10, Step : 6080, Loss : 0.26659, Acc : 0.881, Sensitive_Loss : 0.06079, Sensitive_Acc : 16.500, Run Time : 11.11 sec
INFO:root:2024-04-18 03:11:45, Train, Epoch : 10, Step : 6090, Loss : 0.28828, Acc : 0.869, Sensitive_Loss : 0.08827, Sensitive_Acc : 20.500, Run Time : 11.94 sec
INFO:root:2024-04-18 03:11:57, Train, Epoch : 10, Step : 6100, Loss : 0.33049, Acc : 0.897, Sensitive_Loss : 0.15094, Sensitive_Acc : 19.200, Run Time : 11.59 sec
INFO:root:2024-04-18 03:14:23, Dev, Step : 6100, Loss : 0.56149, Acc : 0.764, Auc : 0.846, Sensitive_Loss : 0.16173, Sensitive_Acc : 21.782, Sensitive_Auc : 1.000, Mean auc: 0.846, Run Time : 146.18 sec
INFO:root:2024-04-18 03:14:32, Train, Epoch : 10, Step : 6110, Loss : 0.25115, Acc : 0.884, Sensitive_Loss : 0.09415, Sensitive_Acc : 24.800, Run Time : 154.68 sec
INFO:root:2024-04-18 03:14:42, Train, Epoch : 10, Step : 6120, Loss : 0.30385, Acc : 0.856, Sensitive_Loss : 0.08924, Sensitive_Acc : 24.200, Run Time : 10.78 sec
INFO:root:2024-04-18 03:14:55, Train, Epoch : 10, Step : 6130, Loss : 0.30828, Acc : 0.906, Sensitive_Loss : 0.10067, Sensitive_Acc : 16.700, Run Time : 12.97 sec
INFO:root:2024-04-18 03:15:07, Train, Epoch : 10, Step : 6140, Loss : 0.26041, Acc : 0.878, Sensitive_Loss : 0.13541, Sensitive_Acc : 21.400, Run Time : 11.33 sec
INFO:root:2024-04-18 03:15:18, Train, Epoch : 10, Step : 6150, Loss : 0.28666, Acc : 0.884, Sensitive_Loss : 0.06677, Sensitive_Acc : 22.300, Run Time : 11.54 sec
INFO:root:2024-04-18 03:15:29, Train, Epoch : 10, Step : 6160, Loss : 0.30195, Acc : 0.884, Sensitive_Loss : 0.10378, Sensitive_Acc : 22.200, Run Time : 10.27 sec
INFO:root:2024-04-18 03:15:41, Train, Epoch : 10, Step : 6170, Loss : 0.29641, Acc : 0.878, Sensitive_Loss : 0.11641, Sensitive_Acc : 20.200, Run Time : 12.60 sec
INFO:root:2024-04-18 03:15:52, Train, Epoch : 10, Step : 6180, Loss : 0.30988, Acc : 0.875, Sensitive_Loss : 0.06186, Sensitive_Acc : 19.700, Run Time : 11.20 sec
INFO:root:2024-04-18 03:16:04, Train, Epoch : 10, Step : 6190, Loss : 0.28741, Acc : 0.894, Sensitive_Loss : 0.06864, Sensitive_Acc : 20.900, Run Time : 11.13 sec
INFO:root:2024-04-18 03:16:15, Train, Epoch : 10, Step : 6200, Loss : 0.26448, Acc : 0.869, Sensitive_Loss : 0.10527, Sensitive_Acc : 24.600, Run Time : 11.46 sec
INFO:root:2024-04-18 03:18:41, Dev, Step : 6200, Loss : 0.56871, Acc : 0.772, Auc : 0.846, Sensitive_Loss : 0.19312, Sensitive_Acc : 21.677, Sensitive_Auc : 1.000, Mean auc: 0.846, Run Time : 146.07 sec
INFO:root:2024-04-18 03:18:50, Train, Epoch : 10, Step : 6210, Loss : 0.28082, Acc : 0.872, Sensitive_Loss : 0.16329, Sensitive_Acc : 21.700, Run Time : 155.03 sec
INFO:root:2024-04-18 03:19:02, Train, Epoch : 10, Step : 6220, Loss : 0.23594, Acc : 0.891, Sensitive_Loss : 0.09777, Sensitive_Acc : 26.200, Run Time : 12.06 sec
INFO:root:2024-04-18 03:19:14, Train, Epoch : 10, Step : 6230, Loss : 0.35170, Acc : 0.838, Sensitive_Loss : 0.07156, Sensitive_Acc : 19.200, Run Time : 11.99 sec
INFO:root:2024-04-18 03:19:25, Train, Epoch : 10, Step : 6240, Loss : 0.27515, Acc : 0.853, Sensitive_Loss : 0.10188, Sensitive_Acc : 19.500, Run Time : 11.35 sec
INFO:root:2024-04-18 03:19:36, Train, Epoch : 10, Step : 6250, Loss : 0.28217, Acc : 0.894, Sensitive_Loss : 0.07957, Sensitive_Acc : 23.400, Run Time : 10.75 sec
INFO:root:2024-04-18 03:19:47, Train, Epoch : 10, Step : 6260, Loss : 0.29044, Acc : 0.863, Sensitive_Loss : 0.06675, Sensitive_Acc : 24.000, Run Time : 11.02 sec
INFO:root:2024-04-18 03:19:58, Train, Epoch : 10, Step : 6270, Loss : 0.27214, Acc : 0.881, Sensitive_Loss : 0.10705, Sensitive_Acc : 22.600, Run Time : 11.09 sec
INFO:root:2024-04-18 03:20:10, Train, Epoch : 10, Step : 6280, Loss : 0.25715, Acc : 0.903, Sensitive_Loss : 0.08407, Sensitive_Acc : 24.300, Run Time : 11.54 sec
INFO:root:2024-04-18 03:20:22, Train, Epoch : 10, Step : 6290, Loss : 0.25961, Acc : 0.881, Sensitive_Loss : 0.06037, Sensitive_Acc : 20.600, Run Time : 12.20 sec
INFO:root:2024-04-18 03:20:33, Train, Epoch : 10, Step : 6300, Loss : 0.33423, Acc : 0.850, Sensitive_Loss : 0.06218, Sensitive_Acc : 26.900, Run Time : 10.83 sec
INFO:root:2024-04-18 03:22:59, Dev, Step : 6300, Loss : 0.58256, Acc : 0.773, Auc : 0.848, Sensitive_Loss : 0.18058, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.848, Run Time : 146.31 sec
INFO:root:2024-04-18 03:23:08, Train, Epoch : 10, Step : 6310, Loss : 0.28593, Acc : 0.875, Sensitive_Loss : 0.09375, Sensitive_Acc : 17.600, Run Time : 154.76 sec
INFO:root:2024-04-18 03:23:20, Train, Epoch : 10, Step : 6320, Loss : 0.29481, Acc : 0.850, Sensitive_Loss : 0.07571, Sensitive_Acc : 24.100, Run Time : 12.23 sec
INFO:root:2024-04-18 03:23:31, Train, Epoch : 10, Step : 6330, Loss : 0.28504, Acc : 0.872, Sensitive_Loss : 0.07231, Sensitive_Acc : 19.700, Run Time : 10.90 sec
INFO:root:2024-04-18 03:23:42, Train, Epoch : 10, Step : 6340, Loss : 0.23195, Acc : 0.903, Sensitive_Loss : 0.12107, Sensitive_Acc : 18.100, Run Time : 10.86 sec
INFO:root:2024-04-18 03:26:07
INFO:root:y_pred: [0.03282575 0.00039945 0.15091388 ... 0.09097413 0.00389112 0.01635038]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [6.34606462e-04 5.32351896e-06 9.16546024e-03 4.31320572e-04
 3.07592540e-03 4.98181134e-06 4.12087429e-05 8.43667513e-05
 6.11478202e-02 9.99979973e-01 5.36324121e-02 3.10896430e-04
 2.19725960e-04 2.96090322e-04 9.99935508e-01 1.24801230e-02
 7.17901159e-04 9.99874473e-01 9.99665380e-01 1.76153099e-03
 9.98243093e-01 3.83222912e-04 7.92817387e-04 1.96321099e-03
 1.13185056e-01 1.32676572e-01 9.75943635e-07 2.95709087e-05
 2.49392924e-05 3.80966067e-03 1.87822309e-06 9.98913527e-01
 1.48519874e-02 8.28500867e-01 1.38704008e-05 4.37932176e-05
 1.40609026e-01 6.82345405e-02 6.54520001e-03 7.34400528e-05
 4.46300358e-02 9.98245716e-01 6.00236526e-04 5.19932131e-04
 9.97227013e-01 5.08433916e-02 2.01149881e-01 1.11673944e-01
 6.47775903e-02 9.97085392e-01 9.94318545e-01 9.99694347e-01
 9.91651833e-01 4.26843151e-04 3.16161476e-03 5.23711229e-03
 6.82313694e-05 4.40918002e-03 9.98389244e-01 4.67432255e-04
 2.66431016e-06 1.08194922e-03 9.91428504e-04 2.05859935e-04
 9.99842882e-01 1.73439369e-01 4.67077043e-05 6.12413976e-03
 8.06372985e-03 9.98924673e-01 9.99491811e-01 9.99977469e-01
 1.11204048e-04 1.49346128e-01 3.67348075e-05 7.98565209e-01
 1.53547945e-03 4.36750298e-07 1.84537013e-04 8.89802631e-03
 7.35422522e-02 1.53782566e-05 9.99743044e-01 9.69892263e-01
 9.56895761e-03 5.10594109e-04 4.70640585e-02 5.83316360e-05
 2.27107052e-02 6.70047419e-04 5.77989826e-03 7.77135952e-04
 5.21152215e-05 1.32075111e-05 3.85364956e-06 4.21224069e-03
 2.38682755e-04 6.78371608e-01 9.73334187e-04 3.03953886e-04
 3.45265376e-04 1.41602010e-04 4.31559272e-02 4.99763992e-05
 9.54251084e-03 1.35499258e-05 4.54177614e-03 2.44914621e-01
 7.35170424e-01 9.48159546e-02 7.64935248e-06 9.99980092e-01
 9.99727547e-01 1.00558191e-06 7.19397217e-02 1.61825144e-03
 4.95737372e-03 1.73136053e-04 1.72238469e-01 1.19384832e-03
 8.15976469e-04 1.63162127e-04 5.29710390e-03 5.25846808e-05
 2.86346389e-04 6.67174101e-01 3.18410894e-05 9.69642699e-01
 1.94382165e-02 1.52901560e-02 1.89771561e-03 3.62019957e-04
 2.16426520e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 03:26:07, Dev, Step : 6340, Loss : 0.57834, Acc : 0.772, Auc : 0.846, Sensitive_Loss : 0.16891, Sensitive_Acc : 21.677, Sensitive_Auc : 0.999, Mean auc: 0.846, Run Time : 145.24 sec
