Running on desktop22:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_val.csv",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.01,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-12 00:43:42, Train, Epoch : 1, Step : 10, Loss : 0.75488, Acc : 0.591, Sensitive_Loss : 1.15950, Sensitive_Acc : 7.800, Run Time : 9.49 sec
INFO:root:2024-04-12 00:43:49, Train, Epoch : 1, Step : 20, Loss : 0.74153, Acc : 0.575, Sensitive_Loss : 1.09765, Sensitive_Acc : 16.000, Run Time : 7.50 sec
INFO:root:2024-04-12 00:43:56, Train, Epoch : 1, Step : 30, Loss : 0.68791, Acc : 0.631, Sensitive_Loss : 1.11389, Sensitive_Acc : 17.400, Run Time : 7.08 sec
INFO:root:2024-04-12 00:44:04, Train, Epoch : 1, Step : 40, Loss : 0.63110, Acc : 0.669, Sensitive_Loss : 1.01001, Sensitive_Acc : 14.800, Run Time : 7.47 sec
INFO:root:2024-04-12 00:44:11, Train, Epoch : 1, Step : 50, Loss : 0.69198, Acc : 0.628, Sensitive_Loss : 1.05088, Sensitive_Acc : 17.100, Run Time : 7.34 sec
INFO:root:2024-04-12 00:44:18, Train, Epoch : 1, Step : 60, Loss : 0.64230, Acc : 0.675, Sensitive_Loss : 1.00012, Sensitive_Acc : 21.100, Run Time : 6.96 sec
INFO:root:2024-04-12 00:44:26, Train, Epoch : 1, Step : 70, Loss : 0.62894, Acc : 0.647, Sensitive_Loss : 1.05851, Sensitive_Acc : 22.800, Run Time : 7.41 sec
INFO:root:2024-04-12 00:44:33, Train, Epoch : 1, Step : 80, Loss : 0.61182, Acc : 0.637, Sensitive_Loss : 0.94872, Sensitive_Acc : 15.600, Run Time : 7.21 sec
INFO:root:2024-04-12 00:44:40, Train, Epoch : 1, Step : 90, Loss : 0.62076, Acc : 0.662, Sensitive_Loss : 1.00808, Sensitive_Acc : 16.100, Run Time : 7.13 sec
INFO:root:2024-04-12 00:44:47, Train, Epoch : 1, Step : 100, Loss : 0.62917, Acc : 0.656, Sensitive_Loss : 0.98145, Sensitive_Acc : 19.700, Run Time : 7.40 sec
INFO:root:2024-04-12 00:46:17, Dev, Step : 100, Loss : 0.74349, Acc : 0.589, Auc : 0.747, Sensitive_Loss : 0.97090, Sensitive_Acc : 21.722, Sensitive_Auc : 0.740, Mean auc: 0.747, Run Time : 89.80 sec
INFO:root:2024-04-12 00:46:18, Best, Step : 100, Loss : 0.74349, Acc : 0.589, Auc : 0.747, Sensitive_Loss : 0.97090, Sensitive_Acc : 21.722, Sensitive_Auc : 0.740, Best Auc : 0.747
INFO:root:2024-04-12 00:46:23, Train, Epoch : 1, Step : 110, Loss : 0.57781, Acc : 0.694, Sensitive_Loss : 0.94475, Sensitive_Acc : 20.700, Run Time : 95.88 sec
INFO:root:2024-04-12 00:46:31, Train, Epoch : 1, Step : 120, Loss : 0.58744, Acc : 0.688, Sensitive_Loss : 0.87314, Sensitive_Acc : 21.200, Run Time : 7.53 sec
INFO:root:2024-04-12 00:46:38, Train, Epoch : 1, Step : 130, Loss : 0.58529, Acc : 0.669, Sensitive_Loss : 0.82875, Sensitive_Acc : 19.700, Run Time : 6.83 sec
INFO:root:2024-04-12 00:46:45, Train, Epoch : 1, Step : 140, Loss : 0.73462, Acc : 0.628, Sensitive_Loss : 0.93956, Sensitive_Acc : 11.200, Run Time : 7.13 sec
INFO:root:2024-04-12 00:46:52, Train, Epoch : 1, Step : 150, Loss : 0.66007, Acc : 0.637, Sensitive_Loss : 0.90864, Sensitive_Acc : 22.800, Run Time : 6.87 sec
INFO:root:2024-04-12 00:46:59, Train, Epoch : 1, Step : 160, Loss : 0.62085, Acc : 0.688, Sensitive_Loss : 0.92578, Sensitive_Acc : 15.700, Run Time : 7.37 sec
INFO:root:2024-04-12 00:47:06, Train, Epoch : 1, Step : 170, Loss : 0.60208, Acc : 0.697, Sensitive_Loss : 0.80790, Sensitive_Acc : 19.600, Run Time : 7.04 sec
INFO:root:2024-04-12 00:47:13, Train, Epoch : 1, Step : 180, Loss : 0.67077, Acc : 0.637, Sensitive_Loss : 0.84377, Sensitive_Acc : 22.100, Run Time : 7.38 sec
INFO:root:2024-04-12 00:47:21, Train, Epoch : 1, Step : 190, Loss : 0.62003, Acc : 0.706, Sensitive_Loss : 0.77576, Sensitive_Acc : 24.100, Run Time : 7.49 sec
INFO:root:2024-04-12 00:47:28, Train, Epoch : 1, Step : 200, Loss : 0.59075, Acc : 0.669, Sensitive_Loss : 0.70407, Sensitive_Acc : 21.100, Run Time : 7.05 sec
INFO:root:2024-04-12 00:48:56, Dev, Step : 200, Loss : 0.63993, Acc : 0.653, Auc : 0.765, Sensitive_Loss : 0.88739, Sensitive_Acc : 13.180, Sensitive_Auc : 0.865, Mean auc: 0.765, Run Time : 88.25 sec
INFO:root:2024-04-12 00:48:57, Best, Step : 200, Loss : 0.63993, Acc : 0.653, Auc : 0.765, Sensitive_Loss : 0.88739, Sensitive_Acc : 13.180, Sensitive_Auc : 0.865, Best Auc : 0.765
INFO:root:2024-04-12 00:49:03, Train, Epoch : 1, Step : 210, Loss : 0.59201, Acc : 0.713, Sensitive_Loss : 0.72191, Sensitive_Acc : 15.800, Run Time : 94.99 sec
INFO:root:2024-04-12 00:49:11, Train, Epoch : 1, Step : 220, Loss : 0.58044, Acc : 0.700, Sensitive_Loss : 0.70008, Sensitive_Acc : 18.000, Run Time : 8.52 sec
INFO:root:2024-04-12 00:49:20, Train, Epoch : 1, Step : 230, Loss : 0.54438, Acc : 0.716, Sensitive_Loss : 0.72302, Sensitive_Acc : 23.400, Run Time : 8.37 sec
INFO:root:2024-04-12 00:49:28, Train, Epoch : 1, Step : 240, Loss : 0.63531, Acc : 0.644, Sensitive_Loss : 0.69573, Sensitive_Acc : 23.000, Run Time : 8.05 sec
INFO:root:2024-04-12 00:49:36, Train, Epoch : 1, Step : 250, Loss : 0.65930, Acc : 0.637, Sensitive_Loss : 0.64865, Sensitive_Acc : 19.200, Run Time : 8.36 sec
INFO:root:2024-04-12 00:49:44, Train, Epoch : 1, Step : 260, Loss : 0.61924, Acc : 0.656, Sensitive_Loss : 0.62412, Sensitive_Acc : 22.400, Run Time : 8.12 sec
INFO:root:2024-04-12 00:49:53, Train, Epoch : 1, Step : 270, Loss : 0.57612, Acc : 0.688, Sensitive_Loss : 0.72142, Sensitive_Acc : 21.100, Run Time : 8.44 sec
INFO:root:2024-04-12 00:50:01, Train, Epoch : 1, Step : 280, Loss : 0.55904, Acc : 0.709, Sensitive_Loss : 0.66316, Sensitive_Acc : 18.300, Run Time : 8.46 sec
INFO:root:2024-04-12 00:50:09, Train, Epoch : 1, Step : 290, Loss : 0.64064, Acc : 0.703, Sensitive_Loss : 0.56794, Sensitive_Acc : 22.700, Run Time : 8.14 sec
INFO:root:2024-04-12 00:50:18, Train, Epoch : 1, Step : 300, Loss : 0.61499, Acc : 0.725, Sensitive_Loss : 0.55755, Sensitive_Acc : 17.800, Run Time : 8.10 sec
INFO:root:2024-04-12 00:51:45, Dev, Step : 300, Loss : 0.61222, Acc : 0.696, Auc : 0.765, Sensitive_Loss : 0.50477, Sensitive_Acc : 18.684, Sensitive_Auc : 0.957, Mean auc: 0.765, Run Time : 87.88 sec
INFO:root:2024-04-12 00:51:46, Best, Step : 300, Loss : 0.61222, Acc : 0.696, Auc : 0.765, Sensitive_Loss : 0.50477, Sensitive_Acc : 18.684, Sensitive_Auc : 0.957, Best Auc : 0.765
INFO:root:2024-04-12 00:51:52, Train, Epoch : 1, Step : 310, Loss : 0.67398, Acc : 0.634, Sensitive_Loss : 0.53167, Sensitive_Acc : 25.000, Run Time : 94.49 sec
INFO:root:2024-04-12 00:52:00, Train, Epoch : 1, Step : 320, Loss : 0.58448, Acc : 0.703, Sensitive_Loss : 0.44336, Sensitive_Acc : 22.900, Run Time : 8.38 sec
INFO:root:2024-04-12 00:52:09, Train, Epoch : 1, Step : 330, Loss : 0.58882, Acc : 0.706, Sensitive_Loss : 0.49871, Sensitive_Acc : 21.800, Run Time : 8.77 sec
INFO:root:2024-04-12 00:52:17, Train, Epoch : 1, Step : 340, Loss : 0.54670, Acc : 0.700, Sensitive_Loss : 0.55926, Sensitive_Acc : 21.900, Run Time : 8.14 sec
INFO:root:2024-04-12 00:52:26, Train, Epoch : 1, Step : 350, Loss : 0.54533, Acc : 0.697, Sensitive_Loss : 0.51761, Sensitive_Acc : 22.000, Run Time : 8.40 sec
INFO:root:2024-04-12 00:52:34, Train, Epoch : 1, Step : 360, Loss : 0.56645, Acc : 0.703, Sensitive_Loss : 0.53399, Sensitive_Acc : 20.300, Run Time : 8.29 sec
INFO:root:2024-04-12 00:52:42, Train, Epoch : 1, Step : 370, Loss : 0.68405, Acc : 0.684, Sensitive_Loss : 0.53854, Sensitive_Acc : 17.600, Run Time : 8.25 sec
INFO:root:2024-04-12 00:52:51, Train, Epoch : 1, Step : 380, Loss : 0.63350, Acc : 0.666, Sensitive_Loss : 0.53585, Sensitive_Acc : 18.900, Run Time : 8.29 sec
INFO:root:2024-04-12 00:52:59, Train, Epoch : 1, Step : 390, Loss : 0.69890, Acc : 0.656, Sensitive_Loss : 0.39958, Sensitive_Acc : 21.100, Run Time : 8.44 sec
INFO:root:2024-04-12 00:53:07, Train, Epoch : 1, Step : 400, Loss : 0.57563, Acc : 0.697, Sensitive_Loss : 0.42050, Sensitive_Acc : 22.400, Run Time : 7.86 sec
INFO:root:2024-04-12 00:54:34, Dev, Step : 400, Loss : 0.64200, Acc : 0.673, Auc : 0.765, Sensitive_Loss : 0.62038, Sensitive_Acc : 17.256, Sensitive_Auc : 0.958, Mean auc: 0.765, Run Time : 87.41 sec
INFO:root:2024-04-12 00:54:40, Train, Epoch : 1, Step : 410, Loss : 0.59373, Acc : 0.700, Sensitive_Loss : 0.51508, Sensitive_Acc : 22.900, Run Time : 93.16 sec
INFO:root:2024-04-12 00:54:48, Train, Epoch : 1, Step : 420, Loss : 0.57713, Acc : 0.703, Sensitive_Loss : 0.42376, Sensitive_Acc : 20.400, Run Time : 8.11 sec
INFO:root:2024-04-12 00:54:56, Train, Epoch : 1, Step : 430, Loss : 0.58496, Acc : 0.700, Sensitive_Loss : 0.38952, Sensitive_Acc : 22.600, Run Time : 8.32 sec
INFO:root:2024-04-12 00:55:05, Train, Epoch : 1, Step : 440, Loss : 0.60871, Acc : 0.688, Sensitive_Loss : 0.51037, Sensitive_Acc : 13.900, Run Time : 8.45 sec
INFO:root:2024-04-12 00:55:13, Train, Epoch : 1, Step : 450, Loss : 0.65497, Acc : 0.666, Sensitive_Loss : 0.39292, Sensitive_Acc : 19.900, Run Time : 8.37 sec
INFO:root:2024-04-12 00:55:21, Train, Epoch : 1, Step : 460, Loss : 0.57205, Acc : 0.688, Sensitive_Loss : 0.35196, Sensitive_Acc : 18.300, Run Time : 7.94 sec
INFO:root:2024-04-12 00:55:29, Train, Epoch : 1, Step : 470, Loss : 0.55162, Acc : 0.725, Sensitive_Loss : 0.55263, Sensitive_Acc : 21.600, Run Time : 7.75 sec
INFO:root:2024-04-12 00:55:37, Train, Epoch : 1, Step : 480, Loss : 0.62411, Acc : 0.672, Sensitive_Loss : 0.33660, Sensitive_Acc : 20.600, Run Time : 7.97 sec
INFO:root:2024-04-12 00:55:46, Train, Epoch : 1, Step : 490, Loss : 0.57633, Acc : 0.719, Sensitive_Loss : 0.39255, Sensitive_Acc : 25.500, Run Time : 8.60 sec
INFO:root:2024-04-12 00:55:53, Train, Epoch : 1, Step : 500, Loss : 0.56122, Acc : 0.709, Sensitive_Loss : 0.52124, Sensitive_Acc : 25.300, Run Time : 7.96 sec
INFO:root:2024-04-12 00:57:23, Dev, Step : 500, Loss : 0.62191, Acc : 0.720, Auc : 0.800, Sensitive_Loss : 0.66624, Sensitive_Acc : 17.271, Sensitive_Auc : 0.963, Mean auc: 0.800, Run Time : 89.36 sec
INFO:root:2024-04-12 00:57:23, Best, Step : 500, Loss : 0.62191, Acc : 0.720, Auc : 0.800, Sensitive_Loss : 0.66624, Sensitive_Acc : 17.271, Sensitive_Auc : 0.963, Best Auc : 0.800
INFO:root:2024-04-12 00:57:30, Train, Epoch : 1, Step : 510, Loss : 0.58700, Acc : 0.700, Sensitive_Loss : 0.44421, Sensitive_Acc : 18.000, Run Time : 96.82 sec
INFO:root:2024-04-12 00:57:41, Train, Epoch : 1, Step : 520, Loss : 0.51395, Acc : 0.744, Sensitive_Loss : 0.27992, Sensitive_Acc : 20.800, Run Time : 10.30 sec
INFO:root:2024-04-12 00:57:50, Train, Epoch : 1, Step : 530, Loss : 0.50706, Acc : 0.775, Sensitive_Loss : 0.32301, Sensitive_Acc : 22.500, Run Time : 9.53 sec
INFO:root:2024-04-12 00:58:01, Train, Epoch : 1, Step : 540, Loss : 0.57543, Acc : 0.759, Sensitive_Loss : 0.45060, Sensitive_Acc : 21.900, Run Time : 11.06 sec
INFO:root:2024-04-12 00:58:11, Train, Epoch : 1, Step : 550, Loss : 0.61248, Acc : 0.672, Sensitive_Loss : 0.48694, Sensitive_Acc : 24.700, Run Time : 9.94 sec
INFO:root:2024-04-12 00:58:21, Train, Epoch : 1, Step : 560, Loss : 0.61357, Acc : 0.728, Sensitive_Loss : 0.40012, Sensitive_Acc : 21.200, Run Time : 9.77 sec
INFO:root:2024-04-12 00:58:30, Train, Epoch : 1, Step : 570, Loss : 0.66728, Acc : 0.681, Sensitive_Loss : 0.36447, Sensitive_Acc : 17.800, Run Time : 9.33 sec
INFO:root:2024-04-12 00:58:40, Train, Epoch : 1, Step : 580, Loss : 0.56849, Acc : 0.709, Sensitive_Loss : 0.54697, Sensitive_Acc : 21.600, Run Time : 9.63 sec
INFO:root:2024-04-12 00:58:49, Train, Epoch : 1, Step : 590, Loss : 0.57104, Acc : 0.697, Sensitive_Loss : 0.49713, Sensitive_Acc : 21.700, Run Time : 9.60 sec
INFO:root:2024-04-12 00:58:59, Train, Epoch : 1, Step : 600, Loss : 0.57794, Acc : 0.728, Sensitive_Loss : 0.38264, Sensitive_Acc : 20.600, Run Time : 9.66 sec
INFO:root:2024-04-12 01:00:27, Dev, Step : 600, Loss : 0.60509, Acc : 0.717, Auc : 0.797, Sensitive_Loss : 0.39968, Sensitive_Acc : 20.669, Sensitive_Auc : 0.970, Mean auc: 0.797, Run Time : 87.90 sec
INFO:root:2024-04-12 01:00:34, Train, Epoch : 1, Step : 610, Loss : 0.57110, Acc : 0.694, Sensitive_Loss : 0.44544, Sensitive_Acc : 24.100, Run Time : 94.72 sec
INFO:root:2024-04-12 01:00:44, Train, Epoch : 1, Step : 620, Loss : 0.55371, Acc : 0.719, Sensitive_Loss : 0.42314, Sensitive_Acc : 19.800, Run Time : 10.00 sec
INFO:root:2024-04-12 01:00:53, Train, Epoch : 1, Step : 630, Loss : 0.65490, Acc : 0.681, Sensitive_Loss : 0.29170, Sensitive_Acc : 23.200, Run Time : 9.56 sec
INFO:root:2024-04-12 01:02:24
INFO:root:y_pred: [0.2228068  0.14283179 0.25393784 ... 0.3401088  0.23600343 0.20222089]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [9.17343497e-02 1.54582039e-02 4.20950241e-02 4.08901662e-01
 2.00097580e-02 2.84095127e-02 1.03457987e-01 1.13785565e-02
 1.64370671e-01 9.92316365e-01 3.71943295e-01 2.04144380e-05
 1.82061777e-01 8.43359157e-03 9.91692543e-01 7.08409250e-02
 1.60835773e-01 9.90874767e-01 9.60372388e-01 8.77949037e-03
 9.13206816e-01 1.02785346e-03 2.69944668e-01 1.79295123e-01
 4.16377522e-02 5.64879060e-01 1.72412340e-02 2.24583503e-02
 2.77231485e-02 1.37793377e-01 9.85085964e-02 9.48344827e-01
 2.72621185e-01 8.82438421e-01 3.43398526e-02 5.36722392e-02
 1.54578034e-02 7.91720748e-02 2.12335512e-01 2.79928118e-01
 3.30056697e-01 9.25518274e-01 4.90455627e-02 3.29032689e-01
 9.24313605e-01 9.11365271e-01 3.87189299e-01 6.96513951e-01
 3.61877531e-01 9.69926059e-01 4.85563964e-01 9.94113982e-01
 9.87504542e-01 5.08212857e-02 1.91954777e-01 1.91423014e-01
 2.45858822e-03 2.22094417e-01 9.44381893e-01 6.40920699e-02
 2.50637298e-04 1.39213102e-02 2.49829818e-03 7.83047144e-05
 9.83311653e-01 1.81626324e-02 2.83551728e-03 4.15135086e-01
 1.79105446e-01 8.74855757e-01 9.82014775e-01 9.93227780e-01
 1.50863184e-02 1.76767886e-01 6.59290031e-02 7.17507958e-01
 7.21578719e-03 7.09538348e-03 1.27431909e-02 2.24731341e-01
 2.91907072e-01 2.25991905e-01 9.77164090e-01 9.59526300e-01
 6.48755908e-01 3.58488500e-01 5.48083961e-01 3.55215818e-01
 3.68151872e-04 1.03008363e-03 4.29938734e-02 3.27512324e-01
 6.88446173e-03 2.16373301e-05 6.67157099e-02 8.20773095e-02
 1.37291921e-04 5.92578173e-01 3.11795454e-02 5.37353307e-02
 1.01143576e-01 1.53546229e-01 2.10651338e-01 3.74836922e-02
 1.69230476e-01 1.20579423e-02 1.27971113e-01 6.07761741e-01
 9.29245651e-01 9.46380317e-01 1.19175890e-03 9.96498704e-01
 9.86575544e-01 1.76121900e-03 4.75471586e-01 8.49742174e-01
 7.32439637e-01 2.57762871e-03 1.20363254e-02 3.31833400e-02
 1.06693819e-01 1.51400478e-03 1.69881180e-01 2.04043686e-02
 2.65637524e-02 9.35334861e-01 8.08801595e-03 9.67426479e-01
 1.14508092e-01 4.64270592e-01 1.09103005e-02 7.05123007e-01
 1.00755751e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 01:02:24, Dev, Step : 634, Loss : 0.57209, Acc : 0.733, Auc : 0.801, Sensitive_Loss : 0.41449, Sensitive_Acc : 20.549, Sensitive_Auc : 0.984, Mean auc: 0.801, Run Time : 86.95 sec
INFO:root:2024-04-12 01:02:24, Best, Step : 634, Loss : 0.57209, Acc : 0.733,Auc : 0.801, Best Auc : 0.801, Sensitive_Loss : 0.41449, Sensitive_Acc : 20.549, Sensitive_Auc : 0.984
INFO:root:2024-04-12 01:02:31, Train, Epoch : 2, Step : 640, Loss : 0.28338, Acc : 0.466, Sensitive_Loss : 0.16736, Sensitive_Acc : 11.100, Run Time : 5.52 sec
INFO:root:2024-04-12 01:02:38, Train, Epoch : 2, Step : 650, Loss : 0.50879, Acc : 0.762, Sensitive_Loss : 0.26526, Sensitive_Acc : 22.500, Run Time : 6.98 sec
INFO:root:2024-04-12 01:02:45, Train, Epoch : 2, Step : 660, Loss : 0.62522, Acc : 0.703, Sensitive_Loss : 0.30277, Sensitive_Acc : 23.800, Run Time : 7.01 sec
INFO:root:2024-04-12 01:02:52, Train, Epoch : 2, Step : 670, Loss : 0.53774, Acc : 0.747, Sensitive_Loss : 0.43087, Sensitive_Acc : 23.300, Run Time : 7.01 sec
INFO:root:2024-04-12 01:02:59, Train, Epoch : 2, Step : 680, Loss : 0.56706, Acc : 0.734, Sensitive_Loss : 0.31512, Sensitive_Acc : 22.500, Run Time : 7.41 sec
INFO:root:2024-04-12 01:03:06, Train, Epoch : 2, Step : 690, Loss : 0.55944, Acc : 0.728, Sensitive_Loss : 0.37218, Sensitive_Acc : 17.500, Run Time : 6.85 sec
INFO:root:2024-04-12 01:03:14, Train, Epoch : 2, Step : 700, Loss : 0.52798, Acc : 0.756, Sensitive_Loss : 0.33118, Sensitive_Acc : 22.800, Run Time : 7.60 sec
INFO:root:2024-04-12 01:04:42, Dev, Step : 700, Loss : 0.57951, Acc : 0.724, Auc : 0.796, Sensitive_Loss : 0.36359, Sensitive_Acc : 20.774, Sensitive_Auc : 0.986, Mean auc: 0.796, Run Time : 88.33 sec
INFO:root:2024-04-12 01:04:48, Train, Epoch : 2, Step : 710, Loss : 0.53020, Acc : 0.744, Sensitive_Loss : 0.33010, Sensitive_Acc : 20.500, Run Time : 94.10 sec
INFO:root:2024-04-12 01:04:56, Train, Epoch : 2, Step : 720, Loss : 0.47867, Acc : 0.766, Sensitive_Loss : 0.37344, Sensitive_Acc : 15.500, Run Time : 7.67 sec
INFO:root:2024-04-12 01:05:03, Train, Epoch : 2, Step : 730, Loss : 0.54090, Acc : 0.762, Sensitive_Loss : 0.30776, Sensitive_Acc : 17.300, Run Time : 7.21 sec
INFO:root:2024-04-12 01:05:11, Train, Epoch : 2, Step : 740, Loss : 0.54274, Acc : 0.744, Sensitive_Loss : 0.36325, Sensitive_Acc : 19.700, Run Time : 8.14 sec
INFO:root:2024-04-12 01:05:18, Train, Epoch : 2, Step : 750, Loss : 0.52123, Acc : 0.775, Sensitive_Loss : 0.37046, Sensitive_Acc : 21.100, Run Time : 7.51 sec
INFO:root:2024-04-12 01:05:26, Train, Epoch : 2, Step : 760, Loss : 0.50980, Acc : 0.741, Sensitive_Loss : 0.26928, Sensitive_Acc : 25.500, Run Time : 7.72 sec
INFO:root:2024-04-12 01:05:34, Train, Epoch : 2, Step : 770, Loss : 0.53516, Acc : 0.787, Sensitive_Loss : 0.33530, Sensitive_Acc : 18.400, Run Time : 7.56 sec
INFO:root:2024-04-12 01:05:42, Train, Epoch : 2, Step : 780, Loss : 0.53907, Acc : 0.753, Sensitive_Loss : 0.33959, Sensitive_Acc : 18.700, Run Time : 7.84 sec
INFO:root:2024-04-12 01:05:49, Train, Epoch : 2, Step : 790, Loss : 0.43919, Acc : 0.753, Sensitive_Loss : 0.24715, Sensitive_Acc : 20.600, Run Time : 7.59 sec
INFO:root:2024-04-12 01:05:57, Train, Epoch : 2, Step : 800, Loss : 0.57913, Acc : 0.684, Sensitive_Loss : 0.36057, Sensitive_Acc : 22.400, Run Time : 7.64 sec
INFO:root:2024-04-12 01:07:25, Dev, Step : 800, Loss : 0.61084, Acc : 0.718, Auc : 0.801, Sensitive_Loss : 0.37664, Sensitive_Acc : 19.887, Sensitive_Auc : 0.993, Mean auc: 0.801, Run Time : 88.27 sec
INFO:root:2024-04-12 01:07:31, Train, Epoch : 2, Step : 810, Loss : 0.52035, Acc : 0.728, Sensitive_Loss : 0.39690, Sensitive_Acc : 23.900, Run Time : 93.93 sec
INFO:root:2024-04-12 01:07:39, Train, Epoch : 2, Step : 820, Loss : 0.66630, Acc : 0.688, Sensitive_Loss : 0.37778, Sensitive_Acc : 22.900, Run Time : 7.90 sec
INFO:root:2024-04-12 01:07:46, Train, Epoch : 2, Step : 830, Loss : 0.60355, Acc : 0.734, Sensitive_Loss : 0.27814, Sensitive_Acc : 24.800, Run Time : 7.66 sec
INFO:root:2024-04-12 01:07:54, Train, Epoch : 2, Step : 840, Loss : 0.56872, Acc : 0.725, Sensitive_Loss : 0.35990, Sensitive_Acc : 22.000, Run Time : 7.75 sec
INFO:root:2024-04-12 01:08:01, Train, Epoch : 2, Step : 850, Loss : 0.58005, Acc : 0.703, Sensitive_Loss : 0.28511, Sensitive_Acc : 20.900, Run Time : 7.48 sec
INFO:root:2024-04-12 01:08:10, Train, Epoch : 2, Step : 860, Loss : 0.45288, Acc : 0.756, Sensitive_Loss : 0.36056, Sensitive_Acc : 19.500, Run Time : 8.06 sec
INFO:root:2024-04-12 01:08:17, Train, Epoch : 2, Step : 870, Loss : 0.50279, Acc : 0.769, Sensitive_Loss : 0.26044, Sensitive_Acc : 15.900, Run Time : 7.01 sec
INFO:root:2024-04-12 01:08:24, Train, Epoch : 2, Step : 880, Loss : 0.57396, Acc : 0.728, Sensitive_Loss : 0.35636, Sensitive_Acc : 22.900, Run Time : 7.52 sec
INFO:root:2024-04-12 01:08:31, Train, Epoch : 2, Step : 890, Loss : 0.54208, Acc : 0.719, Sensitive_Loss : 0.27964, Sensitive_Acc : 22.600, Run Time : 7.45 sec
INFO:root:2024-04-12 01:08:39, Train, Epoch : 2, Step : 900, Loss : 0.51176, Acc : 0.756, Sensitive_Loss : 0.29703, Sensitive_Acc : 19.500, Run Time : 7.42 sec
INFO:root:2024-04-12 01:10:07, Dev, Step : 900, Loss : 0.55531, Acc : 0.740, Auc : 0.818, Sensitive_Loss : 0.28922, Sensitive_Acc : 20.955, Sensitive_Auc : 0.992, Mean auc: 0.818, Run Time : 88.53 sec
INFO:root:2024-04-12 01:10:08, Best, Step : 900, Loss : 0.55531, Acc : 0.740, Auc : 0.818, Sensitive_Loss : 0.28922, Sensitive_Acc : 20.955, Sensitive_Auc : 0.992, Best Auc : 0.818
INFO:root:2024-04-12 01:10:14, Train, Epoch : 2, Step : 910, Loss : 0.54077, Acc : 0.744, Sensitive_Loss : 0.29487, Sensitive_Acc : 20.100, Run Time : 95.36 sec
INFO:root:2024-04-12 01:10:22, Train, Epoch : 2, Step : 920, Loss : 0.53221, Acc : 0.738, Sensitive_Loss : 0.30543, Sensitive_Acc : 14.600, Run Time : 7.93 sec
INFO:root:2024-04-12 01:10:30, Train, Epoch : 2, Step : 930, Loss : 0.57343, Acc : 0.762, Sensitive_Loss : 0.28824, Sensitive_Acc : 22.300, Run Time : 8.10 sec
INFO:root:2024-04-12 01:10:38, Train, Epoch : 2, Step : 940, Loss : 0.49564, Acc : 0.794, Sensitive_Loss : 0.32792, Sensitive_Acc : 24.600, Run Time : 7.43 sec
INFO:root:2024-04-12 01:10:46, Train, Epoch : 2, Step : 950, Loss : 0.53083, Acc : 0.725, Sensitive_Loss : 0.27075, Sensitive_Acc : 20.000, Run Time : 8.00 sec
INFO:root:2024-04-12 01:10:53, Train, Epoch : 2, Step : 960, Loss : 0.56213, Acc : 0.722, Sensitive_Loss : 0.28967, Sensitive_Acc : 17.600, Run Time : 7.46 sec
INFO:root:2024-04-12 01:11:01, Train, Epoch : 2, Step : 970, Loss : 0.53891, Acc : 0.756, Sensitive_Loss : 0.31748, Sensitive_Acc : 24.500, Run Time : 7.91 sec
INFO:root:2024-04-12 01:11:09, Train, Epoch : 2, Step : 980, Loss : 0.59650, Acc : 0.716, Sensitive_Loss : 0.29711, Sensitive_Acc : 23.400, Run Time : 7.46 sec
INFO:root:2024-04-12 01:11:16, Train, Epoch : 2, Step : 990, Loss : 0.53159, Acc : 0.709, Sensitive_Loss : 0.22699, Sensitive_Acc : 19.400, Run Time : 7.87 sec
INFO:root:2024-04-12 01:11:24, Train, Epoch : 2, Step : 1000, Loss : 0.53232, Acc : 0.772, Sensitive_Loss : 0.31024, Sensitive_Acc : 21.800, Run Time : 7.36 sec
INFO:root:2024-04-12 01:12:52, Dev, Step : 1000, Loss : 0.56966, Acc : 0.724, Auc : 0.817, Sensitive_Loss : 0.35559, Sensitive_Acc : 19.992, Sensitive_Auc : 0.986, Mean auc: 0.817, Run Time : 88.13 sec
INFO:root:2024-04-12 01:12:57, Train, Epoch : 2, Step : 1010, Loss : 0.55892, Acc : 0.719, Sensitive_Loss : 0.30994, Sensitive_Acc : 20.100, Run Time : 93.58 sec
INFO:root:2024-04-12 01:13:05, Train, Epoch : 2, Step : 1020, Loss : 0.57117, Acc : 0.762, Sensitive_Loss : 0.35779, Sensitive_Acc : 25.400, Run Time : 7.60 sec
INFO:root:2024-04-12 01:13:13, Train, Epoch : 2, Step : 1030, Loss : 0.49635, Acc : 0.769, Sensitive_Loss : 0.26999, Sensitive_Acc : 18.500, Run Time : 7.87 sec
INFO:root:2024-04-12 01:13:20, Train, Epoch : 2, Step : 1040, Loss : 0.50448, Acc : 0.744, Sensitive_Loss : 0.38926, Sensitive_Acc : 16.800, Run Time : 7.37 sec
INFO:root:2024-04-12 01:13:28, Train, Epoch : 2, Step : 1050, Loss : 0.57366, Acc : 0.744, Sensitive_Loss : 0.24272, Sensitive_Acc : 25.700, Run Time : 7.74 sec
INFO:root:2024-04-12 01:13:36, Train, Epoch : 2, Step : 1060, Loss : 0.57535, Acc : 0.756, Sensitive_Loss : 0.23184, Sensitive_Acc : 18.400, Run Time : 7.72 sec
INFO:root:2024-04-12 01:13:43, Train, Epoch : 2, Step : 1070, Loss : 0.48904, Acc : 0.719, Sensitive_Loss : 0.29039, Sensitive_Acc : 22.500, Run Time : 7.25 sec
INFO:root:2024-04-12 01:13:51, Train, Epoch : 2, Step : 1080, Loss : 0.56719, Acc : 0.734, Sensitive_Loss : 0.26101, Sensitive_Acc : 23.800, Run Time : 7.86 sec
INFO:root:2024-04-12 01:13:59, Train, Epoch : 2, Step : 1090, Loss : 0.64219, Acc : 0.747, Sensitive_Loss : 0.32720, Sensitive_Acc : 20.500, Run Time : 8.24 sec
INFO:root:2024-04-12 01:14:08, Train, Epoch : 2, Step : 1100, Loss : 0.62421, Acc : 0.722, Sensitive_Loss : 0.29630, Sensitive_Acc : 23.000, Run Time : 8.90 sec
INFO:root:2024-04-12 01:15:36, Dev, Step : 1100, Loss : 0.55289, Acc : 0.746, Auc : 0.819, Sensitive_Loss : 0.40625, Sensitive_Acc : 19.226, Sensitive_Auc : 0.990, Mean auc: 0.819, Run Time : 88.38 sec
INFO:root:2024-04-12 01:15:37, Best, Step : 1100, Loss : 0.55289, Acc : 0.746, Auc : 0.819, Sensitive_Loss : 0.40625, Sensitive_Acc : 19.226, Sensitive_Auc : 0.990, Best Auc : 0.819
INFO:root:2024-04-12 01:15:43, Train, Epoch : 2, Step : 1110, Loss : 0.54101, Acc : 0.709, Sensitive_Loss : 0.30791, Sensitive_Acc : 18.900, Run Time : 94.79 sec
INFO:root:2024-04-12 01:15:51, Train, Epoch : 2, Step : 1120, Loss : 0.55192, Acc : 0.747, Sensitive_Loss : 0.21039, Sensitive_Acc : 22.300, Run Time : 8.12 sec
INFO:root:2024-04-12 01:16:01, Train, Epoch : 2, Step : 1130, Loss : 0.49055, Acc : 0.744, Sensitive_Loss : 0.22369, Sensitive_Acc : 19.900, Run Time : 10.02 sec
INFO:root:2024-04-12 01:16:11, Train, Epoch : 2, Step : 1140, Loss : 0.58307, Acc : 0.731, Sensitive_Loss : 0.22792, Sensitive_Acc : 16.200, Run Time : 9.67 sec
INFO:root:2024-04-12 01:16:19, Train, Epoch : 2, Step : 1150, Loss : 0.55173, Acc : 0.681, Sensitive_Loss : 0.25912, Sensitive_Acc : 19.200, Run Time : 8.71 sec
INFO:root:2024-04-12 01:16:28, Train, Epoch : 2, Step : 1160, Loss : 0.58163, Acc : 0.722, Sensitive_Loss : 0.23919, Sensitive_Acc : 21.200, Run Time : 8.44 sec
INFO:root:2024-04-12 01:16:36, Train, Epoch : 2, Step : 1170, Loss : 0.52163, Acc : 0.741, Sensitive_Loss : 0.21212, Sensitive_Acc : 20.100, Run Time : 8.52 sec
INFO:root:2024-04-12 01:16:45, Train, Epoch : 2, Step : 1180, Loss : 0.52069, Acc : 0.734, Sensitive_Loss : 0.25997, Sensitive_Acc : 20.100, Run Time : 8.83 sec
INFO:root:2024-04-12 01:16:53, Train, Epoch : 2, Step : 1190, Loss : 0.55754, Acc : 0.709, Sensitive_Loss : 0.28837, Sensitive_Acc : 23.600, Run Time : 8.41 sec
INFO:root:2024-04-12 01:17:02, Train, Epoch : 2, Step : 1200, Loss : 0.62962, Acc : 0.694, Sensitive_Loss : 0.26096, Sensitive_Acc : 21.700, Run Time : 8.85 sec
INFO:root:2024-04-12 01:18:30, Dev, Step : 1200, Loss : 0.56273, Acc : 0.737, Auc : 0.815, Sensitive_Loss : 0.29483, Sensitive_Acc : 20.534, Sensitive_Auc : 0.990, Mean auc: 0.815, Run Time : 88.09 sec
INFO:root:2024-04-12 01:18:36, Train, Epoch : 2, Step : 1210, Loss : 0.49499, Acc : 0.734, Sensitive_Loss : 0.22819, Sensitive_Acc : 26.400, Run Time : 94.01 sec
INFO:root:2024-04-12 01:18:45, Train, Epoch : 2, Step : 1220, Loss : 0.57806, Acc : 0.759, Sensitive_Loss : 0.18357, Sensitive_Acc : 20.900, Run Time : 8.73 sec
INFO:root:2024-04-12 01:18:53, Train, Epoch : 2, Step : 1230, Loss : 0.50845, Acc : 0.722, Sensitive_Loss : 0.29695, Sensitive_Acc : 21.800, Run Time : 8.34 sec
INFO:root:2024-04-12 01:19:02, Train, Epoch : 2, Step : 1240, Loss : 0.50228, Acc : 0.762, Sensitive_Loss : 0.25039, Sensitive_Acc : 22.200, Run Time : 8.30 sec
INFO:root:2024-04-12 01:19:10, Train, Epoch : 2, Step : 1250, Loss : 0.50010, Acc : 0.728, Sensitive_Loss : 0.23837, Sensitive_Acc : 23.100, Run Time : 8.61 sec
INFO:root:2024-04-12 01:19:19, Train, Epoch : 2, Step : 1260, Loss : 0.54240, Acc : 0.772, Sensitive_Loss : 0.30287, Sensitive_Acc : 14.600, Run Time : 8.81 sec
INFO:root:2024-04-12 01:20:52
INFO:root:y_pred: [0.07620677 0.06912628 0.22344676 ... 0.452427   0.10756521 0.10020942]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.72802038e-03 1.69488844e-02 3.69258933e-02 1.80473328e-01
 1.77612007e-01 7.38292886e-03 8.90767761e-03 6.17370708e-04
 1.64730493e-02 9.99377191e-01 2.38362834e-01 1.57440169e-04
 1.47620598e-02 4.15976538e-05 9.99022603e-01 2.31725890e-02
 4.68991045e-03 9.96415734e-01 9.82500255e-01 4.70165769e-03
 8.46120000e-01 1.78040966e-04 2.34202057e-01 9.71918404e-02
 5.24018183e-02 3.18216503e-01 1.81551324e-04 1.91304449e-03
 4.49428742e-04 2.04592068e-02 2.97040362e-02 4.27642852e-01
 4.66929330e-03 7.78955758e-01 1.97647125e-04 1.90444908e-03
 1.23508961e-03 6.53061494e-02 2.19442293e-01 3.25356163e-02
 8.43307152e-02 7.79493093e-01 3.72950546e-02 9.63713566e-04
 9.54245687e-01 6.32146120e-01 1.18762277e-01 3.11814487e-01
 3.48201007e-01 9.81863618e-01 6.80618286e-01 9.94950891e-01
 9.53739464e-01 4.46011079e-03 8.21199641e-03 2.29706556e-01
 2.67953686e-02 6.15372416e-03 9.48970735e-01 4.34635580e-02
 9.80408979e-04 1.18960058e-02 6.93641603e-02 1.38519999e-05
 9.62399364e-01 5.04757604e-03 1.49547595e-05 2.80683339e-01
 6.56448258e-03 9.39160705e-01 9.98028815e-01 9.95698094e-01
 2.34633125e-03 2.67950773e-01 1.61758985e-03 4.66903150e-01
 9.85214561e-02 2.52508878e-04 5.53630665e-03 3.11813637e-04
 1.68428291e-02 1.35241461e-03 9.37773407e-01 9.33013439e-01
 3.00718158e-01 4.19670623e-03 9.41142440e-02 2.69506732e-03
 1.67769622e-02 4.63044780e-05 6.69942237e-03 6.56352043e-01
 9.87527383e-05 2.67879426e-04 9.81509872e-03 1.80800147e-02
 7.14700320e-04 4.45272177e-01 1.08926371e-01 1.14140762e-02
 3.30710337e-02 5.94290085e-02 3.44163626e-02 1.34562713e-03
 4.26601134e-02 2.41349987e-03 2.79646814e-02 4.55841660e-01
 9.33478773e-01 3.83217603e-01 7.43129349e-04 9.99807537e-01
 9.93914306e-01 4.93086236e-06 3.44639182e-01 1.92460582e-01
 4.34102595e-01 6.44291402e-04 3.59841366e-03 1.23861153e-03
 6.09272197e-02 2.75433471e-04 1.67564042e-02 2.51486083e-04
 1.48988105e-02 9.54297304e-01 5.86534843e-05 9.84747469e-01
 9.98147950e-02 2.69545734e-01 8.81528016e-04 1.87257022e-01
 3.37774327e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 01:20:52, Dev, Step : 1268, Loss : 0.57439, Acc : 0.728, Auc : 0.801, Sensitive_Loss : 0.22958, Sensitive_Acc : 21.692, Sensitive_Auc : 0.989, Mean auc: 0.801, Run Time : 86.86 sec
INFO:root:2024-04-12 01:20:56, Train, Epoch : 3, Step : 1270, Loss : 0.10290, Acc : 0.156, Sensitive_Loss : 0.05321, Sensitive_Acc : 3.400, Run Time : 2.77 sec
INFO:root:2024-04-12 01:21:03, Train, Epoch : 3, Step : 1280, Loss : 0.57451, Acc : 0.697, Sensitive_Loss : 0.21466, Sensitive_Acc : 25.300, Run Time : 6.76 sec
INFO:root:2024-04-12 01:21:10, Train, Epoch : 3, Step : 1290, Loss : 0.61738, Acc : 0.731, Sensitive_Loss : 0.28323, Sensitive_Acc : 16.600, Run Time : 7.23 sec
INFO:root:2024-04-12 01:21:17, Train, Epoch : 3, Step : 1300, Loss : 0.46819, Acc : 0.794, Sensitive_Loss : 0.22876, Sensitive_Acc : 20.100, Run Time : 7.23 sec
INFO:root:2024-04-12 01:22:45, Dev, Step : 1300, Loss : 0.54809, Acc : 0.744, Auc : 0.823, Sensitive_Loss : 0.22329, Sensitive_Acc : 21.075, Sensitive_Auc : 0.994, Mean auc: 0.823, Run Time : 87.59 sec
INFO:root:2024-04-12 01:22:45, Best, Step : 1300, Loss : 0.54809, Acc : 0.744, Auc : 0.823, Sensitive_Loss : 0.22329, Sensitive_Acc : 21.075, Sensitive_Auc : 0.994, Best Auc : 0.823
INFO:root:2024-04-12 01:22:51, Train, Epoch : 3, Step : 1310, Loss : 0.53589, Acc : 0.738, Sensitive_Loss : 0.19761, Sensitive_Acc : 23.200, Run Time : 93.94 sec
INFO:root:2024-04-12 01:22:59, Train, Epoch : 3, Step : 1320, Loss : 0.49097, Acc : 0.775, Sensitive_Loss : 0.29454, Sensitive_Acc : 19.100, Run Time : 7.43 sec
INFO:root:2024-04-12 01:23:06, Train, Epoch : 3, Step : 1330, Loss : 0.49479, Acc : 0.791, Sensitive_Loss : 0.20726, Sensitive_Acc : 27.000, Run Time : 7.37 sec
INFO:root:2024-04-12 01:23:13, Train, Epoch : 3, Step : 1340, Loss : 0.50994, Acc : 0.753, Sensitive_Loss : 0.18837, Sensitive_Acc : 18.000, Run Time : 7.04 sec
INFO:root:2024-04-12 01:23:20, Train, Epoch : 3, Step : 1350, Loss : 0.45873, Acc : 0.772, Sensitive_Loss : 0.22453, Sensitive_Acc : 22.400, Run Time : 7.24 sec
INFO:root:2024-04-12 01:23:28, Train, Epoch : 3, Step : 1360, Loss : 0.51800, Acc : 0.766, Sensitive_Loss : 0.17184, Sensitive_Acc : 22.000, Run Time : 7.58 sec
INFO:root:2024-04-12 01:23:35, Train, Epoch : 3, Step : 1370, Loss : 0.46384, Acc : 0.784, Sensitive_Loss : 0.23166, Sensitive_Acc : 22.700, Run Time : 7.57 sec
INFO:root:2024-04-12 01:23:43, Train, Epoch : 3, Step : 1380, Loss : 0.48272, Acc : 0.762, Sensitive_Loss : 0.21249, Sensitive_Acc : 20.500, Run Time : 7.18 sec
INFO:root:2024-04-12 01:23:50, Train, Epoch : 3, Step : 1390, Loss : 0.48887, Acc : 0.791, Sensitive_Loss : 0.16612, Sensitive_Acc : 21.400, Run Time : 7.15 sec
INFO:root:2024-04-12 01:23:57, Train, Epoch : 3, Step : 1400, Loss : 0.46633, Acc : 0.781, Sensitive_Loss : 0.22200, Sensitive_Acc : 23.100, Run Time : 7.44 sec
INFO:root:2024-04-12 01:25:25, Dev, Step : 1400, Loss : 0.53058, Acc : 0.762, Auc : 0.839, Sensitive_Loss : 0.21444, Sensitive_Acc : 21.361, Sensitive_Auc : 0.995, Mean auc: 0.839, Run Time : 87.75 sec
INFO:root:2024-04-12 01:25:26, Best, Step : 1400, Loss : 0.53058, Acc : 0.762, Auc : 0.839, Sensitive_Loss : 0.21444, Sensitive_Acc : 21.361, Sensitive_Auc : 0.995, Best Auc : 0.839
INFO:root:2024-04-12 01:25:32, Train, Epoch : 3, Step : 1410, Loss : 0.51984, Acc : 0.753, Sensitive_Loss : 0.16254, Sensitive_Acc : 17.900, Run Time : 94.50 sec
INFO:root:2024-04-12 01:25:39, Train, Epoch : 3, Step : 1420, Loss : 0.48304, Acc : 0.762, Sensitive_Loss : 0.22825, Sensitive_Acc : 19.700, Run Time : 7.01 sec
INFO:root:2024-04-12 01:25:46, Train, Epoch : 3, Step : 1430, Loss : 0.44551, Acc : 0.762, Sensitive_Loss : 0.23207, Sensitive_Acc : 26.000, Run Time : 7.22 sec
INFO:root:2024-04-12 01:25:53, Train, Epoch : 3, Step : 1440, Loss : 0.43758, Acc : 0.791, Sensitive_Loss : 0.21168, Sensitive_Acc : 19.500, Run Time : 6.84 sec
INFO:root:2024-04-12 01:26:00, Train, Epoch : 3, Step : 1450, Loss : 0.52235, Acc : 0.791, Sensitive_Loss : 0.24306, Sensitive_Acc : 20.100, Run Time : 7.62 sec
INFO:root:2024-04-12 01:26:08, Train, Epoch : 3, Step : 1460, Loss : 0.47634, Acc : 0.784, Sensitive_Loss : 0.23716, Sensitive_Acc : 25.300, Run Time : 7.37 sec
INFO:root:2024-04-12 01:26:15, Train, Epoch : 3, Step : 1470, Loss : 0.49600, Acc : 0.766, Sensitive_Loss : 0.23016, Sensitive_Acc : 23.700, Run Time : 6.92 sec
INFO:root:2024-04-12 01:26:22, Train, Epoch : 3, Step : 1480, Loss : 0.47168, Acc : 0.781, Sensitive_Loss : 0.15773, Sensitive_Acc : 23.900, Run Time : 7.05 sec
INFO:root:2024-04-12 01:26:29, Train, Epoch : 3, Step : 1490, Loss : 0.40777, Acc : 0.816, Sensitive_Loss : 0.20082, Sensitive_Acc : 19.300, Run Time : 7.44 sec
INFO:root:2024-04-12 01:26:36, Train, Epoch : 3, Step : 1500, Loss : 0.48604, Acc : 0.784, Sensitive_Loss : 0.21999, Sensitive_Acc : 24.000, Run Time : 6.92 sec
INFO:root:2024-04-12 01:28:04, Dev, Step : 1500, Loss : 0.51983, Acc : 0.767, Auc : 0.844, Sensitive_Loss : 0.22303, Sensitive_Acc : 21.406, Sensitive_Auc : 0.996, Mean auc: 0.844, Run Time : 87.84 sec
INFO:root:2024-04-12 01:28:05, Best, Step : 1500, Loss : 0.51983, Acc : 0.767, Auc : 0.844, Sensitive_Loss : 0.22303, Sensitive_Acc : 21.406, Sensitive_Auc : 0.996, Best Auc : 0.844
INFO:root:2024-04-12 01:28:10, Train, Epoch : 3, Step : 1510, Loss : 0.51793, Acc : 0.762, Sensitive_Loss : 0.21738, Sensitive_Acc : 27.000, Run Time : 93.94 sec
INFO:root:2024-04-12 01:28:17, Train, Epoch : 3, Step : 1520, Loss : 0.47079, Acc : 0.819, Sensitive_Loss : 0.16643, Sensitive_Acc : 22.600, Run Time : 7.38 sec
INFO:root:2024-04-12 01:28:25, Train, Epoch : 3, Step : 1530, Loss : 0.41150, Acc : 0.822, Sensitive_Loss : 0.22298, Sensitive_Acc : 20.200, Run Time : 7.62 sec
INFO:root:2024-04-12 01:28:32, Train, Epoch : 3, Step : 1540, Loss : 0.49426, Acc : 0.794, Sensitive_Loss : 0.23316, Sensitive_Acc : 22.600, Run Time : 7.09 sec
INFO:root:2024-04-12 01:28:39, Train, Epoch : 3, Step : 1550, Loss : 0.42350, Acc : 0.828, Sensitive_Loss : 0.14111, Sensitive_Acc : 23.300, Run Time : 6.94 sec
INFO:root:2024-04-12 01:28:46, Train, Epoch : 3, Step : 1560, Loss : 0.44241, Acc : 0.797, Sensitive_Loss : 0.16977, Sensitive_Acc : 20.800, Run Time : 7.24 sec
INFO:root:2024-04-12 01:28:54, Train, Epoch : 3, Step : 1570, Loss : 0.47368, Acc : 0.759, Sensitive_Loss : 0.19037, Sensitive_Acc : 20.000, Run Time : 7.57 sec
INFO:root:2024-04-12 01:29:01, Train, Epoch : 3, Step : 1580, Loss : 0.45658, Acc : 0.797, Sensitive_Loss : 0.15071, Sensitive_Acc : 23.300, Run Time : 6.93 sec
INFO:root:2024-04-12 01:29:08, Train, Epoch : 3, Step : 1590, Loss : 0.54515, Acc : 0.812, Sensitive_Loss : 0.15015, Sensitive_Acc : 22.800, Run Time : 7.45 sec
INFO:root:2024-04-12 01:29:15, Train, Epoch : 3, Step : 1600, Loss : 0.48921, Acc : 0.756, Sensitive_Loss : 0.20937, Sensitive_Acc : 19.000, Run Time : 6.86 sec
INFO:root:2024-04-12 01:30:50, Dev, Step : 1600, Loss : 0.51523, Acc : 0.764, Auc : 0.847, Sensitive_Loss : 0.21115, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.847, Run Time : 95.42 sec
INFO:root:2024-04-12 01:30:51, Best, Step : 1600, Loss : 0.51523, Acc : 0.764, Auc : 0.847, Sensitive_Loss : 0.21115, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Best Auc : 0.847
INFO:root:2024-04-12 01:30:59, Train, Epoch : 3, Step : 1610, Loss : 0.48204, Acc : 0.772, Sensitive_Loss : 0.14101, Sensitive_Acc : 23.800, Run Time : 103.85 sec
INFO:root:2024-04-12 01:31:08, Train, Epoch : 3, Step : 1620, Loss : 0.41617, Acc : 0.794, Sensitive_Loss : 0.27754, Sensitive_Acc : 24.000, Run Time : 9.04 sec
INFO:root:2024-04-12 01:31:17, Train, Epoch : 3, Step : 1630, Loss : 0.47657, Acc : 0.797, Sensitive_Loss : 0.24940, Sensitive_Acc : 17.400, Run Time : 9.07 sec
INFO:root:2024-04-12 01:31:26, Train, Epoch : 3, Step : 1640, Loss : 0.41907, Acc : 0.794, Sensitive_Loss : 0.22507, Sensitive_Acc : 20.400, Run Time : 9.43 sec
INFO:root:2024-04-12 01:31:36, Train, Epoch : 3, Step : 1650, Loss : 0.44757, Acc : 0.812, Sensitive_Loss : 0.18105, Sensitive_Acc : 19.800, Run Time : 9.47 sec
INFO:root:2024-04-12 01:31:46, Train, Epoch : 3, Step : 1660, Loss : 0.45612, Acc : 0.778, Sensitive_Loss : 0.16539, Sensitive_Acc : 20.200, Run Time : 10.51 sec
INFO:root:2024-04-12 01:31:59, Train, Epoch : 3, Step : 1670, Loss : 0.49966, Acc : 0.747, Sensitive_Loss : 0.16916, Sensitive_Acc : 19.100, Run Time : 12.77 sec
INFO:root:2024-04-12 01:32:12, Train, Epoch : 3, Step : 1680, Loss : 0.48156, Acc : 0.809, Sensitive_Loss : 0.23237, Sensitive_Acc : 21.700, Run Time : 12.59 sec
INFO:root:2024-04-12 01:32:22, Train, Epoch : 3, Step : 1690, Loss : 0.51420, Acc : 0.772, Sensitive_Loss : 0.22979, Sensitive_Acc : 21.200, Run Time : 9.87 sec
INFO:root:2024-04-12 01:32:30, Train, Epoch : 3, Step : 1700, Loss : 0.39541, Acc : 0.828, Sensitive_Loss : 0.21742, Sensitive_Acc : 18.200, Run Time : 7.98 sec
INFO:root:2024-04-12 01:34:55, Dev, Step : 1700, Loss : 0.52436, Acc : 0.762, Auc : 0.844, Sensitive_Loss : 0.22613, Sensitive_Acc : 21.030, Sensitive_Auc : 0.996, Mean auc: 0.844, Run Time : 145.30 sec
INFO:root:2024-04-12 01:35:02, Train, Epoch : 3, Step : 1710, Loss : 0.49644, Acc : 0.784, Sensitive_Loss : 0.17553, Sensitive_Acc : 22.800, Run Time : 152.77 sec
INFO:root:2024-04-12 01:35:15, Train, Epoch : 3, Step : 1720, Loss : 0.46632, Acc : 0.787, Sensitive_Loss : 0.15999, Sensitive_Acc : 22.100, Run Time : 12.64 sec
INFO:root:2024-04-12 01:35:27, Train, Epoch : 3, Step : 1730, Loss : 0.52769, Acc : 0.750, Sensitive_Loss : 0.20708, Sensitive_Acc : 16.600, Run Time : 11.87 sec
INFO:root:2024-04-12 01:35:40, Train, Epoch : 3, Step : 1740, Loss : 0.45757, Acc : 0.812, Sensitive_Loss : 0.21085, Sensitive_Acc : 16.100, Run Time : 12.64 sec
INFO:root:2024-04-12 01:35:51, Train, Epoch : 3, Step : 1750, Loss : 0.46543, Acc : 0.769, Sensitive_Loss : 0.16404, Sensitive_Acc : 25.200, Run Time : 11.18 sec
INFO:root:2024-04-12 01:36:03, Train, Epoch : 3, Step : 1760, Loss : 0.49040, Acc : 0.778, Sensitive_Loss : 0.18454, Sensitive_Acc : 22.400, Run Time : 12.37 sec
INFO:root:2024-04-12 01:36:10, Train, Epoch : 3, Step : 1770, Loss : 0.40570, Acc : 0.812, Sensitive_Loss : 0.22208, Sensitive_Acc : 21.600, Run Time : 7.24 sec
INFO:root:2024-04-12 01:36:17, Train, Epoch : 3, Step : 1780, Loss : 0.39915, Acc : 0.800, Sensitive_Loss : 0.23106, Sensitive_Acc : 23.500, Run Time : 6.72 sec
INFO:root:2024-04-12 01:36:25, Train, Epoch : 3, Step : 1790, Loss : 0.45476, Acc : 0.819, Sensitive_Loss : 0.26020, Sensitive_Acc : 23.300, Run Time : 7.72 sec
INFO:root:2024-04-12 01:36:32, Train, Epoch : 3, Step : 1800, Loss : 0.46341, Acc : 0.803, Sensitive_Loss : 0.16045, Sensitive_Acc : 23.000, Run Time : 7.68 sec
INFO:root:2024-04-12 01:38:00, Dev, Step : 1800, Loss : 0.51330, Acc : 0.766, Auc : 0.849, Sensitive_Loss : 0.20889, Sensitive_Acc : 21.556, Sensitive_Auc : 0.997, Mean auc: 0.849, Run Time : 87.73 sec
INFO:root:2024-04-12 01:38:01, Best, Step : 1800, Loss : 0.51330, Acc : 0.766, Auc : 0.849, Sensitive_Loss : 0.20889, Sensitive_Acc : 21.556, Sensitive_Auc : 0.997, Best Auc : 0.849
INFO:root:2024-04-12 01:38:06, Train, Epoch : 3, Step : 1810, Loss : 0.44980, Acc : 0.816, Sensitive_Loss : 0.19057, Sensitive_Acc : 20.600, Run Time : 93.70 sec
INFO:root:2024-04-12 01:38:14, Train, Epoch : 3, Step : 1820, Loss : 0.44995, Acc : 0.803, Sensitive_Loss : 0.11692, Sensitive_Acc : 19.200, Run Time : 7.67 sec
INFO:root:2024-04-12 01:38:21, Train, Epoch : 3, Step : 1830, Loss : 0.45006, Acc : 0.775, Sensitive_Loss : 0.20178, Sensitive_Acc : 20.500, Run Time : 7.55 sec
INFO:root:2024-04-12 01:38:29, Train, Epoch : 3, Step : 1840, Loss : 0.44218, Acc : 0.791, Sensitive_Loss : 0.22067, Sensitive_Acc : 19.800, Run Time : 7.44 sec
INFO:root:2024-04-12 01:38:36, Train, Epoch : 3, Step : 1850, Loss : 0.41197, Acc : 0.812, Sensitive_Loss : 0.18620, Sensitive_Acc : 25.200, Run Time : 7.38 sec
INFO:root:2024-04-12 01:38:43, Train, Epoch : 3, Step : 1860, Loss : 0.43921, Acc : 0.797, Sensitive_Loss : 0.15602, Sensitive_Acc : 20.900, Run Time : 7.02 sec
INFO:root:2024-04-12 01:38:51, Train, Epoch : 3, Step : 1870, Loss : 0.45653, Acc : 0.794, Sensitive_Loss : 0.18910, Sensitive_Acc : 16.500, Run Time : 7.53 sec
INFO:root:2024-04-12 01:38:58, Train, Epoch : 3, Step : 1880, Loss : 0.51241, Acc : 0.791, Sensitive_Loss : 0.18212, Sensitive_Acc : 18.800, Run Time : 7.38 sec
INFO:root:2024-04-12 01:39:06, Train, Epoch : 3, Step : 1890, Loss : 0.50959, Acc : 0.747, Sensitive_Loss : 0.19864, Sensitive_Acc : 24.500, Run Time : 7.65 sec
INFO:root:2024-04-12 01:39:13, Train, Epoch : 3, Step : 1900, Loss : 0.44755, Acc : 0.806, Sensitive_Loss : 0.15798, Sensitive_Acc : 24.100, Run Time : 7.38 sec
INFO:root:2024-04-12 01:40:41, Dev, Step : 1900, Loss : 0.50526, Acc : 0.771, Auc : 0.853, Sensitive_Loss : 0.21035, Sensitive_Acc : 21.556, Sensitive_Auc : 0.997, Mean auc: 0.853, Run Time : 87.68 sec
INFO:root:2024-04-12 01:40:42, Best, Step : 1900, Loss : 0.50526, Acc : 0.771, Auc : 0.853, Sensitive_Loss : 0.21035, Sensitive_Acc : 21.556, Sensitive_Auc : 0.997, Best Auc : 0.853
INFO:root:2024-04-12 01:42:10
INFO:root:y_pred: [0.4627792  0.0297701  0.10807029 ... 0.19301848 0.09619995 0.01083824]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [2.9861206e-02 9.3162684e-03 5.0711464e-03 2.6559880e-01 1.9066574e-01
 2.5917126e-03 8.1798555e-03 4.8073949e-04 1.0006890e-02 9.9893481e-01
 1.9507544e-01 1.1389701e-05 1.2100773e-02 4.7568697e-05 9.9768257e-01
 1.1597422e-02 5.9171272e-03 9.9779558e-01 9.8568708e-01 3.1867067e-03
 8.7136018e-01 1.9459944e-04 8.6164642e-03 2.6915837e-02 3.8636739e-03
 9.7473994e-02 7.9390913e-04 5.2598125e-04 1.7158382e-04 1.1110392e-02
 8.9329416e-03 6.8223530e-01 1.9194540e-03 6.0301900e-01 2.2116593e-04
 1.5216022e-04 3.7583901e-05 5.0541800e-03 3.7420800e-01 1.3118887e-02
 6.6194810e-02 9.4423336e-01 3.6220856e-02 1.4973943e-04 9.6433771e-01
 3.7714681e-01 6.2459912e-02 3.5117903e-01 3.3789057e-01 9.8971140e-01
 7.1513999e-01 9.9848855e-01 9.1083813e-01 8.4739011e-05 3.6441619e-03
 1.2969583e-01 4.9732076e-03 5.1171654e-03 9.7957188e-01 2.8629603e-03
 1.1506540e-03 6.4885989e-02 1.6756091e-02 1.7902079e-07 9.8148924e-01
 2.0994791e-03 1.4771056e-06 2.1362706e-01 2.4070550e-02 9.8794103e-01
 9.9887520e-01 9.9669266e-01 1.8246628e-04 1.6731779e-01 2.1299215e-03
 4.0079388e-01 1.5700253e-02 1.1156493e-05 3.5331715e-04 1.8135271e-03
 1.6522714e-03 4.9500405e-03 9.8992670e-01 9.6014917e-01 1.9712265e-01
 9.4479248e-03 9.3636863e-02 4.6761022e-03 2.1142257e-02 2.0584595e-05
 2.6911381e-03 3.0169317e-01 7.6810342e-05 1.4276053e-05 8.2790181e-03
 7.9450943e-03 1.3726900e-04 8.8375920e-01 3.5847451e-03 3.1713184e-03
 2.9071819e-02 4.1636232e-02 1.3125455e-01 1.4746649e-04 1.5933456e-02
 8.1179012e-04 1.0768576e-02 5.8435202e-01 6.9228792e-01 3.0569729e-01
 6.3834370e-05 9.9933225e-01 9.9565566e-01 3.8500316e-06 5.0524777e-01
 1.9729918e-01 3.1029040e-01 1.4304201e-04 3.1507353e-03 7.7876903e-05
 7.8649499e-02 4.1563620e-04 6.9356393e-03 4.1728446e-05 5.9000226e-03
 9.1318762e-01 2.5886675e-06 9.7910714e-01 1.6108328e-02 1.5011685e-01
 1.1576668e-04 1.2943755e-02 7.8354569e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 01:42:10, Dev, Step : 1902, Loss : 0.50669, Acc : 0.773, Auc : 0.853, Sensitive_Loss : 0.20801, Sensitive_Acc : 21.556, Sensitive_Auc : 0.997, Mean auc: 0.853, Run Time : 86.74 sec
INFO:root:2024-04-12 01:42:17, Train, Epoch : 4, Step : 1910, Loss : 0.37163, Acc : 0.644, Sensitive_Loss : 0.13573, Sensitive_Acc : 15.000, Run Time : 6.60 sec
INFO:root:2024-04-12 01:42:24, Train, Epoch : 4, Step : 1920, Loss : 0.48162, Acc : 0.803, Sensitive_Loss : 0.15160, Sensitive_Acc : 19.800, Run Time : 7.15 sec
INFO:root:2024-04-12 01:42:31, Train, Epoch : 4, Step : 1930, Loss : 0.49805, Acc : 0.753, Sensitive_Loss : 0.22536, Sensitive_Acc : 19.200, Run Time : 7.24 sec
INFO:root:2024-04-12 01:42:39, Train, Epoch : 4, Step : 1940, Loss : 0.51174, Acc : 0.766, Sensitive_Loss : 0.15975, Sensitive_Acc : 24.200, Run Time : 7.05 sec
INFO:root:2024-04-12 01:42:46, Train, Epoch : 4, Step : 1950, Loss : 0.38348, Acc : 0.838, Sensitive_Loss : 0.16028, Sensitive_Acc : 22.300, Run Time : 7.38 sec
INFO:root:2024-04-12 01:42:53, Train, Epoch : 4, Step : 1960, Loss : 0.40079, Acc : 0.816, Sensitive_Loss : 0.22724, Sensitive_Acc : 17.800, Run Time : 6.78 sec
INFO:root:2024-04-12 01:43:00, Train, Epoch : 4, Step : 1970, Loss : 0.43271, Acc : 0.791, Sensitive_Loss : 0.17332, Sensitive_Acc : 21.200, Run Time : 7.24 sec
INFO:root:2024-04-12 01:43:07, Train, Epoch : 4, Step : 1980, Loss : 0.41971, Acc : 0.834, Sensitive_Loss : 0.17023, Sensitive_Acc : 20.000, Run Time : 7.23 sec
INFO:root:2024-04-12 01:43:14, Train, Epoch : 4, Step : 1990, Loss : 0.37174, Acc : 0.816, Sensitive_Loss : 0.17426, Sensitive_Acc : 23.000, Run Time : 6.69 sec
INFO:root:2024-04-12 01:43:21, Train, Epoch : 4, Step : 2000, Loss : 0.47889, Acc : 0.794, Sensitive_Loss : 0.18404, Sensitive_Acc : 24.600, Run Time : 7.64 sec
INFO:root:2024-04-12 01:44:50, Dev, Step : 2000, Loss : 0.51127, Acc : 0.771, Auc : 0.851, Sensitive_Loss : 0.23242, Sensitive_Acc : 21.301, Sensitive_Auc : 0.996, Mean auc: 0.851, Run Time : 88.07 sec
INFO:root:2024-04-12 01:44:55, Train, Epoch : 4, Step : 2010, Loss : 0.45910, Acc : 0.747, Sensitive_Loss : 0.16919, Sensitive_Acc : 21.500, Run Time : 93.67 sec
INFO:root:2024-04-12 01:45:03, Train, Epoch : 4, Step : 2020, Loss : 0.38402, Acc : 0.828, Sensitive_Loss : 0.17991, Sensitive_Acc : 23.100, Run Time : 7.40 sec
INFO:root:2024-04-12 01:45:10, Train, Epoch : 4, Step : 2030, Loss : 0.38044, Acc : 0.834, Sensitive_Loss : 0.17234, Sensitive_Acc : 20.100, Run Time : 7.64 sec
INFO:root:2024-04-12 01:45:17, Train, Epoch : 4, Step : 2040, Loss : 0.45924, Acc : 0.775, Sensitive_Loss : 0.17784, Sensitive_Acc : 23.700, Run Time : 6.80 sec
INFO:root:2024-04-12 01:45:24, Train, Epoch : 4, Step : 2050, Loss : 0.39830, Acc : 0.816, Sensitive_Loss : 0.19036, Sensitive_Acc : 25.100, Run Time : 7.07 sec
INFO:root:2024-04-12 01:45:32, Train, Epoch : 4, Step : 2060, Loss : 0.47517, Acc : 0.791, Sensitive_Loss : 0.18164, Sensitive_Acc : 21.700, Run Time : 7.71 sec
INFO:root:2024-04-12 01:45:39, Train, Epoch : 4, Step : 2070, Loss : 0.43018, Acc : 0.766, Sensitive_Loss : 0.14830, Sensitive_Acc : 21.800, Run Time : 6.82 sec
INFO:root:2024-04-12 01:45:46, Train, Epoch : 4, Step : 2080, Loss : 0.47524, Acc : 0.784, Sensitive_Loss : 0.16931, Sensitive_Acc : 23.200, Run Time : 7.54 sec
INFO:root:2024-04-12 01:45:54, Train, Epoch : 4, Step : 2090, Loss : 0.40385, Acc : 0.803, Sensitive_Loss : 0.26727, Sensitive_Acc : 22.700, Run Time : 7.54 sec
INFO:root:2024-04-12 01:46:01, Train, Epoch : 4, Step : 2100, Loss : 0.42284, Acc : 0.772, Sensitive_Loss : 0.27228, Sensitive_Acc : 20.300, Run Time : 7.14 sec
INFO:root:2024-04-12 01:47:29, Dev, Step : 2100, Loss : 0.52118, Acc : 0.769, Auc : 0.850, Sensitive_Loss : 0.21510, Sensitive_Acc : 21.692, Sensitive_Auc : 0.996, Mean auc: 0.850, Run Time : 88.29 sec
INFO:root:2024-04-12 01:47:35, Train, Epoch : 4, Step : 2110, Loss : 0.45168, Acc : 0.775, Sensitive_Loss : 0.18600, Sensitive_Acc : 16.300, Run Time : 94.01 sec
INFO:root:2024-04-12 01:47:43, Train, Epoch : 4, Step : 2120, Loss : 0.50384, Acc : 0.794, Sensitive_Loss : 0.20349, Sensitive_Acc : 26.100, Run Time : 8.46 sec
INFO:root:2024-04-12 01:47:51, Train, Epoch : 4, Step : 2130, Loss : 0.43789, Acc : 0.812, Sensitive_Loss : 0.15549, Sensitive_Acc : 20.700, Run Time : 7.70 sec
INFO:root:2024-04-12 01:47:59, Train, Epoch : 4, Step : 2140, Loss : 0.54025, Acc : 0.753, Sensitive_Loss : 0.17693, Sensitive_Acc : 20.300, Run Time : 8.33 sec
INFO:root:2024-04-12 01:48:07, Train, Epoch : 4, Step : 2150, Loss : 0.47473, Acc : 0.816, Sensitive_Loss : 0.22126, Sensitive_Acc : 23.100, Run Time : 7.94 sec
INFO:root:2024-04-12 01:48:15, Train, Epoch : 4, Step : 2160, Loss : 0.39807, Acc : 0.791, Sensitive_Loss : 0.10827, Sensitive_Acc : 20.000, Run Time : 8.12 sec
INFO:root:2024-04-12 01:48:23, Train, Epoch : 4, Step : 2170, Loss : 0.42130, Acc : 0.838, Sensitive_Loss : 0.20106, Sensitive_Acc : 22.400, Run Time : 8.04 sec
INFO:root:2024-04-12 01:48:32, Train, Epoch : 4, Step : 2180, Loss : 0.42732, Acc : 0.806, Sensitive_Loss : 0.18606, Sensitive_Acc : 23.900, Run Time : 8.14 sec
INFO:root:2024-04-12 01:48:39, Train, Epoch : 4, Step : 2190, Loss : 0.45140, Acc : 0.791, Sensitive_Loss : 0.15156, Sensitive_Acc : 23.600, Run Time : 7.67 sec
INFO:root:2024-04-12 01:48:47, Train, Epoch : 4, Step : 2200, Loss : 0.45124, Acc : 0.816, Sensitive_Loss : 0.13019, Sensitive_Acc : 18.500, Run Time : 7.81 sec
INFO:root:2024-04-12 01:50:16, Dev, Step : 2200, Loss : 0.50843, Acc : 0.770, Auc : 0.853, Sensitive_Loss : 0.21595, Sensitive_Acc : 21.241, Sensitive_Auc : 0.997, Mean auc: 0.853, Run Time : 88.58 sec
INFO:root:2024-04-12 01:50:21, Train, Epoch : 4, Step : 2210, Loss : 0.49037, Acc : 0.750, Sensitive_Loss : 0.16750, Sensitive_Acc : 19.800, Run Time : 94.43 sec
INFO:root:2024-04-12 01:50:30, Train, Epoch : 4, Step : 2220, Loss : 0.51168, Acc : 0.787, Sensitive_Loss : 0.18725, Sensitive_Acc : 21.000, Run Time : 8.22 sec
INFO:root:2024-04-12 01:50:38, Train, Epoch : 4, Step : 2230, Loss : 0.43942, Acc : 0.825, Sensitive_Loss : 0.13863, Sensitive_Acc : 18.300, Run Time : 7.92 sec
INFO:root:2024-04-12 01:50:45, Train, Epoch : 4, Step : 2240, Loss : 0.45474, Acc : 0.828, Sensitive_Loss : 0.13390, Sensitive_Acc : 24.500, Run Time : 7.71 sec
INFO:root:2024-04-12 01:50:53, Train, Epoch : 4, Step : 2250, Loss : 0.45423, Acc : 0.778, Sensitive_Loss : 0.26360, Sensitive_Acc : 21.300, Run Time : 7.97 sec
INFO:root:2024-04-12 01:51:01, Train, Epoch : 4, Step : 2260, Loss : 0.41164, Acc : 0.841, Sensitive_Loss : 0.19512, Sensitive_Acc : 22.600, Run Time : 7.46 sec
INFO:root:2024-04-12 01:51:09, Train, Epoch : 4, Step : 2270, Loss : 0.46287, Acc : 0.803, Sensitive_Loss : 0.21154, Sensitive_Acc : 16.100, Run Time : 7.84 sec
INFO:root:2024-04-12 01:51:17, Train, Epoch : 4, Step : 2280, Loss : 0.43851, Acc : 0.806, Sensitive_Loss : 0.21447, Sensitive_Acc : 22.800, Run Time : 8.10 sec
INFO:root:2024-04-12 01:51:24, Train, Epoch : 4, Step : 2290, Loss : 0.48592, Acc : 0.762, Sensitive_Loss : 0.22467, Sensitive_Acc : 21.200, Run Time : 7.74 sec
INFO:root:2024-04-12 01:51:32, Train, Epoch : 4, Step : 2300, Loss : 0.45852, Acc : 0.800, Sensitive_Loss : 0.15044, Sensitive_Acc : 25.500, Run Time : 7.42 sec
INFO:root:2024-04-12 01:53:01, Dev, Step : 2300, Loss : 0.51118, Acc : 0.773, Auc : 0.850, Sensitive_Loss : 0.21069, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.850, Run Time : 89.12 sec
INFO:root:2024-04-12 01:53:07, Train, Epoch : 4, Step : 2310, Loss : 0.43565, Acc : 0.775, Sensitive_Loss : 0.16106, Sensitive_Acc : 23.300, Run Time : 95.31 sec
INFO:root:2024-04-12 01:53:18, Train, Epoch : 4, Step : 2320, Loss : 0.44781, Acc : 0.812, Sensitive_Loss : 0.14289, Sensitive_Acc : 16.300, Run Time : 10.49 sec
INFO:root:2024-04-12 01:53:28, Train, Epoch : 4, Step : 2330, Loss : 0.51040, Acc : 0.787, Sensitive_Loss : 0.18445, Sensitive_Acc : 15.500, Run Time : 10.06 sec
INFO:root:2024-04-12 01:53:38, Train, Epoch : 4, Step : 2340, Loss : 0.48176, Acc : 0.803, Sensitive_Loss : 0.15071, Sensitive_Acc : 24.100, Run Time : 10.10 sec
INFO:root:2024-04-12 01:53:47, Train, Epoch : 4, Step : 2350, Loss : 0.48372, Acc : 0.772, Sensitive_Loss : 0.21426, Sensitive_Acc : 20.600, Run Time : 8.81 sec
INFO:root:2024-04-12 01:53:57, Train, Epoch : 4, Step : 2360, Loss : 0.40675, Acc : 0.831, Sensitive_Loss : 0.14030, Sensitive_Acc : 24.500, Run Time : 10.29 sec
INFO:root:2024-04-12 01:54:06, Train, Epoch : 4, Step : 2370, Loss : 0.43380, Acc : 0.791, Sensitive_Loss : 0.17855, Sensitive_Acc : 22.600, Run Time : 8.95 sec
INFO:root:2024-04-12 01:54:15, Train, Epoch : 4, Step : 2380, Loss : 0.43951, Acc : 0.809, Sensitive_Loss : 0.20083, Sensitive_Acc : 18.900, Run Time : 8.68 sec
INFO:root:2024-04-12 01:54:23, Train, Epoch : 4, Step : 2390, Loss : 0.41645, Acc : 0.794, Sensitive_Loss : 0.21630, Sensitive_Acc : 22.600, Run Time : 8.74 sec
INFO:root:2024-04-12 01:54:32, Train, Epoch : 4, Step : 2400, Loss : 0.39494, Acc : 0.853, Sensitive_Loss : 0.23808, Sensitive_Acc : 16.600, Run Time : 8.87 sec
INFO:root:2024-04-12 01:56:01, Dev, Step : 2400, Loss : 0.50798, Acc : 0.772, Auc : 0.853, Sensitive_Loss : 0.20100, Sensitive_Acc : 21.556, Sensitive_Auc : 0.997, Mean auc: 0.853, Run Time : 89.25 sec
INFO:root:2024-04-12 01:56:08, Train, Epoch : 4, Step : 2410, Loss : 0.46146, Acc : 0.753, Sensitive_Loss : 0.17753, Sensitive_Acc : 19.700, Run Time : 95.48 sec
INFO:root:2024-04-12 01:56:16, Train, Epoch : 4, Step : 2420, Loss : 0.36936, Acc : 0.784, Sensitive_Loss : 0.16768, Sensitive_Acc : 21.800, Run Time : 8.83 sec
INFO:root:2024-04-12 01:56:25, Train, Epoch : 4, Step : 2430, Loss : 0.44948, Acc : 0.806, Sensitive_Loss : 0.15753, Sensitive_Acc : 25.500, Run Time : 8.77 sec
INFO:root:2024-04-12 01:56:34, Train, Epoch : 4, Step : 2440, Loss : 0.47213, Acc : 0.819, Sensitive_Loss : 0.14216, Sensitive_Acc : 21.800, Run Time : 8.54 sec
INFO:root:2024-04-12 01:56:42, Train, Epoch : 4, Step : 2450, Loss : 0.47606, Acc : 0.791, Sensitive_Loss : 0.15873, Sensitive_Acc : 13.500, Run Time : 8.60 sec
INFO:root:2024-04-12 01:56:51, Train, Epoch : 4, Step : 2460, Loss : 0.45161, Acc : 0.794, Sensitive_Loss : 0.18081, Sensitive_Acc : 24.800, Run Time : 8.70 sec
INFO:root:2024-04-12 01:56:59, Train, Epoch : 4, Step : 2470, Loss : 0.45320, Acc : 0.791, Sensitive_Loss : 0.19617, Sensitive_Acc : 23.400, Run Time : 8.33 sec
INFO:root:2024-04-12 01:57:08, Train, Epoch : 4, Step : 2480, Loss : 0.44997, Acc : 0.803, Sensitive_Loss : 0.14561, Sensitive_Acc : 23.300, Run Time : 9.04 sec
INFO:root:2024-04-12 01:57:17, Train, Epoch : 4, Step : 2490, Loss : 0.50349, Acc : 0.762, Sensitive_Loss : 0.18393, Sensitive_Acc : 19.000, Run Time : 8.37 sec
INFO:root:2024-04-12 01:57:26, Train, Epoch : 4, Step : 2500, Loss : 0.36955, Acc : 0.822, Sensitive_Loss : 0.14748, Sensitive_Acc : 22.200, Run Time : 8.99 sec
INFO:root:2024-04-12 01:58:54, Dev, Step : 2500, Loss : 0.51163, Acc : 0.773, Auc : 0.853, Sensitive_Loss : 0.20078, Sensitive_Acc : 21.692, Sensitive_Auc : 0.995, Mean auc: 0.853, Run Time : 88.03 sec
INFO:root:2024-04-12 01:58:55, Best, Step : 2500, Loss : 0.51163, Acc : 0.773, Auc : 0.853, Sensitive_Loss : 0.20078, Sensitive_Acc : 21.692, Sensitive_Auc : 0.995, Best Auc : 0.853
INFO:root:2024-04-12 01:59:01, Train, Epoch : 4, Step : 2510, Loss : 0.41351, Acc : 0.838, Sensitive_Loss : 0.20634, Sensitive_Acc : 20.000, Run Time : 95.23 sec
INFO:root:2024-04-12 01:59:10, Train, Epoch : 4, Step : 2520, Loss : 0.46674, Acc : 0.797, Sensitive_Loss : 0.17205, Sensitive_Acc : 19.900, Run Time : 8.93 sec
INFO:root:2024-04-12 01:59:19, Train, Epoch : 4, Step : 2530, Loss : 0.46275, Acc : 0.828, Sensitive_Loss : 0.22662, Sensitive_Acc : 23.200, Run Time : 9.40 sec
INFO:root:2024-04-12 02:00:52
INFO:root:y_pred: [0.24862272 0.01515146 0.06542251 ... 0.19397573 0.10767418 0.00611669]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [5.05186208e-02 1.16304113e-02 2.84976186e-03 2.33548790e-01
 2.56319314e-01 7.51964224e-04 3.72585165e-03 7.25101738e-04
 2.86902413e-02 9.99308586e-01 4.59117830e-01 1.14579889e-05
 1.27663072e-02 2.53345315e-05 9.97256458e-01 1.31552927e-02
 2.23550224e-03 9.97870564e-01 9.93404686e-01 2.82755354e-03
 7.49966025e-01 4.62866883e-05 2.85361707e-02 4.69257161e-02
 2.99424655e-03 9.15689766e-02 6.36537850e-04 1.91855055e-04
 8.35896499e-05 2.23455839e-02 1.04037961e-02 7.26253331e-01
 3.51744494e-03 7.01692343e-01 8.15586682e-05 3.17081664e-04
 1.55559639e-04 4.02054563e-03 3.24935406e-01 4.70994450e-02
 7.04124197e-02 9.63865280e-01 2.55959686e-02 1.47519793e-04
 9.66161013e-01 4.40053463e-01 1.09120786e-01 3.57112169e-01
 3.42122942e-01 9.92897153e-01 8.29903543e-01 9.98762488e-01
 9.67548668e-01 3.24728608e-05 1.40628335e-03 9.51124057e-02
 2.66571832e-03 3.62311699e-03 9.77763236e-01 3.44501855e-03
 3.22905631e-04 5.31243198e-02 1.25539256e-02 1.54906388e-07
 9.84933317e-01 1.05280876e-02 8.10990969e-07 3.78421128e-01
 3.20906788e-02 9.90114093e-01 9.99123275e-01 9.97860491e-01
 3.26555819e-05 2.69176155e-01 3.04803625e-03 4.27096754e-01
 1.46296863e-02 3.73909529e-06 3.76567186e-04 6.85153005e-04
 1.28353655e-03 2.03142688e-03 9.88040149e-01 9.54100490e-01
 6.80570677e-02 9.61118657e-03 1.80794984e-01 1.47367315e-02
 4.97494005e-02 1.26493260e-05 1.70966494e-03 3.30569834e-01
 3.07990849e-05 5.33104321e-06 9.22090653e-03 1.48572400e-02
 3.40202037e-04 9.26123917e-01 4.16909205e-03 4.66906559e-03
 3.11146751e-02 7.71174729e-02 9.25323814e-02 1.36524311e-03
 1.41849834e-02 2.24939524e-03 3.87256928e-02 8.14869523e-01
 6.94868982e-01 3.42336386e-01 3.48213362e-05 9.99455631e-01
 9.97719347e-01 1.50823973e-06 5.03667533e-01 1.82361811e-01
 4.66318071e-01 2.90138705e-04 1.96349667e-03 4.75810113e-04
 8.33486244e-02 3.58641235e-04 7.51115009e-03 3.76297357e-05
 1.80807952e-02 9.59446967e-01 6.89746400e-07 9.88913834e-01
 7.15989470e-02 1.52748853e-01 5.10193524e-04 8.95797182e-03
 1.67649864e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 02:00:52, Dev, Step : 2536, Loss : 0.51329, Acc : 0.768, Auc : 0.853, Sensitive_Loss : 0.20872, Sensitive_Acc : 21.556, Sensitive_Auc : 0.997, Mean auc: 0.853, Run Time : 87.37 sec
INFO:root:2024-04-12 02:00:57, Train, Epoch : 5, Step : 2540, Loss : 0.15079, Acc : 0.322, Sensitive_Loss : 0.05016, Sensitive_Acc : 10.000, Run Time : 4.32 sec
INFO:root:2024-04-12 02:01:05, Train, Epoch : 5, Step : 2550, Loss : 0.35441, Acc : 0.841, Sensitive_Loss : 0.14828, Sensitive_Acc : 19.700, Run Time : 7.48 sec
INFO:root:2024-04-12 02:01:12, Train, Epoch : 5, Step : 2560, Loss : 0.35671, Acc : 0.809, Sensitive_Loss : 0.16672, Sensitive_Acc : 23.700, Run Time : 7.66 sec
INFO:root:2024-04-12 02:01:20, Train, Epoch : 5, Step : 2570, Loss : 0.46129, Acc : 0.831, Sensitive_Loss : 0.10949, Sensitive_Acc : 18.600, Run Time : 7.93 sec
INFO:root:2024-04-12 02:01:28, Train, Epoch : 5, Step : 2580, Loss : 0.38767, Acc : 0.812, Sensitive_Loss : 0.21703, Sensitive_Acc : 24.400, Run Time : 7.45 sec
INFO:root:2024-04-12 02:01:36, Train, Epoch : 5, Step : 2590, Loss : 0.44884, Acc : 0.787, Sensitive_Loss : 0.14499, Sensitive_Acc : 26.500, Run Time : 7.77 sec
INFO:root:2024-04-12 02:01:43, Train, Epoch : 5, Step : 2600, Loss : 0.49208, Acc : 0.812, Sensitive_Loss : 0.17617, Sensitive_Acc : 20.200, Run Time : 7.69 sec
INFO:root:2024-04-12 02:03:11, Dev, Step : 2600, Loss : 0.50406, Acc : 0.773, Auc : 0.855, Sensitive_Loss : 0.20907, Sensitive_Acc : 21.241, Sensitive_Auc : 0.995, Mean auc: 0.855, Run Time : 88.22 sec
INFO:root:2024-04-12 02:03:12, Best, Step : 2600, Loss : 0.50406, Acc : 0.773, Auc : 0.855, Sensitive_Loss : 0.20907, Sensitive_Acc : 21.241, Sensitive_Auc : 0.995, Best Auc : 0.855
INFO:root:2024-04-12 02:03:18, Train, Epoch : 5, Step : 2610, Loss : 0.47307, Acc : 0.775, Sensitive_Loss : 0.16296, Sensitive_Acc : 22.500, Run Time : 95.13 sec
INFO:root:2024-04-12 02:03:26, Train, Epoch : 5, Step : 2620, Loss : 0.34671, Acc : 0.847, Sensitive_Loss : 0.16992, Sensitive_Acc : 23.500, Run Time : 7.53 sec
INFO:root:2024-04-12 02:03:34, Train, Epoch : 5, Step : 2630, Loss : 0.44698, Acc : 0.800, Sensitive_Loss : 0.13949, Sensitive_Acc : 25.000, Run Time : 7.99 sec
INFO:root:2024-04-12 02:03:41, Train, Epoch : 5, Step : 2640, Loss : 0.41947, Acc : 0.791, Sensitive_Loss : 0.15159, Sensitive_Acc : 24.300, Run Time : 7.33 sec
INFO:root:2024-04-12 02:03:49, Train, Epoch : 5, Step : 2650, Loss : 0.41346, Acc : 0.828, Sensitive_Loss : 0.23038, Sensitive_Acc : 20.000, Run Time : 7.40 sec
INFO:root:2024-04-12 02:03:56, Train, Epoch : 5, Step : 2660, Loss : 0.40907, Acc : 0.812, Sensitive_Loss : 0.29399, Sensitive_Acc : 21.400, Run Time : 7.30 sec
INFO:root:2024-04-12 02:04:04, Train, Epoch : 5, Step : 2670, Loss : 0.43531, Acc : 0.803, Sensitive_Loss : 0.13213, Sensitive_Acc : 17.900, Run Time : 7.90 sec
INFO:root:2024-04-12 02:04:11, Train, Epoch : 5, Step : 2680, Loss : 0.50468, Acc : 0.775, Sensitive_Loss : 0.21505, Sensitive_Acc : 22.400, Run Time : 7.61 sec
INFO:root:2024-04-12 02:04:20, Train, Epoch : 5, Step : 2690, Loss : 0.45524, Acc : 0.806, Sensitive_Loss : 0.16651, Sensitive_Acc : 19.700, Run Time : 8.10 sec
INFO:root:2024-04-12 02:04:27, Train, Epoch : 5, Step : 2700, Loss : 0.46080, Acc : 0.806, Sensitive_Loss : 0.15395, Sensitive_Acc : 22.500, Run Time : 7.33 sec
INFO:root:2024-04-12 02:05:55, Dev, Step : 2700, Loss : 0.51676, Acc : 0.771, Auc : 0.851, Sensitive_Loss : 0.20206, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.851, Run Time : 87.88 sec
INFO:root:2024-04-12 02:06:00, Train, Epoch : 5, Step : 2710, Loss : 0.39431, Acc : 0.812, Sensitive_Loss : 0.12112, Sensitive_Acc : 21.000, Run Time : 93.65 sec
INFO:root:2024-04-12 02:06:08, Train, Epoch : 5, Step : 2720, Loss : 0.38727, Acc : 0.838, Sensitive_Loss : 0.16563, Sensitive_Acc : 22.500, Run Time : 7.73 sec
INFO:root:2024-04-12 02:06:16, Train, Epoch : 5, Step : 2730, Loss : 0.40463, Acc : 0.844, Sensitive_Loss : 0.19408, Sensitive_Acc : 19.600, Run Time : 7.38 sec
INFO:root:2024-04-12 02:06:23, Train, Epoch : 5, Step : 2740, Loss : 0.38367, Acc : 0.806, Sensitive_Loss : 0.24841, Sensitive_Acc : 22.900, Run Time : 7.37 sec
INFO:root:2024-04-12 02:06:31, Train, Epoch : 5, Step : 2750, Loss : 0.43633, Acc : 0.822, Sensitive_Loss : 0.12464, Sensitive_Acc : 20.900, Run Time : 7.58 sec
INFO:root:2024-04-12 02:06:38, Train, Epoch : 5, Step : 2760, Loss : 0.47091, Acc : 0.794, Sensitive_Loss : 0.12766, Sensitive_Acc : 21.200, Run Time : 7.42 sec
INFO:root:2024-04-12 02:06:46, Train, Epoch : 5, Step : 2770, Loss : 0.46582, Acc : 0.819, Sensitive_Loss : 0.11869, Sensitive_Acc : 21.300, Run Time : 8.05 sec
INFO:root:2024-04-12 02:06:53, Train, Epoch : 5, Step : 2780, Loss : 0.45991, Acc : 0.775, Sensitive_Loss : 0.13552, Sensitive_Acc : 20.400, Run Time : 7.26 sec
INFO:root:2024-04-12 02:07:01, Train, Epoch : 5, Step : 2790, Loss : 0.39421, Acc : 0.797, Sensitive_Loss : 0.15430, Sensitive_Acc : 24.400, Run Time : 7.62 sec
INFO:root:2024-04-12 02:07:09, Train, Epoch : 5, Step : 2800, Loss : 0.40571, Acc : 0.812, Sensitive_Loss : 0.18868, Sensitive_Acc : 18.600, Run Time : 7.78 sec
INFO:root:2024-04-12 02:08:38, Dev, Step : 2800, Loss : 0.51523, Acc : 0.772, Auc : 0.852, Sensitive_Loss : 0.19385, Sensitive_Acc : 21.767, Sensitive_Auc : 0.997, Mean auc: 0.852, Run Time : 89.08 sec
INFO:root:2024-04-12 02:08:44, Train, Epoch : 5, Step : 2810, Loss : 0.38717, Acc : 0.875, Sensitive_Loss : 0.18364, Sensitive_Acc : 15.700, Run Time : 95.05 sec
INFO:root:2024-04-12 02:08:52, Train, Epoch : 5, Step : 2820, Loss : 0.38894, Acc : 0.844, Sensitive_Loss : 0.21789, Sensitive_Acc : 24.200, Run Time : 8.39 sec
INFO:root:2024-04-12 02:09:00, Train, Epoch : 5, Step : 2830, Loss : 0.51805, Acc : 0.775, Sensitive_Loss : 0.17117, Sensitive_Acc : 19.200, Run Time : 8.03 sec
INFO:root:2024-04-12 02:09:08, Train, Epoch : 5, Step : 2840, Loss : 0.40199, Acc : 0.816, Sensitive_Loss : 0.17946, Sensitive_Acc : 20.400, Run Time : 7.96 sec
INFO:root:2024-04-12 02:09:16, Train, Epoch : 5, Step : 2850, Loss : 0.43817, Acc : 0.794, Sensitive_Loss : 0.16959, Sensitive_Acc : 23.900, Run Time : 7.72 sec
INFO:root:2024-04-12 02:09:24, Train, Epoch : 5, Step : 2860, Loss : 0.40229, Acc : 0.838, Sensitive_Loss : 0.09639, Sensitive_Acc : 25.300, Run Time : 7.99 sec
INFO:root:2024-04-12 02:09:32, Train, Epoch : 5, Step : 2870, Loss : 0.37502, Acc : 0.850, Sensitive_Loss : 0.18891, Sensitive_Acc : 18.700, Run Time : 8.05 sec
INFO:root:2024-04-12 02:09:40, Train, Epoch : 5, Step : 2880, Loss : 0.39346, Acc : 0.825, Sensitive_Loss : 0.08825, Sensitive_Acc : 20.800, Run Time : 8.15 sec
INFO:root:2024-04-12 02:09:48, Train, Epoch : 5, Step : 2890, Loss : 0.35464, Acc : 0.825, Sensitive_Loss : 0.20609, Sensitive_Acc : 22.000, Run Time : 7.87 sec
INFO:root:2024-04-12 02:09:56, Train, Epoch : 5, Step : 2900, Loss : 0.43934, Acc : 0.812, Sensitive_Loss : 0.15419, Sensitive_Acc : 24.900, Run Time : 7.95 sec
INFO:root:2024-04-12 02:11:24, Dev, Step : 2900, Loss : 0.53258, Acc : 0.770, Auc : 0.853, Sensitive_Loss : 0.21239, Sensitive_Acc : 21.361, Sensitive_Auc : 0.997, Mean auc: 0.853, Run Time : 88.20 sec
INFO:root:2024-04-12 02:11:30, Train, Epoch : 5, Step : 2910, Loss : 0.41752, Acc : 0.844, Sensitive_Loss : 0.15033, Sensitive_Acc : 19.800, Run Time : 93.77 sec
INFO:root:2024-04-12 02:11:37, Train, Epoch : 5, Step : 2920, Loss : 0.43374, Acc : 0.825, Sensitive_Loss : 0.17423, Sensitive_Acc : 24.400, Run Time : 7.64 sec
INFO:root:2024-04-12 02:11:45, Train, Epoch : 5, Step : 2930, Loss : 0.45269, Acc : 0.800, Sensitive_Loss : 0.14167, Sensitive_Acc : 23.300, Run Time : 7.72 sec
INFO:root:2024-04-12 02:11:53, Train, Epoch : 5, Step : 2940, Loss : 0.43462, Acc : 0.787, Sensitive_Loss : 0.15743, Sensitive_Acc : 21.400, Run Time : 7.73 sec
INFO:root:2024-04-12 02:12:01, Train, Epoch : 5, Step : 2950, Loss : 0.43476, Acc : 0.797, Sensitive_Loss : 0.18955, Sensitive_Acc : 25.100, Run Time : 8.17 sec
INFO:root:2024-04-12 02:12:08, Train, Epoch : 5, Step : 2960, Loss : 0.38380, Acc : 0.800, Sensitive_Loss : 0.15220, Sensitive_Acc : 19.500, Run Time : 7.56 sec
INFO:root:2024-04-12 02:12:16, Train, Epoch : 5, Step : 2970, Loss : 0.47483, Acc : 0.812, Sensitive_Loss : 0.19857, Sensitive_Acc : 26.700, Run Time : 8.08 sec
INFO:root:2024-04-12 02:12:27, Train, Epoch : 5, Step : 2980, Loss : 0.42539, Acc : 0.800, Sensitive_Loss : 0.20591, Sensitive_Acc : 23.800, Run Time : 10.10 sec
INFO:root:2024-04-12 02:12:36, Train, Epoch : 5, Step : 2990, Loss : 0.39663, Acc : 0.819, Sensitive_Loss : 0.16653, Sensitive_Acc : 22.300, Run Time : 9.83 sec
INFO:root:2024-04-12 02:12:47, Train, Epoch : 5, Step : 3000, Loss : 0.47966, Acc : 0.791, Sensitive_Loss : 0.14064, Sensitive_Acc : 19.300, Run Time : 10.97 sec
INFO:root:2024-04-12 02:14:40, Dev, Step : 3000, Loss : 0.50694, Acc : 0.772, Auc : 0.854, Sensitive_Loss : 0.19657, Sensitive_Acc : 21.797, Sensitive_Auc : 0.997, Mean auc: 0.854, Run Time : 112.24 sec
INFO:root:2024-04-12 02:14:46, Train, Epoch : 5, Step : 3010, Loss : 0.44173, Acc : 0.797, Sensitive_Loss : 0.15322, Sensitive_Acc : 16.400, Run Time : 118.57 sec
INFO:root:2024-04-12 02:14:55, Train, Epoch : 5, Step : 3020, Loss : 0.44434, Acc : 0.831, Sensitive_Loss : 0.14936, Sensitive_Acc : 18.400, Run Time : 9.45 sec
INFO:root:2024-04-12 02:15:04, Train, Epoch : 5, Step : 3030, Loss : 0.40205, Acc : 0.781, Sensitive_Loss : 0.15025, Sensitive_Acc : 20.500, Run Time : 8.94 sec
INFO:root:2024-04-12 02:15:15, Train, Epoch : 5, Step : 3040, Loss : 0.40690, Acc : 0.787, Sensitive_Loss : 0.19122, Sensitive_Acc : 20.300, Run Time : 10.58 sec
INFO:root:2024-04-12 02:15:25, Train, Epoch : 5, Step : 3050, Loss : 0.36257, Acc : 0.838, Sensitive_Loss : 0.14104, Sensitive_Acc : 24.200, Run Time : 9.60 sec
INFO:root:2024-04-12 02:15:35, Train, Epoch : 5, Step : 3060, Loss : 0.42756, Acc : 0.803, Sensitive_Loss : 0.19247, Sensitive_Acc : 26.100, Run Time : 10.76 sec
INFO:root:2024-04-12 02:15:46, Train, Epoch : 5, Step : 3070, Loss : 0.39655, Acc : 0.822, Sensitive_Loss : 0.15254, Sensitive_Acc : 24.100, Run Time : 10.48 sec
INFO:root:2024-04-12 02:15:56, Train, Epoch : 5, Step : 3080, Loss : 0.39372, Acc : 0.838, Sensitive_Loss : 0.22064, Sensitive_Acc : 14.300, Run Time : 10.13 sec
INFO:root:2024-04-12 02:16:05, Train, Epoch : 5, Step : 3090, Loss : 0.37432, Acc : 0.831, Sensitive_Loss : 0.14128, Sensitive_Acc : 22.600, Run Time : 9.46 sec
INFO:root:2024-04-12 02:16:16, Train, Epoch : 5, Step : 3100, Loss : 0.50014, Acc : 0.791, Sensitive_Loss : 0.14796, Sensitive_Acc : 21.500, Run Time : 10.60 sec
INFO:root:2024-04-12 02:17:55, Dev, Step : 3100, Loss : 0.51077, Acc : 0.777, Auc : 0.855, Sensitive_Loss : 0.19408, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.855, Run Time : 98.64 sec
INFO:root:2024-04-12 02:18:02, Train, Epoch : 5, Step : 3110, Loss : 0.38014, Acc : 0.809, Sensitive_Loss : 0.19807, Sensitive_Acc : 22.300, Run Time : 105.78 sec
INFO:root:2024-04-12 02:18:10, Train, Epoch : 5, Step : 3120, Loss : 0.42766, Acc : 0.803, Sensitive_Loss : 0.23746, Sensitive_Acc : 22.300, Run Time : 8.68 sec
INFO:root:2024-04-12 02:18:19, Train, Epoch : 5, Step : 3130, Loss : 0.48304, Acc : 0.784, Sensitive_Loss : 0.14243, Sensitive_Acc : 25.900, Run Time : 8.49 sec
INFO:root:2024-04-12 02:18:28, Train, Epoch : 5, Step : 3140, Loss : 0.43734, Acc : 0.812, Sensitive_Loss : 0.16609, Sensitive_Acc : 25.900, Run Time : 8.94 sec
INFO:root:2024-04-12 02:18:36, Train, Epoch : 5, Step : 3150, Loss : 0.38849, Acc : 0.812, Sensitive_Loss : 0.17975, Sensitive_Acc : 18.800, Run Time : 8.17 sec
INFO:root:2024-04-12 02:18:45, Train, Epoch : 5, Step : 3160, Loss : 0.41639, Acc : 0.787, Sensitive_Loss : 0.12648, Sensitive_Acc : 22.300, Run Time : 8.72 sec
INFO:root:2024-04-12 02:18:53, Train, Epoch : 5, Step : 3170, Loss : 0.40664, Acc : 0.803, Sensitive_Loss : 0.16795, Sensitive_Acc : 24.800, Run Time : 8.61 sec
INFO:root:2024-04-12 02:20:22
INFO:root:y_pred: [0.4083715  0.01653773 0.06172424 ... 0.27912068 0.13489847 0.00311248]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [7.45706335e-02 1.50701648e-03 4.91582323e-03 1.78280920e-01
 2.06960350e-01 7.87448589e-05 2.86903372e-03 1.01160782e-03
 3.92641388e-02 9.99352634e-01 3.35286468e-01 2.37317204e-06
 4.55693156e-03 8.37511743e-06 9.96167958e-01 1.64324846e-02
 6.95579685e-04 9.98739898e-01 9.95255291e-01 1.41440576e-03
 7.72245228e-01 3.07433293e-05 3.27389874e-03 1.52951535e-02
 2.87002098e-04 6.99549615e-02 1.18406559e-03 1.26694256e-04
 1.18980810e-04 2.03023050e-02 6.51782285e-03 7.31217265e-01
 1.67938287e-03 5.53499281e-01 2.28849312e-05 1.26653918e-04
 6.94155824e-06 2.55546859e-03 1.44401833e-01 8.58227536e-03
 6.44797683e-02 9.67032433e-01 1.47693083e-02 3.83324623e-05
 9.72915947e-01 2.47929990e-01 1.31445393e-01 3.05689156e-01
 2.59418637e-01 9.93344069e-01 8.29277158e-01 9.99092817e-01
 9.36441481e-01 3.05943581e-06 5.32609120e-04 7.03389943e-02
 9.38308192e-04 1.05013717e-02 9.81748402e-01 1.03829720e-03
 6.82605314e-05 5.49647361e-02 1.23978015e-02 3.31024168e-08
 9.90283132e-01 7.00309873e-03 6.98513475e-07 4.01429951e-01
 1.34080928e-02 9.86323118e-01 9.99257028e-01 9.97598708e-01
 3.42239764e-05 3.57289612e-01 6.23303116e-04 3.72457385e-01
 5.24767302e-03 1.86089835e-06 6.01376232e-05 2.13006526e-04
 3.97731637e-04 1.97465927e-03 9.87622440e-01 9.51049685e-01
 4.23261672e-02 1.35465898e-03 1.92612231e-01 4.05989075e-03
 5.72403148e-03 8.61657190e-06 1.27741904e-03 1.13749906e-01
 1.08652130e-05 1.89591344e-06 2.35301559e-03 4.72974032e-03
 3.34935059e-04 8.73144627e-01 6.92253700e-04 2.13423036e-02
 5.54094464e-02 2.47564744e-02 3.32783051e-02 1.27803403e-04
 5.52087557e-03 2.58285028e-04 1.03513058e-02 7.71422863e-01
 5.30863523e-01 1.34946644e-01 1.21390740e-05 9.99516964e-01
 9.97876883e-01 3.02717183e-07 5.70495784e-01 1.97295308e-01
 2.83582628e-01 1.23662930e-05 1.41632790e-03 1.70285166e-05
 4.91505973e-02 9.99947893e-04 8.09992291e-03 7.06511310e-06
 5.88321825e-03 8.76145601e-01 1.07607121e-07 9.87190545e-01
 1.76183078e-02 1.48798227e-01 1.87733076e-05 3.46363871e-04
 2.18791612e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 02:20:22, Dev, Step : 3170, Loss : 0.52195, Acc : 0.773, Auc : 0.851, Sensitive_Loss : 0.19526, Sensitive_Acc : 21.556, Sensitive_Auc : 0.997, Mean auc: 0.851, Run Time : 88.06 sec
INFO:root:2024-04-12 02:20:31, Train, Epoch : 6, Step : 3180, Loss : 0.41021, Acc : 0.822, Sensitive_Loss : 0.10369, Sensitive_Acc : 23.900, Run Time : 8.61 sec
INFO:root:2024-04-12 02:20:39, Train, Epoch : 6, Step : 3190, Loss : 0.46040, Acc : 0.825, Sensitive_Loss : 0.15928, Sensitive_Acc : 22.800, Run Time : 7.55 sec
INFO:root:2024-04-12 02:20:46, Train, Epoch : 6, Step : 3200, Loss : 0.32911, Acc : 0.841, Sensitive_Loss : 0.17198, Sensitive_Acc : 21.700, Run Time : 7.60 sec
INFO:root:2024-04-12 02:22:15, Dev, Step : 3200, Loss : 0.51571, Acc : 0.770, Auc : 0.850, Sensitive_Loss : 0.19885, Sensitive_Acc : 21.556, Sensitive_Auc : 0.997, Mean auc: 0.850, Run Time : 88.13 sec
INFO:root:2024-04-12 02:22:20, Train, Epoch : 6, Step : 3210, Loss : 0.40259, Acc : 0.822, Sensitive_Loss : 0.12765, Sensitive_Acc : 21.600, Run Time : 93.78 sec
INFO:root:2024-04-12 02:22:28, Train, Epoch : 6, Step : 3220, Loss : 0.45106, Acc : 0.791, Sensitive_Loss : 0.12615, Sensitive_Acc : 25.100, Run Time : 7.52 sec
INFO:root:2024-04-12 02:22:36, Train, Epoch : 6, Step : 3230, Loss : 0.43952, Acc : 0.812, Sensitive_Loss : 0.14972, Sensitive_Acc : 23.200, Run Time : 7.98 sec
INFO:root:2024-04-12 02:22:43, Train, Epoch : 6, Step : 3240, Loss : 0.36793, Acc : 0.825, Sensitive_Loss : 0.15617, Sensitive_Acc : 24.000, Run Time : 7.59 sec
INFO:root:2024-04-12 02:22:51, Train, Epoch : 6, Step : 3250, Loss : 0.36072, Acc : 0.863, Sensitive_Loss : 0.14845, Sensitive_Acc : 18.200, Run Time : 7.63 sec
INFO:root:2024-04-12 02:22:58, Train, Epoch : 6, Step : 3260, Loss : 0.33687, Acc : 0.869, Sensitive_Loss : 0.13283, Sensitive_Acc : 22.000, Run Time : 7.51 sec
INFO:root:2024-04-12 02:23:06, Train, Epoch : 6, Step : 3270, Loss : 0.35469, Acc : 0.853, Sensitive_Loss : 0.13793, Sensitive_Acc : 22.800, Run Time : 7.52 sec
INFO:root:2024-04-12 02:23:14, Train, Epoch : 6, Step : 3280, Loss : 0.40485, Acc : 0.766, Sensitive_Loss : 0.18361, Sensitive_Acc : 25.700, Run Time : 7.71 sec
INFO:root:2024-04-12 02:23:21, Train, Epoch : 6, Step : 3290, Loss : 0.40605, Acc : 0.800, Sensitive_Loss : 0.11938, Sensitive_Acc : 14.900, Run Time : 7.42 sec
INFO:root:2024-04-12 02:23:29, Train, Epoch : 6, Step : 3300, Loss : 0.39200, Acc : 0.819, Sensitive_Loss : 0.20416, Sensitive_Acc : 23.500, Run Time : 7.75 sec
INFO:root:2024-04-12 02:24:57, Dev, Step : 3300, Loss : 0.55298, Acc : 0.761, Auc : 0.847, Sensitive_Loss : 0.22055, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.847, Run Time : 88.14 sec
INFO:root:2024-04-12 02:25:03, Train, Epoch : 6, Step : 3310, Loss : 0.39843, Acc : 0.841, Sensitive_Loss : 0.17750, Sensitive_Acc : 18.700, Run Time : 93.65 sec
INFO:root:2024-04-12 02:25:10, Train, Epoch : 6, Step : 3320, Loss : 0.49099, Acc : 0.800, Sensitive_Loss : 0.14343, Sensitive_Acc : 14.500, Run Time : 7.80 sec
INFO:root:2024-04-12 02:25:18, Train, Epoch : 6, Step : 3330, Loss : 0.40823, Acc : 0.791, Sensitive_Loss : 0.16710, Sensitive_Acc : 22.300, Run Time : 7.42 sec
INFO:root:2024-04-12 02:25:26, Train, Epoch : 6, Step : 3340, Loss : 0.48449, Acc : 0.787, Sensitive_Loss : 0.15583, Sensitive_Acc : 23.800, Run Time : 8.11 sec
INFO:root:2024-04-12 02:25:33, Train, Epoch : 6, Step : 3350, Loss : 0.42525, Acc : 0.800, Sensitive_Loss : 0.16206, Sensitive_Acc : 21.900, Run Time : 7.31 sec
INFO:root:2024-04-12 02:25:41, Train, Epoch : 6, Step : 3360, Loss : 0.42767, Acc : 0.806, Sensitive_Loss : 0.19560, Sensitive_Acc : 24.600, Run Time : 7.45 sec
INFO:root:2024-04-12 02:25:48, Train, Epoch : 6, Step : 3370, Loss : 0.37576, Acc : 0.859, Sensitive_Loss : 0.19946, Sensitive_Acc : 23.000, Run Time : 7.47 sec
INFO:root:2024-04-12 02:25:56, Train, Epoch : 6, Step : 3380, Loss : 0.38651, Acc : 0.834, Sensitive_Loss : 0.18331, Sensitive_Acc : 24.300, Run Time : 7.44 sec
INFO:root:2024-04-12 02:26:03, Train, Epoch : 6, Step : 3390, Loss : 0.38135, Acc : 0.831, Sensitive_Loss : 0.12837, Sensitive_Acc : 25.100, Run Time : 7.40 sec
INFO:root:2024-04-12 02:26:10, Train, Epoch : 6, Step : 3400, Loss : 0.41539, Acc : 0.809, Sensitive_Loss : 0.17525, Sensitive_Acc : 22.900, Run Time : 7.33 sec
INFO:root:2024-04-12 02:27:38, Dev, Step : 3400, Loss : 0.52300, Acc : 0.770, Auc : 0.853, Sensitive_Loss : 0.22963, Sensitive_Acc : 20.880, Sensitive_Auc : 0.997, Mean auc: 0.853, Run Time : 87.88 sec
INFO:root:2024-04-12 02:27:44, Train, Epoch : 6, Step : 3410, Loss : 0.42533, Acc : 0.831, Sensitive_Loss : 0.11909, Sensitive_Acc : 26.400, Run Time : 93.77 sec
INFO:root:2024-04-12 02:27:51, Train, Epoch : 6, Step : 3420, Loss : 0.36020, Acc : 0.822, Sensitive_Loss : 0.12501, Sensitive_Acc : 19.700, Run Time : 7.23 sec
INFO:root:2024-04-12 02:27:59, Train, Epoch : 6, Step : 3430, Loss : 0.43472, Acc : 0.784, Sensitive_Loss : 0.14245, Sensitive_Acc : 20.400, Run Time : 7.55 sec
INFO:root:2024-04-12 02:28:06, Train, Epoch : 6, Step : 3440, Loss : 0.36101, Acc : 0.806, Sensitive_Loss : 0.14757, Sensitive_Acc : 25.300, Run Time : 7.16 sec
INFO:root:2024-04-12 02:28:14, Train, Epoch : 6, Step : 3450, Loss : 0.32813, Acc : 0.853, Sensitive_Loss : 0.14488, Sensitive_Acc : 21.800, Run Time : 7.88 sec
INFO:root:2024-04-12 02:28:21, Train, Epoch : 6, Step : 3460, Loss : 0.38735, Acc : 0.812, Sensitive_Loss : 0.13417, Sensitive_Acc : 20.700, Run Time : 7.27 sec
INFO:root:2024-04-12 02:28:29, Train, Epoch : 6, Step : 3470, Loss : 0.44653, Acc : 0.791, Sensitive_Loss : 0.14107, Sensitive_Acc : 20.400, Run Time : 8.16 sec
INFO:root:2024-04-12 02:28:37, Train, Epoch : 6, Step : 3480, Loss : 0.36961, Acc : 0.838, Sensitive_Loss : 0.16122, Sensitive_Acc : 18.700, Run Time : 7.59 sec
INFO:root:2024-04-12 02:28:44, Train, Epoch : 6, Step : 3490, Loss : 0.39889, Acc : 0.838, Sensitive_Loss : 0.23533, Sensitive_Acc : 21.200, Run Time : 7.56 sec
INFO:root:2024-04-12 02:28:52, Train, Epoch : 6, Step : 3500, Loss : 0.43048, Acc : 0.828, Sensitive_Loss : 0.13940, Sensitive_Acc : 22.500, Run Time : 7.31 sec
INFO:root:2024-04-12 02:30:20, Dev, Step : 3500, Loss : 0.52871, Acc : 0.769, Auc : 0.852, Sensitive_Loss : 0.21731, Sensitive_Acc : 21.211, Sensitive_Auc : 0.996, Mean auc: 0.852, Run Time : 88.39 sec
INFO:root:2024-04-12 02:30:26, Train, Epoch : 6, Step : 3510, Loss : 0.41866, Acc : 0.781, Sensitive_Loss : 0.13699, Sensitive_Acc : 20.300, Run Time : 93.89 sec
INFO:root:2024-04-12 02:30:33, Train, Epoch : 6, Step : 3520, Loss : 0.40904, Acc : 0.831, Sensitive_Loss : 0.19715, Sensitive_Acc : 23.000, Run Time : 7.77 sec
INFO:root:2024-04-12 02:30:41, Train, Epoch : 6, Step : 3530, Loss : 0.44303, Acc : 0.784, Sensitive_Loss : 0.10292, Sensitive_Acc : 23.100, Run Time : 7.69 sec
INFO:root:2024-04-12 02:30:48, Train, Epoch : 6, Step : 3540, Loss : 0.39723, Acc : 0.816, Sensitive_Loss : 0.19139, Sensitive_Acc : 19.900, Run Time : 7.26 sec
INFO:root:2024-04-12 02:30:56, Train, Epoch : 6, Step : 3550, Loss : 0.40424, Acc : 0.825, Sensitive_Loss : 0.10447, Sensitive_Acc : 20.000, Run Time : 7.38 sec
INFO:root:2024-04-12 02:31:03, Train, Epoch : 6, Step : 3560, Loss : 0.35587, Acc : 0.819, Sensitive_Loss : 0.17568, Sensitive_Acc : 21.900, Run Time : 7.32 sec
INFO:root:2024-04-12 02:31:11, Train, Epoch : 6, Step : 3570, Loss : 0.41785, Acc : 0.816, Sensitive_Loss : 0.18412, Sensitive_Acc : 17.900, Run Time : 7.48 sec
INFO:root:2024-04-12 02:31:18, Train, Epoch : 6, Step : 3580, Loss : 0.44288, Acc : 0.812, Sensitive_Loss : 0.09389, Sensitive_Acc : 17.800, Run Time : 7.18 sec
INFO:root:2024-04-12 02:31:25, Train, Epoch : 6, Step : 3590, Loss : 0.36107, Acc : 0.872, Sensitive_Loss : 0.14455, Sensitive_Acc : 18.000, Run Time : 7.47 sec
INFO:root:2024-04-12 02:31:32, Train, Epoch : 6, Step : 3600, Loss : 0.43066, Acc : 0.819, Sensitive_Loss : 0.20929, Sensitive_Acc : 24.300, Run Time : 7.15 sec
INFO:root:2024-04-12 02:33:02, Dev, Step : 3600, Loss : 0.51904, Acc : 0.767, Auc : 0.852, Sensitive_Loss : 0.20402, Sensitive_Acc : 21.511, Sensitive_Auc : 0.997, Mean auc: 0.852, Run Time : 89.52 sec
INFO:root:2024-04-12 02:33:07, Train, Epoch : 6, Step : 3610, Loss : 0.38703, Acc : 0.819, Sensitive_Loss : 0.16852, Sensitive_Acc : 21.100, Run Time : 95.11 sec
INFO:root:2024-04-12 02:33:15, Train, Epoch : 6, Step : 3620, Loss : 0.49803, Acc : 0.816, Sensitive_Loss : 0.19716, Sensitive_Acc : 20.800, Run Time : 7.53 sec
INFO:root:2024-04-12 02:33:23, Train, Epoch : 6, Step : 3630, Loss : 0.46479, Acc : 0.800, Sensitive_Loss : 0.15349, Sensitive_Acc : 23.900, Run Time : 8.09 sec
INFO:root:2024-04-12 02:33:31, Train, Epoch : 6, Step : 3640, Loss : 0.42376, Acc : 0.812, Sensitive_Loss : 0.10994, Sensitive_Acc : 16.700, Run Time : 8.13 sec
INFO:root:2024-04-12 02:33:39, Train, Epoch : 6, Step : 3650, Loss : 0.39186, Acc : 0.822, Sensitive_Loss : 0.21627, Sensitive_Acc : 23.400, Run Time : 7.85 sec
INFO:root:2024-04-12 02:33:47, Train, Epoch : 6, Step : 3660, Loss : 0.35784, Acc : 0.853, Sensitive_Loss : 0.17434, Sensitive_Acc : 20.400, Run Time : 8.01 sec
INFO:root:2024-04-12 02:33:55, Train, Epoch : 6, Step : 3670, Loss : 0.39602, Acc : 0.816, Sensitive_Loss : 0.16019, Sensitive_Acc : 19.100, Run Time : 7.96 sec
INFO:root:2024-04-12 02:34:02, Train, Epoch : 6, Step : 3680, Loss : 0.41689, Acc : 0.822, Sensitive_Loss : 0.13338, Sensitive_Acc : 23.300, Run Time : 7.21 sec
INFO:root:2024-04-12 02:34:10, Train, Epoch : 6, Step : 3690, Loss : 0.34145, Acc : 0.831, Sensitive_Loss : 0.15403, Sensitive_Acc : 22.000, Run Time : 7.44 sec
INFO:root:2024-04-12 02:34:18, Train, Epoch : 6, Step : 3700, Loss : 0.36888, Acc : 0.847, Sensitive_Loss : 0.15086, Sensitive_Acc : 18.100, Run Time : 8.03 sec
INFO:root:2024-04-12 02:35:49, Dev, Step : 3700, Loss : 0.51598, Acc : 0.774, Auc : 0.854, Sensitive_Loss : 0.20417, Sensitive_Acc : 20.985, Sensitive_Auc : 0.997, Mean auc: 0.854, Run Time : 91.73 sec
INFO:root:2024-04-12 02:35:55, Train, Epoch : 6, Step : 3710, Loss : 0.39462, Acc : 0.841, Sensitive_Loss : 0.17112, Sensitive_Acc : 21.900, Run Time : 97.55 sec
INFO:root:2024-04-12 02:36:03, Train, Epoch : 6, Step : 3720, Loss : 0.44202, Acc : 0.809, Sensitive_Loss : 0.10376, Sensitive_Acc : 15.300, Run Time : 8.06 sec
INFO:root:2024-04-12 02:36:12, Train, Epoch : 6, Step : 3730, Loss : 0.44136, Acc : 0.838, Sensitive_Loss : 0.15043, Sensitive_Acc : 18.700, Run Time : 8.25 sec
INFO:root:2024-04-12 02:36:20, Train, Epoch : 6, Step : 3740, Loss : 0.38037, Acc : 0.844, Sensitive_Loss : 0.13590, Sensitive_Acc : 16.900, Run Time : 8.05 sec
INFO:root:2024-04-12 02:36:27, Train, Epoch : 6, Step : 3750, Loss : 0.41000, Acc : 0.834, Sensitive_Loss : 0.18675, Sensitive_Acc : 19.100, Run Time : 7.44 sec
INFO:root:2024-04-12 02:36:35, Train, Epoch : 6, Step : 3760, Loss : 0.50134, Acc : 0.775, Sensitive_Loss : 0.15951, Sensitive_Acc : 22.800, Run Time : 8.06 sec
INFO:root:2024-04-12 02:36:43, Train, Epoch : 6, Step : 3770, Loss : 0.43550, Acc : 0.809, Sensitive_Loss : 0.17110, Sensitive_Acc : 19.400, Run Time : 7.91 sec
INFO:root:2024-04-12 02:36:51, Train, Epoch : 6, Step : 3780, Loss : 0.35682, Acc : 0.834, Sensitive_Loss : 0.19233, Sensitive_Acc : 21.000, Run Time : 7.63 sec
INFO:root:2024-04-12 02:36:58, Train, Epoch : 6, Step : 3790, Loss : 0.42643, Acc : 0.809, Sensitive_Loss : 0.14852, Sensitive_Acc : 25.100, Run Time : 7.56 sec
INFO:root:2024-04-12 02:37:06, Train, Epoch : 6, Step : 3800, Loss : 0.33005, Acc : 0.869, Sensitive_Loss : 0.17554, Sensitive_Acc : 17.700, Run Time : 7.53 sec
INFO:root:2024-04-12 02:38:34, Dev, Step : 3800, Loss : 0.54821, Acc : 0.764, Auc : 0.850, Sensitive_Loss : 0.18968, Sensitive_Acc : 21.662, Sensitive_Auc : 0.998, Mean auc: 0.850, Run Time : 88.02 sec
INFO:root:2024-04-12 02:40:01
INFO:root:y_pred: [0.14733031 0.01127449 0.04585335 ... 0.14250615 0.08823016 0.00158936]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [4.98763612e-03 8.07672564e-04 4.46049758e-04 1.56543538e-01
 1.27106711e-01 1.64926212e-04 8.74710269e-04 8.81328539e-04
 4.69737649e-02 9.99047339e-01 1.87457815e-01 1.67741348e-06
 1.20864687e-02 6.98504800e-06 9.96369123e-01 5.75655280e-03
 2.22457500e-04 9.98060882e-01 9.89483774e-01 7.16699404e-04
 8.25068712e-01 6.11438224e-07 9.18962469e-04 8.27805046e-03
 3.25106812e-04 6.26259521e-02 8.89069284e-04 4.39491014e-05
 1.75059795e-05 9.38064512e-03 2.58466415e-03 7.20687866e-01
 5.17872104e-04 5.53962648e-01 4.09972426e-06 2.73002835e-04
 1.32035193e-05 5.85909002e-04 6.42127171e-02 6.55099144e-03
 5.96546344e-02 9.66385543e-01 8.66892561e-03 9.91691832e-06
 9.52719688e-01 2.26337034e-02 1.50706798e-01 2.00228691e-01
 3.13298166e-01 9.93844748e-01 6.81597590e-01 9.98181224e-01
 9.45565164e-01 1.50863514e-07 2.01913499e-04 3.49301472e-02
 1.99494325e-03 1.80947699e-03 9.87170100e-01 4.47478757e-04
 9.81168123e-05 4.53832671e-02 1.43370042e-02 1.11536373e-07
 9.81356502e-01 1.39351087e-02 6.24585084e-07 2.03709200e-01
 1.82313249e-02 9.84458447e-01 9.99056876e-01 9.96541083e-01
 1.44968828e-06 2.96101153e-01 2.23013485e-04 3.34239244e-01
 4.53877822e-03 1.92923878e-07 1.93918386e-04 1.01734557e-04
 9.85822553e-05 2.36573210e-03 9.76056457e-01 9.47654247e-01
 1.78342592e-02 8.50722950e-04 8.81436765e-02 2.27571581e-03
 1.00267343e-02 9.87836120e-06 5.58763742e-04 3.98361459e-02
 1.35978080e-05 2.14214992e-06 4.96865017e-04 1.32393301e-03
 3.42991174e-04 8.82987857e-01 9.26373352e-04 1.47345942e-03
 6.47075921e-02 1.08481916e-02 7.40890624e-03 4.32155939e-05
 3.48056341e-03 5.12156839e-05 8.06165207e-03 5.72239220e-01
 1.91860557e-01 1.01046056e-01 1.80805928e-05 9.98852015e-01
 9.96220052e-01 8.36553340e-07 4.84595180e-01 1.89047888e-01
 2.69463807e-01 9.45232296e-06 2.35273736e-04 5.71764722e-05
 2.50154305e-02 7.91437225e-04 1.49226375e-02 4.51935603e-06
 6.15186710e-03 9.29459155e-01 2.31635639e-07 9.92064416e-01
 4.80287150e-03 1.29212961e-01 3.06555303e-05 9.89592882e-05
 1.57113689e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 02:40:01, Dev, Step : 3804, Loss : 0.55096, Acc : 0.762, Auc : 0.850, Sensitive_Loss : 0.18754, Sensitive_Acc : 21.797, Sensitive_Auc : 0.998, Mean auc: 0.850, Run Time : 86.24 sec
INFO:root:2024-04-12 02:40:08, Train, Epoch : 7, Step : 3810, Loss : 0.22612, Acc : 0.512, Sensitive_Loss : 0.07996, Sensitive_Acc : 15.600, Run Time : 5.64 sec
INFO:root:2024-04-12 02:40:15, Train, Epoch : 7, Step : 3820, Loss : 0.38066, Acc : 0.825, Sensitive_Loss : 0.17776, Sensitive_Acc : 20.100, Run Time : 6.68 sec
INFO:root:2024-04-12 02:40:22, Train, Epoch : 7, Step : 3830, Loss : 0.39388, Acc : 0.844, Sensitive_Loss : 0.17329, Sensitive_Acc : 18.500, Run Time : 7.19 sec
INFO:root:2024-04-12 02:40:29, Train, Epoch : 7, Step : 3840, Loss : 0.39045, Acc : 0.828, Sensitive_Loss : 0.10757, Sensitive_Acc : 21.400, Run Time : 7.16 sec
INFO:root:2024-04-12 02:40:36, Train, Epoch : 7, Step : 3850, Loss : 0.36498, Acc : 0.844, Sensitive_Loss : 0.21407, Sensitive_Acc : 22.800, Run Time : 6.99 sec
INFO:root:2024-04-12 02:40:43, Train, Epoch : 7, Step : 3860, Loss : 0.35433, Acc : 0.863, Sensitive_Loss : 0.14743, Sensitive_Acc : 18.600, Run Time : 7.41 sec
INFO:root:2024-04-12 02:40:50, Train, Epoch : 7, Step : 3870, Loss : 0.41930, Acc : 0.816, Sensitive_Loss : 0.11793, Sensitive_Acc : 19.400, Run Time : 6.89 sec
INFO:root:2024-04-12 02:40:57, Train, Epoch : 7, Step : 3880, Loss : 0.30506, Acc : 0.875, Sensitive_Loss : 0.12863, Sensitive_Acc : 21.700, Run Time : 7.07 sec
INFO:root:2024-04-12 02:41:05, Train, Epoch : 7, Step : 3890, Loss : 0.44612, Acc : 0.831, Sensitive_Loss : 0.16061, Sensitive_Acc : 23.900, Run Time : 7.38 sec
INFO:root:2024-04-12 02:41:11, Train, Epoch : 7, Step : 3900, Loss : 0.43347, Acc : 0.831, Sensitive_Loss : 0.12901, Sensitive_Acc : 19.500, Run Time : 6.69 sec
INFO:root:2024-04-12 02:42:39, Dev, Step : 3900, Loss : 0.52394, Acc : 0.773, Auc : 0.850, Sensitive_Loss : 0.19855, Sensitive_Acc : 21.662, Sensitive_Auc : 0.998, Mean auc: 0.850, Run Time : 87.70 sec
INFO:root:2024-04-12 02:42:45, Train, Epoch : 7, Step : 3910, Loss : 0.37034, Acc : 0.838, Sensitive_Loss : 0.15926, Sensitive_Acc : 26.200, Run Time : 93.26 sec
INFO:root:2024-04-12 02:42:52, Train, Epoch : 7, Step : 3920, Loss : 0.39814, Acc : 0.822, Sensitive_Loss : 0.16291, Sensitive_Acc : 22.900, Run Time : 7.19 sec
INFO:root:2024-04-12 02:42:59, Train, Epoch : 7, Step : 3930, Loss : 0.36118, Acc : 0.834, Sensitive_Loss : 0.14684, Sensitive_Acc : 22.000, Run Time : 7.24 sec
INFO:root:2024-04-12 02:43:07, Train, Epoch : 7, Step : 3940, Loss : 0.36590, Acc : 0.828, Sensitive_Loss : 0.18184, Sensitive_Acc : 25.300, Run Time : 7.55 sec
INFO:root:2024-04-12 02:43:14, Train, Epoch : 7, Step : 3950, Loss : 0.41819, Acc : 0.825, Sensitive_Loss : 0.19134, Sensitive_Acc : 17.700, Run Time : 7.11 sec
INFO:root:2024-04-12 02:43:20, Train, Epoch : 7, Step : 3960, Loss : 0.37086, Acc : 0.834, Sensitive_Loss : 0.15573, Sensitive_Acc : 18.600, Run Time : 6.32 sec
INFO:root:2024-04-12 02:43:28, Train, Epoch : 7, Step : 3970, Loss : 0.34212, Acc : 0.834, Sensitive_Loss : 0.15184, Sensitive_Acc : 19.200, Run Time : 7.49 sec
INFO:root:2024-04-12 02:43:34, Train, Epoch : 7, Step : 3980, Loss : 0.40342, Acc : 0.822, Sensitive_Loss : 0.14848, Sensitive_Acc : 19.600, Run Time : 6.73 sec
INFO:root:2024-04-12 02:43:41, Train, Epoch : 7, Step : 3990, Loss : 0.37914, Acc : 0.844, Sensitive_Loss : 0.14226, Sensitive_Acc : 20.200, Run Time : 7.11 sec
INFO:root:2024-04-12 02:43:49, Train, Epoch : 7, Step : 4000, Loss : 0.38222, Acc : 0.809, Sensitive_Loss : 0.11124, Sensitive_Acc : 25.000, Run Time : 7.21 sec
INFO:root:2024-04-12 02:45:16, Dev, Step : 4000, Loss : 0.52474, Acc : 0.772, Auc : 0.852, Sensitive_Loss : 0.18719, Sensitive_Acc : 21.662, Sensitive_Auc : 0.997, Mean auc: 0.852, Run Time : 87.93 sec
INFO:root:2024-04-12 02:45:22, Train, Epoch : 7, Step : 4010, Loss : 0.44282, Acc : 0.812, Sensitive_Loss : 0.15331, Sensitive_Acc : 22.900, Run Time : 93.38 sec
INFO:root:2024-04-12 02:45:29, Train, Epoch : 7, Step : 4020, Loss : 0.39727, Acc : 0.831, Sensitive_Loss : 0.13192, Sensitive_Acc : 19.500, Run Time : 7.21 sec
INFO:root:2024-04-12 02:45:36, Train, Epoch : 7, Step : 4030, Loss : 0.38000, Acc : 0.822, Sensitive_Loss : 0.10269, Sensitive_Acc : 23.300, Run Time : 7.16 sec
INFO:root:2024-04-12 02:45:43, Train, Epoch : 7, Step : 4040, Loss : 0.38824, Acc : 0.828, Sensitive_Loss : 0.13530, Sensitive_Acc : 22.300, Run Time : 6.97 sec
INFO:root:2024-04-12 02:45:51, Train, Epoch : 7, Step : 4050, Loss : 0.32479, Acc : 0.841, Sensitive_Loss : 0.18844, Sensitive_Acc : 18.700, Run Time : 7.63 sec
INFO:root:2024-04-12 02:45:58, Train, Epoch : 7, Step : 4060, Loss : 0.39666, Acc : 0.825, Sensitive_Loss : 0.14616, Sensitive_Acc : 22.000, Run Time : 6.87 sec
INFO:root:2024-04-12 02:46:05, Train, Epoch : 7, Step : 4070, Loss : 0.43220, Acc : 0.806, Sensitive_Loss : 0.23842, Sensitive_Acc : 22.500, Run Time : 7.34 sec
INFO:root:2024-04-12 02:46:12, Train, Epoch : 7, Step : 4080, Loss : 0.39950, Acc : 0.819, Sensitive_Loss : 0.14935, Sensitive_Acc : 19.000, Run Time : 7.28 sec
INFO:root:2024-04-12 02:46:19, Train, Epoch : 7, Step : 4090, Loss : 0.40927, Acc : 0.816, Sensitive_Loss : 0.10557, Sensitive_Acc : 22.800, Run Time : 7.06 sec
INFO:root:2024-04-12 02:46:26, Train, Epoch : 7, Step : 4100, Loss : 0.39782, Acc : 0.850, Sensitive_Loss : 0.15036, Sensitive_Acc : 20.600, Run Time : 6.73 sec
INFO:root:2024-04-12 02:47:54, Dev, Step : 4100, Loss : 0.51946, Acc : 0.771, Auc : 0.851, Sensitive_Loss : 0.19203, Sensitive_Acc : 21.662, Sensitive_Auc : 0.997, Mean auc: 0.851, Run Time : 87.59 sec
INFO:root:2024-04-12 02:47:59, Train, Epoch : 7, Step : 4110, Loss : 0.39132, Acc : 0.838, Sensitive_Loss : 0.14050, Sensitive_Acc : 25.900, Run Time : 93.06 sec
INFO:root:2024-04-12 02:48:06, Train, Epoch : 7, Step : 4120, Loss : 0.39967, Acc : 0.825, Sensitive_Loss : 0.12257, Sensitive_Acc : 15.800, Run Time : 7.17 sec
INFO:root:2024-04-12 02:48:14, Train, Epoch : 7, Step : 4130, Loss : 0.39334, Acc : 0.809, Sensitive_Loss : 0.13155, Sensitive_Acc : 16.300, Run Time : 7.51 sec
INFO:root:2024-04-12 02:48:21, Train, Epoch : 7, Step : 4140, Loss : 0.42326, Acc : 0.791, Sensitive_Loss : 0.14912, Sensitive_Acc : 21.800, Run Time : 7.12 sec
INFO:root:2024-04-12 02:48:28, Train, Epoch : 7, Step : 4150, Loss : 0.43185, Acc : 0.819, Sensitive_Loss : 0.12696, Sensitive_Acc : 17.800, Run Time : 6.85 sec
INFO:root:2024-04-12 02:48:35, Train, Epoch : 7, Step : 4160, Loss : 0.35430, Acc : 0.866, Sensitive_Loss : 0.10488, Sensitive_Acc : 23.600, Run Time : 7.39 sec
INFO:root:2024-04-12 02:48:42, Train, Epoch : 7, Step : 4170, Loss : 0.35851, Acc : 0.841, Sensitive_Loss : 0.18217, Sensitive_Acc : 26.600, Run Time : 7.08 sec
INFO:root:2024-04-12 02:48:49, Train, Epoch : 7, Step : 4180, Loss : 0.40542, Acc : 0.831, Sensitive_Loss : 0.13783, Sensitive_Acc : 21.600, Run Time : 6.42 sec
INFO:root:2024-04-12 02:48:56, Train, Epoch : 7, Step : 4190, Loss : 0.38417, Acc : 0.831, Sensitive_Loss : 0.18418, Sensitive_Acc : 25.500, Run Time : 7.37 sec
INFO:root:2024-04-12 02:49:03, Train, Epoch : 7, Step : 4200, Loss : 0.38715, Acc : 0.822, Sensitive_Loss : 0.10421, Sensitive_Acc : 22.400, Run Time : 6.94 sec
INFO:root:2024-04-12 02:50:31, Dev, Step : 4200, Loss : 0.51920, Acc : 0.770, Auc : 0.850, Sensitive_Loss : 0.21010, Sensitive_Acc : 21.571, Sensitive_Auc : 0.997, Mean auc: 0.850, Run Time : 88.35 sec
INFO:root:2024-04-12 02:50:37, Train, Epoch : 7, Step : 4210, Loss : 0.39526, Acc : 0.831, Sensitive_Loss : 0.11406, Sensitive_Acc : 20.600, Run Time : 94.30 sec
INFO:root:2024-04-12 02:50:44, Train, Epoch : 7, Step : 4220, Loss : 0.36744, Acc : 0.841, Sensitive_Loss : 0.14460, Sensitive_Acc : 21.500, Run Time : 7.03 sec
INFO:root:2024-04-12 02:50:51, Train, Epoch : 7, Step : 4230, Loss : 0.36421, Acc : 0.825, Sensitive_Loss : 0.14224, Sensitive_Acc : 22.000, Run Time : 6.42 sec
INFO:root:2024-04-12 02:50:58, Train, Epoch : 7, Step : 4240, Loss : 0.40021, Acc : 0.828, Sensitive_Loss : 0.12905, Sensitive_Acc : 17.700, Run Time : 7.20 sec
INFO:root:2024-04-12 02:51:06, Train, Epoch : 7, Step : 4250, Loss : 0.41497, Acc : 0.806, Sensitive_Loss : 0.17018, Sensitive_Acc : 22.800, Run Time : 7.49 sec
INFO:root:2024-04-12 02:51:13, Train, Epoch : 7, Step : 4260, Loss : 0.41942, Acc : 0.825, Sensitive_Loss : 0.18328, Sensitive_Acc : 20.600, Run Time : 7.28 sec
INFO:root:2024-04-12 02:51:20, Train, Epoch : 7, Step : 4270, Loss : 0.34597, Acc : 0.859, Sensitive_Loss : 0.21175, Sensitive_Acc : 18.900, Run Time : 6.82 sec
INFO:root:2024-04-12 02:51:27, Train, Epoch : 7, Step : 4280, Loss : 0.37744, Acc : 0.844, Sensitive_Loss : 0.13450, Sensitive_Acc : 18.500, Run Time : 7.32 sec
INFO:root:2024-04-12 02:51:34, Train, Epoch : 7, Step : 4290, Loss : 0.41786, Acc : 0.822, Sensitive_Loss : 0.16210, Sensitive_Acc : 16.600, Run Time : 7.30 sec
INFO:root:2024-04-12 02:51:41, Train, Epoch : 7, Step : 4300, Loss : 0.39624, Acc : 0.825, Sensitive_Loss : 0.13487, Sensitive_Acc : 24.600, Run Time : 6.63 sec
INFO:root:2024-04-12 02:53:09, Dev, Step : 4300, Loss : 0.52026, Acc : 0.774, Auc : 0.852, Sensitive_Loss : 0.21231, Sensitive_Acc : 21.451, Sensitive_Auc : 0.996, Mean auc: 0.852, Run Time : 87.77 sec
INFO:root:2024-04-12 02:53:15, Train, Epoch : 7, Step : 4310, Loss : 0.39908, Acc : 0.838, Sensitive_Loss : 0.15102, Sensitive_Acc : 25.200, Run Time : 93.93 sec
INFO:root:2024-04-12 02:53:22, Train, Epoch : 7, Step : 4320, Loss : 0.35499, Acc : 0.822, Sensitive_Loss : 0.11372, Sensitive_Acc : 22.100, Run Time : 6.75 sec
INFO:root:2024-04-12 02:53:29, Train, Epoch : 7, Step : 4330, Loss : 0.37017, Acc : 0.816, Sensitive_Loss : 0.14295, Sensitive_Acc : 21.800, Run Time : 7.26 sec
INFO:root:2024-04-12 02:53:36, Train, Epoch : 7, Step : 4340, Loss : 0.38056, Acc : 0.816, Sensitive_Loss : 0.15520, Sensitive_Acc : 23.400, Run Time : 7.01 sec
INFO:root:2024-04-12 02:53:43, Train, Epoch : 7, Step : 4350, Loss : 0.41647, Acc : 0.834, Sensitive_Loss : 0.17012, Sensitive_Acc : 20.000, Run Time : 6.92 sec
INFO:root:2024-04-12 02:53:50, Train, Epoch : 7, Step : 4360, Loss : 0.36122, Acc : 0.841, Sensitive_Loss : 0.13290, Sensitive_Acc : 23.200, Run Time : 7.01 sec
INFO:root:2024-04-12 02:53:57, Train, Epoch : 7, Step : 4370, Loss : 0.33376, Acc : 0.856, Sensitive_Loss : 0.15674, Sensitive_Acc : 21.700, Run Time : 7.14 sec
INFO:root:2024-04-12 02:54:04, Train, Epoch : 7, Step : 4380, Loss : 0.38326, Acc : 0.844, Sensitive_Loss : 0.13508, Sensitive_Acc : 14.300, Run Time : 6.83 sec
INFO:root:2024-04-12 02:54:11, Train, Epoch : 7, Step : 4390, Loss : 0.37914, Acc : 0.831, Sensitive_Loss : 0.14339, Sensitive_Acc : 22.500, Run Time : 7.40 sec
INFO:root:2024-04-12 02:54:18, Train, Epoch : 7, Step : 4400, Loss : 0.44209, Acc : 0.828, Sensitive_Loss : 0.12082, Sensitive_Acc : 16.200, Run Time : 7.17 sec
INFO:root:2024-04-12 02:55:47, Dev, Step : 4400, Loss : 0.52666, Acc : 0.770, Auc : 0.851, Sensitive_Loss : 0.20284, Sensitive_Acc : 21.692, Sensitive_Auc : 0.995, Mean auc: 0.851, Run Time : 88.60 sec
INFO:root:2024-04-12 02:55:53, Train, Epoch : 7, Step : 4410, Loss : 0.34219, Acc : 0.838, Sensitive_Loss : 0.09191, Sensitive_Acc : 23.600, Run Time : 94.45 sec
INFO:root:2024-04-12 02:56:00, Train, Epoch : 7, Step : 4420, Loss : 0.44186, Acc : 0.769, Sensitive_Loss : 0.11639, Sensitive_Acc : 17.600, Run Time : 7.01 sec
INFO:root:2024-04-12 02:56:07, Train, Epoch : 7, Step : 4430, Loss : 0.38715, Acc : 0.806, Sensitive_Loss : 0.13140, Sensitive_Acc : 24.300, Run Time : 7.11 sec
INFO:root:2024-04-12 02:57:39
INFO:root:y_pred: [0.19797476 0.01634005 0.12056237 ... 0.25991657 0.19385083 0.00210492]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [9.8805735e-04 2.4160798e-04 2.9628968e-04 1.9602905e-01 9.3698449e-02
 1.9463396e-04 1.0418419e-03 9.1218209e-04 2.2656620e-02 9.9895608e-01
 2.3897749e-01 2.6421726e-06 7.4673872e-03 4.9934652e-06 9.9732059e-01
 1.5362644e-02 7.0257671e-04 9.9843067e-01 9.9406826e-01 7.7956554e-04
 7.0014983e-01 6.6707315e-08 9.2421513e-04 1.4749227e-02 1.7104133e-04
 4.9471147e-02 7.4614381e-04 6.6189728e-05 2.6616599e-05 1.3636335e-02
 5.6629200e-03 8.7102640e-01 2.0504657e-03 5.4656136e-01 4.6303603e-06
 1.1209730e-04 4.8404977e-06 6.4222403e-03 2.4753726e-01 3.3738532e-03
 9.9355243e-02 9.6859211e-01 2.3006693e-02 1.9203906e-06 9.3270004e-01
 3.7808889e-03 2.6655546e-01 3.0590066e-01 1.2736492e-01 9.9637610e-01
 7.8734601e-01 9.9913651e-01 9.6160042e-01 1.6492089e-07 1.7365438e-04
 3.9244555e-02 2.1135602e-03 9.6408004e-04 9.9419665e-01 7.5990742e-04
 1.3287774e-04 4.7423534e-02 2.0640792e-02 1.9628699e-08 9.8252791e-01
 1.1594645e-02 3.6175913e-07 1.6852891e-01 2.4606639e-03 9.8105145e-01
 9.9922466e-01 9.9634355e-01 1.6393642e-06 5.0177395e-01 2.2542452e-04
 4.6586829e-01 1.5343877e-03 4.0204139e-07 5.5003497e-05 1.9064284e-05
 1.9418927e-04 2.3067729e-03 9.8771149e-01 9.3938875e-01 1.6216459e-02
 2.1152988e-04 7.3780492e-02 3.7401291e-03 5.3007356e-03 1.5731064e-05
 3.2526586e-04 9.2274591e-02 2.5768500e-06 3.6569617e-07 2.3829074e-04
 1.3855016e-03 3.3445869e-04 9.2389196e-01 8.5296319e-04 2.4053233e-03
 5.5669099e-03 2.4129177e-02 5.2881567e-03 1.5973381e-05 6.2627196e-03
 9.7381468e-05 2.6912579e-02 6.6526473e-01 1.4325042e-01 2.2306134e-01
 2.8728122e-05 9.9929750e-01 9.9594682e-01 1.0889156e-07 6.7927885e-01
 1.0849212e-01 6.7696892e-02 1.6586411e-06 1.6665943e-03 1.5757550e-05
 1.7530011e-02 5.0041219e-04 6.1337333e-03 8.1597693e-07 1.3389176e-02
 9.5372260e-01 5.8242449e-08 9.8918021e-01 9.1516776e-03 1.0562836e-01
 3.9285096e-06 2.4372547e-04 9.5927226e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 02:57:39, Dev, Step : 4438, Loss : 0.51983, Acc : 0.775, Auc : 0.854, Sensitive_Loss : 0.18643, Sensitive_Acc : 21.571, Sensitive_Auc : 0.997, Mean auc: 0.854, Run Time : 87.18 sec
INFO:root:2024-04-12 02:57:43, Train, Epoch : 8, Step : 4440, Loss : 0.07264, Acc : 0.175, Sensitive_Loss : 0.03389, Sensitive_Acc : 3.400, Run Time : 2.59 sec
INFO:root:2024-04-12 02:57:50, Train, Epoch : 8, Step : 4450, Loss : 0.35789, Acc : 0.847, Sensitive_Loss : 0.09149, Sensitive_Acc : 23.500, Run Time : 7.24 sec
INFO:root:2024-04-12 02:57:57, Train, Epoch : 8, Step : 4460, Loss : 0.35022, Acc : 0.869, Sensitive_Loss : 0.13857, Sensitive_Acc : 21.600, Run Time : 6.61 sec
INFO:root:2024-04-12 02:58:04, Train, Epoch : 8, Step : 4470, Loss : 0.34501, Acc : 0.822, Sensitive_Loss : 0.14324, Sensitive_Acc : 20.600, Run Time : 7.11 sec
INFO:root:2024-04-12 02:58:11, Train, Epoch : 8, Step : 4480, Loss : 0.38445, Acc : 0.838, Sensitive_Loss : 0.14821, Sensitive_Acc : 21.300, Run Time : 7.23 sec
INFO:root:2024-04-12 02:58:18, Train, Epoch : 8, Step : 4490, Loss : 0.42871, Acc : 0.825, Sensitive_Loss : 0.11890, Sensitive_Acc : 21.800, Run Time : 6.96 sec
INFO:root:2024-04-12 02:58:26, Train, Epoch : 8, Step : 4500, Loss : 0.36407, Acc : 0.847, Sensitive_Loss : 0.20322, Sensitive_Acc : 17.400, Run Time : 7.84 sec
INFO:root:2024-04-12 02:59:53, Dev, Step : 4500, Loss : 0.51460, Acc : 0.779, Auc : 0.855, Sensitive_Loss : 0.18165, Sensitive_Acc : 21.782, Sensitive_Auc : 0.997, Mean auc: 0.855, Run Time : 87.53 sec
INFO:root:2024-04-12 02:59:58, Train, Epoch : 8, Step : 4510, Loss : 0.30358, Acc : 0.844, Sensitive_Loss : 0.14998, Sensitive_Acc : 25.100, Run Time : 92.74 sec
INFO:root:2024-04-12 03:00:06, Train, Epoch : 8, Step : 4520, Loss : 0.31832, Acc : 0.856, Sensitive_Loss : 0.12680, Sensitive_Acc : 20.000, Run Time : 7.29 sec
INFO:root:2024-04-12 03:00:13, Train, Epoch : 8, Step : 4530, Loss : 0.41251, Acc : 0.841, Sensitive_Loss : 0.14133, Sensitive_Acc : 19.100, Run Time : 7.02 sec
INFO:root:2024-04-12 03:00:20, Train, Epoch : 8, Step : 4540, Loss : 0.35589, Acc : 0.841, Sensitive_Loss : 0.15788, Sensitive_Acc : 19.400, Run Time : 7.04 sec
INFO:root:2024-04-12 03:00:27, Train, Epoch : 8, Step : 4550, Loss : 0.37369, Acc : 0.850, Sensitive_Loss : 0.09187, Sensitive_Acc : 23.100, Run Time : 7.49 sec
INFO:root:2024-04-12 03:00:34, Train, Epoch : 8, Step : 4560, Loss : 0.32606, Acc : 0.844, Sensitive_Loss : 0.12519, Sensitive_Acc : 18.800, Run Time : 7.08 sec
INFO:root:2024-04-12 03:00:41, Train, Epoch : 8, Step : 4570, Loss : 0.34816, Acc : 0.859, Sensitive_Loss : 0.15608, Sensitive_Acc : 23.600, Run Time : 6.81 sec
INFO:root:2024-04-12 03:00:49, Train, Epoch : 8, Step : 4580, Loss : 0.34874, Acc : 0.856, Sensitive_Loss : 0.12033, Sensitive_Acc : 25.400, Run Time : 7.36 sec
INFO:root:2024-04-12 03:00:55, Train, Epoch : 8, Step : 4590, Loss : 0.30027, Acc : 0.859, Sensitive_Loss : 0.08485, Sensitive_Acc : 23.900, Run Time : 6.83 sec
INFO:root:2024-04-12 03:01:02, Train, Epoch : 8, Step : 4600, Loss : 0.35507, Acc : 0.850, Sensitive_Loss : 0.12935, Sensitive_Acc : 24.400, Run Time : 6.96 sec
INFO:root:2024-04-12 03:02:30, Dev, Step : 4600, Loss : 0.52876, Acc : 0.776, Auc : 0.855, Sensitive_Loss : 0.18691, Sensitive_Acc : 21.571, Sensitive_Auc : 0.997, Mean auc: 0.855, Run Time : 88.10 sec
INFO:root:2024-04-12 03:02:36, Train, Epoch : 8, Step : 4610, Loss : 0.41936, Acc : 0.844, Sensitive_Loss : 0.09073, Sensitive_Acc : 22.000, Run Time : 93.79 sec
INFO:root:2024-04-12 03:02:44, Train, Epoch : 8, Step : 4620, Loss : 0.39822, Acc : 0.831, Sensitive_Loss : 0.13077, Sensitive_Acc : 23.100, Run Time : 7.44 sec
INFO:root:2024-04-12 03:02:50, Train, Epoch : 8, Step : 4630, Loss : 0.39705, Acc : 0.809, Sensitive_Loss : 0.13493, Sensitive_Acc : 22.400, Run Time : 6.68 sec
INFO:root:2024-04-12 03:02:58, Train, Epoch : 8, Step : 4640, Loss : 0.35489, Acc : 0.819, Sensitive_Loss : 0.13235, Sensitive_Acc : 19.700, Run Time : 7.67 sec
INFO:root:2024-04-12 03:03:04, Train, Epoch : 8, Step : 4650, Loss : 0.31653, Acc : 0.856, Sensitive_Loss : 0.11528, Sensitive_Acc : 24.200, Run Time : 6.52 sec
INFO:root:2024-04-12 03:03:12, Train, Epoch : 8, Step : 4660, Loss : 0.37140, Acc : 0.822, Sensitive_Loss : 0.14682, Sensitive_Acc : 24.200, Run Time : 7.17 sec
INFO:root:2024-04-12 03:03:18, Train, Epoch : 8, Step : 4670, Loss : 0.30750, Acc : 0.847, Sensitive_Loss : 0.12918, Sensitive_Acc : 19.900, Run Time : 6.62 sec
INFO:root:2024-04-12 03:03:25, Train, Epoch : 8, Step : 4680, Loss : 0.43186, Acc : 0.794, Sensitive_Loss : 0.17114, Sensitive_Acc : 21.000, Run Time : 7.25 sec
INFO:root:2024-04-12 03:03:33, Train, Epoch : 8, Step : 4690, Loss : 0.33417, Acc : 0.844, Sensitive_Loss : 0.16989, Sensitive_Acc : 15.600, Run Time : 7.21 sec
INFO:root:2024-04-12 03:03:40, Train, Epoch : 8, Step : 4700, Loss : 0.37920, Acc : 0.847, Sensitive_Loss : 0.16504, Sensitive_Acc : 20.700, Run Time : 7.20 sec
INFO:root:2024-04-12 03:05:08, Dev, Step : 4700, Loss : 0.54104, Acc : 0.772, Auc : 0.847, Sensitive_Loss : 0.19517, Sensitive_Acc : 21.571, Sensitive_Auc : 0.998, Mean auc: 0.847, Run Time : 87.68 sec
INFO:root:2024-04-12 03:05:13, Train, Epoch : 8, Step : 4710, Loss : 0.37329, Acc : 0.812, Sensitive_Loss : 0.20109, Sensitive_Acc : 22.900, Run Time : 93.13 sec
INFO:root:2024-04-12 03:05:20, Train, Epoch : 8, Step : 4720, Loss : 0.31829, Acc : 0.878, Sensitive_Loss : 0.19493, Sensitive_Acc : 20.600, Run Time : 6.96 sec
INFO:root:2024-04-12 03:05:27, Train, Epoch : 8, Step : 4730, Loss : 0.35089, Acc : 0.850, Sensitive_Loss : 0.14128, Sensitive_Acc : 24.300, Run Time : 7.14 sec
INFO:root:2024-04-12 03:05:34, Train, Epoch : 8, Step : 4740, Loss : 0.33646, Acc : 0.825, Sensitive_Loss : 0.15594, Sensitive_Acc : 21.800, Run Time : 6.96 sec
INFO:root:2024-04-12 03:05:41, Train, Epoch : 8, Step : 4750, Loss : 0.38680, Acc : 0.812, Sensitive_Loss : 0.18101, Sensitive_Acc : 21.200, Run Time : 7.29 sec
INFO:root:2024-04-12 03:05:48, Train, Epoch : 8, Step : 4760, Loss : 0.35562, Acc : 0.863, Sensitive_Loss : 0.10521, Sensitive_Acc : 23.100, Run Time : 6.87 sec
INFO:root:2024-04-12 03:05:56, Train, Epoch : 8, Step : 4770, Loss : 0.38024, Acc : 0.834, Sensitive_Loss : 0.12820, Sensitive_Acc : 18.400, Run Time : 7.73 sec
INFO:root:2024-04-12 03:06:03, Train, Epoch : 8, Step : 4780, Loss : 0.32153, Acc : 0.875, Sensitive_Loss : 0.12967, Sensitive_Acc : 23.800, Run Time : 6.90 sec
INFO:root:2024-04-12 03:06:10, Train, Epoch : 8, Step : 4790, Loss : 0.34963, Acc : 0.859, Sensitive_Loss : 0.13536, Sensitive_Acc : 19.700, Run Time : 7.14 sec
INFO:root:2024-04-12 03:06:17, Train, Epoch : 8, Step : 4800, Loss : 0.38672, Acc : 0.859, Sensitive_Loss : 0.17585, Sensitive_Acc : 17.100, Run Time : 7.01 sec
INFO:root:2024-04-12 03:07:45, Dev, Step : 4800, Loss : 0.52728, Acc : 0.777, Auc : 0.852, Sensitive_Loss : 0.19750, Sensitive_Acc : 21.451, Sensitive_Auc : 0.997, Mean auc: 0.852, Run Time : 87.57 sec
INFO:root:2024-04-12 03:07:50, Train, Epoch : 8, Step : 4810, Loss : 0.38623, Acc : 0.831, Sensitive_Loss : 0.10912, Sensitive_Acc : 24.300, Run Time : 93.31 sec
INFO:root:2024-04-12 03:07:57, Train, Epoch : 8, Step : 4820, Loss : 0.34818, Acc : 0.850, Sensitive_Loss : 0.14903, Sensitive_Acc : 24.000, Run Time : 6.85 sec
INFO:root:2024-04-12 03:08:05, Train, Epoch : 8, Step : 4830, Loss : 0.35840, Acc : 0.866, Sensitive_Loss : 0.13109, Sensitive_Acc : 24.500, Run Time : 7.38 sec
INFO:root:2024-04-12 03:08:12, Train, Epoch : 8, Step : 4840, Loss : 0.39625, Acc : 0.844, Sensitive_Loss : 0.12667, Sensitive_Acc : 24.100, Run Time : 7.34 sec
INFO:root:2024-04-12 03:08:19, Train, Epoch : 8, Step : 4850, Loss : 0.39637, Acc : 0.812, Sensitive_Loss : 0.14249, Sensitive_Acc : 22.200, Run Time : 7.19 sec
INFO:root:2024-04-12 03:08:26, Train, Epoch : 8, Step : 4860, Loss : 0.43791, Acc : 0.822, Sensitive_Loss : 0.14205, Sensitive_Acc : 20.200, Run Time : 7.02 sec
INFO:root:2024-04-12 03:08:33, Train, Epoch : 8, Step : 4870, Loss : 0.41393, Acc : 0.812, Sensitive_Loss : 0.08166, Sensitive_Acc : 20.100, Run Time : 7.26 sec
INFO:root:2024-04-12 03:08:41, Train, Epoch : 8, Step : 4880, Loss : 0.38054, Acc : 0.831, Sensitive_Loss : 0.16569, Sensitive_Acc : 20.000, Run Time : 7.21 sec
INFO:root:2024-04-12 03:08:47, Train, Epoch : 8, Step : 4890, Loss : 0.39286, Acc : 0.831, Sensitive_Loss : 0.15286, Sensitive_Acc : 20.600, Run Time : 6.73 sec
INFO:root:2024-04-12 03:08:55, Train, Epoch : 8, Step : 4900, Loss : 0.35957, Acc : 0.859, Sensitive_Loss : 0.15268, Sensitive_Acc : 23.400, Run Time : 7.37 sec
INFO:root:2024-04-12 03:10:23, Dev, Step : 4900, Loss : 0.52663, Acc : 0.774, Auc : 0.852, Sensitive_Loss : 0.18061, Sensitive_Acc : 21.571, Sensitive_Auc : 0.996, Mean auc: 0.852, Run Time : 88.51 sec
INFO:root:2024-04-12 03:10:29, Train, Epoch : 8, Step : 4910, Loss : 0.30688, Acc : 0.866, Sensitive_Loss : 0.11920, Sensitive_Acc : 20.900, Run Time : 94.43 sec
INFO:root:2024-04-12 03:10:36, Train, Epoch : 8, Step : 4920, Loss : 0.41454, Acc : 0.822, Sensitive_Loss : 0.13842, Sensitive_Acc : 24.300, Run Time : 7.30 sec
INFO:root:2024-04-12 03:10:43, Train, Epoch : 8, Step : 4930, Loss : 0.41050, Acc : 0.850, Sensitive_Loss : 0.13299, Sensitive_Acc : 23.100, Run Time : 6.99 sec
INFO:root:2024-04-12 03:10:51, Train, Epoch : 8, Step : 4940, Loss : 0.39113, Acc : 0.844, Sensitive_Loss : 0.14973, Sensitive_Acc : 20.100, Run Time : 7.34 sec
INFO:root:2024-04-12 03:10:57, Train, Epoch : 8, Step : 4950, Loss : 0.35496, Acc : 0.853, Sensitive_Loss : 0.15593, Sensitive_Acc : 23.700, Run Time : 6.70 sec
INFO:root:2024-04-12 03:11:04, Train, Epoch : 8, Step : 4960, Loss : 0.45409, Acc : 0.803, Sensitive_Loss : 0.17817, Sensitive_Acc : 25.300, Run Time : 7.06 sec
INFO:root:2024-04-12 03:11:12, Train, Epoch : 8, Step : 4970, Loss : 0.38106, Acc : 0.853, Sensitive_Loss : 0.13275, Sensitive_Acc : 21.500, Run Time : 7.29 sec
INFO:root:2024-04-12 03:11:19, Train, Epoch : 8, Step : 4980, Loss : 0.38500, Acc : 0.819, Sensitive_Loss : 0.08323, Sensitive_Acc : 23.600, Run Time : 7.24 sec
INFO:root:2024-04-12 03:11:26, Train, Epoch : 8, Step : 4990, Loss : 0.37936, Acc : 0.834, Sensitive_Loss : 0.13397, Sensitive_Acc : 23.300, Run Time : 6.69 sec
INFO:root:2024-04-12 03:11:33, Train, Epoch : 8, Step : 5000, Loss : 0.39735, Acc : 0.803, Sensitive_Loss : 0.11905, Sensitive_Acc : 22.700, Run Time : 7.29 sec
INFO:root:2024-04-12 03:13:01, Dev, Step : 5000, Loss : 0.53766, Acc : 0.771, Auc : 0.851, Sensitive_Loss : 0.17989, Sensitive_Acc : 21.571, Sensitive_Auc : 0.997, Mean auc: 0.851, Run Time : 88.04 sec
INFO:root:2024-04-12 03:13:07, Train, Epoch : 8, Step : 5010, Loss : 0.37683, Acc : 0.841, Sensitive_Loss : 0.16559, Sensitive_Acc : 24.000, Run Time : 93.65 sec
INFO:root:2024-04-12 03:13:14, Train, Epoch : 8, Step : 5020, Loss : 0.32413, Acc : 0.866, Sensitive_Loss : 0.07946, Sensitive_Acc : 21.000, Run Time : 7.09 sec
INFO:root:2024-04-12 03:13:21, Train, Epoch : 8, Step : 5030, Loss : 0.31424, Acc : 0.856, Sensitive_Loss : 0.16250, Sensitive_Acc : 20.500, Run Time : 7.42 sec
INFO:root:2024-04-12 03:13:28, Train, Epoch : 8, Step : 5040, Loss : 0.40147, Acc : 0.850, Sensitive_Loss : 0.16314, Sensitive_Acc : 19.500, Run Time : 6.75 sec
INFO:root:2024-04-12 03:13:35, Train, Epoch : 8, Step : 5050, Loss : 0.34722, Acc : 0.859, Sensitive_Loss : 0.14694, Sensitive_Acc : 20.400, Run Time : 6.95 sec
INFO:root:2024-04-12 03:13:42, Train, Epoch : 8, Step : 5060, Loss : 0.37849, Acc : 0.831, Sensitive_Loss : 0.14853, Sensitive_Acc : 18.200, Run Time : 7.28 sec
INFO:root:2024-04-12 03:13:49, Train, Epoch : 8, Step : 5070, Loss : 0.34232, Acc : 0.850, Sensitive_Loss : 0.12164, Sensitive_Acc : 20.400, Run Time : 7.27 sec
INFO:root:2024-04-12 03:15:18
INFO:root:y_pred: [0.18103069 0.01482306 0.04156364 ... 0.1555988  0.08847262 0.00149252]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.24397203e-02 3.40947387e-04 5.23202214e-03 2.74173021e-01
 1.31154403e-01 2.05498320e-04 8.35627958e-04 3.88476212e-04
 1.42797962e-01 9.99287903e-01 4.38682646e-01 2.17174988e-06
 1.32534066e-02 8.58283875e-06 9.98076677e-01 8.40097666e-03
 2.34121093e-04 9.98932779e-01 9.90362048e-01 4.50798823e-03
 8.52605045e-01 9.94910692e-07 5.33177052e-04 9.61678848e-03
 2.91214266e-04 2.04777882e-01 8.63895460e-04 1.09749963e-04
 8.18511307e-06 1.36633173e-01 5.19113103e-03 9.29394841e-01
 2.40541366e-03 6.23792171e-01 5.71723467e-06 1.29110980e-04
 9.87176245e-05 9.42902057e-04 1.69177786e-01 7.29864091e-03
 1.64769039e-01 9.71150398e-01 3.89197865e-03 4.78116890e-05
 9.71586645e-01 7.56289717e-03 1.72475860e-01 1.78458318e-01
 1.86200052e-01 9.97573912e-01 8.91946793e-01 9.99351203e-01
 9.49493945e-01 9.37422797e-08 1.39982949e-04 1.04691885e-01
 3.97585426e-03 1.83414039e-03 9.91122007e-01 5.33518381e-04
 1.40321208e-04 7.15416074e-02 4.91011925e-02 1.60129314e-08
 9.89695549e-01 2.96334513e-02 3.11863187e-06 3.10619682e-01
 1.09719962e-01 9.90908563e-01 9.99357164e-01 9.97546613e-01
 1.59399713e-06 6.31202161e-01 2.59075459e-04 4.67128098e-01
 7.58908316e-03 3.16488467e-07 3.49887297e-04 6.73717732e-05
 2.88605341e-04 2.84020067e-03 9.89116728e-01 9.47498918e-01
 2.21556872e-02 7.00206787e-04 1.31188631e-01 1.85843557e-02
 2.49280618e-03 3.81332473e-04 1.83394208e-04 1.01455569e-01
 5.14700878e-06 4.58978462e-07 7.69502600e-04 2.50906572e-02
 1.14562572e-03 9.48199928e-01 6.51546754e-03 1.28570041e-02
 4.14734632e-02 2.87283529e-02 1.04041211e-02 3.32087802e-05
 6.67131413e-03 1.68806200e-05 7.92124793e-02 6.68885410e-01
 3.39962572e-01 1.32701725e-01 2.44286348e-04 9.99342501e-01
 9.97137904e-01 1.35041418e-07 6.22362435e-01 1.87223196e-01
 1.40333781e-02 6.12517215e-06 1.32403465e-03 6.40842045e-05
 6.15874082e-02 6.77420117e-04 9.59180072e-02 3.35197001e-06
 4.93342755e-03 9.66633081e-01 1.26929422e-06 9.95734394e-01
 4.40304205e-02 8.07452127e-02 4.97140136e-05 1.99152273e-05
 3.71521281e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 03:15:18, Dev, Step : 5072, Loss : 0.53712, Acc : 0.771, Auc : 0.849, Sensitive_Loss : 0.18966, Sensitive_Acc : 21.571, Sensitive_Auc : 0.997, Mean auc: 0.849, Run Time : 87.09 sec
INFO:root:2024-04-12 03:15:25, Train, Epoch : 9, Step : 5080, Loss : 0.25412, Acc : 0.694, Sensitive_Loss : 0.06918, Sensitive_Acc : 18.600, Run Time : 6.68 sec
INFO:root:2024-04-12 03:15:32, Train, Epoch : 9, Step : 5090, Loss : 0.38340, Acc : 0.863, Sensitive_Loss : 0.12077, Sensitive_Acc : 23.400, Run Time : 7.00 sec
INFO:root:2024-04-12 03:15:39, Train, Epoch : 9, Step : 5100, Loss : 0.36213, Acc : 0.844, Sensitive_Loss : 0.13576, Sensitive_Acc : 23.200, Run Time : 7.17 sec
INFO:root:2024-04-12 03:17:07, Dev, Step : 5100, Loss : 0.54146, Acc : 0.771, Auc : 0.847, Sensitive_Loss : 0.19511, Sensitive_Acc : 21.571, Sensitive_Auc : 0.997, Mean auc: 0.847, Run Time : 87.61 sec
INFO:root:2024-04-12 03:17:13, Train, Epoch : 9, Step : 5110, Loss : 0.37459, Acc : 0.859, Sensitive_Loss : 0.14854, Sensitive_Acc : 25.800, Run Time : 93.44 sec
INFO:root:2024-04-12 03:17:19, Train, Epoch : 9, Step : 5120, Loss : 0.31172, Acc : 0.887, Sensitive_Loss : 0.20928, Sensitive_Acc : 21.400, Run Time : 6.63 sec
INFO:root:2024-04-12 03:17:26, Train, Epoch : 9, Step : 5130, Loss : 0.41001, Acc : 0.816, Sensitive_Loss : 0.12473, Sensitive_Acc : 13.500, Run Time : 7.10 sec
INFO:root:2024-04-12 03:17:34, Train, Epoch : 9, Step : 5140, Loss : 0.35612, Acc : 0.853, Sensitive_Loss : 0.10787, Sensitive_Acc : 24.200, Run Time : 7.27 sec
INFO:root:2024-04-12 03:17:41, Train, Epoch : 9, Step : 5150, Loss : 0.38045, Acc : 0.825, Sensitive_Loss : 0.12770, Sensitive_Acc : 21.500, Run Time : 7.68 sec
INFO:root:2024-04-12 03:17:48, Train, Epoch : 9, Step : 5160, Loss : 0.30368, Acc : 0.853, Sensitive_Loss : 0.12021, Sensitive_Acc : 22.700, Run Time : 7.07 sec
INFO:root:2024-04-12 03:17:56, Train, Epoch : 9, Step : 5170, Loss : 0.33285, Acc : 0.863, Sensitive_Loss : 0.12971, Sensitive_Acc : 22.900, Run Time : 7.49 sec
INFO:root:2024-04-12 03:18:03, Train, Epoch : 9, Step : 5180, Loss : 0.30568, Acc : 0.869, Sensitive_Loss : 0.13419, Sensitive_Acc : 21.900, Run Time : 6.99 sec
INFO:root:2024-04-12 03:18:09, Train, Epoch : 9, Step : 5190, Loss : 0.29935, Acc : 0.856, Sensitive_Loss : 0.13000, Sensitive_Acc : 22.700, Run Time : 6.53 sec
INFO:root:2024-04-12 03:18:16, Train, Epoch : 9, Step : 5200, Loss : 0.35128, Acc : 0.809, Sensitive_Loss : 0.09803, Sensitive_Acc : 20.800, Run Time : 6.81 sec
INFO:root:2024-04-12 03:19:44, Dev, Step : 5200, Loss : 0.55751, Acc : 0.769, Auc : 0.848, Sensitive_Loss : 0.18696, Sensitive_Acc : 21.571, Sensitive_Auc : 0.995, Mean auc: 0.848, Run Time : 87.63 sec
INFO:root:2024-04-12 03:19:49, Train, Epoch : 9, Step : 5210, Loss : 0.31803, Acc : 0.859, Sensitive_Loss : 0.11594, Sensitive_Acc : 18.800, Run Time : 93.08 sec
INFO:root:2024-04-12 03:19:56, Train, Epoch : 9, Step : 5220, Loss : 0.37043, Acc : 0.841, Sensitive_Loss : 0.13847, Sensitive_Acc : 21.400, Run Time : 7.14 sec
INFO:root:2024-04-12 03:20:04, Train, Epoch : 9, Step : 5230, Loss : 0.35600, Acc : 0.859, Sensitive_Loss : 0.08471, Sensitive_Acc : 21.300, Run Time : 7.33 sec
INFO:root:2024-04-12 03:20:11, Train, Epoch : 9, Step : 5240, Loss : 0.32016, Acc : 0.859, Sensitive_Loss : 0.11663, Sensitive_Acc : 22.000, Run Time : 7.45 sec
INFO:root:2024-04-12 03:20:18, Train, Epoch : 9, Step : 5250, Loss : 0.32303, Acc : 0.838, Sensitive_Loss : 0.12676, Sensitive_Acc : 24.000, Run Time : 6.91 sec
INFO:root:2024-04-12 03:20:25, Train, Epoch : 9, Step : 5260, Loss : 0.40829, Acc : 0.844, Sensitive_Loss : 0.20677, Sensitive_Acc : 23.600, Run Time : 6.77 sec
INFO:root:2024-04-12 03:20:32, Train, Epoch : 9, Step : 5270, Loss : 0.32309, Acc : 0.853, Sensitive_Loss : 0.14290, Sensitive_Acc : 23.300, Run Time : 7.27 sec
INFO:root:2024-04-12 03:20:39, Train, Epoch : 9, Step : 5280, Loss : 0.42627, Acc : 0.825, Sensitive_Loss : 0.12281, Sensitive_Acc : 21.700, Run Time : 6.78 sec
INFO:root:2024-04-12 03:20:46, Train, Epoch : 9, Step : 5290, Loss : 0.41581, Acc : 0.828, Sensitive_Loss : 0.07417, Sensitive_Acc : 19.400, Run Time : 7.27 sec
INFO:root:2024-04-12 03:20:54, Train, Epoch : 9, Step : 5300, Loss : 0.27635, Acc : 0.881, Sensitive_Loss : 0.09367, Sensitive_Acc : 16.800, Run Time : 7.41 sec
INFO:root:2024-04-12 03:22:21, Dev, Step : 5300, Loss : 0.55100, Acc : 0.772, Auc : 0.846, Sensitive_Loss : 0.18421, Sensitive_Acc : 21.571, Sensitive_Auc : 0.996, Mean auc: 0.846, Run Time : 87.21 sec
INFO:root:2024-04-12 03:22:26, Train, Epoch : 9, Step : 5310, Loss : 0.35256, Acc : 0.856, Sensitive_Loss : 0.12256, Sensitive_Acc : 23.500, Run Time : 92.65 sec
INFO:root:2024-04-12 03:22:33, Train, Epoch : 9, Step : 5320, Loss : 0.34616, Acc : 0.872, Sensitive_Loss : 0.11211, Sensitive_Acc : 19.100, Run Time : 7.14 sec
INFO:root:2024-04-12 03:22:41, Train, Epoch : 9, Step : 5330, Loss : 0.30156, Acc : 0.850, Sensitive_Loss : 0.09851, Sensitive_Acc : 21.800, Run Time : 7.14 sec
INFO:root:2024-04-12 03:22:48, Train, Epoch : 9, Step : 5340, Loss : 0.31840, Acc : 0.863, Sensitive_Loss : 0.13170, Sensitive_Acc : 21.600, Run Time : 7.42 sec
INFO:root:2024-04-12 03:22:55, Train, Epoch : 9, Step : 5350, Loss : 0.39096, Acc : 0.847, Sensitive_Loss : 0.09744, Sensitive_Acc : 21.200, Run Time : 6.84 sec
INFO:root:2024-04-12 03:23:02, Train, Epoch : 9, Step : 5360, Loss : 0.31778, Acc : 0.863, Sensitive_Loss : 0.13175, Sensitive_Acc : 20.600, Run Time : 7.54 sec
INFO:root:2024-04-12 03:23:09, Train, Epoch : 9, Step : 5370, Loss : 0.34647, Acc : 0.847, Sensitive_Loss : 0.09036, Sensitive_Acc : 21.500, Run Time : 6.61 sec
INFO:root:2024-04-12 03:23:16, Train, Epoch : 9, Step : 5380, Loss : 0.30654, Acc : 0.853, Sensitive_Loss : 0.17475, Sensitive_Acc : 23.000, Run Time : 7.44 sec
INFO:root:2024-04-12 03:23:23, Train, Epoch : 9, Step : 5390, Loss : 0.35317, Acc : 0.828, Sensitive_Loss : 0.11046, Sensitive_Acc : 24.200, Run Time : 6.86 sec
INFO:root:2024-04-12 03:23:31, Train, Epoch : 9, Step : 5400, Loss : 0.36729, Acc : 0.819, Sensitive_Loss : 0.10933, Sensitive_Acc : 24.000, Run Time : 7.28 sec
INFO:root:2024-04-12 03:24:58, Dev, Step : 5400, Loss : 0.54838, Acc : 0.773, Auc : 0.847, Sensitive_Loss : 0.18941, Sensitive_Acc : 21.571, Sensitive_Auc : 0.996, Mean auc: 0.847, Run Time : 87.44 sec
INFO:root:2024-04-12 03:25:04, Train, Epoch : 9, Step : 5410, Loss : 0.28903, Acc : 0.856, Sensitive_Loss : 0.15692, Sensitive_Acc : 22.600, Run Time : 93.42 sec
INFO:root:2024-04-12 03:25:11, Train, Epoch : 9, Step : 5420, Loss : 0.36953, Acc : 0.863, Sensitive_Loss : 0.18198, Sensitive_Acc : 20.100, Run Time : 7.39 sec
INFO:root:2024-04-12 03:25:18, Train, Epoch : 9, Step : 5430, Loss : 0.38337, Acc : 0.819, Sensitive_Loss : 0.13183, Sensitive_Acc : 17.600, Run Time : 6.95 sec
INFO:root:2024-04-12 03:25:25, Train, Epoch : 9, Step : 5440, Loss : 0.30836, Acc : 0.856, Sensitive_Loss : 0.15729, Sensitive_Acc : 22.500, Run Time : 6.87 sec
INFO:root:2024-04-12 03:25:32, Train, Epoch : 9, Step : 5450, Loss : 0.29558, Acc : 0.856, Sensitive_Loss : 0.11998, Sensitive_Acc : 21.600, Run Time : 6.97 sec
INFO:root:2024-04-12 03:25:39, Train, Epoch : 9, Step : 5460, Loss : 0.30098, Acc : 0.869, Sensitive_Loss : 0.09819, Sensitive_Acc : 21.800, Run Time : 7.12 sec
INFO:root:2024-04-12 03:25:46, Train, Epoch : 9, Step : 5470, Loss : 0.38949, Acc : 0.856, Sensitive_Loss : 0.12027, Sensitive_Acc : 14.100, Run Time : 6.89 sec
INFO:root:2024-04-12 03:25:53, Train, Epoch : 9, Step : 5480, Loss : 0.40548, Acc : 0.825, Sensitive_Loss : 0.08994, Sensitive_Acc : 20.900, Run Time : 6.80 sec
INFO:root:2024-04-12 03:26:01, Train, Epoch : 9, Step : 5490, Loss : 0.34775, Acc : 0.841, Sensitive_Loss : 0.10255, Sensitive_Acc : 22.600, Run Time : 7.69 sec
INFO:root:2024-04-12 03:26:08, Train, Epoch : 9, Step : 5500, Loss : 0.33818, Acc : 0.853, Sensitive_Loss : 0.08971, Sensitive_Acc : 23.300, Run Time : 7.06 sec
INFO:root:2024-04-12 03:27:35, Dev, Step : 5500, Loss : 0.55116, Acc : 0.770, Auc : 0.845, Sensitive_Loss : 0.18476, Sensitive_Acc : 21.571, Sensitive_Auc : 0.997, Mean auc: 0.845, Run Time : 87.78 sec
INFO:root:2024-04-12 03:27:41, Train, Epoch : 9, Step : 5510, Loss : 0.34460, Acc : 0.863, Sensitive_Loss : 0.12967, Sensitive_Acc : 24.000, Run Time : 93.16 sec
INFO:root:2024-04-12 03:27:48, Train, Epoch : 9, Step : 5520, Loss : 0.33389, Acc : 0.866, Sensitive_Loss : 0.08514, Sensitive_Acc : 23.700, Run Time : 7.07 sec
INFO:root:2024-04-12 03:27:55, Train, Epoch : 9, Step : 5530, Loss : 0.33688, Acc : 0.859, Sensitive_Loss : 0.10692, Sensitive_Acc : 23.500, Run Time : 7.16 sec
INFO:root:2024-04-12 03:28:02, Train, Epoch : 9, Step : 5540, Loss : 0.33887, Acc : 0.844, Sensitive_Loss : 0.16214, Sensitive_Acc : 22.000, Run Time : 6.93 sec
INFO:root:2024-04-12 03:28:09, Train, Epoch : 9, Step : 5550, Loss : 0.34750, Acc : 0.856, Sensitive_Loss : 0.15288, Sensitive_Acc : 21.900, Run Time : 7.33 sec
INFO:root:2024-04-12 03:28:17, Train, Epoch : 9, Step : 5560, Loss : 0.32639, Acc : 0.866, Sensitive_Loss : 0.17036, Sensitive_Acc : 22.100, Run Time : 7.17 sec
INFO:root:2024-04-12 03:28:24, Train, Epoch : 9, Step : 5570, Loss : 0.36772, Acc : 0.875, Sensitive_Loss : 0.10676, Sensitive_Acc : 21.700, Run Time : 7.30 sec
INFO:root:2024-04-12 03:28:31, Train, Epoch : 9, Step : 5580, Loss : 0.34792, Acc : 0.878, Sensitive_Loss : 0.18203, Sensitive_Acc : 19.400, Run Time : 7.48 sec
INFO:root:2024-04-12 03:28:38, Train, Epoch : 9, Step : 5590, Loss : 0.38930, Acc : 0.812, Sensitive_Loss : 0.12522, Sensitive_Acc : 22.900, Run Time : 6.92 sec
INFO:root:2024-04-12 03:28:45, Train, Epoch : 9, Step : 5600, Loss : 0.34993, Acc : 0.822, Sensitive_Loss : 0.15320, Sensitive_Acc : 20.700, Run Time : 6.47 sec
INFO:root:2024-04-12 03:30:13, Dev, Step : 5600, Loss : 0.60890, Acc : 0.752, Auc : 0.849, Sensitive_Loss : 0.19405, Sensitive_Acc : 21.541, Sensitive_Auc : 0.997, Mean auc: 0.849, Run Time : 87.85 sec
INFO:root:2024-04-12 03:30:18, Train, Epoch : 9, Step : 5610, Loss : 0.31123, Acc : 0.838, Sensitive_Loss : 0.13506, Sensitive_Acc : 22.000, Run Time : 93.54 sec
INFO:root:2024-04-12 03:30:25, Train, Epoch : 9, Step : 5620, Loss : 0.38051, Acc : 0.834, Sensitive_Loss : 0.11767, Sensitive_Acc : 22.200, Run Time : 6.98 sec
INFO:root:2024-04-12 03:30:32, Train, Epoch : 9, Step : 5630, Loss : 0.34825, Acc : 0.853, Sensitive_Loss : 0.10665, Sensitive_Acc : 20.100, Run Time : 6.90 sec
INFO:root:2024-04-12 03:30:40, Train, Epoch : 9, Step : 5640, Loss : 0.27920, Acc : 0.863, Sensitive_Loss : 0.10104, Sensitive_Acc : 23.900, Run Time : 7.38 sec
INFO:root:2024-04-12 03:30:47, Train, Epoch : 9, Step : 5650, Loss : 0.33826, Acc : 0.853, Sensitive_Loss : 0.11187, Sensitive_Acc : 21.600, Run Time : 7.13 sec
INFO:root:2024-04-12 03:30:54, Train, Epoch : 9, Step : 5660, Loss : 0.34004, Acc : 0.856, Sensitive_Loss : 0.11471, Sensitive_Acc : 21.200, Run Time : 7.21 sec
INFO:root:2024-04-12 03:31:01, Train, Epoch : 9, Step : 5670, Loss : 0.32447, Acc : 0.869, Sensitive_Loss : 0.11706, Sensitive_Acc : 24.000, Run Time : 7.58 sec
INFO:root:2024-04-12 03:31:08, Train, Epoch : 9, Step : 5680, Loss : 0.37060, Acc : 0.831, Sensitive_Loss : 0.09453, Sensitive_Acc : 22.300, Run Time : 6.59 sec
INFO:root:2024-04-12 03:31:15, Train, Epoch : 9, Step : 5690, Loss : 0.32260, Acc : 0.859, Sensitive_Loss : 0.19413, Sensitive_Acc : 23.600, Run Time : 7.01 sec
INFO:root:2024-04-12 03:31:22, Train, Epoch : 9, Step : 5700, Loss : 0.33597, Acc : 0.844, Sensitive_Loss : 0.12703, Sensitive_Acc : 21.100, Run Time : 7.44 sec
INFO:root:2024-04-12 03:32:50, Dev, Step : 5700, Loss : 0.53990, Acc : 0.776, Auc : 0.849, Sensitive_Loss : 0.16864, Sensitive_Acc : 21.662, Sensitive_Auc : 0.996, Mean auc: 0.849, Run Time : 87.68 sec
INFO:root:2024-04-12 03:34:19
INFO:root:y_pred: [0.24976055 0.03193486 0.04543513 ... 0.09205476 0.10366355 0.00125936]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.07469317e-02 2.44842289e-04 1.16859551e-03 1.91693962e-01
 7.37267435e-02 4.70416213e-04 6.63134269e-04 5.96874052e-05
 2.08945349e-02 9.99059260e-01 1.38326302e-01 7.47896877e-07
 1.44299772e-03 4.47741741e-06 9.96475041e-01 9.32593364e-03
 1.07313936e-04 9.98337388e-01 9.88804638e-01 9.23667627e-04
 6.95403874e-01 1.01787741e-07 1.38617630e-04 3.04574915e-03
 3.73190778e-05 5.38006239e-02 2.30393576e-04 6.54593314e-05
 9.03817545e-06 6.63344190e-02 4.58927767e-04 9.12065148e-01
 2.46532913e-03 6.36133075e-01 5.10624488e-07 3.64793013e-05
 2.49736627e-06 4.31237917e-04 5.02185225e-02 9.73321672e-04
 8.53341743e-02 9.66465235e-01 2.81894044e-03 3.63107165e-06
 9.31133211e-01 2.80885864e-03 2.11419627e-01 2.07449626e-02
 7.29360878e-02 9.96908367e-01 8.03865075e-01 9.98812318e-01
 9.43794310e-01 8.13279399e-09 3.50238224e-05 4.99516875e-02
 3.39588593e-03 2.27729394e-03 9.88489091e-01 2.01665447e-03
 2.40175304e-05 1.94469672e-02 2.21402179e-02 1.32679412e-09
 9.78874266e-01 1.08071668e-02 2.93370249e-07 2.97805279e-01
 1.37575557e-02 9.65695441e-01 9.99427736e-01 9.94574010e-01
 9.54571533e-07 3.24117512e-01 5.54024773e-05 3.83926570e-01
 8.10920319e-04 1.98117629e-08 1.52003166e-04 8.04725005e-06
 7.41560361e-05 1.82950788e-03 9.84461904e-01 9.23160374e-01
 1.27213784e-02 1.31525376e-04 1.72727600e-01 1.28682293e-02
 4.11156099e-04 1.52935481e-05 1.84437071e-04 3.21941301e-02
 1.65688300e-06 4.41267758e-07 3.54122109e-04 5.39645273e-03
 2.39775080e-04 9.05932307e-01 2.59805180e-04 5.46568632e-03
 1.46291563e-02 5.55077754e-03 3.74521781e-03 6.20802666e-06
 2.12502177e-03 4.99494308e-05 1.37631465e-02 6.87796712e-01
 5.42033762e-02 6.66668043e-02 3.44027394e-05 9.98702288e-01
 9.94547367e-01 4.47177442e-08 4.95149225e-01 7.91316330e-02
 7.30955377e-02 6.46604860e-07 9.59044846e-04 8.73431054e-05
 1.34222116e-02 9.72841881e-05 1.81110203e-02 2.71769068e-07
 1.77856849e-03 9.54888642e-01 2.49009489e-08 9.91514683e-01
 2.78786011e-03 4.85645421e-02 2.76649534e-05 1.06396883e-05
 5.39894017e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 03:34:19, Dev, Step : 5706, Loss : 0.54229, Acc : 0.775, Auc : 0.848, Sensitive_Loss : 0.16919, Sensitive_Acc : 21.797, Sensitive_Auc : 0.997, Mean auc: 0.848, Run Time : 86.42 sec
INFO:root:2024-04-12 03:34:24, Train, Epoch : 10, Step : 5710, Loss : 0.13714, Acc : 0.331, Sensitive_Loss : 0.07804, Sensitive_Acc : 10.700, Run Time : 4.26 sec
INFO:root:2024-04-12 03:34:31, Train, Epoch : 10, Step : 5720, Loss : 0.32632, Acc : 0.850, Sensitive_Loss : 0.14125, Sensitive_Acc : 19.100, Run Time : 6.64 sec
INFO:root:2024-04-12 03:34:38, Train, Epoch : 10, Step : 5730, Loss : 0.33547, Acc : 0.863, Sensitive_Loss : 0.09417, Sensitive_Acc : 22.000, Run Time : 7.19 sec
INFO:root:2024-04-12 03:34:45, Train, Epoch : 10, Step : 5740, Loss : 0.33399, Acc : 0.869, Sensitive_Loss : 0.14835, Sensitive_Acc : 21.400, Run Time : 6.83 sec
INFO:root:2024-04-12 03:34:52, Train, Epoch : 10, Step : 5750, Loss : 0.36773, Acc : 0.859, Sensitive_Loss : 0.16199, Sensitive_Acc : 16.300, Run Time : 7.20 sec
INFO:root:2024-04-12 03:34:59, Train, Epoch : 10, Step : 5760, Loss : 0.37828, Acc : 0.822, Sensitive_Loss : 0.07900, Sensitive_Acc : 19.100, Run Time : 7.03 sec
INFO:root:2024-04-12 03:35:06, Train, Epoch : 10, Step : 5770, Loss : 0.29100, Acc : 0.891, Sensitive_Loss : 0.07463, Sensitive_Acc : 19.500, Run Time : 7.13 sec
INFO:root:2024-04-12 03:35:14, Train, Epoch : 10, Step : 5780, Loss : 0.31694, Acc : 0.884, Sensitive_Loss : 0.10766, Sensitive_Acc : 24.200, Run Time : 7.36 sec
INFO:root:2024-04-12 03:35:20, Train, Epoch : 10, Step : 5790, Loss : 0.32455, Acc : 0.863, Sensitive_Loss : 0.12787, Sensitive_Acc : 15.500, Run Time : 6.78 sec
INFO:root:2024-04-12 03:35:28, Train, Epoch : 10, Step : 5800, Loss : 0.38361, Acc : 0.831, Sensitive_Loss : 0.11883, Sensitive_Acc : 25.300, Run Time : 7.20 sec
INFO:root:2024-04-12 03:36:55, Dev, Step : 5800, Loss : 0.54261, Acc : 0.773, Auc : 0.849, Sensitive_Loss : 0.18358, Sensitive_Acc : 21.541, Sensitive_Auc : 0.996, Mean auc: 0.849, Run Time : 87.89 sec
INFO:root:2024-04-12 03:37:01, Train, Epoch : 10, Step : 5810, Loss : 0.33677, Acc : 0.828, Sensitive_Loss : 0.12929, Sensitive_Acc : 25.200, Run Time : 93.57 sec
INFO:root:2024-04-12 03:37:09, Train, Epoch : 10, Step : 5820, Loss : 0.29884, Acc : 0.856, Sensitive_Loss : 0.12370, Sensitive_Acc : 20.300, Run Time : 7.62 sec
INFO:root:2024-04-12 03:37:16, Train, Epoch : 10, Step : 5830, Loss : 0.31877, Acc : 0.881, Sensitive_Loss : 0.09766, Sensitive_Acc : 18.500, Run Time : 6.90 sec
INFO:root:2024-04-12 03:37:23, Train, Epoch : 10, Step : 5840, Loss : 0.30187, Acc : 0.875, Sensitive_Loss : 0.13017, Sensitive_Acc : 20.200, Run Time : 6.91 sec
INFO:root:2024-04-12 03:37:30, Train, Epoch : 10, Step : 5850, Loss : 0.34830, Acc : 0.841, Sensitive_Loss : 0.11658, Sensitive_Acc : 21.000, Run Time : 7.21 sec
INFO:root:2024-04-12 03:37:36, Train, Epoch : 10, Step : 5860, Loss : 0.32338, Acc : 0.847, Sensitive_Loss : 0.13418, Sensitive_Acc : 23.800, Run Time : 6.70 sec
INFO:root:2024-04-12 03:37:44, Train, Epoch : 10, Step : 5870, Loss : 0.30498, Acc : 0.859, Sensitive_Loss : 0.13776, Sensitive_Acc : 19.800, Run Time : 7.84 sec
INFO:root:2024-04-12 03:37:51, Train, Epoch : 10, Step : 5880, Loss : 0.33385, Acc : 0.869, Sensitive_Loss : 0.06548, Sensitive_Acc : 23.400, Run Time : 6.64 sec
INFO:root:2024-04-12 03:37:58, Train, Epoch : 10, Step : 5890, Loss : 0.38727, Acc : 0.841, Sensitive_Loss : 0.10718, Sensitive_Acc : 21.900, Run Time : 7.35 sec
INFO:root:2024-04-12 03:38:05, Train, Epoch : 10, Step : 5900, Loss : 0.29754, Acc : 0.866, Sensitive_Loss : 0.15028, Sensitive_Acc : 21.900, Run Time : 6.86 sec
INFO:root:2024-04-12 03:39:33, Dev, Step : 5900, Loss : 0.56083, Acc : 0.772, Auc : 0.848, Sensitive_Loss : 0.18831, Sensitive_Acc : 21.451, Sensitive_Auc : 0.996, Mean auc: 0.848, Run Time : 87.72 sec
INFO:root:2024-04-12 03:39:39, Train, Epoch : 10, Step : 5910, Loss : 0.34338, Acc : 0.866, Sensitive_Loss : 0.06545, Sensitive_Acc : 22.500, Run Time : 93.48 sec
INFO:root:2024-04-12 03:39:46, Train, Epoch : 10, Step : 5920, Loss : 0.35443, Acc : 0.847, Sensitive_Loss : 0.07195, Sensitive_Acc : 19.600, Run Time : 7.00 sec
INFO:root:2024-04-12 03:39:52, Train, Epoch : 10, Step : 5930, Loss : 0.27636, Acc : 0.900, Sensitive_Loss : 0.10375, Sensitive_Acc : 23.500, Run Time : 6.80 sec
INFO:root:2024-04-12 03:40:00, Train, Epoch : 10, Step : 5940, Loss : 0.32217, Acc : 0.856, Sensitive_Loss : 0.13565, Sensitive_Acc : 18.300, Run Time : 7.11 sec
INFO:root:2024-04-12 03:40:07, Train, Epoch : 10, Step : 5950, Loss : 0.32511, Acc : 0.863, Sensitive_Loss : 0.12002, Sensitive_Acc : 23.500, Run Time : 7.24 sec
INFO:root:2024-04-12 03:40:14, Train, Epoch : 10, Step : 5960, Loss : 0.35204, Acc : 0.859, Sensitive_Loss : 0.07781, Sensitive_Acc : 21.100, Run Time : 6.96 sec
INFO:root:2024-04-12 03:40:21, Train, Epoch : 10, Step : 5970, Loss : 0.33535, Acc : 0.856, Sensitive_Loss : 0.10190, Sensitive_Acc : 19.800, Run Time : 7.39 sec
INFO:root:2024-04-12 03:40:29, Train, Epoch : 10, Step : 5980, Loss : 0.30464, Acc : 0.878, Sensitive_Loss : 0.16223, Sensitive_Acc : 16.300, Run Time : 7.33 sec
INFO:root:2024-04-12 03:40:36, Train, Epoch : 10, Step : 5990, Loss : 0.31093, Acc : 0.850, Sensitive_Loss : 0.10483, Sensitive_Acc : 21.300, Run Time : 7.06 sec
INFO:root:2024-04-12 03:40:42, Train, Epoch : 10, Step : 6000, Loss : 0.31889, Acc : 0.853, Sensitive_Loss : 0.10946, Sensitive_Acc : 20.600, Run Time : 6.61 sec
INFO:root:2024-04-12 03:42:10, Dev, Step : 6000, Loss : 0.57096, Acc : 0.770, Auc : 0.847, Sensitive_Loss : 0.18087, Sensitive_Acc : 21.662, Sensitive_Auc : 0.996, Mean auc: 0.847, Run Time : 87.67 sec
INFO:root:2024-04-12 03:42:15, Train, Epoch : 10, Step : 6010, Loss : 0.26389, Acc : 0.887, Sensitive_Loss : 0.06362, Sensitive_Acc : 16.200, Run Time : 92.97 sec
INFO:root:2024-04-12 03:42:22, Train, Epoch : 10, Step : 6020, Loss : 0.35659, Acc : 0.863, Sensitive_Loss : 0.09484, Sensitive_Acc : 16.500, Run Time : 7.01 sec
INFO:root:2024-04-12 03:42:29, Train, Epoch : 10, Step : 6030, Loss : 0.37264, Acc : 0.831, Sensitive_Loss : 0.11589, Sensitive_Acc : 24.200, Run Time : 7.32 sec
INFO:root:2024-04-12 03:42:37, Train, Epoch : 10, Step : 6040, Loss : 0.30534, Acc : 0.863, Sensitive_Loss : 0.12843, Sensitive_Acc : 21.300, Run Time : 7.53 sec
INFO:root:2024-04-12 03:42:43, Train, Epoch : 10, Step : 6050, Loss : 0.33335, Acc : 0.834, Sensitive_Loss : 0.07441, Sensitive_Acc : 27.200, Run Time : 6.40 sec
INFO:root:2024-04-12 03:42:51, Train, Epoch : 10, Step : 6060, Loss : 0.33635, Acc : 0.869, Sensitive_Loss : 0.12336, Sensitive_Acc : 24.100, Run Time : 7.19 sec
INFO:root:2024-04-12 03:42:58, Train, Epoch : 10, Step : 6070, Loss : 0.32881, Acc : 0.863, Sensitive_Loss : 0.09420, Sensitive_Acc : 16.300, Run Time : 7.15 sec
INFO:root:2024-04-12 03:43:06, Train, Epoch : 10, Step : 6080, Loss : 0.33258, Acc : 0.828, Sensitive_Loss : 0.15921, Sensitive_Acc : 20.900, Run Time : 7.95 sec
INFO:root:2024-04-12 03:43:13, Train, Epoch : 10, Step : 6090, Loss : 0.37477, Acc : 0.834, Sensitive_Loss : 0.10466, Sensitive_Acc : 19.500, Run Time : 6.90 sec
INFO:root:2024-04-12 03:43:19, Train, Epoch : 10, Step : 6100, Loss : 0.28131, Acc : 0.884, Sensitive_Loss : 0.12635, Sensitive_Acc : 21.500, Run Time : 6.40 sec
INFO:root:2024-04-12 03:44:47, Dev, Step : 6100, Loss : 0.59387, Acc : 0.765, Auc : 0.844, Sensitive_Loss : 0.18192, Sensitive_Acc : 21.662, Sensitive_Auc : 0.996, Mean auc: 0.844, Run Time : 87.95 sec
INFO:root:2024-04-12 03:44:52, Train, Epoch : 10, Step : 6110, Loss : 0.37070, Acc : 0.822, Sensitive_Loss : 0.10838, Sensitive_Acc : 19.200, Run Time : 93.37 sec
INFO:root:2024-04-12 03:45:00, Train, Epoch : 10, Step : 6120, Loss : 0.35243, Acc : 0.844, Sensitive_Loss : 0.18339, Sensitive_Acc : 20.200, Run Time : 7.31 sec
INFO:root:2024-04-12 03:45:07, Train, Epoch : 10, Step : 6130, Loss : 0.40572, Acc : 0.844, Sensitive_Loss : 0.20591, Sensitive_Acc : 23.000, Run Time : 7.08 sec
INFO:root:2024-04-12 03:45:14, Train, Epoch : 10, Step : 6140, Loss : 0.38054, Acc : 0.822, Sensitive_Loss : 0.15441, Sensitive_Acc : 23.300, Run Time : 7.22 sec
INFO:root:2024-04-12 03:45:21, Train, Epoch : 10, Step : 6150, Loss : 0.27813, Acc : 0.872, Sensitive_Loss : 0.06943, Sensitive_Acc : 19.100, Run Time : 7.45 sec
INFO:root:2024-04-12 03:45:28, Train, Epoch : 10, Step : 6160, Loss : 0.31191, Acc : 0.878, Sensitive_Loss : 0.12873, Sensitive_Acc : 22.000, Run Time : 6.86 sec
INFO:root:2024-04-12 03:45:35, Train, Epoch : 10, Step : 6170, Loss : 0.33531, Acc : 0.866, Sensitive_Loss : 0.07532, Sensitive_Acc : 20.200, Run Time : 7.14 sec
INFO:root:2024-04-12 03:45:42, Train, Epoch : 10, Step : 6180, Loss : 0.32874, Acc : 0.875, Sensitive_Loss : 0.08313, Sensitive_Acc : 18.500, Run Time : 6.50 sec
INFO:root:2024-04-12 03:45:49, Train, Epoch : 10, Step : 6190, Loss : 0.28627, Acc : 0.881, Sensitive_Loss : 0.14624, Sensitive_Acc : 22.600, Run Time : 7.17 sec
INFO:root:2024-04-12 03:45:56, Train, Epoch : 10, Step : 6200, Loss : 0.25829, Acc : 0.906, Sensitive_Loss : 0.12347, Sensitive_Acc : 24.300, Run Time : 7.15 sec
INFO:root:2024-04-12 03:47:24, Dev, Step : 6200, Loss : 0.54053, Acc : 0.775, Auc : 0.848, Sensitive_Loss : 0.19399, Sensitive_Acc : 21.571, Sensitive_Auc : 0.997, Mean auc: 0.848, Run Time : 87.67 sec
INFO:root:2024-04-12 03:47:30, Train, Epoch : 10, Step : 6210, Loss : 0.27893, Acc : 0.866, Sensitive_Loss : 0.10048, Sensitive_Acc : 17.900, Run Time : 93.49 sec
INFO:root:2024-04-12 03:47:37, Train, Epoch : 10, Step : 6220, Loss : 0.30024, Acc : 0.872, Sensitive_Loss : 0.10498, Sensitive_Acc : 22.400, Run Time : 7.43 sec
INFO:root:2024-04-12 03:47:44, Train, Epoch : 10, Step : 6230, Loss : 0.37918, Acc : 0.841, Sensitive_Loss : 0.20323, Sensitive_Acc : 22.500, Run Time : 6.55 sec
INFO:root:2024-04-12 03:47:51, Train, Epoch : 10, Step : 6240, Loss : 0.34845, Acc : 0.866, Sensitive_Loss : 0.10118, Sensitive_Acc : 27.000, Run Time : 6.99 sec
INFO:root:2024-04-12 03:47:58, Train, Epoch : 10, Step : 6250, Loss : 0.34152, Acc : 0.869, Sensitive_Loss : 0.09939, Sensitive_Acc : 26.100, Run Time : 7.52 sec
INFO:root:2024-04-12 03:48:05, Train, Epoch : 10, Step : 6260, Loss : 0.33132, Acc : 0.853, Sensitive_Loss : 0.09318, Sensitive_Acc : 26.000, Run Time : 6.41 sec
INFO:root:2024-04-12 03:48:12, Train, Epoch : 10, Step : 6270, Loss : 0.29810, Acc : 0.884, Sensitive_Loss : 0.10289, Sensitive_Acc : 23.600, Run Time : 7.53 sec
INFO:root:2024-04-12 03:48:19, Train, Epoch : 10, Step : 6280, Loss : 0.32924, Acc : 0.816, Sensitive_Loss : 0.09787, Sensitive_Acc : 23.700, Run Time : 7.18 sec
INFO:root:2024-04-12 03:48:26, Train, Epoch : 10, Step : 6290, Loss : 0.34400, Acc : 0.850, Sensitive_Loss : 0.10945, Sensitive_Acc : 23.800, Run Time : 7.00 sec
INFO:root:2024-04-12 03:48:33, Train, Epoch : 10, Step : 6300, Loss : 0.30133, Acc : 0.866, Sensitive_Loss : 0.11274, Sensitive_Acc : 23.500, Run Time : 6.90 sec
INFO:root:2024-04-12 03:50:02, Dev, Step : 6300, Loss : 0.56276, Acc : 0.763, Auc : 0.843, Sensitive_Loss : 0.18369, Sensitive_Acc : 21.662, Sensitive_Auc : 0.998, Mean auc: 0.843, Run Time : 88.33 sec
INFO:root:2024-04-12 03:50:07, Train, Epoch : 10, Step : 6310, Loss : 0.29318, Acc : 0.881, Sensitive_Loss : 0.14079, Sensitive_Acc : 21.800, Run Time : 93.52 sec
INFO:root:2024-04-12 03:50:14, Train, Epoch : 10, Step : 6320, Loss : 0.36387, Acc : 0.841, Sensitive_Loss : 0.12662, Sensitive_Acc : 18.100, Run Time : 7.68 sec
INFO:root:2024-04-12 03:50:22, Train, Epoch : 10, Step : 6330, Loss : 0.28605, Acc : 0.875, Sensitive_Loss : 0.14698, Sensitive_Acc : 18.200, Run Time : 7.14 sec
INFO:root:2024-04-12 03:50:28, Train, Epoch : 10, Step : 6340, Loss : 0.36552, Acc : 0.859, Sensitive_Loss : 0.11557, Sensitive_Acc : 23.100, Run Time : 6.42 sec
INFO:root:2024-04-12 03:51:55
INFO:root:y_pred: [0.16637816 0.01495119 0.11045709 ... 0.20792338 0.17529869 0.00085825]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [2.89627141e-03 6.41500170e-04 7.55875488e-04 2.41049454e-01
 9.87235680e-02 4.82910487e-04 5.26382762e-04 6.36344776e-05
 6.35212138e-02 9.99614120e-01 3.29513758e-01 2.76647552e-06
 7.83696491e-03 2.42719466e-06 9.98144627e-01 9.82198771e-03
 7.90358536e-05 9.98853803e-01 9.96374667e-01 3.45492014e-03
 8.63854527e-01 9.05916142e-09 1.93445070e-04 6.18187478e-03
 1.23869788e-04 1.09585606e-01 2.00942843e-04 3.18811981e-05
 2.32662751e-06 6.75404146e-02 1.45316729e-03 8.95930588e-01
 4.67091426e-03 5.68451405e-01 1.61310049e-06 6.92838439e-05
 1.90691389e-05 7.50939187e-04 4.44809757e-02 3.51768802e-04
 4.35143448e-02 9.78406549e-01 4.94155241e-03 2.67951737e-06
 9.73349273e-01 3.91994923e-04 4.96081889e-01 5.72859868e-02
 1.04666501e-01 9.98262227e-01 8.54896247e-01 9.99177754e-01
 9.74836230e-01 1.07549880e-07 9.31319883e-05 1.42838761e-01
 3.77147505e-03 1.11492723e-03 9.92428124e-01 1.89460727e-04
 4.82904870e-05 2.97363866e-02 3.41597721e-02 6.71587372e-08
 9.91175532e-01 1.76667012e-02 4.34195840e-07 3.22715044e-01
 1.17211193e-02 9.92540181e-01 9.99792278e-01 9.97938335e-01
 1.30251647e-05 4.20276999e-01 4.81487717e-04 3.99580896e-01
 3.62370093e-03 1.29479886e-07 6.08375936e-04 7.86491000e-05
 1.50696520e-04 1.23276236e-03 9.89445508e-01 9.48638380e-01
 1.78661253e-02 3.37529113e-04 2.08353132e-01 1.28693199e-02
 7.77293416e-03 4.37085619e-05 2.75848124e-05 3.65195982e-02
 1.26990990e-05 4.31907836e-07 2.74235994e-04 2.56989468e-02
 4.50115767e-04 9.58058417e-01 1.29898195e-03 7.56282185e-04
 3.14006843e-02 6.43783761e-03 2.82773492e-03 2.08973470e-05
 1.74075346e-02 3.26871123e-05 1.04452744e-01 6.66327953e-01
 9.95013118e-02 1.31836221e-01 1.20074728e-05 9.99421477e-01
 9.95136559e-01 1.85324698e-08 6.23535156e-01 1.53858498e-01
 7.39732236e-02 1.76004107e-06 2.90656346e-03 7.75591834e-05
 1.31698055e-02 3.73320887e-04 5.74330576e-02 4.52013040e-08
 5.97711932e-03 9.53605473e-01 1.07869759e-07 9.93129015e-01
 4.22769738e-03 2.14776695e-02 2.05018023e-05 6.12832009e-05
 5.33824959e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 03:51:55, Dev, Step : 6340, Loss : 0.55325, Acc : 0.764, Auc : 0.845, Sensitive_Loss : 0.18224, Sensitive_Acc : 21.662, Sensitive_Auc : 0.997, Mean auc: 0.845, Run Time : 87.06 sec
