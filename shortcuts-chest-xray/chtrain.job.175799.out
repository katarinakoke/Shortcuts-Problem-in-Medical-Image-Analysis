Running on desktop18:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
3
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_val.csv",
    "pred_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Balanced_Sex_0_0.csv",
    "pred_model": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Balanced_Sex_0_01.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-17 22:25:19, Train, Epoch : 1, Step : 10, Loss : 0.69076, Acc : 0.550, Sensitive_Loss : 1.16178, Sensitive_Acc : 6.400, Run Time : 21.55 sec
INFO:root:2024-04-17 22:25:37, Train, Epoch : 1, Step : 20, Loss : 0.69154, Acc : 0.591, Sensitive_Loss : 1.06877, Sensitive_Acc : 17.800, Run Time : 17.86 sec
INFO:root:2024-04-17 22:25:55, Train, Epoch : 1, Step : 30, Loss : 0.66242, Acc : 0.603, Sensitive_Loss : 1.06762, Sensitive_Acc : 20.400, Run Time : 18.48 sec
INFO:root:2024-04-17 22:26:14, Train, Epoch : 1, Step : 40, Loss : 0.65355, Acc : 0.688, Sensitive_Loss : 0.97761, Sensitive_Acc : 18.900, Run Time : 18.50 sec
INFO:root:2024-04-17 22:26:32, Train, Epoch : 1, Step : 50, Loss : 0.67192, Acc : 0.584, Sensitive_Loss : 1.06733, Sensitive_Acc : 20.500, Run Time : 18.01 sec
INFO:root:2024-04-17 22:26:49, Train, Epoch : 1, Step : 60, Loss : 0.69741, Acc : 0.631, Sensitive_Loss : 1.08548, Sensitive_Acc : 16.000, Run Time : 17.17 sec
INFO:root:2024-04-17 22:27:07, Train, Epoch : 1, Step : 70, Loss : 0.66367, Acc : 0.603, Sensitive_Loss : 1.08167, Sensitive_Acc : 19.300, Run Time : 18.18 sec
INFO:root:2024-04-17 22:27:26, Train, Epoch : 1, Step : 80, Loss : 0.64350, Acc : 0.684, Sensitive_Loss : 0.97990, Sensitive_Acc : 18.700, Run Time : 18.33 sec
INFO:root:2024-04-17 22:27:44, Train, Epoch : 1, Step : 90, Loss : 0.67890, Acc : 0.659, Sensitive_Loss : 1.00122, Sensitive_Acc : 18.400, Run Time : 17.97 sec
INFO:root:2024-04-17 22:28:02, Train, Epoch : 1, Step : 100, Loss : 0.67085, Acc : 0.641, Sensitive_Loss : 0.97553, Sensitive_Acc : 18.800, Run Time : 18.75 sec
INFO:root:2024-04-17 22:32:00, Dev, Step : 100, Loss : 0.65705, Acc : 0.638, Auc : 0.731, Sensitive_Loss : 0.97317, Sensitive_Acc : 18.158, Sensitive_Auc : 0.654, Mean auc: 0.731, Run Time : 237.60 sec
INFO:root:2024-04-17 22:32:01, Best, Step : 100, Loss : 0.65705, Acc : 0.638, Auc : 0.731, Sensitive_Loss : 0.97317, Sensitive_Acc : 18.158, Sensitive_Auc : 0.654, Best Auc : 0.731
INFO:root:2024-04-17 22:32:13, Train, Epoch : 1, Step : 110, Loss : 0.59957, Acc : 0.681, Sensitive_Loss : 0.95344, Sensitive_Acc : 19.500, Run Time : 250.79 sec
INFO:root:2024-04-17 22:32:31, Train, Epoch : 1, Step : 120, Loss : 0.57659, Acc : 0.684, Sensitive_Loss : 0.92064, Sensitive_Acc : 21.000, Run Time : 18.30 sec
INFO:root:2024-04-17 22:32:48, Train, Epoch : 1, Step : 130, Loss : 0.63707, Acc : 0.678, Sensitive_Loss : 1.06455, Sensitive_Acc : 26.900, Run Time : 16.86 sec
INFO:root:2024-04-17 22:33:07, Train, Epoch : 1, Step : 140, Loss : 0.70519, Acc : 0.653, Sensitive_Loss : 0.99051, Sensitive_Acc : 16.400, Run Time : 19.07 sec
INFO:root:2024-04-17 22:33:25, Train, Epoch : 1, Step : 150, Loss : 0.55981, Acc : 0.684, Sensitive_Loss : 0.99020, Sensitive_Acc : 13.400, Run Time : 17.49 sec
INFO:root:2024-04-17 22:33:43, Train, Epoch : 1, Step : 160, Loss : 0.56562, Acc : 0.728, Sensitive_Loss : 0.90843, Sensitive_Acc : 17.400, Run Time : 18.32 sec
INFO:root:2024-04-17 22:34:00, Train, Epoch : 1, Step : 170, Loss : 0.63309, Acc : 0.684, Sensitive_Loss : 1.03165, Sensitive_Acc : 18.500, Run Time : 16.69 sec
INFO:root:2024-04-17 22:34:20, Train, Epoch : 1, Step : 180, Loss : 0.56780, Acc : 0.666, Sensitive_Loss : 0.92792, Sensitive_Acc : 18.500, Run Time : 19.68 sec
INFO:root:2024-04-17 22:34:37, Train, Epoch : 1, Step : 190, Loss : 0.67012, Acc : 0.650, Sensitive_Loss : 0.98493, Sensitive_Acc : 21.600, Run Time : 16.91 sec
INFO:root:2024-04-17 22:34:56, Train, Epoch : 1, Step : 200, Loss : 0.62148, Acc : 0.681, Sensitive_Loss : 0.85598, Sensitive_Acc : 23.200, Run Time : 19.19 sec
INFO:root:2024-04-17 22:38:51, Dev, Step : 200, Loss : 0.64844, Acc : 0.686, Auc : 0.767, Sensitive_Loss : 0.91122, Sensitive_Acc : 20.128, Sensitive_Auc : 0.729, Mean auc: 0.767, Run Time : 235.25 sec
INFO:root:2024-04-17 22:38:52, Best, Step : 200, Loss : 0.64844, Acc : 0.686, Auc : 0.767, Sensitive_Loss : 0.91122, Sensitive_Acc : 20.128, Sensitive_Auc : 0.729, Best Auc : 0.767
INFO:root:2024-04-17 22:39:04, Train, Epoch : 1, Step : 210, Loss : 0.57071, Acc : 0.669, Sensitive_Loss : 0.88291, Sensitive_Acc : 17.900, Run Time : 248.62 sec
INFO:root:2024-04-17 22:39:23, Train, Epoch : 1, Step : 220, Loss : 0.69230, Acc : 0.628, Sensitive_Loss : 0.96494, Sensitive_Acc : 19.500, Run Time : 18.79 sec
INFO:root:2024-04-17 22:39:42, Train, Epoch : 1, Step : 230, Loss : 0.65330, Acc : 0.681, Sensitive_Loss : 0.89897, Sensitive_Acc : 15.600, Run Time : 18.52 sec
INFO:root:2024-04-17 22:39:59, Train, Epoch : 1, Step : 240, Loss : 0.63963, Acc : 0.697, Sensitive_Loss : 0.94096, Sensitive_Acc : 15.500, Run Time : 17.20 sec
INFO:root:2024-04-17 22:40:18, Train, Epoch : 1, Step : 250, Loss : 0.54959, Acc : 0.716, Sensitive_Loss : 0.91173, Sensitive_Acc : 18.100, Run Time : 18.92 sec
INFO:root:2024-04-17 22:40:35, Train, Epoch : 1, Step : 260, Loss : 0.58224, Acc : 0.700, Sensitive_Loss : 0.85727, Sensitive_Acc : 21.700, Run Time : 17.64 sec
INFO:root:2024-04-17 22:40:53, Train, Epoch : 1, Step : 270, Loss : 0.59995, Acc : 0.662, Sensitive_Loss : 0.83904, Sensitive_Acc : 15.600, Run Time : 18.07 sec
INFO:root:2024-04-17 22:41:11, Train, Epoch : 1, Step : 280, Loss : 0.54446, Acc : 0.697, Sensitive_Loss : 0.84752, Sensitive_Acc : 13.300, Run Time : 17.37 sec
INFO:root:2024-04-17 22:41:30, Train, Epoch : 1, Step : 290, Loss : 0.57562, Acc : 0.700, Sensitive_Loss : 0.84939, Sensitive_Acc : 14.100, Run Time : 19.31 sec
INFO:root:2024-04-17 22:41:48, Train, Epoch : 1, Step : 300, Loss : 0.58650, Acc : 0.716, Sensitive_Loss : 0.87520, Sensitive_Acc : 13.400, Run Time : 17.75 sec
INFO:root:2024-04-17 22:45:46, Dev, Step : 300, Loss : 0.69296, Acc : 0.676, Auc : 0.786, Sensitive_Loss : 1.03452, Sensitive_Acc : 10.098, Sensitive_Auc : 0.825, Mean auc: 0.786, Run Time : 238.15 sec
INFO:root:2024-04-17 22:45:47, Best, Step : 300, Loss : 0.69296, Acc : 0.676, Auc : 0.786, Sensitive_Loss : 1.03452, Sensitive_Acc : 10.098, Sensitive_Auc : 0.825, Best Auc : 0.786
INFO:root:2024-04-17 22:45:59, Train, Epoch : 1, Step : 310, Loss : 0.57835, Acc : 0.716, Sensitive_Loss : 0.88104, Sensitive_Acc : 18.800, Run Time : 251.57 sec
INFO:root:2024-04-17 22:46:17, Train, Epoch : 1, Step : 320, Loss : 0.55372, Acc : 0.653, Sensitive_Loss : 0.85764, Sensitive_Acc : 14.300, Run Time : 17.56 sec
INFO:root:2024-04-17 22:46:36, Train, Epoch : 1, Step : 330, Loss : 0.65902, Acc : 0.691, Sensitive_Loss : 0.77092, Sensitive_Acc : 21.800, Run Time : 19.13 sec
INFO:root:2024-04-17 22:46:53, Train, Epoch : 1, Step : 340, Loss : 0.60273, Acc : 0.694, Sensitive_Loss : 0.85625, Sensitive_Acc : 23.600, Run Time : 17.12 sec
INFO:root:2024-04-17 22:47:12, Train, Epoch : 1, Step : 350, Loss : 0.63042, Acc : 0.744, Sensitive_Loss : 0.89748, Sensitive_Acc : 24.900, Run Time : 18.91 sec
INFO:root:2024-04-17 22:47:29, Train, Epoch : 1, Step : 360, Loss : 0.63738, Acc : 0.703, Sensitive_Loss : 0.81840, Sensitive_Acc : 14.300, Run Time : 16.74 sec
INFO:root:2024-04-17 22:47:47, Train, Epoch : 1, Step : 370, Loss : 0.58729, Acc : 0.681, Sensitive_Loss : 0.78844, Sensitive_Acc : 18.000, Run Time : 18.16 sec
INFO:root:2024-04-17 22:48:06, Train, Epoch : 1, Step : 380, Loss : 0.59694, Acc : 0.647, Sensitive_Loss : 0.74898, Sensitive_Acc : 17.500, Run Time : 19.28 sec
INFO:root:2024-04-17 22:48:23, Train, Epoch : 1, Step : 390, Loss : 0.54829, Acc : 0.728, Sensitive_Loss : 0.67623, Sensitive_Acc : 24.800, Run Time : 16.92 sec
INFO:root:2024-04-17 22:48:42, Train, Epoch : 1, Step : 400, Loss : 0.54991, Acc : 0.738, Sensitive_Loss : 0.74746, Sensitive_Acc : 20.400, Run Time : 19.00 sec
INFO:root:2024-04-17 22:52:40, Dev, Step : 400, Loss : 0.58063, Acc : 0.724, Auc : 0.794, Sensitive_Loss : 0.71415, Sensitive_Acc : 19.211, Sensitive_Auc : 0.850, Mean auc: 0.794, Run Time : 238.11 sec
INFO:root:2024-04-17 22:52:42, Best, Step : 400, Loss : 0.58063, Acc : 0.724, Auc : 0.794, Sensitive_Loss : 0.71415, Sensitive_Acc : 19.211, Sensitive_Auc : 0.850, Best Auc : 0.794
INFO:root:2024-04-17 22:52:55, Train, Epoch : 1, Step : 410, Loss : 0.60899, Acc : 0.741, Sensitive_Loss : 0.72587, Sensitive_Acc : 20.000, Run Time : 253.09 sec
INFO:root:2024-04-17 22:53:12, Train, Epoch : 1, Step : 420, Loss : 0.60928, Acc : 0.719, Sensitive_Loss : 0.71978, Sensitive_Acc : 21.100, Run Time : 17.07 sec
INFO:root:2024-04-17 22:53:31, Train, Epoch : 1, Step : 430, Loss : 0.54662, Acc : 0.722, Sensitive_Loss : 0.90957, Sensitive_Acc : 20.200, Run Time : 18.33 sec
INFO:root:2024-04-17 22:53:50, Train, Epoch : 1, Step : 440, Loss : 0.51336, Acc : 0.725, Sensitive_Loss : 0.72554, Sensitive_Acc : 18.800, Run Time : 18.81 sec
INFO:root:2024-04-17 22:54:07, Train, Epoch : 1, Step : 450, Loss : 0.56970, Acc : 0.738, Sensitive_Loss : 0.85365, Sensitive_Acc : 21.400, Run Time : 17.87 sec
INFO:root:2024-04-17 22:54:25, Train, Epoch : 1, Step : 460, Loss : 0.51696, Acc : 0.722, Sensitive_Loss : 0.80982, Sensitive_Acc : 22.400, Run Time : 17.48 sec
INFO:root:2024-04-17 22:54:45, Train, Epoch : 1, Step : 470, Loss : 0.54820, Acc : 0.734, Sensitive_Loss : 0.74130, Sensitive_Acc : 17.600, Run Time : 20.35 sec
INFO:root:2024-04-17 22:55:03, Train, Epoch : 1, Step : 480, Loss : 0.59566, Acc : 0.672, Sensitive_Loss : 0.64127, Sensitive_Acc : 21.000, Run Time : 17.70 sec
INFO:root:2024-04-17 22:55:20, Train, Epoch : 1, Step : 490, Loss : 0.65429, Acc : 0.688, Sensitive_Loss : 0.74818, Sensitive_Acc : 14.200, Run Time : 17.34 sec
INFO:root:2024-04-17 22:55:37, Train, Epoch : 1, Step : 500, Loss : 0.55624, Acc : 0.697, Sensitive_Loss : 0.63802, Sensitive_Acc : 19.000, Run Time : 16.46 sec
INFO:root:2024-04-17 22:59:34, Dev, Step : 500, Loss : 0.59371, Acc : 0.724, Auc : 0.788, Sensitive_Loss : 0.60015, Sensitive_Acc : 18.053, Sensitive_Auc : 0.929, Mean auc: 0.788, Run Time : 237.64 sec
INFO:root:2024-04-17 22:59:47, Train, Epoch : 1, Step : 510, Loss : 0.55947, Acc : 0.725, Sensitive_Loss : 0.63483, Sensitive_Acc : 19.800, Run Time : 250.43 sec
INFO:root:2024-04-17 23:00:06, Train, Epoch : 1, Step : 520, Loss : 0.57613, Acc : 0.744, Sensitive_Loss : 0.65932, Sensitive_Acc : 18.400, Run Time : 18.55 sec
INFO:root:2024-04-17 23:00:24, Train, Epoch : 1, Step : 530, Loss : 0.58311, Acc : 0.722, Sensitive_Loss : 0.57610, Sensitive_Acc : 16.000, Run Time : 18.01 sec
INFO:root:2024-04-17 23:00:58, Train, Epoch : 1, Step : 540, Loss : 0.59649, Acc : 0.659, Sensitive_Loss : 0.60346, Sensitive_Acc : 19.700, Run Time : 34.33 sec
INFO:root:2024-04-17 23:01:40, Train, Epoch : 1, Step : 550, Loss : 0.62189, Acc : 0.669, Sensitive_Loss : 0.59077, Sensitive_Acc : 23.700, Run Time : 41.94 sec
INFO:root:2024-04-17 23:02:24, Train, Epoch : 1, Step : 560, Loss : 0.55276, Acc : 0.719, Sensitive_Loss : 0.57665, Sensitive_Acc : 20.500, Run Time : 44.37 sec
INFO:root:2024-04-17 23:02:55, Train, Epoch : 1, Step : 570, Loss : 0.60791, Acc : 0.713, Sensitive_Loss : 0.58060, Sensitive_Acc : 16.700, Run Time : 30.96 sec
INFO:root:2024-04-17 23:03:35, Train, Epoch : 1, Step : 580, Loss : 0.52047, Acc : 0.747, Sensitive_Loss : 0.58287, Sensitive_Acc : 19.400, Run Time : 39.31 sec
INFO:root:2024-04-17 23:04:01, Train, Epoch : 1, Step : 590, Loss : 0.53455, Acc : 0.747, Sensitive_Loss : 0.67574, Sensitive_Acc : 16.500, Run Time : 26.03 sec
INFO:root:2024-04-17 23:04:29, Train, Epoch : 1, Step : 600, Loss : 0.61468, Acc : 0.694, Sensitive_Loss : 0.54564, Sensitive_Acc : 17.700, Run Time : 28.42 sec
INFO:root:2024-04-17 23:09:18, Dev, Step : 600, Loss : 0.58834, Acc : 0.727, Auc : 0.793, Sensitive_Loss : 0.48328, Sensitive_Acc : 20.263, Sensitive_Auc : 0.926, Mean auc: 0.793, Run Time : 288.91 sec
INFO:root:2024-04-17 23:09:31, Train, Epoch : 1, Step : 610, Loss : 0.54258, Acc : 0.756, Sensitive_Loss : 0.49799, Sensitive_Acc : 20.500, Run Time : 301.73 sec
INFO:root:2024-04-17 23:09:49, Train, Epoch : 1, Step : 620, Loss : 0.62463, Acc : 0.728, Sensitive_Loss : 0.54007, Sensitive_Acc : 16.500, Run Time : 18.09 sec
INFO:root:2024-04-17 23:10:08, Train, Epoch : 1, Step : 630, Loss : 0.53200, Acc : 0.759, Sensitive_Loss : 0.45357, Sensitive_Acc : 14.200, Run Time : 18.91 sec
INFO:root:2024-04-17 23:14:05
INFO:root:y_pred: [0.21645312 0.05859699 0.314482   ... 0.14282045 0.12207981 0.4100249 ]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.9328322e-01 1.4049987e-01 7.9053946e-02 8.9186192e-02 1.3175519e-01
 1.7551742e-01 2.6141901e-02 6.4162426e-03 4.0831819e-01 8.9771074e-01
 6.9083339e-01 2.0037495e-02 2.0826115e-01 5.9057417e-04 9.5013815e-01
 2.4273748e-02 1.4348517e-02 9.2197984e-01 6.3583207e-01 2.0680133e-02
 7.6868045e-01 4.9010046e-02 2.0451930e-01 1.3303860e-01 2.1358402e-01
 2.3367129e-01 4.3032351e-03 5.9061900e-02 2.7896913e-03 4.6794482e-02
 6.5055482e-02 6.4005566e-01 8.0833090e-03 7.3888528e-01 3.8539558e-03
 1.5748523e-05 8.6561218e-03 3.4138933e-01 4.1802064e-01 5.8640797e-02
 5.6287932e-01 5.4836464e-01 2.4612606e-01 1.5252653e-01 9.0877551e-01
 7.5528377e-01 3.9963847e-01 4.7114170e-01 1.4743260e-01 8.5825515e-01
 3.0738041e-01 9.4332469e-01 9.3910193e-01 8.9715207e-03 5.0033320e-02
 2.4044859e-01 4.4967337e-03 1.8603130e-01 8.9309561e-01 3.3428095e-02
 4.0769470e-03 3.6893897e-03 1.8716035e-02 1.3323036e-05 8.9947748e-01
 3.6808875e-01 6.1482051e-04 5.7917154e-01 2.2049674e-01 6.2493175e-01
 7.8556192e-01 9.5212710e-01 2.0752202e-01 3.0423403e-01 7.7866487e-02
 2.8230035e-01 3.9080408e-01 9.4728172e-02 7.8432849e-03 3.5305578e-02
 7.5642414e-02 7.2390592e-04 2.8724805e-01 9.0455616e-01 4.2513713e-01
 2.1459131e-01 3.3644199e-01 1.0543677e-01 4.6610716e-01 1.0311169e-03
 1.9212253e-01 7.3800975e-01 4.5292401e-03 1.5971962e-02 6.2442976e-01
 3.0517682e-01 2.6093097e-04 1.4937463e-01 7.3903650e-03 8.3440304e-02
 7.2188437e-02 1.8141292e-01 2.5675833e-01 8.1517771e-02 3.9447600e-01
 5.5142563e-02 5.7510406e-01 8.4979308e-01 7.4683440e-01 8.3875144e-01
 6.2860752e-04 9.4406247e-01 9.6348917e-01 2.2027176e-04 3.9175656e-01
 8.4653336e-01 3.4594789e-02 7.2289095e-03 5.2321422e-01 2.4717787e-01
 9.9841371e-02 2.1192005e-03 4.8475269e-02 3.8314732e-03 3.5938588e-01
 8.5187560e-01 1.4457694e-03 9.2419547e-01 1.4515086e-01 4.6252571e-02
 1.8483276e-02 6.2822139e-01 5.2204587e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-17 23:14:05, Dev, Step : 634, Loss : 0.57092, Acc : 0.740, Auc : 0.804, Sensitive_Loss : 0.45385, Sensitive_Acc : 20.429, Sensitive_Auc : 0.941, Mean auc: 0.804, Run Time : 231.65 sec
INFO:root:2024-04-17 23:14:05, Best, Step : 634, Loss : 0.57092, Acc : 0.740,Auc : 0.804, Best Auc : 0.804, Sensitive_Loss : 0.45385, Sensitive_Acc : 20.429, Sensitive_Auc : 0.941
INFO:root:2024-04-17 23:14:19, Train, Epoch : 2, Step : 640, Loss : 0.31216, Acc : 0.456, Sensitive_Loss : 0.23187, Sensitive_Acc : 13.400, Run Time : 12.74 sec
INFO:root:2024-04-17 23:14:37, Train, Epoch : 2, Step : 650, Loss : 0.57469, Acc : 0.697, Sensitive_Loss : 0.45902, Sensitive_Acc : 17.600, Run Time : 17.92 sec
INFO:root:2024-04-17 23:14:56, Train, Epoch : 2, Step : 660, Loss : 0.53361, Acc : 0.750, Sensitive_Loss : 0.44206, Sensitive_Acc : 20.500, Run Time : 18.86 sec
INFO:root:2024-04-17 23:15:14, Train, Epoch : 2, Step : 670, Loss : 0.54621, Acc : 0.747, Sensitive_Loss : 0.51139, Sensitive_Acc : 20.100, Run Time : 18.68 sec
INFO:root:2024-04-17 23:15:32, Train, Epoch : 2, Step : 680, Loss : 0.53743, Acc : 0.738, Sensitive_Loss : 0.44102, Sensitive_Acc : 16.100, Run Time : 17.63 sec
INFO:root:2024-04-17 23:15:50, Train, Epoch : 2, Step : 690, Loss : 0.47530, Acc : 0.766, Sensitive_Loss : 0.40478, Sensitive_Acc : 20.500, Run Time : 17.58 sec
INFO:root:2024-04-17 23:16:08, Train, Epoch : 2, Step : 700, Loss : 0.55801, Acc : 0.719, Sensitive_Loss : 0.47950, Sensitive_Acc : 25.600, Run Time : 18.24 sec
INFO:root:2024-04-17 23:20:04, Dev, Step : 700, Loss : 0.61891, Acc : 0.677, Auc : 0.808, Sensitive_Loss : 0.45525, Sensitive_Acc : 19.526, Sensitive_Auc : 0.936, Mean auc: 0.808, Run Time : 236.17 sec
INFO:root:2024-04-17 23:20:05, Best, Step : 700, Loss : 0.61891, Acc : 0.677, Auc : 0.808, Sensitive_Loss : 0.45525, Sensitive_Acc : 19.526, Sensitive_Auc : 0.936, Best Auc : 0.808
INFO:root:2024-04-17 23:20:19, Train, Epoch : 2, Step : 710, Loss : 0.52563, Acc : 0.756, Sensitive_Loss : 0.41915, Sensitive_Acc : 20.900, Run Time : 250.90 sec
INFO:root:2024-04-17 23:20:37, Train, Epoch : 2, Step : 720, Loss : 0.61391, Acc : 0.716, Sensitive_Loss : 0.48510, Sensitive_Acc : 15.800, Run Time : 18.59 sec
INFO:root:2024-04-17 23:20:55, Train, Epoch : 2, Step : 730, Loss : 0.50546, Acc : 0.762, Sensitive_Loss : 0.44001, Sensitive_Acc : 24.200, Run Time : 18.11 sec
INFO:root:2024-04-17 23:21:12, Train, Epoch : 2, Step : 740, Loss : 0.62049, Acc : 0.725, Sensitive_Loss : 0.42978, Sensitive_Acc : 21.800, Run Time : 16.58 sec
INFO:root:2024-04-17 23:21:30, Train, Epoch : 2, Step : 750, Loss : 0.53014, Acc : 0.728, Sensitive_Loss : 0.40060, Sensitive_Acc : 22.300, Run Time : 17.65 sec
INFO:root:2024-04-17 23:21:48, Train, Epoch : 2, Step : 760, Loss : 0.54455, Acc : 0.750, Sensitive_Loss : 0.39822, Sensitive_Acc : 20.900, Run Time : 17.98 sec
INFO:root:2024-04-17 23:22:07, Train, Epoch : 2, Step : 770, Loss : 0.59237, Acc : 0.731, Sensitive_Loss : 0.51363, Sensitive_Acc : 22.600, Run Time : 19.68 sec
INFO:root:2024-04-17 23:22:24, Train, Epoch : 2, Step : 780, Loss : 0.46939, Acc : 0.772, Sensitive_Loss : 0.46986, Sensitive_Acc : 18.900, Run Time : 16.95 sec
INFO:root:2024-04-17 23:22:41, Train, Epoch : 2, Step : 790, Loss : 0.52303, Acc : 0.741, Sensitive_Loss : 0.48053, Sensitive_Acc : 17.900, Run Time : 17.05 sec
INFO:root:2024-04-17 23:23:00, Train, Epoch : 2, Step : 800, Loss : 0.55047, Acc : 0.744, Sensitive_Loss : 0.45617, Sensitive_Acc : 19.600, Run Time : 18.60 sec
INFO:root:2024-04-17 23:26:58, Dev, Step : 800, Loss : 0.61234, Acc : 0.687, Auc : 0.797, Sensitive_Loss : 0.38891, Sensitive_Acc : 19.857, Sensitive_Auc : 0.971, Mean auc: 0.797, Run Time : 237.72 sec
INFO:root:2024-04-17 23:27:12, Train, Epoch : 2, Step : 810, Loss : 0.57277, Acc : 0.738, Sensitive_Loss : 0.37161, Sensitive_Acc : 20.400, Run Time : 251.60 sec
INFO:root:2024-04-17 23:27:30, Train, Epoch : 2, Step : 820, Loss : 0.63711, Acc : 0.731, Sensitive_Loss : 0.30483, Sensitive_Acc : 24.200, Run Time : 17.96 sec
INFO:root:2024-04-17 23:27:46, Train, Epoch : 2, Step : 830, Loss : 0.60335, Acc : 0.700, Sensitive_Loss : 0.32514, Sensitive_Acc : 23.300, Run Time : 16.67 sec
INFO:root:2024-04-17 23:28:05, Train, Epoch : 2, Step : 840, Loss : 0.60955, Acc : 0.697, Sensitive_Loss : 0.38785, Sensitive_Acc : 23.700, Run Time : 19.27 sec
INFO:root:2024-04-17 23:28:24, Train, Epoch : 2, Step : 850, Loss : 0.50341, Acc : 0.719, Sensitive_Loss : 0.41272, Sensitive_Acc : 20.800, Run Time : 18.49 sec
INFO:root:2024-04-17 23:28:42, Train, Epoch : 2, Step : 860, Loss : 0.52591, Acc : 0.744, Sensitive_Loss : 0.48098, Sensitive_Acc : 22.400, Run Time : 18.00 sec
INFO:root:2024-04-17 23:29:00, Train, Epoch : 2, Step : 870, Loss : 0.57973, Acc : 0.697, Sensitive_Loss : 0.34378, Sensitive_Acc : 25.000, Run Time : 18.09 sec
INFO:root:2024-04-17 23:29:17, Train, Epoch : 2, Step : 880, Loss : 0.55437, Acc : 0.753, Sensitive_Loss : 0.34476, Sensitive_Acc : 24.000, Run Time : 17.14 sec
INFO:root:2024-04-17 23:29:39, Train, Epoch : 2, Step : 890, Loss : 0.52224, Acc : 0.784, Sensitive_Loss : 0.32180, Sensitive_Acc : 22.300, Run Time : 22.16 sec
INFO:root:2024-04-17 23:30:07, Train, Epoch : 2, Step : 900, Loss : 0.54875, Acc : 0.716, Sensitive_Loss : 0.31552, Sensitive_Acc : 22.700, Run Time : 28.01 sec
INFO:root:2024-04-17 23:34:37, Dev, Step : 900, Loss : 0.56250, Acc : 0.743, Auc : 0.816, Sensitive_Loss : 0.56912, Sensitive_Acc : 17.195, Sensitive_Auc : 0.962, Mean auc: 0.816, Run Time : 269.86 sec
INFO:root:2024-04-17 23:34:38, Best, Step : 900, Loss : 0.56250, Acc : 0.743, Auc : 0.816, Sensitive_Loss : 0.56912, Sensitive_Acc : 17.195, Sensitive_Auc : 0.962, Best Auc : 0.816
INFO:root:2024-04-17 23:34:51, Train, Epoch : 2, Step : 910, Loss : 0.53099, Acc : 0.731, Sensitive_Loss : 0.26230, Sensitive_Acc : 19.000, Run Time : 283.64 sec
INFO:root:2024-04-17 23:35:08, Train, Epoch : 2, Step : 920, Loss : 0.49399, Acc : 0.750, Sensitive_Loss : 0.34091, Sensitive_Acc : 19.500, Run Time : 17.10 sec
INFO:root:2024-04-17 23:35:26, Train, Epoch : 2, Step : 930, Loss : 0.52557, Acc : 0.747, Sensitive_Loss : 0.33698, Sensitive_Acc : 23.600, Run Time : 17.90 sec
INFO:root:2024-04-17 23:35:46, Train, Epoch : 2, Step : 940, Loss : 0.55207, Acc : 0.750, Sensitive_Loss : 0.41076, Sensitive_Acc : 24.100, Run Time : 19.70 sec
INFO:root:2024-04-17 23:36:03, Train, Epoch : 2, Step : 950, Loss : 0.56856, Acc : 0.738, Sensitive_Loss : 0.30096, Sensitive_Acc : 22.400, Run Time : 17.32 sec
INFO:root:2024-04-17 23:36:21, Train, Epoch : 2, Step : 960, Loss : 0.53884, Acc : 0.731, Sensitive_Loss : 0.40380, Sensitive_Acc : 25.000, Run Time : 18.13 sec
INFO:root:2024-04-17 23:36:40, Train, Epoch : 2, Step : 970, Loss : 0.50252, Acc : 0.750, Sensitive_Loss : 0.28666, Sensitive_Acc : 20.100, Run Time : 19.33 sec
INFO:root:2024-04-17 23:36:58, Train, Epoch : 2, Step : 980, Loss : 0.56467, Acc : 0.750, Sensitive_Loss : 0.32890, Sensitive_Acc : 21.000, Run Time : 17.52 sec
INFO:root:2024-04-17 23:37:16, Train, Epoch : 2, Step : 990, Loss : 0.44271, Acc : 0.747, Sensitive_Loss : 0.26613, Sensitive_Acc : 23.100, Run Time : 17.75 sec
INFO:root:2024-04-17 23:37:33, Train, Epoch : 2, Step : 1000, Loss : 0.53884, Acc : 0.713, Sensitive_Loss : 0.34348, Sensitive_Acc : 18.700, Run Time : 17.49 sec
INFO:root:2024-04-17 23:41:29, Dev, Step : 1000, Loss : 0.54936, Acc : 0.748, Auc : 0.826, Sensitive_Loss : 0.28626, Sensitive_Acc : 21.301, Sensitive_Auc : 0.988, Mean auc: 0.826, Run Time : 235.60 sec
INFO:root:2024-04-17 23:41:29, Best, Step : 1000, Loss : 0.54936, Acc : 0.748, Auc : 0.826, Sensitive_Loss : 0.28626, Sensitive_Acc : 21.301, Sensitive_Auc : 0.988, Best Auc : 0.826
INFO:root:2024-04-17 23:41:42, Train, Epoch : 2, Step : 1010, Loss : 0.48977, Acc : 0.787, Sensitive_Loss : 0.28562, Sensitive_Acc : 23.600, Run Time : 248.71 sec
INFO:root:2024-04-17 23:42:01, Train, Epoch : 2, Step : 1020, Loss : 0.45877, Acc : 0.787, Sensitive_Loss : 0.34148, Sensitive_Acc : 24.800, Run Time : 19.31 sec
INFO:root:2024-04-17 23:42:19, Train, Epoch : 2, Step : 1030, Loss : 0.51228, Acc : 0.731, Sensitive_Loss : 0.29861, Sensitive_Acc : 18.400, Run Time : 17.85 sec
INFO:root:2024-04-17 23:42:36, Train, Epoch : 2, Step : 1040, Loss : 0.49528, Acc : 0.756, Sensitive_Loss : 0.38357, Sensitive_Acc : 17.400, Run Time : 16.87 sec
INFO:root:2024-04-17 23:42:55, Train, Epoch : 2, Step : 1050, Loss : 0.55998, Acc : 0.728, Sensitive_Loss : 0.32031, Sensitive_Acc : 19.700, Run Time : 19.34 sec
INFO:root:2024-04-17 23:43:12, Train, Epoch : 2, Step : 1060, Loss : 0.56995, Acc : 0.697, Sensitive_Loss : 0.39225, Sensitive_Acc : 22.600, Run Time : 16.71 sec
INFO:root:2024-04-17 23:43:30, Train, Epoch : 2, Step : 1070, Loss : 0.56592, Acc : 0.728, Sensitive_Loss : 0.28661, Sensitive_Acc : 21.500, Run Time : 17.73 sec
INFO:root:2024-04-17 23:43:47, Train, Epoch : 2, Step : 1080, Loss : 0.53676, Acc : 0.713, Sensitive_Loss : 0.26691, Sensitive_Acc : 24.100, Run Time : 17.55 sec
INFO:root:2024-04-17 23:44:03, Train, Epoch : 2, Step : 1090, Loss : 0.54158, Acc : 0.728, Sensitive_Loss : 0.31144, Sensitive_Acc : 22.800, Run Time : 16.10 sec
INFO:root:2024-04-17 23:44:21, Train, Epoch : 2, Step : 1100, Loss : 0.51455, Acc : 0.709, Sensitive_Loss : 0.27771, Sensitive_Acc : 22.300, Run Time : 18.10 sec
INFO:root:2024-04-17 23:48:20, Dev, Step : 1100, Loss : 0.58857, Acc : 0.731, Auc : 0.820, Sensitive_Loss : 0.31234, Sensitive_Acc : 20.669, Sensitive_Auc : 0.981, Mean auc: 0.820, Run Time : 238.15 sec
INFO:root:2024-04-17 23:48:34, Train, Epoch : 2, Step : 1110, Loss : 0.60969, Acc : 0.722, Sensitive_Loss : 0.29539, Sensitive_Acc : 22.200, Run Time : 252.19 sec
INFO:root:2024-04-17 23:48:51, Train, Epoch : 2, Step : 1120, Loss : 0.54464, Acc : 0.738, Sensitive_Loss : 0.25147, Sensitive_Acc : 24.200, Run Time : 17.43 sec
INFO:root:2024-04-17 23:49:07, Train, Epoch : 2, Step : 1130, Loss : 0.46666, Acc : 0.775, Sensitive_Loss : 0.27046, Sensitive_Acc : 21.500, Run Time : 15.96 sec
INFO:root:2024-04-17 23:49:26, Train, Epoch : 2, Step : 1140, Loss : 0.57767, Acc : 0.731, Sensitive_Loss : 0.27071, Sensitive_Acc : 19.700, Run Time : 18.86 sec
INFO:root:2024-04-17 23:49:44, Train, Epoch : 2, Step : 1150, Loss : 0.49833, Acc : 0.778, Sensitive_Loss : 0.40399, Sensitive_Acc : 18.100, Run Time : 18.14 sec
INFO:root:2024-04-17 23:50:02, Train, Epoch : 2, Step : 1160, Loss : 0.46476, Acc : 0.778, Sensitive_Loss : 0.32006, Sensitive_Acc : 17.600, Run Time : 17.71 sec
INFO:root:2024-04-17 23:50:20, Train, Epoch : 2, Step : 1170, Loss : 0.56982, Acc : 0.725, Sensitive_Loss : 0.24744, Sensitive_Acc : 13.800, Run Time : 18.01 sec
INFO:root:2024-04-17 23:50:39, Train, Epoch : 2, Step : 1180, Loss : 0.50995, Acc : 0.750, Sensitive_Loss : 0.28301, Sensitive_Acc : 15.000, Run Time : 18.86 sec
INFO:root:2024-04-17 23:50:54, Train, Epoch : 2, Step : 1190, Loss : 0.54694, Acc : 0.741, Sensitive_Loss : 0.25081, Sensitive_Acc : 21.500, Run Time : 15.77 sec
INFO:root:2024-04-17 23:51:13, Train, Epoch : 2, Step : 1200, Loss : 0.54476, Acc : 0.706, Sensitive_Loss : 0.33890, Sensitive_Acc : 21.000, Run Time : 18.77 sec
INFO:root:2024-04-17 23:55:09, Dev, Step : 1200, Loss : 0.56140, Acc : 0.729, Auc : 0.817, Sensitive_Loss : 0.31770, Sensitive_Acc : 20.353, Sensitive_Auc : 0.990, Mean auc: 0.817, Run Time : 235.76 sec
INFO:root:2024-04-17 23:55:22, Train, Epoch : 2, Step : 1210, Loss : 0.61317, Acc : 0.731, Sensitive_Loss : 0.26631, Sensitive_Acc : 18.900, Run Time : 248.52 sec
INFO:root:2024-04-17 23:55:39, Train, Epoch : 2, Step : 1220, Loss : 0.50322, Acc : 0.756, Sensitive_Loss : 0.31311, Sensitive_Acc : 23.200, Run Time : 17.56 sec
INFO:root:2024-04-17 23:55:57, Train, Epoch : 2, Step : 1230, Loss : 0.61025, Acc : 0.728, Sensitive_Loss : 0.25499, Sensitive_Acc : 21.100, Run Time : 17.92 sec
INFO:root:2024-04-17 23:56:16, Train, Epoch : 2, Step : 1240, Loss : 0.54511, Acc : 0.744, Sensitive_Loss : 0.29771, Sensitive_Acc : 22.100, Run Time : 18.68 sec
INFO:root:2024-04-17 23:56:33, Train, Epoch : 2, Step : 1250, Loss : 0.54111, Acc : 0.703, Sensitive_Loss : 0.27858, Sensitive_Acc : 23.700, Run Time : 17.33 sec
INFO:root:2024-04-17 23:56:52, Train, Epoch : 2, Step : 1260, Loss : 0.50588, Acc : 0.700, Sensitive_Loss : 0.23741, Sensitive_Acc : 19.000, Run Time : 18.53 sec
INFO:root:2024-04-18 00:00:58
INFO:root:y_pred: [0.33943412 0.04414943 0.49595436 ... 0.6856569  0.31063697 0.7399068 ]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [2.34149673e-04 8.22971463e-02 3.28917219e-03 4.72362898e-02
 1.32786199e-01 5.75731683e-04 5.10714389e-02 5.39254351e-03
 3.57653238e-02 9.95510340e-01 6.75135314e-01 1.83104143e-01
 2.04402413e-02 2.35318614e-04 9.97905135e-01 3.14575791e-01
 2.28244327e-02 9.93926585e-01 9.88940299e-01 9.82969301e-04
 7.21220553e-01 6.04550354e-04 4.00639027e-01 9.01744515e-02
 3.49158645e-01 1.73157990e-01 4.62599019e-05 1.30918147e-02
 3.72542650e-04 1.19337149e-01 8.82230997e-02 9.41496432e-01
 3.12149614e-01 9.26449537e-01 1.59098562e-02 6.00621934e-05
 1.33829294e-02 4.18944597e-01 6.65688992e-01 3.93631356e-03
 4.22049314e-01 9.17009950e-01 2.29407668e-01 2.21851980e-03
 9.06684995e-01 9.40931857e-01 6.28230929e-01 1.26349404e-01
 1.08992994e-01 9.52898562e-01 9.33030009e-01 9.94445264e-01
 9.62208748e-01 1.93815431e-04 2.24599928e-01 7.31618226e-01
 8.52373336e-03 1.42830715e-01 9.92379546e-01 3.76411737e-03
 8.46734736e-04 8.55528400e-04 2.37775356e-01 4.78263246e-05
 9.86727059e-01 1.03718810e-01 1.07661821e-04 4.18617696e-01
 2.02024430e-02 9.46439743e-01 9.97074842e-01 9.92654443e-01
 2.17196092e-01 6.91410244e-01 3.17222029e-02 7.17882156e-01
 2.26175040e-01 8.51733703e-03 2.01144312e-02 4.78115602e-04
 2.37318635e-01 1.46328693e-03 6.18125021e-01 9.75203156e-01
 1.69195592e-01 4.44785366e-03 2.86393464e-01 1.01226643e-02
 5.11490293e-02 9.42226325e-04 1.08844250e-01 9.27641869e-01
 2.69918419e-05 6.55750185e-03 2.78921932e-01 8.07524323e-01
 1.45820441e-05 2.92772979e-01 9.54773091e-03 4.67603980e-03
 8.19876697e-03 1.19077459e-01 8.27182010e-02 1.72786252e-03
 3.96793410e-02 5.87725872e-03 1.79350331e-01 7.83181548e-01
 9.48845863e-01 6.68366075e-01 1.21076846e-05 9.95404720e-01
 9.90381181e-01 2.57188603e-05 1.32884443e-01 8.31101954e-01
 1.25700623e-01 2.47334708e-02 2.52974927e-01 5.83332628e-02
 9.42178518e-02 8.92973767e-05 3.75751927e-02 2.09465288e-05
 4.12598550e-02 9.09015536e-01 6.03648823e-06 9.92513716e-01
 6.23554647e-01 4.70309615e-01 2.27516331e-02 7.89418042e-01
 3.43640168e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 00:00:58, Dev, Step : 1268, Loss : 0.60110, Acc : 0.686, Auc : 0.812, Sensitive_Loss : 0.34835, Sensitive_Acc : 20.068, Sensitive_Auc : 0.976, Mean auc: 0.812, Run Time : 232.92 sec
INFO:root:2024-04-18 00:01:05, Train, Epoch : 3, Step : 1270, Loss : 0.10588, Acc : 0.144, Sensitive_Loss : 0.05204, Sensitive_Acc : 5.400, Run Time : 6.18 sec
INFO:root:2024-04-18 00:01:24, Train, Epoch : 3, Step : 1280, Loss : 0.43589, Acc : 0.772, Sensitive_Loss : 0.30582, Sensitive_Acc : 21.700, Run Time : 18.85 sec
INFO:root:2024-04-18 00:01:43, Train, Epoch : 3, Step : 1290, Loss : 0.48164, Acc : 0.775, Sensitive_Loss : 0.29498, Sensitive_Acc : 21.000, Run Time : 19.16 sec
INFO:root:2024-04-18 00:02:02, Train, Epoch : 3, Step : 1300, Loss : 0.48581, Acc : 0.806, Sensitive_Loss : 0.21575, Sensitive_Acc : 17.200, Run Time : 18.34 sec
INFO:root:2024-04-18 00:05:57, Dev, Step : 1300, Loss : 0.53386, Acc : 0.756, Auc : 0.834, Sensitive_Loss : 0.39100, Sensitive_Acc : 19.180, Sensitive_Auc : 0.986, Mean auc: 0.834, Run Time : 235.57 sec
INFO:root:2024-04-18 00:05:58, Best, Step : 1300, Loss : 0.53386, Acc : 0.756, Auc : 0.834, Sensitive_Loss : 0.39100, Sensitive_Acc : 19.180, Sensitive_Auc : 0.986, Best Auc : 0.834
INFO:root:2024-04-18 00:06:11, Train, Epoch : 3, Step : 1310, Loss : 0.48661, Acc : 0.772, Sensitive_Loss : 0.31391, Sensitive_Acc : 22.700, Run Time : 249.43 sec
INFO:root:2024-04-18 00:06:29, Train, Epoch : 3, Step : 1320, Loss : 0.46808, Acc : 0.791, Sensitive_Loss : 0.16969, Sensitive_Acc : 20.500, Run Time : 18.19 sec
INFO:root:2024-04-18 00:06:47, Train, Epoch : 3, Step : 1330, Loss : 0.50820, Acc : 0.772, Sensitive_Loss : 0.28305, Sensitive_Acc : 20.800, Run Time : 17.62 sec
INFO:root:2024-04-18 00:07:06, Train, Epoch : 3, Step : 1340, Loss : 0.49945, Acc : 0.781, Sensitive_Loss : 0.29761, Sensitive_Acc : 24.600, Run Time : 18.81 sec
INFO:root:2024-04-18 00:07:24, Train, Epoch : 3, Step : 1350, Loss : 0.50409, Acc : 0.769, Sensitive_Loss : 0.21301, Sensitive_Acc : 20.300, Run Time : 18.48 sec
INFO:root:2024-04-18 00:07:43, Train, Epoch : 3, Step : 1360, Loss : 0.51610, Acc : 0.797, Sensitive_Loss : 0.17373, Sensitive_Acc : 20.400, Run Time : 18.46 sec
INFO:root:2024-04-18 00:08:01, Train, Epoch : 3, Step : 1370, Loss : 0.44017, Acc : 0.828, Sensitive_Loss : 0.30303, Sensitive_Acc : 17.400, Run Time : 18.10 sec
INFO:root:2024-04-18 00:08:19, Train, Epoch : 3, Step : 1380, Loss : 0.48908, Acc : 0.781, Sensitive_Loss : 0.23326, Sensitive_Acc : 17.800, Run Time : 18.50 sec
INFO:root:2024-04-18 00:08:37, Train, Epoch : 3, Step : 1390, Loss : 0.47609, Acc : 0.766, Sensitive_Loss : 0.23022, Sensitive_Acc : 16.100, Run Time : 18.02 sec
INFO:root:2024-04-18 00:08:56, Train, Epoch : 3, Step : 1400, Loss : 0.47421, Acc : 0.775, Sensitive_Loss : 0.23319, Sensitive_Acc : 19.400, Run Time : 18.42 sec
INFO:root:2024-04-18 00:12:51, Dev, Step : 1400, Loss : 0.52220, Acc : 0.759, Auc : 0.842, Sensitive_Loss : 0.27910, Sensitive_Acc : 20.820, Sensitive_Auc : 0.992, Mean auc: 0.842, Run Time : 234.84 sec
INFO:root:2024-04-18 00:12:51, Best, Step : 1400, Loss : 0.52220, Acc : 0.759, Auc : 0.842, Sensitive_Loss : 0.27910, Sensitive_Acc : 20.820, Sensitive_Auc : 0.992, Best Auc : 0.842
INFO:root:2024-04-18 00:13:04, Train, Epoch : 3, Step : 1410, Loss : 0.46693, Acc : 0.766, Sensitive_Loss : 0.22057, Sensitive_Acc : 23.800, Run Time : 248.51 sec
INFO:root:2024-04-18 00:13:24, Train, Epoch : 3, Step : 1420, Loss : 0.47740, Acc : 0.797, Sensitive_Loss : 0.18681, Sensitive_Acc : 20.000, Run Time : 19.56 sec
INFO:root:2024-04-18 00:13:41, Train, Epoch : 3, Step : 1430, Loss : 0.47897, Acc : 0.797, Sensitive_Loss : 0.22773, Sensitive_Acc : 20.000, Run Time : 16.86 sec
INFO:root:2024-04-18 00:13:59, Train, Epoch : 3, Step : 1440, Loss : 0.44432, Acc : 0.806, Sensitive_Loss : 0.20712, Sensitive_Acc : 20.400, Run Time : 18.56 sec
INFO:root:2024-04-18 00:14:16, Train, Epoch : 3, Step : 1450, Loss : 0.43495, Acc : 0.816, Sensitive_Loss : 0.28816, Sensitive_Acc : 20.800, Run Time : 16.90 sec
INFO:root:2024-04-18 00:14:34, Train, Epoch : 3, Step : 1460, Loss : 0.45504, Acc : 0.803, Sensitive_Loss : 0.23301, Sensitive_Acc : 13.300, Run Time : 18.10 sec
INFO:root:2024-04-18 00:14:52, Train, Epoch : 3, Step : 1470, Loss : 0.46479, Acc : 0.812, Sensitive_Loss : 0.19903, Sensitive_Acc : 22.400, Run Time : 18.12 sec
INFO:root:2024-04-18 00:15:11, Train, Epoch : 3, Step : 1480, Loss : 0.39286, Acc : 0.806, Sensitive_Loss : 0.25834, Sensitive_Acc : 19.700, Run Time : 18.23 sec
INFO:root:2024-04-18 00:15:29, Train, Epoch : 3, Step : 1490, Loss : 0.49434, Acc : 0.769, Sensitive_Loss : 0.21056, Sensitive_Acc : 21.400, Run Time : 18.17 sec
INFO:root:2024-04-18 00:15:47, Train, Epoch : 3, Step : 1500, Loss : 0.46274, Acc : 0.791, Sensitive_Loss : 0.16582, Sensitive_Acc : 26.100, Run Time : 17.80 sec
INFO:root:2024-04-18 00:19:45, Dev, Step : 1500, Loss : 0.52914, Acc : 0.760, Auc : 0.842, Sensitive_Loss : 0.27493, Sensitive_Acc : 20.820, Sensitive_Auc : 0.995, Mean auc: 0.842, Run Time : 238.00 sec
INFO:root:2024-04-18 00:19:45, Best, Step : 1500, Loss : 0.52914, Acc : 0.760, Auc : 0.842, Sensitive_Loss : 0.27493, Sensitive_Acc : 20.820, Sensitive_Auc : 0.995, Best Auc : 0.842
INFO:root:2024-04-18 00:19:58, Train, Epoch : 3, Step : 1510, Loss : 0.51948, Acc : 0.803, Sensitive_Loss : 0.20183, Sensitive_Acc : 22.400, Run Time : 251.33 sec
INFO:root:2024-04-18 00:20:17, Train, Epoch : 3, Step : 1520, Loss : 0.46236, Acc : 0.775, Sensitive_Loss : 0.24734, Sensitive_Acc : 21.100, Run Time : 18.73 sec
INFO:root:2024-04-18 00:20:36, Train, Epoch : 3, Step : 1530, Loss : 0.51813, Acc : 0.769, Sensitive_Loss : 0.20975, Sensitive_Acc : 21.900, Run Time : 19.59 sec
INFO:root:2024-04-18 00:20:54, Train, Epoch : 3, Step : 1540, Loss : 0.48300, Acc : 0.772, Sensitive_Loss : 0.18127, Sensitive_Acc : 23.100, Run Time : 17.77 sec
INFO:root:2024-04-18 00:21:12, Train, Epoch : 3, Step : 1550, Loss : 0.50128, Acc : 0.769, Sensitive_Loss : 0.25983, Sensitive_Acc : 21.900, Run Time : 18.11 sec
INFO:root:2024-04-18 00:21:29, Train, Epoch : 3, Step : 1560, Loss : 0.44897, Acc : 0.803, Sensitive_Loss : 0.21437, Sensitive_Acc : 22.700, Run Time : 16.90 sec
INFO:root:2024-04-18 00:21:47, Train, Epoch : 3, Step : 1570, Loss : 0.42370, Acc : 0.784, Sensitive_Loss : 0.28359, Sensitive_Acc : 25.300, Run Time : 18.04 sec
INFO:root:2024-04-18 00:22:04, Train, Epoch : 3, Step : 1580, Loss : 0.48623, Acc : 0.794, Sensitive_Loss : 0.20573, Sensitive_Acc : 20.900, Run Time : 17.08 sec
INFO:root:2024-04-18 00:22:23, Train, Epoch : 3, Step : 1590, Loss : 0.53643, Acc : 0.769, Sensitive_Loss : 0.23311, Sensitive_Acc : 24.600, Run Time : 19.29 sec
INFO:root:2024-04-18 00:22:40, Train, Epoch : 3, Step : 1600, Loss : 0.46301, Acc : 0.800, Sensitive_Loss : 0.18111, Sensitive_Acc : 11.600, Run Time : 17.01 sec
INFO:root:2024-04-18 00:26:38, Dev, Step : 1600, Loss : 0.52019, Acc : 0.767, Auc : 0.846, Sensitive_Loss : 0.26421, Sensitive_Acc : 20.805, Sensitive_Auc : 0.995, Mean auc: 0.846, Run Time : 237.36 sec
INFO:root:2024-04-18 00:26:39, Best, Step : 1600, Loss : 0.52019, Acc : 0.767, Auc : 0.846, Sensitive_Loss : 0.26421, Sensitive_Acc : 20.805, Sensitive_Auc : 0.995, Best Auc : 0.846
INFO:root:2024-04-18 00:26:52, Train, Epoch : 3, Step : 1610, Loss : 0.38351, Acc : 0.834, Sensitive_Loss : 0.20549, Sensitive_Acc : 23.500, Run Time : 252.00 sec
INFO:root:2024-04-18 00:27:09, Train, Epoch : 3, Step : 1620, Loss : 0.47601, Acc : 0.806, Sensitive_Loss : 0.15821, Sensitive_Acc : 22.100, Run Time : 16.89 sec
INFO:root:2024-04-18 00:27:28, Train, Epoch : 3, Step : 1630, Loss : 0.50776, Acc : 0.741, Sensitive_Loss : 0.25534, Sensitive_Acc : 24.500, Run Time : 19.07 sec
INFO:root:2024-04-18 00:27:45, Train, Epoch : 3, Step : 1640, Loss : 0.45744, Acc : 0.784, Sensitive_Loss : 0.23910, Sensitive_Acc : 21.500, Run Time : 16.31 sec
INFO:root:2024-04-18 00:28:04, Train, Epoch : 3, Step : 1650, Loss : 0.48691, Acc : 0.775, Sensitive_Loss : 0.19092, Sensitive_Acc : 22.200, Run Time : 19.45 sec
INFO:root:2024-04-18 00:28:23, Train, Epoch : 3, Step : 1660, Loss : 0.48916, Acc : 0.775, Sensitive_Loss : 0.19465, Sensitive_Acc : 26.500, Run Time : 18.43 sec
INFO:root:2024-04-18 00:28:39, Train, Epoch : 3, Step : 1670, Loss : 0.45822, Acc : 0.797, Sensitive_Loss : 0.23894, Sensitive_Acc : 20.000, Run Time : 16.39 sec
INFO:root:2024-04-18 00:28:57, Train, Epoch : 3, Step : 1680, Loss : 0.41237, Acc : 0.784, Sensitive_Loss : 0.16302, Sensitive_Acc : 20.500, Run Time : 17.92 sec
INFO:root:2024-04-18 00:29:15, Train, Epoch : 3, Step : 1690, Loss : 0.40303, Acc : 0.819, Sensitive_Loss : 0.24920, Sensitive_Acc : 19.000, Run Time : 18.49 sec
INFO:root:2024-04-18 00:29:34, Train, Epoch : 3, Step : 1700, Loss : 0.48072, Acc : 0.778, Sensitive_Loss : 0.22213, Sensitive_Acc : 18.600, Run Time : 18.42 sec
INFO:root:2024-04-18 00:33:31, Dev, Step : 1700, Loss : 0.51691, Acc : 0.767, Auc : 0.848, Sensitive_Loss : 0.23073, Sensitive_Acc : 21.286, Sensitive_Auc : 0.995, Mean auc: 0.848, Run Time : 237.16 sec
INFO:root:2024-04-18 00:33:32, Best, Step : 1700, Loss : 0.51691, Acc : 0.767, Auc : 0.848, Sensitive_Loss : 0.23073, Sensitive_Acc : 21.286, Sensitive_Auc : 0.995, Best Auc : 0.848
INFO:root:2024-04-18 00:33:46, Train, Epoch : 3, Step : 1710, Loss : 0.50692, Acc : 0.766, Sensitive_Loss : 0.21511, Sensitive_Acc : 17.500, Run Time : 251.95 sec
INFO:root:2024-04-18 00:34:04, Train, Epoch : 3, Step : 1720, Loss : 0.39730, Acc : 0.819, Sensitive_Loss : 0.26014, Sensitive_Acc : 21.200, Run Time : 18.19 sec
INFO:root:2024-04-18 00:34:21, Train, Epoch : 3, Step : 1730, Loss : 0.50107, Acc : 0.794, Sensitive_Loss : 0.28801, Sensitive_Acc : 20.100, Run Time : 17.26 sec
INFO:root:2024-04-18 00:34:40, Train, Epoch : 3, Step : 1740, Loss : 0.47644, Acc : 0.769, Sensitive_Loss : 0.20422, Sensitive_Acc : 23.800, Run Time : 18.50 sec
INFO:root:2024-04-18 00:34:57, Train, Epoch : 3, Step : 1750, Loss : 0.44321, Acc : 0.787, Sensitive_Loss : 0.19290, Sensitive_Acc : 20.800, Run Time : 17.14 sec
INFO:root:2024-04-18 00:35:15, Train, Epoch : 3, Step : 1760, Loss : 0.50529, Acc : 0.803, Sensitive_Loss : 0.23016, Sensitive_Acc : 18.900, Run Time : 18.20 sec
INFO:root:2024-04-18 00:35:32, Train, Epoch : 3, Step : 1770, Loss : 0.48061, Acc : 0.772, Sensitive_Loss : 0.21048, Sensitive_Acc : 25.600, Run Time : 16.91 sec
INFO:root:2024-04-18 00:35:50, Train, Epoch : 3, Step : 1780, Loss : 0.42985, Acc : 0.766, Sensitive_Loss : 0.24233, Sensitive_Acc : 17.700, Run Time : 17.91 sec
INFO:root:2024-04-18 00:36:08, Train, Epoch : 3, Step : 1790, Loss : 0.43893, Acc : 0.806, Sensitive_Loss : 0.21742, Sensitive_Acc : 18.400, Run Time : 18.55 sec
INFO:root:2024-04-18 00:36:27, Train, Epoch : 3, Step : 1800, Loss : 0.42505, Acc : 0.844, Sensitive_Loss : 0.21444, Sensitive_Acc : 23.600, Run Time : 18.78 sec
INFO:root:2024-04-18 00:40:23, Dev, Step : 1800, Loss : 0.51151, Acc : 0.771, Auc : 0.849, Sensitive_Loss : 0.23545, Sensitive_Acc : 20.880, Sensitive_Auc : 0.997, Mean auc: 0.849, Run Time : 235.38 sec
INFO:root:2024-04-18 00:40:23, Best, Step : 1800, Loss : 0.51151, Acc : 0.771, Auc : 0.849, Sensitive_Loss : 0.23545, Sensitive_Acc : 20.880, Sensitive_Auc : 0.997, Best Auc : 0.849
INFO:root:2024-04-18 00:40:37, Train, Epoch : 3, Step : 1810, Loss : 0.40864, Acc : 0.800, Sensitive_Loss : 0.22544, Sensitive_Acc : 21.900, Run Time : 249.48 sec
INFO:root:2024-04-18 00:40:54, Train, Epoch : 3, Step : 1820, Loss : 0.45367, Acc : 0.775, Sensitive_Loss : 0.22374, Sensitive_Acc : 21.600, Run Time : 17.77 sec
INFO:root:2024-04-18 00:41:13, Train, Epoch : 3, Step : 1830, Loss : 0.40816, Acc : 0.791, Sensitive_Loss : 0.20459, Sensitive_Acc : 15.000, Run Time : 18.54 sec
INFO:root:2024-04-18 00:41:30, Train, Epoch : 3, Step : 1840, Loss : 0.42006, Acc : 0.806, Sensitive_Loss : 0.23646, Sensitive_Acc : 22.400, Run Time : 16.96 sec
INFO:root:2024-04-18 00:41:50, Train, Epoch : 3, Step : 1850, Loss : 0.46028, Acc : 0.803, Sensitive_Loss : 0.30725, Sensitive_Acc : 22.500, Run Time : 20.12 sec
INFO:root:2024-04-18 00:42:07, Train, Epoch : 3, Step : 1860, Loss : 0.40378, Acc : 0.838, Sensitive_Loss : 0.16800, Sensitive_Acc : 16.600, Run Time : 17.09 sec
INFO:root:2024-04-18 00:42:25, Train, Epoch : 3, Step : 1870, Loss : 0.41454, Acc : 0.825, Sensitive_Loss : 0.14725, Sensitive_Acc : 22.200, Run Time : 17.86 sec
INFO:root:2024-04-18 00:42:42, Train, Epoch : 3, Step : 1880, Loss : 0.41655, Acc : 0.781, Sensitive_Loss : 0.18645, Sensitive_Acc : 22.500, Run Time : 17.51 sec
INFO:root:2024-04-18 00:43:00, Train, Epoch : 3, Step : 1890, Loss : 0.45719, Acc : 0.816, Sensitive_Loss : 0.28022, Sensitive_Acc : 23.200, Run Time : 17.44 sec
INFO:root:2024-04-18 00:43:19, Train, Epoch : 3, Step : 1900, Loss : 0.48043, Acc : 0.781, Sensitive_Loss : 0.19333, Sensitive_Acc : 19.000, Run Time : 19.15 sec
INFO:root:2024-04-18 00:47:16, Dev, Step : 1900, Loss : 0.50930, Acc : 0.773, Auc : 0.852, Sensitive_Loss : 0.23327, Sensitive_Acc : 20.910, Sensitive_Auc : 0.996, Mean auc: 0.852, Run Time : 237.30 sec
INFO:root:2024-04-18 00:47:17, Best, Step : 1900, Loss : 0.50930, Acc : 0.773, Auc : 0.852, Sensitive_Loss : 0.23327, Sensitive_Acc : 20.910, Sensitive_Auc : 0.996, Best Auc : 0.852
INFO:root:2024-04-18 00:51:07
INFO:root:y_pred: [0.18219757 0.01269758 0.23428018 ... 0.31425697 0.07753698 0.03888315]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.29558512e-05 3.80457030e-03 2.42589833e-03 3.86036724e-01
 1.99788049e-01 1.10991998e-03 2.16066302e-03 3.73148639e-03
 1.98402166e-01 9.94929075e-01 7.23349750e-01 9.76534095e-03
 4.82563488e-03 4.87959216e-04 9.94546056e-01 1.78614005e-01
 1.94148230e-03 9.81254101e-01 9.86693561e-01 5.20256872e-04
 7.51312971e-01 2.78095922e-05 2.90095776e-01 5.30161802e-03
 1.51159195e-02 1.15932837e-01 2.11786391e-04 1.07154787e-01
 1.33616326e-04 2.68432051e-02 4.02823538e-02 8.98166299e-01
 6.35643769e-03 8.84340107e-01 6.76779018e-05 5.68428322e-06
 5.85946906e-03 8.26323852e-02 4.44372177e-01 5.47409116e-04
 2.93832630e-01 9.33061659e-01 1.24199264e-01 5.49535362e-05
 9.68460679e-01 8.77240121e-01 3.31985652e-01 1.29682213e-01
 3.12873036e-01 9.58614469e-01 9.29523349e-01 9.94140804e-01
 9.76927996e-01 1.14919232e-04 5.11968285e-02 5.95505118e-01
 8.75011552e-04 4.04647412e-03 9.88303781e-01 9.76574593e-05
 3.04345886e-05 8.81218351e-03 1.83946802e-03 7.25864098e-08
 9.63112772e-01 4.05099720e-01 1.96434601e-04 4.53031570e-01
 3.91228497e-02 9.62999403e-01 9.96807456e-01 9.92779791e-01
 1.75659906e-03 5.80183208e-01 6.68156240e-03 6.07270420e-01
 4.08821404e-01 7.53060667e-05 3.11436295e-03 1.83392642e-03
 2.27960013e-02 3.87887121e-04 9.09042537e-01 9.68022704e-01
 3.56877036e-02 6.83784578e-03 2.15620056e-01 1.93647854e-02
 1.34106604e-02 6.65384359e-05 5.81340194e-02 7.94014856e-02
 4.81871211e-05 1.74886853e-04 1.69736251e-01 1.22331709e-01
 9.88013198e-06 7.13803232e-01 4.58157388e-04 2.17781213e-04
 1.72906853e-02 1.73245780e-02 4.17171121e-02 6.20325841e-03
 3.07116099e-03 4.91336395e-05 3.45924556e-01 7.61528492e-01
 1.71494886e-01 3.47652823e-01 5.55290026e-05 9.89840806e-01
 9.89619911e-01 5.21054826e-05 5.23249030e-01 3.79059672e-01
 1.49179772e-01 1.22275615e-05 1.74058676e-01 3.90552059e-02
 2.41636205e-02 2.44604278e-04 3.73677630e-03 2.97420393e-05
 2.96787526e-02 9.16253209e-01 2.97998895e-06 9.82633650e-01
 6.40094131e-02 2.23618314e-01 2.52834032e-03 5.46032667e-01
 1.10023257e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 00:51:07, Dev, Step : 1902, Loss : 0.51027, Acc : 0.774, Auc : 0.852, Sensitive_Loss : 0.23744, Sensitive_Acc : 20.910, Sensitive_Auc : 0.996, Mean auc: 0.852, Run Time : 229.10 sec
INFO:root:2024-04-18 00:51:08, Best, Step : 1902, Loss : 0.51027, Acc : 0.774,Auc : 0.852, Best Auc : 0.852, Sensitive_Loss : 0.23744, Sensitive_Acc : 20.910, Sensitive_Auc : 0.996
INFO:root:2024-04-18 00:51:25, Train, Epoch : 4, Step : 1910, Loss : 0.35432, Acc : 0.616, Sensitive_Loss : 0.12326, Sensitive_Acc : 18.300, Run Time : 16.39 sec
INFO:root:2024-04-18 00:51:43, Train, Epoch : 4, Step : 1920, Loss : 0.40633, Acc : 0.812, Sensitive_Loss : 0.17114, Sensitive_Acc : 20.700, Run Time : 17.43 sec
INFO:root:2024-04-18 00:52:00, Train, Epoch : 4, Step : 1930, Loss : 0.41816, Acc : 0.806, Sensitive_Loss : 0.20119, Sensitive_Acc : 20.600, Run Time : 16.80 sec
INFO:root:2024-04-18 00:52:18, Train, Epoch : 4, Step : 1940, Loss : 0.41671, Acc : 0.816, Sensitive_Loss : 0.20831, Sensitive_Acc : 23.500, Run Time : 18.10 sec
INFO:root:2024-04-18 00:52:36, Train, Epoch : 4, Step : 1950, Loss : 0.42862, Acc : 0.819, Sensitive_Loss : 0.20195, Sensitive_Acc : 20.600, Run Time : 18.23 sec
INFO:root:2024-04-18 00:52:53, Train, Epoch : 4, Step : 1960, Loss : 0.51966, Acc : 0.791, Sensitive_Loss : 0.21194, Sensitive_Acc : 20.900, Run Time : 16.55 sec
INFO:root:2024-04-18 00:53:10, Train, Epoch : 4, Step : 1970, Loss : 0.48753, Acc : 0.775, Sensitive_Loss : 0.16792, Sensitive_Acc : 23.300, Run Time : 17.64 sec
INFO:root:2024-04-18 00:53:27, Train, Epoch : 4, Step : 1980, Loss : 0.40170, Acc : 0.841, Sensitive_Loss : 0.16185, Sensitive_Acc : 22.300, Run Time : 16.85 sec
INFO:root:2024-04-18 00:53:43, Train, Epoch : 4, Step : 1990, Loss : 0.50460, Acc : 0.766, Sensitive_Loss : 0.16512, Sensitive_Acc : 23.500, Run Time : 16.15 sec
INFO:root:2024-04-18 00:54:01, Train, Epoch : 4, Step : 2000, Loss : 0.44838, Acc : 0.825, Sensitive_Loss : 0.19222, Sensitive_Acc : 23.200, Run Time : 17.99 sec
INFO:root:2024-04-18 00:57:52, Dev, Step : 2000, Loss : 0.51872, Acc : 0.773, Auc : 0.850, Sensitive_Loss : 0.21678, Sensitive_Acc : 21.376, Sensitive_Auc : 0.995, Mean auc: 0.850, Run Time : 230.85 sec
INFO:root:2024-04-18 00:58:04, Train, Epoch : 4, Step : 2010, Loss : 0.48997, Acc : 0.781, Sensitive_Loss : 0.17316, Sensitive_Acc : 20.900, Run Time : 243.11 sec
INFO:root:2024-04-18 00:58:21, Train, Epoch : 4, Step : 2020, Loss : 0.41064, Acc : 0.812, Sensitive_Loss : 0.14980, Sensitive_Acc : 25.800, Run Time : 17.15 sec
INFO:root:2024-04-18 00:58:40, Train, Epoch : 4, Step : 2030, Loss : 0.55234, Acc : 0.766, Sensitive_Loss : 0.20833, Sensitive_Acc : 22.300, Run Time : 18.47 sec
INFO:root:2024-04-18 00:58:56, Train, Epoch : 4, Step : 2040, Loss : 0.48702, Acc : 0.766, Sensitive_Loss : 0.24176, Sensitive_Acc : 22.900, Run Time : 16.30 sec
INFO:root:2024-04-18 00:59:13, Train, Epoch : 4, Step : 2050, Loss : 0.48143, Acc : 0.775, Sensitive_Loss : 0.16888, Sensitive_Acc : 21.500, Run Time : 16.32 sec
INFO:root:2024-04-18 00:59:29, Train, Epoch : 4, Step : 2060, Loss : 0.41128, Acc : 0.797, Sensitive_Loss : 0.19601, Sensitive_Acc : 20.300, Run Time : 16.91 sec
INFO:root:2024-04-18 00:59:47, Train, Epoch : 4, Step : 2070, Loss : 0.40676, Acc : 0.822, Sensitive_Loss : 0.19078, Sensitive_Acc : 24.100, Run Time : 17.54 sec
INFO:root:2024-04-18 01:00:03, Train, Epoch : 4, Step : 2080, Loss : 0.45959, Acc : 0.778, Sensitive_Loss : 0.19411, Sensitive_Acc : 18.900, Run Time : 16.20 sec
INFO:root:2024-04-18 01:00:21, Train, Epoch : 4, Step : 2090, Loss : 0.39255, Acc : 0.816, Sensitive_Loss : 0.22601, Sensitive_Acc : 24.900, Run Time : 17.74 sec
INFO:root:2024-04-18 01:00:38, Train, Epoch : 4, Step : 2100, Loss : 0.44498, Acc : 0.803, Sensitive_Loss : 0.16298, Sensitive_Acc : 21.800, Run Time : 16.92 sec
INFO:root:2024-04-18 01:04:31, Dev, Step : 2100, Loss : 0.52088, Acc : 0.773, Auc : 0.851, Sensitive_Loss : 0.23990, Sensitive_Acc : 20.925, Sensitive_Auc : 0.995, Mean auc: 0.851, Run Time : 232.72 sec
INFO:root:2024-04-18 01:04:43, Train, Epoch : 4, Step : 2110, Loss : 0.38193, Acc : 0.819, Sensitive_Loss : 0.18878, Sensitive_Acc : 17.900, Run Time : 244.81 sec
INFO:root:2024-04-18 01:05:00, Train, Epoch : 4, Step : 2120, Loss : 0.47331, Acc : 0.778, Sensitive_Loss : 0.16283, Sensitive_Acc : 18.400, Run Time : 17.61 sec
INFO:root:2024-04-18 01:05:17, Train, Epoch : 4, Step : 2130, Loss : 0.40369, Acc : 0.825, Sensitive_Loss : 0.18593, Sensitive_Acc : 21.500, Run Time : 16.94 sec
INFO:root:2024-04-18 01:05:35, Train, Epoch : 4, Step : 2140, Loss : 0.53376, Acc : 0.766, Sensitive_Loss : 0.16002, Sensitive_Acc : 24.500, Run Time : 17.58 sec
INFO:root:2024-04-18 01:05:51, Train, Epoch : 4, Step : 2150, Loss : 0.44512, Acc : 0.809, Sensitive_Loss : 0.20885, Sensitive_Acc : 20.700, Run Time : 16.08 sec
INFO:root:2024-04-18 01:06:09, Train, Epoch : 4, Step : 2160, Loss : 0.45132, Acc : 0.800, Sensitive_Loss : 0.15589, Sensitive_Acc : 20.900, Run Time : 17.65 sec
INFO:root:2024-04-18 01:06:26, Train, Epoch : 4, Step : 2170, Loss : 0.42521, Acc : 0.806, Sensitive_Loss : 0.18512, Sensitive_Acc : 19.300, Run Time : 17.09 sec
INFO:root:2024-04-18 01:06:43, Train, Epoch : 4, Step : 2180, Loss : 0.35727, Acc : 0.816, Sensitive_Loss : 0.22351, Sensitive_Acc : 17.900, Run Time : 17.67 sec
INFO:root:2024-04-18 01:07:01, Train, Epoch : 4, Step : 2190, Loss : 0.47573, Acc : 0.778, Sensitive_Loss : 0.13973, Sensitive_Acc : 16.800, Run Time : 17.52 sec
INFO:root:2024-04-18 01:07:18, Train, Epoch : 4, Step : 2200, Loss : 0.50355, Acc : 0.784, Sensitive_Loss : 0.24371, Sensitive_Acc : 24.300, Run Time : 17.60 sec
INFO:root:2024-04-18 01:11:18, Dev, Step : 2200, Loss : 0.51285, Acc : 0.776, Auc : 0.850, Sensitive_Loss : 0.20025, Sensitive_Acc : 21.376, Sensitive_Auc : 0.998, Mean auc: 0.850, Run Time : 239.78 sec
INFO:root:2024-04-18 01:11:30, Train, Epoch : 4, Step : 2210, Loss : 0.41246, Acc : 0.809, Sensitive_Loss : 0.15339, Sensitive_Acc : 21.100, Run Time : 252.06 sec
INFO:root:2024-04-18 01:11:47, Train, Epoch : 4, Step : 2220, Loss : 0.46758, Acc : 0.791, Sensitive_Loss : 0.16829, Sensitive_Acc : 23.700, Run Time : 16.95 sec
INFO:root:2024-04-18 01:12:05, Train, Epoch : 4, Step : 2230, Loss : 0.39986, Acc : 0.828, Sensitive_Loss : 0.18019, Sensitive_Acc : 23.600, Run Time : 17.73 sec
INFO:root:2024-04-18 01:12:23, Train, Epoch : 4, Step : 2240, Loss : 0.44445, Acc : 0.797, Sensitive_Loss : 0.16515, Sensitive_Acc : 18.100, Run Time : 17.49 sec
INFO:root:2024-04-18 01:12:40, Train, Epoch : 4, Step : 2250, Loss : 0.37053, Acc : 0.828, Sensitive_Loss : 0.21303, Sensitive_Acc : 16.400, Run Time : 16.96 sec
INFO:root:2024-04-18 01:12:57, Train, Epoch : 4, Step : 2260, Loss : 0.46254, Acc : 0.806, Sensitive_Loss : 0.18916, Sensitive_Acc : 18.900, Run Time : 17.11 sec
INFO:root:2024-04-18 01:13:13, Train, Epoch : 4, Step : 2270, Loss : 0.41587, Acc : 0.822, Sensitive_Loss : 0.19546, Sensitive_Acc : 19.700, Run Time : 16.44 sec
INFO:root:2024-04-18 01:13:31, Train, Epoch : 4, Step : 2280, Loss : 0.36210, Acc : 0.841, Sensitive_Loss : 0.21555, Sensitive_Acc : 21.600, Run Time : 17.67 sec
INFO:root:2024-04-18 01:13:48, Train, Epoch : 4, Step : 2290, Loss : 0.40814, Acc : 0.822, Sensitive_Loss : 0.15882, Sensitive_Acc : 19.000, Run Time : 17.17 sec
INFO:root:2024-04-18 01:14:05, Train, Epoch : 4, Step : 2300, Loss : 0.42060, Acc : 0.812, Sensitive_Loss : 0.20315, Sensitive_Acc : 21.800, Run Time : 17.18 sec
INFO:root:2024-04-18 01:17:58, Dev, Step : 2300, Loss : 0.51410, Acc : 0.769, Auc : 0.850, Sensitive_Loss : 0.21701, Sensitive_Acc : 21.195, Sensitive_Auc : 0.997, Mean auc: 0.850, Run Time : 232.69 sec
INFO:root:2024-04-18 01:18:10, Train, Epoch : 4, Step : 2310, Loss : 0.41772, Acc : 0.816, Sensitive_Loss : 0.24614, Sensitive_Acc : 20.200, Run Time : 244.63 sec
INFO:root:2024-04-18 01:18:27, Train, Epoch : 4, Step : 2320, Loss : 0.46890, Acc : 0.784, Sensitive_Loss : 0.18182, Sensitive_Acc : 22.000, Run Time : 17.28 sec
INFO:root:2024-04-18 01:18:44, Train, Epoch : 4, Step : 2330, Loss : 0.41546, Acc : 0.778, Sensitive_Loss : 0.17192, Sensitive_Acc : 21.700, Run Time : 17.04 sec
INFO:root:2024-04-18 01:19:03, Train, Epoch : 4, Step : 2340, Loss : 0.36634, Acc : 0.825, Sensitive_Loss : 0.17686, Sensitive_Acc : 18.500, Run Time : 19.06 sec
INFO:root:2024-04-18 01:19:20, Train, Epoch : 4, Step : 2350, Loss : 0.44300, Acc : 0.791, Sensitive_Loss : 0.15871, Sensitive_Acc : 22.600, Run Time : 16.77 sec
INFO:root:2024-04-18 01:19:37, Train, Epoch : 4, Step : 2360, Loss : 0.48575, Acc : 0.769, Sensitive_Loss : 0.15653, Sensitive_Acc : 23.000, Run Time : 16.65 sec
INFO:root:2024-04-18 01:19:54, Train, Epoch : 4, Step : 2370, Loss : 0.44442, Acc : 0.803, Sensitive_Loss : 0.15193, Sensitive_Acc : 24.200, Run Time : 17.74 sec
INFO:root:2024-04-18 01:20:13, Train, Epoch : 4, Step : 2380, Loss : 0.40675, Acc : 0.797, Sensitive_Loss : 0.22460, Sensitive_Acc : 19.900, Run Time : 18.44 sec
INFO:root:2024-04-18 01:20:29, Train, Epoch : 4, Step : 2390, Loss : 0.43200, Acc : 0.794, Sensitive_Loss : 0.26346, Sensitive_Acc : 20.000, Run Time : 16.21 sec
INFO:root:2024-04-18 01:20:46, Train, Epoch : 4, Step : 2400, Loss : 0.40163, Acc : 0.825, Sensitive_Loss : 0.25916, Sensitive_Acc : 19.200, Run Time : 16.62 sec
INFO:root:2024-04-18 01:24:38, Dev, Step : 2400, Loss : 0.52431, Acc : 0.775, Auc : 0.852, Sensitive_Loss : 0.21964, Sensitive_Acc : 21.030, Sensitive_Auc : 0.996, Mean auc: 0.852, Run Time : 232.35 sec
INFO:root:2024-04-18 01:24:50, Train, Epoch : 4, Step : 2410, Loss : 0.39626, Acc : 0.819, Sensitive_Loss : 0.14406, Sensitive_Acc : 21.500, Run Time : 244.02 sec
INFO:root:2024-04-18 01:25:08, Train, Epoch : 4, Step : 2420, Loss : 0.43585, Acc : 0.800, Sensitive_Loss : 0.17071, Sensitive_Acc : 23.100, Run Time : 18.48 sec
INFO:root:2024-04-18 01:25:24, Train, Epoch : 4, Step : 2430, Loss : 0.46825, Acc : 0.787, Sensitive_Loss : 0.14522, Sensitive_Acc : 19.500, Run Time : 16.33 sec
INFO:root:2024-04-18 01:25:43, Train, Epoch : 4, Step : 2440, Loss : 0.47729, Acc : 0.769, Sensitive_Loss : 0.18568, Sensitive_Acc : 16.800, Run Time : 18.68 sec
INFO:root:2024-04-18 01:26:01, Train, Epoch : 4, Step : 2450, Loss : 0.42313, Acc : 0.819, Sensitive_Loss : 0.16142, Sensitive_Acc : 25.100, Run Time : 18.13 sec
INFO:root:2024-04-18 01:26:19, Train, Epoch : 4, Step : 2460, Loss : 0.44717, Acc : 0.800, Sensitive_Loss : 0.18687, Sensitive_Acc : 21.300, Run Time : 18.17 sec
INFO:root:2024-04-18 01:26:36, Train, Epoch : 4, Step : 2470, Loss : 0.37917, Acc : 0.825, Sensitive_Loss : 0.20487, Sensitive_Acc : 24.200, Run Time : 16.78 sec
INFO:root:2024-04-18 01:26:53, Train, Epoch : 4, Step : 2480, Loss : 0.40885, Acc : 0.822, Sensitive_Loss : 0.19483, Sensitive_Acc : 24.200, Run Time : 16.30 sec
INFO:root:2024-04-18 01:27:11, Train, Epoch : 4, Step : 2490, Loss : 0.39693, Acc : 0.831, Sensitive_Loss : 0.10644, Sensitive_Acc : 20.700, Run Time : 18.66 sec
INFO:root:2024-04-18 01:27:28, Train, Epoch : 4, Step : 2500, Loss : 0.40131, Acc : 0.819, Sensitive_Loss : 0.26099, Sensitive_Acc : 20.100, Run Time : 16.71 sec
INFO:root:2024-04-18 01:31:21, Dev, Step : 2500, Loss : 0.51588, Acc : 0.774, Auc : 0.852, Sensitive_Loss : 0.20515, Sensitive_Acc : 21.376, Sensitive_Auc : 0.999, Mean auc: 0.852, Run Time : 232.78 sec
INFO:root:2024-04-18 01:31:34, Train, Epoch : 4, Step : 2510, Loss : 0.41342, Acc : 0.816, Sensitive_Loss : 0.16625, Sensitive_Acc : 20.900, Run Time : 245.74 sec
INFO:root:2024-04-18 01:31:52, Train, Epoch : 4, Step : 2520, Loss : 0.44312, Acc : 0.819, Sensitive_Loss : 0.22028, Sensitive_Acc : 16.000, Run Time : 18.44 sec
INFO:root:2024-04-18 01:32:09, Train, Epoch : 4, Step : 2530, Loss : 0.40011, Acc : 0.784, Sensitive_Loss : 0.16579, Sensitive_Acc : 19.600, Run Time : 17.39 sec
INFO:root:2024-04-18 01:36:05
INFO:root:y_pred: [0.1201814  0.00511772 0.18237688 ... 0.45307705 0.09341254 0.02189526]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.19499975e-06 4.56250476e-04 3.72794893e-04 1.98440179e-01
 1.30346879e-01 9.44359461e-04 2.44058698e-04 8.23629729e-04
 4.27349918e-02 9.96656775e-01 7.11548805e-01 1.08709384e-03
 3.39832186e-04 2.81911722e-04 9.95280445e-01 4.81158346e-02
 1.00434222e-03 9.86648619e-01 9.86716628e-01 2.09449674e-04
 7.31476128e-01 1.03620177e-05 4.85906713e-02 4.42084827e-04
 5.30267227e-03 5.27201407e-02 9.48135785e-05 5.45574613e-02
 2.48769884e-05 2.21010596e-02 5.64512191e-03 8.89755785e-01
 1.14978896e-03 9.21844482e-01 5.20652748e-06 9.66357447e-08
 1.02215586e-03 3.04777324e-02 3.15290809e-01 5.61279303e-05
 1.08208135e-01 9.23970163e-01 6.43727630e-02 6.90333366e-07
 9.69687581e-01 7.63665020e-01 1.18894145e-01 8.88857991e-02
 3.29950988e-01 9.57449079e-01 9.27859902e-01 9.95055199e-01
 9.75647926e-01 1.05571926e-05 6.04778016e-03 4.43565965e-01
 9.96697490e-05 3.42204497e-04 9.86365438e-01 2.64956216e-05
 1.66368009e-05 3.76839167e-03 3.04020871e-03 2.58836579e-08
 9.71443594e-01 1.83202118e-01 2.14784697e-04 4.05213475e-01
 3.21811326e-02 9.42764580e-01 9.96897101e-01 9.94154394e-01
 5.34628634e-04 5.68273485e-01 2.08400982e-03 3.50496262e-01
 3.73708665e-01 9.20135699e-06 1.52351713e-04 3.64426221e-03
 5.68515994e-03 3.59741767e-04 9.53890443e-01 9.45851147e-01
 8.83744564e-03 4.03981516e-03 1.21977903e-01 7.15677906e-03
 2.60184575e-02 5.30170037e-05 4.15467992e-02 2.04726844e-03
 1.31803725e-04 3.46024608e-05 5.34727145e-03 1.02448249e-02
 4.28317298e-06 8.11849177e-01 2.65092931e-05 2.62713638e-05
 1.27297603e-02 1.79480226e-03 2.03460362e-02 4.77845460e-04
 4.67185251e-04 1.10509418e-05 1.55190185e-01 7.23102868e-01
 1.77356586e-01 1.40927941e-01 3.12035809e-05 9.93905604e-01
 9.90043402e-01 3.75684904e-05 6.07343853e-01 2.45508719e-02
 7.37362877e-02 3.64342014e-07 9.82682258e-02 9.37975105e-03
 1.70436148e-02 9.14943375e-05 1.99466594e-03 3.96426276e-06
 1.74435470e-02 8.78754973e-01 8.41986434e-07 9.77036178e-01
 4.53928486e-03 2.04221278e-01 6.11329568e-04 2.29358599e-01
 1.23086139e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 01:36:05, Dev, Step : 2536, Loss : 0.51339, Acc : 0.777, Auc : 0.851, Sensitive_Loss : 0.20284, Sensitive_Acc : 21.286, Sensitive_Auc : 0.999, Mean auc: 0.851, Run Time : 227.52 sec
INFO:root:2024-04-18 01:36:16, Train, Epoch : 5, Step : 2540, Loss : 0.14697, Acc : 0.338, Sensitive_Loss : 0.07269, Sensitive_Acc : 8.700, Run Time : 9.38 sec
INFO:root:2024-04-18 01:36:35, Train, Epoch : 5, Step : 2550, Loss : 0.42256, Acc : 0.800, Sensitive_Loss : 0.22142, Sensitive_Acc : 19.300, Run Time : 19.17 sec
INFO:root:2024-04-18 01:36:52, Train, Epoch : 5, Step : 2560, Loss : 0.43608, Acc : 0.794, Sensitive_Loss : 0.14554, Sensitive_Acc : 22.200, Run Time : 17.61 sec
INFO:root:2024-04-18 01:37:09, Train, Epoch : 5, Step : 2570, Loss : 0.40022, Acc : 0.822, Sensitive_Loss : 0.15440, Sensitive_Acc : 24.200, Run Time : 17.04 sec
INFO:root:2024-04-18 01:37:27, Train, Epoch : 5, Step : 2580, Loss : 0.42639, Acc : 0.809, Sensitive_Loss : 0.15067, Sensitive_Acc : 21.700, Run Time : 17.59 sec
INFO:root:2024-04-18 01:37:45, Train, Epoch : 5, Step : 2590, Loss : 0.36866, Acc : 0.816, Sensitive_Loss : 0.11433, Sensitive_Acc : 24.000, Run Time : 18.44 sec
INFO:root:2024-04-18 01:38:03, Train, Epoch : 5, Step : 2600, Loss : 0.41057, Acc : 0.812, Sensitive_Loss : 0.19453, Sensitive_Acc : 25.600, Run Time : 17.86 sec
INFO:root:2024-04-18 01:41:55, Dev, Step : 2600, Loss : 0.51631, Acc : 0.777, Auc : 0.851, Sensitive_Loss : 0.20152, Sensitive_Acc : 21.165, Sensitive_Auc : 0.998, Mean auc: 0.851, Run Time : 232.21 sec
INFO:root:2024-04-18 01:42:08, Train, Epoch : 5, Step : 2610, Loss : 0.42919, Acc : 0.828, Sensitive_Loss : 0.16984, Sensitive_Acc : 21.000, Run Time : 244.72 sec
INFO:root:2024-04-18 01:42:27, Train, Epoch : 5, Step : 2620, Loss : 0.43352, Acc : 0.806, Sensitive_Loss : 0.22172, Sensitive_Acc : 24.700, Run Time : 18.94 sec
INFO:root:2024-04-18 01:42:44, Train, Epoch : 5, Step : 2630, Loss : 0.39530, Acc : 0.816, Sensitive_Loss : 0.12514, Sensitive_Acc : 20.500, Run Time : 16.60 sec
INFO:root:2024-04-18 01:43:01, Train, Epoch : 5, Step : 2640, Loss : 0.44663, Acc : 0.822, Sensitive_Loss : 0.13584, Sensitive_Acc : 18.500, Run Time : 17.47 sec
INFO:root:2024-04-18 01:43:19, Train, Epoch : 5, Step : 2650, Loss : 0.42291, Acc : 0.803, Sensitive_Loss : 0.13488, Sensitive_Acc : 22.800, Run Time : 18.20 sec
INFO:root:2024-04-18 01:43:38, Train, Epoch : 5, Step : 2660, Loss : 0.36212, Acc : 0.819, Sensitive_Loss : 0.11910, Sensitive_Acc : 25.300, Run Time : 18.68 sec
INFO:root:2024-04-18 01:43:54, Train, Epoch : 5, Step : 2670, Loss : 0.48423, Acc : 0.809, Sensitive_Loss : 0.17248, Sensitive_Acc : 18.200, Run Time : 16.15 sec
INFO:root:2024-04-18 01:44:13, Train, Epoch : 5, Step : 2680, Loss : 0.40019, Acc : 0.822, Sensitive_Loss : 0.13214, Sensitive_Acc : 11.700, Run Time : 18.69 sec
INFO:root:2024-04-18 01:44:29, Train, Epoch : 5, Step : 2690, Loss : 0.36249, Acc : 0.850, Sensitive_Loss : 0.14278, Sensitive_Acc : 23.000, Run Time : 16.59 sec
INFO:root:2024-04-18 01:44:47, Train, Epoch : 5, Step : 2700, Loss : 0.41787, Acc : 0.838, Sensitive_Loss : 0.16753, Sensitive_Acc : 21.800, Run Time : 17.82 sec
INFO:root:2024-04-18 01:48:39, Dev, Step : 2700, Loss : 0.51273, Acc : 0.782, Auc : 0.854, Sensitive_Loss : 0.22585, Sensitive_Acc : 21.165, Sensitive_Auc : 0.997, Mean auc: 0.854, Run Time : 231.49 sec
INFO:root:2024-04-18 01:48:39, Best, Step : 2700, Loss : 0.51273, Acc : 0.782, Auc : 0.854, Sensitive_Loss : 0.22585, Sensitive_Acc : 21.165, Sensitive_Auc : 0.997, Best Auc : 0.854
INFO:root:2024-04-18 01:48:52, Train, Epoch : 5, Step : 2710, Loss : 0.40596, Acc : 0.828, Sensitive_Loss : 0.13380, Sensitive_Acc : 20.600, Run Time : 244.44 sec
INFO:root:2024-04-18 01:49:10, Train, Epoch : 5, Step : 2720, Loss : 0.43066, Acc : 0.809, Sensitive_Loss : 0.15276, Sensitive_Acc : 23.400, Run Time : 18.27 sec
INFO:root:2024-04-18 01:49:28, Train, Epoch : 5, Step : 2730, Loss : 0.39335, Acc : 0.831, Sensitive_Loss : 0.10996, Sensitive_Acc : 20.600, Run Time : 18.34 sec
INFO:root:2024-04-18 01:49:46, Train, Epoch : 5, Step : 2740, Loss : 0.41611, Acc : 0.831, Sensitive_Loss : 0.17281, Sensitive_Acc : 16.400, Run Time : 17.41 sec
INFO:root:2024-04-18 01:50:03, Train, Epoch : 5, Step : 2750, Loss : 0.41732, Acc : 0.838, Sensitive_Loss : 0.16504, Sensitive_Acc : 19.900, Run Time : 17.32 sec
INFO:root:2024-04-18 01:50:20, Train, Epoch : 5, Step : 2760, Loss : 0.39936, Acc : 0.816, Sensitive_Loss : 0.16103, Sensitive_Acc : 17.600, Run Time : 17.57 sec
INFO:root:2024-04-18 01:50:38, Train, Epoch : 5, Step : 2770, Loss : 0.32858, Acc : 0.859, Sensitive_Loss : 0.14925, Sensitive_Acc : 20.500, Run Time : 17.68 sec
INFO:root:2024-04-18 01:50:55, Train, Epoch : 5, Step : 2780, Loss : 0.43255, Acc : 0.803, Sensitive_Loss : 0.18404, Sensitive_Acc : 25.200, Run Time : 17.38 sec
INFO:root:2024-04-18 01:51:13, Train, Epoch : 5, Step : 2790, Loss : 0.44055, Acc : 0.791, Sensitive_Loss : 0.11830, Sensitive_Acc : 17.700, Run Time : 17.98 sec
INFO:root:2024-04-18 01:51:30, Train, Epoch : 5, Step : 2800, Loss : 0.40776, Acc : 0.816, Sensitive_Loss : 0.22125, Sensitive_Acc : 24.700, Run Time : 16.36 sec
INFO:root:2024-04-18 01:55:23, Dev, Step : 2800, Loss : 0.51969, Acc : 0.773, Auc : 0.852, Sensitive_Loss : 0.19437, Sensitive_Acc : 21.346, Sensitive_Auc : 0.998, Mean auc: 0.852, Run Time : 233.01 sec
INFO:root:2024-04-18 01:55:36, Train, Epoch : 5, Step : 2810, Loss : 0.40906, Acc : 0.838, Sensitive_Loss : 0.17315, Sensitive_Acc : 20.400, Run Time : 245.66 sec
INFO:root:2024-04-18 01:55:53, Train, Epoch : 5, Step : 2820, Loss : 0.47975, Acc : 0.794, Sensitive_Loss : 0.16208, Sensitive_Acc : 20.800, Run Time : 17.12 sec
INFO:root:2024-04-18 01:56:10, Train, Epoch : 5, Step : 2830, Loss : 0.44771, Acc : 0.781, Sensitive_Loss : 0.21318, Sensitive_Acc : 21.300, Run Time : 17.72 sec
INFO:root:2024-04-18 01:56:29, Train, Epoch : 5, Step : 2840, Loss : 0.38428, Acc : 0.838, Sensitive_Loss : 0.25047, Sensitive_Acc : 22.200, Run Time : 18.71 sec
INFO:root:2024-04-18 01:56:46, Train, Epoch : 5, Step : 2850, Loss : 0.36536, Acc : 0.850, Sensitive_Loss : 0.21072, Sensitive_Acc : 22.900, Run Time : 16.65 sec
INFO:root:2024-04-18 01:57:02, Train, Epoch : 5, Step : 2860, Loss : 0.39841, Acc : 0.831, Sensitive_Loss : 0.13084, Sensitive_Acc : 24.300, Run Time : 16.73 sec
INFO:root:2024-04-18 01:57:21, Train, Epoch : 5, Step : 2870, Loss : 0.39071, Acc : 0.834, Sensitive_Loss : 0.17852, Sensitive_Acc : 25.600, Run Time : 18.54 sec
INFO:root:2024-04-18 01:57:38, Train, Epoch : 5, Step : 2880, Loss : 0.40827, Acc : 0.828, Sensitive_Loss : 0.15584, Sensitive_Acc : 17.800, Run Time : 16.99 sec
INFO:root:2024-04-18 01:57:56, Train, Epoch : 5, Step : 2890, Loss : 0.36956, Acc : 0.850, Sensitive_Loss : 0.18474, Sensitive_Acc : 25.000, Run Time : 18.19 sec
INFO:root:2024-04-18 01:58:15, Train, Epoch : 5, Step : 2900, Loss : 0.42909, Acc : 0.806, Sensitive_Loss : 0.18430, Sensitive_Acc : 20.500, Run Time : 18.87 sec
INFO:root:2024-04-18 02:02:06, Dev, Step : 2900, Loss : 0.51351, Acc : 0.778, Auc : 0.853, Sensitive_Loss : 0.22151, Sensitive_Acc : 21.075, Sensitive_Auc : 0.998, Mean auc: 0.853, Run Time : 231.28 sec
INFO:root:2024-04-18 02:02:20, Train, Epoch : 5, Step : 2910, Loss : 0.45743, Acc : 0.797, Sensitive_Loss : 0.12584, Sensitive_Acc : 18.900, Run Time : 244.60 sec
INFO:root:2024-04-18 02:02:38, Train, Epoch : 5, Step : 2920, Loss : 0.45527, Acc : 0.816, Sensitive_Loss : 0.15743, Sensitive_Acc : 21.900, Run Time : 18.50 sec
INFO:root:2024-04-18 02:02:55, Train, Epoch : 5, Step : 2930, Loss : 0.36399, Acc : 0.834, Sensitive_Loss : 0.15444, Sensitive_Acc : 24.400, Run Time : 16.94 sec
INFO:root:2024-04-18 02:03:13, Train, Epoch : 5, Step : 2940, Loss : 0.38654, Acc : 0.800, Sensitive_Loss : 0.14258, Sensitive_Acc : 23.600, Run Time : 17.98 sec
INFO:root:2024-04-18 02:03:29, Train, Epoch : 5, Step : 2950, Loss : 0.42553, Acc : 0.766, Sensitive_Loss : 0.16233, Sensitive_Acc : 23.600, Run Time : 16.31 sec
INFO:root:2024-04-18 02:03:48, Train, Epoch : 5, Step : 2960, Loss : 0.54950, Acc : 0.759, Sensitive_Loss : 0.19979, Sensitive_Acc : 21.700, Run Time : 18.46 sec
INFO:root:2024-04-18 02:04:05, Train, Epoch : 5, Step : 2970, Loss : 0.40080, Acc : 0.822, Sensitive_Loss : 0.14364, Sensitive_Acc : 20.000, Run Time : 17.09 sec
INFO:root:2024-04-18 02:04:23, Train, Epoch : 5, Step : 2980, Loss : 0.45928, Acc : 0.816, Sensitive_Loss : 0.21218, Sensitive_Acc : 19.800, Run Time : 17.64 sec
INFO:root:2024-04-18 02:04:39, Train, Epoch : 5, Step : 2990, Loss : 0.41898, Acc : 0.822, Sensitive_Loss : 0.18307, Sensitive_Acc : 23.300, Run Time : 16.67 sec
INFO:root:2024-04-18 02:04:58, Train, Epoch : 5, Step : 3000, Loss : 0.42339, Acc : 0.806, Sensitive_Loss : 0.16521, Sensitive_Acc : 23.100, Run Time : 18.91 sec
INFO:root:2024-04-18 02:08:50, Dev, Step : 3000, Loss : 0.50912, Acc : 0.777, Auc : 0.855, Sensitive_Loss : 0.22211, Sensitive_Acc : 21.060, Sensitive_Auc : 0.997, Mean auc: 0.855, Run Time : 231.62 sec
INFO:root:2024-04-18 02:08:50, Best, Step : 3000, Loss : 0.50912, Acc : 0.777, Auc : 0.855, Sensitive_Loss : 0.22211, Sensitive_Acc : 21.060, Sensitive_Auc : 0.997, Best Auc : 0.855
INFO:root:2024-04-18 02:09:04, Train, Epoch : 5, Step : 3010, Loss : 0.37384, Acc : 0.828, Sensitive_Loss : 0.16384, Sensitive_Acc : 21.900, Run Time : 246.07 sec
INFO:root:2024-04-18 02:09:22, Train, Epoch : 5, Step : 3020, Loss : 0.41443, Acc : 0.825, Sensitive_Loss : 0.20039, Sensitive_Acc : 23.300, Run Time : 17.73 sec
INFO:root:2024-04-18 02:09:39, Train, Epoch : 5, Step : 3030, Loss : 0.40955, Acc : 0.806, Sensitive_Loss : 0.17389, Sensitive_Acc : 22.900, Run Time : 17.09 sec
INFO:root:2024-04-18 02:09:57, Train, Epoch : 5, Step : 3040, Loss : 0.36371, Acc : 0.828, Sensitive_Loss : 0.15006, Sensitive_Acc : 21.700, Run Time : 18.05 sec
INFO:root:2024-04-18 02:10:14, Train, Epoch : 5, Step : 3050, Loss : 0.37981, Acc : 0.838, Sensitive_Loss : 0.15459, Sensitive_Acc : 20.500, Run Time : 16.57 sec
INFO:root:2024-04-18 02:10:31, Train, Epoch : 5, Step : 3060, Loss : 0.45881, Acc : 0.806, Sensitive_Loss : 0.16639, Sensitive_Acc : 21.900, Run Time : 17.10 sec
INFO:root:2024-04-18 02:10:48, Train, Epoch : 5, Step : 3070, Loss : 0.42414, Acc : 0.831, Sensitive_Loss : 0.17792, Sensitive_Acc : 22.400, Run Time : 17.39 sec
INFO:root:2024-04-18 02:11:06, Train, Epoch : 5, Step : 3080, Loss : 0.43884, Acc : 0.816, Sensitive_Loss : 0.14743, Sensitive_Acc : 20.500, Run Time : 17.68 sec
INFO:root:2024-04-18 02:11:24, Train, Epoch : 5, Step : 3090, Loss : 0.45925, Acc : 0.784, Sensitive_Loss : 0.12411, Sensitive_Acc : 19.800, Run Time : 18.22 sec
INFO:root:2024-04-18 02:11:41, Train, Epoch : 5, Step : 3100, Loss : 0.44592, Acc : 0.806, Sensitive_Loss : 0.13091, Sensitive_Acc : 19.300, Run Time : 16.71 sec
INFO:root:2024-04-18 02:15:34, Dev, Step : 3100, Loss : 0.51988, Acc : 0.775, Auc : 0.853, Sensitive_Loss : 0.18912, Sensitive_Acc : 21.436, Sensitive_Auc : 0.999, Mean auc: 0.853, Run Time : 233.22 sec
INFO:root:2024-04-18 02:15:48, Train, Epoch : 5, Step : 3110, Loss : 0.42258, Acc : 0.816, Sensitive_Loss : 0.11729, Sensitive_Acc : 19.800, Run Time : 246.89 sec
INFO:root:2024-04-18 02:16:05, Train, Epoch : 5, Step : 3120, Loss : 0.34387, Acc : 0.834, Sensitive_Loss : 0.16478, Sensitive_Acc : 22.000, Run Time : 17.12 sec
INFO:root:2024-04-18 02:16:23, Train, Epoch : 5, Step : 3130, Loss : 0.43179, Acc : 0.806, Sensitive_Loss : 0.22541, Sensitive_Acc : 21.900, Run Time : 17.74 sec
INFO:root:2024-04-18 02:16:39, Train, Epoch : 5, Step : 3140, Loss : 0.36123, Acc : 0.841, Sensitive_Loss : 0.15771, Sensitive_Acc : 18.300, Run Time : 16.96 sec
INFO:root:2024-04-18 02:16:57, Train, Epoch : 5, Step : 3150, Loss : 0.33506, Acc : 0.859, Sensitive_Loss : 0.19770, Sensitive_Acc : 24.000, Run Time : 17.75 sec
INFO:root:2024-04-18 02:17:15, Train, Epoch : 5, Step : 3160, Loss : 0.38796, Acc : 0.844, Sensitive_Loss : 0.14082, Sensitive_Acc : 22.600, Run Time : 17.64 sec
INFO:root:2024-04-18 02:17:31, Train, Epoch : 5, Step : 3170, Loss : 0.46767, Acc : 0.800, Sensitive_Loss : 0.14032, Sensitive_Acc : 15.900, Run Time : 15.81 sec
INFO:root:2024-04-18 02:21:20
INFO:root:y_pred: [0.1571288  0.00372548 0.19449882 ... 0.50143135 0.05844196 0.0119976 ]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [8.99366296e-06 2.70367775e-04 5.49560646e-03 2.42452770e-01
 1.21538267e-01 1.17872318e-03 1.01267258e-04 2.77069933e-03
 1.39729619e-01 9.97337043e-01 8.16711187e-01 8.62360816e-04
 3.04110115e-04 1.38215884e-03 9.96568561e-01 1.34315267e-01
 8.94776778e-04 9.90696549e-01 9.90955770e-01 8.30910343e-04
 7.41873443e-01 1.99537226e-05 2.80037615e-02 5.75876038e-04
 2.03752965e-02 2.93963999e-01 2.50179117e-04 4.11744975e-02
 4.00372628e-05 1.10471016e-02 3.61706130e-03 9.57684100e-01
 2.88394000e-03 9.36477125e-01 2.48111201e-06 1.07445111e-07
 6.99504802e-04 7.28514493e-02 5.06120920e-01 7.18684823e-05
 8.61515477e-02 9.28107858e-01 7.31959790e-02 8.55556607e-07
 9.81208086e-01 5.14082551e-01 7.15233237e-02 9.67044011e-02
 2.54225820e-01 9.58171010e-01 9.69678700e-01 9.96026158e-01
 9.80508029e-01 9.43502600e-05 5.19434772e-02 5.24136007e-01
 3.99770244e-04 9.51599155e-04 9.90385890e-01 2.04965072e-05
 1.21949670e-05 8.10204167e-03 7.98488501e-03 1.15501649e-07
 9.86726403e-01 3.13341796e-01 5.64686023e-04 5.63831687e-01
 3.77563238e-02 9.58301485e-01 9.97881472e-01 9.95416403e-01
 1.57549497e-04 6.60351336e-01 5.44078927e-03 4.72050965e-01
 4.12267417e-01 8.24687049e-06 3.40969622e-04 1.27367619e-02
 4.67391573e-02 5.07032790e-04 9.80659664e-01 9.60960567e-01
 7.25656422e-03 3.78564256e-03 1.86326995e-01 2.35936735e-02
 2.85187364e-02 3.55015654e-05 7.02197924e-02 2.05041864e-03
 3.51389172e-04 3.03201286e-05 4.03147237e-03 2.44611446e-02
 4.37705739e-06 8.96819830e-01 2.60769797e-04 1.22679266e-04
 1.82858743e-02 1.50466419e-03 7.90623799e-02 6.84987812e-04
 2.04837299e-04 9.10218387e-06 2.78041065e-01 8.17033350e-01
 1.53683066e-01 2.74675667e-01 1.85825091e-04 9.95447218e-01
 9.87133622e-01 4.36084301e-05 7.07625568e-01 4.32272954e-03
 1.20236389e-01 8.71907730e-07 2.03336850e-01 4.87189321e-03
 4.54516970e-02 3.74581636e-04 7.05944235e-03 1.07456890e-05
 1.44762667e-02 9.38784361e-01 8.85274403e-06 9.79342461e-01
 1.35801546e-03 2.51001805e-01 5.02568786e-04 1.14331484e-01
 2.44024960e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 02:21:20, Dev, Step : 3170, Loss : 0.51675, Acc : 0.769, Auc : 0.851, Sensitive_Loss : 0.21235, Sensitive_Acc : 20.925, Sensitive_Auc : 0.998, Mean auc: 0.851, Run Time : 228.86 sec
INFO:root:2024-04-18 02:21:41, Train, Epoch : 6, Step : 3180, Loss : 0.39188, Acc : 0.850, Sensitive_Loss : 0.15210, Sensitive_Acc : 18.700, Run Time : 20.36 sec
INFO:root:2024-04-18 02:22:00, Train, Epoch : 6, Step : 3190, Loss : 0.33732, Acc : 0.869, Sensitive_Loss : 0.12091, Sensitive_Acc : 16.900, Run Time : 18.76 sec
INFO:root:2024-04-18 02:22:18, Train, Epoch : 6, Step : 3200, Loss : 0.40765, Acc : 0.812, Sensitive_Loss : 0.12636, Sensitive_Acc : 18.200, Run Time : 18.76 sec
INFO:root:2024-04-18 02:26:51, Dev, Step : 3200, Loss : 0.51616, Acc : 0.777, Auc : 0.852, Sensitive_Loss : 0.18379, Sensitive_Acc : 21.526, Sensitive_Auc : 0.999, Mean auc: 0.852, Run Time : 272.08 sec
INFO:root:2024-04-18 02:27:03, Train, Epoch : 6, Step : 3210, Loss : 0.41767, Acc : 0.819, Sensitive_Loss : 0.15495, Sensitive_Acc : 20.600, Run Time : 284.95 sec
INFO:root:2024-04-18 02:27:22, Train, Epoch : 6, Step : 3220, Loss : 0.39844, Acc : 0.812, Sensitive_Loss : 0.18081, Sensitive_Acc : 21.400, Run Time : 18.53 sec
INFO:root:2024-04-18 02:27:39, Train, Epoch : 6, Step : 3230, Loss : 0.37607, Acc : 0.816, Sensitive_Loss : 0.16705, Sensitive_Acc : 21.900, Run Time : 16.91 sec
INFO:root:2024-04-18 02:27:58, Train, Epoch : 6, Step : 3240, Loss : 0.32895, Acc : 0.863, Sensitive_Loss : 0.16665, Sensitive_Acc : 20.500, Run Time : 18.94 sec
INFO:root:2024-04-18 02:28:15, Train, Epoch : 6, Step : 3250, Loss : 0.42876, Acc : 0.834, Sensitive_Loss : 0.15128, Sensitive_Acc : 15.300, Run Time : 17.55 sec
INFO:root:2024-04-18 02:28:31, Train, Epoch : 6, Step : 3260, Loss : 0.33301, Acc : 0.866, Sensitive_Loss : 0.11401, Sensitive_Acc : 22.400, Run Time : 16.04 sec
INFO:root:2024-04-18 02:28:49, Train, Epoch : 6, Step : 3270, Loss : 0.36607, Acc : 0.853, Sensitive_Loss : 0.12550, Sensitive_Acc : 20.900, Run Time : 17.27 sec
INFO:root:2024-04-18 02:29:07, Train, Epoch : 6, Step : 3280, Loss : 0.42608, Acc : 0.816, Sensitive_Loss : 0.12026, Sensitive_Acc : 22.600, Run Time : 18.36 sec
INFO:root:2024-04-18 02:29:23, Train, Epoch : 6, Step : 3290, Loss : 0.39085, Acc : 0.838, Sensitive_Loss : 0.12310, Sensitive_Acc : 22.400, Run Time : 15.67 sec
INFO:root:2024-04-18 02:29:42, Train, Epoch : 6, Step : 3300, Loss : 0.37302, Acc : 0.809, Sensitive_Loss : 0.20735, Sensitive_Acc : 20.500, Run Time : 19.13 sec
INFO:root:2024-04-18 02:33:34, Dev, Step : 3300, Loss : 0.51583, Acc : 0.774, Auc : 0.852, Sensitive_Loss : 0.18855, Sensitive_Acc : 21.346, Sensitive_Auc : 0.999, Mean auc: 0.852, Run Time : 231.95 sec
INFO:root:2024-04-18 02:33:48, Train, Epoch : 6, Step : 3310, Loss : 0.41577, Acc : 0.822, Sensitive_Loss : 0.13138, Sensitive_Acc : 17.700, Run Time : 245.83 sec
INFO:root:2024-04-18 02:34:05, Train, Epoch : 6, Step : 3320, Loss : 0.43105, Acc : 0.825, Sensitive_Loss : 0.15194, Sensitive_Acc : 26.600, Run Time : 16.81 sec
INFO:root:2024-04-18 02:34:22, Train, Epoch : 6, Step : 3330, Loss : 0.37465, Acc : 0.831, Sensitive_Loss : 0.14961, Sensitive_Acc : 15.500, Run Time : 17.00 sec
INFO:root:2024-04-18 02:34:39, Train, Epoch : 6, Step : 3340, Loss : 0.35390, Acc : 0.800, Sensitive_Loss : 0.21947, Sensitive_Acc : 19.600, Run Time : 17.71 sec
INFO:root:2024-04-18 02:34:57, Train, Epoch : 6, Step : 3350, Loss : 0.39928, Acc : 0.831, Sensitive_Loss : 0.20998, Sensitive_Acc : 21.200, Run Time : 17.77 sec
INFO:root:2024-04-18 02:35:15, Train, Epoch : 6, Step : 3360, Loss : 0.40123, Acc : 0.819, Sensitive_Loss : 0.12472, Sensitive_Acc : 21.700, Run Time : 18.40 sec
INFO:root:2024-04-18 02:35:33, Train, Epoch : 6, Step : 3370, Loss : 0.42551, Acc : 0.819, Sensitive_Loss : 0.12950, Sensitive_Acc : 22.400, Run Time : 17.48 sec
INFO:root:2024-04-18 02:35:51, Train, Epoch : 6, Step : 3380, Loss : 0.38832, Acc : 0.797, Sensitive_Loss : 0.11951, Sensitive_Acc : 20.400, Run Time : 17.90 sec
INFO:root:2024-04-18 02:36:07, Train, Epoch : 6, Step : 3390, Loss : 0.41414, Acc : 0.791, Sensitive_Loss : 0.18743, Sensitive_Acc : 25.800, Run Time : 16.62 sec
INFO:root:2024-04-18 02:36:26, Train, Epoch : 6, Step : 3400, Loss : 0.43758, Acc : 0.812, Sensitive_Loss : 0.16610, Sensitive_Acc : 21.600, Run Time : 18.53 sec
INFO:root:2024-04-18 02:40:18, Dev, Step : 3400, Loss : 0.53014, Acc : 0.774, Auc : 0.851, Sensitive_Loss : 0.20048, Sensitive_Acc : 21.436, Sensitive_Auc : 0.998, Mean auc: 0.851, Run Time : 231.58 sec
INFO:root:2024-04-18 02:40:30, Train, Epoch : 6, Step : 3410, Loss : 0.41646, Acc : 0.825, Sensitive_Loss : 0.17090, Sensitive_Acc : 23.800, Run Time : 244.42 sec
INFO:root:2024-04-18 02:40:49, Train, Epoch : 6, Step : 3420, Loss : 0.35023, Acc : 0.841, Sensitive_Loss : 0.15238, Sensitive_Acc : 22.200, Run Time : 18.49 sec
INFO:root:2024-04-18 02:41:06, Train, Epoch : 6, Step : 3430, Loss : 0.50039, Acc : 0.816, Sensitive_Loss : 0.19795, Sensitive_Acc : 23.100, Run Time : 17.63 sec
INFO:root:2024-04-18 02:41:24, Train, Epoch : 6, Step : 3440, Loss : 0.38969, Acc : 0.844, Sensitive_Loss : 0.12787, Sensitive_Acc : 23.500, Run Time : 17.45 sec
INFO:root:2024-04-18 02:41:41, Train, Epoch : 6, Step : 3450, Loss : 0.43900, Acc : 0.844, Sensitive_Loss : 0.10240, Sensitive_Acc : 20.100, Run Time : 17.40 sec
INFO:root:2024-04-18 02:41:58, Train, Epoch : 6, Step : 3460, Loss : 0.37486, Acc : 0.819, Sensitive_Loss : 0.10104, Sensitive_Acc : 22.100, Run Time : 16.88 sec
INFO:root:2024-04-18 02:42:16, Train, Epoch : 6, Step : 3470, Loss : 0.39433, Acc : 0.816, Sensitive_Loss : 0.15939, Sensitive_Acc : 21.100, Run Time : 18.19 sec
INFO:root:2024-04-18 02:42:33, Train, Epoch : 6, Step : 3480, Loss : 0.40868, Acc : 0.816, Sensitive_Loss : 0.15194, Sensitive_Acc : 20.300, Run Time : 16.76 sec
INFO:root:2024-04-18 02:42:51, Train, Epoch : 6, Step : 3490, Loss : 0.38228, Acc : 0.809, Sensitive_Loss : 0.19731, Sensitive_Acc : 22.000, Run Time : 17.44 sec
INFO:root:2024-04-18 02:43:08, Train, Epoch : 6, Step : 3500, Loss : 0.40096, Acc : 0.800, Sensitive_Loss : 0.17450, Sensitive_Acc : 21.200, Run Time : 17.76 sec
INFO:root:2024-04-18 02:47:03, Dev, Step : 3500, Loss : 0.54226, Acc : 0.770, Auc : 0.852, Sensitive_Loss : 0.18608, Sensitive_Acc : 21.421, Sensitive_Auc : 0.997, Mean auc: 0.852, Run Time : 234.91 sec
INFO:root:2024-04-18 02:47:17, Train, Epoch : 6, Step : 3510, Loss : 0.39043, Acc : 0.822, Sensitive_Loss : 0.15136, Sensitive_Acc : 23.500, Run Time : 248.16 sec
INFO:root:2024-04-18 02:47:37, Train, Epoch : 6, Step : 3520, Loss : 0.40039, Acc : 0.841, Sensitive_Loss : 0.16732, Sensitive_Acc : 23.900, Run Time : 20.61 sec
INFO:root:2024-04-18 02:47:56, Train, Epoch : 6, Step : 3530, Loss : 0.37430, Acc : 0.831, Sensitive_Loss : 0.14517, Sensitive_Acc : 26.200, Run Time : 19.36 sec
INFO:root:2024-04-18 02:48:18, Train, Epoch : 6, Step : 3540, Loss : 0.42424, Acc : 0.828, Sensitive_Loss : 0.13959, Sensitive_Acc : 17.600, Run Time : 21.13 sec
INFO:root:2024-04-18 02:48:35, Train, Epoch : 6, Step : 3550, Loss : 0.36928, Acc : 0.850, Sensitive_Loss : 0.22733, Sensitive_Acc : 25.100, Run Time : 17.39 sec
INFO:root:2024-04-18 02:48:53, Train, Epoch : 6, Step : 3560, Loss : 0.40693, Acc : 0.806, Sensitive_Loss : 0.12946, Sensitive_Acc : 21.900, Run Time : 18.01 sec
INFO:root:2024-04-18 02:49:11, Train, Epoch : 6, Step : 3570, Loss : 0.29569, Acc : 0.863, Sensitive_Loss : 0.13711, Sensitive_Acc : 18.100, Run Time : 17.58 sec
INFO:root:2024-04-18 02:49:29, Train, Epoch : 6, Step : 3580, Loss : 0.35279, Acc : 0.850, Sensitive_Loss : 0.12857, Sensitive_Acc : 17.100, Run Time : 18.61 sec
INFO:root:2024-04-18 02:49:46, Train, Epoch : 6, Step : 3590, Loss : 0.40606, Acc : 0.844, Sensitive_Loss : 0.10770, Sensitive_Acc : 19.800, Run Time : 17.06 sec
INFO:root:2024-04-18 02:50:04, Train, Epoch : 6, Step : 3600, Loss : 0.42961, Acc : 0.794, Sensitive_Loss : 0.14930, Sensitive_Acc : 21.000, Run Time : 17.49 sec
INFO:root:2024-04-18 02:53:56, Dev, Step : 3600, Loss : 0.51343, Acc : 0.782, Auc : 0.855, Sensitive_Loss : 0.18484, Sensitive_Acc : 21.316, Sensitive_Auc : 0.998, Mean auc: 0.855, Run Time : 232.24 sec
INFO:root:2024-04-18 02:53:57, Best, Step : 3600, Loss : 0.51343, Acc : 0.782, Auc : 0.855, Sensitive_Loss : 0.18484, Sensitive_Acc : 21.316, Sensitive_Auc : 0.998, Best Auc : 0.855
INFO:root:2024-04-18 02:54:10, Train, Epoch : 6, Step : 3610, Loss : 0.45083, Acc : 0.784, Sensitive_Loss : 0.09364, Sensitive_Acc : 23.700, Run Time : 246.15 sec
INFO:root:2024-04-18 02:54:28, Train, Epoch : 6, Step : 3620, Loss : 0.34785, Acc : 0.838, Sensitive_Loss : 0.11623, Sensitive_Acc : 17.000, Run Time : 18.23 sec
INFO:root:2024-04-18 02:54:46, Train, Epoch : 6, Step : 3630, Loss : 0.30416, Acc : 0.872, Sensitive_Loss : 0.14046, Sensitive_Acc : 23.800, Run Time : 18.19 sec
INFO:root:2024-04-18 02:55:03, Train, Epoch : 6, Step : 3640, Loss : 0.41400, Acc : 0.822, Sensitive_Loss : 0.17593, Sensitive_Acc : 18.400, Run Time : 16.32 sec
INFO:root:2024-04-18 02:55:20, Train, Epoch : 6, Step : 3650, Loss : 0.38475, Acc : 0.841, Sensitive_Loss : 0.06307, Sensitive_Acc : 20.000, Run Time : 17.18 sec
INFO:root:2024-04-18 02:55:37, Train, Epoch : 6, Step : 3660, Loss : 0.41252, Acc : 0.838, Sensitive_Loss : 0.13853, Sensitive_Acc : 23.900, Run Time : 17.17 sec
INFO:root:2024-04-18 02:55:55, Train, Epoch : 6, Step : 3670, Loss : 0.39935, Acc : 0.844, Sensitive_Loss : 0.15380, Sensitive_Acc : 21.900, Run Time : 18.15 sec
INFO:root:2024-04-18 02:56:14, Train, Epoch : 6, Step : 3680, Loss : 0.38461, Acc : 0.841, Sensitive_Loss : 0.10461, Sensitive_Acc : 21.400, Run Time : 18.56 sec
INFO:root:2024-04-18 02:56:31, Train, Epoch : 6, Step : 3690, Loss : 0.32019, Acc : 0.841, Sensitive_Loss : 0.09282, Sensitive_Acc : 23.200, Run Time : 16.78 sec
INFO:root:2024-04-18 02:56:47, Train, Epoch : 6, Step : 3700, Loss : 0.39107, Acc : 0.797, Sensitive_Loss : 0.15981, Sensitive_Acc : 21.200, Run Time : 16.87 sec
INFO:root:2024-04-18 03:00:42, Dev, Step : 3700, Loss : 0.55642, Acc : 0.768, Auc : 0.850, Sensitive_Loss : 0.18576, Sensitive_Acc : 21.406, Sensitive_Auc : 0.998, Mean auc: 0.850, Run Time : 234.23 sec
INFO:root:2024-04-18 03:00:54, Train, Epoch : 6, Step : 3710, Loss : 0.38973, Acc : 0.819, Sensitive_Loss : 0.16204, Sensitive_Acc : 24.900, Run Time : 246.96 sec
INFO:root:2024-04-18 03:01:11, Train, Epoch : 6, Step : 3720, Loss : 0.40543, Acc : 0.809, Sensitive_Loss : 0.16265, Sensitive_Acc : 23.800, Run Time : 17.10 sec
INFO:root:2024-04-18 03:01:30, Train, Epoch : 6, Step : 3730, Loss : 0.36553, Acc : 0.859, Sensitive_Loss : 0.10281, Sensitive_Acc : 19.700, Run Time : 19.03 sec
INFO:root:2024-04-18 03:01:49, Train, Epoch : 6, Step : 3740, Loss : 0.40929, Acc : 0.812, Sensitive_Loss : 0.15626, Sensitive_Acc : 22.100, Run Time : 18.01 sec
INFO:root:2024-04-18 03:02:04, Train, Epoch : 6, Step : 3750, Loss : 0.40756, Acc : 0.800, Sensitive_Loss : 0.12082, Sensitive_Acc : 21.800, Run Time : 15.66 sec
INFO:root:2024-04-18 03:02:23, Train, Epoch : 6, Step : 3760, Loss : 0.40525, Acc : 0.803, Sensitive_Loss : 0.10004, Sensitive_Acc : 22.600, Run Time : 18.53 sec
INFO:root:2024-04-18 03:02:40, Train, Epoch : 6, Step : 3770, Loss : 0.42003, Acc : 0.797, Sensitive_Loss : 0.19089, Sensitive_Acc : 21.500, Run Time : 17.01 sec
INFO:root:2024-04-18 03:02:57, Train, Epoch : 6, Step : 3780, Loss : 0.37814, Acc : 0.863, Sensitive_Loss : 0.16196, Sensitive_Acc : 21.300, Run Time : 16.97 sec
INFO:root:2024-04-18 03:03:15, Train, Epoch : 6, Step : 3790, Loss : 0.43620, Acc : 0.822, Sensitive_Loss : 0.18797, Sensitive_Acc : 20.900, Run Time : 18.56 sec
INFO:root:2024-04-18 03:03:33, Train, Epoch : 6, Step : 3800, Loss : 0.37850, Acc : 0.831, Sensitive_Loss : 0.12279, Sensitive_Acc : 22.500, Run Time : 17.45 sec
INFO:root:2024-04-18 03:07:25, Dev, Step : 3800, Loss : 0.53318, Acc : 0.776, Auc : 0.853, Sensitive_Loss : 0.19595, Sensitive_Acc : 21.451, Sensitive_Auc : 0.998, Mean auc: 0.853, Run Time : 232.40 sec
INFO:root:2024-04-18 03:11:15
INFO:root:y_pred: [0.03745736 0.00441309 0.12154614 ... 0.4693242  0.05051245 0.00379156]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.6157132e-06 1.0850850e-05 5.6100480e-04 2.6264235e-01 1.7178954e-01
 4.6445781e-04 4.6242654e-05 2.2472964e-04 6.5837033e-02 9.9648798e-01
 4.4976252e-01 9.4267662e-06 1.6412587e-05 3.7644029e-04 9.9595696e-01
 6.3263834e-02 1.5856921e-04 9.9219161e-01 9.8869413e-01 5.7732468e-05
 6.6944832e-01 1.9090528e-06 4.1308147e-03 6.6084326e-06 4.0430110e-04
 1.0945744e-01 1.4923881e-04 2.0962710e-02 7.2813300e-06 4.8134518e-03
 1.7610371e-04 9.2234564e-01 6.1385968e-04 9.2612463e-01 3.5093407e-07
 5.1070317e-09 2.1230819e-05 8.7820485e-02 2.6919892e-01 6.1618016e-06
 1.2294717e-02 8.9018780e-01 2.0879075e-02 4.1008864e-08 9.6868628e-01
 2.7435234e-01 1.2426238e-02 1.8675562e-02 5.1733810e-01 9.5141178e-01
 9.7912669e-01 9.9535179e-01 9.7759616e-01 1.6907541e-05 1.8604607e-03
 3.0984893e-01 4.3939785e-04 5.7405528e-06 9.8714334e-01 2.6045257e-07
 3.5563473e-06 7.9912450e-03 5.7339370e-03 6.2983601e-09 9.8591113e-01
 4.7823778e-01 9.8745397e-05 3.8567692e-01 6.3520721e-03 9.4555920e-01
 9.9749160e-01 9.9428856e-01 8.8143715e-06 6.9366932e-01 6.6411967e-04
 6.3830757e-01 1.3443623e-01 1.0307813e-06 1.1961775e-06 3.5596595e-03
 7.9571013e-04 4.2249818e-04 9.8386282e-01 9.4878364e-01 1.3399605e-03
 1.7335443e-04 6.5075323e-02 1.6222000e-03 3.6619245e-03 1.5417979e-06
 3.7887223e-02 2.2484337e-05 7.0073882e-05 2.3955017e-06 4.6020750e-06
 1.7831916e-03 4.6934332e-07 9.0071797e-01 2.6747617e-05 3.0916442e-05
 4.5320895e-03 2.0616147e-04 2.6795967e-02 2.9928002e-05 4.4663889e-06
 3.4379994e-08 7.2731934e-02 7.8523773e-01 4.9584880e-02 2.4873881e-01
 8.7940745e-05 9.9463731e-01 9.8960286e-01 9.8705103e-05 6.6222054e-01
 1.4211682e-03 3.0470140e-02 3.8549764e-08 1.7436823e-02 1.6236164e-04
 2.1107681e-03 2.8618076e-04 7.1055454e-04 7.2741494e-07 2.3152147e-04
 9.0225381e-01 4.0702233e-07 9.7002274e-01 4.1898591e-05 3.1153759e-01
 1.0385821e-05 6.0680476e-03 6.3318453e-09]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 03:11:15, Dev, Step : 3804, Loss : 0.53927, Acc : 0.774, Auc : 0.852, Sensitive_Loss : 0.19603, Sensitive_Acc : 21.301, Sensitive_Auc : 0.998, Mean auc: 0.852, Run Time : 227.88 sec
INFO:root:2024-04-18 03:11:29, Train, Epoch : 7, Step : 3810, Loss : 0.30273, Acc : 0.469, Sensitive_Loss : 0.10695, Sensitive_Acc : 10.100, Run Time : 12.64 sec
INFO:root:2024-04-18 03:11:47, Train, Epoch : 7, Step : 3820, Loss : 0.37094, Acc : 0.847, Sensitive_Loss : 0.09404, Sensitive_Acc : 18.700, Run Time : 18.67 sec
INFO:root:2024-04-18 03:12:07, Train, Epoch : 7, Step : 3830, Loss : 0.34543, Acc : 0.853, Sensitive_Loss : 0.12235, Sensitive_Acc : 21.800, Run Time : 19.39 sec
INFO:root:2024-04-18 03:12:24, Train, Epoch : 7, Step : 3840, Loss : 0.32663, Acc : 0.869, Sensitive_Loss : 0.08370, Sensitive_Acc : 21.200, Run Time : 16.85 sec
INFO:root:2024-04-18 03:12:40, Train, Epoch : 7, Step : 3850, Loss : 0.39219, Acc : 0.844, Sensitive_Loss : 0.13827, Sensitive_Acc : 22.700, Run Time : 16.10 sec
INFO:root:2024-04-18 03:12:58, Train, Epoch : 7, Step : 3860, Loss : 0.45507, Acc : 0.806, Sensitive_Loss : 0.18147, Sensitive_Acc : 24.000, Run Time : 18.21 sec
INFO:root:2024-04-18 03:13:17, Train, Epoch : 7, Step : 3870, Loss : 0.46192, Acc : 0.797, Sensitive_Loss : 0.11599, Sensitive_Acc : 23.900, Run Time : 19.34 sec
INFO:root:2024-04-18 03:13:33, Train, Epoch : 7, Step : 3880, Loss : 0.31343, Acc : 0.850, Sensitive_Loss : 0.11728, Sensitive_Acc : 19.300, Run Time : 15.30 sec
INFO:root:2024-04-18 03:13:51, Train, Epoch : 7, Step : 3890, Loss : 0.42648, Acc : 0.825, Sensitive_Loss : 0.10156, Sensitive_Acc : 17.600, Run Time : 18.02 sec
INFO:root:2024-04-18 03:14:09, Train, Epoch : 7, Step : 3900, Loss : 0.31336, Acc : 0.878, Sensitive_Loss : 0.10002, Sensitive_Acc : 22.200, Run Time : 18.66 sec
INFO:root:2024-04-18 03:18:00, Dev, Step : 3900, Loss : 0.52745, Acc : 0.774, Auc : 0.851, Sensitive_Loss : 0.17587, Sensitive_Acc : 21.436, Sensitive_Auc : 0.998, Mean auc: 0.851, Run Time : 230.87 sec
INFO:root:2024-04-18 03:18:14, Train, Epoch : 7, Step : 3910, Loss : 0.39653, Acc : 0.822, Sensitive_Loss : 0.12368, Sensitive_Acc : 23.900, Run Time : 244.28 sec
INFO:root:2024-04-18 03:18:31, Train, Epoch : 7, Step : 3920, Loss : 0.40329, Acc : 0.828, Sensitive_Loss : 0.12768, Sensitive_Acc : 23.700, Run Time : 17.52 sec
INFO:root:2024-04-18 03:18:50, Train, Epoch : 7, Step : 3930, Loss : 0.31143, Acc : 0.869, Sensitive_Loss : 0.12391, Sensitive_Acc : 24.500, Run Time : 18.74 sec
INFO:root:2024-04-18 03:19:07, Train, Epoch : 7, Step : 3940, Loss : 0.45600, Acc : 0.803, Sensitive_Loss : 0.12146, Sensitive_Acc : 24.900, Run Time : 17.61 sec
INFO:root:2024-04-18 03:19:24, Train, Epoch : 7, Step : 3950, Loss : 0.36846, Acc : 0.809, Sensitive_Loss : 0.12892, Sensitive_Acc : 19.800, Run Time : 17.01 sec
INFO:root:2024-04-18 03:19:41, Train, Epoch : 7, Step : 3960, Loss : 0.36885, Acc : 0.819, Sensitive_Loss : 0.11249, Sensitive_Acc : 25.500, Run Time : 16.17 sec
INFO:root:2024-04-18 03:19:58, Train, Epoch : 7, Step : 3970, Loss : 0.36367, Acc : 0.844, Sensitive_Loss : 0.11343, Sensitive_Acc : 23.100, Run Time : 17.02 sec
INFO:root:2024-04-18 03:20:16, Train, Epoch : 7, Step : 3980, Loss : 0.38389, Acc : 0.828, Sensitive_Loss : 0.13792, Sensitive_Acc : 17.900, Run Time : 18.16 sec
INFO:root:2024-04-18 03:20:33, Train, Epoch : 7, Step : 3990, Loss : 0.38838, Acc : 0.838, Sensitive_Loss : 0.17182, Sensitive_Acc : 16.200, Run Time : 17.31 sec
INFO:root:2024-04-18 03:20:52, Train, Epoch : 7, Step : 4000, Loss : 0.45040, Acc : 0.825, Sensitive_Loss : 0.17289, Sensitive_Acc : 19.800, Run Time : 18.71 sec
INFO:root:2024-04-18 03:24:42, Dev, Step : 4000, Loss : 0.54578, Acc : 0.773, Auc : 0.848, Sensitive_Loss : 0.18119, Sensitive_Acc : 21.436, Sensitive_Auc : 0.999, Mean auc: 0.848, Run Time : 230.20 sec
INFO:root:2024-04-18 03:24:56, Train, Epoch : 7, Step : 4010, Loss : 0.36651, Acc : 0.847, Sensitive_Loss : 0.16856, Sensitive_Acc : 21.500, Run Time : 243.70 sec
INFO:root:2024-04-18 03:25:14, Train, Epoch : 7, Step : 4020, Loss : 0.38663, Acc : 0.828, Sensitive_Loss : 0.11336, Sensitive_Acc : 24.700, Run Time : 18.13 sec
INFO:root:2024-04-18 03:25:30, Train, Epoch : 7, Step : 4030, Loss : 0.38969, Acc : 0.847, Sensitive_Loss : 0.08925, Sensitive_Acc : 17.500, Run Time : 16.57 sec
INFO:root:2024-04-18 03:25:49, Train, Epoch : 7, Step : 4040, Loss : 0.37509, Acc : 0.831, Sensitive_Loss : 0.11251, Sensitive_Acc : 21.600, Run Time : 18.51 sec
INFO:root:2024-04-18 03:26:06, Train, Epoch : 7, Step : 4050, Loss : 0.37187, Acc : 0.841, Sensitive_Loss : 0.10821, Sensitive_Acc : 22.100, Run Time : 16.68 sec
INFO:root:2024-04-18 03:26:24, Train, Epoch : 7, Step : 4060, Loss : 0.41501, Acc : 0.834, Sensitive_Loss : 0.14407, Sensitive_Acc : 17.200, Run Time : 18.36 sec
INFO:root:2024-04-18 03:26:41, Train, Epoch : 7, Step : 4070, Loss : 0.35136, Acc : 0.819, Sensitive_Loss : 0.11530, Sensitive_Acc : 17.700, Run Time : 17.05 sec
INFO:root:2024-04-18 03:26:58, Train, Epoch : 7, Step : 4080, Loss : 0.35881, Acc : 0.834, Sensitive_Loss : 0.13412, Sensitive_Acc : 19.400, Run Time : 17.43 sec
INFO:root:2024-04-18 03:27:15, Train, Epoch : 7, Step : 4090, Loss : 0.38472, Acc : 0.834, Sensitive_Loss : 0.09550, Sensitive_Acc : 19.700, Run Time : 17.11 sec
INFO:root:2024-04-18 03:27:34, Train, Epoch : 7, Step : 4100, Loss : 0.35722, Acc : 0.841, Sensitive_Loss : 0.11927, Sensitive_Acc : 20.400, Run Time : 18.46 sec
INFO:root:2024-04-18 03:31:26, Dev, Step : 4100, Loss : 0.51632, Acc : 0.776, Auc : 0.854, Sensitive_Loss : 0.19580, Sensitive_Acc : 21.226, Sensitive_Auc : 0.998, Mean auc: 0.854, Run Time : 232.43 sec
INFO:root:2024-04-18 03:31:40, Train, Epoch : 7, Step : 4110, Loss : 0.39031, Acc : 0.831, Sensitive_Loss : 0.13418, Sensitive_Acc : 21.900, Run Time : 246.02 sec
INFO:root:2024-04-18 03:31:57, Train, Epoch : 7, Step : 4120, Loss : 0.39666, Acc : 0.816, Sensitive_Loss : 0.13881, Sensitive_Acc : 20.500, Run Time : 17.26 sec
INFO:root:2024-04-18 03:32:15, Train, Epoch : 7, Step : 4130, Loss : 0.37355, Acc : 0.831, Sensitive_Loss : 0.13983, Sensitive_Acc : 23.300, Run Time : 17.37 sec
INFO:root:2024-04-18 03:32:32, Train, Epoch : 7, Step : 4140, Loss : 0.40763, Acc : 0.816, Sensitive_Loss : 0.18894, Sensitive_Acc : 24.600, Run Time : 17.01 sec
INFO:root:2024-04-18 03:32:49, Train, Epoch : 7, Step : 4150, Loss : 0.37798, Acc : 0.825, Sensitive_Loss : 0.15582, Sensitive_Acc : 18.300, Run Time : 17.80 sec
INFO:root:2024-04-18 03:33:07, Train, Epoch : 7, Step : 4160, Loss : 0.38618, Acc : 0.822, Sensitive_Loss : 0.14563, Sensitive_Acc : 22.900, Run Time : 18.06 sec
INFO:root:2024-04-18 03:33:25, Train, Epoch : 7, Step : 4170, Loss : 0.37405, Acc : 0.838, Sensitive_Loss : 0.12888, Sensitive_Acc : 24.300, Run Time : 17.82 sec
INFO:root:2024-04-18 03:33:42, Train, Epoch : 7, Step : 4180, Loss : 0.43993, Acc : 0.806, Sensitive_Loss : 0.14072, Sensitive_Acc : 14.200, Run Time : 16.36 sec
INFO:root:2024-04-18 03:34:01, Train, Epoch : 7, Step : 4190, Loss : 0.44265, Acc : 0.797, Sensitive_Loss : 0.13408, Sensitive_Acc : 21.700, Run Time : 18.92 sec
INFO:root:2024-04-18 03:34:19, Train, Epoch : 7, Step : 4200, Loss : 0.37597, Acc : 0.844, Sensitive_Loss : 0.11779, Sensitive_Acc : 20.700, Run Time : 17.98 sec
INFO:root:2024-04-18 03:38:10, Dev, Step : 4200, Loss : 0.53116, Acc : 0.781, Auc : 0.854, Sensitive_Loss : 0.19111, Sensitive_Acc : 21.211, Sensitive_Auc : 0.998, Mean auc: 0.854, Run Time : 231.83 sec
INFO:root:2024-04-18 03:38:23, Train, Epoch : 7, Step : 4210, Loss : 0.31653, Acc : 0.866, Sensitive_Loss : 0.13154, Sensitive_Acc : 23.000, Run Time : 243.94 sec
INFO:root:2024-04-18 03:38:41, Train, Epoch : 7, Step : 4220, Loss : 0.35723, Acc : 0.856, Sensitive_Loss : 0.11558, Sensitive_Acc : 24.300, Run Time : 18.75 sec
INFO:root:2024-04-18 03:38:58, Train, Epoch : 7, Step : 4230, Loss : 0.34596, Acc : 0.834, Sensitive_Loss : 0.11663, Sensitive_Acc : 23.200, Run Time : 16.66 sec
INFO:root:2024-04-18 03:39:16, Train, Epoch : 7, Step : 4240, Loss : 0.39647, Acc : 0.819, Sensitive_Loss : 0.17361, Sensitive_Acc : 16.100, Run Time : 17.76 sec
INFO:root:2024-04-18 03:39:34, Train, Epoch : 7, Step : 4250, Loss : 0.35736, Acc : 0.834, Sensitive_Loss : 0.12836, Sensitive_Acc : 21.100, Run Time : 18.08 sec
INFO:root:2024-04-18 03:39:51, Train, Epoch : 7, Step : 4260, Loss : 0.34331, Acc : 0.844, Sensitive_Loss : 0.08694, Sensitive_Acc : 21.900, Run Time : 17.33 sec
INFO:root:2024-04-18 03:40:10, Train, Epoch : 7, Step : 4270, Loss : 0.35129, Acc : 0.850, Sensitive_Loss : 0.09284, Sensitive_Acc : 15.900, Run Time : 18.69 sec
INFO:root:2024-04-18 03:40:27, Train, Epoch : 7, Step : 4280, Loss : 0.37889, Acc : 0.834, Sensitive_Loss : 0.09768, Sensitive_Acc : 22.300, Run Time : 17.47 sec
INFO:root:2024-04-18 03:40:45, Train, Epoch : 7, Step : 4290, Loss : 0.34878, Acc : 0.834, Sensitive_Loss : 0.14170, Sensitive_Acc : 22.600, Run Time : 17.66 sec
INFO:root:2024-04-18 03:41:04, Train, Epoch : 7, Step : 4300, Loss : 0.35028, Acc : 0.850, Sensitive_Loss : 0.10561, Sensitive_Acc : 21.600, Run Time : 18.97 sec
INFO:root:2024-04-18 03:44:55, Dev, Step : 4300, Loss : 0.53301, Acc : 0.773, Auc : 0.852, Sensitive_Loss : 0.18668, Sensitive_Acc : 21.571, Sensitive_Auc : 0.997, Mean auc: 0.852, Run Time : 230.75 sec
INFO:root:2024-04-18 03:45:08, Train, Epoch : 7, Step : 4310, Loss : 0.33375, Acc : 0.863, Sensitive_Loss : 0.16825, Sensitive_Acc : 19.400, Run Time : 244.51 sec
INFO:root:2024-04-18 03:45:27, Train, Epoch : 7, Step : 4320, Loss : 0.32845, Acc : 0.841, Sensitive_Loss : 0.18863, Sensitive_Acc : 22.600, Run Time : 18.22 sec
INFO:root:2024-04-18 03:45:44, Train, Epoch : 7, Step : 4330, Loss : 0.34721, Acc : 0.828, Sensitive_Loss : 0.10529, Sensitive_Acc : 21.800, Run Time : 17.30 sec
INFO:root:2024-04-18 03:46:00, Train, Epoch : 7, Step : 4340, Loss : 0.38782, Acc : 0.841, Sensitive_Loss : 0.11976, Sensitive_Acc : 19.500, Run Time : 16.35 sec
INFO:root:2024-04-18 03:46:18, Train, Epoch : 7, Step : 4350, Loss : 0.38772, Acc : 0.834, Sensitive_Loss : 0.12918, Sensitive_Acc : 19.900, Run Time : 17.38 sec
INFO:root:2024-04-18 03:46:36, Train, Epoch : 7, Step : 4360, Loss : 0.41205, Acc : 0.825, Sensitive_Loss : 0.14329, Sensitive_Acc : 22.300, Run Time : 17.92 sec
INFO:root:2024-04-18 03:46:54, Train, Epoch : 7, Step : 4370, Loss : 0.38188, Acc : 0.844, Sensitive_Loss : 0.16025, Sensitive_Acc : 25.400, Run Time : 18.26 sec
INFO:root:2024-04-18 03:47:12, Train, Epoch : 7, Step : 4380, Loss : 0.37232, Acc : 0.841, Sensitive_Loss : 0.17851, Sensitive_Acc : 15.100, Run Time : 17.88 sec
INFO:root:2024-04-18 03:47:30, Train, Epoch : 7, Step : 4390, Loss : 0.36605, Acc : 0.844, Sensitive_Loss : 0.12813, Sensitive_Acc : 23.900, Run Time : 18.02 sec
INFO:root:2024-04-18 03:47:45, Train, Epoch : 7, Step : 4400, Loss : 0.34964, Acc : 0.838, Sensitive_Loss : 0.20635, Sensitive_Acc : 16.000, Run Time : 15.50 sec
INFO:root:2024-04-18 03:51:39, Dev, Step : 4400, Loss : 0.55291, Acc : 0.773, Auc : 0.850, Sensitive_Loss : 0.18216, Sensitive_Acc : 20.955, Sensitive_Auc : 0.998, Mean auc: 0.850, Run Time : 233.42 sec
INFO:root:2024-04-18 03:51:53, Train, Epoch : 7, Step : 4410, Loss : 0.38155, Acc : 0.856, Sensitive_Loss : 0.12325, Sensitive_Acc : 22.900, Run Time : 247.40 sec
INFO:root:2024-04-18 03:52:09, Train, Epoch : 7, Step : 4420, Loss : 0.36054, Acc : 0.831, Sensitive_Loss : 0.11180, Sensitive_Acc : 22.200, Run Time : 16.76 sec
INFO:root:2024-04-18 03:52:27, Train, Epoch : 7, Step : 4430, Loss : 0.31272, Acc : 0.875, Sensitive_Loss : 0.15836, Sensitive_Acc : 20.000, Run Time : 17.31 sec
INFO:root:2024-04-18 03:56:27
INFO:root:y_pred: [0.0563308  0.00435346 0.12959802 ... 0.43372753 0.09937865 0.00149944]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [8.2178063e-05 3.8210594e-05 1.4996185e-03 1.9831233e-02 6.1190981e-02
 1.3665335e-04 3.0679545e-05 2.5892388e-03 4.1564900e-02 9.9680519e-01
 6.6293007e-01 9.5408777e-06 1.2358958e-04 4.6695687e-04 9.9703288e-01
 7.7407248e-02 1.3818081e-04 9.9402088e-01 9.8877597e-01 2.1499475e-03
 8.1796664e-01 5.7889320e-06 1.8688796e-02 6.2701351e-05 1.6824426e-03
 2.9863235e-01 1.3185940e-05 4.4018854e-04 3.5181342e-06 4.3555189e-04
 5.1685062e-04 9.5119566e-01 3.7220288e-03 9.3719745e-01 4.1653473e-07
 9.7325511e-08 7.1614125e-04 7.7273697e-02 6.1555464e-02 5.1911429e-06
 9.7055333e-03 9.0171027e-01 1.2022565e-02 5.4153286e-08 9.7503304e-01
 3.6096406e-01 7.0925407e-02 7.2888047e-02 1.3491529e-01 9.6125114e-01
 9.8524481e-01 9.9721044e-01 9.8983663e-01 1.4595203e-04 2.0520838e-02
 5.2304125e-01 3.2983118e-04 1.9959889e-05 9.9043965e-01 4.4261898e-07
 7.0984088e-06 2.6999196e-04 3.4192771e-02 6.0705744e-08 9.8618364e-01
 2.7106503e-01 1.8948746e-04 3.3568656e-01 2.8170416e-02 9.6009749e-01
 9.9854302e-01 9.9463552e-01 6.1393366e-06 6.2441015e-01 8.1810472e-04
 2.8591609e-01 2.3325813e-01 2.0647271e-06 2.3052542e-05 3.4622913e-03
 6.4384104e-03 2.6045251e-04 9.8600578e-01 9.6348566e-01 2.7265353e-04
 7.8936829e-04 2.9830182e-01 5.4146943e-04 3.9976346e-01 9.3402596e-06
 6.6714309e-02 1.8462764e-04 2.0123237e-04 3.4537781e-05 4.1700036e-06
 1.0459477e-02 4.1647181e-06 8.6253399e-01 7.5462100e-04 1.0873068e-04
 2.9373074e-02 4.6170378e-04 6.4088695e-02 1.7929566e-04 3.6929125e-06
 7.3123340e-07 2.2933464e-01 8.4170318e-01 4.0125126e-01 3.3148327e-01
 8.3518098e-05 9.9560320e-01 9.9349976e-01 8.1601451e-05 7.4739611e-01
 4.1599836e-02 7.3236264e-02 1.7195831e-06 3.5373133e-02 7.3695468e-04
 4.4920333e-03 3.0767766e-04 3.1729951e-03 2.5745726e-06 1.7827207e-03
 9.6576357e-01 8.0629197e-06 9.8908699e-01 4.9980834e-05 2.0597357e-01
 1.3245258e-04 1.0514360e-01 4.2379675e-08]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 03:56:27, Dev, Step : 4438, Loss : 0.54208, Acc : 0.770, Auc : 0.849, Sensitive_Loss : 0.19059, Sensitive_Acc : 21.346, Sensitive_Auc : 0.997, Mean auc: 0.849, Run Time : 228.23 sec
INFO:root:2024-04-18 03:56:34, Train, Epoch : 8, Step : 4440, Loss : 0.06373, Acc : 0.172, Sensitive_Loss : 0.02467, Sensitive_Acc : 3.800, Run Time : 5.60 sec
INFO:root:2024-04-18 03:56:53, Train, Epoch : 8, Step : 4450, Loss : 0.30579, Acc : 0.853, Sensitive_Loss : 0.15443, Sensitive_Acc : 19.700, Run Time : 18.85 sec
INFO:root:2024-04-18 03:57:11, Train, Epoch : 8, Step : 4460, Loss : 0.31387, Acc : 0.863, Sensitive_Loss : 0.09851, Sensitive_Acc : 21.200, Run Time : 18.66 sec
INFO:root:2024-04-18 03:57:29, Train, Epoch : 8, Step : 4470, Loss : 0.42367, Acc : 0.806, Sensitive_Loss : 0.10227, Sensitive_Acc : 22.900, Run Time : 17.96 sec
INFO:root:2024-04-18 03:57:47, Train, Epoch : 8, Step : 4480, Loss : 0.34852, Acc : 0.863, Sensitive_Loss : 0.11311, Sensitive_Acc : 23.500, Run Time : 17.60 sec
INFO:root:2024-04-18 03:58:04, Train, Epoch : 8, Step : 4490, Loss : 0.36132, Acc : 0.853, Sensitive_Loss : 0.11200, Sensitive_Acc : 22.100, Run Time : 16.60 sec
INFO:root:2024-04-18 03:58:22, Train, Epoch : 8, Step : 4500, Loss : 0.37326, Acc : 0.853, Sensitive_Loss : 0.10213, Sensitive_Acc : 21.400, Run Time : 18.23 sec
INFO:root:2024-04-18 04:02:14, Dev, Step : 4500, Loss : 0.52678, Acc : 0.765, Auc : 0.850, Sensitive_Loss : 0.17999, Sensitive_Acc : 21.662, Sensitive_Auc : 0.997, Mean auc: 0.850, Run Time : 232.28 sec
INFO:root:2024-04-18 04:02:27, Train, Epoch : 8, Step : 4510, Loss : 0.31382, Acc : 0.872, Sensitive_Loss : 0.14098, Sensitive_Acc : 20.800, Run Time : 245.43 sec
INFO:root:2024-04-18 04:02:45, Train, Epoch : 8, Step : 4520, Loss : 0.33284, Acc : 0.841, Sensitive_Loss : 0.13021, Sensitive_Acc : 16.700, Run Time : 17.44 sec
INFO:root:2024-04-18 04:03:02, Train, Epoch : 8, Step : 4530, Loss : 0.36427, Acc : 0.831, Sensitive_Loss : 0.14810, Sensitive_Acc : 22.000, Run Time : 17.33 sec
INFO:root:2024-04-18 04:03:21, Train, Epoch : 8, Step : 4540, Loss : 0.34927, Acc : 0.850, Sensitive_Loss : 0.16014, Sensitive_Acc : 22.300, Run Time : 19.22 sec
INFO:root:2024-04-18 04:03:37, Train, Epoch : 8, Step : 4550, Loss : 0.35081, Acc : 0.856, Sensitive_Loss : 0.12078, Sensitive_Acc : 24.600, Run Time : 15.92 sec
INFO:root:2024-04-18 04:03:55, Train, Epoch : 8, Step : 4560, Loss : 0.38378, Acc : 0.819, Sensitive_Loss : 0.12581, Sensitive_Acc : 19.400, Run Time : 17.80 sec
INFO:root:2024-04-18 04:04:13, Train, Epoch : 8, Step : 4570, Loss : 0.31319, Acc : 0.838, Sensitive_Loss : 0.13345, Sensitive_Acc : 20.200, Run Time : 18.39 sec
INFO:root:2024-04-18 04:04:30, Train, Epoch : 8, Step : 4580, Loss : 0.32329, Acc : 0.859, Sensitive_Loss : 0.09974, Sensitive_Acc : 20.000, Run Time : 16.31 sec
INFO:root:2024-04-18 04:04:47, Train, Epoch : 8, Step : 4590, Loss : 0.38009, Acc : 0.844, Sensitive_Loss : 0.12868, Sensitive_Acc : 20.600, Run Time : 17.03 sec
INFO:root:2024-04-18 04:05:05, Train, Epoch : 8, Step : 4600, Loss : 0.31121, Acc : 0.872, Sensitive_Loss : 0.12904, Sensitive_Acc : 22.000, Run Time : 18.05 sec
INFO:root:2024-04-18 04:08:58, Dev, Step : 4600, Loss : 0.56625, Acc : 0.767, Auc : 0.846, Sensitive_Loss : 0.19651, Sensitive_Acc : 21.436, Sensitive_Auc : 0.999, Mean auc: 0.846, Run Time : 233.00 sec
INFO:root:2024-04-18 04:09:11, Train, Epoch : 8, Step : 4610, Loss : 0.38252, Acc : 0.822, Sensitive_Loss : 0.15194, Sensitive_Acc : 16.300, Run Time : 246.34 sec
INFO:root:2024-04-18 04:09:30, Train, Epoch : 8, Step : 4620, Loss : 0.35820, Acc : 0.872, Sensitive_Loss : 0.09943, Sensitive_Acc : 25.700, Run Time : 18.38 sec
INFO:root:2024-04-18 04:09:47, Train, Epoch : 8, Step : 4630, Loss : 0.33897, Acc : 0.850, Sensitive_Loss : 0.13506, Sensitive_Acc : 25.800, Run Time : 17.71 sec
INFO:root:2024-04-18 04:10:03, Train, Epoch : 8, Step : 4640, Loss : 0.35337, Acc : 0.847, Sensitive_Loss : 0.11485, Sensitive_Acc : 26.000, Run Time : 15.93 sec
INFO:root:2024-04-18 04:10:23, Train, Epoch : 8, Step : 4650, Loss : 0.29808, Acc : 0.869, Sensitive_Loss : 0.08627, Sensitive_Acc : 23.100, Run Time : 19.38 sec
INFO:root:2024-04-18 04:10:39, Train, Epoch : 8, Step : 4660, Loss : 0.36491, Acc : 0.853, Sensitive_Loss : 0.10546, Sensitive_Acc : 21.300, Run Time : 16.58 sec
INFO:root:2024-04-18 04:10:58, Train, Epoch : 8, Step : 4670, Loss : 0.35162, Acc : 0.853, Sensitive_Loss : 0.08920, Sensitive_Acc : 23.600, Run Time : 18.66 sec
INFO:root:2024-04-18 04:11:15, Train, Epoch : 8, Step : 4680, Loss : 0.40564, Acc : 0.831, Sensitive_Loss : 0.10179, Sensitive_Acc : 21.800, Run Time : 17.01 sec
INFO:root:2024-04-18 04:11:32, Train, Epoch : 8, Step : 4690, Loss : 0.39332, Acc : 0.838, Sensitive_Loss : 0.07853, Sensitive_Acc : 18.600, Run Time : 17.60 sec
INFO:root:2024-04-18 04:11:48, Train, Epoch : 8, Step : 4700, Loss : 0.38218, Acc : 0.828, Sensitive_Loss : 0.12480, Sensitive_Acc : 18.200, Run Time : 15.29 sec
INFO:root:2024-04-18 04:15:41, Dev, Step : 4700, Loss : 0.53139, Acc : 0.774, Auc : 0.848, Sensitive_Loss : 0.19211, Sensitive_Acc : 21.436, Sensitive_Auc : 0.998, Mean auc: 0.848, Run Time : 232.94 sec
INFO:root:2024-04-18 04:15:54, Train, Epoch : 8, Step : 4710, Loss : 0.36576, Acc : 0.872, Sensitive_Loss : 0.11056, Sensitive_Acc : 23.400, Run Time : 246.07 sec
INFO:root:2024-04-18 04:16:12, Train, Epoch : 8, Step : 4720, Loss : 0.39490, Acc : 0.834, Sensitive_Loss : 0.12060, Sensitive_Acc : 18.300, Run Time : 18.43 sec
INFO:root:2024-04-18 04:16:29, Train, Epoch : 8, Step : 4730, Loss : 0.39781, Acc : 0.828, Sensitive_Loss : 0.08284, Sensitive_Acc : 20.900, Run Time : 16.59 sec
INFO:root:2024-04-18 04:16:46, Train, Epoch : 8, Step : 4740, Loss : 0.39812, Acc : 0.866, Sensitive_Loss : 0.11981, Sensitive_Acc : 24.900, Run Time : 17.27 sec
INFO:root:2024-04-18 04:17:04, Train, Epoch : 8, Step : 4750, Loss : 0.33188, Acc : 0.828, Sensitive_Loss : 0.17103, Sensitive_Acc : 22.900, Run Time : 18.30 sec
INFO:root:2024-04-18 04:17:21, Train, Epoch : 8, Step : 4760, Loss : 0.34276, Acc : 0.847, Sensitive_Loss : 0.19663, Sensitive_Acc : 23.800, Run Time : 16.64 sec
INFO:root:2024-04-18 04:17:40, Train, Epoch : 8, Step : 4770, Loss : 0.35567, Acc : 0.831, Sensitive_Loss : 0.09977, Sensitive_Acc : 21.800, Run Time : 18.62 sec
INFO:root:2024-04-18 04:17:57, Train, Epoch : 8, Step : 4780, Loss : 0.36842, Acc : 0.853, Sensitive_Loss : 0.10520, Sensitive_Acc : 22.700, Run Time : 17.80 sec
INFO:root:2024-04-18 04:18:14, Train, Epoch : 8, Step : 4790, Loss : 0.44880, Acc : 0.812, Sensitive_Loss : 0.11712, Sensitive_Acc : 22.400, Run Time : 17.04 sec
INFO:root:2024-04-18 04:18:32, Train, Epoch : 8, Step : 4800, Loss : 0.39472, Acc : 0.863, Sensitive_Loss : 0.10380, Sensitive_Acc : 22.300, Run Time : 17.99 sec
INFO:root:2024-04-18 04:22:26, Dev, Step : 4800, Loss : 0.53217, Acc : 0.769, Auc : 0.847, Sensitive_Loss : 0.18358, Sensitive_Acc : 21.662, Sensitive_Auc : 0.998, Mean auc: 0.847, Run Time : 233.16 sec
INFO:root:2024-04-18 04:22:39, Train, Epoch : 8, Step : 4810, Loss : 0.33790, Acc : 0.844, Sensitive_Loss : 0.10683, Sensitive_Acc : 21.100, Run Time : 246.31 sec
INFO:root:2024-04-18 04:22:58, Train, Epoch : 8, Step : 4820, Loss : 0.32749, Acc : 0.866, Sensitive_Loss : 0.15470, Sensitive_Acc : 13.100, Run Time : 19.08 sec
INFO:root:2024-04-18 04:23:14, Train, Epoch : 8, Step : 4830, Loss : 0.33918, Acc : 0.844, Sensitive_Loss : 0.11567, Sensitive_Acc : 22.700, Run Time : 16.43 sec
INFO:root:2024-04-18 04:23:31, Train, Epoch : 8, Step : 4840, Loss : 0.37724, Acc : 0.850, Sensitive_Loss : 0.08233, Sensitive_Acc : 18.400, Run Time : 16.71 sec
INFO:root:2024-04-18 04:23:49, Train, Epoch : 8, Step : 4850, Loss : 0.28656, Acc : 0.872, Sensitive_Loss : 0.08736, Sensitive_Acc : 18.200, Run Time : 18.40 sec
INFO:root:2024-04-18 04:24:07, Train, Epoch : 8, Step : 4860, Loss : 0.33397, Acc : 0.816, Sensitive_Loss : 0.11963, Sensitive_Acc : 21.300, Run Time : 17.71 sec
INFO:root:2024-04-18 04:24:27, Train, Epoch : 8, Step : 4870, Loss : 0.31386, Acc : 0.869, Sensitive_Loss : 0.09050, Sensitive_Acc : 19.300, Run Time : 20.21 sec
INFO:root:2024-04-18 04:24:50, Train, Epoch : 8, Step : 4880, Loss : 0.33896, Acc : 0.856, Sensitive_Loss : 0.08465, Sensitive_Acc : 19.900, Run Time : 22.23 sec
INFO:root:2024-04-18 04:25:11, Train, Epoch : 8, Step : 4890, Loss : 0.31199, Acc : 0.856, Sensitive_Loss : 0.10081, Sensitive_Acc : 23.500, Run Time : 21.14 sec
INFO:root:2024-04-18 04:25:31, Train, Epoch : 8, Step : 4900, Loss : 0.41591, Acc : 0.803, Sensitive_Loss : 0.16072, Sensitive_Acc : 21.800, Run Time : 19.82 sec
INFO:root:2024-04-18 04:29:34, Dev, Step : 4900, Loss : 0.55613, Acc : 0.771, Auc : 0.850, Sensitive_Loss : 0.17630, Sensitive_Acc : 21.571, Sensitive_Auc : 0.998, Mean auc: 0.850, Run Time : 243.51 sec
INFO:root:2024-04-18 04:29:46, Train, Epoch : 8, Step : 4910, Loss : 0.39648, Acc : 0.856, Sensitive_Loss : 0.25205, Sensitive_Acc : 22.700, Run Time : 255.63 sec
INFO:root:2024-04-18 04:30:05, Train, Epoch : 8, Step : 4920, Loss : 0.36282, Acc : 0.850, Sensitive_Loss : 0.11821, Sensitive_Acc : 16.500, Run Time : 18.92 sec
INFO:root:2024-04-18 04:30:23, Train, Epoch : 8, Step : 4930, Loss : 0.37193, Acc : 0.844, Sensitive_Loss : 0.11963, Sensitive_Acc : 25.800, Run Time : 17.80 sec
INFO:root:2024-04-18 04:30:40, Train, Epoch : 8, Step : 4940, Loss : 0.40136, Acc : 0.816, Sensitive_Loss : 0.08179, Sensitive_Acc : 22.800, Run Time : 16.79 sec
INFO:root:2024-04-18 04:30:58, Train, Epoch : 8, Step : 4950, Loss : 0.40095, Acc : 0.822, Sensitive_Loss : 0.13196, Sensitive_Acc : 23.000, Run Time : 17.85 sec
INFO:root:2024-04-18 04:31:14, Train, Epoch : 8, Step : 4960, Loss : 0.36136, Acc : 0.841, Sensitive_Loss : 0.13814, Sensitive_Acc : 16.300, Run Time : 16.84 sec
INFO:root:2024-04-18 04:31:32, Train, Epoch : 8, Step : 4970, Loss : 0.32655, Acc : 0.816, Sensitive_Loss : 0.11096, Sensitive_Acc : 20.300, Run Time : 17.54 sec
INFO:root:2024-04-18 04:31:50, Train, Epoch : 8, Step : 4980, Loss : 0.27776, Acc : 0.875, Sensitive_Loss : 0.11925, Sensitive_Acc : 21.700, Run Time : 18.20 sec
INFO:root:2024-04-18 04:32:08, Train, Epoch : 8, Step : 4990, Loss : 0.42399, Acc : 0.841, Sensitive_Loss : 0.12868, Sensitive_Acc : 15.800, Run Time : 18.19 sec
INFO:root:2024-04-18 04:32:26, Train, Epoch : 8, Step : 5000, Loss : 0.38399, Acc : 0.825, Sensitive_Loss : 0.12745, Sensitive_Acc : 23.400, Run Time : 17.31 sec
INFO:root:2024-04-18 04:36:18, Dev, Step : 5000, Loss : 0.53764, Acc : 0.776, Auc : 0.850, Sensitive_Loss : 0.18934, Sensitive_Acc : 21.346, Sensitive_Auc : 0.997, Mean auc: 0.850, Run Time : 232.22 sec
INFO:root:2024-04-18 04:36:31, Train, Epoch : 8, Step : 5010, Loss : 0.33979, Acc : 0.856, Sensitive_Loss : 0.12716, Sensitive_Acc : 20.500, Run Time : 245.61 sec
INFO:root:2024-04-18 04:36:47, Train, Epoch : 8, Step : 5020, Loss : 0.30489, Acc : 0.863, Sensitive_Loss : 0.09395, Sensitive_Acc : 19.400, Run Time : 16.21 sec
INFO:root:2024-04-18 04:37:05, Train, Epoch : 8, Step : 5030, Loss : 0.38467, Acc : 0.831, Sensitive_Loss : 0.10648, Sensitive_Acc : 22.600, Run Time : 17.47 sec
INFO:root:2024-04-18 04:37:23, Train, Epoch : 8, Step : 5040, Loss : 0.37372, Acc : 0.809, Sensitive_Loss : 0.09289, Sensitive_Acc : 22.100, Run Time : 18.07 sec
INFO:root:2024-04-18 04:37:41, Train, Epoch : 8, Step : 5050, Loss : 0.33611, Acc : 0.866, Sensitive_Loss : 0.08611, Sensitive_Acc : 24.500, Run Time : 17.96 sec
INFO:root:2024-04-18 04:37:58, Train, Epoch : 8, Step : 5060, Loss : 0.26718, Acc : 0.884, Sensitive_Loss : 0.10992, Sensitive_Acc : 22.300, Run Time : 16.73 sec
INFO:root:2024-04-18 04:38:15, Train, Epoch : 8, Step : 5070, Loss : 0.34016, Acc : 0.856, Sensitive_Loss : 0.07952, Sensitive_Acc : 23.800, Run Time : 17.83 sec
INFO:root:2024-04-18 04:42:06
INFO:root:y_pred: [0.09503105 0.002844   0.1484837  ... 0.4752954  0.03298626 0.00152661]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [4.47102248e-05 1.26299192e-05 1.92436550e-04 2.90270932e-02
 9.06365961e-02 2.44930765e-04 6.57065539e-05 5.84275287e-04
 7.74730276e-03 9.97486472e-01 5.83088517e-01 2.35044809e-06
 2.19339872e-05 9.41177015e-04 9.97470498e-01 1.12706318e-01
 3.89190129e-04 9.94943678e-01 9.93180871e-01 4.35789407e-04
 7.68634677e-01 7.46022306e-06 2.79128253e-02 1.55056841e-05
 1.21105739e-04 3.69487405e-01 7.10207269e-06 1.70934107e-03
 2.22877929e-06 1.12239616e-02 8.09559060e-05 9.68801022e-01
 1.18643912e-02 9.51434970e-01 1.07404696e-06 1.15322081e-08
 1.20333803e-04 9.60390791e-02 6.97255507e-02 4.48841206e-07
 1.42699433e-02 9.20002460e-01 4.56816889e-03 7.76684104e-08
 9.83911693e-01 2.58075446e-01 1.75251085e-02 1.13103271e-01
 1.95991829e-01 9.67790842e-01 9.87638235e-01 9.97978985e-01
 9.88581598e-01 2.62074573e-05 8.98583978e-02 6.68888986e-01
 5.33904997e-04 3.93133541e-06 9.90112245e-01 2.32408919e-08
 4.60634328e-06 7.92328443e-04 4.76737358e-02 2.11078621e-08
 9.92162883e-01 2.25474581e-01 1.15221526e-04 2.82321990e-01
 5.84850740e-03 9.70645308e-01 9.98742044e-01 9.95321929e-01
 8.97786049e-06 5.16281426e-01 4.63279197e-04 1.43246025e-01
 7.38144219e-02 2.21738063e-07 1.38954715e-06 4.73011052e-03
 2.00370457e-02 2.05307355e-04 9.89463151e-01 9.58421350e-01
 3.93654220e-04 1.95773598e-03 2.13520646e-01 1.19718560e-03
 1.74606502e-01 1.10322935e-05 8.22654814e-02 2.90483877e-04
 8.77590428e-05 1.34074517e-05 1.16778983e-06 1.80442724e-02
 7.07568006e-06 9.50904310e-01 1.26799408e-04 1.25160994e-04
 6.65626070e-03 2.67119904e-04 8.61996710e-02 1.02668419e-05
 1.71377133e-06 9.37782332e-08 1.00819759e-01 8.79000783e-01
 1.71913981e-01 1.10468879e-01 9.17942743e-05 9.96749640e-01
 9.94312882e-01 7.26916696e-05 7.93258905e-01 5.23932604e-03
 1.97530165e-02 7.61947874e-08 1.59280002e-02 5.37596534e-05
 6.59121026e-04 3.26434529e-04 2.10662116e-03 1.72738214e-06
 4.58919210e-04 9.65367019e-01 2.96109602e-06 9.90789235e-01
 9.60588004e-05 3.84519279e-01 8.32185324e-05 1.40451984e-02
 5.96789249e-08]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 04:42:06, Dev, Step : 5072, Loss : 0.54038, Acc : 0.775, Auc : 0.851, Sensitive_Loss : 0.20153, Sensitive_Acc : 21.346, Sensitive_Auc : 0.997, Mean auc: 0.851, Run Time : 228.30 sec
INFO:root:2024-04-18 04:42:23, Train, Epoch : 9, Step : 5080, Loss : 0.24707, Acc : 0.700, Sensitive_Loss : 0.05480, Sensitive_Acc : 19.300, Run Time : 16.34 sec
INFO:root:2024-04-18 04:42:43, Train, Epoch : 9, Step : 5090, Loss : 0.30019, Acc : 0.856, Sensitive_Loss : 0.09056, Sensitive_Acc : 21.700, Run Time : 19.66 sec
INFO:root:2024-04-18 04:43:00, Train, Epoch : 9, Step : 5100, Loss : 0.28474, Acc : 0.878, Sensitive_Loss : 0.11654, Sensitive_Acc : 21.200, Run Time : 16.52 sec
INFO:root:2024-04-18 04:46:52, Dev, Step : 5100, Loss : 0.54691, Acc : 0.774, Auc : 0.851, Sensitive_Loss : 0.17780, Sensitive_Acc : 21.662, Sensitive_Auc : 0.998, Mean auc: 0.851, Run Time : 232.80 sec
INFO:root:2024-04-18 04:47:05, Train, Epoch : 9, Step : 5110, Loss : 0.36121, Acc : 0.828, Sensitive_Loss : 0.16488, Sensitive_Acc : 26.600, Run Time : 245.60 sec
INFO:root:2024-04-18 04:47:23, Train, Epoch : 9, Step : 5120, Loss : 0.32616, Acc : 0.847, Sensitive_Loss : 0.13020, Sensitive_Acc : 23.300, Run Time : 17.87 sec
INFO:root:2024-04-18 04:47:41, Train, Epoch : 9, Step : 5130, Loss : 0.31539, Acc : 0.850, Sensitive_Loss : 0.11422, Sensitive_Acc : 22.000, Run Time : 18.08 sec
INFO:root:2024-04-18 04:47:59, Train, Epoch : 9, Step : 5140, Loss : 0.33035, Acc : 0.828, Sensitive_Loss : 0.09294, Sensitive_Acc : 18.900, Run Time : 17.62 sec
INFO:root:2024-04-18 04:48:18, Train, Epoch : 9, Step : 5150, Loss : 0.25938, Acc : 0.900, Sensitive_Loss : 0.10896, Sensitive_Acc : 22.700, Run Time : 19.27 sec
INFO:root:2024-04-18 04:48:35, Train, Epoch : 9, Step : 5160, Loss : 0.31255, Acc : 0.869, Sensitive_Loss : 0.07667, Sensitive_Acc : 22.800, Run Time : 16.57 sec
INFO:root:2024-04-18 04:48:50, Train, Epoch : 9, Step : 5170, Loss : 0.30822, Acc : 0.838, Sensitive_Loss : 0.13613, Sensitive_Acc : 25.700, Run Time : 15.78 sec
INFO:root:2024-04-18 04:49:10, Train, Epoch : 9, Step : 5180, Loss : 0.30577, Acc : 0.850, Sensitive_Loss : 0.14433, Sensitive_Acc : 19.300, Run Time : 19.34 sec
INFO:root:2024-04-18 04:49:29, Train, Epoch : 9, Step : 5190, Loss : 0.35768, Acc : 0.844, Sensitive_Loss : 0.10485, Sensitive_Acc : 18.600, Run Time : 18.83 sec
INFO:root:2024-04-18 04:49:46, Train, Epoch : 9, Step : 5200, Loss : 0.36937, Acc : 0.866, Sensitive_Loss : 0.07912, Sensitive_Acc : 22.900, Run Time : 17.44 sec
INFO:root:2024-04-18 04:53:37, Dev, Step : 5200, Loss : 0.56601, Acc : 0.765, Auc : 0.845, Sensitive_Loss : 0.18692, Sensitive_Acc : 21.346, Sensitive_Auc : 0.998, Mean auc: 0.845, Run Time : 231.12 sec
INFO:root:2024-04-18 04:53:51, Train, Epoch : 9, Step : 5210, Loss : 0.32840, Acc : 0.841, Sensitive_Loss : 0.08213, Sensitive_Acc : 21.000, Run Time : 244.59 sec
INFO:root:2024-04-18 04:54:07, Train, Epoch : 9, Step : 5220, Loss : 0.27239, Acc : 0.897, Sensitive_Loss : 0.07057, Sensitive_Acc : 22.500, Run Time : 16.38 sec
INFO:root:2024-04-18 04:54:25, Train, Epoch : 9, Step : 5230, Loss : 0.32856, Acc : 0.856, Sensitive_Loss : 0.10739, Sensitive_Acc : 18.900, Run Time : 17.77 sec
INFO:root:2024-04-18 04:54:43, Train, Epoch : 9, Step : 5240, Loss : 0.30327, Acc : 0.863, Sensitive_Loss : 0.09770, Sensitive_Acc : 23.300, Run Time : 18.71 sec
INFO:root:2024-04-18 04:55:02, Train, Epoch : 9, Step : 5250, Loss : 0.33622, Acc : 0.881, Sensitive_Loss : 0.10255, Sensitive_Acc : 21.800, Run Time : 18.75 sec
INFO:root:2024-04-18 04:55:20, Train, Epoch : 9, Step : 5260, Loss : 0.30061, Acc : 0.881, Sensitive_Loss : 0.11936, Sensitive_Acc : 15.700, Run Time : 18.06 sec
INFO:root:2024-04-18 04:55:38, Train, Epoch : 9, Step : 5270, Loss : 0.45683, Acc : 0.800, Sensitive_Loss : 0.22059, Sensitive_Acc : 18.700, Run Time : 17.89 sec
INFO:root:2024-04-18 04:55:56, Train, Epoch : 9, Step : 5280, Loss : 0.30108, Acc : 0.875, Sensitive_Loss : 0.10491, Sensitive_Acc : 23.000, Run Time : 17.56 sec
INFO:root:2024-04-18 04:56:12, Train, Epoch : 9, Step : 5290, Loss : 0.37110, Acc : 0.838, Sensitive_Loss : 0.12479, Sensitive_Acc : 24.900, Run Time : 16.48 sec
INFO:root:2024-04-18 04:56:29, Train, Epoch : 9, Step : 5300, Loss : 0.31889, Acc : 0.847, Sensitive_Loss : 0.04829, Sensitive_Acc : 21.700, Run Time : 17.07 sec
INFO:root:2024-04-18 05:00:21, Dev, Step : 5300, Loss : 0.54441, Acc : 0.773, Auc : 0.850, Sensitive_Loss : 0.19008, Sensitive_Acc : 21.256, Sensitive_Auc : 0.999, Mean auc: 0.850, Run Time : 232.18 sec
INFO:root:2024-04-18 05:00:34, Train, Epoch : 9, Step : 5310, Loss : 0.40164, Acc : 0.834, Sensitive_Loss : 0.11378, Sensitive_Acc : 19.700, Run Time : 244.48 sec
INFO:root:2024-04-18 05:00:51, Train, Epoch : 9, Step : 5320, Loss : 0.30568, Acc : 0.866, Sensitive_Loss : 0.09408, Sensitive_Acc : 24.700, Run Time : 17.72 sec
INFO:root:2024-04-18 05:01:08, Train, Epoch : 9, Step : 5330, Loss : 0.39422, Acc : 0.838, Sensitive_Loss : 0.11810, Sensitive_Acc : 16.500, Run Time : 17.02 sec
INFO:root:2024-04-18 05:01:27, Train, Epoch : 9, Step : 5340, Loss : 0.41740, Acc : 0.822, Sensitive_Loss : 0.08839, Sensitive_Acc : 23.600, Run Time : 18.74 sec
INFO:root:2024-04-18 05:01:45, Train, Epoch : 9, Step : 5350, Loss : 0.29611, Acc : 0.872, Sensitive_Loss : 0.11922, Sensitive_Acc : 18.900, Run Time : 17.50 sec
INFO:root:2024-04-18 05:02:02, Train, Epoch : 9, Step : 5360, Loss : 0.30886, Acc : 0.878, Sensitive_Loss : 0.12959, Sensitive_Acc : 21.700, Run Time : 17.74 sec
INFO:root:2024-04-18 05:02:20, Train, Epoch : 9, Step : 5370, Loss : 0.38149, Acc : 0.838, Sensitive_Loss : 0.10383, Sensitive_Acc : 22.200, Run Time : 17.85 sec
INFO:root:2024-04-18 05:02:39, Train, Epoch : 9, Step : 5380, Loss : 0.29565, Acc : 0.866, Sensitive_Loss : 0.07727, Sensitive_Acc : 23.500, Run Time : 18.70 sec
INFO:root:2024-04-18 05:02:57, Train, Epoch : 9, Step : 5390, Loss : 0.38737, Acc : 0.838, Sensitive_Loss : 0.07774, Sensitive_Acc : 23.700, Run Time : 17.99 sec
INFO:root:2024-04-18 05:03:13, Train, Epoch : 9, Step : 5400, Loss : 0.32310, Acc : 0.869, Sensitive_Loss : 0.07244, Sensitive_Acc : 16.700, Run Time : 16.41 sec
INFO:root:2024-04-18 05:07:05, Dev, Step : 5400, Loss : 0.57919, Acc : 0.764, Auc : 0.843, Sensitive_Loss : 0.19905, Sensitive_Acc : 21.662, Sensitive_Auc : 0.999, Mean auc: 0.843, Run Time : 231.65 sec
INFO:root:2024-04-18 05:07:19, Train, Epoch : 9, Step : 5410, Loss : 0.38538, Acc : 0.825, Sensitive_Loss : 0.06914, Sensitive_Acc : 19.300, Run Time : 245.47 sec
INFO:root:2024-04-18 05:07:35, Train, Epoch : 9, Step : 5420, Loss : 0.37111, Acc : 0.856, Sensitive_Loss : 0.17039, Sensitive_Acc : 21.900, Run Time : 16.58 sec
INFO:root:2024-04-18 05:07:53, Train, Epoch : 9, Step : 5430, Loss : 0.36653, Acc : 0.812, Sensitive_Loss : 0.10932, Sensitive_Acc : 21.900, Run Time : 17.02 sec
INFO:root:2024-04-18 05:08:12, Train, Epoch : 9, Step : 5440, Loss : 0.37856, Acc : 0.850, Sensitive_Loss : 0.10508, Sensitive_Acc : 18.600, Run Time : 19.81 sec
INFO:root:2024-04-18 05:08:30, Train, Epoch : 9, Step : 5450, Loss : 0.29830, Acc : 0.881, Sensitive_Loss : 0.11802, Sensitive_Acc : 27.200, Run Time : 17.20 sec
INFO:root:2024-04-18 05:08:45, Train, Epoch : 9, Step : 5460, Loss : 0.31460, Acc : 0.887, Sensitive_Loss : 0.10904, Sensitive_Acc : 25.000, Run Time : 15.98 sec
INFO:root:2024-04-18 05:09:02, Train, Epoch : 9, Step : 5470, Loss : 0.34891, Acc : 0.850, Sensitive_Loss : 0.09286, Sensitive_Acc : 16.700, Run Time : 16.95 sec
INFO:root:2024-04-18 05:09:22, Train, Epoch : 9, Step : 5480, Loss : 0.38707, Acc : 0.844, Sensitive_Loss : 0.12637, Sensitive_Acc : 21.900, Run Time : 19.14 sec
INFO:root:2024-04-18 05:09:39, Train, Epoch : 9, Step : 5490, Loss : 0.34168, Acc : 0.859, Sensitive_Loss : 0.08743, Sensitive_Acc : 19.500, Run Time : 17.60 sec
INFO:root:2024-04-18 05:09:55, Train, Epoch : 9, Step : 5500, Loss : 0.32786, Acc : 0.881, Sensitive_Loss : 0.09767, Sensitive_Acc : 21.100, Run Time : 15.73 sec
INFO:root:2024-04-18 05:13:48, Dev, Step : 5500, Loss : 0.55585, Acc : 0.771, Auc : 0.846, Sensitive_Loss : 0.21635, Sensitive_Acc : 20.910, Sensitive_Auc : 0.998, Mean auc: 0.846, Run Time : 232.98 sec
INFO:root:2024-04-18 05:14:01, Train, Epoch : 9, Step : 5510, Loss : 0.38496, Acc : 0.825, Sensitive_Loss : 0.13343, Sensitive_Acc : 24.400, Run Time : 245.88 sec
INFO:root:2024-04-18 05:14:19, Train, Epoch : 9, Step : 5520, Loss : 0.31826, Acc : 0.884, Sensitive_Loss : 0.08603, Sensitive_Acc : 18.700, Run Time : 18.24 sec
INFO:root:2024-04-18 05:14:37, Train, Epoch : 9, Step : 5530, Loss : 0.27617, Acc : 0.875, Sensitive_Loss : 0.06398, Sensitive_Acc : 18.000, Run Time : 17.52 sec
INFO:root:2024-04-18 05:14:53, Train, Epoch : 9, Step : 5540, Loss : 0.35143, Acc : 0.844, Sensitive_Loss : 0.11391, Sensitive_Acc : 23.100, Run Time : 16.75 sec
INFO:root:2024-04-18 05:15:10, Train, Epoch : 9, Step : 5550, Loss : 0.35473, Acc : 0.828, Sensitive_Loss : 0.10559, Sensitive_Acc : 22.100, Run Time : 17.00 sec
INFO:root:2024-04-18 05:15:29, Train, Epoch : 9, Step : 5560, Loss : 0.36530, Acc : 0.831, Sensitive_Loss : 0.15554, Sensitive_Acc : 15.000, Run Time : 18.85 sec
INFO:root:2024-04-18 05:15:47, Train, Epoch : 9, Step : 5570, Loss : 0.31310, Acc : 0.872, Sensitive_Loss : 0.12649, Sensitive_Acc : 25.300, Run Time : 17.70 sec
INFO:root:2024-04-18 05:16:04, Train, Epoch : 9, Step : 5580, Loss : 0.38831, Acc : 0.847, Sensitive_Loss : 0.10680, Sensitive_Acc : 24.300, Run Time : 17.08 sec
INFO:root:2024-04-18 05:16:22, Train, Epoch : 9, Step : 5590, Loss : 0.35863, Acc : 0.859, Sensitive_Loss : 0.12828, Sensitive_Acc : 20.400, Run Time : 17.97 sec
INFO:root:2024-04-18 05:16:39, Train, Epoch : 9, Step : 5600, Loss : 0.33829, Acc : 0.844, Sensitive_Loss : 0.09223, Sensitive_Acc : 25.500, Run Time : 16.88 sec
INFO:root:2024-04-18 05:20:33, Dev, Step : 5600, Loss : 0.55334, Acc : 0.771, Auc : 0.845, Sensitive_Loss : 0.20212, Sensitive_Acc : 21.571, Sensitive_Auc : 1.000, Mean auc: 0.845, Run Time : 234.30 sec
INFO:root:2024-04-18 05:20:46, Train, Epoch : 9, Step : 5610, Loss : 0.27213, Acc : 0.866, Sensitive_Loss : 0.16745, Sensitive_Acc : 23.600, Run Time : 247.24 sec
INFO:root:2024-04-18 05:21:03, Train, Epoch : 9, Step : 5620, Loss : 0.30479, Acc : 0.894, Sensitive_Loss : 0.09118, Sensitive_Acc : 22.000, Run Time : 17.34 sec
INFO:root:2024-04-18 05:21:22, Train, Epoch : 9, Step : 5630, Loss : 0.34376, Acc : 0.831, Sensitive_Loss : 0.11630, Sensitive_Acc : 21.900, Run Time : 18.36 sec
INFO:root:2024-04-18 05:21:39, Train, Epoch : 9, Step : 5640, Loss : 0.33915, Acc : 0.847, Sensitive_Loss : 0.12036, Sensitive_Acc : 23.000, Run Time : 17.36 sec
INFO:root:2024-04-18 05:21:57, Train, Epoch : 9, Step : 5650, Loss : 0.34244, Acc : 0.872, Sensitive_Loss : 0.10286, Sensitive_Acc : 23.400, Run Time : 18.36 sec
INFO:root:2024-04-18 05:22:13, Train, Epoch : 9, Step : 5660, Loss : 0.34011, Acc : 0.847, Sensitive_Loss : 0.09116, Sensitive_Acc : 20.200, Run Time : 15.71 sec
INFO:root:2024-04-18 05:22:31, Train, Epoch : 9, Step : 5670, Loss : 0.28922, Acc : 0.875, Sensitive_Loss : 0.08441, Sensitive_Acc : 18.100, Run Time : 17.44 sec
INFO:root:2024-04-18 05:22:50, Train, Epoch : 9, Step : 5680, Loss : 0.30225, Acc : 0.872, Sensitive_Loss : 0.11552, Sensitive_Acc : 16.700, Run Time : 19.85 sec
INFO:root:2024-04-18 05:23:06, Train, Epoch : 9, Step : 5690, Loss : 0.40266, Acc : 0.834, Sensitive_Loss : 0.07337, Sensitive_Acc : 23.800, Run Time : 15.72 sec
INFO:root:2024-04-18 05:23:24, Train, Epoch : 9, Step : 5700, Loss : 0.30262, Acc : 0.894, Sensitive_Loss : 0.09046, Sensitive_Acc : 20.200, Run Time : 17.73 sec
INFO:root:2024-04-18 05:27:16, Dev, Step : 5700, Loss : 0.56783, Acc : 0.769, Auc : 0.846, Sensitive_Loss : 0.17120, Sensitive_Acc : 21.571, Sensitive_Auc : 0.999, Mean auc: 0.846, Run Time : 232.07 sec
INFO:root:2024-04-18 05:31:10
INFO:root:y_pred: [5.1063512e-02 5.6389561e-03 4.9025930e-02 ... 4.9714372e-01 5.4689739e-02
 4.4343379e-04]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.46337425e-05 2.48693959e-06 1.41532207e-03 1.57063268e-02
 3.55702341e-02 1.95312139e-04 9.57631073e-06 2.59420514e-04
 1.10435011e-02 9.96693492e-01 1.58109307e-01 1.41324904e-06
 8.52190533e-06 3.30738985e-04 9.97035623e-01 4.11501117e-02
 1.45971935e-04 9.95148003e-01 9.93608952e-01 3.15212645e-04
 8.45778763e-01 9.52074913e-07 1.42483437e-03 6.60112028e-06
 5.92274591e-05 2.58860677e-01 4.12507097e-06 5.15387976e-04
 1.21202879e-06 2.12273560e-03 1.45215052e-03 9.65347826e-01
 8.41439585e-04 9.69038069e-01 4.21682635e-08 7.56191110e-09
 4.06008825e-04 4.01697904e-02 4.73033749e-02 1.05359902e-07
 3.10398755e-03 9.40123260e-01 2.55237613e-03 6.75483403e-09
 9.88456368e-01 3.88269946e-02 3.71101946e-02 2.15646513e-02
 1.45806417e-01 9.66329515e-01 9.87068653e-01 9.97445822e-01
 9.88678873e-01 3.51156559e-05 2.47227568e-02 6.02723479e-01
 3.29390750e-05 7.44725355e-07 9.92408991e-01 5.44131362e-09
 2.44639705e-06 6.90842062e-05 1.08670387e-02 1.15047243e-08
 9.92115319e-01 1.55182481e-01 6.56960328e-05 9.25393924e-02
 5.75057883e-03 9.73093927e-01 9.98762012e-01 9.92653966e-01
 3.99419008e-07 2.27889657e-01 8.34050006e-05 1.33145735e-01
 5.05744033e-02 1.86290322e-07 4.63465904e-06 4.77691507e-03
 8.57800059e-03 1.16967065e-04 9.84714091e-01 9.69875932e-01
 1.83966025e-04 8.05893971e-04 2.60652632e-01 9.76339739e-04
 1.51972905e-01 6.33513935e-07 2.07754318e-02 1.46152906e-05
 2.86345494e-05 5.24469124e-06 4.28155181e-06 3.52769694e-03
 5.92303013e-06 9.47457612e-01 5.66184144e-06 5.25638789e-05
 1.58739109e-02 4.08159940e-05 4.63982783e-02 1.22616138e-05
 6.23355106e-07 2.12436113e-08 2.80519146e-02 8.47994387e-01
 1.28855765e-01 1.46325305e-01 5.00716633e-05 9.95886862e-01
 9.94576812e-01 2.30077167e-05 8.09542000e-01 2.16448051e-03
 2.63169408e-02 2.60797979e-08 2.25386862e-02 3.40710510e-04
 9.55666110e-05 2.40345238e-04 8.72337667e-04 1.90679123e-06
 2.62031572e-05 8.91020060e-01 5.67314771e-07 9.87769067e-01
 1.63104341e-05 1.96328908e-01 4.00095159e-05 3.96503396e-02
 9.51264401e-09]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 05:31:10, Dev, Step : 5706, Loss : 0.57606, Acc : 0.767, Auc : 0.845, Sensitive_Loss : 0.17231, Sensitive_Acc : 21.571, Sensitive_Auc : 0.999, Mean auc: 0.845, Run Time : 229.29 sec
INFO:root:2024-04-18 05:31:20, Train, Epoch : 10, Step : 5710, Loss : 0.12278, Acc : 0.344, Sensitive_Loss : 0.02931, Sensitive_Acc : 8.500, Run Time : 8.53 sec
INFO:root:2024-04-18 05:31:38, Train, Epoch : 10, Step : 5720, Loss : 0.37374, Acc : 0.859, Sensitive_Loss : 0.10722, Sensitive_Acc : 22.200, Run Time : 18.22 sec
INFO:root:2024-04-18 05:31:58, Train, Epoch : 10, Step : 5730, Loss : 0.40561, Acc : 0.853, Sensitive_Loss : 0.08434, Sensitive_Acc : 20.600, Run Time : 19.88 sec
INFO:root:2024-04-18 05:32:15, Train, Epoch : 10, Step : 5740, Loss : 0.29509, Acc : 0.866, Sensitive_Loss : 0.08062, Sensitive_Acc : 17.300, Run Time : 16.78 sec
INFO:root:2024-04-18 05:32:33, Train, Epoch : 10, Step : 5750, Loss : 0.32341, Acc : 0.891, Sensitive_Loss : 0.08897, Sensitive_Acc : 20.900, Run Time : 18.65 sec
INFO:root:2024-04-18 05:32:50, Train, Epoch : 10, Step : 5760, Loss : 0.29488, Acc : 0.875, Sensitive_Loss : 0.07342, Sensitive_Acc : 21.400, Run Time : 16.64 sec
INFO:root:2024-04-18 05:33:08, Train, Epoch : 10, Step : 5770, Loss : 0.28486, Acc : 0.891, Sensitive_Loss : 0.12499, Sensitive_Acc : 21.800, Run Time : 17.70 sec
INFO:root:2024-04-18 05:33:26, Train, Epoch : 10, Step : 5780, Loss : 0.36285, Acc : 0.828, Sensitive_Loss : 0.09251, Sensitive_Acc : 23.300, Run Time : 18.17 sec
INFO:root:2024-04-18 05:33:44, Train, Epoch : 10, Step : 5790, Loss : 0.33374, Acc : 0.878, Sensitive_Loss : 0.07989, Sensitive_Acc : 19.400, Run Time : 17.71 sec
INFO:root:2024-04-18 05:34:02, Train, Epoch : 10, Step : 5800, Loss : 0.27795, Acc : 0.903, Sensitive_Loss : 0.10865, Sensitive_Acc : 23.000, Run Time : 18.87 sec
INFO:root:2024-04-18 05:37:54, Dev, Step : 5800, Loss : 0.54991, Acc : 0.761, Auc : 0.843, Sensitive_Loss : 0.18835, Sensitive_Acc : 21.571, Sensitive_Auc : 0.999, Mean auc: 0.843, Run Time : 231.24 sec
INFO:root:2024-04-18 05:38:06, Train, Epoch : 10, Step : 5810, Loss : 0.34803, Acc : 0.819, Sensitive_Loss : 0.07176, Sensitive_Acc : 18.800, Run Time : 243.90 sec
INFO:root:2024-04-18 05:38:24, Train, Epoch : 10, Step : 5820, Loss : 0.29755, Acc : 0.881, Sensitive_Loss : 0.10487, Sensitive_Acc : 25.900, Run Time : 17.62 sec
INFO:root:2024-04-18 05:38:42, Train, Epoch : 10, Step : 5830, Loss : 0.30715, Acc : 0.838, Sensitive_Loss : 0.12292, Sensitive_Acc : 21.700, Run Time : 17.87 sec
INFO:root:2024-04-18 05:39:00, Train, Epoch : 10, Step : 5840, Loss : 0.34134, Acc : 0.853, Sensitive_Loss : 0.13992, Sensitive_Acc : 22.200, Run Time : 18.31 sec
INFO:root:2024-04-18 05:39:17, Train, Epoch : 10, Step : 5850, Loss : 0.30517, Acc : 0.869, Sensitive_Loss : 0.13696, Sensitive_Acc : 21.600, Run Time : 17.20 sec
INFO:root:2024-04-18 05:39:36, Train, Epoch : 10, Step : 5860, Loss : 0.28958, Acc : 0.894, Sensitive_Loss : 0.06735, Sensitive_Acc : 22.400, Run Time : 18.40 sec
INFO:root:2024-04-18 05:39:52, Train, Epoch : 10, Step : 5870, Loss : 0.35608, Acc : 0.866, Sensitive_Loss : 0.10915, Sensitive_Acc : 24.500, Run Time : 16.50 sec
INFO:root:2024-04-18 05:40:10, Train, Epoch : 10, Step : 5880, Loss : 0.32762, Acc : 0.847, Sensitive_Loss : 0.09369, Sensitive_Acc : 26.700, Run Time : 17.57 sec
INFO:root:2024-04-18 05:40:27, Train, Epoch : 10, Step : 5890, Loss : 0.30232, Acc : 0.859, Sensitive_Loss : 0.09751, Sensitive_Acc : 21.000, Run Time : 16.65 sec
INFO:root:2024-04-18 05:40:44, Train, Epoch : 10, Step : 5900, Loss : 0.31523, Acc : 0.869, Sensitive_Loss : 0.08020, Sensitive_Acc : 22.000, Run Time : 17.35 sec
INFO:root:2024-04-18 05:46:03, Dev, Step : 5900, Loss : 0.55745, Acc : 0.770, Auc : 0.843, Sensitive_Loss : 0.17560, Sensitive_Acc : 21.662, Sensitive_Auc : 0.999, Mean auc: 0.843, Run Time : 319.36 sec
INFO:root:2024-04-18 05:46:15, Train, Epoch : 10, Step : 5910, Loss : 0.33960, Acc : 0.856, Sensitive_Loss : 0.06931, Sensitive_Acc : 20.800, Run Time : 331.16 sec
INFO:root:2024-04-18 05:46:32, Train, Epoch : 10, Step : 5920, Loss : 0.29864, Acc : 0.866, Sensitive_Loss : 0.07442, Sensitive_Acc : 23.500, Run Time : 16.76 sec
INFO:root:2024-04-18 05:46:50, Train, Epoch : 10, Step : 5930, Loss : 0.30155, Acc : 0.875, Sensitive_Loss : 0.09608, Sensitive_Acc : 21.600, Run Time : 18.57 sec
INFO:root:2024-04-18 05:47:07, Train, Epoch : 10, Step : 5940, Loss : 0.27868, Acc : 0.866, Sensitive_Loss : 0.08139, Sensitive_Acc : 23.000, Run Time : 16.62 sec
INFO:root:2024-04-18 05:47:24, Train, Epoch : 10, Step : 5950, Loss : 0.31094, Acc : 0.819, Sensitive_Loss : 0.09495, Sensitive_Acc : 22.700, Run Time : 17.23 sec
INFO:root:2024-04-18 05:47:42, Train, Epoch : 10, Step : 5960, Loss : 0.27544, Acc : 0.869, Sensitive_Loss : 0.06591, Sensitive_Acc : 19.600, Run Time : 17.98 sec
INFO:root:2024-04-18 05:48:00, Train, Epoch : 10, Step : 5970, Loss : 0.32298, Acc : 0.878, Sensitive_Loss : 0.11150, Sensitive_Acc : 23.100, Run Time : 17.77 sec
INFO:root:2024-04-18 05:48:18, Train, Epoch : 10, Step : 5980, Loss : 0.40932, Acc : 0.838, Sensitive_Loss : 0.12873, Sensitive_Acc : 22.700, Run Time : 17.84 sec
INFO:root:2024-04-18 05:48:35, Train, Epoch : 10, Step : 5990, Loss : 0.32423, Acc : 0.872, Sensitive_Loss : 0.08518, Sensitive_Acc : 21.900, Run Time : 17.57 sec
INFO:root:2024-04-18 05:48:53, Train, Epoch : 10, Step : 6000, Loss : 0.29320, Acc : 0.872, Sensitive_Loss : 0.06496, Sensitive_Acc : 22.600, Run Time : 17.13 sec
INFO:root:2024-04-18 05:52:45, Dev, Step : 6000, Loss : 0.56809, Acc : 0.762, Auc : 0.841, Sensitive_Loss : 0.17525, Sensitive_Acc : 21.662, Sensitive_Auc : 0.998, Mean auc: 0.841, Run Time : 232.58 sec
INFO:root:2024-04-18 05:52:58, Train, Epoch : 10, Step : 6010, Loss : 0.31924, Acc : 0.869, Sensitive_Loss : 0.13569, Sensitive_Acc : 20.300, Run Time : 245.66 sec
INFO:root:2024-04-18 05:53:17, Train, Epoch : 10, Step : 6020, Loss : 0.29385, Acc : 0.878, Sensitive_Loss : 0.16572, Sensitive_Acc : 21.200, Run Time : 18.93 sec
INFO:root:2024-04-18 05:53:33, Train, Epoch : 10, Step : 6030, Loss : 0.36215, Acc : 0.853, Sensitive_Loss : 0.09126, Sensitive_Acc : 21.000, Run Time : 16.05 sec
INFO:root:2024-04-18 05:53:51, Train, Epoch : 10, Step : 6040, Loss : 0.34637, Acc : 0.831, Sensitive_Loss : 0.10857, Sensitive_Acc : 21.500, Run Time : 17.89 sec
INFO:root:2024-04-18 05:54:09, Train, Epoch : 10, Step : 6050, Loss : 0.31476, Acc : 0.863, Sensitive_Loss : 0.11272, Sensitive_Acc : 22.700, Run Time : 17.44 sec
INFO:root:2024-04-18 05:54:26, Train, Epoch : 10, Step : 6060, Loss : 0.25939, Acc : 0.884, Sensitive_Loss : 0.12928, Sensitive_Acc : 21.200, Run Time : 17.84 sec
INFO:root:2024-04-18 05:54:44, Train, Epoch : 10, Step : 6070, Loss : 0.28957, Acc : 0.869, Sensitive_Loss : 0.08975, Sensitive_Acc : 21.100, Run Time : 17.90 sec
INFO:root:2024-04-18 05:55:01, Train, Epoch : 10, Step : 6080, Loss : 0.29255, Acc : 0.853, Sensitive_Loss : 0.06407, Sensitive_Acc : 16.500, Run Time : 16.56 sec
INFO:root:2024-04-18 05:55:18, Train, Epoch : 10, Step : 6090, Loss : 0.29632, Acc : 0.859, Sensitive_Loss : 0.07081, Sensitive_Acc : 20.500, Run Time : 17.51 sec
INFO:root:2024-04-18 05:55:36, Train, Epoch : 10, Step : 6100, Loss : 0.33266, Acc : 0.872, Sensitive_Loss : 0.13032, Sensitive_Acc : 19.200, Run Time : 17.38 sec
INFO:root:2024-04-18 05:59:28, Dev, Step : 6100, Loss : 0.56364, Acc : 0.769, Auc : 0.845, Sensitive_Loss : 0.17959, Sensitive_Acc : 21.346, Sensitive_Auc : 0.998, Mean auc: 0.845, Run Time : 231.79 sec
INFO:root:2024-04-18 05:59:40, Train, Epoch : 10, Step : 6110, Loss : 0.27921, Acc : 0.872, Sensitive_Loss : 0.15813, Sensitive_Acc : 24.800, Run Time : 244.33 sec
INFO:root:2024-04-18 06:00:03, Train, Epoch : 10, Step : 6120, Loss : 0.31697, Acc : 0.853, Sensitive_Loss : 0.08260, Sensitive_Acc : 24.200, Run Time : 22.47 sec
INFO:root:2024-04-18 06:00:21, Train, Epoch : 10, Step : 6130, Loss : 0.32987, Acc : 0.872, Sensitive_Loss : 0.08324, Sensitive_Acc : 16.700, Run Time : 17.99 sec
INFO:root:2024-04-18 06:00:41, Train, Epoch : 10, Step : 6140, Loss : 0.25389, Acc : 0.891, Sensitive_Loss : 0.07037, Sensitive_Acc : 23.800, Run Time : 20.65 sec
INFO:root:2024-04-18 06:01:01, Train, Epoch : 10, Step : 6150, Loss : 0.31647, Acc : 0.847, Sensitive_Loss : 0.09059, Sensitive_Acc : 20.300, Run Time : 19.64 sec
INFO:root:2024-04-18 06:01:21, Train, Epoch : 10, Step : 6160, Loss : 0.36291, Acc : 0.847, Sensitive_Loss : 0.10568, Sensitive_Acc : 22.200, Run Time : 20.01 sec
INFO:root:2024-04-18 06:01:40, Train, Epoch : 10, Step : 6170, Loss : 0.30944, Acc : 0.863, Sensitive_Loss : 0.15754, Sensitive_Acc : 20.200, Run Time : 18.94 sec
INFO:root:2024-04-18 06:01:57, Train, Epoch : 10, Step : 6180, Loss : 0.34814, Acc : 0.847, Sensitive_Loss : 0.05322, Sensitive_Acc : 19.700, Run Time : 17.45 sec
INFO:root:2024-04-18 06:02:17, Train, Epoch : 10, Step : 6190, Loss : 0.32427, Acc : 0.859, Sensitive_Loss : 0.07539, Sensitive_Acc : 23.500, Run Time : 20.28 sec
INFO:root:2024-04-18 06:02:36, Train, Epoch : 10, Step : 6200, Loss : 0.29220, Acc : 0.884, Sensitive_Loss : 0.09614, Sensitive_Acc : 24.600, Run Time : 18.39 sec
INFO:root:2024-04-18 06:06:30, Dev, Step : 6200, Loss : 0.55326, Acc : 0.771, Auc : 0.845, Sensitive_Loss : 0.19841, Sensitive_Acc : 21.361, Sensitive_Auc : 0.998, Mean auc: 0.845, Run Time : 234.05 sec
INFO:root:2024-04-18 06:06:43, Train, Epoch : 10, Step : 6210, Loss : 0.28355, Acc : 0.853, Sensitive_Loss : 0.13306, Sensitive_Acc : 24.700, Run Time : 247.38 sec
INFO:root:2024-04-18 06:07:00, Train, Epoch : 10, Step : 6220, Loss : 0.23532, Acc : 0.922, Sensitive_Loss : 0.08428, Sensitive_Acc : 26.200, Run Time : 16.89 sec
INFO:root:2024-04-18 06:07:19, Train, Epoch : 10, Step : 6230, Loss : 0.38203, Acc : 0.809, Sensitive_Loss : 0.05721, Sensitive_Acc : 22.000, Run Time : 18.58 sec
INFO:root:2024-04-18 06:07:37, Train, Epoch : 10, Step : 6240, Loss : 0.28231, Acc : 0.878, Sensitive_Loss : 0.07918, Sensitive_Acc : 21.900, Run Time : 17.86 sec
INFO:root:2024-04-18 06:07:54, Train, Epoch : 10, Step : 6250, Loss : 0.34973, Acc : 0.856, Sensitive_Loss : 0.07818, Sensitive_Acc : 23.400, Run Time : 17.80 sec
INFO:root:2024-04-18 06:08:11, Train, Epoch : 10, Step : 6260, Loss : 0.32688, Acc : 0.859, Sensitive_Loss : 0.07478, Sensitive_Acc : 24.000, Run Time : 16.58 sec
INFO:root:2024-04-18 06:08:27, Train, Epoch : 10, Step : 6270, Loss : 0.34165, Acc : 0.834, Sensitive_Loss : 0.13214, Sensitive_Acc : 22.600, Run Time : 15.95 sec
INFO:root:2024-04-18 06:08:46, Train, Epoch : 10, Step : 6280, Loss : 0.28134, Acc : 0.884, Sensitive_Loss : 0.08528, Sensitive_Acc : 24.300, Run Time : 18.78 sec
INFO:root:2024-04-18 06:09:04, Train, Epoch : 10, Step : 6290, Loss : 0.28642, Acc : 0.887, Sensitive_Loss : 0.10029, Sensitive_Acc : 18.600, Run Time : 18.04 sec
INFO:root:2024-04-18 06:09:21, Train, Epoch : 10, Step : 6300, Loss : 0.36917, Acc : 0.812, Sensitive_Loss : 0.08321, Sensitive_Acc : 26.900, Run Time : 17.34 sec
INFO:root:2024-04-18 06:13:12, Dev, Step : 6300, Loss : 0.58621, Acc : 0.767, Auc : 0.844, Sensitive_Loss : 0.19020, Sensitive_Acc : 21.481, Sensitive_Auc : 0.998, Mean auc: 0.844, Run Time : 230.45 sec
INFO:root:2024-04-18 06:13:25, Train, Epoch : 10, Step : 6310, Loss : 0.36286, Acc : 0.834, Sensitive_Loss : 0.06349, Sensitive_Acc : 17.400, Run Time : 243.79 sec
INFO:root:2024-04-18 06:13:41, Train, Epoch : 10, Step : 6320, Loss : 0.31398, Acc : 0.863, Sensitive_Loss : 0.08420, Sensitive_Acc : 24.100, Run Time : 16.41 sec
INFO:root:2024-04-18 06:14:01, Train, Epoch : 10, Step : 6330, Loss : 0.27242, Acc : 0.900, Sensitive_Loss : 0.06923, Sensitive_Acc : 19.700, Run Time : 19.60 sec
INFO:root:2024-04-18 06:14:16, Train, Epoch : 10, Step : 6340, Loss : 0.27361, Acc : 0.881, Sensitive_Loss : 0.09661, Sensitive_Acc : 18.100, Run Time : 14.78 sec
slurmstepd: error: *** JOB 175799 ON desktop18 CANCELLED AT 2024-04-18T06:15:31 DUE TO TIME LIMIT ***
