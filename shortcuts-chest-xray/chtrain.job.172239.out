Running on desktop22:
stdin: is not a tty
/home/pmen/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_pmen.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/subsets2/30k-2-train.csv",
    "dev_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/subsets2/30k-2-val.csv",
    "backbone": "densenet121",
    "lambda_val": 1.0,
    "sensitive_attribute": "Sex",
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-03-27 11:59:59, Train, Epoch : 1, Step : 10, Loss : 0.76660, Acc : 0.528, Sensitive_Loss : 0.77476, Sensitive_Acc : 0.469, Run Time : 8.91 sec
INFO:root:2024-03-27 12:00:06, Train, Epoch : 1, Step : 20, Loss : 0.59651, Acc : 0.597, Sensitive_Loss : 0.65812, Sensitive_Acc : 0.428, Run Time : 6.88 sec
INFO:root:2024-03-27 12:00:13, Train, Epoch : 1, Step : 30, Loss : 0.69816, Acc : 0.631, Sensitive_Loss : 0.51813, Sensitive_Acc : 0.463, Run Time : 6.93 sec
INFO:root:2024-03-27 12:00:20, Train, Epoch : 1, Step : 40, Loss : 0.65499, Acc : 0.606, Sensitive_Loss : 0.38460, Sensitive_Acc : 0.566, Run Time : 7.54 sec
INFO:root:2024-03-27 12:00:27, Train, Epoch : 1, Step : 50, Loss : 0.64972, Acc : 0.650, Sensitive_Loss : 0.36257, Sensitive_Acc : 0.534, Run Time : 6.82 sec
INFO:root:2024-03-27 12:00:35, Train, Epoch : 1, Step : 60, Loss : 0.63423, Acc : 0.709, Sensitive_Loss : 0.36067, Sensitive_Acc : 0.487, Run Time : 7.94 sec
INFO:root:2024-03-27 12:00:42, Train, Epoch : 1, Step : 70, Loss : 0.66437, Acc : 0.606, Sensitive_Loss : 0.28387, Sensitive_Acc : 0.438, Run Time : 7.19 sec
INFO:root:2024-03-27 12:00:50, Train, Epoch : 1, Step : 80, Loss : 0.52374, Acc : 0.697, Sensitive_Loss : 0.29239, Sensitive_Acc : 0.431, Run Time : 7.46 sec
INFO:root:2024-03-27 12:00:56, Train, Epoch : 1, Step : 90, Loss : 0.57893, Acc : 0.637, Sensitive_Loss : 0.33200, Sensitive_Acc : 0.409, Run Time : 6.65 sec
INFO:root:2024-03-27 12:01:04, Train, Epoch : 1, Step : 100, Loss : 0.63653, Acc : 0.669, Sensitive_Loss : 0.27015, Sensitive_Acc : 0.484, Run Time : 7.52 sec
INFO:root:2024-03-27 12:03:09, Dev, Step : 100, Loss : 0.65891, Acc : 0.637, Auc : 0.707, Sensitive_Loss : 0.42451, Sensitive_Acc : 0.479, Sensitive_Auc : 0.967, Mean auc: 0.707, Run Time : 125.76 sec
INFO:root:2024-03-27 12:03:10, Best, Step : 100, Loss : 0.65891, Acc : 0.637, Auc : 0.707, Sensitive_Loss : 0.42451, Sensitive_Acc : 0.479, Sensitive_Auc : 0.967, Best Auc : 0.707
INFO:root:2024-03-27 12:03:16, Train, Epoch : 1, Step : 110, Loss : 0.62653, Acc : 0.634, Sensitive_Loss : 0.30340, Sensitive_Acc : 0.500, Run Time : 132.06 sec
INFO:root:2024-03-27 12:03:23, Train, Epoch : 1, Step : 120, Loss : 0.65896, Acc : 0.619, Sensitive_Loss : 0.32291, Sensitive_Acc : 0.403, Run Time : 7.48 sec
INFO:root:2024-03-27 12:03:30, Train, Epoch : 1, Step : 130, Loss : 0.62233, Acc : 0.594, Sensitive_Loss : 0.20200, Sensitive_Acc : 0.416, Run Time : 7.24 sec
INFO:root:2024-03-27 12:03:37, Train, Epoch : 1, Step : 140, Loss : 0.58323, Acc : 0.659, Sensitive_Loss : 0.25694, Sensitive_Acc : 0.562, Run Time : 6.97 sec
INFO:root:2024-03-27 12:03:45, Train, Epoch : 1, Step : 150, Loss : 0.65598, Acc : 0.691, Sensitive_Loss : 0.30870, Sensitive_Acc : 0.537, Run Time : 7.54 sec
INFO:root:2024-03-27 12:03:53, Train, Epoch : 1, Step : 160, Loss : 0.59666, Acc : 0.669, Sensitive_Loss : 0.16710, Sensitive_Acc : 0.428, Run Time : 7.90 sec
INFO:root:2024-03-27 12:03:59, Train, Epoch : 1, Step : 170, Loss : 0.59559, Acc : 0.653, Sensitive_Loss : 0.22003, Sensitive_Acc : 0.438, Run Time : 6.56 sec
INFO:root:2024-03-27 12:04:07, Train, Epoch : 1, Step : 180, Loss : 0.59281, Acc : 0.650, Sensitive_Loss : 0.20798, Sensitive_Acc : 0.419, Run Time : 7.45 sec
INFO:root:2024-03-27 12:04:14, Train, Epoch : 1, Step : 190, Loss : 0.54051, Acc : 0.662, Sensitive_Loss : 0.20222, Sensitive_Acc : 0.475, Run Time : 7.16 sec
INFO:root:2024-03-27 12:04:22, Train, Epoch : 1, Step : 200, Loss : 0.61245, Acc : 0.684, Sensitive_Loss : 0.25464, Sensitive_Acc : 0.491, Run Time : 7.65 sec
INFO:root:2024-03-27 12:06:27, Dev, Step : 200, Loss : 0.65963, Acc : 0.639, Auc : 0.735, Sensitive_Loss : 0.20134, Sensitive_Acc : 0.527, Sensitive_Auc : 0.974, Mean auc: 0.735, Run Time : 125.04 sec
INFO:root:2024-03-27 12:06:27, Best, Step : 200, Loss : 0.65963, Acc : 0.639, Auc : 0.735, Sensitive_Loss : 0.20134, Sensitive_Acc : 0.527, Sensitive_Auc : 0.974, Best Auc : 0.735
INFO:root:2024-03-27 12:06:33, Train, Epoch : 1, Step : 210, Loss : 0.59461, Acc : 0.653, Sensitive_Loss : 0.28480, Sensitive_Acc : 0.509, Run Time : 131.44 sec
INFO:root:2024-03-27 12:06:40, Train, Epoch : 1, Step : 220, Loss : 0.61846, Acc : 0.678, Sensitive_Loss : 0.18112, Sensitive_Acc : 0.484, Run Time : 7.11 sec
INFO:root:2024-03-27 12:06:48, Train, Epoch : 1, Step : 230, Loss : 0.69442, Acc : 0.647, Sensitive_Loss : 0.21439, Sensitive_Acc : 0.484, Run Time : 7.78 sec
INFO:root:2024-03-27 12:06:55, Train, Epoch : 1, Step : 240, Loss : 0.59959, Acc : 0.678, Sensitive_Loss : 0.15931, Sensitive_Acc : 0.450, Run Time : 7.08 sec
INFO:root:2024-03-27 12:07:02, Train, Epoch : 1, Step : 250, Loss : 0.58073, Acc : 0.672, Sensitive_Loss : 0.24604, Sensitive_Acc : 0.491, Run Time : 6.63 sec
INFO:root:2024-03-27 12:07:09, Train, Epoch : 1, Step : 260, Loss : 0.60822, Acc : 0.684, Sensitive_Loss : 0.26623, Sensitive_Acc : 0.481, Run Time : 7.32 sec
INFO:root:2024-03-27 12:07:16, Train, Epoch : 1, Step : 270, Loss : 0.52881, Acc : 0.691, Sensitive_Loss : 0.18008, Sensitive_Acc : 0.566, Run Time : 7.30 sec
INFO:root:2024-03-27 12:07:24, Train, Epoch : 1, Step : 280, Loss : 0.66088, Acc : 0.644, Sensitive_Loss : 0.20216, Sensitive_Acc : 0.463, Run Time : 7.37 sec
INFO:root:2024-03-27 12:07:31, Train, Epoch : 1, Step : 290, Loss : 0.62077, Acc : 0.644, Sensitive_Loss : 0.15817, Sensitive_Acc : 0.422, Run Time : 7.26 sec
INFO:root:2024-03-27 12:07:38, Train, Epoch : 1, Step : 300, Loss : 0.61254, Acc : 0.637, Sensitive_Loss : 0.19813, Sensitive_Acc : 0.456, Run Time : 7.47 sec
INFO:root:2024-03-27 12:09:44, Dev, Step : 300, Loss : 0.59003, Acc : 0.688, Auc : 0.757, Sensitive_Loss : 0.15211, Sensitive_Acc : 0.450, Sensitive_Auc : 0.974, Mean auc: 0.757, Run Time : 125.13 sec
INFO:root:2024-03-27 12:09:44, Best, Step : 300, Loss : 0.59003, Acc : 0.688, Auc : 0.757, Sensitive_Loss : 0.15211, Sensitive_Acc : 0.450, Sensitive_Auc : 0.974, Best Auc : 0.757
INFO:root:2024-03-27 12:09:50, Train, Epoch : 1, Step : 310, Loss : 0.58238, Acc : 0.700, Sensitive_Loss : 0.17564, Sensitive_Acc : 0.478, Run Time : 131.49 sec
INFO:root:2024-03-27 12:09:57, Train, Epoch : 1, Step : 320, Loss : 0.53941, Acc : 0.662, Sensitive_Loss : 0.15069, Sensitive_Acc : 0.438, Run Time : 7.41 sec
INFO:root:2024-03-27 12:10:05, Train, Epoch : 1, Step : 330, Loss : 0.52837, Acc : 0.731, Sensitive_Loss : 0.17233, Sensitive_Acc : 0.519, Run Time : 7.16 sec
INFO:root:2024-03-27 12:10:12, Train, Epoch : 1, Step : 340, Loss : 0.59291, Acc : 0.700, Sensitive_Loss : 0.21168, Sensitive_Acc : 0.541, Run Time : 7.33 sec
INFO:root:2024-03-27 12:10:19, Train, Epoch : 1, Step : 350, Loss : 0.60569, Acc : 0.684, Sensitive_Loss : 0.15014, Sensitive_Acc : 0.541, Run Time : 6.89 sec
INFO:root:2024-03-27 12:10:27, Train, Epoch : 1, Step : 360, Loss : 0.71223, Acc : 0.659, Sensitive_Loss : 0.19780, Sensitive_Acc : 0.463, Run Time : 7.79 sec
INFO:root:2024-03-27 12:10:34, Train, Epoch : 1, Step : 370, Loss : 0.57680, Acc : 0.672, Sensitive_Loss : 0.11629, Sensitive_Acc : 0.394, Run Time : 7.04 sec
INFO:root:2024-03-27 12:10:41, Train, Epoch : 1, Step : 380, Loss : 0.62756, Acc : 0.678, Sensitive_Loss : 0.14789, Sensitive_Acc : 0.431, Run Time : 7.12 sec
INFO:root:2024-03-27 12:10:48, Train, Epoch : 1, Step : 390, Loss : 0.48093, Acc : 0.672, Sensitive_Loss : 0.11671, Sensitive_Acc : 0.466, Run Time : 7.30 sec
INFO:root:2024-03-27 12:10:55, Train, Epoch : 1, Step : 400, Loss : 0.56970, Acc : 0.669, Sensitive_Loss : 0.23649, Sensitive_Acc : 0.428, Run Time : 7.49 sec
INFO:root:2024-03-27 12:13:00, Dev, Step : 400, Loss : 0.60435, Acc : 0.684, Auc : 0.763, Sensitive_Loss : 0.18733, Sensitive_Acc : 0.418, Sensitive_Auc : 0.972, Mean auc: 0.763, Run Time : 124.45 sec
INFO:root:2024-03-27 12:13:01, Best, Step : 400, Loss : 0.60435, Acc : 0.684, Auc : 0.763, Sensitive_Loss : 0.18733, Sensitive_Acc : 0.418, Sensitive_Auc : 0.972, Best Auc : 0.763
INFO:root:2024-03-27 12:13:06, Train, Epoch : 1, Step : 410, Loss : 0.58618, Acc : 0.678, Sensitive_Loss : 0.21446, Sensitive_Acc : 0.550, Run Time : 130.85 sec
INFO:root:2024-03-27 12:13:14, Train, Epoch : 1, Step : 420, Loss : 0.57129, Acc : 0.684, Sensitive_Loss : 0.20092, Sensitive_Acc : 0.584, Run Time : 7.98 sec
INFO:root:2024-03-27 12:13:22, Train, Epoch : 1, Step : 430, Loss : 0.65137, Acc : 0.659, Sensitive_Loss : 0.20757, Sensitive_Acc : 0.434, Run Time : 7.62 sec
INFO:root:2024-03-27 12:13:29, Train, Epoch : 1, Step : 440, Loss : 0.58742, Acc : 0.713, Sensitive_Loss : 0.13669, Sensitive_Acc : 0.438, Run Time : 7.28 sec
INFO:root:2024-03-27 12:13:37, Train, Epoch : 1, Step : 450, Loss : 0.61083, Acc : 0.706, Sensitive_Loss : 0.18050, Sensitive_Acc : 0.472, Run Time : 7.51 sec
INFO:root:2024-03-27 12:13:44, Train, Epoch : 1, Step : 460, Loss : 0.56809, Acc : 0.713, Sensitive_Loss : 0.16308, Sensitive_Acc : 0.509, Run Time : 7.29 sec
INFO:root:2024-03-27 12:13:52, Train, Epoch : 1, Step : 470, Loss : 0.60786, Acc : 0.691, Sensitive_Loss : 0.20953, Sensitive_Acc : 0.531, Run Time : 7.80 sec
INFO:root:2024-03-27 12:13:59, Train, Epoch : 1, Step : 480, Loss : 0.56510, Acc : 0.688, Sensitive_Loss : 0.16150, Sensitive_Acc : 0.472, Run Time : 7.14 sec
INFO:root:2024-03-27 12:14:06, Train, Epoch : 1, Step : 490, Loss : 0.66754, Acc : 0.694, Sensitive_Loss : 0.10669, Sensitive_Acc : 0.472, Run Time : 6.93 sec
INFO:root:2024-03-27 12:14:13, Train, Epoch : 1, Step : 500, Loss : 0.54450, Acc : 0.700, Sensitive_Loss : 0.15106, Sensitive_Acc : 0.528, Run Time : 7.30 sec
INFO:root:2024-03-27 12:16:17, Dev, Step : 500, Loss : 0.58280, Acc : 0.696, Auc : 0.772, Sensitive_Loss : 0.20492, Sensitive_Acc : 0.497, Sensitive_Auc : 0.979, Mean auc: 0.772, Run Time : 123.82 sec
INFO:root:2024-03-27 12:16:18, Best, Step : 500, Loss : 0.58280, Acc : 0.696, Auc : 0.772, Sensitive_Loss : 0.20492, Sensitive_Acc : 0.497, Sensitive_Auc : 0.979, Best Auc : 0.772
INFO:root:2024-03-27 12:16:24, Train, Epoch : 1, Step : 510, Loss : 0.60319, Acc : 0.688, Sensitive_Loss : 0.19061, Sensitive_Acc : 0.447, Run Time : 130.38 sec
INFO:root:2024-03-27 12:16:31, Train, Epoch : 1, Step : 520, Loss : 0.52495, Acc : 0.713, Sensitive_Loss : 0.14080, Sensitive_Acc : 0.416, Run Time : 7.95 sec
INFO:root:2024-03-27 12:16:39, Train, Epoch : 1, Step : 530, Loss : 0.54406, Acc : 0.709, Sensitive_Loss : 0.14587, Sensitive_Acc : 0.469, Run Time : 7.16 sec
INFO:root:2024-03-27 12:16:45, Train, Epoch : 1, Step : 540, Loss : 0.64324, Acc : 0.678, Sensitive_Loss : 0.11205, Sensitive_Acc : 0.506, Run Time : 6.60 sec
INFO:root:2024-03-27 12:16:52, Train, Epoch : 1, Step : 550, Loss : 0.62610, Acc : 0.666, Sensitive_Loss : 0.15630, Sensitive_Acc : 0.469, Run Time : 7.12 sec
INFO:root:2024-03-27 12:17:00, Train, Epoch : 1, Step : 560, Loss : 0.62752, Acc : 0.659, Sensitive_Loss : 0.16533, Sensitive_Acc : 0.453, Run Time : 7.58 sec
INFO:root:2024-03-27 12:17:07, Train, Epoch : 1, Step : 570, Loss : 0.59289, Acc : 0.653, Sensitive_Loss : 0.09409, Sensitive_Acc : 0.369, Run Time : 7.38 sec
INFO:root:2024-03-27 12:17:15, Train, Epoch : 1, Step : 580, Loss : 0.60439, Acc : 0.700, Sensitive_Loss : 0.12944, Sensitive_Acc : 0.491, Run Time : 7.68 sec
INFO:root:2024-03-27 12:17:22, Train, Epoch : 1, Step : 590, Loss : 0.52475, Acc : 0.725, Sensitive_Loss : 0.11851, Sensitive_Acc : 0.537, Run Time : 6.72 sec
INFO:root:2024-03-27 12:17:29, Train, Epoch : 1, Step : 600, Loss : 0.55486, Acc : 0.694, Sensitive_Loss : 0.19892, Sensitive_Acc : 0.506, Run Time : 7.64 sec
INFO:root:2024-03-27 12:19:34, Dev, Step : 600, Loss : 0.62255, Acc : 0.682, Auc : 0.768, Sensitive_Loss : 0.38970, Sensitive_Acc : 0.460, Sensitive_Auc : 0.985, Mean auc: 0.768, Run Time : 124.30 sec
INFO:root:2024-03-27 12:19:39, Train, Epoch : 1, Step : 610, Loss : 0.64580, Acc : 0.681, Sensitive_Loss : 0.23228, Sensitive_Acc : 0.500, Run Time : 130.06 sec
INFO:root:2024-03-27 12:19:47, Train, Epoch : 1, Step : 620, Loss : 0.57454, Acc : 0.719, Sensitive_Loss : 0.18004, Sensitive_Acc : 0.525, Run Time : 7.06 sec
INFO:root:2024-03-27 12:19:54, Train, Epoch : 1, Step : 630, Loss : 0.57832, Acc : 0.697, Sensitive_Loss : 0.18418, Sensitive_Acc : 0.512, Run Time : 7.58 sec
INFO:root:2024-03-27 12:20:01, Train, Epoch : 1, Step : 640, Loss : 0.57252, Acc : 0.653, Sensitive_Loss : 0.16470, Sensitive_Acc : 0.509, Run Time : 7.26 sec
INFO:root:2024-03-27 12:20:09, Train, Epoch : 1, Step : 650, Loss : 0.53699, Acc : 0.716, Sensitive_Loss : 0.12125, Sensitive_Acc : 0.456, Run Time : 7.49 sec
INFO:root:2024-03-27 12:20:16, Train, Epoch : 1, Step : 660, Loss : 0.60668, Acc : 0.700, Sensitive_Loss : 0.12205, Sensitive_Acc : 0.431, Run Time : 7.13 sec
INFO:root:2024-03-27 12:20:23, Train, Epoch : 1, Step : 670, Loss : 0.52976, Acc : 0.738, Sensitive_Loss : 0.10006, Sensitive_Acc : 0.450, Run Time : 7.15 sec
INFO:root:2024-03-27 12:20:31, Train, Epoch : 1, Step : 680, Loss : 0.56958, Acc : 0.694, Sensitive_Loss : 0.14449, Sensitive_Acc : 0.487, Run Time : 7.45 sec
INFO:root:2024-03-27 12:20:37, Train, Epoch : 1, Step : 690, Loss : 0.59690, Acc : 0.731, Sensitive_Loss : 0.12770, Sensitive_Acc : 0.425, Run Time : 6.76 sec
INFO:root:2024-03-27 12:20:44, Train, Epoch : 1, Step : 700, Loss : 0.47728, Acc : 0.731, Sensitive_Loss : 0.15632, Sensitive_Acc : 0.450, Run Time : 7.11 sec
INFO:root:2024-03-27 12:22:49, Dev, Step : 700, Loss : 0.56271, Acc : 0.721, Auc : 0.788, Sensitive_Loss : 0.14134, Sensitive_Acc : 0.456, Sensitive_Auc : 0.988, Mean auc: 0.788, Run Time : 124.44 sec
INFO:root:2024-03-27 12:22:50, Best, Step : 700, Loss : 0.56271, Acc : 0.721, Auc : 0.788, Sensitive_Loss : 0.14134, Sensitive_Acc : 0.456, Sensitive_Auc : 0.988, Best Auc : 0.788
INFO:root:2024-03-27 12:22:55, Train, Epoch : 1, Step : 710, Loss : 0.62073, Acc : 0.684, Sensitive_Loss : 0.15279, Sensitive_Acc : 0.494, Run Time : 130.99 sec
INFO:root:2024-03-27 12:23:02, Train, Epoch : 1, Step : 720, Loss : 0.64329, Acc : 0.656, Sensitive_Loss : 0.11025, Sensitive_Acc : 0.469, Run Time : 6.72 sec
INFO:root:2024-03-27 12:23:10, Train, Epoch : 1, Step : 730, Loss : 0.53563, Acc : 0.662, Sensitive_Loss : 0.17400, Sensitive_Acc : 0.503, Run Time : 7.35 sec
INFO:root:2024-03-27 12:23:16, Train, Epoch : 1, Step : 740, Loss : 0.51013, Acc : 0.684, Sensitive_Loss : 0.15797, Sensitive_Acc : 0.487, Run Time : 6.92 sec
INFO:root:2024-03-27 12:23:23, Train, Epoch : 1, Step : 750, Loss : 0.66496, Acc : 0.709, Sensitive_Loss : 0.13032, Sensitive_Acc : 0.509, Run Time : 6.86 sec
INFO:root:2024-03-27 12:25:24
INFO:root:y_pred: [0.28774303 0.56396353 0.73675334 ... 0.7761415  0.5382351  0.40550113]
INFO:root:y_true: [1. 1. 1. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [3.5334623e-01 1.8148407e-02 1.7264914e-05 9.9043673e-01 2.4158609e-04
 9.9991596e-01 9.9641454e-01 7.9258514e-04 3.0662143e-04 9.9999750e-01
 9.9948800e-01 8.0006020e-03 3.6499407e-02 7.6139253e-03 3.9006508e-04
 2.0450067e-03 5.0116298e-03 1.2339540e-02 9.7803081e-07 4.8300973e-03
 9.9994791e-01 2.2319432e-03 8.3441359e-05 8.1352945e-03 4.4641130e-02
 2.0745555e-01 9.9477452e-01 9.5553571e-01 2.6778851e-04 2.1330551e-03
 2.2589901e-02 9.9998629e-01 9.9754059e-01 4.1138148e-01 8.4827698e-05
 9.8735112e-01 5.4844801e-05 6.2589318e-04 9.9962437e-01 7.4540544e-03
 8.2948819e-02 3.3952245e-05 8.4857762e-01 9.9795556e-01 9.1921175e-01
 1.6377172e-02 9.9442011e-01 6.5373129e-01 2.9328206e-01 8.8554880e-05
 2.9837672e-02 2.9000964e-03 5.2090390e-03 3.0103816e-02 9.9976140e-01
 9.9999344e-01 2.4010660e-04 9.0742513e-04 9.8936826e-01 3.0857591e-05
 1.2769738e-03 9.9998808e-01 2.1485761e-03 9.9970597e-01 5.2316347e-04
 5.4818651e-05 2.8370422e-01 4.7191638e-02 8.7446888e-04 9.9557614e-01
 9.9987197e-01 9.9997878e-01 8.1513748e-03 9.9994159e-01 1.1811601e-02
 2.2109874e-01 9.9938405e-01 5.5738603e-04 2.6702250e-05 9.9645364e-01
 3.3952887e-03 2.6935714e-04 9.7549510e-01 3.1127958e-04 9.8198777e-01
 1.1253265e-04 4.9236021e-04 8.4766361e-06 9.9981374e-01 9.9004805e-01
 5.1423878e-04 1.7034703e-01 9.9856722e-01 3.2292408e-04 9.8660046e-01
 5.1071437e-04 9.9612051e-01 9.5330179e-02 5.1832730e-01 3.2841921e-04
 4.7691599e-03 2.1035206e-01 1.8185863e-03 1.0006821e-03 9.9251539e-01
 4.4150020e-05 2.7435809e-01 1.8610719e-04 2.2057814e-03 6.7381081e-03
 6.0533468e-05 5.7612951e-03 1.5722848e-04 2.5591027e-04 9.9998415e-01
 7.2198681e-04 1.0082283e-03 7.4412614e-02 5.5391155e-04 1.0032892e-03
 1.0919783e-03 9.9999070e-01 8.2830548e-01 9.9999893e-01 3.4227218e-02
 1.2665782e-03 2.6796754e-03 3.0824254e-04 3.3832817e-05 9.6239157e-05
 2.4674378e-02 8.1738764e-01 3.1282713e-03 8.6367041e-01 3.7701731e-03
 2.2748227e-03 2.1950042e-04 9.9996340e-01 7.2800301e-02 9.9958080e-01
 2.7564164e-02 9.9519265e-01 9.7592396e-01 5.8976084e-04 9.9976152e-01
 1.0842309e-02 9.9942744e-01 9.9855584e-01 9.9986613e-01 9.9926168e-01
 8.6235786e-03 2.5576755e-05 3.4111174e-04 5.1475683e-05 9.9298966e-01
 2.7834438e-03 9.4802535e-01 1.6264182e-02 5.0140923e-01 8.1684107e-01
 5.5584830e-04 1.9035862e-01 2.4907288e-04 9.9957389e-01 6.0165054e-03
 9.9985445e-01 9.6876091e-01 1.3811297e-04 9.9994218e-01 5.6741247e-04
 9.9102658e-01 9.3450248e-01 5.4497602e-03 9.9982858e-01 9.3748683e-01
 9.9999905e-01 2.3698976e-04 1.4823596e-02 9.9985671e-01 3.6996100e-02
 5.5727607e-04 6.9634011e-04 1.4148012e-04 3.7334419e-05 3.8877053e-05
 9.8699516e-01 9.8897517e-01 3.1521773e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.
 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.
 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0.
 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]
INFO:root:2024-03-27 12:25:24, Dev, Step : 750, Loss : 0.59322, Acc : 0.691, Auc : 0.777, Sensitive_Loss : 0.12214, Sensitive_Acc : 0.571, Sensitive_Auc : 0.989, Mean auc: 0.777, Run Time : 120.83 sec
INFO:root:2024-03-27 12:25:33, Train, Epoch : 2, Step : 760, Loss : 0.55528, Acc : 0.741, Sensitive_Loss : 0.08449, Sensitive_Acc : 0.550, Run Time : 8.33 sec
INFO:root:2024-03-27 12:25:41, Train, Epoch : 2, Step : 770, Loss : 0.53741, Acc : 0.753, Sensitive_Loss : 0.11503, Sensitive_Acc : 0.547, Run Time : 7.24 sec
INFO:root:2024-03-27 12:25:48, Train, Epoch : 2, Step : 780, Loss : 0.50250, Acc : 0.703, Sensitive_Loss : 0.11564, Sensitive_Acc : 0.438, Run Time : 7.46 sec
INFO:root:2024-03-27 12:25:55, Train, Epoch : 2, Step : 790, Loss : 0.56948, Acc : 0.691, Sensitive_Loss : 0.11707, Sensitive_Acc : 0.456, Run Time : 7.18 sec
INFO:root:2024-03-27 12:26:04, Train, Epoch : 2, Step : 800, Loss : 0.55561, Acc : 0.706, Sensitive_Loss : 0.15204, Sensitive_Acc : 0.466, Run Time : 8.69 sec
INFO:root:2024-03-27 12:28:15, Dev, Step : 800, Loss : 0.61251, Acc : 0.677, Auc : 0.791, Sensitive_Loss : 0.10041, Sensitive_Acc : 0.408, Sensitive_Auc : 0.989, Mean auc: 0.791, Run Time : 131.55 sec
INFO:root:2024-03-27 12:28:16, Best, Step : 800, Loss : 0.61251, Acc : 0.677, Auc : 0.791, Sensitive_Loss : 0.10041, Sensitive_Acc : 0.408, Sensitive_Auc : 0.989, Best Auc : 0.791
INFO:root:2024-03-27 12:28:22, Train, Epoch : 2, Step : 810, Loss : 0.58021, Acc : 0.731, Sensitive_Loss : 0.09609, Sensitive_Acc : 0.472, Run Time : 138.26 sec
INFO:root:2024-03-27 12:28:30, Train, Epoch : 2, Step : 820, Loss : 0.49261, Acc : 0.725, Sensitive_Loss : 0.13756, Sensitive_Acc : 0.525, Run Time : 7.63 sec
INFO:root:2024-03-27 12:28:37, Train, Epoch : 2, Step : 830, Loss : 0.50841, Acc : 0.728, Sensitive_Loss : 0.06412, Sensitive_Acc : 0.537, Run Time : 7.01 sec
INFO:root:2024-03-27 12:28:44, Train, Epoch : 2, Step : 840, Loss : 0.53864, Acc : 0.728, Sensitive_Loss : 0.15598, Sensitive_Acc : 0.516, Run Time : 7.37 sec
INFO:root:2024-03-27 12:28:52, Train, Epoch : 2, Step : 850, Loss : 0.57950, Acc : 0.722, Sensitive_Loss : 0.10829, Sensitive_Acc : 0.459, Run Time : 7.77 sec
INFO:root:2024-03-27 12:28:59, Train, Epoch : 2, Step : 860, Loss : 0.57978, Acc : 0.709, Sensitive_Loss : 0.11724, Sensitive_Acc : 0.506, Run Time : 7.15 sec
INFO:root:2024-03-27 12:29:07, Train, Epoch : 2, Step : 870, Loss : 0.55148, Acc : 0.697, Sensitive_Loss : 0.08074, Sensitive_Acc : 0.534, Run Time : 7.46 sec
INFO:root:2024-03-27 12:29:14, Train, Epoch : 2, Step : 880, Loss : 0.46401, Acc : 0.759, Sensitive_Loss : 0.09793, Sensitive_Acc : 0.391, Run Time : 7.18 sec
INFO:root:2024-03-27 12:29:21, Train, Epoch : 2, Step : 890, Loss : 0.52296, Acc : 0.731, Sensitive_Loss : 0.19204, Sensitive_Acc : 0.434, Run Time : 7.48 sec
INFO:root:2024-03-27 12:29:29, Train, Epoch : 2, Step : 900, Loss : 0.51339, Acc : 0.703, Sensitive_Loss : 0.08097, Sensitive_Acc : 0.481, Run Time : 7.52 sec
INFO:root:2024-03-27 12:31:32, Dev, Step : 900, Loss : 0.56387, Acc : 0.724, Auc : 0.793, Sensitive_Loss : 0.13618, Sensitive_Acc : 0.529, Sensitive_Auc : 0.988, Mean auc: 0.793, Run Time : 122.93 sec
INFO:root:2024-03-27 12:31:33, Best, Step : 900, Loss : 0.56387, Acc : 0.724, Auc : 0.793, Sensitive_Loss : 0.13618, Sensitive_Acc : 0.529, Sensitive_Auc : 0.988, Best Auc : 0.793
INFO:root:2024-03-27 12:31:39, Train, Epoch : 2, Step : 910, Loss : 0.49444, Acc : 0.738, Sensitive_Loss : 0.10079, Sensitive_Acc : 0.466, Run Time : 129.97 sec
INFO:root:2024-03-27 12:31:46, Train, Epoch : 2, Step : 920, Loss : 0.50473, Acc : 0.766, Sensitive_Loss : 0.12722, Sensitive_Acc : 0.450, Run Time : 7.29 sec
INFO:root:2024-03-27 12:31:53, Train, Epoch : 2, Step : 930, Loss : 0.57930, Acc : 0.691, Sensitive_Loss : 0.09413, Sensitive_Acc : 0.463, Run Time : 6.92 sec
INFO:root:2024-03-27 12:32:01, Train, Epoch : 2, Step : 940, Loss : 0.55562, Acc : 0.709, Sensitive_Loss : 0.08341, Sensitive_Acc : 0.556, Run Time : 7.86 sec
INFO:root:2024-03-27 12:32:08, Train, Epoch : 2, Step : 950, Loss : 0.64388, Acc : 0.681, Sensitive_Loss : 0.10860, Sensitive_Acc : 0.475, Run Time : 7.33 sec
INFO:root:2024-03-27 12:32:15, Train, Epoch : 2, Step : 960, Loss : 0.56491, Acc : 0.703, Sensitive_Loss : 0.11164, Sensitive_Acc : 0.553, Run Time : 7.26 sec
INFO:root:2024-03-27 12:32:23, Train, Epoch : 2, Step : 970, Loss : 0.49063, Acc : 0.728, Sensitive_Loss : 0.08499, Sensitive_Acc : 0.453, Run Time : 7.10 sec
INFO:root:2024-03-27 12:32:30, Train, Epoch : 2, Step : 980, Loss : 0.49791, Acc : 0.738, Sensitive_Loss : 0.09854, Sensitive_Acc : 0.409, Run Time : 7.22 sec
INFO:root:2024-03-27 12:32:37, Train, Epoch : 2, Step : 990, Loss : 0.60117, Acc : 0.681, Sensitive_Loss : 0.11801, Sensitive_Acc : 0.459, Run Time : 7.55 sec
INFO:root:2024-03-27 12:32:44, Train, Epoch : 2, Step : 1000, Loss : 0.62596, Acc : 0.644, Sensitive_Loss : 0.08740, Sensitive_Acc : 0.497, Run Time : 6.99 sec
INFO:root:2024-03-27 12:34:49, Dev, Step : 1000, Loss : 0.57633, Acc : 0.709, Auc : 0.774, Sensitive_Loss : 0.08409, Sensitive_Acc : 0.552, Sensitive_Auc : 0.991, Mean auc: 0.774, Run Time : 124.26 sec
INFO:root:2024-03-27 12:34:54, Train, Epoch : 2, Step : 1010, Loss : 0.52303, Acc : 0.766, Sensitive_Loss : 0.09867, Sensitive_Acc : 0.534, Run Time : 129.96 sec
INFO:root:2024-03-27 12:35:01, Train, Epoch : 2, Step : 1020, Loss : 0.52122, Acc : 0.734, Sensitive_Loss : 0.09453, Sensitive_Acc : 0.478, Run Time : 7.16 sec
INFO:root:2024-03-27 12:35:09, Train, Epoch : 2, Step : 1030, Loss : 0.55118, Acc : 0.706, Sensitive_Loss : 0.08040, Sensitive_Acc : 0.447, Run Time : 7.59 sec
INFO:root:2024-03-27 12:35:16, Train, Epoch : 2, Step : 1040, Loss : 0.50472, Acc : 0.738, Sensitive_Loss : 0.08739, Sensitive_Acc : 0.441, Run Time : 7.38 sec
INFO:root:2024-03-27 12:35:24, Train, Epoch : 2, Step : 1050, Loss : 0.56753, Acc : 0.719, Sensitive_Loss : 0.13429, Sensitive_Acc : 0.516, Run Time : 7.67 sec
INFO:root:2024-03-27 12:35:31, Train, Epoch : 2, Step : 1060, Loss : 0.55337, Acc : 0.728, Sensitive_Loss : 0.13461, Sensitive_Acc : 0.503, Run Time : 7.00 sec
INFO:root:2024-03-27 12:35:38, Train, Epoch : 2, Step : 1070, Loss : 0.62277, Acc : 0.678, Sensitive_Loss : 0.12821, Sensitive_Acc : 0.500, Run Time : 7.38 sec
INFO:root:2024-03-27 12:35:45, Train, Epoch : 2, Step : 1080, Loss : 0.55579, Acc : 0.734, Sensitive_Loss : 0.12365, Sensitive_Acc : 0.475, Run Time : 6.91 sec
INFO:root:2024-03-27 12:35:52, Train, Epoch : 2, Step : 1090, Loss : 0.55193, Acc : 0.694, Sensitive_Loss : 0.11626, Sensitive_Acc : 0.441, Run Time : 7.01 sec
INFO:root:2024-03-27 12:36:00, Train, Epoch : 2, Step : 1100, Loss : 0.65596, Acc : 0.688, Sensitive_Loss : 0.07643, Sensitive_Acc : 0.463, Run Time : 7.82 sec
INFO:root:2024-03-27 12:38:05, Dev, Step : 1100, Loss : 0.57295, Acc : 0.710, Auc : 0.777, Sensitive_Loss : 0.09580, Sensitive_Acc : 0.496, Sensitive_Auc : 0.987, Mean auc: 0.777, Run Time : 124.41 sec
INFO:root:2024-03-27 12:38:10, Train, Epoch : 2, Step : 1110, Loss : 0.52951, Acc : 0.772, Sensitive_Loss : 0.14553, Sensitive_Acc : 0.553, Run Time : 129.88 sec
INFO:root:2024-03-27 12:38:17, Train, Epoch : 2, Step : 1120, Loss : 0.54482, Acc : 0.725, Sensitive_Loss : 0.10602, Sensitive_Acc : 0.556, Run Time : 7.43 sec
INFO:root:2024-03-27 12:38:25, Train, Epoch : 2, Step : 1130, Loss : 0.54022, Acc : 0.734, Sensitive_Loss : 0.11551, Sensitive_Acc : 0.447, Run Time : 7.42 sec
INFO:root:2024-03-27 12:38:32, Train, Epoch : 2, Step : 1140, Loss : 0.53677, Acc : 0.713, Sensitive_Loss : 0.10683, Sensitive_Acc : 0.503, Run Time : 6.92 sec
INFO:root:2024-03-27 12:38:39, Train, Epoch : 2, Step : 1150, Loss : 0.51155, Acc : 0.741, Sensitive_Loss : 0.10214, Sensitive_Acc : 0.537, Run Time : 7.56 sec
INFO:root:2024-03-27 12:38:47, Train, Epoch : 2, Step : 1160, Loss : 0.54538, Acc : 0.716, Sensitive_Loss : 0.13236, Sensitive_Acc : 0.509, Run Time : 7.36 sec
INFO:root:2024-03-27 12:38:54, Train, Epoch : 2, Step : 1170, Loss : 0.53810, Acc : 0.691, Sensitive_Loss : 0.08714, Sensitive_Acc : 0.459, Run Time : 7.44 sec
INFO:root:2024-03-27 12:39:02, Train, Epoch : 2, Step : 1180, Loss : 0.54040, Acc : 0.744, Sensitive_Loss : 0.08758, Sensitive_Acc : 0.466, Run Time : 7.36 sec
INFO:root:2024-03-27 12:39:08, Train, Epoch : 2, Step : 1190, Loss : 0.55768, Acc : 0.722, Sensitive_Loss : 0.14634, Sensitive_Acc : 0.438, Run Time : 6.81 sec
INFO:root:2024-03-27 12:39:16, Train, Epoch : 2, Step : 1200, Loss : 0.50221, Acc : 0.738, Sensitive_Loss : 0.12282, Sensitive_Acc : 0.481, Run Time : 7.92 sec
INFO:root:2024-03-27 12:41:20, Dev, Step : 1200, Loss : 0.56437, Acc : 0.720, Auc : 0.793, Sensitive_Loss : 0.18339, Sensitive_Acc : 0.518, Sensitive_Auc : 0.989, Mean auc: 0.793, Run Time : 123.28 sec
INFO:root:2024-03-27 12:41:20, Best, Step : 1200, Loss : 0.56437, Acc : 0.720, Auc : 0.793, Sensitive_Loss : 0.18339, Sensitive_Acc : 0.518, Sensitive_Auc : 0.989, Best Auc : 0.793
INFO:root:2024-03-27 12:41:26, Train, Epoch : 2, Step : 1210, Loss : 0.51617, Acc : 0.716, Sensitive_Loss : 0.09046, Sensitive_Acc : 0.506, Run Time : 129.60 sec
INFO:root:2024-03-27 12:41:33, Train, Epoch : 2, Step : 1220, Loss : 0.52675, Acc : 0.722, Sensitive_Loss : 0.11321, Sensitive_Acc : 0.453, Run Time : 7.55 sec
INFO:root:2024-03-27 12:41:41, Train, Epoch : 2, Step : 1230, Loss : 0.51507, Acc : 0.725, Sensitive_Loss : 0.12507, Sensitive_Acc : 0.500, Run Time : 7.14 sec
INFO:root:2024-03-27 12:41:48, Train, Epoch : 2, Step : 1240, Loss : 0.51534, Acc : 0.716, Sensitive_Loss : 0.12406, Sensitive_Acc : 0.456, Run Time : 7.11 sec
INFO:root:2024-03-27 12:41:56, Train, Epoch : 2, Step : 1250, Loss : 0.57746, Acc : 0.706, Sensitive_Loss : 0.11391, Sensitive_Acc : 0.509, Run Time : 7.94 sec
INFO:root:2024-03-27 12:42:03, Train, Epoch : 2, Step : 1260, Loss : 0.51378, Acc : 0.769, Sensitive_Loss : 0.08066, Sensitive_Acc : 0.469, Run Time : 7.06 sec
INFO:root:2024-03-27 12:42:10, Train, Epoch : 2, Step : 1270, Loss : 0.54605, Acc : 0.703, Sensitive_Loss : 0.08838, Sensitive_Acc : 0.478, Run Time : 7.20 sec
INFO:root:2024-03-27 12:42:17, Train, Epoch : 2, Step : 1280, Loss : 0.54940, Acc : 0.741, Sensitive_Loss : 0.10675, Sensitive_Acc : 0.463, Run Time : 7.51 sec
INFO:root:2024-03-27 12:42:25, Train, Epoch : 2, Step : 1290, Loss : 0.58293, Acc : 0.716, Sensitive_Loss : 0.06479, Sensitive_Acc : 0.506, Run Time : 7.32 sec
INFO:root:2024-03-27 12:42:31, Train, Epoch : 2, Step : 1300, Loss : 0.55773, Acc : 0.728, Sensitive_Loss : 0.13010, Sensitive_Acc : 0.506, Run Time : 6.72 sec
INFO:root:2024-03-27 12:44:34, Dev, Step : 1300, Loss : 0.60188, Acc : 0.697, Auc : 0.804, Sensitive_Loss : 0.09116, Sensitive_Acc : 0.534, Sensitive_Auc : 0.992, Mean auc: 0.804, Run Time : 122.14 sec
INFO:root:2024-03-27 12:44:35, Best, Step : 1300, Loss : 0.60188, Acc : 0.697, Auc : 0.804, Sensitive_Loss : 0.09116, Sensitive_Acc : 0.534, Sensitive_Auc : 0.992, Best Auc : 0.804
INFO:root:2024-03-27 12:44:41, Train, Epoch : 2, Step : 1310, Loss : 0.51831, Acc : 0.756, Sensitive_Loss : 0.09922, Sensitive_Acc : 0.481, Run Time : 129.20 sec
INFO:root:2024-03-27 12:44:48, Train, Epoch : 2, Step : 1320, Loss : 0.56937, Acc : 0.706, Sensitive_Loss : 0.04759, Sensitive_Acc : 0.478, Run Time : 6.95 sec
INFO:root:2024-03-27 12:44:55, Train, Epoch : 2, Step : 1330, Loss : 0.58868, Acc : 0.719, Sensitive_Loss : 0.11009, Sensitive_Acc : 0.519, Run Time : 7.13 sec
INFO:root:2024-03-27 12:45:02, Train, Epoch : 2, Step : 1340, Loss : 0.57553, Acc : 0.700, Sensitive_Loss : 0.09506, Sensitive_Acc : 0.478, Run Time : 7.32 sec
INFO:root:2024-03-27 12:45:10, Train, Epoch : 2, Step : 1350, Loss : 0.53096, Acc : 0.747, Sensitive_Loss : 0.08314, Sensitive_Acc : 0.381, Run Time : 7.63 sec
INFO:root:2024-03-27 12:45:17, Train, Epoch : 2, Step : 1360, Loss : 0.57900, Acc : 0.728, Sensitive_Loss : 0.08640, Sensitive_Acc : 0.419, Run Time : 7.10 sec
INFO:root:2024-03-27 12:45:24, Train, Epoch : 2, Step : 1370, Loss : 0.55103, Acc : 0.741, Sensitive_Loss : 0.12573, Sensitive_Acc : 0.509, Run Time : 7.65 sec
INFO:root:2024-03-27 12:45:32, Train, Epoch : 2, Step : 1380, Loss : 0.60726, Acc : 0.691, Sensitive_Loss : 0.10840, Sensitive_Acc : 0.537, Run Time : 7.26 sec
INFO:root:2024-03-27 12:45:40, Train, Epoch : 2, Step : 1390, Loss : 0.56616, Acc : 0.722, Sensitive_Loss : 0.10681, Sensitive_Acc : 0.403, Run Time : 7.94 sec
INFO:root:2024-03-27 12:45:47, Train, Epoch : 2, Step : 1400, Loss : 0.54815, Acc : 0.722, Sensitive_Loss : 0.13004, Sensitive_Acc : 0.478, Run Time : 6.92 sec
INFO:root:2024-03-27 12:47:52, Dev, Step : 1400, Loss : 0.54180, Acc : 0.737, Auc : 0.809, Sensitive_Loss : 0.09293, Sensitive_Acc : 0.495, Sensitive_Auc : 0.993, Mean auc: 0.809, Run Time : 125.27 sec
INFO:root:2024-03-27 12:47:52, Best, Step : 1400, Loss : 0.54180, Acc : 0.737, Auc : 0.809, Sensitive_Loss : 0.09293, Sensitive_Acc : 0.495, Sensitive_Auc : 0.993, Best Auc : 0.809
INFO:root:2024-03-27 12:47:58, Train, Epoch : 2, Step : 1410, Loss : 0.57788, Acc : 0.691, Sensitive_Loss : 0.13984, Sensitive_Acc : 0.494, Run Time : 131.40 sec
INFO:root:2024-03-27 12:48:06, Train, Epoch : 2, Step : 1420, Loss : 0.48780, Acc : 0.722, Sensitive_Loss : 0.10208, Sensitive_Acc : 0.512, Run Time : 7.64 sec
INFO:root:2024-03-27 12:48:13, Train, Epoch : 2, Step : 1430, Loss : 0.52872, Acc : 0.750, Sensitive_Loss : 0.09080, Sensitive_Acc : 0.491, Run Time : 7.69 sec
INFO:root:2024-03-27 12:48:20, Train, Epoch : 2, Step : 1440, Loss : 0.51335, Acc : 0.713, Sensitive_Loss : 0.13974, Sensitive_Acc : 0.494, Run Time : 6.95 sec
INFO:root:2024-03-27 12:48:27, Train, Epoch : 2, Step : 1450, Loss : 0.57805, Acc : 0.709, Sensitive_Loss : 0.12286, Sensitive_Acc : 0.494, Run Time : 7.11 sec
INFO:root:2024-03-27 12:48:34, Train, Epoch : 2, Step : 1460, Loss : 0.55965, Acc : 0.722, Sensitive_Loss : 0.11064, Sensitive_Acc : 0.475, Run Time : 7.18 sec
INFO:root:2024-03-27 12:48:42, Train, Epoch : 2, Step : 1470, Loss : 0.51883, Acc : 0.738, Sensitive_Loss : 0.13655, Sensitive_Acc : 0.487, Run Time : 7.03 sec
INFO:root:2024-03-27 12:48:49, Train, Epoch : 2, Step : 1480, Loss : 0.55159, Acc : 0.722, Sensitive_Loss : 0.12793, Sensitive_Acc : 0.422, Run Time : 7.50 sec
INFO:root:2024-03-27 12:48:57, Train, Epoch : 2, Step : 1490, Loss : 0.51145, Acc : 0.706, Sensitive_Loss : 0.05647, Sensitive_Acc : 0.428, Run Time : 7.56 sec
INFO:root:2024-03-27 12:49:03, Train, Epoch : 2, Step : 1500, Loss : 0.54985, Acc : 0.703, Sensitive_Loss : 0.09957, Sensitive_Acc : 0.487, Run Time : 6.36 sec
INFO:root:2024-03-27 12:51:06, Dev, Step : 1500, Loss : 0.55375, Acc : 0.724, Auc : 0.795, Sensitive_Loss : 0.08048, Sensitive_Acc : 0.488, Sensitive_Auc : 0.996, Mean auc: 0.795, Run Time : 123.07 sec
INFO:root:2024-03-27 12:53:11
INFO:root:y_pred: [0.20411673 0.47058824 0.9663972  ... 0.9210618  0.5519674  0.5709691 ]
INFO:root:y_true: [1. 1. 1. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.63087082e-01 6.70178056e-01 1.00326190e-06 9.99046981e-01
 2.89468793e-03 9.99957442e-01 9.99965787e-01 1.39923848e-03
 3.79796466e-03 9.99998927e-01 9.99930382e-01 8.90905038e-03
 1.18870018e-02 9.21004862e-02 5.68241114e-03 1.53330830e-03
 9.28291902e-02 8.22253060e-03 3.24168263e-06 1.35177627e-01
 9.99932170e-01 6.98707849e-02 8.72917299e-05 7.17807747e-03
 6.49170339e-01 8.79970372e-01 9.99923706e-01 9.56744671e-01
 1.47599718e-02 4.29304410e-03 9.74633824e-03 9.99992728e-01
 9.99233365e-01 9.44053590e-01 2.40724590e-02 9.98293698e-01
 2.14662687e-05 2.55281059e-03 9.99844432e-01 1.29140588e-02
 2.07746804e-01 3.43450723e-04 8.67570996e-01 9.97098565e-01
 9.99829292e-01 9.65733752e-02 9.97615695e-01 9.58799005e-01
 7.66072989e-01 8.43096423e-05 2.07634009e-02 8.19501095e-03
 7.30146421e-03 6.07540429e-01 9.99951124e-01 9.99943495e-01
 7.71677369e-05 1.13155143e-02 9.97740865e-01 5.37760968e-07
 7.17611169e-04 9.99998450e-01 1.22068857e-04 9.99948740e-01
 6.57788405e-05 1.31646381e-03 4.22018813e-03 4.31984256e-04
 8.39380641e-03 9.95040834e-01 9.99988317e-01 9.99956608e-01
 3.50521982e-01 9.99589741e-01 6.57021850e-02 2.36413255e-03
 9.99968648e-01 1.17725146e-03 1.64719531e-05 9.70082641e-01
 2.05083545e-02 5.92584815e-03 9.99507904e-01 1.80404677e-04
 9.87734735e-01 7.43956247e-04 2.49161269e-04 7.59143877e-05
 9.99958992e-01 9.97095108e-01 1.49955449e-04 4.18652408e-03
 9.59663212e-01 2.14300089e-04 9.99937057e-01 2.10754364e-03
 9.99963403e-01 1.97311587e-04 5.68142474e-01 2.52305763e-04
 1.31474018e-01 9.27439053e-03 9.03916894e-04 9.40250829e-02
 9.99798119e-01 5.50493605e-05 4.29916441e-01 2.18030735e-04
 1.52362464e-02 2.10147724e-01 1.89914499e-04 1.78801864e-02
 7.68185491e-05 1.47160003e-03 9.99910355e-01 1.99350365e-03
 1.84000255e-05 3.97906546e-03 4.56126928e-02 2.76163151e-03
 8.06680706e-04 9.99987483e-01 9.74619210e-01 9.99954343e-01
 1.04786776e-01 1.18468633e-04 1.70259941e-02 1.07336557e-04
 9.06590285e-05 2.18490037e-04 1.06793143e-01 2.86379904e-01
 4.23559191e-04 9.91259158e-01 5.78181073e-03 2.37095542e-03
 9.02046449e-04 9.99982238e-01 7.90695623e-02 9.99996424e-01
 1.53580569e-02 9.99898553e-01 9.98129904e-01 2.61178799e-03
 9.99948502e-01 3.96179408e-01 9.99913335e-01 9.99743879e-01
 9.99998808e-01 9.99978423e-01 6.65213048e-01 2.48383712e-05
 1.97610017e-04 1.13954702e-05 9.91934299e-01 2.74479352e-02
 9.99134362e-01 1.92902163e-02 9.94609118e-01 9.98829901e-01
 1.84247445e-04 2.12924350e-02 8.41592671e-04 9.98214126e-01
 6.88738897e-02 9.98340249e-01 9.98397171e-01 2.97093247e-05
 9.99997616e-01 7.05430066e-05 9.98595297e-01 9.86023068e-01
 7.03974103e-04 9.99899745e-01 9.87126052e-01 9.99997616e-01
 3.35803838e-03 6.02551550e-02 9.99993443e-01 7.05468833e-01
 1.90825108e-02 2.14586072e-02 2.75045371e-04 4.56935493e-04
 2.59709603e-04 9.97968972e-01 9.92774487e-01 2.22931877e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.
 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.
 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0.
 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]
INFO:root:2024-03-27 12:53:11, Dev, Step : 1500, Loss : 0.55375, Acc : 0.724, Auc : 0.795, Sensitive_Loss : 0.08048, Sensitive_Acc : 0.488, Sensitive_Auc : 0.996, Mean auc: 0.795, Run Time : 124.75 sec
INFO:root:2024-03-27 12:53:20, Train, Epoch : 3, Step : 1510, Loss : 0.52361, Acc : 0.722, Sensitive_Loss : 0.10372, Sensitive_Acc : 0.516, Run Time : 8.30 sec
INFO:root:2024-03-27 12:53:27, Train, Epoch : 3, Step : 1520, Loss : 0.52428, Acc : 0.722, Sensitive_Loss : 0.07859, Sensitive_Acc : 0.494, Run Time : 7.18 sec
INFO:root:2024-03-27 12:53:35, Train, Epoch : 3, Step : 1530, Loss : 0.55380, Acc : 0.697, Sensitive_Loss : 0.06437, Sensitive_Acc : 0.466, Run Time : 7.49 sec
INFO:root:2024-03-27 12:53:42, Train, Epoch : 3, Step : 1540, Loss : 0.45937, Acc : 0.762, Sensitive_Loss : 0.08151, Sensitive_Acc : 0.484, Run Time : 7.41 sec
INFO:root:2024-03-27 12:53:50, Train, Epoch : 3, Step : 1550, Loss : 0.50394, Acc : 0.744, Sensitive_Loss : 0.02341, Sensitive_Acc : 0.484, Run Time : 7.27 sec
INFO:root:2024-03-27 12:53:57, Train, Epoch : 3, Step : 1560, Loss : 0.43748, Acc : 0.812, Sensitive_Loss : 0.05979, Sensitive_Acc : 0.478, Run Time : 7.24 sec
INFO:root:2024-03-27 12:54:04, Train, Epoch : 3, Step : 1570, Loss : 0.51054, Acc : 0.744, Sensitive_Loss : 0.05514, Sensitive_Acc : 0.459, Run Time : 7.06 sec
INFO:root:2024-03-27 12:54:11, Train, Epoch : 3, Step : 1580, Loss : 0.50328, Acc : 0.753, Sensitive_Loss : 0.09515, Sensitive_Acc : 0.475, Run Time : 7.32 sec
INFO:root:2024-03-27 12:54:19, Train, Epoch : 3, Step : 1590, Loss : 0.55775, Acc : 0.731, Sensitive_Loss : 0.05341, Sensitive_Acc : 0.466, Run Time : 7.54 sec
INFO:root:2024-03-27 12:54:25, Train, Epoch : 3, Step : 1600, Loss : 0.50146, Acc : 0.753, Sensitive_Loss : 0.04054, Sensitive_Acc : 0.453, Run Time : 6.80 sec
INFO:root:2024-03-27 12:56:30, Dev, Step : 1600, Loss : 0.55092, Acc : 0.736, Auc : 0.817, Sensitive_Loss : 0.08985, Sensitive_Acc : 0.520, Sensitive_Auc : 0.997, Mean auc: 0.817, Run Time : 124.26 sec
INFO:root:2024-03-27 12:56:30, Best, Step : 1600, Loss : 0.55092, Acc : 0.736, Auc : 0.817, Sensitive_Loss : 0.08985, Sensitive_Acc : 0.520, Sensitive_Auc : 0.997, Best Auc : 0.817
INFO:root:2024-03-27 12:56:36, Train, Epoch : 3, Step : 1610, Loss : 0.48107, Acc : 0.803, Sensitive_Loss : 0.05556, Sensitive_Acc : 0.472, Run Time : 130.31 sec
INFO:root:2024-03-27 12:56:43, Train, Epoch : 3, Step : 1620, Loss : 0.50421, Acc : 0.775, Sensitive_Loss : 0.07273, Sensitive_Acc : 0.497, Run Time : 7.55 sec
INFO:root:2024-03-27 12:56:51, Train, Epoch : 3, Step : 1630, Loss : 0.46191, Acc : 0.794, Sensitive_Loss : 0.06289, Sensitive_Acc : 0.450, Run Time : 7.33 sec
INFO:root:2024-03-27 12:56:58, Train, Epoch : 3, Step : 1640, Loss : 0.46791, Acc : 0.744, Sensitive_Loss : 0.06852, Sensitive_Acc : 0.472, Run Time : 7.53 sec
INFO:root:2024-03-27 12:57:05, Train, Epoch : 3, Step : 1650, Loss : 0.52407, Acc : 0.750, Sensitive_Loss : 0.05541, Sensitive_Acc : 0.469, Run Time : 7.21 sec
INFO:root:2024-03-27 12:57:13, Train, Epoch : 3, Step : 1660, Loss : 0.56922, Acc : 0.744, Sensitive_Loss : 0.06277, Sensitive_Acc : 0.481, Run Time : 7.95 sec
INFO:root:2024-03-27 12:57:21, Train, Epoch : 3, Step : 1670, Loss : 0.45544, Acc : 0.750, Sensitive_Loss : 0.08775, Sensitive_Acc : 0.531, Run Time : 7.51 sec
INFO:root:2024-03-27 12:57:27, Train, Epoch : 3, Step : 1680, Loss : 0.46588, Acc : 0.791, Sensitive_Loss : 0.05962, Sensitive_Acc : 0.500, Run Time : 6.50 sec
INFO:root:2024-03-27 12:57:35, Train, Epoch : 3, Step : 1690, Loss : 0.51704, Acc : 0.744, Sensitive_Loss : 0.05004, Sensitive_Acc : 0.512, Run Time : 7.15 sec
INFO:root:2024-03-27 12:57:42, Train, Epoch : 3, Step : 1700, Loss : 0.52517, Acc : 0.747, Sensitive_Loss : 0.04803, Sensitive_Acc : 0.506, Run Time : 7.08 sec
INFO:root:2024-03-27 12:59:45, Dev, Step : 1700, Loss : 0.54000, Acc : 0.745, Auc : 0.820, Sensitive_Loss : 0.10697, Sensitive_Acc : 0.496, Sensitive_Auc : 0.997, Mean auc: 0.820, Run Time : 123.37 sec
INFO:root:2024-03-27 12:59:46, Best, Step : 1700, Loss : 0.54000, Acc : 0.745, Auc : 0.820, Sensitive_Loss : 0.10697, Sensitive_Acc : 0.496, Sensitive_Auc : 0.997, Best Auc : 0.820
INFO:root:2024-03-27 12:59:51, Train, Epoch : 3, Step : 1710, Loss : 0.48334, Acc : 0.756, Sensitive_Loss : 0.05495, Sensitive_Acc : 0.516, Run Time : 129.59 sec
INFO:root:2024-03-27 12:59:59, Train, Epoch : 3, Step : 1720, Loss : 0.52874, Acc : 0.734, Sensitive_Loss : 0.04076, Sensitive_Acc : 0.434, Run Time : 7.67 sec
INFO:root:2024-03-27 13:00:06, Train, Epoch : 3, Step : 1730, Loss : 0.46011, Acc : 0.778, Sensitive_Loss : 0.06150, Sensitive_Acc : 0.459, Run Time : 6.73 sec
INFO:root:2024-03-27 13:00:13, Train, Epoch : 3, Step : 1740, Loss : 0.46670, Acc : 0.762, Sensitive_Loss : 0.03622, Sensitive_Acc : 0.450, Run Time : 7.24 sec
INFO:root:2024-03-27 13:00:21, Train, Epoch : 3, Step : 1750, Loss : 0.49888, Acc : 0.784, Sensitive_Loss : 0.04820, Sensitive_Acc : 0.487, Run Time : 7.70 sec
INFO:root:2024-03-27 13:00:28, Train, Epoch : 3, Step : 1760, Loss : 0.55602, Acc : 0.697, Sensitive_Loss : 0.05151, Sensitive_Acc : 0.472, Run Time : 7.12 sec
INFO:root:2024-03-27 13:00:35, Train, Epoch : 3, Step : 1770, Loss : 0.45436, Acc : 0.781, Sensitive_Loss : 0.04159, Sensitive_Acc : 0.481, Run Time : 7.38 sec
INFO:root:2024-03-27 13:00:42, Train, Epoch : 3, Step : 1780, Loss : 0.49759, Acc : 0.762, Sensitive_Loss : 0.05621, Sensitive_Acc : 0.494, Run Time : 7.26 sec
INFO:root:2024-03-27 13:00:50, Train, Epoch : 3, Step : 1790, Loss : 0.55326, Acc : 0.775, Sensitive_Loss : 0.03710, Sensitive_Acc : 0.469, Run Time : 7.27 sec
INFO:root:2024-03-27 13:00:58, Train, Epoch : 3, Step : 1800, Loss : 0.50362, Acc : 0.766, Sensitive_Loss : 0.08715, Sensitive_Acc : 0.466, Run Time : 7.99 sec
INFO:root:2024-03-27 13:03:06, Dev, Step : 1800, Loss : 0.53934, Acc : 0.745, Auc : 0.821, Sensitive_Loss : 0.09885, Sensitive_Acc : 0.503, Sensitive_Auc : 0.997, Mean auc: 0.821, Run Time : 128.76 sec
INFO:root:2024-03-27 13:03:07, Best, Step : 1800, Loss : 0.53934, Acc : 0.745, Auc : 0.821, Sensitive_Loss : 0.09885, Sensitive_Acc : 0.503, Sensitive_Auc : 0.997, Best Auc : 0.821
INFO:root:2024-03-27 13:03:13, Train, Epoch : 3, Step : 1810, Loss : 0.46503, Acc : 0.797, Sensitive_Loss : 0.03851, Sensitive_Acc : 0.481, Run Time : 135.37 sec
INFO:root:2024-03-27 13:03:24, Train, Epoch : 3, Step : 1820, Loss : 0.51075, Acc : 0.769, Sensitive_Loss : 0.08902, Sensitive_Acc : 0.478, Run Time : 11.41 sec
INFO:root:2024-03-27 13:03:32, Train, Epoch : 3, Step : 1830, Loss : 0.47736, Acc : 0.725, Sensitive_Loss : 0.05718, Sensitive_Acc : 0.450, Run Time : 7.26 sec
INFO:root:2024-03-27 13:03:39, Train, Epoch : 3, Step : 1840, Loss : 0.49721, Acc : 0.747, Sensitive_Loss : 0.04562, Sensitive_Acc : 0.509, Run Time : 7.23 sec
INFO:root:2024-03-27 13:04:58, Train, Epoch : 3, Step : 1850, Loss : 0.48630, Acc : 0.766, Sensitive_Loss : 0.10011, Sensitive_Acc : 0.453, Run Time : 79.59 sec
INFO:root:2024-03-27 13:05:20, Train, Epoch : 3, Step : 1860, Loss : 0.52687, Acc : 0.759, Sensitive_Loss : 0.04184, Sensitive_Acc : 0.469, Run Time : 21.84 sec
INFO:root:2024-03-27 13:05:29, Train, Epoch : 3, Step : 1870, Loss : 0.48519, Acc : 0.766, Sensitive_Loss : 0.05958, Sensitive_Acc : 0.438, Run Time : 9.12 sec
INFO:root:2024-03-27 13:05:37, Train, Epoch : 3, Step : 1880, Loss : 0.53127, Acc : 0.747, Sensitive_Loss : 0.07908, Sensitive_Acc : 0.469, Run Time : 7.18 sec
INFO:root:2024-03-27 13:05:44, Train, Epoch : 3, Step : 1890, Loss : 0.49308, Acc : 0.762, Sensitive_Loss : 0.03574, Sensitive_Acc : 0.456, Run Time : 7.31 sec
INFO:root:2024-03-27 13:05:51, Train, Epoch : 3, Step : 1900, Loss : 0.41753, Acc : 0.741, Sensitive_Loss : 0.05318, Sensitive_Acc : 0.484, Run Time : 7.53 sec
INFO:root:2024-03-27 13:07:55, Dev, Step : 1900, Loss : 0.55590, Acc : 0.733, Auc : 0.821, Sensitive_Loss : 0.08374, Sensitive_Acc : 0.481, Sensitive_Auc : 0.997, Mean auc: 0.821, Run Time : 123.43 sec
INFO:root:2024-03-27 13:07:56, Best, Step : 1900, Loss : 0.55590, Acc : 0.733, Auc : 0.821, Sensitive_Loss : 0.08374, Sensitive_Acc : 0.481, Sensitive_Auc : 0.997, Best Auc : 0.821
INFO:root:2024-03-27 13:08:02, Train, Epoch : 3, Step : 1910, Loss : 0.51472, Acc : 0.769, Sensitive_Loss : 0.06660, Sensitive_Acc : 0.428, Run Time : 130.44 sec
INFO:root:2024-03-27 13:08:09, Train, Epoch : 3, Step : 1920, Loss : 0.46615, Acc : 0.750, Sensitive_Loss : 0.06663, Sensitive_Acc : 0.466, Run Time : 6.85 sec
INFO:root:2024-03-27 13:08:17, Train, Epoch : 3, Step : 1930, Loss : 0.48550, Acc : 0.753, Sensitive_Loss : 0.07578, Sensitive_Acc : 0.466, Run Time : 8.20 sec
INFO:root:2024-03-27 13:08:24, Train, Epoch : 3, Step : 1940, Loss : 0.51651, Acc : 0.772, Sensitive_Loss : 0.05583, Sensitive_Acc : 0.487, Run Time : 6.79 sec
INFO:root:2024-03-27 13:08:32, Train, Epoch : 3, Step : 1950, Loss : 0.51243, Acc : 0.731, Sensitive_Loss : 0.07448, Sensitive_Acc : 0.484, Run Time : 7.89 sec
INFO:root:2024-03-27 13:08:39, Train, Epoch : 3, Step : 1960, Loss : 0.51665, Acc : 0.719, Sensitive_Loss : 0.04359, Sensitive_Acc : 0.466, Run Time : 7.11 sec
INFO:root:2024-03-27 13:08:46, Train, Epoch : 3, Step : 1970, Loss : 0.50431, Acc : 0.725, Sensitive_Loss : 0.06364, Sensitive_Acc : 0.500, Run Time : 6.94 sec
INFO:root:2024-03-27 13:08:53, Train, Epoch : 3, Step : 1980, Loss : 0.45021, Acc : 0.781, Sensitive_Loss : 0.06359, Sensitive_Acc : 0.516, Run Time : 6.90 sec
INFO:root:2024-03-27 13:09:00, Train, Epoch : 3, Step : 1990, Loss : 0.49011, Acc : 0.778, Sensitive_Loss : 0.02586, Sensitive_Acc : 0.506, Run Time : 7.44 sec
INFO:root:2024-03-27 13:09:07, Train, Epoch : 3, Step : 2000, Loss : 0.49390, Acc : 0.791, Sensitive_Loss : 0.09320, Sensitive_Acc : 0.478, Run Time : 7.23 sec
INFO:root:2024-03-27 13:11:11, Dev, Step : 2000, Loss : 0.54135, Acc : 0.742, Auc : 0.822, Sensitive_Loss : 0.10286, Sensitive_Acc : 0.492, Sensitive_Auc : 0.997, Mean auc: 0.822, Run Time : 124.14 sec
INFO:root:2024-03-27 13:11:12, Best, Step : 2000, Loss : 0.54135, Acc : 0.742, Auc : 0.822, Sensitive_Loss : 0.10286, Sensitive_Acc : 0.492, Sensitive_Auc : 0.997, Best Auc : 0.822
INFO:root:2024-03-27 13:11:17, Train, Epoch : 3, Step : 2010, Loss : 0.48767, Acc : 0.784, Sensitive_Loss : 0.04981, Sensitive_Acc : 0.481, Run Time : 130.21 sec
INFO:root:2024-03-27 13:11:25, Train, Epoch : 3, Step : 2020, Loss : 0.51975, Acc : 0.744, Sensitive_Loss : 0.04567, Sensitive_Acc : 0.481, Run Time : 7.40 sec
INFO:root:2024-03-27 13:11:33, Train, Epoch : 3, Step : 2030, Loss : 0.42467, Acc : 0.806, Sensitive_Loss : 0.08826, Sensitive_Acc : 0.453, Run Time : 7.83 sec
INFO:root:2024-03-27 13:11:40, Train, Epoch : 3, Step : 2040, Loss : 0.48517, Acc : 0.762, Sensitive_Loss : 0.05089, Sensitive_Acc : 0.512, Run Time : 7.38 sec
INFO:root:2024-03-27 13:11:47, Train, Epoch : 3, Step : 2050, Loss : 0.49438, Acc : 0.759, Sensitive_Loss : 0.05468, Sensitive_Acc : 0.506, Run Time : 6.80 sec
INFO:root:2024-03-27 13:11:54, Train, Epoch : 3, Step : 2060, Loss : 0.41058, Acc : 0.787, Sensitive_Loss : 0.06046, Sensitive_Acc : 0.478, Run Time : 7.25 sec
INFO:root:2024-03-27 13:12:02, Train, Epoch : 3, Step : 2070, Loss : 0.40243, Acc : 0.806, Sensitive_Loss : 0.03857, Sensitive_Acc : 0.481, Run Time : 7.40 sec
INFO:root:2024-03-27 13:12:09, Train, Epoch : 3, Step : 2080, Loss : 0.47197, Acc : 0.784, Sensitive_Loss : 0.05736, Sensitive_Acc : 0.491, Run Time : 7.33 sec
INFO:root:2024-03-27 13:12:16, Train, Epoch : 3, Step : 2090, Loss : 0.42111, Acc : 0.791, Sensitive_Loss : 0.08424, Sensitive_Acc : 0.531, Run Time : 7.59 sec
INFO:root:2024-03-27 13:12:24, Train, Epoch : 3, Step : 2100, Loss : 0.44589, Acc : 0.787, Sensitive_Loss : 0.03346, Sensitive_Acc : 0.494, Run Time : 7.24 sec
INFO:root:2024-03-27 13:14:29, Dev, Step : 2100, Loss : 0.54715, Acc : 0.741, Auc : 0.823, Sensitive_Loss : 0.07372, Sensitive_Acc : 0.489, Sensitive_Auc : 0.997, Mean auc: 0.823, Run Time : 125.37 sec
INFO:root:2024-03-27 13:14:30, Best, Step : 2100, Loss : 0.54715, Acc : 0.741, Auc : 0.823, Sensitive_Loss : 0.07372, Sensitive_Acc : 0.489, Sensitive_Auc : 0.997, Best Auc : 0.823
INFO:root:2024-03-27 13:14:36, Train, Epoch : 3, Step : 2110, Loss : 0.48925, Acc : 0.753, Sensitive_Loss : 0.04606, Sensitive_Acc : 0.491, Run Time : 131.86 sec
INFO:root:2024-03-27 13:14:43, Train, Epoch : 3, Step : 2120, Loss : 0.51467, Acc : 0.703, Sensitive_Loss : 0.06062, Sensitive_Acc : 0.478, Run Time : 7.44 sec
INFO:root:2024-03-27 13:14:50, Train, Epoch : 3, Step : 2130, Loss : 0.54228, Acc : 0.750, Sensitive_Loss : 0.04182, Sensitive_Acc : 0.469, Run Time : 7.26 sec
INFO:root:2024-03-27 13:14:58, Train, Epoch : 3, Step : 2140, Loss : 0.43832, Acc : 0.756, Sensitive_Loss : 0.04594, Sensitive_Acc : 0.475, Run Time : 7.89 sec
INFO:root:2024-03-27 13:15:05, Train, Epoch : 3, Step : 2150, Loss : 0.48915, Acc : 0.784, Sensitive_Loss : 0.04076, Sensitive_Acc : 0.463, Run Time : 7.04 sec
INFO:root:2024-03-27 13:15:13, Train, Epoch : 3, Step : 2160, Loss : 0.40419, Acc : 0.806, Sensitive_Loss : 0.04525, Sensitive_Acc : 0.494, Run Time : 7.64 sec
INFO:root:2024-03-27 13:15:20, Train, Epoch : 3, Step : 2170, Loss : 0.47056, Acc : 0.759, Sensitive_Loss : 0.10464, Sensitive_Acc : 0.500, Run Time : 7.28 sec
INFO:root:2024-03-27 13:15:27, Train, Epoch : 3, Step : 2180, Loss : 0.53216, Acc : 0.738, Sensitive_Loss : 0.04793, Sensitive_Acc : 0.466, Run Time : 6.73 sec
INFO:root:2024-03-27 13:15:34, Train, Epoch : 3, Step : 2190, Loss : 0.50853, Acc : 0.756, Sensitive_Loss : 0.04920, Sensitive_Acc : 0.506, Run Time : 7.10 sec
INFO:root:2024-03-27 13:15:42, Train, Epoch : 3, Step : 2200, Loss : 0.48515, Acc : 0.750, Sensitive_Loss : 0.05668, Sensitive_Acc : 0.472, Run Time : 8.00 sec
INFO:root:2024-03-27 13:17:47, Dev, Step : 2200, Loss : 0.52748, Acc : 0.755, Auc : 0.825, Sensitive_Loss : 0.07205, Sensitive_Acc : 0.501, Sensitive_Auc : 0.997, Mean auc: 0.825, Run Time : 124.84 sec
INFO:root:2024-03-27 13:17:47, Best, Step : 2200, Loss : 0.52748, Acc : 0.755, Auc : 0.825, Sensitive_Loss : 0.07205, Sensitive_Acc : 0.501, Sensitive_Auc : 0.997, Best Auc : 0.825
INFO:root:2024-03-27 13:17:53, Train, Epoch : 3, Step : 2210, Loss : 0.48077, Acc : 0.747, Sensitive_Loss : 0.03712, Sensitive_Acc : 0.497, Run Time : 130.92 sec
INFO:root:2024-03-27 13:18:00, Train, Epoch : 3, Step : 2220, Loss : 0.46901, Acc : 0.759, Sensitive_Loss : 0.08071, Sensitive_Acc : 0.450, Run Time : 7.27 sec
INFO:root:2024-03-27 13:18:08, Train, Epoch : 3, Step : 2230, Loss : 0.41443, Acc : 0.781, Sensitive_Loss : 0.05055, Sensitive_Acc : 0.475, Run Time : 7.81 sec
INFO:root:2024-03-27 13:18:15, Train, Epoch : 3, Step : 2240, Loss : 0.51836, Acc : 0.769, Sensitive_Loss : 0.04336, Sensitive_Acc : 0.478, Run Time : 7.02 sec
INFO:root:2024-03-27 13:18:21, Train, Epoch : 3, Step : 2250, Loss : 0.44952, Acc : 0.756, Sensitive_Loss : 0.09046, Sensitive_Acc : 0.450, Run Time : 6.50 sec
INFO:root:2024-03-27 13:20:24
INFO:root:y_pred: [0.24796145 0.5699508  0.92955035 ... 0.8580818  0.64916223 0.69320863]
INFO:root:y_true: [1. 1. 1. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.05004621e-01 5.33173919e-01 1.67324333e-06 9.99861598e-01
 1.27684430e-03 9.99912500e-01 9.99977469e-01 1.36971544e-03
 1.10473076e-03 9.99999166e-01 9.99833703e-01 8.88678769e-04
 6.13415800e-03 1.09305494e-01 3.69285629e-03 6.80232071e-04
 2.88646389e-02 8.29561427e-03 8.24517247e-06 1.49424514e-02
 9.99992251e-01 1.32180247e-02 1.01252394e-04 2.67863311e-02
 6.46208167e-01 7.71775782e-01 9.99990344e-01 9.73950565e-01
 8.05569440e-03 8.57937895e-03 9.48695838e-03 9.99994874e-01
 9.99910951e-01 9.70128059e-01 1.12851737e-02 9.98693764e-01
 1.37322431e-05 2.93731224e-03 9.99789178e-01 1.94317233e-02
 9.44027528e-02 1.37428127e-04 6.10170066e-01 9.99854326e-01
 9.99869347e-01 4.91458327e-01 9.99544561e-01 8.15335512e-01
 7.67080665e-01 1.05361934e-04 4.09401441e-03 3.82262771e-03
 1.91043001e-02 7.44220316e-01 9.99997854e-01 9.99992251e-01
 3.16399237e-05 3.01985233e-03 9.98971939e-01 2.44121134e-06
 9.49960900e-04 9.99997497e-01 8.94620462e-05 9.99992847e-01
 2.17164852e-05 1.81772193e-04 2.28240131e-03 3.34749976e-03
 1.57974893e-03 9.99281943e-01 9.99995470e-01 9.99958396e-01
 1.06845938e-01 9.99985456e-01 3.24326046e-02 7.02008698e-03
 9.99996424e-01 5.50191151e-04 1.00384859e-05 9.99863625e-01
 1.77885704e-02 4.21481300e-03 9.99326587e-01 1.18539952e-04
 9.93156254e-01 8.28810909e-04 1.96119989e-04 2.98058421e-05
 9.99988794e-01 9.99088645e-01 2.61080510e-04 2.58568190e-02
 9.95482802e-01 3.68876033e-03 9.99957919e-01 9.36846714e-03
 9.99959111e-01 2.38295016e-03 9.00770187e-01 2.11630744e-04
 7.81609043e-02 7.05990987e-03 6.96907006e-03 3.21139023e-02
 9.99983430e-01 1.86138259e-05 2.23088175e-01 5.25675350e-05
 5.80298621e-03 4.01009582e-02 6.66312626e-05 2.03321259e-02
 6.98339485e-04 3.37109901e-04 9.99967694e-01 3.82048165e-04
 1.42565972e-04 2.73274276e-02 9.22301970e-03 1.19367498e-03
 4.94623696e-03 9.99998093e-01 9.85439181e-01 9.99999642e-01
 1.46996900e-01 6.42597908e-04 8.22005421e-03 6.95473063e-05
 1.98640133e-04 8.89495423e-04 1.02469899e-01 8.18883300e-01
 1.52539986e-03 9.96234238e-01 2.74364254e-03 6.06598007e-03
 2.08670599e-03 9.99992967e-01 9.57350805e-02 9.99996662e-01
 3.68639920e-03 9.99953866e-01 9.99413967e-01 1.16717815e-03
 9.99989390e-01 3.93234760e-01 9.99958634e-01 9.99955654e-01
 9.99997377e-01 9.99995708e-01 4.57438916e-01 2.68951007e-05
 1.41882731e-04 6.09236122e-06 9.99667764e-01 4.66559343e-02
 9.99252260e-01 5.70933931e-02 9.96920943e-01 9.99526739e-01
 3.36524361e-04 3.55877243e-02 5.72477293e-04 9.99919772e-01
 2.03288555e-01 9.98881042e-01 9.99895811e-01 1.84165274e-05
 9.99999404e-01 1.32585512e-04 9.99719679e-01 9.97863352e-01
 1.73976424e-03 9.99968529e-01 9.99664068e-01 9.99999881e-01
 1.34689838e-03 1.36338910e-02 9.99984384e-01 6.94947362e-01
 2.00319709e-03 1.25713926e-02 1.04200415e-04 5.40778856e-04
 1.03523969e-04 9.99693751e-01 9.99237061e-01 1.95746750e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.
 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.
 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0.
 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]
INFO:root:2024-03-27 13:20:24, Dev, Step : 2250, Loss : 0.52496, Acc : 0.751, Auc : 0.824, Sensitive_Loss : 0.06677, Sensitive_Acc : 0.494, Sensitive_Auc : 0.997, Mean auc: 0.824, Run Time : 122.28 sec
INFO:root:2024-03-27 13:20:33, Train, Epoch : 4, Step : 2260, Loss : 0.42160, Acc : 0.787, Sensitive_Loss : 0.04331, Sensitive_Acc : 0.494, Run Time : 8.17 sec
INFO:root:2024-03-27 13:20:42, Train, Epoch : 4, Step : 2270, Loss : 0.46713, Acc : 0.787, Sensitive_Loss : 0.05133, Sensitive_Acc : 0.475, Run Time : 8.69 sec
INFO:root:2024-03-27 13:20:50, Train, Epoch : 4, Step : 2280, Loss : 0.46446, Acc : 0.734, Sensitive_Loss : 0.07062, Sensitive_Acc : 0.512, Run Time : 8.00 sec
INFO:root:2024-03-27 13:20:57, Train, Epoch : 4, Step : 2290, Loss : 0.41223, Acc : 0.825, Sensitive_Loss : 0.02965, Sensitive_Acc : 0.475, Run Time : 7.29 sec
INFO:root:2024-03-27 13:21:04, Train, Epoch : 4, Step : 2300, Loss : 0.50825, Acc : 0.766, Sensitive_Loss : 0.04730, Sensitive_Acc : 0.469, Run Time : 6.90 sec
INFO:root:2024-03-27 13:23:32, Dev, Step : 2300, Loss : 0.52422, Acc : 0.754, Auc : 0.825, Sensitive_Loss : 0.06077, Sensitive_Acc : 0.476, Sensitive_Auc : 0.997, Mean auc: 0.825, Run Time : 148.55 sec
INFO:root:2024-03-27 13:23:33, Best, Step : 2300, Loss : 0.52422, Acc : 0.754, Auc : 0.825, Sensitive_Loss : 0.06077, Sensitive_Acc : 0.476, Sensitive_Auc : 0.997, Best Auc : 0.825
INFO:root:2024-03-27 13:23:38, Train, Epoch : 4, Step : 2310, Loss : 0.47474, Acc : 0.791, Sensitive_Loss : 0.03169, Sensitive_Acc : 0.453, Run Time : 154.58 sec
INFO:root:2024-03-27 13:23:46, Train, Epoch : 4, Step : 2320, Loss : 0.43389, Acc : 0.791, Sensitive_Loss : 0.05079, Sensitive_Acc : 0.466, Run Time : 7.51 sec
INFO:root:2024-03-27 13:23:54, Train, Epoch : 4, Step : 2330, Loss : 0.45021, Acc : 0.753, Sensitive_Loss : 0.03314, Sensitive_Acc : 0.487, Run Time : 7.85 sec
INFO:root:2024-03-27 13:24:01, Train, Epoch : 4, Step : 2340, Loss : 0.50644, Acc : 0.713, Sensitive_Loss : 0.03811, Sensitive_Acc : 0.466, Run Time : 6.95 sec
INFO:root:2024-03-27 13:24:08, Train, Epoch : 4, Step : 2350, Loss : 0.44611, Acc : 0.816, Sensitive_Loss : 0.04672, Sensitive_Acc : 0.491, Run Time : 6.94 sec
INFO:root:2024-03-27 13:24:15, Train, Epoch : 4, Step : 2360, Loss : 0.53113, Acc : 0.759, Sensitive_Loss : 0.03206, Sensitive_Acc : 0.481, Run Time : 7.32 sec
INFO:root:2024-03-27 13:24:22, Train, Epoch : 4, Step : 2370, Loss : 0.53634, Acc : 0.766, Sensitive_Loss : 0.04477, Sensitive_Acc : 0.506, Run Time : 7.37 sec
INFO:root:2024-03-27 13:24:29, Train, Epoch : 4, Step : 2380, Loss : 0.48682, Acc : 0.781, Sensitive_Loss : 0.05023, Sensitive_Acc : 0.441, Run Time : 7.27 sec
INFO:root:2024-03-27 13:24:37, Train, Epoch : 4, Step : 2390, Loss : 0.44641, Acc : 0.806, Sensitive_Loss : 0.04563, Sensitive_Acc : 0.481, Run Time : 7.33 sec
INFO:root:2024-03-27 13:24:44, Train, Epoch : 4, Step : 2400, Loss : 0.46056, Acc : 0.778, Sensitive_Loss : 0.09176, Sensitive_Acc : 0.428, Run Time : 7.39 sec
INFO:root:2024-03-27 13:26:48, Dev, Step : 2400, Loss : 0.53311, Acc : 0.750, Auc : 0.826, Sensitive_Loss : 0.05798, Sensitive_Acc : 0.482, Sensitive_Auc : 0.997, Mean auc: 0.826, Run Time : 123.51 sec
INFO:root:2024-03-27 13:26:48, Best, Step : 2400, Loss : 0.53311, Acc : 0.750, Auc : 0.826, Sensitive_Loss : 0.05798, Sensitive_Acc : 0.482, Sensitive_Auc : 0.997, Best Auc : 0.826
INFO:root:2024-03-27 13:26:54, Train, Epoch : 4, Step : 2410, Loss : 0.49929, Acc : 0.747, Sensitive_Loss : 0.04229, Sensitive_Acc : 0.444, Run Time : 130.24 sec
INFO:root:2024-03-27 13:27:02, Train, Epoch : 4, Step : 2420, Loss : 0.52135, Acc : 0.762, Sensitive_Loss : 0.03976, Sensitive_Acc : 0.487, Run Time : 7.11 sec
INFO:root:2024-03-27 13:27:09, Train, Epoch : 4, Step : 2430, Loss : 0.42389, Acc : 0.791, Sensitive_Loss : 0.08240, Sensitive_Acc : 0.491, Run Time : 7.14 sec
INFO:root:2024-03-27 13:27:16, Train, Epoch : 4, Step : 2440, Loss : 0.52702, Acc : 0.731, Sensitive_Loss : 0.03865, Sensitive_Acc : 0.450, Run Time : 7.42 sec
INFO:root:2024-03-27 13:27:24, Train, Epoch : 4, Step : 2450, Loss : 0.47249, Acc : 0.769, Sensitive_Loss : 0.03807, Sensitive_Acc : 0.544, Run Time : 7.42 sec
INFO:root:2024-03-27 13:27:31, Train, Epoch : 4, Step : 2460, Loss : 0.52859, Acc : 0.716, Sensitive_Loss : 0.04667, Sensitive_Acc : 0.459, Run Time : 7.57 sec
INFO:root:2024-03-27 13:27:38, Train, Epoch : 4, Step : 2470, Loss : 0.46689, Acc : 0.794, Sensitive_Loss : 0.03583, Sensitive_Acc : 0.497, Run Time : 6.93 sec
INFO:root:2024-03-27 13:27:46, Train, Epoch : 4, Step : 2480, Loss : 0.41611, Acc : 0.803, Sensitive_Loss : 0.07208, Sensitive_Acc : 0.487, Run Time : 7.68 sec
INFO:root:2024-03-27 13:27:53, Train, Epoch : 4, Step : 2490, Loss : 0.44399, Acc : 0.738, Sensitive_Loss : 0.02418, Sensitive_Acc : 0.403, Run Time : 7.17 sec
INFO:root:2024-03-27 13:28:01, Train, Epoch : 4, Step : 2500, Loss : 0.35976, Acc : 0.825, Sensitive_Loss : 0.03322, Sensitive_Acc : 0.459, Run Time : 7.67 sec
INFO:root:2024-03-27 13:30:05, Dev, Step : 2500, Loss : 0.53166, Acc : 0.749, Auc : 0.826, Sensitive_Loss : 0.07360, Sensitive_Acc : 0.481, Sensitive_Auc : 0.997, Mean auc: 0.826, Run Time : 123.98 sec
INFO:root:2024-03-27 13:30:05, Best, Step : 2500, Loss : 0.53166, Acc : 0.749, Auc : 0.826, Sensitive_Loss : 0.07360, Sensitive_Acc : 0.481, Sensitive_Auc : 0.997, Best Auc : 0.826
INFO:root:2024-03-27 13:30:11, Train, Epoch : 4, Step : 2510, Loss : 0.50634, Acc : 0.766, Sensitive_Loss : 0.03625, Sensitive_Acc : 0.463, Run Time : 130.34 sec
INFO:root:2024-03-27 13:30:18, Train, Epoch : 4, Step : 2520, Loss : 0.41921, Acc : 0.797, Sensitive_Loss : 0.05904, Sensitive_Acc : 0.475, Run Time : 7.09 sec
INFO:root:2024-03-27 13:30:26, Train, Epoch : 4, Step : 2530, Loss : 0.46501, Acc : 0.766, Sensitive_Loss : 0.04093, Sensitive_Acc : 0.506, Run Time : 7.81 sec
INFO:root:2024-03-27 13:30:33, Train, Epoch : 4, Step : 2540, Loss : 0.46525, Acc : 0.787, Sensitive_Loss : 0.03421, Sensitive_Acc : 0.481, Run Time : 6.95 sec
INFO:root:2024-03-27 13:30:40, Train, Epoch : 4, Step : 2550, Loss : 0.51741, Acc : 0.784, Sensitive_Loss : 0.06703, Sensitive_Acc : 0.481, Run Time : 7.16 sec
INFO:root:2024-03-27 13:30:47, Train, Epoch : 4, Step : 2560, Loss : 0.51498, Acc : 0.769, Sensitive_Loss : 0.04612, Sensitive_Acc : 0.541, Run Time : 6.93 sec
INFO:root:2024-03-27 13:30:54, Train, Epoch : 4, Step : 2570, Loss : 0.50132, Acc : 0.778, Sensitive_Loss : 0.05440, Sensitive_Acc : 0.503, Run Time : 7.10 sec
INFO:root:2024-03-27 13:31:02, Train, Epoch : 4, Step : 2580, Loss : 0.49413, Acc : 0.738, Sensitive_Loss : 0.02708, Sensitive_Acc : 0.500, Run Time : 7.76 sec
INFO:root:2024-03-27 13:31:09, Train, Epoch : 4, Step : 2590, Loss : 0.40850, Acc : 0.819, Sensitive_Loss : 0.06724, Sensitive_Acc : 0.441, Run Time : 7.32 sec
INFO:root:2024-03-27 13:31:16, Train, Epoch : 4, Step : 2600, Loss : 0.39856, Acc : 0.794, Sensitive_Loss : 0.05258, Sensitive_Acc : 0.450, Run Time : 7.11 sec
INFO:root:2024-03-27 13:33:21, Dev, Step : 2600, Loss : 0.54813, Acc : 0.741, Auc : 0.826, Sensitive_Loss : 0.06192, Sensitive_Acc : 0.499, Sensitive_Auc : 0.997, Mean auc: 0.826, Run Time : 124.99 sec
INFO:root:2024-03-27 13:33:27, Train, Epoch : 4, Step : 2610, Loss : 0.45690, Acc : 0.753, Sensitive_Loss : 0.04453, Sensitive_Acc : 0.484, Run Time : 130.58 sec
INFO:root:2024-03-27 13:33:34, Train, Epoch : 4, Step : 2620, Loss : 0.43295, Acc : 0.791, Sensitive_Loss : 0.05134, Sensitive_Acc : 0.494, Run Time : 7.75 sec
INFO:root:2024-03-27 13:33:42, Train, Epoch : 4, Step : 2630, Loss : 0.49144, Acc : 0.769, Sensitive_Loss : 0.03703, Sensitive_Acc : 0.441, Run Time : 7.64 sec
INFO:root:2024-03-27 13:33:49, Train, Epoch : 4, Step : 2640, Loss : 0.45724, Acc : 0.744, Sensitive_Loss : 0.05987, Sensitive_Acc : 0.547, Run Time : 6.64 sec
INFO:root:2024-03-27 13:33:56, Train, Epoch : 4, Step : 2650, Loss : 0.44112, Acc : 0.800, Sensitive_Loss : 0.04291, Sensitive_Acc : 0.475, Run Time : 7.27 sec
INFO:root:2024-03-27 13:34:04, Train, Epoch : 4, Step : 2660, Loss : 0.49433, Acc : 0.775, Sensitive_Loss : 0.06957, Sensitive_Acc : 0.519, Run Time : 7.57 sec
INFO:root:2024-03-27 13:34:11, Train, Epoch : 4, Step : 2670, Loss : 0.44803, Acc : 0.750, Sensitive_Loss : 0.03609, Sensitive_Acc : 0.494, Run Time : 7.09 sec
INFO:root:2024-03-27 13:34:18, Train, Epoch : 4, Step : 2680, Loss : 0.45985, Acc : 0.787, Sensitive_Loss : 0.03934, Sensitive_Acc : 0.491, Run Time : 7.67 sec
INFO:root:2024-03-27 13:34:25, Train, Epoch : 4, Step : 2690, Loss : 0.45856, Acc : 0.781, Sensitive_Loss : 0.03394, Sensitive_Acc : 0.472, Run Time : 6.86 sec
INFO:root:2024-03-27 13:34:33, Train, Epoch : 4, Step : 2700, Loss : 0.52321, Acc : 0.791, Sensitive_Loss : 0.02763, Sensitive_Acc : 0.491, Run Time : 7.43 sec
INFO:root:2024-03-27 13:36:36, Dev, Step : 2700, Loss : 0.54952, Acc : 0.746, Auc : 0.828, Sensitive_Loss : 0.06235, Sensitive_Acc : 0.496, Sensitive_Auc : 0.997, Mean auc: 0.828, Run Time : 123.32 sec
INFO:root:2024-03-27 13:36:37, Best, Step : 2700, Loss : 0.54952, Acc : 0.746, Auc : 0.828, Sensitive_Loss : 0.06235, Sensitive_Acc : 0.496, Sensitive_Auc : 0.997, Best Auc : 0.828
INFO:root:2024-03-27 13:36:42, Train, Epoch : 4, Step : 2710, Loss : 0.45512, Acc : 0.784, Sensitive_Loss : 0.02506, Sensitive_Acc : 0.537, Run Time : 129.67 sec
INFO:root:2024-03-27 13:36:50, Train, Epoch : 4, Step : 2720, Loss : 0.45018, Acc : 0.800, Sensitive_Loss : 0.03481, Sensitive_Acc : 0.500, Run Time : 7.61 sec
INFO:root:2024-03-27 13:36:57, Train, Epoch : 4, Step : 2730, Loss : 0.47597, Acc : 0.741, Sensitive_Loss : 0.08216, Sensitive_Acc : 0.487, Run Time : 6.62 sec
INFO:root:2024-03-27 13:37:04, Train, Epoch : 4, Step : 2740, Loss : 0.53708, Acc : 0.747, Sensitive_Loss : 0.04584, Sensitive_Acc : 0.447, Run Time : 7.56 sec
INFO:root:2024-03-27 13:37:11, Train, Epoch : 4, Step : 2750, Loss : 0.44333, Acc : 0.753, Sensitive_Loss : 0.03961, Sensitive_Acc : 0.466, Run Time : 7.33 sec
INFO:root:2024-03-27 13:37:19, Train, Epoch : 4, Step : 2760, Loss : 0.44382, Acc : 0.769, Sensitive_Loss : 0.04653, Sensitive_Acc : 0.484, Run Time : 7.71 sec
INFO:root:2024-03-27 13:37:26, Train, Epoch : 4, Step : 2770, Loss : 0.41250, Acc : 0.769, Sensitive_Loss : 0.03034, Sensitive_Acc : 0.475, Run Time : 7.22 sec
INFO:root:2024-03-27 13:37:34, Train, Epoch : 4, Step : 2780, Loss : 0.46426, Acc : 0.806, Sensitive_Loss : 0.04153, Sensitive_Acc : 0.441, Run Time : 7.83 sec
INFO:root:2024-03-27 13:37:41, Train, Epoch : 4, Step : 2790, Loss : 0.48274, Acc : 0.766, Sensitive_Loss : 0.02825, Sensitive_Acc : 0.472, Run Time : 7.13 sec
INFO:root:2024-03-27 13:37:49, Train, Epoch : 4, Step : 2800, Loss : 0.45877, Acc : 0.797, Sensitive_Loss : 0.08668, Sensitive_Acc : 0.519, Run Time : 7.42 sec
INFO:root:2024-03-27 13:39:51, Dev, Step : 2800, Loss : 0.54163, Acc : 0.749, Auc : 0.826, Sensitive_Loss : 0.07033, Sensitive_Acc : 0.520, Sensitive_Auc : 0.998, Mean auc: 0.826, Run Time : 122.50 sec
INFO:root:2024-03-27 13:39:57, Train, Epoch : 4, Step : 2810, Loss : 0.42087, Acc : 0.816, Sensitive_Loss : 0.04448, Sensitive_Acc : 0.481, Run Time : 128.13 sec
INFO:root:2024-03-27 13:40:04, Train, Epoch : 4, Step : 2820, Loss : 0.43438, Acc : 0.809, Sensitive_Loss : 0.05534, Sensitive_Acc : 0.447, Run Time : 7.58 sec
INFO:root:2024-03-27 13:40:11, Train, Epoch : 4, Step : 2830, Loss : 0.41332, Acc : 0.769, Sensitive_Loss : 0.07290, Sensitive_Acc : 0.481, Run Time : 6.68 sec
INFO:root:2024-03-27 13:40:19, Train, Epoch : 4, Step : 2840, Loss : 0.43620, Acc : 0.772, Sensitive_Loss : 0.05224, Sensitive_Acc : 0.466, Run Time : 7.64 sec
INFO:root:2024-03-27 13:40:25, Train, Epoch : 4, Step : 2850, Loss : 0.44813, Acc : 0.816, Sensitive_Loss : 0.04738, Sensitive_Acc : 0.537, Run Time : 6.67 sec
INFO:root:2024-03-27 13:40:33, Train, Epoch : 4, Step : 2860, Loss : 0.48901, Acc : 0.753, Sensitive_Loss : 0.05446, Sensitive_Acc : 0.475, Run Time : 7.45 sec
INFO:root:2024-03-27 13:40:40, Train, Epoch : 4, Step : 2870, Loss : 0.40779, Acc : 0.803, Sensitive_Loss : 0.07814, Sensitive_Acc : 0.506, Run Time : 7.45 sec
INFO:root:2024-03-27 13:40:48, Train, Epoch : 4, Step : 2880, Loss : 0.40785, Acc : 0.800, Sensitive_Loss : 0.05321, Sensitive_Acc : 0.456, Run Time : 7.34 sec
INFO:root:2024-03-27 13:40:55, Train, Epoch : 4, Step : 2890, Loss : 0.41678, Acc : 0.819, Sensitive_Loss : 0.03813, Sensitive_Acc : 0.506, Run Time : 7.44 sec
INFO:root:2024-03-27 13:41:03, Train, Epoch : 4, Step : 2900, Loss : 0.47270, Acc : 0.738, Sensitive_Loss : 0.03903, Sensitive_Acc : 0.453, Run Time : 7.61 sec
INFO:root:2024-03-27 13:43:08, Dev, Step : 2900, Loss : 0.52743, Acc : 0.757, Auc : 0.827, Sensitive_Loss : 0.06159, Sensitive_Acc : 0.462, Sensitive_Auc : 0.997, Mean auc: 0.827, Run Time : 125.63 sec
INFO:root:2024-03-27 13:43:14, Train, Epoch : 4, Step : 2910, Loss : 0.51031, Acc : 0.772, Sensitive_Loss : 0.08251, Sensitive_Acc : 0.478, Run Time : 131.31 sec
INFO:root:2024-03-27 13:43:21, Train, Epoch : 4, Step : 2920, Loss : 0.51535, Acc : 0.766, Sensitive_Loss : 0.05022, Sensitive_Acc : 0.475, Run Time : 7.42 sec
INFO:root:2024-03-27 13:43:29, Train, Epoch : 4, Step : 2930, Loss : 0.49844, Acc : 0.766, Sensitive_Loss : 0.03814, Sensitive_Acc : 0.509, Run Time : 7.55 sec
INFO:root:2024-03-27 13:43:36, Train, Epoch : 4, Step : 2940, Loss : 0.47425, Acc : 0.775, Sensitive_Loss : 0.03809, Sensitive_Acc : 0.509, Run Time : 7.21 sec
INFO:root:2024-03-27 13:43:44, Train, Epoch : 4, Step : 2950, Loss : 0.46912, Acc : 0.784, Sensitive_Loss : 0.03572, Sensitive_Acc : 0.450, Run Time : 7.32 sec
INFO:root:2024-03-27 13:43:51, Train, Epoch : 4, Step : 2960, Loss : 0.48096, Acc : 0.744, Sensitive_Loss : 0.06977, Sensitive_Acc : 0.472, Run Time : 6.99 sec
INFO:root:2024-03-27 13:43:58, Train, Epoch : 4, Step : 2970, Loss : 0.49709, Acc : 0.772, Sensitive_Loss : 0.05148, Sensitive_Acc : 0.481, Run Time : 7.36 sec
INFO:root:2024-03-27 13:44:05, Train, Epoch : 4, Step : 2980, Loss : 0.41027, Acc : 0.794, Sensitive_Loss : 0.04005, Sensitive_Acc : 0.522, Run Time : 7.12 sec
INFO:root:2024-03-27 13:44:13, Train, Epoch : 4, Step : 2990, Loss : 0.46198, Acc : 0.812, Sensitive_Loss : 0.02280, Sensitive_Acc : 0.506, Run Time : 8.02 sec
INFO:root:2024-03-27 13:44:19, Train, Epoch : 4, Step : 3000, Loss : 0.51649, Acc : 0.759, Sensitive_Loss : 0.05518, Sensitive_Acc : 0.500, Run Time : 6.11 sec
INFO:root:2024-03-27 13:46:23, Dev, Step : 3000, Loss : 0.53388, Acc : 0.753, Auc : 0.828, Sensitive_Loss : 0.07160, Sensitive_Acc : 0.487, Sensitive_Auc : 0.997, Mean auc: 0.828, Run Time : 124.15 sec
INFO:root:2024-03-27 13:46:24, Best, Step : 3000, Loss : 0.53388, Acc : 0.753, Auc : 0.828, Sensitive_Loss : 0.07160, Sensitive_Acc : 0.487, Sensitive_Auc : 0.997, Best Auc : 0.828
INFO:root:2024-03-27 13:48:32
INFO:root:y_pred: [0.17544137 0.53849846 0.9156216  ... 0.8678371  0.6308348  0.5987993 ]
INFO:root:y_true: [1. 1. 1. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.3543345e-01 3.8430029e-01 1.1832252e-06 9.9988401e-01 1.0981769e-03
 9.9994397e-01 9.9999678e-01 9.4220432e-04 7.1782421e-04 9.9999952e-01
 9.9977154e-01 7.5678539e-04 1.1110267e-02 1.0219782e-01 2.5500525e-03
 4.6517028e-04 3.4236975e-02 5.2827168e-03 5.0948074e-06 4.1300263e-02
 9.9999344e-01 1.4282276e-02 9.7083743e-05 3.8393904e-02 6.1184394e-01
 8.7983453e-01 9.9999797e-01 9.9306184e-01 1.1235070e-02 8.7523647e-03
 1.9387068e-02 9.9999654e-01 9.9996519e-01 9.8393345e-01 2.2018733e-02
 9.9920696e-01 5.2689907e-06 9.5930928e-03 9.9980217e-01 2.0439019e-02
 1.1261429e-01 1.1571371e-04 7.8285056e-01 9.9984920e-01 9.9994695e-01
 7.0244128e-01 9.9987209e-01 9.7536904e-01 8.4281266e-01 1.3313166e-04
 2.6988110e-03 9.7216060e-03 2.6064252e-02 9.1267425e-01 9.9999809e-01
 9.9999511e-01 1.7788252e-05 1.1791948e-03 9.9987960e-01 1.9232552e-06
 1.4761455e-03 9.9999774e-01 5.7901740e-05 9.9999738e-01 3.8691349e-05
 2.1553677e-04 1.0552563e-03 4.1278200e-03 7.6653896e-04 9.9980730e-01
 9.9999630e-01 9.9997020e-01 1.8772104e-01 9.9999774e-01 7.8340851e-02
 1.1469668e-02 9.9999845e-01 6.4277032e-04 7.6903134e-06 9.9995911e-01
 2.0708896e-02 2.7124793e-03 9.9841309e-01 6.5580425e-05 9.7931540e-01
 1.0369024e-03 1.4475441e-04 5.2971423e-05 9.9998653e-01 9.9861920e-01
 2.1476504e-04 1.2914885e-02 9.9802208e-01 2.9090811e-03 9.9999213e-01
 8.8176699e-03 9.9998581e-01 2.4620804e-03 9.0491217e-01 1.2323690e-04
 1.4930032e-01 5.7819537e-03 3.3388373e-03 5.1858444e-02 9.9998868e-01
 1.4021498e-05 1.9327439e-01 4.2430354e-05 2.3281612e-03 9.1062836e-02
 9.8800694e-05 1.1057353e-02 5.7424325e-04 3.8540998e-04 9.9998510e-01
 4.2034252e-04 6.7088600e-05 2.3944050e-02 1.0259849e-02 1.9733985e-03
 2.8016237e-03 9.9999917e-01 9.8686373e-01 9.9999976e-01 9.7629644e-02
 4.1633294e-04 6.2525552e-03 5.0787439e-05 2.1480312e-04 5.5493857e-04
 2.0801967e-01 9.2442113e-01 1.2968681e-03 9.9859375e-01 4.7700354e-03
 2.9532909e-03 1.3617241e-03 9.9999654e-01 7.7196419e-02 9.9999917e-01
 8.4850742e-03 9.9997985e-01 9.9940848e-01 2.3704951e-03 9.9999642e-01
 2.5269806e-01 9.9997735e-01 9.9998450e-01 9.9999905e-01 9.9999845e-01
 4.2217359e-01 5.9011541e-06 2.3129531e-04 1.2435322e-05 9.9954933e-01
 8.4434360e-02 9.9953818e-01 5.6474559e-02 9.9928868e-01 9.9937832e-01
 1.2100773e-04 4.1596480e-02 6.9850695e-04 9.9996269e-01 1.1984508e-01
 9.9980026e-01 9.9996543e-01 1.6941985e-05 9.9999988e-01 2.0373426e-04
 9.9989653e-01 9.9889416e-01 1.4890798e-03 9.9998879e-01 9.9989581e-01
 1.0000000e+00 2.3829762e-03 7.3345415e-03 9.9998426e-01 7.5927001e-01
 1.0806925e-03 1.0302436e-02 2.0469575e-04 5.2178995e-04 1.1370147e-04
 9.9986947e-01 9.9979442e-01 1.4633445e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.
 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.
 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0.
 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]
INFO:root:2024-03-27 13:48:32, Dev, Step : 3000, Loss : 0.53388, Acc : 0.753, Auc : 0.828, Sensitive_Loss : 0.07160, Sensitive_Acc : 0.487, Sensitive_Auc : 0.997, Mean auc: 0.828, Run Time : 127.65 sec
INFO:root:2024-03-27 13:48:32, Best, Step : 3000, Loss : 0.53388, Acc : 0.753,Auc : 0.828, Best Auc : 0.828, Sensitive_Loss : 0.07160, Sensitive_Acc : 0.487, Sensitive_Auc : 0.997
INFO:root:2024-03-27 13:48:42, Train, Epoch : 5, Step : 3010, Loss : 0.45580, Acc : 0.762, Sensitive_Loss : 0.03579, Sensitive_Acc : 0.422, Run Time : 8.49 sec
INFO:root:2024-03-27 13:48:50, Train, Epoch : 5, Step : 3020, Loss : 0.47751, Acc : 0.778, Sensitive_Loss : 0.02886, Sensitive_Acc : 0.463, Run Time : 7.54 sec
INFO:root:2024-03-27 13:48:57, Train, Epoch : 5, Step : 3030, Loss : 0.41249, Acc : 0.800, Sensitive_Loss : 0.02807, Sensitive_Acc : 0.516, Run Time : 7.52 sec
INFO:root:2024-03-27 13:49:05, Train, Epoch : 5, Step : 3040, Loss : 0.44435, Acc : 0.800, Sensitive_Loss : 0.04542, Sensitive_Acc : 0.512, Run Time : 7.91 sec
INFO:root:2024-03-27 13:49:13, Train, Epoch : 5, Step : 3050, Loss : 0.44398, Acc : 0.816, Sensitive_Loss : 0.04319, Sensitive_Acc : 0.481, Run Time : 7.90 sec
INFO:root:2024-03-27 13:49:21, Train, Epoch : 5, Step : 3060, Loss : 0.42887, Acc : 0.772, Sensitive_Loss : 0.04593, Sensitive_Acc : 0.463, Run Time : 7.57 sec
INFO:root:2024-03-27 13:49:28, Train, Epoch : 5, Step : 3070, Loss : 0.46037, Acc : 0.775, Sensitive_Loss : 0.06399, Sensitive_Acc : 0.459, Run Time : 7.32 sec
INFO:root:2024-03-27 13:49:36, Train, Epoch : 5, Step : 3080, Loss : 0.45700, Acc : 0.809, Sensitive_Loss : 0.02922, Sensitive_Acc : 0.412, Run Time : 7.65 sec
INFO:root:2024-03-27 13:49:44, Train, Epoch : 5, Step : 3090, Loss : 0.37858, Acc : 0.850, Sensitive_Loss : 0.05098, Sensitive_Acc : 0.487, Run Time : 8.19 sec
INFO:root:2024-03-27 13:49:51, Train, Epoch : 5, Step : 3100, Loss : 0.43171, Acc : 0.759, Sensitive_Loss : 0.03855, Sensitive_Acc : 0.491, Run Time : 7.49 sec
INFO:root:2024-03-27 13:51:54, Dev, Step : 3100, Loss : 0.53588, Acc : 0.752, Auc : 0.828, Sensitive_Loss : 0.06076, Sensitive_Acc : 0.503, Sensitive_Auc : 0.997, Mean auc: 0.828, Run Time : 123.02 sec
INFO:root:2024-03-27 13:52:00, Train, Epoch : 5, Step : 3110, Loss : 0.40692, Acc : 0.800, Sensitive_Loss : 0.05747, Sensitive_Acc : 0.459, Run Time : 128.44 sec
INFO:root:2024-03-27 13:52:08, Train, Epoch : 5, Step : 3120, Loss : 0.44284, Acc : 0.787, Sensitive_Loss : 0.04333, Sensitive_Acc : 0.487, Run Time : 7.78 sec
INFO:root:2024-03-27 13:52:15, Train, Epoch : 5, Step : 3130, Loss : 0.41981, Acc : 0.822, Sensitive_Loss : 0.01911, Sensitive_Acc : 0.481, Run Time : 7.37 sec
INFO:root:2024-03-27 13:52:24, Train, Epoch : 5, Step : 3140, Loss : 0.37132, Acc : 0.834, Sensitive_Loss : 0.03137, Sensitive_Acc : 0.519, Run Time : 9.13 sec
INFO:root:2024-03-27 13:52:31, Train, Epoch : 5, Step : 3150, Loss : 0.46113, Acc : 0.775, Sensitive_Loss : 0.03171, Sensitive_Acc : 0.416, Run Time : 6.81 sec
INFO:root:2024-03-27 13:52:38, Train, Epoch : 5, Step : 3160, Loss : 0.42850, Acc : 0.816, Sensitive_Loss : 0.04029, Sensitive_Acc : 0.494, Run Time : 7.08 sec
INFO:root:2024-03-27 13:52:45, Train, Epoch : 5, Step : 3170, Loss : 0.39002, Acc : 0.812, Sensitive_Loss : 0.04247, Sensitive_Acc : 0.438, Run Time : 7.48 sec
INFO:root:2024-03-27 13:52:53, Train, Epoch : 5, Step : 3180, Loss : 0.41898, Acc : 0.812, Sensitive_Loss : 0.04338, Sensitive_Acc : 0.456, Run Time : 7.45 sec
INFO:root:2024-03-27 13:53:00, Train, Epoch : 5, Step : 3190, Loss : 0.41857, Acc : 0.781, Sensitive_Loss : 0.04138, Sensitive_Acc : 0.431, Run Time : 7.00 sec
INFO:root:2024-03-27 13:53:07, Train, Epoch : 5, Step : 3200, Loss : 0.47198, Acc : 0.775, Sensitive_Loss : 0.03306, Sensitive_Acc : 0.509, Run Time : 7.34 sec
INFO:root:2024-03-27 13:55:13, Dev, Step : 3200, Loss : 0.56034, Acc : 0.742, Auc : 0.826, Sensitive_Loss : 0.07108, Sensitive_Acc : 0.486, Sensitive_Auc : 0.998, Mean auc: 0.826, Run Time : 125.59 sec
INFO:root:2024-03-27 13:55:19, Train, Epoch : 5, Step : 3210, Loss : 0.42802, Acc : 0.762, Sensitive_Loss : 0.04169, Sensitive_Acc : 0.459, Run Time : 131.84 sec
INFO:root:2024-03-27 13:55:26, Train, Epoch : 5, Step : 3220, Loss : 0.46627, Acc : 0.778, Sensitive_Loss : 0.05576, Sensitive_Acc : 0.503, Run Time : 6.79 sec
INFO:root:2024-03-27 13:55:33, Train, Epoch : 5, Step : 3230, Loss : 0.48233, Acc : 0.756, Sensitive_Loss : 0.05494, Sensitive_Acc : 0.475, Run Time : 7.38 sec
INFO:root:2024-03-27 13:55:41, Train, Epoch : 5, Step : 3240, Loss : 0.55505, Acc : 0.766, Sensitive_Loss : 0.04077, Sensitive_Acc : 0.500, Run Time : 7.81 sec
INFO:root:2024-03-27 13:55:49, Train, Epoch : 5, Step : 3250, Loss : 0.44723, Acc : 0.747, Sensitive_Loss : 0.03614, Sensitive_Acc : 0.522, Run Time : 7.55 sec
INFO:root:2024-03-27 13:55:56, Train, Epoch : 5, Step : 3260, Loss : 0.40003, Acc : 0.772, Sensitive_Loss : 0.02345, Sensitive_Acc : 0.512, Run Time : 7.50 sec
INFO:root:2024-03-27 13:56:04, Train, Epoch : 5, Step : 3270, Loss : 0.45982, Acc : 0.806, Sensitive_Loss : 0.03990, Sensitive_Acc : 0.487, Run Time : 7.49 sec
INFO:root:2024-03-27 13:56:11, Train, Epoch : 5, Step : 3280, Loss : 0.43041, Acc : 0.806, Sensitive_Loss : 0.04401, Sensitive_Acc : 0.475, Run Time : 7.31 sec
INFO:root:2024-03-27 13:56:19, Train, Epoch : 5, Step : 3290, Loss : 0.45933, Acc : 0.781, Sensitive_Loss : 0.03698, Sensitive_Acc : 0.428, Run Time : 7.80 sec
INFO:root:2024-03-27 13:56:26, Train, Epoch : 5, Step : 3300, Loss : 0.39244, Acc : 0.809, Sensitive_Loss : 0.02406, Sensitive_Acc : 0.456, Run Time : 7.20 sec
INFO:root:2024-03-27 13:58:29, Dev, Step : 3300, Loss : 0.53258, Acc : 0.752, Auc : 0.824, Sensitive_Loss : 0.06661, Sensitive_Acc : 0.475, Sensitive_Auc : 0.997, Mean auc: 0.824, Run Time : 123.38 sec
INFO:root:2024-03-27 13:58:35, Train, Epoch : 5, Step : 3310, Loss : 0.43537, Acc : 0.725, Sensitive_Loss : 0.03916, Sensitive_Acc : 0.516, Run Time : 129.53 sec
INFO:root:2024-03-27 13:58:43, Train, Epoch : 5, Step : 3320, Loss : 0.42669, Acc : 0.784, Sensitive_Loss : 0.05207, Sensitive_Acc : 0.500, Run Time : 7.21 sec
INFO:root:2024-03-27 13:58:50, Train, Epoch : 5, Step : 3330, Loss : 0.42476, Acc : 0.791, Sensitive_Loss : 0.03091, Sensitive_Acc : 0.537, Run Time : 7.81 sec
INFO:root:2024-03-27 13:58:58, Train, Epoch : 5, Step : 3340, Loss : 0.50971, Acc : 0.753, Sensitive_Loss : 0.04221, Sensitive_Acc : 0.512, Run Time : 7.12 sec
INFO:root:2024-03-27 13:59:05, Train, Epoch : 5, Step : 3350, Loss : 0.43827, Acc : 0.766, Sensitive_Loss : 0.02362, Sensitive_Acc : 0.503, Run Time : 7.48 sec
INFO:root:2024-03-27 13:59:12, Train, Epoch : 5, Step : 3360, Loss : 0.46998, Acc : 0.787, Sensitive_Loss : 0.03564, Sensitive_Acc : 0.512, Run Time : 7.31 sec
INFO:root:2024-03-27 13:59:20, Train, Epoch : 5, Step : 3370, Loss : 0.47363, Acc : 0.775, Sensitive_Loss : 0.04640, Sensitive_Acc : 0.528, Run Time : 7.42 sec
INFO:root:2024-03-27 13:59:28, Train, Epoch : 5, Step : 3380, Loss : 0.44028, Acc : 0.787, Sensitive_Loss : 0.04523, Sensitive_Acc : 0.484, Run Time : 8.27 sec
INFO:root:2024-03-27 13:59:35, Train, Epoch : 5, Step : 3390, Loss : 0.37590, Acc : 0.822, Sensitive_Loss : 0.03758, Sensitive_Acc : 0.537, Run Time : 7.07 sec
INFO:root:2024-03-27 13:59:42, Train, Epoch : 5, Step : 3400, Loss : 0.39064, Acc : 0.850, Sensitive_Loss : 0.02606, Sensitive_Acc : 0.472, Run Time : 7.13 sec
INFO:root:2024-03-27 14:01:46, Dev, Step : 3400, Loss : 0.53670, Acc : 0.751, Auc : 0.829, Sensitive_Loss : 0.06127, Sensitive_Acc : 0.487, Sensitive_Auc : 0.998, Mean auc: 0.829, Run Time : 123.64 sec
INFO:root:2024-03-27 14:01:47, Best, Step : 3400, Loss : 0.53670, Acc : 0.751, Auc : 0.829, Sensitive_Loss : 0.06127, Sensitive_Acc : 0.487, Sensitive_Auc : 0.998, Best Auc : 0.829
INFO:root:2024-03-27 14:01:53, Train, Epoch : 5, Step : 3410, Loss : 0.41958, Acc : 0.775, Sensitive_Loss : 0.05124, Sensitive_Acc : 0.444, Run Time : 130.50 sec
INFO:root:2024-03-27 14:02:00, Train, Epoch : 5, Step : 3420, Loss : 0.37405, Acc : 0.781, Sensitive_Loss : 0.03992, Sensitive_Acc : 0.506, Run Time : 7.46 sec
INFO:root:2024-03-27 14:02:08, Train, Epoch : 5, Step : 3430, Loss : 0.38878, Acc : 0.803, Sensitive_Loss : 0.02556, Sensitive_Acc : 0.478, Run Time : 7.41 sec
INFO:root:2024-03-27 14:02:15, Train, Epoch : 5, Step : 3440, Loss : 0.48403, Acc : 0.769, Sensitive_Loss : 0.04451, Sensitive_Acc : 0.500, Run Time : 7.57 sec
INFO:root:2024-03-27 14:02:23, Train, Epoch : 5, Step : 3450, Loss : 0.41590, Acc : 0.812, Sensitive_Loss : 0.03239, Sensitive_Acc : 0.487, Run Time : 7.98 sec
INFO:root:2024-03-27 14:02:31, Train, Epoch : 5, Step : 3460, Loss : 0.47611, Acc : 0.759, Sensitive_Loss : 0.01994, Sensitive_Acc : 0.494, Run Time : 7.35 sec
INFO:root:2024-03-27 14:02:38, Train, Epoch : 5, Step : 3470, Loss : 0.55203, Acc : 0.784, Sensitive_Loss : 0.05429, Sensitive_Acc : 0.494, Run Time : 7.28 sec
INFO:root:2024-03-27 14:02:45, Train, Epoch : 5, Step : 3480, Loss : 0.47238, Acc : 0.762, Sensitive_Loss : 0.01507, Sensitive_Acc : 0.497, Run Time : 7.57 sec
INFO:root:2024-03-27 14:02:53, Train, Epoch : 5, Step : 3490, Loss : 0.46750, Acc : 0.772, Sensitive_Loss : 0.02432, Sensitive_Acc : 0.497, Run Time : 7.36 sec
INFO:root:2024-03-27 14:03:00, Train, Epoch : 5, Step : 3500, Loss : 0.46349, Acc : 0.797, Sensitive_Loss : 0.02943, Sensitive_Acc : 0.525, Run Time : 7.08 sec
INFO:root:2024-03-27 14:05:03, Dev, Step : 3500, Loss : 0.53239, Acc : 0.753, Auc : 0.827, Sensitive_Loss : 0.07657, Sensitive_Acc : 0.473, Sensitive_Auc : 0.997, Mean auc: 0.827, Run Time : 123.61 sec
INFO:root:2024-03-27 14:05:09, Train, Epoch : 5, Step : 3510, Loss : 0.46451, Acc : 0.778, Sensitive_Loss : 0.03768, Sensitive_Acc : 0.509, Run Time : 129.38 sec
INFO:root:2024-03-27 14:05:16, Train, Epoch : 5, Step : 3520, Loss : 0.48413, Acc : 0.753, Sensitive_Loss : 0.06326, Sensitive_Acc : 0.519, Run Time : 7.11 sec
INFO:root:2024-03-27 14:05:24, Train, Epoch : 5, Step : 3530, Loss : 0.38845, Acc : 0.806, Sensitive_Loss : 0.02314, Sensitive_Acc : 0.469, Run Time : 7.52 sec
INFO:root:2024-03-27 14:05:31, Train, Epoch : 5, Step : 3540, Loss : 0.48658, Acc : 0.787, Sensitive_Loss : 0.03812, Sensitive_Acc : 0.506, Run Time : 7.36 sec
INFO:root:2024-03-27 14:05:39, Train, Epoch : 5, Step : 3550, Loss : 0.41237, Acc : 0.803, Sensitive_Loss : 0.04619, Sensitive_Acc : 0.503, Run Time : 7.80 sec
INFO:root:2024-03-27 14:05:46, Train, Epoch : 5, Step : 3560, Loss : 0.49713, Acc : 0.794, Sensitive_Loss : 0.04985, Sensitive_Acc : 0.481, Run Time : 6.85 sec
INFO:root:2024-03-27 14:05:53, Train, Epoch : 5, Step : 3570, Loss : 0.44404, Acc : 0.791, Sensitive_Loss : 0.05589, Sensitive_Acc : 0.453, Run Time : 7.62 sec
INFO:root:2024-03-27 14:06:01, Train, Epoch : 5, Step : 3580, Loss : 0.43949, Acc : 0.791, Sensitive_Loss : 0.04020, Sensitive_Acc : 0.441, Run Time : 7.84 sec
INFO:root:2024-03-27 14:06:09, Train, Epoch : 5, Step : 3590, Loss : 0.50414, Acc : 0.781, Sensitive_Loss : 0.06340, Sensitive_Acc : 0.450, Run Time : 7.38 sec
INFO:root:2024-03-27 14:06:16, Train, Epoch : 5, Step : 3600, Loss : 0.40961, Acc : 0.816, Sensitive_Loss : 0.03621, Sensitive_Acc : 0.466, Run Time : 7.10 sec
INFO:root:2024-03-27 14:08:21, Dev, Step : 3600, Loss : 0.54121, Acc : 0.748, Auc : 0.825, Sensitive_Loss : 0.06624, Sensitive_Acc : 0.450, Sensitive_Auc : 0.997, Mean auc: 0.825, Run Time : 124.95 sec
INFO:root:2024-03-27 14:08:26, Train, Epoch : 5, Step : 3610, Loss : 0.44453, Acc : 0.775, Sensitive_Loss : 0.05900, Sensitive_Acc : 0.431, Run Time : 130.59 sec
INFO:root:2024-03-27 14:08:34, Train, Epoch : 5, Step : 3620, Loss : 0.49385, Acc : 0.787, Sensitive_Loss : 0.03280, Sensitive_Acc : 0.494, Run Time : 7.42 sec
INFO:root:2024-03-27 14:08:41, Train, Epoch : 5, Step : 3630, Loss : 0.45578, Acc : 0.762, Sensitive_Loss : 0.02851, Sensitive_Acc : 0.475, Run Time : 7.54 sec
INFO:root:2024-03-27 14:08:49, Train, Epoch : 5, Step : 3640, Loss : 0.44705, Acc : 0.772, Sensitive_Loss : 0.04018, Sensitive_Acc : 0.481, Run Time : 7.46 sec
INFO:root:2024-03-27 14:08:56, Train, Epoch : 5, Step : 3650, Loss : 0.45796, Acc : 0.800, Sensitive_Loss : 0.07698, Sensitive_Acc : 0.484, Run Time : 7.25 sec
INFO:root:2024-03-27 14:09:03, Train, Epoch : 5, Step : 3660, Loss : 0.44121, Acc : 0.791, Sensitive_Loss : 0.01527, Sensitive_Acc : 0.472, Run Time : 7.38 sec
INFO:root:2024-03-27 14:09:11, Train, Epoch : 5, Step : 3670, Loss : 0.40152, Acc : 0.787, Sensitive_Loss : 0.03485, Sensitive_Acc : 0.484, Run Time : 7.87 sec
INFO:root:2024-03-27 14:09:19, Train, Epoch : 5, Step : 3680, Loss : 0.45328, Acc : 0.759, Sensitive_Loss : 0.03061, Sensitive_Acc : 0.463, Run Time : 7.94 sec
INFO:root:2024-03-27 14:09:26, Train, Epoch : 5, Step : 3690, Loss : 0.49658, Acc : 0.759, Sensitive_Loss : 0.03339, Sensitive_Acc : 0.525, Run Time : 7.02 sec
INFO:root:2024-03-27 14:09:34, Train, Epoch : 5, Step : 3700, Loss : 0.45895, Acc : 0.756, Sensitive_Loss : 0.02884, Sensitive_Acc : 0.469, Run Time : 7.91 sec
INFO:root:2024-03-27 14:11:46, Dev, Step : 3700, Loss : 0.54115, Acc : 0.749, Auc : 0.828, Sensitive_Loss : 0.06250, Sensitive_Acc : 0.495, Sensitive_Auc : 0.997, Mean auc: 0.828, Run Time : 132.26 sec
INFO:root:2024-03-27 14:11:52, Train, Epoch : 5, Step : 3710, Loss : 0.54615, Acc : 0.734, Sensitive_Loss : 0.04805, Sensitive_Acc : 0.497, Run Time : 137.78 sec
INFO:root:2024-03-27 14:12:00, Train, Epoch : 5, Step : 3720, Loss : 0.43250, Acc : 0.787, Sensitive_Loss : 0.03382, Sensitive_Acc : 0.469, Run Time : 7.82 sec
INFO:root:2024-03-27 14:12:07, Train, Epoch : 5, Step : 3730, Loss : 0.45685, Acc : 0.769, Sensitive_Loss : 0.03184, Sensitive_Acc : 0.500, Run Time : 7.43 sec
INFO:root:2024-03-27 14:12:14, Train, Epoch : 5, Step : 3740, Loss : 0.50675, Acc : 0.753, Sensitive_Loss : 0.06681, Sensitive_Acc : 0.459, Run Time : 7.13 sec
INFO:root:2024-03-27 14:12:22, Train, Epoch : 5, Step : 3750, Loss : 0.44581, Acc : 0.816, Sensitive_Loss : 0.02838, Sensitive_Acc : 0.466, Run Time : 7.32 sec
INFO:root:2024-03-27 14:14:23
INFO:root:y_pred: [0.15249895 0.47989222 0.85945463 ... 0.8055073  0.62934446 0.5919711 ]
INFO:root:y_true: [1. 1. 1. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [8.5598248e-01 3.5740793e-01 5.5453984e-07 9.9988735e-01 3.2959387e-04
 9.9990094e-01 9.9999738e-01 1.1024320e-03 8.7558699e-04 9.9999928e-01
 9.9959642e-01 5.7943800e-04 1.1575780e-02 1.4261220e-01 1.2027681e-03
 9.6895528e-05 2.4972040e-02 1.3430088e-03 1.7586316e-06 1.5590599e-02
 9.9999213e-01 9.6245212e-03 3.0449362e-05 3.3354286e-02 6.3150811e-01
 8.1590205e-01 9.9999928e-01 9.9487597e-01 1.3756236e-02 4.8251059e-03
 3.1671379e-02 9.9999297e-01 9.9998343e-01 9.7284150e-01 1.8702751e-02
 9.9960035e-01 3.5523647e-06 4.5704464e-03 9.9985635e-01 1.7283920e-02
 8.6950608e-02 2.5079698e-05 8.9625394e-01 9.9990726e-01 9.9997067e-01
 5.0745445e-01 9.9988818e-01 9.8922658e-01 6.7594922e-01 5.9354679e-05
 8.0417946e-04 4.2547747e-03 1.8525727e-02 9.0214831e-01 9.9999821e-01
 9.9999750e-01 9.2803612e-06 7.4538763e-04 9.9991846e-01 4.4901799e-07
 4.0132436e-04 9.9999571e-01 2.4319297e-05 9.9999928e-01 3.2324831e-05
 8.4361840e-05 7.9755718e-04 2.4548967e-03 1.4179474e-03 9.9987817e-01
 9.9999547e-01 9.9996758e-01 1.5686961e-01 9.9999893e-01 1.5241921e-01
 4.9817455e-03 9.9999952e-01 2.9348698e-04 2.4268454e-06 9.9998498e-01
 8.6107599e-03 1.6138447e-03 9.9865037e-01 3.3497043e-05 9.8648459e-01
 3.9413615e-04 3.6593410e-05 8.3225768e-06 9.9996221e-01 9.9922514e-01
 1.0645394e-04 9.2602838e-03 9.9969983e-01 7.5603067e-04 9.9998987e-01
 7.0221252e-03 9.9997330e-01 1.9306605e-03 9.6409422e-01 2.8650338e-05
 1.0390842e-01 5.2795028e-03 2.3378134e-03 2.5174730e-02 9.9999440e-01
 9.7628099e-06 3.5957462e-01 1.2098231e-05 1.4488664e-03 9.4520427e-02
 3.0357309e-05 5.4638665e-03 5.1674637e-04 5.9129830e-05 9.9999082e-01
 1.4946781e-04 2.7711530e-05 1.9462248e-02 5.9234127e-03 1.9971193e-03
 3.4084937e-03 9.9999928e-01 9.9230063e-01 9.9999964e-01 6.3229755e-02
 3.6027978e-04 2.6700152e-03 4.7399564e-05 3.4221986e-05 4.4534024e-04
 5.1341813e-02 9.4329476e-01 3.4971999e-03 9.9848253e-01 3.3939562e-03
 5.1613515e-03 5.3460343e-04 9.9999666e-01 3.5947122e-02 9.9999940e-01
 8.2235960e-03 9.9995995e-01 9.9948335e-01 2.6153165e-03 9.9999547e-01
 1.9645397e-01 9.9999368e-01 9.9998903e-01 9.9999928e-01 9.9999821e-01
 5.7505751e-01 4.6376827e-06 1.4809371e-04 4.7098438e-06 9.9969280e-01
 5.4742567e-02 9.9965560e-01 3.7868053e-02 9.9943763e-01 9.9916077e-01
 4.1453743e-05 4.8045252e-02 5.1506609e-04 9.9996555e-01 7.6815538e-02
 9.9990892e-01 9.9998105e-01 4.7559411e-06 9.9999988e-01 1.9774346e-04
 9.9985600e-01 9.9793375e-01 4.5535585e-04 9.9999201e-01 9.9994719e-01
 9.9999988e-01 1.9093807e-03 3.9918381e-03 9.9997807e-01 7.3879528e-01
 2.7768736e-04 3.3194355e-03 1.0529374e-04 2.7403867e-04 5.2458920e-05
 9.9994826e-01 9.9975234e-01 1.8592922e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.
 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.
 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0.
 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]
INFO:root:2024-03-27 14:14:23, Dev, Step : 3750, Loss : 0.53613, Acc : 0.751, Auc : 0.828, Sensitive_Loss : 0.07013, Sensitive_Acc : 0.501, Sensitive_Auc : 0.998, Mean auc: 0.828, Run Time : 120.79 sec
INFO:root:2024-03-27 14:14:33, Train, Epoch : 6, Step : 3760, Loss : 0.40590, Acc : 0.787, Sensitive_Loss : 0.04967, Sensitive_Acc : 0.487, Run Time : 8.48 sec
INFO:root:2024-03-27 14:14:41, Train, Epoch : 6, Step : 3770, Loss : 0.42417, Acc : 0.803, Sensitive_Loss : 0.02703, Sensitive_Acc : 0.463, Run Time : 7.23 sec
INFO:root:2024-03-27 14:14:48, Train, Epoch : 6, Step : 3780, Loss : 0.42173, Acc : 0.766, Sensitive_Loss : 0.03051, Sensitive_Acc : 0.466, Run Time : 7.18 sec
INFO:root:2024-03-27 14:14:56, Train, Epoch : 6, Step : 3790, Loss : 0.51645, Acc : 0.772, Sensitive_Loss : 0.01858, Sensitive_Acc : 0.503, Run Time : 7.90 sec
INFO:root:2024-03-27 14:15:03, Train, Epoch : 6, Step : 3800, Loss : 0.41615, Acc : 0.775, Sensitive_Loss : 0.03752, Sensitive_Acc : 0.487, Run Time : 7.81 sec
INFO:root:2024-03-27 14:17:15, Dev, Step : 3800, Loss : 0.53570, Acc : 0.749, Auc : 0.827, Sensitive_Loss : 0.05291, Sensitive_Acc : 0.481, Sensitive_Auc : 0.998, Mean auc: 0.827, Run Time : 131.19 sec
INFO:root:2024-03-27 14:17:21, Train, Epoch : 6, Step : 3810, Loss : 0.38665, Acc : 0.812, Sensitive_Loss : 0.03108, Sensitive_Acc : 0.466, Run Time : 137.02 sec
INFO:root:2024-03-27 14:17:29, Train, Epoch : 6, Step : 3820, Loss : 0.38873, Acc : 0.794, Sensitive_Loss : 0.04229, Sensitive_Acc : 0.494, Run Time : 8.36 sec
INFO:root:2024-03-27 14:17:36, Train, Epoch : 6, Step : 3830, Loss : 0.35517, Acc : 0.847, Sensitive_Loss : 0.04299, Sensitive_Acc : 0.456, Run Time : 7.46 sec
INFO:root:2024-03-27 14:17:44, Train, Epoch : 6, Step : 3840, Loss : 0.45399, Acc : 0.778, Sensitive_Loss : 0.04052, Sensitive_Acc : 0.519, Run Time : 7.62 sec
INFO:root:2024-03-27 14:17:51, Train, Epoch : 6, Step : 3850, Loss : 0.41386, Acc : 0.784, Sensitive_Loss : 0.06552, Sensitive_Acc : 0.528, Run Time : 7.36 sec
INFO:root:2024-03-27 14:17:59, Train, Epoch : 6, Step : 3860, Loss : 0.42670, Acc : 0.803, Sensitive_Loss : 0.03223, Sensitive_Acc : 0.509, Run Time : 7.24 sec
INFO:root:2024-03-27 14:18:06, Train, Epoch : 6, Step : 3870, Loss : 0.47269, Acc : 0.775, Sensitive_Loss : 0.03059, Sensitive_Acc : 0.487, Run Time : 7.73 sec
INFO:root:2024-03-27 14:18:13, Train, Epoch : 6, Step : 3880, Loss : 0.43778, Acc : 0.800, Sensitive_Loss : 0.01663, Sensitive_Acc : 0.512, Run Time : 7.19 sec
INFO:root:2024-03-27 14:18:21, Train, Epoch : 6, Step : 3890, Loss : 0.50574, Acc : 0.791, Sensitive_Loss : 0.02885, Sensitive_Acc : 0.522, Run Time : 7.39 sec
INFO:root:2024-03-27 14:18:28, Train, Epoch : 6, Step : 3900, Loss : 0.42714, Acc : 0.819, Sensitive_Loss : 0.03241, Sensitive_Acc : 0.481, Run Time : 7.52 sec
INFO:root:2024-03-27 14:20:41, Dev, Step : 3900, Loss : 0.53162, Acc : 0.752, Auc : 0.828, Sensitive_Loss : 0.06123, Sensitive_Acc : 0.468, Sensitive_Auc : 0.998, Mean auc: 0.828, Run Time : 132.67 sec
INFO:root:2024-03-27 14:20:47, Train, Epoch : 6, Step : 3910, Loss : 0.44293, Acc : 0.828, Sensitive_Loss : 0.02612, Sensitive_Acc : 0.503, Run Time : 138.14 sec
INFO:root:2024-03-27 14:20:54, Train, Epoch : 6, Step : 3920, Loss : 0.45182, Acc : 0.791, Sensitive_Loss : 0.02476, Sensitive_Acc : 0.459, Run Time : 7.73 sec
INFO:root:2024-03-27 14:21:01, Train, Epoch : 6, Step : 3930, Loss : 0.46259, Acc : 0.781, Sensitive_Loss : 0.06411, Sensitive_Acc : 0.453, Run Time : 6.97 sec
INFO:root:2024-03-27 14:21:09, Train, Epoch : 6, Step : 3940, Loss : 0.43103, Acc : 0.787, Sensitive_Loss : 0.03831, Sensitive_Acc : 0.519, Run Time : 7.95 sec
INFO:root:2024-03-27 14:21:16, Train, Epoch : 6, Step : 3950, Loss : 0.43335, Acc : 0.772, Sensitive_Loss : 0.02467, Sensitive_Acc : 0.466, Run Time : 7.13 sec
INFO:root:2024-03-27 14:21:24, Train, Epoch : 6, Step : 3960, Loss : 0.43747, Acc : 0.794, Sensitive_Loss : 0.02873, Sensitive_Acc : 0.525, Run Time : 7.45 sec
INFO:root:2024-03-27 14:21:31, Train, Epoch : 6, Step : 3970, Loss : 0.48929, Acc : 0.791, Sensitive_Loss : 0.02761, Sensitive_Acc : 0.522, Run Time : 7.05 sec
INFO:root:2024-03-27 14:21:38, Train, Epoch : 6, Step : 3980, Loss : 0.43715, Acc : 0.794, Sensitive_Loss : 0.01956, Sensitive_Acc : 0.491, Run Time : 7.46 sec
INFO:root:2024-03-27 14:21:46, Train, Epoch : 6, Step : 3990, Loss : 0.45296, Acc : 0.787, Sensitive_Loss : 0.01583, Sensitive_Acc : 0.534, Run Time : 7.77 sec
INFO:root:2024-03-27 14:21:53, Train, Epoch : 6, Step : 4000, Loss : 0.39046, Acc : 0.838, Sensitive_Loss : 0.03701, Sensitive_Acc : 0.459, Run Time : 7.43 sec
INFO:root:2024-03-27 14:24:02, Dev, Step : 4000, Loss : 0.56058, Acc : 0.739, Auc : 0.826, Sensitive_Loss : 0.07510, Sensitive_Acc : 0.478, Sensitive_Auc : 0.998, Mean auc: 0.826, Run Time : 128.99 sec
INFO:root:2024-03-27 14:24:08, Train, Epoch : 6, Step : 4010, Loss : 0.47801, Acc : 0.800, Sensitive_Loss : 0.06730, Sensitive_Acc : 0.466, Run Time : 134.51 sec
INFO:root:2024-03-27 14:24:15, Train, Epoch : 6, Step : 4020, Loss : 0.37733, Acc : 0.784, Sensitive_Loss : 0.05225, Sensitive_Acc : 0.456, Run Time : 7.35 sec
INFO:root:2024-03-27 14:24:23, Train, Epoch : 6, Step : 4030, Loss : 0.41371, Acc : 0.794, Sensitive_Loss : 0.02064, Sensitive_Acc : 0.500, Run Time : 7.88 sec
INFO:root:2024-03-27 14:24:31, Train, Epoch : 6, Step : 4040, Loss : 0.44157, Acc : 0.831, Sensitive_Loss : 0.04816, Sensitive_Acc : 0.512, Run Time : 7.45 sec
INFO:root:2024-03-27 14:24:38, Train, Epoch : 6, Step : 4050, Loss : 0.46245, Acc : 0.759, Sensitive_Loss : 0.04853, Sensitive_Acc : 0.522, Run Time : 6.95 sec
INFO:root:2024-03-27 14:24:45, Train, Epoch : 6, Step : 4060, Loss : 0.45260, Acc : 0.766, Sensitive_Loss : 0.04732, Sensitive_Acc : 0.503, Run Time : 7.19 sec
INFO:root:2024-03-27 14:24:52, Train, Epoch : 6, Step : 4070, Loss : 0.46580, Acc : 0.738, Sensitive_Loss : 0.02293, Sensitive_Acc : 0.522, Run Time : 7.01 sec
INFO:root:2024-03-27 14:25:00, Train, Epoch : 6, Step : 4080, Loss : 0.42497, Acc : 0.816, Sensitive_Loss : 0.05579, Sensitive_Acc : 0.503, Run Time : 7.76 sec
INFO:root:2024-03-27 14:25:07, Train, Epoch : 6, Step : 4090, Loss : 0.40289, Acc : 0.772, Sensitive_Loss : 0.03267, Sensitive_Acc : 0.450, Run Time : 7.23 sec
INFO:root:2024-03-27 14:25:14, Train, Epoch : 6, Step : 4100, Loss : 0.39275, Acc : 0.819, Sensitive_Loss : 0.03716, Sensitive_Acc : 0.428, Run Time : 7.21 sec
INFO:root:2024-03-27 14:27:18, Dev, Step : 4100, Loss : 0.56698, Acc : 0.741, Auc : 0.827, Sensitive_Loss : 0.08668, Sensitive_Acc : 0.496, Sensitive_Auc : 0.998, Mean auc: 0.827, Run Time : 124.42 sec
INFO:root:2024-03-27 14:27:24, Train, Epoch : 6, Step : 4110, Loss : 0.46241, Acc : 0.784, Sensitive_Loss : 0.04976, Sensitive_Acc : 0.497, Run Time : 130.07 sec
INFO:root:2024-03-27 14:27:32, Train, Epoch : 6, Step : 4120, Loss : 0.37635, Acc : 0.831, Sensitive_Loss : 0.04428, Sensitive_Acc : 0.522, Run Time : 7.67 sec
INFO:root:2024-03-27 14:27:39, Train, Epoch : 6, Step : 4130, Loss : 0.47978, Acc : 0.775, Sensitive_Loss : 0.01611, Sensitive_Acc : 0.459, Run Time : 7.10 sec
INFO:root:2024-03-27 14:27:46, Train, Epoch : 6, Step : 4140, Loss : 0.45277, Acc : 0.806, Sensitive_Loss : 0.02609, Sensitive_Acc : 0.469, Run Time : 7.58 sec
INFO:root:2024-03-27 14:27:54, Train, Epoch : 6, Step : 4150, Loss : 0.57389, Acc : 0.747, Sensitive_Loss : 0.03530, Sensitive_Acc : 0.487, Run Time : 7.61 sec
INFO:root:2024-03-27 14:28:01, Train, Epoch : 6, Step : 4160, Loss : 0.42092, Acc : 0.797, Sensitive_Loss : 0.03546, Sensitive_Acc : 0.481, Run Time : 7.37 sec
INFO:root:2024-03-27 14:28:08, Train, Epoch : 6, Step : 4170, Loss : 0.38265, Acc : 0.844, Sensitive_Loss : 0.01618, Sensitive_Acc : 0.463, Run Time : 7.02 sec
INFO:root:2024-03-27 14:28:16, Train, Epoch : 6, Step : 4180, Loss : 0.40884, Acc : 0.816, Sensitive_Loss : 0.04101, Sensitive_Acc : 0.509, Run Time : 7.24 sec
INFO:root:2024-03-27 14:28:23, Train, Epoch : 6, Step : 4190, Loss : 0.43335, Acc : 0.822, Sensitive_Loss : 0.06062, Sensitive_Acc : 0.459, Run Time : 7.00 sec
INFO:root:2024-03-27 14:28:30, Train, Epoch : 6, Step : 4200, Loss : 0.41088, Acc : 0.809, Sensitive_Loss : 0.03860, Sensitive_Acc : 0.525, Run Time : 7.83 sec
INFO:root:2024-03-27 14:30:35, Dev, Step : 4200, Loss : 0.52596, Acc : 0.752, Auc : 0.830, Sensitive_Loss : 0.05417, Sensitive_Acc : 0.480, Sensitive_Auc : 0.998, Mean auc: 0.830, Run Time : 124.76 sec
INFO:root:2024-03-27 14:30:36, Best, Step : 4200, Loss : 0.52596, Acc : 0.752, Auc : 0.830, Sensitive_Loss : 0.05417, Sensitive_Acc : 0.480, Sensitive_Auc : 0.998, Best Auc : 0.830
INFO:root:2024-03-27 14:30:42, Train, Epoch : 6, Step : 4210, Loss : 0.43641, Acc : 0.800, Sensitive_Loss : 0.05411, Sensitive_Acc : 0.472, Run Time : 131.29 sec
INFO:root:2024-03-27 14:30:49, Train, Epoch : 6, Step : 4220, Loss : 0.41707, Acc : 0.787, Sensitive_Loss : 0.04588, Sensitive_Acc : 0.509, Run Time : 7.43 sec
INFO:root:2024-03-27 14:30:57, Train, Epoch : 6, Step : 4230, Loss : 0.38730, Acc : 0.819, Sensitive_Loss : 0.04619, Sensitive_Acc : 0.472, Run Time : 7.91 sec
INFO:root:2024-03-27 14:31:04, Train, Epoch : 6, Step : 4240, Loss : 0.43402, Acc : 0.797, Sensitive_Loss : 0.03205, Sensitive_Acc : 0.459, Run Time : 6.99 sec
INFO:root:2024-03-27 14:31:11, Train, Epoch : 6, Step : 4250, Loss : 0.39838, Acc : 0.819, Sensitive_Loss : 0.01873, Sensitive_Acc : 0.459, Run Time : 7.27 sec
INFO:root:2024-03-27 14:31:19, Train, Epoch : 6, Step : 4260, Loss : 0.39472, Acc : 0.787, Sensitive_Loss : 0.04247, Sensitive_Acc : 0.475, Run Time : 7.46 sec
INFO:root:2024-03-27 14:31:26, Train, Epoch : 6, Step : 4270, Loss : 0.35764, Acc : 0.841, Sensitive_Loss : 0.03177, Sensitive_Acc : 0.459, Run Time : 7.35 sec
INFO:root:2024-03-27 14:31:34, Train, Epoch : 6, Step : 4280, Loss : 0.44174, Acc : 0.794, Sensitive_Loss : 0.04246, Sensitive_Acc : 0.469, Run Time : 7.40 sec
INFO:root:2024-03-27 14:31:42, Train, Epoch : 6, Step : 4290, Loss : 0.39209, Acc : 0.834, Sensitive_Loss : 0.04307, Sensitive_Acc : 0.444, Run Time : 8.05 sec
INFO:root:2024-03-27 14:31:49, Train, Epoch : 6, Step : 4300, Loss : 0.39173, Acc : 0.809, Sensitive_Loss : 0.02240, Sensitive_Acc : 0.500, Run Time : 7.19 sec
INFO:root:2024-03-27 14:33:53, Dev, Step : 4300, Loss : 0.53708, Acc : 0.748, Auc : 0.827, Sensitive_Loss : 0.05401, Sensitive_Acc : 0.462, Sensitive_Auc : 0.997, Mean auc: 0.827, Run Time : 124.60 sec
INFO:root:2024-03-27 14:33:59, Train, Epoch : 6, Step : 4310, Loss : 0.37603, Acc : 0.819, Sensitive_Loss : 0.03622, Sensitive_Acc : 0.487, Run Time : 130.32 sec
INFO:root:2024-03-27 14:34:07, Train, Epoch : 6, Step : 4320, Loss : 0.42332, Acc : 0.769, Sensitive_Loss : 0.02398, Sensitive_Acc : 0.475, Run Time : 7.75 sec
INFO:root:2024-03-27 14:34:14, Train, Epoch : 6, Step : 4330, Loss : 0.37446, Acc : 0.831, Sensitive_Loss : 0.04890, Sensitive_Acc : 0.428, Run Time : 7.40 sec
INFO:root:2024-03-27 14:34:21, Train, Epoch : 6, Step : 4340, Loss : 0.41018, Acc : 0.775, Sensitive_Loss : 0.03279, Sensitive_Acc : 0.491, Run Time : 7.11 sec
INFO:root:2024-03-27 14:34:29, Train, Epoch : 6, Step : 4350, Loss : 0.43929, Acc : 0.794, Sensitive_Loss : 0.03097, Sensitive_Acc : 0.469, Run Time : 7.12 sec
INFO:root:2024-03-27 14:34:36, Train, Epoch : 6, Step : 4360, Loss : 0.45137, Acc : 0.800, Sensitive_Loss : 0.02925, Sensitive_Acc : 0.466, Run Time : 7.41 sec
INFO:root:2024-03-27 14:34:43, Train, Epoch : 6, Step : 4370, Loss : 0.45888, Acc : 0.762, Sensitive_Loss : 0.02330, Sensitive_Acc : 0.494, Run Time : 7.34 sec
INFO:root:2024-03-27 14:34:51, Train, Epoch : 6, Step : 4380, Loss : 0.42820, Acc : 0.772, Sensitive_Loss : 0.02087, Sensitive_Acc : 0.519, Run Time : 7.57 sec
INFO:root:2024-03-27 14:34:59, Train, Epoch : 6, Step : 4390, Loss : 0.42209, Acc : 0.778, Sensitive_Loss : 0.02678, Sensitive_Acc : 0.469, Run Time : 7.66 sec
INFO:root:2024-03-27 14:35:06, Train, Epoch : 6, Step : 4400, Loss : 0.37597, Acc : 0.812, Sensitive_Loss : 0.03666, Sensitive_Acc : 0.506, Run Time : 7.28 sec
INFO:root:2024-03-27 14:37:14, Dev, Step : 4400, Loss : 0.54217, Acc : 0.747, Auc : 0.829, Sensitive_Loss : 0.06473, Sensitive_Acc : 0.487, Sensitive_Auc : 0.998, Mean auc: 0.829, Run Time : 128.50 sec
INFO:root:2024-03-27 14:37:19, Train, Epoch : 6, Step : 4410, Loss : 0.41874, Acc : 0.797, Sensitive_Loss : 0.02345, Sensitive_Acc : 0.509, Run Time : 133.69 sec
INFO:root:2024-03-27 14:37:27, Train, Epoch : 6, Step : 4420, Loss : 0.42904, Acc : 0.806, Sensitive_Loss : 0.03678, Sensitive_Acc : 0.500, Run Time : 7.99 sec
INFO:root:2024-03-27 14:37:35, Train, Epoch : 6, Step : 4430, Loss : 0.44500, Acc : 0.800, Sensitive_Loss : 0.04862, Sensitive_Acc : 0.456, Run Time : 7.12 sec
INFO:root:2024-03-27 14:37:42, Train, Epoch : 6, Step : 4440, Loss : 0.45364, Acc : 0.800, Sensitive_Loss : 0.03674, Sensitive_Acc : 0.497, Run Time : 7.90 sec
INFO:root:2024-03-27 14:37:50, Train, Epoch : 6, Step : 4450, Loss : 0.46879, Acc : 0.759, Sensitive_Loss : 0.03603, Sensitive_Acc : 0.491, Run Time : 7.43 sec
INFO:root:2024-03-27 14:37:57, Train, Epoch : 6, Step : 4460, Loss : 0.40073, Acc : 0.806, Sensitive_Loss : 0.03393, Sensitive_Acc : 0.472, Run Time : 7.16 sec
INFO:root:2024-03-27 14:38:04, Train, Epoch : 6, Step : 4470, Loss : 0.46078, Acc : 0.781, Sensitive_Loss : 0.02730, Sensitive_Acc : 0.494, Run Time : 7.11 sec
INFO:root:2024-03-27 14:38:11, Train, Epoch : 6, Step : 4480, Loss : 0.43606, Acc : 0.787, Sensitive_Loss : 0.02294, Sensitive_Acc : 0.528, Run Time : 7.30 sec
INFO:root:2024-03-27 14:38:19, Train, Epoch : 6, Step : 4490, Loss : 0.38216, Acc : 0.800, Sensitive_Loss : 0.06207, Sensitive_Acc : 0.481, Run Time : 7.52 sec
INFO:root:2024-03-27 14:38:26, Train, Epoch : 6, Step : 4500, Loss : 0.48528, Acc : 0.762, Sensitive_Loss : 0.01883, Sensitive_Acc : 0.463, Run Time : 7.10 sec
INFO:root:2024-03-27 14:40:38, Dev, Step : 4500, Loss : 0.55084, Acc : 0.743, Auc : 0.826, Sensitive_Loss : 0.08536, Sensitive_Acc : 0.477, Sensitive_Auc : 0.998, Mean auc: 0.826, Run Time : 131.67 sec
INFO:root:2024-03-27 14:42:51
INFO:root:y_pred: [0.1650685  0.48934773 0.8860887  ... 0.8483179  0.62034357 0.6625568 ]
INFO:root:y_true: [1. 1. 1. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [8.17789316e-01 4.17242020e-01 7.66744904e-07 9.99933004e-01
 8.08473327e-04 9.99992609e-01 9.99998569e-01 1.31451734e-03
 1.95993995e-03 9.99999881e-01 9.99974132e-01 5.51202626e-04
 1.86155606e-02 1.58640549e-01 6.78261369e-03 4.15122631e-04
 1.08662181e-01 3.77500476e-03 3.57884346e-06 1.78337488e-02
 9.99999523e-01 1.54316872e-02 5.39536377e-05 6.89635724e-02
 7.88169384e-01 7.82633901e-01 9.99999762e-01 9.97285962e-01
 1.43435951e-02 1.37889767e-02 2.36410704e-02 9.99999166e-01
 9.99991775e-01 9.97335374e-01 3.34418118e-02 9.99604166e-01
 1.85734154e-06 3.73474206e-03 9.99985576e-01 3.23802195e-02
 2.42259905e-01 2.87485109e-05 9.92229164e-01 9.99994516e-01
 9.99988079e-01 8.07425559e-01 9.99984980e-01 9.90547359e-01
 6.83322191e-01 2.84157373e-04 2.73106666e-03 1.20601524e-02
 7.72473728e-03 9.10045564e-01 9.99999762e-01 9.99999642e-01
 1.34489565e-05 2.49409024e-03 9.99986529e-01 1.10688813e-06
 9.63479397e-04 9.99999046e-01 1.37734796e-05 9.99999523e-01
 5.48461103e-05 1.58803130e-04 2.87541677e-03 6.86138403e-04
 3.61384824e-04 9.99959946e-01 9.99999404e-01 9.99998927e-01
 1.64122790e-01 9.99999642e-01 3.49000245e-01 1.31450156e-02
 9.99999762e-01 9.92431073e-04 6.84493671e-06 9.99991417e-01
 2.14689635e-02 1.59490446e-03 9.99752223e-01 1.33792390e-04
 9.97910798e-01 2.84159265e-04 2.38962166e-05 7.91741422e-06
 9.99997258e-01 9.99396801e-01 2.60129629e-04 3.95298451e-02
 9.99955654e-01 1.17237971e-03 9.99999285e-01 1.31237963e-02
 9.99994397e-01 1.58777309e-03 9.14231122e-01 4.75289307e-05
 1.61326960e-01 5.27036795e-03 1.07536605e-03 1.46876886e-01
 9.99998569e-01 1.08824679e-05 5.41968763e-01 3.52827228e-05
 2.45874049e-03 1.27167389e-01 4.37238195e-05 1.24064926e-02
 1.39131545e-04 4.19845055e-05 9.99997735e-01 3.34260083e-04
 7.73440333e-05 1.75830182e-02 8.43222439e-03 6.14640256e-03
 2.31162319e-03 9.99999881e-01 9.95217204e-01 1.00000000e+00
 2.80858845e-01 2.99635751e-04 5.96343400e-03 8.46337789e-05
 1.25943843e-04 3.26341484e-04 1.11343175e-01 9.56108809e-01
 2.88130133e-03 9.99767721e-01 1.11006312e-02 8.69418401e-03
 7.40229385e-04 9.99999523e-01 8.70818496e-02 9.99999881e-01
 3.52203241e-03 9.99991894e-01 9.99884844e-01 6.91166939e-03
 9.99999046e-01 3.69735509e-01 9.99997020e-01 9.99995947e-01
 9.99999762e-01 9.99999166e-01 6.50827706e-01 6.40735061e-06
 3.62925493e-04 1.00944926e-05 9.99985695e-01 2.68995892e-02
 9.99848127e-01 2.16539744e-02 9.99774396e-01 9.99828458e-01
 9.91889465e-05 1.47471912e-02 3.18269769e-04 9.99995589e-01
 1.77746430e-01 9.99946952e-01 9.99996305e-01 1.54956178e-05
 1.00000000e+00 2.81938090e-04 9.99987125e-01 9.99918103e-01
 1.12276035e-03 9.99999285e-01 9.99980688e-01 1.00000000e+00
 5.85510919e-04 8.52774829e-03 9.99997258e-01 7.21705019e-01
 5.15993801e-04 7.50551047e-03 3.12173244e-04 4.56602895e-04
 7.33379493e-05 9.99982238e-01 9.99910951e-01 8.06060508e-02]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.
 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.
 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0.
 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]
INFO:root:2024-03-27 14:42:51, Dev, Step : 4500, Loss : 0.55084, Acc : 0.743, Auc : 0.826, Sensitive_Loss : 0.08536, Sensitive_Acc : 0.477, Sensitive_Auc : 0.998, Mean auc: 0.826, Run Time : 133.24 sec
INFO:root:2024-03-27 14:43:03, Train, Epoch : 7, Step : 4510, Loss : 0.39595, Acc : 0.816, Sensitive_Loss : 0.03143, Sensitive_Acc : 0.459, Run Time : 8.72 sec
INFO:root:2024-03-27 14:43:10, Train, Epoch : 7, Step : 4520, Loss : 0.44790, Acc : 0.803, Sensitive_Loss : 0.03215, Sensitive_Acc : 0.478, Run Time : 7.39 sec
INFO:root:2024-03-27 14:43:17, Train, Epoch : 7, Step : 4530, Loss : 0.40343, Acc : 0.822, Sensitive_Loss : 0.06429, Sensitive_Acc : 0.481, Run Time : 7.50 sec
INFO:root:2024-03-27 14:43:26, Train, Epoch : 7, Step : 4540, Loss : 0.40501, Acc : 0.812, Sensitive_Loss : 0.03455, Sensitive_Acc : 0.503, Run Time : 8.71 sec
INFO:root:2024-03-27 14:43:34, Train, Epoch : 7, Step : 4550, Loss : 0.37849, Acc : 0.831, Sensitive_Loss : 0.02691, Sensitive_Acc : 0.453, Run Time : 7.71 sec
INFO:root:2024-03-27 14:43:41, Train, Epoch : 7, Step : 4560, Loss : 0.43382, Acc : 0.794, Sensitive_Loss : 0.01631, Sensitive_Acc : 0.487, Run Time : 7.20 sec
INFO:root:2024-03-27 14:43:48, Train, Epoch : 7, Step : 4570, Loss : 0.39170, Acc : 0.816, Sensitive_Loss : 0.03568, Sensitive_Acc : 0.509, Run Time : 7.24 sec
INFO:root:2024-03-27 14:43:55, Train, Epoch : 7, Step : 4580, Loss : 0.40168, Acc : 0.812, Sensitive_Loss : 0.02068, Sensitive_Acc : 0.475, Run Time : 6.90 sec
INFO:root:2024-03-27 14:44:03, Train, Epoch : 7, Step : 4590, Loss : 0.38312, Acc : 0.806, Sensitive_Loss : 0.03622, Sensitive_Acc : 0.450, Run Time : 7.72 sec
INFO:root:2024-03-27 14:44:10, Train, Epoch : 7, Step : 4600, Loss : 0.47627, Acc : 0.797, Sensitive_Loss : 0.01194, Sensitive_Acc : 0.487, Run Time : 7.25 sec
INFO:root:2024-03-27 14:46:20, Dev, Step : 4600, Loss : 0.53810, Acc : 0.753, Auc : 0.826, Sensitive_Loss : 0.05546, Sensitive_Acc : 0.473, Sensitive_Auc : 0.997, Mean auc: 0.826, Run Time : 129.51 sec
INFO:root:2024-03-27 14:46:26, Train, Epoch : 7, Step : 4610, Loss : 0.38983, Acc : 0.834, Sensitive_Loss : 0.02404, Sensitive_Acc : 0.509, Run Time : 135.43 sec
INFO:root:2024-03-27 14:46:33, Train, Epoch : 7, Step : 4620, Loss : 0.44266, Acc : 0.787, Sensitive_Loss : 0.04602, Sensitive_Acc : 0.475, Run Time : 7.60 sec
INFO:root:2024-03-27 14:46:40, Train, Epoch : 7, Step : 4630, Loss : 0.43995, Acc : 0.806, Sensitive_Loss : 0.04605, Sensitive_Acc : 0.463, Run Time : 7.01 sec
INFO:root:2024-03-27 14:46:48, Train, Epoch : 7, Step : 4640, Loss : 0.44965, Acc : 0.819, Sensitive_Loss : 0.02753, Sensitive_Acc : 0.472, Run Time : 7.41 sec
INFO:root:2024-03-27 14:46:55, Train, Epoch : 7, Step : 4650, Loss : 0.40851, Acc : 0.825, Sensitive_Loss : 0.02951, Sensitive_Acc : 0.500, Run Time : 7.25 sec
INFO:root:2024-03-27 14:47:03, Train, Epoch : 7, Step : 4660, Loss : 0.39611, Acc : 0.819, Sensitive_Loss : 0.04517, Sensitive_Acc : 0.481, Run Time : 7.87 sec
INFO:root:2024-03-27 14:47:09, Train, Epoch : 7, Step : 4670, Loss : 0.47641, Acc : 0.784, Sensitive_Loss : 0.03773, Sensitive_Acc : 0.456, Run Time : 6.65 sec
INFO:root:2024-03-27 14:47:17, Train, Epoch : 7, Step : 4680, Loss : 0.43873, Acc : 0.803, Sensitive_Loss : 0.03000, Sensitive_Acc : 0.478, Run Time : 7.52 sec
INFO:root:2024-03-27 14:47:24, Train, Epoch : 7, Step : 4690, Loss : 0.43601, Acc : 0.784, Sensitive_Loss : 0.02609, Sensitive_Acc : 0.450, Run Time : 7.38 sec
INFO:root:2024-03-27 14:47:32, Train, Epoch : 7, Step : 4700, Loss : 0.38667, Acc : 0.844, Sensitive_Loss : 0.02818, Sensitive_Acc : 0.453, Run Time : 7.44 sec
INFO:root:2024-03-27 14:49:45, Dev, Step : 4700, Loss : 0.53835, Acc : 0.748, Auc : 0.825, Sensitive_Loss : 0.05919, Sensitive_Acc : 0.458, Sensitive_Auc : 0.997, Mean auc: 0.825, Run Time : 133.06 sec
INFO:root:2024-03-27 14:49:50, Train, Epoch : 7, Step : 4710, Loss : 0.45101, Acc : 0.775, Sensitive_Loss : 0.02833, Sensitive_Acc : 0.500, Run Time : 138.55 sec
INFO:root:2024-03-27 14:49:58, Train, Epoch : 7, Step : 4720, Loss : 0.38257, Acc : 0.844, Sensitive_Loss : 0.03455, Sensitive_Acc : 0.497, Run Time : 7.69 sec
INFO:root:2024-03-27 14:50:05, Train, Epoch : 7, Step : 4730, Loss : 0.36865, Acc : 0.825, Sensitive_Loss : 0.01735, Sensitive_Acc : 0.463, Run Time : 7.00 sec
INFO:root:2024-03-27 14:50:14, Train, Epoch : 7, Step : 4740, Loss : 0.44711, Acc : 0.778, Sensitive_Loss : 0.03422, Sensitive_Acc : 0.494, Run Time : 8.64 sec
INFO:root:2024-03-27 14:50:21, Train, Epoch : 7, Step : 4750, Loss : 0.48578, Acc : 0.809, Sensitive_Loss : 0.05145, Sensitive_Acc : 0.487, Run Time : 7.68 sec
INFO:root:2024-03-27 14:50:29, Train, Epoch : 7, Step : 4760, Loss : 0.39513, Acc : 0.809, Sensitive_Loss : 0.03906, Sensitive_Acc : 0.472, Run Time : 7.52 sec
INFO:root:2024-03-27 14:50:36, Train, Epoch : 7, Step : 4770, Loss : 0.36638, Acc : 0.816, Sensitive_Loss : 0.05970, Sensitive_Acc : 0.450, Run Time : 7.21 sec
INFO:root:2024-03-27 14:50:45, Train, Epoch : 7, Step : 4780, Loss : 0.43134, Acc : 0.819, Sensitive_Loss : 0.03631, Sensitive_Acc : 0.469, Run Time : 8.72 sec
INFO:root:2024-03-27 14:50:53, Train, Epoch : 7, Step : 4790, Loss : 0.36378, Acc : 0.831, Sensitive_Loss : 0.04412, Sensitive_Acc : 0.487, Run Time : 8.10 sec
INFO:root:2024-03-27 14:51:00, Train, Epoch : 7, Step : 4800, Loss : 0.40760, Acc : 0.822, Sensitive_Loss : 0.03689, Sensitive_Acc : 0.434, Run Time : 7.18 sec
INFO:root:2024-03-27 14:53:11, Dev, Step : 4800, Loss : 0.56678, Acc : 0.745, Auc : 0.828, Sensitive_Loss : 0.08742, Sensitive_Acc : 0.499, Sensitive_Auc : 0.998, Mean auc: 0.828, Run Time : 130.59 sec
INFO:root:2024-03-27 14:53:16, Train, Epoch : 7, Step : 4810, Loss : 0.36497, Acc : 0.800, Sensitive_Loss : 0.06182, Sensitive_Acc : 0.472, Run Time : 136.21 sec
INFO:root:2024-03-27 14:53:23, Train, Epoch : 7, Step : 4820, Loss : 0.34898, Acc : 0.822, Sensitive_Loss : 0.04473, Sensitive_Acc : 0.494, Run Time : 7.06 sec
INFO:root:2024-03-27 14:53:31, Train, Epoch : 7, Step : 4830, Loss : 0.38725, Acc : 0.831, Sensitive_Loss : 0.02160, Sensitive_Acc : 0.472, Run Time : 7.83 sec
INFO:root:2024-03-27 14:53:39, Train, Epoch : 7, Step : 4840, Loss : 0.40840, Acc : 0.806, Sensitive_Loss : 0.04064, Sensitive_Acc : 0.475, Run Time : 7.65 sec
INFO:root:2024-03-27 14:53:46, Train, Epoch : 7, Step : 4850, Loss : 0.38767, Acc : 0.784, Sensitive_Loss : 0.01899, Sensitive_Acc : 0.528, Run Time : 7.30 sec
INFO:root:2024-03-27 14:53:54, Train, Epoch : 7, Step : 4860, Loss : 0.35448, Acc : 0.831, Sensitive_Loss : 0.03024, Sensitive_Acc : 0.500, Run Time : 7.43 sec
INFO:root:2024-03-27 14:54:01, Train, Epoch : 7, Step : 4870, Loss : 0.41969, Acc : 0.816, Sensitive_Loss : 0.03402, Sensitive_Acc : 0.450, Run Time : 7.63 sec
INFO:root:2024-03-27 14:54:08, Train, Epoch : 7, Step : 4880, Loss : 0.34661, Acc : 0.844, Sensitive_Loss : 0.02971, Sensitive_Acc : 0.491, Run Time : 7.05 sec
INFO:root:2024-03-27 14:54:16, Train, Epoch : 7, Step : 4890, Loss : 0.39481, Acc : 0.838, Sensitive_Loss : 0.06995, Sensitive_Acc : 0.472, Run Time : 7.97 sec
INFO:root:2024-03-27 14:54:25, Train, Epoch : 7, Step : 4900, Loss : 0.39945, Acc : 0.797, Sensitive_Loss : 0.03673, Sensitive_Acc : 0.434, Run Time : 8.37 sec
INFO:root:2024-03-27 14:56:35, Dev, Step : 4900, Loss : 0.53602, Acc : 0.749, Auc : 0.830, Sensitive_Loss : 0.05585, Sensitive_Acc : 0.498, Sensitive_Auc : 0.998, Mean auc: 0.830, Run Time : 130.80 sec
INFO:root:2024-03-27 14:56:41, Train, Epoch : 7, Step : 4910, Loss : 0.34194, Acc : 0.847, Sensitive_Loss : 0.02629, Sensitive_Acc : 0.428, Run Time : 136.40 sec
INFO:root:2024-03-27 14:56:49, Train, Epoch : 7, Step : 4920, Loss : 0.41849, Acc : 0.781, Sensitive_Loss : 0.04894, Sensitive_Acc : 0.528, Run Time : 7.92 sec
INFO:root:2024-03-27 14:56:56, Train, Epoch : 7, Step : 4930, Loss : 0.39397, Acc : 0.822, Sensitive_Loss : 0.02818, Sensitive_Acc : 0.516, Run Time : 7.56 sec
INFO:root:2024-03-27 14:57:04, Train, Epoch : 7, Step : 4940, Loss : 0.42340, Acc : 0.794, Sensitive_Loss : 0.03113, Sensitive_Acc : 0.447, Run Time : 7.10 sec
INFO:root:2024-03-27 14:57:11, Train, Epoch : 7, Step : 4950, Loss : 0.40257, Acc : 0.800, Sensitive_Loss : 0.01725, Sensitive_Acc : 0.475, Run Time : 7.86 sec
INFO:root:2024-03-27 14:57:19, Train, Epoch : 7, Step : 4960, Loss : 0.39892, Acc : 0.803, Sensitive_Loss : 0.01526, Sensitive_Acc : 0.466, Run Time : 7.62 sec
INFO:root:2024-03-27 14:57:27, Train, Epoch : 7, Step : 4970, Loss : 0.38700, Acc : 0.816, Sensitive_Loss : 0.02286, Sensitive_Acc : 0.463, Run Time : 8.16 sec
INFO:root:2024-03-27 14:57:35, Train, Epoch : 7, Step : 4980, Loss : 0.38932, Acc : 0.797, Sensitive_Loss : 0.04104, Sensitive_Acc : 0.494, Run Time : 7.61 sec
INFO:root:2024-03-27 14:57:42, Train, Epoch : 7, Step : 4990, Loss : 0.41467, Acc : 0.809, Sensitive_Loss : 0.02997, Sensitive_Acc : 0.519, Run Time : 7.69 sec
INFO:root:2024-03-27 14:57:50, Train, Epoch : 7, Step : 5000, Loss : 0.39326, Acc : 0.787, Sensitive_Loss : 0.02279, Sensitive_Acc : 0.506, Run Time : 7.98 sec
INFO:root:2024-03-27 15:00:02, Dev, Step : 5000, Loss : 0.57441, Acc : 0.743, Auc : 0.830, Sensitive_Loss : 0.05893, Sensitive_Acc : 0.506, Sensitive_Auc : 0.998, Mean auc: 0.830, Run Time : 131.19 sec
INFO:root:2024-03-27 15:00:02, Best, Step : 5000, Loss : 0.57441, Acc : 0.743, Auc : 0.830, Sensitive_Loss : 0.05893, Sensitive_Acc : 0.506, Sensitive_Auc : 0.998, Best Auc : 0.830
INFO:root:2024-03-27 15:00:09, Train, Epoch : 7, Step : 5010, Loss : 0.41717, Acc : 0.778, Sensitive_Loss : 0.01851, Sensitive_Acc : 0.500, Run Time : 138.36 sec
INFO:root:2024-03-27 15:00:19, Train, Epoch : 7, Step : 5020, Loss : 0.47939, Acc : 0.769, Sensitive_Loss : 0.03612, Sensitive_Acc : 0.503, Run Time : 9.81 sec
INFO:root:2024-03-27 15:00:27, Train, Epoch : 7, Step : 5030, Loss : 0.53715, Acc : 0.772, Sensitive_Loss : 0.03971, Sensitive_Acc : 0.469, Run Time : 8.60 sec
INFO:root:2024-03-27 15:00:35, Train, Epoch : 7, Step : 5040, Loss : 0.41837, Acc : 0.797, Sensitive_Loss : 0.01766, Sensitive_Acc : 0.422, Run Time : 7.48 sec
INFO:root:2024-03-27 15:00:44, Train, Epoch : 7, Step : 5050, Loss : 0.40605, Acc : 0.787, Sensitive_Loss : 0.02323, Sensitive_Acc : 0.434, Run Time : 9.35 sec
INFO:root:2024-03-27 15:00:52, Train, Epoch : 7, Step : 5060, Loss : 0.38970, Acc : 0.803, Sensitive_Loss : 0.04257, Sensitive_Acc : 0.491, Run Time : 8.43 sec
INFO:root:2024-03-27 15:01:01, Train, Epoch : 7, Step : 5070, Loss : 0.35933, Acc : 0.834, Sensitive_Loss : 0.02945, Sensitive_Acc : 0.484, Run Time : 8.62 sec
INFO:root:2024-03-27 15:01:09, Train, Epoch : 7, Step : 5080, Loss : 0.35304, Acc : 0.825, Sensitive_Loss : 0.01919, Sensitive_Acc : 0.525, Run Time : 8.08 sec
INFO:root:2024-03-27 15:01:19, Train, Epoch : 7, Step : 5090, Loss : 0.42858, Acc : 0.797, Sensitive_Loss : 0.01740, Sensitive_Acc : 0.531, Run Time : 9.75 sec
INFO:root:2024-03-27 15:01:28, Train, Epoch : 7, Step : 5100, Loss : 0.38857, Acc : 0.800, Sensitive_Loss : 0.03909, Sensitive_Acc : 0.556, Run Time : 9.08 sec
INFO:root:2024-03-27 15:03:39, Dev, Step : 5100, Loss : 0.54395, Acc : 0.748, Auc : 0.827, Sensitive_Loss : 0.05485, Sensitive_Acc : 0.509, Sensitive_Auc : 0.997, Mean auc: 0.827, Run Time : 131.00 sec
INFO:root:2024-03-27 15:03:47, Train, Epoch : 7, Step : 5110, Loss : 0.40380, Acc : 0.819, Sensitive_Loss : 0.04612, Sensitive_Acc : 0.509, Run Time : 138.59 sec
INFO:root:2024-03-27 15:03:56, Train, Epoch : 7, Step : 5120, Loss : 0.40277, Acc : 0.794, Sensitive_Loss : 0.02823, Sensitive_Acc : 0.525, Run Time : 9.01 sec
INFO:root:2024-03-27 15:04:04, Train, Epoch : 7, Step : 5130, Loss : 0.46825, Acc : 0.753, Sensitive_Loss : 0.02110, Sensitive_Acc : 0.506, Run Time : 8.41 sec
INFO:root:2024-03-27 15:04:13, Train, Epoch : 7, Step : 5140, Loss : 0.39475, Acc : 0.791, Sensitive_Loss : 0.04749, Sensitive_Acc : 0.519, Run Time : 9.30 sec
INFO:root:2024-03-27 15:04:24, Train, Epoch : 7, Step : 5150, Loss : 0.38789, Acc : 0.838, Sensitive_Loss : 0.03150, Sensitive_Acc : 0.544, Run Time : 10.36 sec
INFO:root:2024-03-27 15:04:32, Train, Epoch : 7, Step : 5160, Loss : 0.41119, Acc : 0.812, Sensitive_Loss : 0.02099, Sensitive_Acc : 0.444, Run Time : 8.53 sec
INFO:root:2024-03-27 15:04:40, Train, Epoch : 7, Step : 5170, Loss : 0.42660, Acc : 0.803, Sensitive_Loss : 0.01800, Sensitive_Acc : 0.525, Run Time : 8.00 sec
INFO:root:2024-03-27 15:04:51, Train, Epoch : 7, Step : 5180, Loss : 0.37940, Acc : 0.816, Sensitive_Loss : 0.02277, Sensitive_Acc : 0.481, Run Time : 10.89 sec
INFO:root:2024-03-27 15:05:00, Train, Epoch : 7, Step : 5190, Loss : 0.40117, Acc : 0.819, Sensitive_Loss : 0.03523, Sensitive_Acc : 0.522, Run Time : 9.14 sec
INFO:root:2024-03-27 15:05:09, Train, Epoch : 7, Step : 5200, Loss : 0.41787, Acc : 0.828, Sensitive_Loss : 0.06737, Sensitive_Acc : 0.522, Run Time : 8.76 sec
INFO:root:2024-03-27 15:07:23, Dev, Step : 5200, Loss : 0.54315, Acc : 0.747, Auc : 0.826, Sensitive_Loss : 0.06453, Sensitive_Acc : 0.495, Sensitive_Auc : 0.998, Mean auc: 0.826, Run Time : 133.92 sec
INFO:root:2024-03-27 15:07:29, Train, Epoch : 7, Step : 5210, Loss : 0.44153, Acc : 0.772, Sensitive_Loss : 0.04293, Sensitive_Acc : 0.494, Run Time : 139.95 sec
INFO:root:2024-03-27 15:07:37, Train, Epoch : 7, Step : 5220, Loss : 0.44858, Acc : 0.781, Sensitive_Loss : 0.04531, Sensitive_Acc : 0.472, Run Time : 8.49 sec
INFO:root:2024-03-27 15:07:46, Train, Epoch : 7, Step : 5230, Loss : 0.39923, Acc : 0.819, Sensitive_Loss : 0.01677, Sensitive_Acc : 0.506, Run Time : 8.75 sec
INFO:root:2024-03-27 15:07:55, Train, Epoch : 7, Step : 5240, Loss : 0.45340, Acc : 0.797, Sensitive_Loss : 0.02498, Sensitive_Acc : 0.484, Run Time : 8.82 sec
INFO:root:2024-03-27 15:08:04, Train, Epoch : 7, Step : 5250, Loss : 0.39008, Acc : 0.809, Sensitive_Loss : 0.06036, Sensitive_Acc : 0.472, Run Time : 9.07 sec
INFO:root:2024-03-27 15:10:15
INFO:root:y_pred: [0.19842461 0.37528983 0.93900627 ... 0.8789225  0.66476816 0.52145624]
INFO:root:y_true: [1. 1. 1. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [7.80906320e-01 2.10869163e-01 9.37515878e-08 9.99899030e-01
 8.24362141e-05 9.99950647e-01 9.99997973e-01 1.90591891e-04
 3.11912940e-04 9.99999762e-01 9.99621272e-01 1.66231403e-04
 7.51457410e-03 4.52700108e-02 7.41615368e-04 2.81086395e-05
 7.06568873e-03 4.58486786e-04 4.06821670e-07 4.90220124e-03
 9.99998927e-01 3.19969910e-03 1.99911301e-05 2.71928795e-02
 1.71108007e-01 5.20269096e-01 9.99999881e-01 9.95658278e-01
 9.22154286e-04 2.03479663e-03 5.50371222e-03 9.99997497e-01
 9.99977469e-01 9.77281928e-01 4.97816782e-03 9.98820245e-01
 4.91728713e-07 1.35698193e-03 9.99951839e-01 1.39579885e-02
 3.49871255e-02 5.48813341e-06 9.78410721e-01 9.99964952e-01
 9.99987841e-01 1.70312211e-01 9.99861240e-01 9.76626217e-01
 4.28355098e-01 4.34183385e-05 2.28656208e-04 1.67100725e-03
 1.58801570e-03 8.86978328e-01 9.99999404e-01 9.99997973e-01
 1.20017257e-05 4.46602440e-04 9.99895573e-01 2.89009080e-07
 3.02784581e-04 9.99997616e-01 1.82897691e-06 9.99999166e-01
 3.83050019e-06 7.88190755e-06 3.37808218e-04 3.41408275e-04
 1.02706952e-04 9.99924183e-01 9.99997735e-01 9.99985337e-01
 9.31998715e-02 9.99997854e-01 1.25213414e-01 2.30209273e-03
 9.99998689e-01 2.25669239e-04 6.49620574e-07 9.99969721e-01
 1.82731578e-03 2.09939331e-04 9.98067558e-01 2.09708742e-05
 9.79909122e-01 7.17138391e-05 5.95885467e-06 1.70590829e-06
 9.99984264e-01 9.98127520e-01 4.99551461e-05 1.38600823e-02
 9.99682307e-01 4.58376890e-04 9.99998450e-01 3.22749512e-03
 9.99966264e-01 8.28330463e-04 7.84552813e-01 1.26104933e-05
 2.06954163e-02 2.37421505e-03 3.89031949e-04 1.86938699e-02
 9.99990582e-01 3.17232025e-06 1.62292287e-01 1.78921528e-05
 6.72384514e-04 1.41913649e-02 4.68058715e-06 1.47403777e-03
 4.82512187e-05 4.97624615e-06 9.99994278e-01 4.68138751e-05
 1.81167507e-05 7.09970808e-03 9.44519183e-04 1.00519613e-03
 2.40027861e-04 9.99999762e-01 9.96188939e-01 9.99999523e-01
 5.37550002e-02 8.09621561e-05 9.66499676e-04 9.70844940e-06
 2.31801823e-05 5.21393813e-05 2.67332364e-02 8.78264010e-01
 2.13780627e-03 9.99082923e-01 4.65696631e-03 2.04741955e-03
 1.96900757e-04 9.99994874e-01 1.03632798e-02 9.99999881e-01
 7.86544988e-04 9.99977708e-01 9.99036551e-01 1.03514839e-03
 9.99998569e-01 2.72382438e-01 9.99992371e-01 9.99992371e-01
 9.99999642e-01 9.99997735e-01 4.97432202e-01 1.05540266e-06
 6.06467802e-05 1.59878664e-06 9.99874353e-01 7.97478389e-03
 9.99772727e-01 8.93844850e-03 9.99603689e-01 9.98970270e-01
 1.57527138e-05 7.35350361e-04 3.55189404e-05 9.99946356e-01
 3.57383639e-02 9.99776065e-01 9.99986649e-01 2.88149499e-06
 9.99999881e-01 3.80895908e-05 9.99940872e-01 9.99784887e-01
 2.88861367e-04 9.99998212e-01 9.99935627e-01 1.00000000e+00
 7.20280150e-05 4.26262058e-03 9.99993205e-01 4.02351916e-01
 9.16411227e-05 1.46695075e-03 3.10523355e-05 3.14223144e-05
 2.65074486e-05 9.99828219e-01 9.99750078e-01 3.20357606e-02]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.
 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.
 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0.
 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]
INFO:root:2024-03-27 15:10:15, Dev, Step : 5250, Loss : 0.54724, Acc : 0.751, Auc : 0.828, Sensitive_Loss : 0.05788, Sensitive_Acc : 0.493, Sensitive_Auc : 0.998, Mean auc: 0.828, Run Time : 130.55 sec
INFO:root:2024-03-27 15:10:25, Train, Epoch : 8, Step : 5260, Loss : 0.35728, Acc : 0.822, Sensitive_Loss : 0.04536, Sensitive_Acc : 0.466, Run Time : 9.03 sec
INFO:root:2024-03-27 15:10:33, Train, Epoch : 8, Step : 5270, Loss : 0.38919, Acc : 0.828, Sensitive_Loss : 0.03514, Sensitive_Acc : 0.531, Run Time : 8.28 sec
INFO:root:2024-03-27 15:10:41, Train, Epoch : 8, Step : 5280, Loss : 0.50125, Acc : 0.778, Sensitive_Loss : 0.02342, Sensitive_Acc : 0.487, Run Time : 8.29 sec
INFO:root:2024-03-27 15:10:53, Train, Epoch : 8, Step : 5290, Loss : 0.40767, Acc : 0.850, Sensitive_Loss : 0.02693, Sensitive_Acc : 0.438, Run Time : 11.25 sec
INFO:root:2024-03-27 15:11:02, Train, Epoch : 8, Step : 5300, Loss : 0.36034, Acc : 0.806, Sensitive_Loss : 0.02876, Sensitive_Acc : 0.466, Run Time : 9.67 sec
INFO:root:2024-03-27 15:13:15, Dev, Step : 5300, Loss : 0.54820, Acc : 0.751, Auc : 0.829, Sensitive_Loss : 0.05635, Sensitive_Acc : 0.499, Sensitive_Auc : 0.998, Mean auc: 0.829, Run Time : 132.67 sec
INFO:root:2024-03-27 15:13:23, Train, Epoch : 8, Step : 5310, Loss : 0.34500, Acc : 0.844, Sensitive_Loss : 0.06023, Sensitive_Acc : 0.478, Run Time : 140.62 sec
INFO:root:2024-03-27 15:13:31, Train, Epoch : 8, Step : 5320, Loss : 0.47806, Acc : 0.744, Sensitive_Loss : 0.03183, Sensitive_Acc : 0.484, Run Time : 8.10 sec
INFO:root:2024-03-27 15:13:40, Train, Epoch : 8, Step : 5330, Loss : 0.35289, Acc : 0.850, Sensitive_Loss : 0.03315, Sensitive_Acc : 0.478, Run Time : 9.41 sec
INFO:root:2024-03-27 15:13:51, Train, Epoch : 8, Step : 5340, Loss : 0.41956, Acc : 0.825, Sensitive_Loss : 0.02835, Sensitive_Acc : 0.509, Run Time : 10.40 sec
INFO:root:2024-03-27 15:14:00, Train, Epoch : 8, Step : 5350, Loss : 0.32634, Acc : 0.847, Sensitive_Loss : 0.01427, Sensitive_Acc : 0.506, Run Time : 8.82 sec
INFO:root:2024-03-27 15:14:09, Train, Epoch : 8, Step : 5360, Loss : 0.40311, Acc : 0.791, Sensitive_Loss : 0.02560, Sensitive_Acc : 0.556, Run Time : 9.01 sec
INFO:root:2024-03-27 15:14:18, Train, Epoch : 8, Step : 5370, Loss : 0.37431, Acc : 0.850, Sensitive_Loss : 0.03683, Sensitive_Acc : 0.497, Run Time : 9.66 sec
INFO:root:2024-03-27 15:14:27, Train, Epoch : 8, Step : 5380, Loss : 0.38834, Acc : 0.791, Sensitive_Loss : 0.04339, Sensitive_Acc : 0.528, Run Time : 8.84 sec
INFO:root:2024-03-27 15:14:36, Train, Epoch : 8, Step : 5390, Loss : 0.37725, Acc : 0.834, Sensitive_Loss : 0.02063, Sensitive_Acc : 0.478, Run Time : 8.85 sec
INFO:root:2024-03-27 15:14:45, Train, Epoch : 8, Step : 5400, Loss : 0.41849, Acc : 0.800, Sensitive_Loss : 0.03492, Sensitive_Acc : 0.478, Run Time : 8.53 sec
INFO:root:2024-03-27 15:16:56, Dev, Step : 5400, Loss : 0.54899, Acc : 0.748, Auc : 0.826, Sensitive_Loss : 0.06445, Sensitive_Acc : 0.458, Sensitive_Auc : 0.998, Mean auc: 0.826, Run Time : 131.65 sec
INFO:root:2024-03-27 15:17:02, Train, Epoch : 8, Step : 5410, Loss : 0.40732, Acc : 0.778, Sensitive_Loss : 0.02224, Sensitive_Acc : 0.444, Run Time : 137.64 sec
INFO:root:2024-03-27 15:17:12, Train, Epoch : 8, Step : 5420, Loss : 0.41384, Acc : 0.828, Sensitive_Loss : 0.01758, Sensitive_Acc : 0.456, Run Time : 9.89 sec
INFO:root:2024-03-27 15:17:21, Train, Epoch : 8, Step : 5430, Loss : 0.35746, Acc : 0.816, Sensitive_Loss : 0.04867, Sensitive_Acc : 0.491, Run Time : 8.84 sec
INFO:root:2024-03-27 15:17:31, Train, Epoch : 8, Step : 5440, Loss : 0.39463, Acc : 0.816, Sensitive_Loss : 0.02101, Sensitive_Acc : 0.475, Run Time : 9.68 sec
INFO:root:2024-03-27 15:17:39, Train, Epoch : 8, Step : 5450, Loss : 0.39096, Acc : 0.816, Sensitive_Loss : 0.03036, Sensitive_Acc : 0.475, Run Time : 8.23 sec
INFO:root:2024-03-27 15:17:50, Train, Epoch : 8, Step : 5460, Loss : 0.39335, Acc : 0.831, Sensitive_Loss : 0.01769, Sensitive_Acc : 0.487, Run Time : 10.71 sec
INFO:root:2024-03-27 15:17:58, Train, Epoch : 8, Step : 5470, Loss : 0.39168, Acc : 0.828, Sensitive_Loss : 0.02757, Sensitive_Acc : 0.478, Run Time : 8.81 sec
INFO:root:2024-03-27 15:18:07, Train, Epoch : 8, Step : 5480, Loss : 0.35497, Acc : 0.838, Sensitive_Loss : 0.01123, Sensitive_Acc : 0.519, Run Time : 8.66 sec
INFO:root:2024-03-27 15:18:17, Train, Epoch : 8, Step : 5490, Loss : 0.38234, Acc : 0.794, Sensitive_Loss : 0.05079, Sensitive_Acc : 0.503, Run Time : 9.53 sec
INFO:root:2024-03-27 15:18:25, Train, Epoch : 8, Step : 5500, Loss : 0.35269, Acc : 0.834, Sensitive_Loss : 0.02540, Sensitive_Acc : 0.491, Run Time : 8.78 sec
INFO:root:2024-03-27 15:20:39, Dev, Step : 5500, Loss : 0.56570, Acc : 0.746, Auc : 0.826, Sensitive_Loss : 0.06155, Sensitive_Acc : 0.519, Sensitive_Auc : 0.998, Mean auc: 0.826, Run Time : 133.30 sec
INFO:root:2024-03-27 15:20:46, Train, Epoch : 8, Step : 5510, Loss : 0.45472, Acc : 0.794, Sensitive_Loss : 0.04061, Sensitive_Acc : 0.528, Run Time : 140.23 sec
INFO:root:2024-03-27 15:20:55, Train, Epoch : 8, Step : 5520, Loss : 0.39074, Acc : 0.806, Sensitive_Loss : 0.02221, Sensitive_Acc : 0.509, Run Time : 9.17 sec
INFO:root:2024-03-27 15:21:03, Train, Epoch : 8, Step : 5530, Loss : 0.35511, Acc : 0.819, Sensitive_Loss : 0.02584, Sensitive_Acc : 0.506, Run Time : 8.53 sec
INFO:root:2024-03-27 15:21:12, Train, Epoch : 8, Step : 5540, Loss : 0.38538, Acc : 0.831, Sensitive_Loss : 0.04958, Sensitive_Acc : 0.525, Run Time : 8.52 sec
INFO:root:2024-03-27 15:21:21, Train, Epoch : 8, Step : 5550, Loss : 0.35582, Acc : 0.819, Sensitive_Loss : 0.03962, Sensitive_Acc : 0.487, Run Time : 9.16 sec
INFO:root:2024-03-27 15:21:31, Train, Epoch : 8, Step : 5560, Loss : 0.45237, Acc : 0.787, Sensitive_Loss : 0.04158, Sensitive_Acc : 0.425, Run Time : 10.30 sec
INFO:root:2024-03-27 15:21:40, Train, Epoch : 8, Step : 5570, Loss : 0.45323, Acc : 0.784, Sensitive_Loss : 0.01582, Sensitive_Acc : 0.494, Run Time : 8.50 sec
INFO:root:2024-03-27 15:21:50, Train, Epoch : 8, Step : 5580, Loss : 0.36683, Acc : 0.838, Sensitive_Loss : 0.02198, Sensitive_Acc : 0.519, Run Time : 10.69 sec
INFO:root:2024-03-27 15:22:00, Train, Epoch : 8, Step : 5590, Loss : 0.35991, Acc : 0.822, Sensitive_Loss : 0.03253, Sensitive_Acc : 0.575, Run Time : 10.05 sec
INFO:root:2024-03-27 15:22:08, Train, Epoch : 8, Step : 5600, Loss : 0.40529, Acc : 0.806, Sensitive_Loss : 0.02365, Sensitive_Acc : 0.522, Run Time : 7.60 sec
INFO:root:2024-03-27 15:24:22, Dev, Step : 5600, Loss : 0.54784, Acc : 0.747, Auc : 0.826, Sensitive_Loss : 0.06081, Sensitive_Acc : 0.509, Sensitive_Auc : 0.998, Mean auc: 0.826, Run Time : 133.69 sec
INFO:root:2024-03-27 15:24:28, Train, Epoch : 8, Step : 5610, Loss : 0.41668, Acc : 0.803, Sensitive_Loss : 0.04268, Sensitive_Acc : 0.491, Run Time : 139.83 sec
INFO:root:2024-03-27 15:24:39, Train, Epoch : 8, Step : 5620, Loss : 0.39830, Acc : 0.816, Sensitive_Loss : 0.02861, Sensitive_Acc : 0.506, Run Time : 10.78 sec
INFO:root:2024-03-27 15:24:48, Train, Epoch : 8, Step : 5630, Loss : 0.40009, Acc : 0.822, Sensitive_Loss : 0.03557, Sensitive_Acc : 0.500, Run Time : 8.80 sec
INFO:root:2024-03-27 15:24:56, Train, Epoch : 8, Step : 5640, Loss : 0.40761, Acc : 0.812, Sensitive_Loss : 0.02905, Sensitive_Acc : 0.478, Run Time : 8.79 sec
INFO:root:2024-03-27 15:25:07, Train, Epoch : 8, Step : 5650, Loss : 0.42347, Acc : 0.803, Sensitive_Loss : 0.03845, Sensitive_Acc : 0.475, Run Time : 10.67 sec
INFO:root:2024-03-27 15:25:16, Train, Epoch : 8, Step : 5660, Loss : 0.35048, Acc : 0.819, Sensitive_Loss : 0.02283, Sensitive_Acc : 0.469, Run Time : 9.27 sec
INFO:root:2024-03-27 15:25:25, Train, Epoch : 8, Step : 5670, Loss : 0.40041, Acc : 0.803, Sensitive_Loss : 0.02717, Sensitive_Acc : 0.503, Run Time : 8.72 sec
INFO:root:2024-03-27 15:25:34, Train, Epoch : 8, Step : 5680, Loss : 0.34059, Acc : 0.866, Sensitive_Loss : 0.03971, Sensitive_Acc : 0.487, Run Time : 8.98 sec
INFO:root:2024-03-27 15:25:44, Train, Epoch : 8, Step : 5690, Loss : 0.34993, Acc : 0.844, Sensitive_Loss : 0.02851, Sensitive_Acc : 0.522, Run Time : 10.17 sec
INFO:root:2024-03-27 15:25:53, Train, Epoch : 8, Step : 5700, Loss : 0.39971, Acc : 0.812, Sensitive_Loss : 0.01548, Sensitive_Acc : 0.453, Run Time : 8.69 sec
INFO:root:2024-03-27 15:28:06, Dev, Step : 5700, Loss : 0.55783, Acc : 0.749, Auc : 0.827, Sensitive_Loss : 0.05927, Sensitive_Acc : 0.486, Sensitive_Auc : 0.998, Mean auc: 0.827, Run Time : 132.88 sec
INFO:root:2024-03-27 15:28:12, Train, Epoch : 8, Step : 5710, Loss : 0.34695, Acc : 0.825, Sensitive_Loss : 0.02594, Sensitive_Acc : 0.456, Run Time : 138.84 sec
INFO:root:2024-03-27 15:28:21, Train, Epoch : 8, Step : 5720, Loss : 0.48284, Acc : 0.744, Sensitive_Loss : 0.02228, Sensitive_Acc : 0.453, Run Time : 9.00 sec
INFO:root:2024-03-27 15:28:31, Train, Epoch : 8, Step : 5730, Loss : 0.36540, Acc : 0.828, Sensitive_Loss : 0.03751, Sensitive_Acc : 0.456, Run Time : 10.33 sec
INFO:root:2024-03-27 15:28:39, Train, Epoch : 8, Step : 5740, Loss : 0.32538, Acc : 0.841, Sensitive_Loss : 0.01625, Sensitive_Acc : 0.506, Run Time : 8.43 sec
INFO:root:2024-03-27 15:28:48, Train, Epoch : 8, Step : 5750, Loss : 0.40807, Acc : 0.809, Sensitive_Loss : 0.01756, Sensitive_Acc : 0.512, Run Time : 8.40 sec
INFO:root:2024-03-27 15:29:00, Train, Epoch : 8, Step : 5760, Loss : 0.39164, Acc : 0.819, Sensitive_Loss : 0.01515, Sensitive_Acc : 0.544, Run Time : 11.86 sec
INFO:root:2024-03-27 15:29:09, Train, Epoch : 8, Step : 5770, Loss : 0.39085, Acc : 0.809, Sensitive_Loss : 0.04626, Sensitive_Acc : 0.512, Run Time : 9.58 sec
INFO:root:2024-03-27 15:29:18, Train, Epoch : 8, Step : 5780, Loss : 0.38125, Acc : 0.828, Sensitive_Loss : 0.06291, Sensitive_Acc : 0.444, Run Time : 8.39 sec
INFO:root:2024-03-27 15:29:26, Train, Epoch : 8, Step : 5790, Loss : 0.47779, Acc : 0.772, Sensitive_Loss : 0.02097, Sensitive_Acc : 0.519, Run Time : 8.61 sec
INFO:root:2024-03-27 15:29:36, Train, Epoch : 8, Step : 5800, Loss : 0.42781, Acc : 0.809, Sensitive_Loss : 0.05991, Sensitive_Acc : 0.466, Run Time : 10.24 sec
INFO:root:2024-03-27 15:31:49, Dev, Step : 5800, Loss : 0.55114, Acc : 0.750, Auc : 0.828, Sensitive_Loss : 0.06241, Sensitive_Acc : 0.491, Sensitive_Auc : 0.997, Mean auc: 0.828, Run Time : 132.42 sec
INFO:root:2024-03-27 15:31:55, Train, Epoch : 8, Step : 5810, Loss : 0.35268, Acc : 0.803, Sensitive_Loss : 0.02685, Sensitive_Acc : 0.509, Run Time : 138.41 sec
INFO:root:2024-03-27 15:32:04, Train, Epoch : 8, Step : 5820, Loss : 0.35073, Acc : 0.853, Sensitive_Loss : 0.01501, Sensitive_Acc : 0.481, Run Time : 8.83 sec
INFO:root:2024-03-27 15:32:14, Train, Epoch : 8, Step : 5830, Loss : 0.41635, Acc : 0.812, Sensitive_Loss : 0.03575, Sensitive_Acc : 0.466, Run Time : 10.50 sec
INFO:root:2024-03-27 15:32:23, Train, Epoch : 8, Step : 5840, Loss : 0.37539, Acc : 0.819, Sensitive_Loss : 0.02886, Sensitive_Acc : 0.472, Run Time : 8.74 sec
INFO:root:2024-03-27 15:32:37, Train, Epoch : 8, Step : 5850, Loss : 0.46443, Acc : 0.778, Sensitive_Loss : 0.01667, Sensitive_Acc : 0.438, Run Time : 14.21 sec
INFO:root:2024-03-27 15:32:47, Train, Epoch : 8, Step : 5860, Loss : 0.36054, Acc : 0.828, Sensitive_Loss : 0.04981, Sensitive_Acc : 0.463, Run Time : 9.38 sec
INFO:root:2024-03-27 15:32:55, Train, Epoch : 8, Step : 5870, Loss : 0.34788, Acc : 0.809, Sensitive_Loss : 0.02086, Sensitive_Acc : 0.469, Run Time : 8.09 sec
INFO:root:2024-03-27 15:33:03, Train, Epoch : 8, Step : 5880, Loss : 0.45721, Acc : 0.794, Sensitive_Loss : 0.05158, Sensitive_Acc : 0.512, Run Time : 8.74 sec
INFO:root:2024-03-27 15:33:14, Train, Epoch : 8, Step : 5890, Loss : 0.39486, Acc : 0.812, Sensitive_Loss : 0.01458, Sensitive_Acc : 0.512, Run Time : 10.47 sec
INFO:root:2024-03-27 15:33:22, Train, Epoch : 8, Step : 5900, Loss : 0.35821, Acc : 0.831, Sensitive_Loss : 0.02192, Sensitive_Acc : 0.519, Run Time : 8.62 sec
INFO:root:2024-03-27 15:35:36, Dev, Step : 5900, Loss : 0.55301, Acc : 0.748, Auc : 0.827, Sensitive_Loss : 0.06405, Sensitive_Acc : 0.491, Sensitive_Auc : 0.997, Mean auc: 0.827, Run Time : 133.07 sec
INFO:root:2024-03-27 15:35:42, Train, Epoch : 8, Step : 5910, Loss : 0.35645, Acc : 0.844, Sensitive_Loss : 0.01906, Sensitive_Acc : 0.500, Run Time : 139.77 sec
INFO:root:2024-03-27 15:35:51, Train, Epoch : 8, Step : 5920, Loss : 0.44727, Acc : 0.762, Sensitive_Loss : 0.04819, Sensitive_Acc : 0.453, Run Time : 9.02 sec
INFO:root:2024-03-27 15:36:01, Train, Epoch : 8, Step : 5930, Loss : 0.35591, Acc : 0.834, Sensitive_Loss : 0.06082, Sensitive_Acc : 0.478, Run Time : 10.16 sec
INFO:root:2024-03-27 15:36:11, Train, Epoch : 8, Step : 5940, Loss : 0.33419, Acc : 0.856, Sensitive_Loss : 0.02825, Sensitive_Acc : 0.459, Run Time : 9.72 sec
INFO:root:2024-03-27 15:36:21, Train, Epoch : 8, Step : 5950, Loss : 0.40324, Acc : 0.803, Sensitive_Loss : 0.01431, Sensitive_Acc : 0.450, Run Time : 9.57 sec
INFO:root:2024-03-27 15:36:29, Train, Epoch : 8, Step : 5960, Loss : 0.37359, Acc : 0.834, Sensitive_Loss : 0.05539, Sensitive_Acc : 0.434, Run Time : 8.28 sec
INFO:root:2024-03-27 15:36:38, Train, Epoch : 8, Step : 5970, Loss : 0.41138, Acc : 0.769, Sensitive_Loss : 0.03153, Sensitive_Acc : 0.481, Run Time : 9.14 sec
INFO:root:2024-03-27 15:36:47, Train, Epoch : 8, Step : 5980, Loss : 0.42382, Acc : 0.756, Sensitive_Loss : 0.03463, Sensitive_Acc : 0.438, Run Time : 9.00 sec
INFO:root:2024-03-27 15:36:55, Train, Epoch : 8, Step : 5990, Loss : 0.36064, Acc : 0.794, Sensitive_Loss : 0.01214, Sensitive_Acc : 0.453, Run Time : 8.38 sec
INFO:root:2024-03-27 15:37:06, Train, Epoch : 8, Step : 6000, Loss : 0.38162, Acc : 0.822, Sensitive_Loss : 0.05268, Sensitive_Acc : 0.456, Run Time : 10.46 sec
INFO:root:2024-03-27 15:39:18, Dev, Step : 6000, Loss : 0.55802, Acc : 0.752, Auc : 0.828, Sensitive_Loss : 0.07276, Sensitive_Acc : 0.485, Sensitive_Auc : 0.999, Mean auc: 0.828, Run Time : 132.08 sec
INFO:root:2024-03-27 15:41:33
INFO:root:y_pred: [0.10357991 0.43174943 0.9135488  ... 0.91947836 0.65419173 0.5722336 ]
INFO:root:y_true: [1. 1. 1. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [7.5950849e-01 1.6186164e-01 2.2997753e-07 9.9994683e-01 1.2760382e-04
 9.9996293e-01 9.9999702e-01 2.3613311e-04 1.1356393e-03 9.9999976e-01
 9.9985528e-01 7.6186651e-04 1.6757773e-02 9.3126446e-02 1.6208360e-03
 9.0878617e-05 1.9668745e-02 3.0268816e-04 8.4704988e-07 4.7251163e-03
 9.9999619e-01 5.6823255e-03 1.1140199e-04 4.9012218e-02 6.3980699e-01
 7.4535370e-01 9.9999988e-01 9.9799758e-01 6.5896432e-03 3.5480356e-03
 1.4979366e-02 9.9999630e-01 9.9997473e-01 9.9692374e-01 6.4858845e-03
 9.9950743e-01 5.7540080e-07 4.9240640e-03 9.9991155e-01 2.0005560e-02
 7.7861674e-02 1.1194381e-05 9.9299967e-01 9.9992812e-01 9.9996817e-01
 5.2755028e-01 9.9996591e-01 9.8920631e-01 5.4352391e-01 2.1597420e-05
 1.1741471e-03 6.9497060e-03 2.0926047e-02 8.9885610e-01 9.9999928e-01
 9.9999952e-01 4.5394113e-06 3.3465802e-04 9.9996400e-01 2.6895765e-07
 3.5876894e-04 9.9999547e-01 5.6717372e-06 9.9999869e-01 1.7231703e-05
 2.0557287e-05 7.4088149e-04 1.3857577e-03 2.2414180e-04 9.9994087e-01
 9.9999666e-01 9.9998403e-01 7.8972787e-02 9.9999928e-01 1.5760155e-01
 1.8045371e-02 9.9999881e-01 1.9627945e-03 1.7893507e-06 9.9998319e-01
 2.7027400e-03 3.8288039e-04 9.9876523e-01 1.9223811e-05 9.8868227e-01
 1.7154291e-04 1.6527940e-05 4.8047843e-07 9.9999022e-01 9.9806243e-01
 8.0140257e-05 1.0140257e-02 9.9960262e-01 4.9352407e-04 9.9999702e-01
 1.1205123e-02 9.9997509e-01 2.0176405e-03 9.6304452e-01 1.4057393e-05
 1.2408123e-01 5.6844540e-03 2.4224850e-03 3.9556898e-02 9.9999738e-01
 9.5336009e-06 5.7145321e-01 9.1865395e-06 9.5184933e-04 2.2807593e-02
 1.1989736e-05 2.5797028e-03 1.0974034e-04 5.5920868e-06 9.9999237e-01
 2.8451279e-04 7.0999362e-05 2.9513996e-02 2.2464553e-03 8.1125591e-03
 1.3329523e-03 9.9999964e-01 9.9894696e-01 9.9999976e-01 6.7077450e-02
 6.5118755e-04 1.9369720e-03 2.3020494e-04 2.3525681e-05 6.2808649e-05
 9.1528147e-02 9.7437775e-01 3.4187685e-03 9.9961877e-01 6.3831350e-03
 4.9821730e-03 3.1254577e-04 9.9999821e-01 1.7654261e-02 9.9999988e-01
 4.5180228e-03 9.9997711e-01 9.9935120e-01 1.6559723e-03 9.9999797e-01
 1.0983140e-01 9.9999380e-01 9.9999273e-01 9.9999964e-01 9.9999940e-01
 6.8697029e-01 3.0440515e-06 1.0307204e-04 1.1332494e-05 9.9992859e-01
 2.3595136e-02 9.9994648e-01 1.0057736e-01 9.9950004e-01 9.9921966e-01
 1.3193726e-05 7.7021825e-03 2.4022475e-04 9.9996352e-01 5.0411750e-02
 9.9987483e-01 9.9999034e-01 2.7383790e-06 1.0000000e+00 3.4063062e-04
 9.9992633e-01 9.9955624e-01 8.9871744e-04 9.9999607e-01 9.9996769e-01
 1.0000000e+00 2.0272857e-04 1.3656618e-02 9.9999011e-01 5.8791584e-01
 1.4599059e-04 9.1728591e-04 9.9920602e-05 1.6640364e-04 1.6640205e-04
 9.9997914e-01 9.9978966e-01 2.1730626e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.
 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.
 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0.
 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]
INFO:root:2024-03-27 15:41:33, Dev, Step : 6000, Loss : 0.55802, Acc : 0.752, Auc : 0.828, Sensitive_Loss : 0.07276, Sensitive_Acc : 0.485, Sensitive_Auc : 0.999, Mean auc: 0.828, Run Time : 134.47 sec
INFO:root:2024-03-27 15:41:45, Train, Epoch : 9, Step : 6010, Loss : 0.37045, Acc : 0.825, Sensitive_Loss : 0.03393, Sensitive_Acc : 0.500, Run Time : 11.06 sec
INFO:root:2024-03-27 15:41:54, Train, Epoch : 9, Step : 6020, Loss : 0.39704, Acc : 0.834, Sensitive_Loss : 0.03401, Sensitive_Acc : 0.487, Run Time : 8.71 sec
INFO:root:2024-03-27 15:42:02, Train, Epoch : 9, Step : 6030, Loss : 0.33193, Acc : 0.853, Sensitive_Loss : 0.03513, Sensitive_Acc : 0.506, Run Time : 8.26 sec
INFO:root:2024-03-27 15:42:12, Train, Epoch : 9, Step : 6040, Loss : 0.39156, Acc : 0.822, Sensitive_Loss : 0.01892, Sensitive_Acc : 0.478, Run Time : 9.90 sec
INFO:root:2024-03-27 15:42:21, Train, Epoch : 9, Step : 6050, Loss : 0.36898, Acc : 0.834, Sensitive_Loss : 0.02471, Sensitive_Acc : 0.497, Run Time : 9.23 sec
INFO:root:2024-03-27 15:42:30, Train, Epoch : 9, Step : 6060, Loss : 0.35981, Acc : 0.872, Sensitive_Loss : 0.02206, Sensitive_Acc : 0.441, Run Time : 8.55 sec
INFO:root:2024-03-27 15:42:39, Train, Epoch : 9, Step : 6070, Loss : 0.36890, Acc : 0.819, Sensitive_Loss : 0.02878, Sensitive_Acc : 0.500, Run Time : 9.37 sec
INFO:root:2024-03-27 15:42:48, Train, Epoch : 9, Step : 6080, Loss : 0.38118, Acc : 0.831, Sensitive_Loss : 0.03292, Sensitive_Acc : 0.484, Run Time : 8.94 sec
INFO:root:2024-03-27 15:42:56, Train, Epoch : 9, Step : 6090, Loss : 0.39345, Acc : 0.834, Sensitive_Loss : 0.02490, Sensitive_Acc : 0.472, Run Time : 8.66 sec
INFO:root:2024-03-27 15:43:06, Train, Epoch : 9, Step : 6100, Loss : 0.38695, Acc : 0.838, Sensitive_Loss : 0.02231, Sensitive_Acc : 0.434, Run Time : 9.45 sec
INFO:root:2024-03-27 15:45:21, Dev, Step : 6100, Loss : 0.55470, Acc : 0.750, Auc : 0.825, Sensitive_Loss : 0.06581, Sensitive_Acc : 0.462, Sensitive_Auc : 0.998, Mean auc: 0.825, Run Time : 134.85 sec
INFO:root:2024-03-27 15:45:27, Train, Epoch : 9, Step : 6110, Loss : 0.31540, Acc : 0.869, Sensitive_Loss : 0.03457, Sensitive_Acc : 0.475, Run Time : 140.95 sec
INFO:root:2024-03-27 15:45:36, Train, Epoch : 9, Step : 6120, Loss : 0.37033, Acc : 0.794, Sensitive_Loss : 0.02080, Sensitive_Acc : 0.466, Run Time : 8.65 sec
INFO:root:2024-03-27 15:45:47, Train, Epoch : 9, Step : 6130, Loss : 0.36576, Acc : 0.825, Sensitive_Loss : 0.01113, Sensitive_Acc : 0.547, Run Time : 11.09 sec
INFO:root:2024-03-27 15:45:56, Train, Epoch : 9, Step : 6140, Loss : 0.41606, Acc : 0.828, Sensitive_Loss : 0.02798, Sensitive_Acc : 0.497, Run Time : 9.54 sec
INFO:root:2024-03-27 15:46:05, Train, Epoch : 9, Step : 6150, Loss : 0.34831, Acc : 0.834, Sensitive_Loss : 0.04297, Sensitive_Acc : 0.500, Run Time : 8.78 sec
INFO:root:2024-03-27 15:46:16, Train, Epoch : 9, Step : 6160, Loss : 0.37108, Acc : 0.828, Sensitive_Loss : 0.02522, Sensitive_Acc : 0.469, Run Time : 10.92 sec
INFO:root:2024-03-27 15:46:26, Train, Epoch : 9, Step : 6170, Loss : 0.39479, Acc : 0.812, Sensitive_Loss : 0.02754, Sensitive_Acc : 0.463, Run Time : 10.56 sec
INFO:root:2024-03-27 15:46:35, Train, Epoch : 9, Step : 6180, Loss : 0.37793, Acc : 0.794, Sensitive_Loss : 0.03207, Sensitive_Acc : 0.419, Run Time : 8.50 sec
INFO:root:2024-03-27 15:46:44, Train, Epoch : 9, Step : 6190, Loss : 0.31962, Acc : 0.850, Sensitive_Loss : 0.02099, Sensitive_Acc : 0.506, Run Time : 8.77 sec
INFO:root:2024-03-27 15:46:54, Train, Epoch : 9, Step : 6200, Loss : 0.36028, Acc : 0.841, Sensitive_Loss : 0.03006, Sensitive_Acc : 0.497, Run Time : 10.25 sec
INFO:root:2024-03-27 15:49:08, Dev, Step : 6200, Loss : 0.55900, Acc : 0.747, Auc : 0.826, Sensitive_Loss : 0.05972, Sensitive_Acc : 0.484, Sensitive_Auc : 0.998, Mean auc: 0.826, Run Time : 134.49 sec
INFO:root:2024-03-27 15:49:16, Train, Epoch : 9, Step : 6210, Loss : 0.32622, Acc : 0.828, Sensitive_Loss : 0.03263, Sensitive_Acc : 0.459, Run Time : 141.58 sec
INFO:root:2024-03-27 15:49:26, Train, Epoch : 9, Step : 6220, Loss : 0.35207, Acc : 0.825, Sensitive_Loss : 0.01564, Sensitive_Acc : 0.509, Run Time : 10.92 sec
INFO:root:2024-03-27 15:49:35, Train, Epoch : 9, Step : 6230, Loss : 0.46375, Acc : 0.787, Sensitive_Loss : 0.01514, Sensitive_Acc : 0.503, Run Time : 8.57 sec
INFO:root:2024-03-27 15:49:44, Train, Epoch : 9, Step : 6240, Loss : 0.37199, Acc : 0.825, Sensitive_Loss : 0.01428, Sensitive_Acc : 0.519, Run Time : 9.02 sec
INFO:root:2024-03-27 15:49:54, Train, Epoch : 9, Step : 6250, Loss : 0.40669, Acc : 0.819, Sensitive_Loss : 0.01581, Sensitive_Acc : 0.516, Run Time : 10.01 sec
INFO:root:2024-03-27 15:50:03, Train, Epoch : 9, Step : 6260, Loss : 0.33287, Acc : 0.838, Sensitive_Loss : 0.01908, Sensitive_Acc : 0.487, Run Time : 9.08 sec
INFO:root:2024-03-27 15:50:12, Train, Epoch : 9, Step : 6270, Loss : 0.38413, Acc : 0.828, Sensitive_Loss : 0.02156, Sensitive_Acc : 0.550, Run Time : 9.04 sec
INFO:root:2024-03-27 15:50:22, Train, Epoch : 9, Step : 6280, Loss : 0.38338, Acc : 0.844, Sensitive_Loss : 0.02013, Sensitive_Acc : 0.506, Run Time : 9.67 sec
INFO:root:2024-03-27 15:50:31, Train, Epoch : 9, Step : 6290, Loss : 0.36055, Acc : 0.828, Sensitive_Loss : 0.01761, Sensitive_Acc : 0.481, Run Time : 9.13 sec
INFO:root:2024-03-27 15:50:42, Train, Epoch : 9, Step : 6300, Loss : 0.38428, Acc : 0.819, Sensitive_Loss : 0.01781, Sensitive_Acc : 0.475, Run Time : 10.87 sec
INFO:root:2024-03-27 15:52:59, Dev, Step : 6300, Loss : 0.57630, Acc : 0.745, Auc : 0.825, Sensitive_Loss : 0.06100, Sensitive_Acc : 0.492, Sensitive_Auc : 0.998, Mean auc: 0.825, Run Time : 137.33 sec
INFO:root:2024-03-27 15:53:05, Train, Epoch : 9, Step : 6310, Loss : 0.38357, Acc : 0.825, Sensitive_Loss : 0.04030, Sensitive_Acc : 0.494, Run Time : 143.34 sec
INFO:root:2024-03-27 15:53:14, Train, Epoch : 9, Step : 6320, Loss : 0.35296, Acc : 0.838, Sensitive_Loss : 0.03295, Sensitive_Acc : 0.503, Run Time : 8.84 sec
INFO:root:2024-03-27 15:53:24, Train, Epoch : 9, Step : 6330, Loss : 0.38121, Acc : 0.853, Sensitive_Loss : 0.02183, Sensitive_Acc : 0.453, Run Time : 9.99 sec
INFO:root:2024-03-27 15:53:34, Train, Epoch : 9, Step : 6340, Loss : 0.37246, Acc : 0.828, Sensitive_Loss : 0.01556, Sensitive_Acc : 0.478, Run Time : 9.94 sec
INFO:root:2024-03-27 15:53:43, Train, Epoch : 9, Step : 6350, Loss : 0.37751, Acc : 0.834, Sensitive_Loss : 0.02298, Sensitive_Acc : 0.463, Run Time : 8.60 sec
INFO:root:2024-03-27 15:53:52, Train, Epoch : 9, Step : 6360, Loss : 0.38196, Acc : 0.816, Sensitive_Loss : 0.03011, Sensitive_Acc : 0.487, Run Time : 9.52 sec
INFO:root:2024-03-27 15:54:04, Train, Epoch : 9, Step : 6370, Loss : 0.36037, Acc : 0.844, Sensitive_Loss : 0.02547, Sensitive_Acc : 0.472, Run Time : 11.44 sec
INFO:root:2024-03-27 15:54:12, Train, Epoch : 9, Step : 6380, Loss : 0.33292, Acc : 0.828, Sensitive_Loss : 0.01819, Sensitive_Acc : 0.422, Run Time : 8.94 sec
INFO:root:2024-03-27 15:54:22, Train, Epoch : 9, Step : 6390, Loss : 0.38118, Acc : 0.806, Sensitive_Loss : 0.03707, Sensitive_Acc : 0.484, Run Time : 9.30 sec
INFO:root:2024-03-27 15:54:32, Train, Epoch : 9, Step : 6400, Loss : 0.38149, Acc : 0.800, Sensitive_Loss : 0.01590, Sensitive_Acc : 0.484, Run Time : 10.21 sec
INFO:root:2024-03-27 15:56:45, Dev, Step : 6400, Loss : 0.55767, Acc : 0.746, Auc : 0.825, Sensitive_Loss : 0.06187, Sensitive_Acc : 0.517, Sensitive_Auc : 0.998, Mean auc: 0.825, Run Time : 133.11 sec
INFO:root:2024-03-27 15:56:52, Train, Epoch : 9, Step : 6410, Loss : 0.35669, Acc : 0.803, Sensitive_Loss : 0.02303, Sensitive_Acc : 0.509, Run Time : 140.48 sec
INFO:root:2024-03-27 15:57:02, Train, Epoch : 9, Step : 6420, Loss : 0.38274, Acc : 0.844, Sensitive_Loss : 0.01532, Sensitive_Acc : 0.441, Run Time : 9.56 sec
INFO:root:2024-03-27 15:57:10, Train, Epoch : 9, Step : 6430, Loss : 0.42091, Acc : 0.822, Sensitive_Loss : 0.02637, Sensitive_Acc : 0.453, Run Time : 8.48 sec
INFO:root:2024-03-27 15:57:20, Train, Epoch : 9, Step : 6440, Loss : 0.39057, Acc : 0.828, Sensitive_Loss : 0.04044, Sensitive_Acc : 0.494, Run Time : 9.20 sec
INFO:root:2024-03-27 15:57:32, Train, Epoch : 9, Step : 6450, Loss : 0.34365, Acc : 0.841, Sensitive_Loss : 0.01287, Sensitive_Acc : 0.463, Run Time : 12.25 sec
INFO:root:2024-03-27 15:57:41, Train, Epoch : 9, Step : 6460, Loss : 0.38009, Acc : 0.816, Sensitive_Loss : 0.03993, Sensitive_Acc : 0.522, Run Time : 8.95 sec
INFO:root:2024-03-27 15:57:49, Train, Epoch : 9, Step : 6470, Loss : 0.35238, Acc : 0.825, Sensitive_Loss : 0.05173, Sensitive_Acc : 0.531, Run Time : 8.56 sec
INFO:root:2024-03-27 15:58:01, Train, Epoch : 9, Step : 6480, Loss : 0.33831, Acc : 0.847, Sensitive_Loss : 0.03040, Sensitive_Acc : 0.472, Run Time : 11.16 sec
INFO:root:2024-03-27 15:58:10, Train, Epoch : 9, Step : 6490, Loss : 0.36768, Acc : 0.812, Sensitive_Loss : 0.01922, Sensitive_Acc : 0.459, Run Time : 9.01 sec
INFO:root:2024-03-27 15:58:19, Train, Epoch : 9, Step : 6500, Loss : 0.29397, Acc : 0.847, Sensitive_Loss : 0.02627, Sensitive_Acc : 0.500, Run Time : 8.90 sec
INFO:root:2024-03-27 16:00:33, Dev, Step : 6500, Loss : 0.58339, Acc : 0.741, Auc : 0.822, Sensitive_Loss : 0.05287, Sensitive_Acc : 0.485, Sensitive_Auc : 0.998, Mean auc: 0.822, Run Time : 134.09 sec
INFO:root:2024-03-27 16:00:39, Train, Epoch : 9, Step : 6510, Loss : 0.34242, Acc : 0.828, Sensitive_Loss : 0.02905, Sensitive_Acc : 0.478, Run Time : 140.28 sec
INFO:root:2024-03-27 16:00:48, Train, Epoch : 9, Step : 6520, Loss : 0.38670, Acc : 0.856, Sensitive_Loss : 0.03365, Sensitive_Acc : 0.512, Run Time : 9.39 sec
INFO:root:2024-03-27 16:00:57, Train, Epoch : 9, Step : 6530, Loss : 0.36679, Acc : 0.828, Sensitive_Loss : 0.01417, Sensitive_Acc : 0.494, Run Time : 8.58 sec
INFO:root:2024-03-27 16:01:06, Train, Epoch : 9, Step : 6540, Loss : 0.41965, Acc : 0.812, Sensitive_Loss : 0.01696, Sensitive_Acc : 0.500, Run Time : 9.26 sec
INFO:root:2024-03-27 16:01:15, Train, Epoch : 9, Step : 6550, Loss : 0.36431, Acc : 0.844, Sensitive_Loss : 0.06582, Sensitive_Acc : 0.484, Run Time : 9.01 sec
INFO:root:2024-03-27 16:01:25, Train, Epoch : 9, Step : 6560, Loss : 0.41294, Acc : 0.841, Sensitive_Loss : 0.01573, Sensitive_Acc : 0.541, Run Time : 9.53 sec
INFO:root:2024-03-27 16:01:33, Train, Epoch : 9, Step : 6570, Loss : 0.31039, Acc : 0.859, Sensitive_Loss : 0.01958, Sensitive_Acc : 0.491, Run Time : 8.35 sec
INFO:root:2024-03-27 16:01:42, Train, Epoch : 9, Step : 6580, Loss : 0.39105, Acc : 0.809, Sensitive_Loss : 0.02973, Sensitive_Acc : 0.494, Run Time : 9.27 sec
INFO:root:2024-03-27 16:01:51, Train, Epoch : 9, Step : 6590, Loss : 0.41355, Acc : 0.816, Sensitive_Loss : 0.01445, Sensitive_Acc : 0.481, Run Time : 9.22 sec
INFO:root:2024-03-27 16:02:01, Train, Epoch : 9, Step : 6600, Loss : 0.44698, Acc : 0.816, Sensitive_Loss : 0.03854, Sensitive_Acc : 0.459, Run Time : 9.96 sec
INFO:root:2024-03-27 16:04:17, Dev, Step : 6600, Loss : 0.56516, Acc : 0.752, Auc : 0.826, Sensitive_Loss : 0.05419, Sensitive_Acc : 0.457, Sensitive_Auc : 0.998, Mean auc: 0.826, Run Time : 135.64 sec
INFO:root:2024-03-27 16:04:24, Train, Epoch : 9, Step : 6610, Loss : 0.32264, Acc : 0.834, Sensitive_Loss : 0.01966, Sensitive_Acc : 0.416, Run Time : 142.19 sec
INFO:root:2024-03-27 16:04:33, Train, Epoch : 9, Step : 6620, Loss : 0.29622, Acc : 0.884, Sensitive_Loss : 0.02686, Sensitive_Acc : 0.475, Run Time : 9.22 sec
INFO:root:2024-03-27 16:04:42, Train, Epoch : 9, Step : 6630, Loss : 0.35018, Acc : 0.847, Sensitive_Loss : 0.02909, Sensitive_Acc : 0.466, Run Time : 8.84 sec
INFO:root:2024-03-27 16:04:53, Train, Epoch : 9, Step : 6640, Loss : 0.41776, Acc : 0.809, Sensitive_Loss : 0.04124, Sensitive_Acc : 0.487, Run Time : 11.01 sec
INFO:root:2024-03-27 16:05:02, Train, Epoch : 9, Step : 6650, Loss : 0.39307, Acc : 0.828, Sensitive_Loss : 0.01365, Sensitive_Acc : 0.500, Run Time : 8.88 sec
INFO:root:2024-03-27 16:05:10, Train, Epoch : 9, Step : 6660, Loss : 0.38495, Acc : 0.800, Sensitive_Loss : 0.03459, Sensitive_Acc : 0.516, Run Time : 8.29 sec
INFO:root:2024-03-27 16:05:19, Train, Epoch : 9, Step : 6670, Loss : 0.33439, Acc : 0.872, Sensitive_Loss : 0.01774, Sensitive_Acc : 0.512, Run Time : 9.08 sec
INFO:root:2024-03-27 16:05:28, Train, Epoch : 9, Step : 6680, Loss : 0.35162, Acc : 0.841, Sensitive_Loss : 0.04441, Sensitive_Acc : 0.475, Run Time : 8.98 sec
INFO:root:2024-03-27 16:05:37, Train, Epoch : 9, Step : 6690, Loss : 0.40376, Acc : 0.819, Sensitive_Loss : 0.02403, Sensitive_Acc : 0.481, Run Time : 8.93 sec
INFO:root:2024-03-27 16:05:45, Train, Epoch : 9, Step : 6700, Loss : 0.36003, Acc : 0.838, Sensitive_Loss : 0.01942, Sensitive_Acc : 0.494, Run Time : 8.45 sec
INFO:root:2024-03-27 16:07:56, Dev, Step : 6700, Loss : 0.58540, Acc : 0.741, Auc : 0.823, Sensitive_Loss : 0.05197, Sensitive_Acc : 0.491, Sensitive_Auc : 0.997, Mean auc: 0.823, Run Time : 131.23 sec
INFO:root:2024-03-27 16:08:03, Train, Epoch : 9, Step : 6710, Loss : 0.38776, Acc : 0.822, Sensitive_Loss : 0.02385, Sensitive_Acc : 0.497, Run Time : 137.63 sec
INFO:root:2024-03-27 16:08:11, Train, Epoch : 9, Step : 6720, Loss : 0.44561, Acc : 0.787, Sensitive_Loss : 0.04191, Sensitive_Acc : 0.469, Run Time : 8.55 sec
INFO:root:2024-03-27 16:08:21, Train, Epoch : 9, Step : 6730, Loss : 0.29885, Acc : 0.863, Sensitive_Loss : 0.01266, Sensitive_Acc : 0.484, Run Time : 9.42 sec
INFO:root:2024-03-27 16:08:29, Train, Epoch : 9, Step : 6740, Loss : 0.33103, Acc : 0.841, Sensitive_Loss : 0.03839, Sensitive_Acc : 0.422, Run Time : 8.13 sec
INFO:root:2024-03-27 16:08:38, Train, Epoch : 9, Step : 6750, Loss : 0.38232, Acc : 0.841, Sensitive_Loss : 0.04084, Sensitive_Acc : 0.481, Run Time : 8.54 sec
INFO:root:2024-03-27 16:10:46
INFO:root:y_pred: [0.23360437 0.4524235  0.9168638  ... 0.8622867  0.65903276 0.5833648 ]
INFO:root:y_true: [1. 1. 1. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [6.71472073e-01 7.57277608e-02 6.40050146e-08 9.99701321e-01
 2.41295638e-05 9.99959469e-01 9.99996066e-01 1.46097125e-04
 2.52173253e-04 9.99999762e-01 9.99934912e-01 8.44993629e-05
 9.06614773e-03 1.04385495e-01 5.95136138e-04 1.19631122e-05
 9.66806151e-03 2.37553963e-04 6.86192948e-07 5.05900662e-03
 9.99993682e-01 5.50470408e-03 2.35082534e-05 9.43215936e-03
 1.77821472e-01 3.32532644e-01 9.99999762e-01 9.90179181e-01
 2.94094556e-03 8.15522159e-04 1.22708017e-02 9.99998093e-01
 9.99966025e-01 9.94935095e-01 4.51221224e-03 9.99640226e-01
 7.89719010e-08 1.27153960e-03 9.99730527e-01 1.65458955e-03
 5.19386120e-02 5.28372402e-06 9.78407443e-01 9.99964952e-01
 9.99989510e-01 2.28024468e-01 9.99943852e-01 9.62618589e-01
 6.46781683e-01 9.05841898e-06 5.10000915e-04 2.38987384e-03
 2.33674841e-03 8.87917340e-01 9.99999166e-01 9.99998689e-01
 3.10002201e-06 8.96884885e-05 9.99979973e-01 1.16197754e-08
 8.10125130e-05 9.99994874e-01 9.72339194e-07 9.99998331e-01
 3.49185530e-06 1.37399193e-05 9.91320267e-05 2.34373030e-04
 4.79041082e-05 9.99884844e-01 9.99993324e-01 9.99980330e-01
 2.94499882e-02 9.99998808e-01 9.72026512e-02 1.03713549e-03
 9.99999762e-01 1.13107788e-03 3.63087736e-07 9.99905229e-01
 8.01150571e-04 2.29432335e-05 9.93969202e-01 3.88960098e-06
 9.71715152e-01 7.66769372e-05 1.99285500e-06 4.32331888e-07
 9.99979258e-01 9.95891690e-01 3.66815657e-05 3.87955527e-03
 9.99413610e-01 5.03036543e-04 9.99999166e-01 1.98045745e-03
 9.99920487e-01 2.79200001e-04 7.26008117e-01 5.20786853e-06
 5.06894775e-02 2.66049011e-03 5.03508316e-04 1.58830602e-02
 9.99996662e-01 1.41479165e-06 1.82673231e-01 4.09097129e-06
 5.38973662e-04 1.72130428e-02 2.51428810e-06 4.05939325e-04
 1.21756057e-05 6.24424683e-06 9.99982357e-01 7.66835146e-05
 1.88584163e-05 5.09168662e-04 4.85887722e-04 1.29370554e-03
 4.31049499e-04 9.99999762e-01 9.97916639e-01 9.99999404e-01
 2.65072342e-02 1.20835582e-04 4.88620601e-04 7.57498719e-06
 1.20467112e-05 1.77406291e-05 5.52088879e-02 9.59356666e-01
 1.08475436e-03 9.98722374e-01 3.76143446e-03 2.34456180e-04
 9.06027708e-05 9.99997497e-01 1.34303011e-02 9.99999642e-01
 4.77307476e-03 9.99975801e-01 9.99094605e-01 8.98338971e-04
 9.99999642e-01 8.42887461e-02 9.99994993e-01 9.99988317e-01
 9.99999404e-01 9.99999285e-01 3.77455741e-01 5.99399812e-07
 1.14490802e-04 8.52382345e-06 9.99805748e-01 5.15561318e-03
 9.99872088e-01 9.71124228e-03 9.99222279e-01 9.97330189e-01
 7.73510783e-06 1.49721990e-03 2.55969844e-05 9.99812543e-01
 1.82548892e-02 9.99282420e-01 9.99974489e-01 1.57353020e-06
 1.00000000e+00 7.62369964e-05 9.99969602e-01 9.99552310e-01
 5.31324535e-04 9.99998331e-01 9.99944329e-01 1.00000000e+00
 4.66351121e-05 6.28162501e-03 9.99973774e-01 3.52446407e-01
 3.97509757e-05 2.33115410e-04 2.77300624e-05 1.26145649e-04
 2.27662313e-05 9.99957323e-01 9.99680638e-01 3.47316042e-02]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.
 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.
 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0.
 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]
INFO:root:2024-03-27 16:10:46, Dev, Step : 6750, Loss : 0.57463, Acc : 0.749, Auc : 0.828, Sensitive_Loss : 0.05547, Sensitive_Acc : 0.485, Sensitive_Auc : 0.998, Mean auc: 0.828, Run Time : 128.77 sec
INFO:root:2024-03-27 16:10:57, Train, Epoch : 10, Step : 6760, Loss : 0.33893, Acc : 0.844, Sensitive_Loss : 0.01213, Sensitive_Acc : 0.484, Run Time : 9.84 sec
INFO:root:2024-03-27 16:11:06, Train, Epoch : 10, Step : 6770, Loss : 0.38978, Acc : 0.828, Sensitive_Loss : 0.02781, Sensitive_Acc : 0.444, Run Time : 9.23 sec
INFO:root:2024-03-27 16:11:14, Train, Epoch : 10, Step : 6780, Loss : 0.35817, Acc : 0.859, Sensitive_Loss : 0.02656, Sensitive_Acc : 0.478, Run Time : 7.99 sec
INFO:root:2024-03-27 16:11:23, Train, Epoch : 10, Step : 6790, Loss : 0.30478, Acc : 0.859, Sensitive_Loss : 0.03287, Sensitive_Acc : 0.484, Run Time : 8.80 sec
INFO:root:2024-03-27 16:11:33, Train, Epoch : 10, Step : 6800, Loss : 0.30912, Acc : 0.844, Sensitive_Loss : 0.03373, Sensitive_Acc : 0.494, Run Time : 9.40 sec
INFO:root:2024-03-27 16:13:43, Dev, Step : 6800, Loss : 0.57786, Acc : 0.749, Auc : 0.828, Sensitive_Loss : 0.06410, Sensitive_Acc : 0.505, Sensitive_Auc : 0.998, Mean auc: 0.828, Run Time : 130.53 sec
INFO:root:2024-03-27 16:13:49, Train, Epoch : 10, Step : 6810, Loss : 0.34686, Acc : 0.850, Sensitive_Loss : 0.01254, Sensitive_Acc : 0.484, Run Time : 136.77 sec
INFO:root:2024-03-27 16:13:58, Train, Epoch : 10, Step : 6820, Loss : 0.35428, Acc : 0.831, Sensitive_Loss : 0.02823, Sensitive_Acc : 0.503, Run Time : 9.08 sec
INFO:root:2024-03-27 16:14:07, Train, Epoch : 10, Step : 6830, Loss : 0.38755, Acc : 0.844, Sensitive_Loss : 0.01685, Sensitive_Acc : 0.503, Run Time : 9.01 sec
INFO:root:2024-03-27 16:14:17, Train, Epoch : 10, Step : 6840, Loss : 0.32027, Acc : 0.834, Sensitive_Loss : 0.01034, Sensitive_Acc : 0.472, Run Time : 9.13 sec
INFO:root:2024-03-27 16:14:26, Train, Epoch : 10, Step : 6850, Loss : 0.32100, Acc : 0.856, Sensitive_Loss : 0.02357, Sensitive_Acc : 0.512, Run Time : 9.22 sec
INFO:root:2024-03-27 16:14:37, Train, Epoch : 10, Step : 6860, Loss : 0.35753, Acc : 0.850, Sensitive_Loss : 0.04454, Sensitive_Acc : 0.544, Run Time : 10.92 sec
INFO:root:2024-03-27 16:14:50, Train, Epoch : 10, Step : 6870, Loss : 0.39878, Acc : 0.838, Sensitive_Loss : 0.02585, Sensitive_Acc : 0.466, Run Time : 13.12 sec
INFO:root:2024-03-27 16:15:02, Train, Epoch : 10, Step : 6880, Loss : 0.33079, Acc : 0.853, Sensitive_Loss : 0.03328, Sensitive_Acc : 0.478, Run Time : 12.50 sec
INFO:root:2024-03-27 16:15:15, Train, Epoch : 10, Step : 6890, Loss : 0.32247, Acc : 0.850, Sensitive_Loss : 0.01605, Sensitive_Acc : 0.497, Run Time : 12.46 sec
INFO:root:2024-03-27 16:15:25, Train, Epoch : 10, Step : 6900, Loss : 0.38753, Acc : 0.828, Sensitive_Loss : 0.00732, Sensitive_Acc : 0.500, Run Time : 10.36 sec
INFO:root:2024-03-27 16:18:06, Dev, Step : 6900, Loss : 0.58967, Acc : 0.751, Auc : 0.828, Sensitive_Loss : 0.05247, Sensitive_Acc : 0.486, Sensitive_Auc : 0.998, Mean auc: 0.828, Run Time : 161.15 sec
INFO:root:2024-03-27 16:18:14, Train, Epoch : 10, Step : 6910, Loss : 0.35834, Acc : 0.816, Sensitive_Loss : 0.02966, Sensitive_Acc : 0.497, Run Time : 168.79 sec
INFO:root:2024-03-27 16:18:27, Train, Epoch : 10, Step : 6920, Loss : 0.32826, Acc : 0.859, Sensitive_Loss : 0.02556, Sensitive_Acc : 0.503, Run Time : 13.06 sec
INFO:root:2024-03-27 16:18:38, Train, Epoch : 10, Step : 6930, Loss : 0.41841, Acc : 0.825, Sensitive_Loss : 0.01332, Sensitive_Acc : 0.509, Run Time : 11.47 sec
INFO:root:2024-03-27 16:18:49, Train, Epoch : 10, Step : 6940, Loss : 0.34700, Acc : 0.841, Sensitive_Loss : 0.02412, Sensitive_Acc : 0.469, Run Time : 11.01 sec
INFO:root:2024-03-27 16:19:03, Train, Epoch : 10, Step : 6950, Loss : 0.36710, Acc : 0.850, Sensitive_Loss : 0.02031, Sensitive_Acc : 0.478, Run Time : 13.65 sec
INFO:root:2024-03-27 16:19:15, Train, Epoch : 10, Step : 6960, Loss : 0.40694, Acc : 0.816, Sensitive_Loss : 0.01717, Sensitive_Acc : 0.525, Run Time : 12.33 sec
INFO:root:2024-03-27 16:19:26, Train, Epoch : 10, Step : 6970, Loss : 0.33922, Acc : 0.844, Sensitive_Loss : 0.02210, Sensitive_Acc : 0.422, Run Time : 10.93 sec
INFO:root:2024-03-27 16:19:39, Train, Epoch : 10, Step : 6980, Loss : 0.33892, Acc : 0.831, Sensitive_Loss : 0.03573, Sensitive_Acc : 0.481, Run Time : 12.50 sec
INFO:root:2024-03-27 16:19:49, Train, Epoch : 10, Step : 6990, Loss : 0.40252, Acc : 0.822, Sensitive_Loss : 0.03303, Sensitive_Acc : 0.484, Run Time : 10.62 sec
INFO:root:2024-03-27 16:20:01, Train, Epoch : 10, Step : 7000, Loss : 0.41022, Acc : 0.838, Sensitive_Loss : 0.02521, Sensitive_Acc : 0.444, Run Time : 11.28 sec
INFO:root:2024-03-27 16:22:08, Dev, Step : 7000, Loss : 0.56513, Acc : 0.749, Auc : 0.823, Sensitive_Loss : 0.06217, Sensitive_Acc : 0.476, Sensitive_Auc : 0.998, Mean auc: 0.823, Run Time : 127.07 sec
INFO:root:2024-03-27 16:22:15, Train, Epoch : 10, Step : 7010, Loss : 0.39626, Acc : 0.797, Sensitive_Loss : 0.02515, Sensitive_Acc : 0.497, Run Time : 134.25 sec
INFO:root:2024-03-27 16:22:27, Train, Epoch : 10, Step : 7020, Loss : 0.33780, Acc : 0.822, Sensitive_Loss : 0.01273, Sensitive_Acc : 0.516, Run Time : 12.41 sec
INFO:root:2024-03-27 16:22:39, Train, Epoch : 10, Step : 7030, Loss : 0.32774, Acc : 0.866, Sensitive_Loss : 0.02520, Sensitive_Acc : 0.491, Run Time : 11.19 sec
INFO:root:2024-03-27 16:22:49, Train, Epoch : 10, Step : 7040, Loss : 0.37749, Acc : 0.859, Sensitive_Loss : 0.05209, Sensitive_Acc : 0.481, Run Time : 10.49 sec
INFO:root:2024-03-27 16:23:00, Train, Epoch : 10, Step : 7050, Loss : 0.38776, Acc : 0.812, Sensitive_Loss : 0.02641, Sensitive_Acc : 0.484, Run Time : 10.88 sec
INFO:root:2024-03-27 16:23:10, Train, Epoch : 10, Step : 7060, Loss : 0.31276, Acc : 0.866, Sensitive_Loss : 0.04084, Sensitive_Acc : 0.466, Run Time : 10.34 sec
INFO:root:2024-03-27 16:23:20, Train, Epoch : 10, Step : 7070, Loss : 0.32086, Acc : 0.844, Sensitive_Loss : 0.05004, Sensitive_Acc : 0.528, Run Time : 9.64 sec
INFO:root:2024-03-27 16:23:30, Train, Epoch : 10, Step : 7080, Loss : 0.30135, Acc : 0.847, Sensitive_Loss : 0.02234, Sensitive_Acc : 0.491, Run Time : 10.12 sec
INFO:root:2024-03-27 16:23:43, Train, Epoch : 10, Step : 7090, Loss : 0.38830, Acc : 0.819, Sensitive_Loss : 0.02782, Sensitive_Acc : 0.512, Run Time : 13.06 sec
INFO:root:2024-03-27 16:23:53, Train, Epoch : 10, Step : 7100, Loss : 0.40089, Acc : 0.809, Sensitive_Loss : 0.02076, Sensitive_Acc : 0.484, Run Time : 9.94 sec
INFO:root:2024-03-27 16:26:01, Dev, Step : 7100, Loss : 0.56602, Acc : 0.749, Auc : 0.822, Sensitive_Loss : 0.05111, Sensitive_Acc : 0.464, Sensitive_Auc : 0.998, Mean auc: 0.822, Run Time : 127.86 sec
INFO:root:2024-03-27 16:26:08, Train, Epoch : 10, Step : 7110, Loss : 0.37960, Acc : 0.841, Sensitive_Loss : 0.02814, Sensitive_Acc : 0.422, Run Time : 135.37 sec
INFO:root:2024-03-27 16:26:18, Train, Epoch : 10, Step : 7120, Loss : 0.35107, Acc : 0.841, Sensitive_Loss : 0.04418, Sensitive_Acc : 0.444, Run Time : 9.83 sec
INFO:root:2024-03-27 16:26:29, Train, Epoch : 10, Step : 7130, Loss : 0.32467, Acc : 0.847, Sensitive_Loss : 0.03069, Sensitive_Acc : 0.494, Run Time : 10.26 sec
INFO:root:2024-03-27 16:26:41, Train, Epoch : 10, Step : 7140, Loss : 0.33780, Acc : 0.859, Sensitive_Loss : 0.01862, Sensitive_Acc : 0.500, Run Time : 12.14 sec
INFO:root:2024-03-27 16:26:50, Train, Epoch : 10, Step : 7150, Loss : 0.32094, Acc : 0.844, Sensitive_Loss : 0.01327, Sensitive_Acc : 0.528, Run Time : 9.42 sec
INFO:root:2024-03-27 16:27:00, Train, Epoch : 10, Step : 7160, Loss : 0.32371, Acc : 0.853, Sensitive_Loss : 0.01537, Sensitive_Acc : 0.506, Run Time : 9.86 sec
INFO:root:2024-03-27 16:27:13, Train, Epoch : 10, Step : 7170, Loss : 0.29938, Acc : 0.859, Sensitive_Loss : 0.04522, Sensitive_Acc : 0.512, Run Time : 13.37 sec
INFO:root:2024-03-27 16:27:25, Train, Epoch : 10, Step : 7180, Loss : 0.40672, Acc : 0.822, Sensitive_Loss : 0.02925, Sensitive_Acc : 0.453, Run Time : 11.66 sec
INFO:root:2024-03-27 16:27:35, Train, Epoch : 10, Step : 7190, Loss : 0.43242, Acc : 0.822, Sensitive_Loss : 0.02858, Sensitive_Acc : 0.475, Run Time : 9.83 sec
INFO:root:2024-03-27 16:27:46, Train, Epoch : 10, Step : 7200, Loss : 0.32851, Acc : 0.844, Sensitive_Loss : 0.02547, Sensitive_Acc : 0.475, Run Time : 10.78 sec
INFO:root:2024-03-27 16:29:54, Dev, Step : 7200, Loss : 0.56713, Acc : 0.747, Auc : 0.823, Sensitive_Loss : 0.06083, Sensitive_Acc : 0.475, Sensitive_Auc : 0.998, Mean auc: 0.823, Run Time : 128.67 sec
INFO:root:2024-03-27 16:30:01, Train, Epoch : 10, Step : 7210, Loss : 0.35843, Acc : 0.819, Sensitive_Loss : 0.01699, Sensitive_Acc : 0.484, Run Time : 135.83 sec
INFO:root:2024-03-27 16:30:14, Train, Epoch : 10, Step : 7220, Loss : 0.34079, Acc : 0.850, Sensitive_Loss : 0.03754, Sensitive_Acc : 0.487, Run Time : 12.76 sec
INFO:root:2024-03-27 16:30:24, Train, Epoch : 10, Step : 7230, Loss : 0.35068, Acc : 0.838, Sensitive_Loss : 0.03321, Sensitive_Acc : 0.447, Run Time : 9.70 sec
INFO:root:2024-03-27 16:30:34, Train, Epoch : 10, Step : 7240, Loss : 0.35198, Acc : 0.831, Sensitive_Loss : 0.04110, Sensitive_Acc : 0.469, Run Time : 10.12 sec
INFO:root:2024-03-27 16:30:45, Train, Epoch : 10, Step : 7250, Loss : 0.30926, Acc : 0.869, Sensitive_Loss : 0.02558, Sensitive_Acc : 0.484, Run Time : 11.19 sec
INFO:root:2024-03-27 16:30:55, Train, Epoch : 10, Step : 7260, Loss : 0.38262, Acc : 0.806, Sensitive_Loss : 0.01867, Sensitive_Acc : 0.516, Run Time : 9.66 sec
INFO:root:2024-03-27 16:31:05, Train, Epoch : 10, Step : 7270, Loss : 0.34592, Acc : 0.828, Sensitive_Loss : 0.02202, Sensitive_Acc : 0.475, Run Time : 9.98 sec
INFO:root:2024-03-27 16:31:20, Train, Epoch : 10, Step : 7280, Loss : 0.30979, Acc : 0.825, Sensitive_Loss : 0.01731, Sensitive_Acc : 0.447, Run Time : 15.11 sec
INFO:root:2024-03-27 16:31:31, Train, Epoch : 10, Step : 7290, Loss : 0.33369, Acc : 0.838, Sensitive_Loss : 0.02392, Sensitive_Acc : 0.444, Run Time : 11.02 sec
INFO:root:2024-03-27 16:31:40, Train, Epoch : 10, Step : 7300, Loss : 0.42447, Acc : 0.809, Sensitive_Loss : 0.01637, Sensitive_Acc : 0.450, Run Time : 9.46 sec
INFO:root:2024-03-27 16:33:48, Dev, Step : 7300, Loss : 0.57189, Acc : 0.748, Auc : 0.822, Sensitive_Loss : 0.05234, Sensitive_Acc : 0.476, Sensitive_Auc : 0.998, Mean auc: 0.822, Run Time : 127.93 sec
INFO:root:2024-03-27 16:33:56, Train, Epoch : 10, Step : 7310, Loss : 0.35177, Acc : 0.853, Sensitive_Loss : 0.01312, Sensitive_Acc : 0.466, Run Time : 135.59 sec
INFO:root:2024-03-27 16:34:07, Train, Epoch : 10, Step : 7320, Loss : 0.32132, Acc : 0.825, Sensitive_Loss : 0.03618, Sensitive_Acc : 0.500, Run Time : 11.04 sec
INFO:root:2024-03-27 16:34:17, Train, Epoch : 10, Step : 7330, Loss : 0.32390, Acc : 0.838, Sensitive_Loss : 0.02715, Sensitive_Acc : 0.509, Run Time : 10.11 sec
INFO:root:2024-03-27 16:34:27, Train, Epoch : 10, Step : 7340, Loss : 0.39508, Acc : 0.822, Sensitive_Loss : 0.03100, Sensitive_Acc : 0.472, Run Time : 9.79 sec
INFO:root:2024-03-27 16:34:38, Train, Epoch : 10, Step : 7350, Loss : 0.35782, Acc : 0.847, Sensitive_Loss : 0.03539, Sensitive_Acc : 0.516, Run Time : 11.38 sec
INFO:root:2024-03-27 16:34:48, Train, Epoch : 10, Step : 7360, Loss : 0.32378, Acc : 0.881, Sensitive_Loss : 0.01799, Sensitive_Acc : 0.503, Run Time : 9.54 sec
INFO:root:2024-03-27 16:34:58, Train, Epoch : 10, Step : 7370, Loss : 0.29253, Acc : 0.828, Sensitive_Loss : 0.02436, Sensitive_Acc : 0.481, Run Time : 10.09 sec
INFO:root:2024-03-27 16:35:08, Train, Epoch : 10, Step : 7380, Loss : 0.36766, Acc : 0.831, Sensitive_Loss : 0.01402, Sensitive_Acc : 0.522, Run Time : 10.34 sec
INFO:root:2024-03-27 16:35:18, Train, Epoch : 10, Step : 7390, Loss : 0.37709, Acc : 0.841, Sensitive_Loss : 0.02220, Sensitive_Acc : 0.506, Run Time : 9.73 sec
INFO:root:2024-03-27 16:35:28, Train, Epoch : 10, Step : 7400, Loss : 0.31712, Acc : 0.866, Sensitive_Loss : 0.04690, Sensitive_Acc : 0.509, Run Time : 9.55 sec
INFO:root:2024-03-27 16:37:36, Dev, Step : 7400, Loss : 0.57814, Acc : 0.749, Auc : 0.825, Sensitive_Loss : 0.05871, Sensitive_Acc : 0.515, Sensitive_Auc : 0.998, Mean auc: 0.825, Run Time : 128.25 sec
INFO:root:2024-03-27 16:37:43, Train, Epoch : 10, Step : 7410, Loss : 0.32164, Acc : 0.831, Sensitive_Loss : 0.01482, Sensitive_Acc : 0.491, Run Time : 135.58 sec
INFO:root:2024-03-27 16:37:53, Train, Epoch : 10, Step : 7420, Loss : 0.35194, Acc : 0.844, Sensitive_Loss : 0.02769, Sensitive_Acc : 0.534, Run Time : 9.75 sec
INFO:root:2024-03-27 16:38:07, Train, Epoch : 10, Step : 7430, Loss : 0.32735, Acc : 0.853, Sensitive_Loss : 0.02138, Sensitive_Acc : 0.453, Run Time : 13.68 sec
INFO:root:2024-03-27 16:38:20, Train, Epoch : 10, Step : 7440, Loss : 0.32782, Acc : 0.825, Sensitive_Loss : 0.03854, Sensitive_Acc : 0.487, Run Time : 13.72 sec
INFO:root:2024-03-27 16:38:30, Train, Epoch : 10, Step : 7450, Loss : 0.36958, Acc : 0.816, Sensitive_Loss : 0.02200, Sensitive_Acc : 0.466, Run Time : 9.53 sec
INFO:root:2024-03-27 16:38:42, Train, Epoch : 10, Step : 7460, Loss : 0.34558, Acc : 0.834, Sensitive_Loss : 0.01445, Sensitive_Acc : 0.475, Run Time : 12.55 sec
INFO:root:2024-03-27 16:38:52, Train, Epoch : 10, Step : 7470, Loss : 0.30518, Acc : 0.869, Sensitive_Loss : 0.01869, Sensitive_Acc : 0.491, Run Time : 10.00 sec
INFO:root:2024-03-27 16:39:02, Train, Epoch : 10, Step : 7480, Loss : 0.40696, Acc : 0.806, Sensitive_Loss : 0.02788, Sensitive_Acc : 0.481, Run Time : 9.43 sec
INFO:root:2024-03-27 16:39:13, Train, Epoch : 10, Step : 7490, Loss : 0.37872, Acc : 0.819, Sensitive_Loss : 0.01557, Sensitive_Acc : 0.503, Run Time : 11.53 sec
INFO:root:2024-03-27 16:39:23, Train, Epoch : 10, Step : 7500, Loss : 0.38236, Acc : 0.812, Sensitive_Loss : 0.02209, Sensitive_Acc : 0.481, Run Time : 9.23 sec
INFO:root:2024-03-27 16:41:29, Dev, Step : 7500, Loss : 0.59442, Acc : 0.746, Auc : 0.827, Sensitive_Loss : 0.06071, Sensitive_Acc : 0.507, Sensitive_Auc : 0.998, Mean auc: 0.827, Run Time : 126.24 sec
INFO:root:2024-03-27 16:43:33
INFO:root:y_pred: [0.13570103 0.45920998 0.87778044 ... 0.80137825 0.60730034 0.49238738]
INFO:root:y_true: [1. 1. 1. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [8.1657505e-01 1.7753401e-01 7.5325907e-08 9.9997747e-01 2.1415723e-05
 9.9998319e-01 9.9999940e-01 3.1705748e-04 2.9039054e-04 9.9999988e-01
 9.9995470e-01 7.4876421e-05 1.0414284e-02 6.7211732e-02 7.0232234e-04
 5.0767150e-05 1.0343419e-02 1.7676687e-04 5.7284876e-07 5.4357266e-03
 9.9999952e-01 9.3123969e-03 4.1547180e-05 1.6216330e-02 3.3962163e-01
 7.2325885e-01 9.9999988e-01 9.9835485e-01 1.1375827e-02 7.9755374e-04
 1.0052478e-02 9.9999905e-01 9.9999285e-01 9.9706972e-01 4.9227206e-03
 9.9988532e-01 2.5431851e-07 3.3804048e-03 9.9996924e-01 2.3297288e-03
 3.9393175e-02 1.2566471e-05 9.8660070e-01 9.9995828e-01 9.9999249e-01
 2.3998278e-01 9.9996781e-01 9.9651587e-01 4.1016486e-01 4.9517621e-06
 3.9502897e-04 3.0576743e-03 5.6611537e-03 9.1658020e-01 9.9999964e-01
 9.9999988e-01 3.0142821e-06 1.4521394e-04 9.9999595e-01 1.6699763e-08
 6.3612577e-05 9.9999762e-01 1.0672702e-06 9.9999940e-01 8.3494788e-06
 7.3163001e-06 2.7685019e-04 1.1815883e-04 1.6328492e-04 9.9997663e-01
 9.9999523e-01 9.9999571e-01 2.1606209e-02 9.9999964e-01 2.7975419e-01
 9.2047889e-04 9.9999976e-01 8.3071372e-04 6.6541219e-07 9.9995387e-01
 2.6483939e-03 1.3507891e-04 9.9866903e-01 9.3151648e-06 9.8969787e-01
 4.8124664e-05 3.0051776e-06 2.5355641e-07 9.9999344e-01 9.9766219e-01
 4.1197858e-05 5.0253947e-03 9.9961346e-01 2.4673130e-04 9.9999893e-01
 3.1675852e-03 9.9999237e-01 6.8467134e-04 8.6109287e-01 3.7740174e-06
 4.1960943e-02 6.7616621e-04 1.2456529e-03 2.0960156e-02 9.9999893e-01
 4.2666002e-06 3.0828694e-01 7.8994408e-06 5.0847535e-04 4.2643752e-02
 3.2917301e-06 3.1674025e-04 4.9823648e-05 6.8770501e-06 9.9999285e-01
 6.3789405e-05 1.3317612e-05 1.4268012e-03 6.8928598e-04 1.0787345e-03
 9.5020980e-04 9.9999988e-01 9.9828196e-01 9.9999988e-01 3.7738871e-02
 1.6806759e-04 3.7626555e-04 1.6331664e-05 1.1774320e-05 1.0940043e-05
 3.4224935e-02 9.5872641e-01 3.5781411e-03 9.9952960e-01 5.1529333e-03
 5.7186076e-04 8.5323962e-05 9.9999869e-01 5.8307767e-02 9.9999988e-01
 6.8383140e-04 9.9998939e-01 9.9940109e-01 1.0640748e-03 9.9999952e-01
 1.0703182e-01 9.9999547e-01 9.9999356e-01 9.9999988e-01 9.9999976e-01
 5.0688857e-01 1.2388380e-06 2.0029783e-04 9.9252447e-06 9.9997318e-01
 4.8433109e-03 9.9984097e-01 2.5553398e-02 9.9973506e-01 9.9957293e-01
 4.4050303e-06 4.3828776e-03 3.3577351e-05 9.9986196e-01 4.4419546e-02
 9.9988377e-01 9.9999261e-01 3.8451294e-06 1.0000000e+00 7.6818476e-05
 9.9999094e-01 9.9991727e-01 3.3033406e-04 9.9999976e-01 9.9998701e-01
 1.0000000e+00 4.5598543e-05 4.4291685e-03 9.9999452e-01 3.5692710e-01
 3.8898124e-05 4.7347389e-04 6.0587929e-05 1.5477164e-04 3.2001273e-05
 9.9998844e-01 9.9969566e-01 1.4396152e-02]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.
 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1.
 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.
 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0.
 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.
 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.
 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]
INFO:root:2024-03-27 16:43:33, Dev, Step : 7500, Loss : 0.59442, Acc : 0.746, Auc : 0.827, Sensitive_Loss : 0.06071, Sensitive_Acc : 0.507, Sensitive_Auc : 0.998, Mean auc: 0.827, Run Time : 124.10 sec
