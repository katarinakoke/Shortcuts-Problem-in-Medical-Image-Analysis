Running on desktop18:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
3
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_val.csv",
    "pred_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Balanced_Sex_0_0.csv",
    "pred_model": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Balanced_Sex_0_01.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.05,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-18 08:27:56, Train, Epoch : 1, Step : 10, Loss : 0.70192, Acc : 0.547, Sensitive_Loss : 1.12740, Sensitive_Acc : 12.800, Run Time : 20.97 sec
INFO:root:2024-04-18 08:28:14, Train, Epoch : 1, Step : 20, Loss : 0.67461, Acc : 0.622, Sensitive_Loss : 1.04125, Sensitive_Acc : 19.400, Run Time : 18.17 sec
INFO:root:2024-04-18 08:28:32, Train, Epoch : 1, Step : 30, Loss : 0.65582, Acc : 0.622, Sensitive_Loss : 1.03059, Sensitive_Acc : 17.400, Run Time : 18.17 sec
INFO:root:2024-04-18 08:28:50, Train, Epoch : 1, Step : 40, Loss : 0.65409, Acc : 0.637, Sensitive_Loss : 0.91288, Sensitive_Acc : 20.100, Run Time : 17.87 sec
INFO:root:2024-04-18 08:29:07, Train, Epoch : 1, Step : 50, Loss : 0.66985, Acc : 0.559, Sensitive_Loss : 0.94146, Sensitive_Acc : 21.300, Run Time : 16.99 sec
INFO:root:2024-04-18 08:29:24, Train, Epoch : 1, Step : 60, Loss : 0.67139, Acc : 0.637, Sensitive_Loss : 0.92118, Sensitive_Acc : 18.800, Run Time : 16.76 sec
INFO:root:2024-04-18 08:29:42, Train, Epoch : 1, Step : 70, Loss : 0.68124, Acc : 0.581, Sensitive_Loss : 0.84188, Sensitive_Acc : 16.300, Run Time : 17.62 sec
INFO:root:2024-04-18 08:29:59, Train, Epoch : 1, Step : 80, Loss : 0.66471, Acc : 0.659, Sensitive_Loss : 0.80883, Sensitive_Acc : 13.300, Run Time : 17.12 sec
INFO:root:2024-04-18 08:30:17, Train, Epoch : 1, Step : 90, Loss : 0.70235, Acc : 0.637, Sensitive_Loss : 0.80645, Sensitive_Acc : 20.000, Run Time : 18.18 sec
INFO:root:2024-04-18 08:30:34, Train, Epoch : 1, Step : 100, Loss : 0.68864, Acc : 0.634, Sensitive_Loss : 0.84250, Sensitive_Acc : 20.200, Run Time : 16.67 sec
INFO:root:2024-04-18 08:34:27, Dev, Step : 100, Loss : 0.68702, Acc : 0.611, Auc : 0.721, Sensitive_Loss : 0.75592, Sensitive_Acc : 18.729, Sensitive_Auc : 0.840, Mean auc: 0.721, Run Time : 233.12 sec
INFO:root:2024-04-18 08:34:27, Best, Step : 100, Loss : 0.68702, Acc : 0.611, Auc : 0.721, Sensitive_Loss : 0.75592, Sensitive_Acc : 18.729, Sensitive_Auc : 0.840, Best Auc : 0.721
INFO:root:2024-04-18 08:34:39, Train, Epoch : 1, Step : 110, Loss : 0.60772, Acc : 0.681, Sensitive_Loss : 0.69680, Sensitive_Acc : 20.100, Run Time : 245.64 sec
INFO:root:2024-04-18 08:34:58, Train, Epoch : 1, Step : 120, Loss : 0.57702, Acc : 0.688, Sensitive_Loss : 0.59093, Sensitive_Acc : 18.800, Run Time : 18.52 sec
INFO:root:2024-04-18 08:35:14, Train, Epoch : 1, Step : 130, Loss : 0.65783, Acc : 0.669, Sensitive_Loss : 0.73979, Sensitive_Acc : 17.300, Run Time : 16.58 sec
INFO:root:2024-04-18 08:35:32, Train, Epoch : 1, Step : 140, Loss : 0.70756, Acc : 0.672, Sensitive_Loss : 0.67849, Sensitive_Acc : 19.000, Run Time : 17.25 sec
INFO:root:2024-04-18 08:35:51, Train, Epoch : 1, Step : 150, Loss : 0.58901, Acc : 0.669, Sensitive_Loss : 0.53469, Sensitive_Acc : 19.600, Run Time : 19.18 sec
INFO:root:2024-04-18 08:36:09, Train, Epoch : 1, Step : 160, Loss : 0.59031, Acc : 0.675, Sensitive_Loss : 0.53110, Sensitive_Acc : 15.800, Run Time : 18.06 sec
INFO:root:2024-04-18 08:36:26, Train, Epoch : 1, Step : 170, Loss : 0.63863, Acc : 0.681, Sensitive_Loss : 0.70059, Sensitive_Acc : 20.300, Run Time : 17.13 sec
INFO:root:2024-04-18 08:36:44, Train, Epoch : 1, Step : 180, Loss : 0.55800, Acc : 0.697, Sensitive_Loss : 0.46438, Sensitive_Acc : 23.700, Run Time : 18.07 sec
INFO:root:2024-04-18 08:37:01, Train, Epoch : 1, Step : 190, Loss : 0.65351, Acc : 0.659, Sensitive_Loss : 0.49199, Sensitive_Acc : 18.000, Run Time : 17.09 sec
INFO:root:2024-04-18 08:37:20, Train, Epoch : 1, Step : 200, Loss : 0.62608, Acc : 0.694, Sensitive_Loss : 0.47752, Sensitive_Acc : 21.200, Run Time : 18.89 sec
INFO:root:2024-04-18 08:41:12, Dev, Step : 200, Loss : 0.66388, Acc : 0.677, Auc : 0.752, Sensitive_Loss : 0.42113, Sensitive_Acc : 20.729, Sensitive_Auc : 0.941, Mean auc: 0.752, Run Time : 231.72 sec
INFO:root:2024-04-18 08:41:12, Best, Step : 200, Loss : 0.66388, Acc : 0.677, Auc : 0.752, Sensitive_Loss : 0.42113, Sensitive_Acc : 20.729, Sensitive_Auc : 0.941, Best Auc : 0.752
INFO:root:2024-04-18 08:41:25, Train, Epoch : 1, Step : 210, Loss : 0.56744, Acc : 0.691, Sensitive_Loss : 0.49459, Sensitive_Acc : 21.700, Run Time : 245.45 sec
INFO:root:2024-04-18 08:41:43, Train, Epoch : 1, Step : 220, Loss : 0.70041, Acc : 0.644, Sensitive_Loss : 0.60489, Sensitive_Acc : 18.900, Run Time : 17.89 sec
INFO:root:2024-04-18 08:42:01, Train, Epoch : 1, Step : 230, Loss : 0.67065, Acc : 0.662, Sensitive_Loss : 0.55242, Sensitive_Acc : 19.600, Run Time : 17.15 sec
INFO:root:2024-04-18 08:42:19, Train, Epoch : 1, Step : 240, Loss : 0.60558, Acc : 0.709, Sensitive_Loss : 0.51529, Sensitive_Acc : 24.500, Run Time : 18.08 sec
INFO:root:2024-04-18 08:42:37, Train, Epoch : 1, Step : 250, Loss : 0.56853, Acc : 0.675, Sensitive_Loss : 0.40058, Sensitive_Acc : 18.500, Run Time : 18.47 sec
INFO:root:2024-04-18 08:42:55, Train, Epoch : 1, Step : 260, Loss : 0.59662, Acc : 0.691, Sensitive_Loss : 0.41007, Sensitive_Acc : 19.700, Run Time : 17.53 sec
INFO:root:2024-04-18 08:43:11, Train, Epoch : 1, Step : 270, Loss : 0.59960, Acc : 0.672, Sensitive_Loss : 0.48995, Sensitive_Acc : 18.600, Run Time : 16.55 sec
INFO:root:2024-04-18 08:43:29, Train, Epoch : 1, Step : 280, Loss : 0.53598, Acc : 0.700, Sensitive_Loss : 0.42310, Sensitive_Acc : 19.900, Run Time : 17.73 sec
INFO:root:2024-04-18 08:43:46, Train, Epoch : 1, Step : 290, Loss : 0.56891, Acc : 0.706, Sensitive_Loss : 0.33676, Sensitive_Acc : 23.300, Run Time : 16.64 sec
INFO:root:2024-04-18 08:44:02, Train, Epoch : 1, Step : 300, Loss : 0.60007, Acc : 0.709, Sensitive_Loss : 0.39373, Sensitive_Acc : 21.200, Run Time : 16.31 sec
INFO:root:2024-04-18 08:47:56, Dev, Step : 300, Loss : 0.77537, Acc : 0.652, Auc : 0.781, Sensitive_Loss : 0.37519, Sensitive_Acc : 21.541, Sensitive_Auc : 0.948, Mean auc: 0.781, Run Time : 233.93 sec
INFO:root:2024-04-18 08:47:56, Best, Step : 300, Loss : 0.77537, Acc : 0.652, Auc : 0.781, Sensitive_Loss : 0.37519, Sensitive_Acc : 21.541, Sensitive_Auc : 0.948, Best Auc : 0.781
INFO:root:2024-04-18 08:48:10, Train, Epoch : 1, Step : 310, Loss : 0.57017, Acc : 0.734, Sensitive_Loss : 0.43291, Sensitive_Acc : 20.800, Run Time : 247.75 sec
INFO:root:2024-04-18 08:48:27, Train, Epoch : 1, Step : 320, Loss : 0.54078, Acc : 0.700, Sensitive_Loss : 0.41797, Sensitive_Acc : 19.100, Run Time : 17.41 sec
INFO:root:2024-04-18 08:48:45, Train, Epoch : 1, Step : 330, Loss : 0.64826, Acc : 0.681, Sensitive_Loss : 0.39069, Sensitive_Acc : 18.200, Run Time : 18.10 sec
INFO:root:2024-04-18 08:49:03, Train, Epoch : 1, Step : 340, Loss : 0.58327, Acc : 0.703, Sensitive_Loss : 0.42227, Sensitive_Acc : 23.600, Run Time : 17.72 sec
INFO:root:2024-04-18 08:49:20, Train, Epoch : 1, Step : 350, Loss : 0.63287, Acc : 0.741, Sensitive_Loss : 0.37184, Sensitive_Acc : 22.300, Run Time : 16.79 sec
INFO:root:2024-04-18 08:49:36, Train, Epoch : 1, Step : 360, Loss : 0.63064, Acc : 0.719, Sensitive_Loss : 0.35880, Sensitive_Acc : 19.300, Run Time : 16.84 sec
INFO:root:2024-04-18 08:49:53, Train, Epoch : 1, Step : 370, Loss : 0.61346, Acc : 0.672, Sensitive_Loss : 0.32381, Sensitive_Acc : 24.200, Run Time : 16.84 sec
INFO:root:2024-04-18 08:50:11, Train, Epoch : 1, Step : 380, Loss : 0.59418, Acc : 0.625, Sensitive_Loss : 0.50845, Sensitive_Acc : 20.300, Run Time : 17.64 sec
INFO:root:2024-04-18 08:50:28, Train, Epoch : 1, Step : 390, Loss : 0.56714, Acc : 0.719, Sensitive_Loss : 0.38168, Sensitive_Acc : 24.200, Run Time : 17.19 sec
INFO:root:2024-04-18 08:50:46, Train, Epoch : 1, Step : 400, Loss : 0.56719, Acc : 0.722, Sensitive_Loss : 0.33578, Sensitive_Acc : 26.400, Run Time : 18.33 sec
INFO:root:2024-04-18 08:54:39, Dev, Step : 400, Loss : 0.60221, Acc : 0.714, Auc : 0.791, Sensitive_Loss : 0.30805, Sensitive_Acc : 21.391, Sensitive_Auc : 0.968, Mean auc: 0.791, Run Time : 232.45 sec
INFO:root:2024-04-18 08:54:40, Best, Step : 400, Loss : 0.60221, Acc : 0.714, Auc : 0.791, Sensitive_Loss : 0.30805, Sensitive_Acc : 21.391, Sensitive_Auc : 0.968, Best Auc : 0.791
INFO:root:2024-04-18 08:54:52, Train, Epoch : 1, Step : 410, Loss : 0.59740, Acc : 0.722, Sensitive_Loss : 0.36084, Sensitive_Acc : 21.800, Run Time : 245.71 sec
INFO:root:2024-04-18 08:55:10, Train, Epoch : 1, Step : 420, Loss : 0.61318, Acc : 0.719, Sensitive_Loss : 0.32848, Sensitive_Acc : 21.500, Run Time : 17.66 sec
INFO:root:2024-04-18 08:55:28, Train, Epoch : 1, Step : 430, Loss : 0.54851, Acc : 0.681, Sensitive_Loss : 0.40721, Sensitive_Acc : 19.800, Run Time : 17.88 sec
INFO:root:2024-04-18 08:55:45, Train, Epoch : 1, Step : 440, Loss : 0.50987, Acc : 0.744, Sensitive_Loss : 0.37081, Sensitive_Acc : 18.600, Run Time : 17.71 sec
INFO:root:2024-04-18 08:56:03, Train, Epoch : 1, Step : 450, Loss : 0.55297, Acc : 0.734, Sensitive_Loss : 0.56356, Sensitive_Acc : 23.800, Run Time : 17.59 sec
INFO:root:2024-04-18 08:56:20, Train, Epoch : 1, Step : 460, Loss : 0.51596, Acc : 0.744, Sensitive_Loss : 0.49618, Sensitive_Acc : 22.200, Run Time : 17.09 sec
INFO:root:2024-04-18 08:56:39, Train, Epoch : 1, Step : 470, Loss : 0.54756, Acc : 0.731, Sensitive_Loss : 0.36155, Sensitive_Acc : 19.400, Run Time : 18.50 sec
INFO:root:2024-04-18 08:56:55, Train, Epoch : 1, Step : 480, Loss : 0.61900, Acc : 0.691, Sensitive_Loss : 0.31076, Sensitive_Acc : 20.400, Run Time : 16.19 sec
INFO:root:2024-04-18 08:57:12, Train, Epoch : 1, Step : 490, Loss : 0.63284, Acc : 0.703, Sensitive_Loss : 0.39314, Sensitive_Acc : 19.800, Run Time : 17.52 sec
INFO:root:2024-04-18 08:57:29, Train, Epoch : 1, Step : 500, Loss : 0.53030, Acc : 0.706, Sensitive_Loss : 0.29368, Sensitive_Acc : 18.800, Run Time : 16.18 sec
INFO:root:2024-04-18 09:01:22, Dev, Step : 500, Loss : 0.57778, Acc : 0.719, Auc : 0.799, Sensitive_Loss : 0.37086, Sensitive_Acc : 19.211, Sensitive_Auc : 0.978, Mean auc: 0.799, Run Time : 233.56 sec
INFO:root:2024-04-18 09:01:23, Best, Step : 500, Loss : 0.57778, Acc : 0.719, Auc : 0.799, Sensitive_Loss : 0.37086, Sensitive_Acc : 19.211, Sensitive_Auc : 0.978, Best Auc : 0.799
INFO:root:2024-04-18 09:01:35, Train, Epoch : 1, Step : 510, Loss : 0.58026, Acc : 0.713, Sensitive_Loss : 0.30692, Sensitive_Acc : 21.600, Run Time : 246.79 sec
INFO:root:2024-04-18 09:01:53, Train, Epoch : 1, Step : 520, Loss : 0.55484, Acc : 0.744, Sensitive_Loss : 0.29676, Sensitive_Acc : 18.400, Run Time : 17.70 sec
INFO:root:2024-04-18 09:02:11, Train, Epoch : 1, Step : 530, Loss : 0.58349, Acc : 0.697, Sensitive_Loss : 0.34869, Sensitive_Acc : 22.200, Run Time : 18.38 sec
INFO:root:2024-04-18 09:02:28, Train, Epoch : 1, Step : 540, Loss : 0.60453, Acc : 0.669, Sensitive_Loss : 0.27435, Sensitive_Acc : 18.500, Run Time : 16.56 sec
INFO:root:2024-04-18 09:02:45, Train, Epoch : 1, Step : 550, Loss : 0.58656, Acc : 0.694, Sensitive_Loss : 0.34794, Sensitive_Acc : 25.700, Run Time : 16.58 sec
INFO:root:2024-04-18 09:03:03, Train, Epoch : 1, Step : 560, Loss : 0.53537, Acc : 0.706, Sensitive_Loss : 0.23304, Sensitive_Acc : 21.900, Run Time : 18.03 sec
INFO:root:2024-04-18 09:03:20, Train, Epoch : 1, Step : 570, Loss : 0.62581, Acc : 0.709, Sensitive_Loss : 0.27592, Sensitive_Acc : 19.500, Run Time : 17.26 sec
INFO:root:2024-04-18 09:03:39, Train, Epoch : 1, Step : 580, Loss : 0.52149, Acc : 0.759, Sensitive_Loss : 0.37191, Sensitive_Acc : 19.200, Run Time : 18.91 sec
INFO:root:2024-04-18 09:03:55, Train, Epoch : 1, Step : 590, Loss : 0.54393, Acc : 0.728, Sensitive_Loss : 0.33025, Sensitive_Acc : 18.100, Run Time : 16.73 sec
INFO:root:2024-04-18 09:04:12, Train, Epoch : 1, Step : 600, Loss : 0.62077, Acc : 0.725, Sensitive_Loss : 0.34741, Sensitive_Acc : 17.700, Run Time : 17.02 sec
INFO:root:2024-04-18 09:08:09, Dev, Step : 600, Loss : 0.64981, Acc : 0.695, Auc : 0.794, Sensitive_Loss : 0.30437, Sensitive_Acc : 20.579, Sensitive_Auc : 0.981, Mean auc: 0.794, Run Time : 236.90 sec
INFO:root:2024-04-18 09:08:22, Train, Epoch : 1, Step : 610, Loss : 0.54841, Acc : 0.728, Sensitive_Loss : 0.28935, Sensitive_Acc : 22.100, Run Time : 250.04 sec
INFO:root:2024-04-18 09:08:41, Train, Epoch : 1, Step : 620, Loss : 0.59348, Acc : 0.753, Sensitive_Loss : 0.37013, Sensitive_Acc : 21.500, Run Time : 18.28 sec
INFO:root:2024-04-18 09:08:57, Train, Epoch : 1, Step : 630, Loss : 0.56207, Acc : 0.759, Sensitive_Loss : 0.27250, Sensitive_Acc : 14.200, Run Time : 16.49 sec
INFO:root:2024-04-18 09:13:11
INFO:root:y_pred: [0.0868576  0.02391407 0.20119731 ... 0.16693045 0.08097939 0.12559718]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [4.25895900e-02 6.90713525e-02 2.05384046e-02 3.01278173e-03
 7.83063918e-02 7.24201724e-02 3.40325013e-02 4.68220469e-03
 2.21271664e-01 9.99283016e-01 2.57667542e-01 1.01307735e-01
 2.86245774e-02 6.09007431e-04 9.92323637e-01 8.08728114e-02
 1.06733516e-02 9.97624457e-01 9.94832754e-01 1.17608845e-01
 6.78710878e-01 5.35930647e-03 3.23957592e-01 3.28122318e-01
 7.11306706e-02 3.41554314e-01 1.32153509e-03 6.02474390e-03
 1.34691189e-03 1.02431662e-02 2.35671178e-01 9.89352465e-01
 9.08791833e-03 6.75375164e-01 3.71170277e-03 3.69170238e-03
 5.73014328e-03 3.33357722e-01 3.12433153e-01 1.23077512e-01
 7.94477344e-01 9.97691631e-01 1.52655274e-01 1.42693715e-02
 9.52038884e-01 4.37475652e-01 9.56794739e-01 3.48819464e-01
 2.69624501e-01 9.88904655e-01 8.39276433e-01 9.96596396e-01
 9.92545187e-01 1.71881095e-02 9.53271333e-03 2.07990229e-01
 1.78996827e-02 2.22092450e-01 9.97307777e-01 4.45096642e-02
 1.36346626e-03 1.09805769e-05 1.56634729e-02 7.06671504e-04
 9.88530576e-01 5.80994263e-02 5.17518565e-05 4.34744447e-01
 8.64117034e-03 8.88942778e-01 9.97544348e-01 9.97654259e-01
 1.32636994e-01 6.21639550e-01 2.67698523e-02 4.27174360e-01
 4.08733279e-01 1.73071714e-03 1.05913635e-03 1.32707274e-02
 2.16704607e-02 2.23328895e-03 8.36736560e-01 9.73067403e-01
 2.91876402e-02 1.33626968e-01 5.36716759e-01 4.48903022e-03
 4.19012457e-02 1.12035330e-02 2.39537247e-02 6.44763112e-01
 2.47221742e-05 3.52212111e-03 1.60887346e-01 3.25819897e-03
 1.10316696e-03 8.79405975e-01 5.54410415e-03 3.32829237e-01
 9.89646558e-03 2.95887679e-01 6.09685659e-01 1.18129812e-02
 6.14567876e-01 1.95976757e-02 4.92584705e-01 9.19857383e-01
 2.72443056e-01 5.72970748e-01 3.10837422e-05 9.99529123e-01
 9.95685101e-01 1.44146412e-04 4.03586030e-01 1.72972605e-01
 4.54876363e-01 1.77648279e-03 5.66531241e-01 9.55349114e-03
 4.43509780e-03 3.04291007e-05 2.28001028e-02 1.04729983e-03
 9.41249728e-02 8.83922100e-01 6.92006957e-04 9.50007796e-01
 1.84177890e-01 1.61053196e-01 2.51128580e-02 3.29091698e-01
 2.34673987e-03]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 09:13:11, Dev, Step : 634, Loss : 0.61264, Acc : 0.720, Auc : 0.803, Sensitive_Loss : 0.29617, Sensitive_Acc : 20.639, Sensitive_Auc : 0.992, Mean auc: 0.803, Run Time : 247.42 sec
INFO:root:2024-04-18 09:13:12, Best, Step : 634, Loss : 0.61264, Acc : 0.720,Auc : 0.803, Best Auc : 0.803, Sensitive_Loss : 0.29617, Sensitive_Acc : 20.639, Sensitive_Auc : 0.992
INFO:root:2024-04-18 09:13:29, Train, Epoch : 2, Step : 640, Loss : 0.31060, Acc : 0.459, Sensitive_Loss : 0.12724, Sensitive_Acc : 9.800, Run Time : 15.48 sec
INFO:root:2024-04-18 09:13:49, Train, Epoch : 2, Step : 650, Loss : 0.60155, Acc : 0.709, Sensitive_Loss : 0.23220, Sensitive_Acc : 17.600, Run Time : 20.49 sec
INFO:root:2024-04-18 09:14:12, Train, Epoch : 2, Step : 660, Loss : 0.51482, Acc : 0.741, Sensitive_Loss : 0.20275, Sensitive_Acc : 22.500, Run Time : 23.16 sec
INFO:root:2024-04-18 09:14:33, Train, Epoch : 2, Step : 670, Loss : 0.55736, Acc : 0.747, Sensitive_Loss : 0.27639, Sensitive_Acc : 20.100, Run Time : 20.99 sec
INFO:root:2024-04-18 09:14:56, Train, Epoch : 2, Step : 680, Loss : 0.56551, Acc : 0.722, Sensitive_Loss : 0.28014, Sensitive_Acc : 18.300, Run Time : 22.33 sec
INFO:root:2024-04-18 09:15:12, Train, Epoch : 2, Step : 690, Loss : 0.51184, Acc : 0.741, Sensitive_Loss : 0.26567, Sensitive_Acc : 18.300, Run Time : 16.00 sec
INFO:root:2024-04-18 09:15:31, Train, Epoch : 2, Step : 700, Loss : 0.56686, Acc : 0.719, Sensitive_Loss : 0.27384, Sensitive_Acc : 25.600, Run Time : 19.51 sec
INFO:root:2024-04-18 09:19:52, Dev, Step : 700, Loss : 0.58060, Acc : 0.733, Auc : 0.807, Sensitive_Loss : 0.67376, Sensitive_Acc : 16.398, Sensitive_Auc : 0.970, Mean auc: 0.807, Run Time : 260.38 sec
INFO:root:2024-04-18 09:19:52, Best, Step : 700, Loss : 0.58060, Acc : 0.733, Auc : 0.807, Sensitive_Loss : 0.67376, Sensitive_Acc : 16.398, Sensitive_Auc : 0.970, Best Auc : 0.807
INFO:root:2024-04-18 09:20:05, Train, Epoch : 2, Step : 710, Loss : 0.53130, Acc : 0.738, Sensitive_Loss : 0.20925, Sensitive_Acc : 20.900, Run Time : 273.53 sec
INFO:root:2024-04-18 09:20:23, Train, Epoch : 2, Step : 720, Loss : 0.58758, Acc : 0.731, Sensitive_Loss : 0.34023, Sensitive_Acc : 15.800, Run Time : 18.09 sec
INFO:root:2024-04-18 09:20:41, Train, Epoch : 2, Step : 730, Loss : 0.48890, Acc : 0.775, Sensitive_Loss : 0.22604, Sensitive_Acc : 24.200, Run Time : 17.62 sec
INFO:root:2024-04-18 09:20:57, Train, Epoch : 2, Step : 740, Loss : 0.62003, Acc : 0.703, Sensitive_Loss : 0.29128, Sensitive_Acc : 22.000, Run Time : 16.28 sec
INFO:root:2024-04-18 09:21:15, Train, Epoch : 2, Step : 750, Loss : 0.52863, Acc : 0.731, Sensitive_Loss : 0.21391, Sensitive_Acc : 22.300, Run Time : 17.85 sec
INFO:root:2024-04-18 09:21:32, Train, Epoch : 2, Step : 760, Loss : 0.53790, Acc : 0.734, Sensitive_Loss : 0.28899, Sensitive_Acc : 20.900, Run Time : 17.57 sec
INFO:root:2024-04-18 09:21:49, Train, Epoch : 2, Step : 770, Loss : 0.59886, Acc : 0.747, Sensitive_Loss : 0.27529, Sensitive_Acc : 24.000, Run Time : 16.75 sec
INFO:root:2024-04-18 09:22:07, Train, Epoch : 2, Step : 780, Loss : 0.48367, Acc : 0.769, Sensitive_Loss : 0.27989, Sensitive_Acc : 21.100, Run Time : 17.74 sec
INFO:root:2024-04-18 09:22:25, Train, Epoch : 2, Step : 790, Loss : 0.54163, Acc : 0.744, Sensitive_Loss : 0.30076, Sensitive_Acc : 23.500, Run Time : 18.14 sec
INFO:root:2024-04-18 09:22:42, Train, Epoch : 2, Step : 800, Loss : 0.53148, Acc : 0.762, Sensitive_Loss : 0.27735, Sensitive_Acc : 21.400, Run Time : 17.40 sec
INFO:root:2024-04-18 09:26:53, Dev, Step : 800, Loss : 0.60586, Acc : 0.687, Auc : 0.820, Sensitive_Loss : 0.23405, Sensitive_Acc : 21.436, Sensitive_Auc : 0.989, Mean auc: 0.820, Run Time : 250.31 sec
INFO:root:2024-04-18 09:26:58, Best, Step : 800, Loss : 0.60586, Acc : 0.687, Auc : 0.820, Sensitive_Loss : 0.23405, Sensitive_Acc : 21.436, Sensitive_Auc : 0.989, Best Auc : 0.820
INFO:root:2024-04-18 09:27:11, Train, Epoch : 2, Step : 810, Loss : 0.55567, Acc : 0.741, Sensitive_Loss : 0.25150, Sensitive_Acc : 18.800, Run Time : 268.28 sec
INFO:root:2024-04-18 09:27:29, Train, Epoch : 2, Step : 820, Loss : 0.60633, Acc : 0.766, Sensitive_Loss : 0.14732, Sensitive_Acc : 24.200, Run Time : 18.19 sec
INFO:root:2024-04-18 09:27:47, Train, Epoch : 2, Step : 830, Loss : 0.58419, Acc : 0.728, Sensitive_Loss : 0.22832, Sensitive_Acc : 23.300, Run Time : 18.04 sec
INFO:root:2024-04-18 09:28:07, Train, Epoch : 2, Step : 840, Loss : 0.60844, Acc : 0.703, Sensitive_Loss : 0.18654, Sensitive_Acc : 21.500, Run Time : 20.29 sec
INFO:root:2024-04-18 09:28:32, Train, Epoch : 2, Step : 850, Loss : 0.51569, Acc : 0.722, Sensitive_Loss : 0.22093, Sensitive_Acc : 21.600, Run Time : 24.91 sec
INFO:root:2024-04-18 09:28:49, Train, Epoch : 2, Step : 860, Loss : 0.50182, Acc : 0.722, Sensitive_Loss : 0.32651, Sensitive_Acc : 23.600, Run Time : 16.99 sec
INFO:root:2024-04-18 09:29:06, Train, Epoch : 2, Step : 870, Loss : 0.59665, Acc : 0.716, Sensitive_Loss : 0.22803, Sensitive_Acc : 25.000, Run Time : 17.46 sec
INFO:root:2024-04-18 09:29:24, Train, Epoch : 2, Step : 880, Loss : 0.56408, Acc : 0.741, Sensitive_Loss : 0.19420, Sensitive_Acc : 24.000, Run Time : 17.52 sec
INFO:root:2024-04-18 09:29:42, Train, Epoch : 2, Step : 890, Loss : 0.52023, Acc : 0.775, Sensitive_Loss : 0.17096, Sensitive_Acc : 22.300, Run Time : 18.48 sec
INFO:root:2024-04-18 09:30:01, Train, Epoch : 2, Step : 900, Loss : 0.53972, Acc : 0.734, Sensitive_Loss : 0.27472, Sensitive_Acc : 24.900, Run Time : 18.62 sec
INFO:root:2024-04-18 09:33:51, Dev, Step : 900, Loss : 0.55338, Acc : 0.746, Auc : 0.822, Sensitive_Loss : 0.33262, Sensitive_Acc : 19.481, Sensitive_Auc : 0.992, Mean auc: 0.822, Run Time : 230.36 sec
INFO:root:2024-04-18 09:33:52, Best, Step : 900, Loss : 0.55338, Acc : 0.746, Auc : 0.822, Sensitive_Loss : 0.33262, Sensitive_Acc : 19.481, Sensitive_Auc : 0.992, Best Auc : 0.822
INFO:root:2024-04-18 09:34:04, Train, Epoch : 2, Step : 910, Loss : 0.51775, Acc : 0.778, Sensitive_Loss : 0.17309, Sensitive_Acc : 19.000, Run Time : 242.77 sec
INFO:root:2024-04-18 09:34:22, Train, Epoch : 2, Step : 920, Loss : 0.49175, Acc : 0.728, Sensitive_Loss : 0.20227, Sensitive_Acc : 18.100, Run Time : 18.15 sec
INFO:root:2024-04-18 09:34:39, Train, Epoch : 2, Step : 930, Loss : 0.50968, Acc : 0.750, Sensitive_Loss : 0.25234, Sensitive_Acc : 23.600, Run Time : 16.87 sec
INFO:root:2024-04-18 09:34:58, Train, Epoch : 2, Step : 940, Loss : 0.54736, Acc : 0.734, Sensitive_Loss : 0.31329, Sensitive_Acc : 23.900, Run Time : 19.13 sec
INFO:root:2024-04-18 09:35:16, Train, Epoch : 2, Step : 950, Loss : 0.57177, Acc : 0.734, Sensitive_Loss : 0.20525, Sensitive_Acc : 20.400, Run Time : 17.96 sec
INFO:root:2024-04-18 09:35:31, Train, Epoch : 2, Step : 960, Loss : 0.50988, Acc : 0.747, Sensitive_Loss : 0.27310, Sensitive_Acc : 25.000, Run Time : 15.26 sec
INFO:root:2024-04-18 09:35:51, Train, Epoch : 2, Step : 970, Loss : 0.48174, Acc : 0.759, Sensitive_Loss : 0.18804, Sensitive_Acc : 20.100, Run Time : 19.59 sec
INFO:root:2024-04-18 09:36:08, Train, Epoch : 2, Step : 980, Loss : 0.59350, Acc : 0.731, Sensitive_Loss : 0.21037, Sensitive_Acc : 18.200, Run Time : 16.93 sec
INFO:root:2024-04-18 09:36:23, Train, Epoch : 2, Step : 990, Loss : 0.43580, Acc : 0.772, Sensitive_Loss : 0.22471, Sensitive_Acc : 21.300, Run Time : 15.54 sec
INFO:root:2024-04-18 09:36:41, Train, Epoch : 2, Step : 1000, Loss : 0.56367, Acc : 0.731, Sensitive_Loss : 0.23908, Sensitive_Acc : 18.700, Run Time : 17.41 sec
INFO:root:2024-04-18 09:40:32, Dev, Step : 1000, Loss : 0.57838, Acc : 0.734, Auc : 0.833, Sensitive_Loss : 0.28733, Sensitive_Acc : 20.278, Sensitive_Auc : 0.973, Mean auc: 0.833, Run Time : 230.93 sec
INFO:root:2024-04-18 09:40:33, Best, Step : 1000, Loss : 0.57838, Acc : 0.734, Auc : 0.833, Sensitive_Loss : 0.28733, Sensitive_Acc : 20.278, Sensitive_Auc : 0.973, Best Auc : 0.833
INFO:root:2024-04-18 09:40:45, Train, Epoch : 2, Step : 1010, Loss : 0.47322, Acc : 0.781, Sensitive_Loss : 0.24058, Sensitive_Acc : 23.600, Run Time : 244.06 sec
INFO:root:2024-04-18 09:41:03, Train, Epoch : 2, Step : 1020, Loss : 0.47457, Acc : 0.762, Sensitive_Loss : 0.17903, Sensitive_Acc : 24.800, Run Time : 18.70 sec
INFO:root:2024-04-18 09:41:20, Train, Epoch : 2, Step : 1030, Loss : 0.49272, Acc : 0.750, Sensitive_Loss : 0.26009, Sensitive_Acc : 18.000, Run Time : 16.73 sec
INFO:root:2024-04-18 09:41:39, Train, Epoch : 2, Step : 1040, Loss : 0.50456, Acc : 0.756, Sensitive_Loss : 0.32308, Sensitive_Acc : 19.400, Run Time : 18.59 sec
INFO:root:2024-04-18 09:41:55, Train, Epoch : 2, Step : 1050, Loss : 0.53864, Acc : 0.753, Sensitive_Loss : 0.23138, Sensitive_Acc : 21.500, Run Time : 16.65 sec
INFO:root:2024-04-18 09:42:13, Train, Epoch : 2, Step : 1060, Loss : 0.55764, Acc : 0.731, Sensitive_Loss : 0.27771, Sensitive_Acc : 22.600, Run Time : 17.46 sec
INFO:root:2024-04-18 09:42:31, Train, Epoch : 2, Step : 1070, Loss : 0.59316, Acc : 0.703, Sensitive_Loss : 0.23142, Sensitive_Acc : 21.500, Run Time : 17.91 sec
INFO:root:2024-04-18 09:42:49, Train, Epoch : 2, Step : 1080, Loss : 0.54247, Acc : 0.734, Sensitive_Loss : 0.23653, Sensitive_Acc : 24.100, Run Time : 18.37 sec
INFO:root:2024-04-18 09:43:06, Train, Epoch : 2, Step : 1090, Loss : 0.51756, Acc : 0.741, Sensitive_Loss : 0.19893, Sensitive_Acc : 22.800, Run Time : 16.55 sec
INFO:root:2024-04-18 09:43:24, Train, Epoch : 2, Step : 1100, Loss : 0.52135, Acc : 0.716, Sensitive_Loss : 0.28818, Sensitive_Acc : 22.300, Run Time : 18.02 sec
INFO:root:2024-04-18 09:47:15, Dev, Step : 1100, Loss : 0.55069, Acc : 0.752, Auc : 0.824, Sensitive_Loss : 0.23236, Sensitive_Acc : 21.722, Sensitive_Auc : 0.987, Mean auc: 0.824, Run Time : 231.52 sec
INFO:root:2024-04-18 09:47:28, Train, Epoch : 2, Step : 1110, Loss : 0.56177, Acc : 0.762, Sensitive_Loss : 0.25520, Sensitive_Acc : 19.800, Run Time : 244.69 sec
INFO:root:2024-04-18 09:47:45, Train, Epoch : 2, Step : 1120, Loss : 0.53331, Acc : 0.756, Sensitive_Loss : 0.31543, Sensitive_Acc : 24.200, Run Time : 16.94 sec
INFO:root:2024-04-18 09:48:03, Train, Epoch : 2, Step : 1130, Loss : 0.46979, Acc : 0.784, Sensitive_Loss : 0.21200, Sensitive_Acc : 21.500, Run Time : 17.57 sec
INFO:root:2024-04-18 09:48:21, Train, Epoch : 2, Step : 1140, Loss : 0.54970, Acc : 0.747, Sensitive_Loss : 0.20261, Sensitive_Acc : 19.700, Run Time : 18.04 sec
INFO:root:2024-04-18 09:48:38, Train, Epoch : 2, Step : 1150, Loss : 0.50456, Acc : 0.750, Sensitive_Loss : 0.36148, Sensitive_Acc : 14.500, Run Time : 16.95 sec
INFO:root:2024-04-18 09:48:56, Train, Epoch : 2, Step : 1160, Loss : 0.46139, Acc : 0.797, Sensitive_Loss : 0.25941, Sensitive_Acc : 17.600, Run Time : 18.16 sec
INFO:root:2024-04-18 09:49:14, Train, Epoch : 2, Step : 1170, Loss : 0.56646, Acc : 0.709, Sensitive_Loss : 0.20177, Sensitive_Acc : 13.800, Run Time : 17.46 sec
INFO:root:2024-04-18 09:49:30, Train, Epoch : 2, Step : 1180, Loss : 0.46920, Acc : 0.787, Sensitive_Loss : 0.22938, Sensitive_Acc : 15.000, Run Time : 16.79 sec
INFO:root:2024-04-18 09:49:48, Train, Epoch : 2, Step : 1190, Loss : 0.52199, Acc : 0.722, Sensitive_Loss : 0.17489, Sensitive_Acc : 21.500, Run Time : 17.63 sec
INFO:root:2024-04-18 09:50:05, Train, Epoch : 2, Step : 1200, Loss : 0.51770, Acc : 0.719, Sensitive_Loss : 0.40501, Sensitive_Acc : 21.000, Run Time : 16.91 sec
INFO:root:2024-04-18 09:53:58, Dev, Step : 1200, Loss : 0.54058, Acc : 0.754, Auc : 0.834, Sensitive_Loss : 0.29705, Sensitive_Acc : 20.383, Sensitive_Auc : 0.975, Mean auc: 0.834, Run Time : 233.58 sec
INFO:root:2024-04-18 09:53:59, Best, Step : 1200, Loss : 0.54058, Acc : 0.754, Auc : 0.834, Sensitive_Loss : 0.29705, Sensitive_Acc : 20.383, Sensitive_Auc : 0.975, Best Auc : 0.834
INFO:root:2024-04-18 09:54:12, Train, Epoch : 2, Step : 1210, Loss : 0.58656, Acc : 0.778, Sensitive_Loss : 0.23037, Sensitive_Acc : 20.300, Run Time : 247.27 sec
INFO:root:2024-04-18 09:54:31, Train, Epoch : 2, Step : 1220, Loss : 0.51168, Acc : 0.753, Sensitive_Loss : 0.25655, Sensitive_Acc : 20.400, Run Time : 18.48 sec
INFO:root:2024-04-18 09:54:48, Train, Epoch : 2, Step : 1230, Loss : 0.60685, Acc : 0.728, Sensitive_Loss : 0.14813, Sensitive_Acc : 21.100, Run Time : 16.94 sec
INFO:root:2024-04-18 09:55:05, Train, Epoch : 2, Step : 1240, Loss : 0.55110, Acc : 0.734, Sensitive_Loss : 0.17319, Sensitive_Acc : 25.900, Run Time : 17.28 sec
INFO:root:2024-04-18 09:55:21, Train, Epoch : 2, Step : 1250, Loss : 0.52318, Acc : 0.756, Sensitive_Loss : 0.21403, Sensitive_Acc : 21.700, Run Time : 16.47 sec
INFO:root:2024-04-18 09:55:39, Train, Epoch : 2, Step : 1260, Loss : 0.48923, Acc : 0.719, Sensitive_Loss : 0.20281, Sensitive_Acc : 19.600, Run Time : 17.93 sec
INFO:root:2024-04-18 09:59:41
INFO:root:y_pred: [0.24064629 0.01669758 0.1592555  ... 0.37328026 0.09629218 0.09822644]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [3.9879143e-01 4.9213439e-01 1.9186486e-02 2.3456077e-01 1.9978952e-02
 3.3441722e-02 3.1955975e-01 5.4261755e-02 4.1723379e-01 9.9988139e-01
 9.4470251e-01 2.5662359e-02 7.1084090e-02 5.7244115e-03 9.9993336e-01
 3.4336755e-01 8.5158385e-02 9.9997282e-01 9.9969518e-01 1.9506054e-01
 9.5518833e-01 7.7827767e-02 6.2062418e-01 1.4381930e-01 2.7199042e-01
 7.4494249e-01 1.3006321e-03 2.1570206e-03 3.1355543e-03 2.6379821e-01
 2.9824588e-01 9.9498630e-01 4.8490044e-01 9.0476537e-01 1.8852080e-01
 5.3099128e-03 3.5538432e-01 7.4493366e-01 2.5141501e-01 3.2732329e-01
 4.6951067e-01 9.9911779e-01 9.9657983e-02 1.2399874e-02 9.3015712e-01
 8.8737822e-01 7.1924961e-01 9.3528724e-01 9.0960258e-01 9.9782085e-01
 9.8976117e-01 9.9987829e-01 9.9698645e-01 6.8199940e-02 8.1369454e-01
 6.6961282e-01 3.2376304e-01 8.5287608e-02 9.9761200e-01 1.9239151e-01
 5.6131310e-03 2.6329909e-03 4.2842537e-01 3.3927605e-02 9.9922502e-01
 1.9904533e-01 5.8151135e-04 7.9393053e-01 2.3384126e-01 9.9959522e-01
 9.9997568e-01 9.9991274e-01 9.0513825e-02 9.1902393e-01 3.0081583e-02
 9.5423877e-01 5.7527298e-01 1.6223561e-02 7.9714000e-02 3.5327069e-02
 4.8066926e-01 1.5201755e-03 9.9780124e-01 9.9946481e-01 2.2028433e-02
 3.0629125e-01 4.0772992e-01 1.1543553e-01 1.7072369e-01 3.6227372e-02
 1.6177341e-01 9.7840631e-01 9.1959182e-03 1.5080107e-03 3.6957714e-01
 4.1242361e-01 2.5154386e-02 7.8900015e-01 5.0377530e-01 2.9323238e-01
 3.4757870e-01 1.6109529e-01 9.6366197e-01 3.0745525e-02 6.1585343e-01
 1.2655372e-02 5.2988154e-01 8.2177782e-01 9.9686867e-01 8.0716401e-01
 2.0479689e-04 9.9995458e-01 9.9985611e-01 3.6605191e-03 9.0157174e-02
 9.6007991e-01 6.1664462e-01 3.7396017e-01 8.9126146e-01 1.9642882e-01
 1.7306358e-01 1.5503109e-02 3.0695453e-01 6.3620033e-03 2.8908396e-02
 9.8510748e-01 2.3968076e-02 9.9972397e-01 6.4638227e-01 5.2445060e-01
 6.8534672e-01 9.7356343e-01 1.1794610e-03]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 09:59:41, Dev, Step : 1268, Loss : 0.53944, Acc : 0.757, Auc : 0.832, Sensitive_Loss : 0.61940, Sensitive_Acc : 18.113, Sensitive_Auc : 0.983, Mean auc: 0.832, Run Time : 228.79 sec
INFO:root:2024-04-18 09:59:48, Train, Epoch : 3, Step : 1270, Loss : 0.09641, Acc : 0.156, Sensitive_Loss : 0.03561, Sensitive_Acc : 5.400, Run Time : 6.55 sec
INFO:root:2024-04-18 10:00:06, Train, Epoch : 3, Step : 1280, Loss : 0.42358, Acc : 0.800, Sensitive_Loss : 0.22700, Sensitive_Acc : 23.300, Run Time : 18.01 sec
INFO:root:2024-04-18 10:00:24, Train, Epoch : 3, Step : 1290, Loss : 0.45813, Acc : 0.803, Sensitive_Loss : 0.15763, Sensitive_Acc : 21.000, Run Time : 18.31 sec
INFO:root:2024-04-18 10:00:42, Train, Epoch : 3, Step : 1300, Loss : 0.50288, Acc : 0.769, Sensitive_Loss : 0.16331, Sensitive_Acc : 17.200, Run Time : 17.20 sec
INFO:root:2024-04-18 10:04:33, Dev, Step : 1300, Loss : 0.52295, Acc : 0.768, Auc : 0.844, Sensitive_Loss : 0.29196, Sensitive_Acc : 20.699, Sensitive_Auc : 0.989, Mean auc: 0.844, Run Time : 231.42 sec
INFO:root:2024-04-18 10:04:34, Best, Step : 1300, Loss : 0.52295, Acc : 0.768, Auc : 0.844, Sensitive_Loss : 0.29196, Sensitive_Acc : 20.699, Sensitive_Auc : 0.989, Best Auc : 0.844
INFO:root:2024-04-18 10:04:46, Train, Epoch : 3, Step : 1310, Loss : 0.47550, Acc : 0.791, Sensitive_Loss : 0.20235, Sensitive_Acc : 24.500, Run Time : 244.42 sec
INFO:root:2024-04-18 10:05:04, Train, Epoch : 3, Step : 1320, Loss : 0.44563, Acc : 0.797, Sensitive_Loss : 0.12488, Sensitive_Acc : 20.500, Run Time : 17.73 sec
INFO:root:2024-04-18 10:05:22, Train, Epoch : 3, Step : 1330, Loss : 0.49030, Acc : 0.800, Sensitive_Loss : 0.23286, Sensitive_Acc : 18.600, Run Time : 17.93 sec
INFO:root:2024-04-18 10:05:39, Train, Epoch : 3, Step : 1340, Loss : 0.51652, Acc : 0.769, Sensitive_Loss : 0.14713, Sensitive_Acc : 20.600, Run Time : 17.37 sec
INFO:root:2024-04-18 10:05:56, Train, Epoch : 3, Step : 1350, Loss : 0.47830, Acc : 0.806, Sensitive_Loss : 0.15841, Sensitive_Acc : 20.300, Run Time : 16.53 sec
INFO:root:2024-04-18 10:06:15, Train, Epoch : 3, Step : 1360, Loss : 0.51547, Acc : 0.797, Sensitive_Loss : 0.12174, Sensitive_Acc : 20.400, Run Time : 19.37 sec
INFO:root:2024-04-18 10:06:33, Train, Epoch : 3, Step : 1370, Loss : 0.45621, Acc : 0.803, Sensitive_Loss : 0.23668, Sensitive_Acc : 18.000, Run Time : 17.76 sec
INFO:root:2024-04-18 10:06:49, Train, Epoch : 3, Step : 1380, Loss : 0.46844, Acc : 0.766, Sensitive_Loss : 0.16947, Sensitive_Acc : 17.800, Run Time : 16.63 sec
INFO:root:2024-04-18 10:07:06, Train, Epoch : 3, Step : 1390, Loss : 0.45087, Acc : 0.791, Sensitive_Loss : 0.16415, Sensitive_Acc : 20.700, Run Time : 17.06 sec
INFO:root:2024-04-18 10:07:23, Train, Epoch : 3, Step : 1400, Loss : 0.48478, Acc : 0.781, Sensitive_Loss : 0.15679, Sensitive_Acc : 18.600, Run Time : 16.74 sec
INFO:root:2024-04-18 10:11:15, Dev, Step : 1400, Loss : 0.51017, Acc : 0.777, Auc : 0.851, Sensitive_Loss : 0.22831, Sensitive_Acc : 21.211, Sensitive_Auc : 0.992, Mean auc: 0.851, Run Time : 232.12 sec
INFO:root:2024-04-18 10:11:16, Best, Step : 1400, Loss : 0.51017, Acc : 0.777, Auc : 0.851, Sensitive_Loss : 0.22831, Sensitive_Acc : 21.211, Sensitive_Auc : 0.992, Best Auc : 0.851
INFO:root:2024-04-18 10:11:28, Train, Epoch : 3, Step : 1410, Loss : 0.45393, Acc : 0.753, Sensitive_Loss : 0.15140, Sensitive_Acc : 23.800, Run Time : 244.88 sec
INFO:root:2024-04-18 10:11:46, Train, Epoch : 3, Step : 1420, Loss : 0.45446, Acc : 0.812, Sensitive_Loss : 0.11545, Sensitive_Acc : 20.800, Run Time : 17.99 sec
INFO:root:2024-04-18 10:12:05, Train, Epoch : 3, Step : 1430, Loss : 0.46353, Acc : 0.791, Sensitive_Loss : 0.17899, Sensitive_Acc : 20.000, Run Time : 18.56 sec
INFO:root:2024-04-18 10:12:21, Train, Epoch : 3, Step : 1440, Loss : 0.44855, Acc : 0.822, Sensitive_Loss : 0.15160, Sensitive_Acc : 20.400, Run Time : 16.14 sec
INFO:root:2024-04-18 10:12:38, Train, Epoch : 3, Step : 1450, Loss : 0.42431, Acc : 0.797, Sensitive_Loss : 0.19334, Sensitive_Acc : 18.800, Run Time : 17.14 sec
INFO:root:2024-04-18 10:12:56, Train, Epoch : 3, Step : 1460, Loss : 0.44866, Acc : 0.800, Sensitive_Loss : 0.14539, Sensitive_Acc : 16.700, Run Time : 17.74 sec
INFO:root:2024-04-18 10:13:13, Train, Epoch : 3, Step : 1470, Loss : 0.48012, Acc : 0.825, Sensitive_Loss : 0.14358, Sensitive_Acc : 22.400, Run Time : 17.65 sec
INFO:root:2024-04-18 10:13:30, Train, Epoch : 3, Step : 1480, Loss : 0.37421, Acc : 0.828, Sensitive_Loss : 0.18578, Sensitive_Acc : 21.300, Run Time : 16.98 sec
INFO:root:2024-04-18 10:13:48, Train, Epoch : 3, Step : 1490, Loss : 0.50099, Acc : 0.772, Sensitive_Loss : 0.16483, Sensitive_Acc : 24.400, Run Time : 17.99 sec
INFO:root:2024-04-18 10:14:06, Train, Epoch : 3, Step : 1500, Loss : 0.45696, Acc : 0.809, Sensitive_Loss : 0.14927, Sensitive_Acc : 23.500, Run Time : 18.06 sec
INFO:root:2024-04-18 10:17:58, Dev, Step : 1500, Loss : 0.51644, Acc : 0.771, Auc : 0.852, Sensitive_Loss : 0.21628, Sensitive_Acc : 21.526, Sensitive_Auc : 0.994, Mean auc: 0.852, Run Time : 231.50 sec
INFO:root:2024-04-18 10:17:59, Best, Step : 1500, Loss : 0.51644, Acc : 0.771, Auc : 0.852, Sensitive_Loss : 0.21628, Sensitive_Acc : 21.526, Sensitive_Auc : 0.994, Best Auc : 0.852
INFO:root:2024-04-18 10:18:11, Train, Epoch : 3, Step : 1510, Loss : 0.51688, Acc : 0.791, Sensitive_Loss : 0.13799, Sensitive_Acc : 22.400, Run Time : 244.24 sec
INFO:root:2024-04-18 10:18:29, Train, Epoch : 3, Step : 1520, Loss : 0.47069, Acc : 0.753, Sensitive_Loss : 0.15374, Sensitive_Acc : 21.100, Run Time : 18.43 sec
INFO:root:2024-04-18 10:18:46, Train, Epoch : 3, Step : 1530, Loss : 0.49384, Acc : 0.784, Sensitive_Loss : 0.19078, Sensitive_Acc : 21.900, Run Time : 16.88 sec
INFO:root:2024-04-18 10:19:03, Train, Epoch : 3, Step : 1540, Loss : 0.46785, Acc : 0.787, Sensitive_Loss : 0.12407, Sensitive_Acc : 25.100, Run Time : 16.80 sec
INFO:root:2024-04-18 10:19:20, Train, Epoch : 3, Step : 1550, Loss : 0.49607, Acc : 0.756, Sensitive_Loss : 0.21277, Sensitive_Acc : 19.700, Run Time : 17.37 sec
INFO:root:2024-04-18 10:19:38, Train, Epoch : 3, Step : 1560, Loss : 0.42313, Acc : 0.806, Sensitive_Loss : 0.14390, Sensitive_Acc : 22.700, Run Time : 17.74 sec
INFO:root:2024-04-18 10:19:56, Train, Epoch : 3, Step : 1570, Loss : 0.40623, Acc : 0.784, Sensitive_Loss : 0.18528, Sensitive_Acc : 23.300, Run Time : 18.03 sec
INFO:root:2024-04-18 10:20:14, Train, Epoch : 3, Step : 1580, Loss : 0.47104, Acc : 0.778, Sensitive_Loss : 0.16526, Sensitive_Acc : 23.700, Run Time : 17.76 sec
INFO:root:2024-04-18 10:20:30, Train, Epoch : 3, Step : 1590, Loss : 0.50505, Acc : 0.769, Sensitive_Loss : 0.17094, Sensitive_Acc : 24.600, Run Time : 16.60 sec
INFO:root:2024-04-18 10:20:48, Train, Epoch : 3, Step : 1600, Loss : 0.45751, Acc : 0.797, Sensitive_Loss : 0.13260, Sensitive_Acc : 18.400, Run Time : 17.67 sec
INFO:root:2024-04-18 10:24:40, Dev, Step : 1600, Loss : 0.51420, Acc : 0.773, Auc : 0.854, Sensitive_Loss : 0.22360, Sensitive_Acc : 20.820, Sensitive_Auc : 0.995, Mean auc: 0.854, Run Time : 231.73 sec
INFO:root:2024-04-18 10:24:40, Best, Step : 1600, Loss : 0.51420, Acc : 0.773, Auc : 0.854, Sensitive_Loss : 0.22360, Sensitive_Acc : 20.820, Sensitive_Auc : 0.995, Best Auc : 0.854
INFO:root:2024-04-18 10:24:53, Train, Epoch : 3, Step : 1610, Loss : 0.40786, Acc : 0.816, Sensitive_Loss : 0.13126, Sensitive_Acc : 21.500, Run Time : 244.62 sec
INFO:root:2024-04-18 10:25:11, Train, Epoch : 3, Step : 1620, Loss : 0.47332, Acc : 0.800, Sensitive_Loss : 0.11600, Sensitive_Acc : 24.300, Run Time : 18.28 sec
INFO:root:2024-04-18 10:25:28, Train, Epoch : 3, Step : 1630, Loss : 0.49888, Acc : 0.753, Sensitive_Loss : 0.23882, Sensitive_Acc : 24.500, Run Time : 17.47 sec
INFO:root:2024-04-18 10:25:46, Train, Epoch : 3, Step : 1640, Loss : 0.44190, Acc : 0.791, Sensitive_Loss : 0.18917, Sensitive_Acc : 21.500, Run Time : 17.28 sec
INFO:root:2024-04-18 10:26:04, Train, Epoch : 3, Step : 1650, Loss : 0.48915, Acc : 0.787, Sensitive_Loss : 0.14027, Sensitive_Acc : 22.200, Run Time : 18.20 sec
INFO:root:2024-04-18 10:26:19, Train, Epoch : 3, Step : 1660, Loss : 0.46876, Acc : 0.781, Sensitive_Loss : 0.12865, Sensitive_Acc : 24.700, Run Time : 15.72 sec
INFO:root:2024-04-18 10:26:38, Train, Epoch : 3, Step : 1670, Loss : 0.45060, Acc : 0.791, Sensitive_Loss : 0.12117, Sensitive_Acc : 17.600, Run Time : 18.21 sec
INFO:root:2024-04-18 10:26:55, Train, Epoch : 3, Step : 1680, Loss : 0.40058, Acc : 0.787, Sensitive_Loss : 0.12541, Sensitive_Acc : 22.700, Run Time : 17.57 sec
INFO:root:2024-04-18 10:27:13, Train, Epoch : 3, Step : 1690, Loss : 0.40322, Acc : 0.822, Sensitive_Loss : 0.17216, Sensitive_Acc : 20.400, Run Time : 17.36 sec
INFO:root:2024-04-18 10:27:29, Train, Epoch : 3, Step : 1700, Loss : 0.48044, Acc : 0.784, Sensitive_Loss : 0.16605, Sensitive_Acc : 20.800, Run Time : 16.81 sec
INFO:root:2024-04-18 10:31:21, Dev, Step : 1700, Loss : 0.51292, Acc : 0.776, Auc : 0.854, Sensitive_Loss : 0.18628, Sensitive_Acc : 21.902, Sensitive_Auc : 0.995, Mean auc: 0.854, Run Time : 231.75 sec
INFO:root:2024-04-18 10:31:27, Best, Step : 1700, Loss : 0.51292, Acc : 0.776, Auc : 0.854, Sensitive_Loss : 0.18628, Sensitive_Acc : 21.902, Sensitive_Auc : 0.995, Best Auc : 0.854
INFO:root:2024-04-18 10:31:39, Train, Epoch : 3, Step : 1710, Loss : 0.51638, Acc : 0.766, Sensitive_Loss : 0.18968, Sensitive_Acc : 17.500, Run Time : 249.94 sec
INFO:root:2024-04-18 10:31:57, Train, Epoch : 3, Step : 1720, Loss : 0.41516, Acc : 0.772, Sensitive_Loss : 0.19094, Sensitive_Acc : 21.200, Run Time : 17.26 sec
INFO:root:2024-04-18 10:32:16, Train, Epoch : 3, Step : 1730, Loss : 0.44816, Acc : 0.812, Sensitive_Loss : 0.20546, Sensitive_Acc : 22.100, Run Time : 19.28 sec
INFO:root:2024-04-18 10:32:34, Train, Epoch : 3, Step : 1740, Loss : 0.46741, Acc : 0.766, Sensitive_Loss : 0.15028, Sensitive_Acc : 23.800, Run Time : 18.48 sec
INFO:root:2024-04-18 10:32:51, Train, Epoch : 3, Step : 1750, Loss : 0.47220, Acc : 0.784, Sensitive_Loss : 0.15852, Sensitive_Acc : 22.600, Run Time : 16.98 sec
INFO:root:2024-04-18 10:33:09, Train, Epoch : 3, Step : 1760, Loss : 0.56115, Acc : 0.781, Sensitive_Loss : 0.19354, Sensitive_Acc : 19.900, Run Time : 17.24 sec
INFO:root:2024-04-18 10:33:26, Train, Epoch : 3, Step : 1770, Loss : 0.48705, Acc : 0.756, Sensitive_Loss : 0.15326, Sensitive_Acc : 25.600, Run Time : 17.17 sec
INFO:root:2024-04-18 10:33:44, Train, Epoch : 3, Step : 1780, Loss : 0.42075, Acc : 0.784, Sensitive_Loss : 0.21456, Sensitive_Acc : 18.900, Run Time : 18.48 sec
INFO:root:2024-04-18 10:34:03, Train, Epoch : 3, Step : 1790, Loss : 0.45809, Acc : 0.794, Sensitive_Loss : 0.13749, Sensitive_Acc : 22.800, Run Time : 18.36 sec
INFO:root:2024-04-18 10:34:20, Train, Epoch : 3, Step : 1800, Loss : 0.41270, Acc : 0.841, Sensitive_Loss : 0.20136, Sensitive_Acc : 22.400, Run Time : 17.14 sec
INFO:root:2024-04-18 10:38:12, Dev, Step : 1800, Loss : 0.50043, Acc : 0.780, Auc : 0.856, Sensitive_Loss : 0.20626, Sensitive_Acc : 21.632, Sensitive_Auc : 0.998, Mean auc: 0.856, Run Time : 232.66 sec
INFO:root:2024-04-18 10:38:13, Best, Step : 1800, Loss : 0.50043, Acc : 0.780, Auc : 0.856, Sensitive_Loss : 0.20626, Sensitive_Acc : 21.632, Sensitive_Auc : 0.998, Best Auc : 0.856
INFO:root:2024-04-18 10:38:26, Train, Epoch : 3, Step : 1810, Loss : 0.37941, Acc : 0.841, Sensitive_Loss : 0.22234, Sensitive_Acc : 21.900, Run Time : 246.11 sec
INFO:root:2024-04-18 10:38:43, Train, Epoch : 3, Step : 1820, Loss : 0.45633, Acc : 0.781, Sensitive_Loss : 0.12735, Sensitive_Acc : 19.400, Run Time : 17.40 sec
INFO:root:2024-04-18 10:39:00, Train, Epoch : 3, Step : 1830, Loss : 0.41796, Acc : 0.809, Sensitive_Loss : 0.14310, Sensitive_Acc : 17.200, Run Time : 17.07 sec
INFO:root:2024-04-18 10:39:18, Train, Epoch : 3, Step : 1840, Loss : 0.43106, Acc : 0.806, Sensitive_Loss : 0.16550, Sensitive_Acc : 22.400, Run Time : 17.49 sec
INFO:root:2024-04-18 10:39:34, Train, Epoch : 3, Step : 1850, Loss : 0.44989, Acc : 0.809, Sensitive_Loss : 0.23474, Sensitive_Acc : 22.500, Run Time : 16.58 sec
INFO:root:2024-04-18 10:39:53, Train, Epoch : 3, Step : 1860, Loss : 0.39522, Acc : 0.816, Sensitive_Loss : 0.11257, Sensitive_Acc : 18.600, Run Time : 18.93 sec
INFO:root:2024-04-18 10:40:12, Train, Epoch : 3, Step : 1870, Loss : 0.39979, Acc : 0.834, Sensitive_Loss : 0.10650, Sensitive_Acc : 22.200, Run Time : 18.62 sec
INFO:root:2024-04-18 10:40:29, Train, Epoch : 3, Step : 1880, Loss : 0.42348, Acc : 0.781, Sensitive_Loss : 0.11765, Sensitive_Acc : 22.500, Run Time : 17.37 sec
INFO:root:2024-04-18 10:40:48, Train, Epoch : 3, Step : 1890, Loss : 0.45495, Acc : 0.787, Sensitive_Loss : 0.19348, Sensitive_Acc : 23.200, Run Time : 18.56 sec
INFO:root:2024-04-18 10:41:05, Train, Epoch : 3, Step : 1900, Loss : 0.47567, Acc : 0.775, Sensitive_Loss : 0.15082, Sensitive_Acc : 19.000, Run Time : 16.60 sec
INFO:root:2024-04-18 10:44:56, Dev, Step : 1900, Loss : 0.51164, Acc : 0.779, Auc : 0.858, Sensitive_Loss : 0.19755, Sensitive_Acc : 21.902, Sensitive_Auc : 0.997, Mean auc: 0.858, Run Time : 231.79 sec
INFO:root:2024-04-18 10:44:57, Best, Step : 1900, Loss : 0.51164, Acc : 0.779, Auc : 0.858, Sensitive_Loss : 0.19755, Sensitive_Acc : 21.902, Sensitive_Auc : 0.997, Best Auc : 0.858
INFO:root:2024-04-18 10:48:46
INFO:root:y_pred: [0.21826513 0.00685098 0.11137976 ... 0.33247727 0.05563758 0.02763316]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [5.2461601e-03 8.8997945e-02 8.9795858e-04 1.7006242e-01 2.3342620e-03
 1.0660015e-03 1.2105631e-03 3.5628374e-03 1.1463554e-01 9.9940586e-01
 4.1741142e-01 6.4516375e-03 3.1769597e-03 3.4008920e-04 9.9941063e-01
 4.0827580e-02 1.9434201e-02 9.9986076e-01 9.9963582e-01 1.1586927e-02
 8.8324457e-01 1.5159820e-03 3.4077127e-02 2.3522545e-03 1.3355073e-02
 1.4855115e-01 3.7568907e-04 1.8393453e-03 4.9528590e-04 2.6834609e-02
 3.1038327e-02 9.7935289e-01 1.8595312e-02 5.6328773e-01 2.9442259e-03
 6.0792669e-04 4.2738989e-02 6.3265175e-02 1.0019719e-01 2.3444323e-02
 3.9940558e-02 9.9867874e-01 1.9661607e-03 2.0245691e-03 8.5756236e-01
 2.8731378e-02 2.9787150e-01 2.6398382e-01 4.5580590e-01 9.9315488e-01
 9.7484618e-01 9.9966347e-01 9.9223495e-01 9.0028369e-04 8.1914030e-02
 2.4262652e-01 4.9709948e-03 4.1476241e-03 9.9631208e-01 1.1588903e-02
 2.0126374e-05 8.3708199e-04 7.6049156e-03 4.0465736e-04 9.9659890e-01
 1.2262893e-02 4.0863351e-06 2.5961515e-01 6.0955631e-03 9.9754983e-01
 9.9995708e-01 9.9955529e-01 3.9942693e-03 2.9151067e-01 1.0652620e-03
 3.3456853e-01 8.8684753e-02 1.2111959e-04 6.9989421e-04 1.0248808e-03
 1.4435377e-02 2.0363281e-05 9.9214983e-01 9.9669147e-01 5.7738944e-04
 5.9892060e-03 1.2512417e-01 5.4531032e-03 1.1248839e-03 1.3072909e-03
 8.3552049e-03 1.5637785e-01 9.0055713e-05 1.0650816e-05 1.3257869e-01
 9.8611219e-03 4.5490594e-04 3.6698443e-01 5.7432121e-03 7.6620560e-03
 6.0290657e-03 6.8568061e-03 3.2985339e-01 1.1556437e-03 4.6568483e-02
 2.2136571e-03 3.3988707e-02 2.9586759e-01 6.9221777e-01 2.0021480e-01
 9.3491444e-06 9.9985456e-01 9.9903250e-01 1.5024162e-05 6.7949764e-02
 1.2512016e-01 8.1341200e-02 1.6518796e-03 4.2796618e-01 2.0356689e-02
 5.8077280e-03 7.5018138e-04 8.5713062e-04 5.1124232e-05 4.5891320e-03
 7.9551733e-01 9.1388742e-05 9.9465406e-01 7.1929149e-02 2.3535554e-01
 2.1197006e-02 4.1448814e-01 1.4073865e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 10:48:46, Dev, Step : 1902, Loss : 0.51380, Acc : 0.780, Auc : 0.858, Sensitive_Loss : 0.19785, Sensitive_Acc : 21.902, Sensitive_Auc : 0.997, Mean auc: 0.858, Run Time : 228.52 sec
INFO:root:2024-04-18 10:48:47, Best, Step : 1902, Loss : 0.51380, Acc : 0.780,Auc : 0.858, Best Auc : 0.858, Sensitive_Loss : 0.19785, Sensitive_Acc : 21.902, Sensitive_Auc : 0.997
INFO:root:2024-04-18 10:49:05, Train, Epoch : 4, Step : 1910, Loss : 0.34321, Acc : 0.650, Sensitive_Loss : 0.07610, Sensitive_Acc : 18.300, Run Time : 16.70 sec
INFO:root:2024-04-18 10:49:23, Train, Epoch : 4, Step : 1920, Loss : 0.42546, Acc : 0.841, Sensitive_Loss : 0.14277, Sensitive_Acc : 23.700, Run Time : 17.72 sec
INFO:root:2024-04-18 10:49:40, Train, Epoch : 4, Step : 1930, Loss : 0.39720, Acc : 0.831, Sensitive_Loss : 0.15277, Sensitive_Acc : 20.600, Run Time : 17.78 sec
INFO:root:2024-04-18 10:49:59, Train, Epoch : 4, Step : 1940, Loss : 0.43790, Acc : 0.816, Sensitive_Loss : 0.12366, Sensitive_Acc : 23.500, Run Time : 18.34 sec
INFO:root:2024-04-18 10:50:15, Train, Epoch : 4, Step : 1950, Loss : 0.44442, Acc : 0.816, Sensitive_Loss : 0.17047, Sensitive_Acc : 16.600, Run Time : 16.33 sec
INFO:root:2024-04-18 10:50:32, Train, Epoch : 4, Step : 1960, Loss : 0.50958, Acc : 0.769, Sensitive_Loss : 0.15322, Sensitive_Acc : 20.900, Run Time : 17.47 sec
INFO:root:2024-04-18 10:50:51, Train, Epoch : 4, Step : 1970, Loss : 0.43617, Acc : 0.794, Sensitive_Loss : 0.11935, Sensitive_Acc : 23.300, Run Time : 18.37 sec
INFO:root:2024-04-18 10:51:09, Train, Epoch : 4, Step : 1980, Loss : 0.41755, Acc : 0.809, Sensitive_Loss : 0.13826, Sensitive_Acc : 21.300, Run Time : 18.48 sec
INFO:root:2024-04-18 10:51:26, Train, Epoch : 4, Step : 1990, Loss : 0.48533, Acc : 0.772, Sensitive_Loss : 0.16676, Sensitive_Acc : 23.500, Run Time : 17.00 sec
INFO:root:2024-04-18 10:51:43, Train, Epoch : 4, Step : 2000, Loss : 0.42041, Acc : 0.831, Sensitive_Loss : 0.13874, Sensitive_Acc : 23.200, Run Time : 16.69 sec
INFO:root:2024-04-18 10:55:34, Dev, Step : 2000, Loss : 0.51325, Acc : 0.780, Auc : 0.856, Sensitive_Loss : 0.19891, Sensitive_Acc : 21.752, Sensitive_Auc : 0.998, Mean auc: 0.856, Run Time : 231.03 sec
INFO:root:2024-04-18 10:55:45, Train, Epoch : 4, Step : 2010, Loss : 0.48598, Acc : 0.775, Sensitive_Loss : 0.11707, Sensitive_Acc : 20.900, Run Time : 242.00 sec
INFO:root:2024-04-18 10:56:04, Train, Epoch : 4, Step : 2020, Loss : 0.41358, Acc : 0.787, Sensitive_Loss : 0.14180, Sensitive_Acc : 25.800, Run Time : 18.73 sec
INFO:root:2024-04-18 10:56:20, Train, Epoch : 4, Step : 2030, Loss : 0.53151, Acc : 0.778, Sensitive_Loss : 0.17193, Sensitive_Acc : 22.300, Run Time : 16.39 sec
INFO:root:2024-04-18 10:56:39, Train, Epoch : 4, Step : 2040, Loss : 0.48642, Acc : 0.753, Sensitive_Loss : 0.17803, Sensitive_Acc : 21.300, Run Time : 18.66 sec
INFO:root:2024-04-18 10:56:57, Train, Epoch : 4, Step : 2050, Loss : 0.44724, Acc : 0.806, Sensitive_Loss : 0.10700, Sensitive_Acc : 21.500, Run Time : 18.53 sec
INFO:root:2024-04-18 10:57:13, Train, Epoch : 4, Step : 2060, Loss : 0.37395, Acc : 0.847, Sensitive_Loss : 0.14560, Sensitive_Acc : 20.300, Run Time : 15.78 sec
INFO:root:2024-04-18 10:57:30, Train, Epoch : 4, Step : 2070, Loss : 0.43147, Acc : 0.803, Sensitive_Loss : 0.18786, Sensitive_Acc : 24.100, Run Time : 17.35 sec
INFO:root:2024-04-18 10:57:48, Train, Epoch : 4, Step : 2080, Loss : 0.42871, Acc : 0.797, Sensitive_Loss : 0.14096, Sensitive_Acc : 18.900, Run Time : 17.51 sec
INFO:root:2024-04-18 10:58:05, Train, Epoch : 4, Step : 2090, Loss : 0.40015, Acc : 0.816, Sensitive_Loss : 0.15789, Sensitive_Acc : 24.900, Run Time : 16.91 sec
INFO:root:2024-04-18 10:58:22, Train, Epoch : 4, Step : 2100, Loss : 0.43156, Acc : 0.794, Sensitive_Loss : 0.13518, Sensitive_Acc : 21.800, Run Time : 17.09 sec
INFO:root:2024-04-18 11:02:15, Dev, Step : 2100, Loss : 0.51294, Acc : 0.779, Auc : 0.858, Sensitive_Loss : 0.19284, Sensitive_Acc : 21.752, Sensitive_Auc : 0.997, Mean auc: 0.858, Run Time : 232.86 sec
INFO:root:2024-04-18 11:02:27, Train, Epoch : 4, Step : 2110, Loss : 0.37861, Acc : 0.841, Sensitive_Loss : 0.15036, Sensitive_Acc : 17.900, Run Time : 245.28 sec
INFO:root:2024-04-18 11:02:45, Train, Epoch : 4, Step : 2120, Loss : 0.46166, Acc : 0.800, Sensitive_Loss : 0.10954, Sensitive_Acc : 18.400, Run Time : 18.24 sec
INFO:root:2024-04-18 11:03:03, Train, Epoch : 4, Step : 2130, Loss : 0.43831, Acc : 0.828, Sensitive_Loss : 0.14854, Sensitive_Acc : 23.500, Run Time : 17.15 sec
INFO:root:2024-04-18 11:03:20, Train, Epoch : 4, Step : 2140, Loss : 0.50344, Acc : 0.778, Sensitive_Loss : 0.10638, Sensitive_Acc : 24.500, Run Time : 16.98 sec
INFO:root:2024-04-18 11:03:37, Train, Epoch : 4, Step : 2150, Loss : 0.42610, Acc : 0.828, Sensitive_Loss : 0.16089, Sensitive_Acc : 20.700, Run Time : 17.11 sec
INFO:root:2024-04-18 11:03:56, Train, Epoch : 4, Step : 2160, Loss : 0.45478, Acc : 0.797, Sensitive_Loss : 0.09340, Sensitive_Acc : 20.900, Run Time : 18.86 sec
INFO:root:2024-04-18 11:04:12, Train, Epoch : 4, Step : 2170, Loss : 0.40772, Acc : 0.831, Sensitive_Loss : 0.17323, Sensitive_Acc : 19.300, Run Time : 16.53 sec
INFO:root:2024-04-18 11:04:30, Train, Epoch : 4, Step : 2180, Loss : 0.32116, Acc : 0.863, Sensitive_Loss : 0.15866, Sensitive_Acc : 16.500, Run Time : 17.66 sec
INFO:root:2024-04-18 11:04:47, Train, Epoch : 4, Step : 2190, Loss : 0.48306, Acc : 0.766, Sensitive_Loss : 0.12324, Sensitive_Acc : 16.800, Run Time : 16.96 sec
INFO:root:2024-04-18 11:05:04, Train, Epoch : 4, Step : 2200, Loss : 0.47296, Acc : 0.803, Sensitive_Loss : 0.17120, Sensitive_Acc : 24.300, Run Time : 17.16 sec
INFO:root:2024-04-18 11:08:51, Dev, Step : 2200, Loss : 0.51854, Acc : 0.776, Auc : 0.856, Sensitive_Loss : 0.19062, Sensitive_Acc : 21.752, Sensitive_Auc : 0.997, Mean auc: 0.856, Run Time : 227.57 sec
INFO:root:2024-04-18 11:09:05, Train, Epoch : 4, Step : 2210, Loss : 0.40251, Acc : 0.828, Sensitive_Loss : 0.10438, Sensitive_Acc : 21.100, Run Time : 241.46 sec
INFO:root:2024-04-18 11:09:23, Train, Epoch : 4, Step : 2220, Loss : 0.45488, Acc : 0.791, Sensitive_Loss : 0.11214, Sensitive_Acc : 20.300, Run Time : 17.26 sec
INFO:root:2024-04-18 11:09:38, Train, Epoch : 4, Step : 2230, Loss : 0.40101, Acc : 0.806, Sensitive_Loss : 0.10875, Sensitive_Acc : 23.600, Run Time : 15.24 sec
INFO:root:2024-04-18 11:09:58, Train, Epoch : 4, Step : 2240, Loss : 0.42902, Acc : 0.816, Sensitive_Loss : 0.12858, Sensitive_Acc : 18.100, Run Time : 19.72 sec
INFO:root:2024-04-18 11:10:14, Train, Epoch : 4, Step : 2250, Loss : 0.35128, Acc : 0.828, Sensitive_Loss : 0.12795, Sensitive_Acc : 16.400, Run Time : 16.86 sec
INFO:root:2024-04-18 11:10:32, Train, Epoch : 4, Step : 2260, Loss : 0.46598, Acc : 0.803, Sensitive_Loss : 0.14186, Sensitive_Acc : 18.900, Run Time : 17.12 sec
INFO:root:2024-04-18 11:10:49, Train, Epoch : 4, Step : 2270, Loss : 0.42797, Acc : 0.819, Sensitive_Loss : 0.14060, Sensitive_Acc : 19.700, Run Time : 17.48 sec
INFO:root:2024-04-18 11:11:06, Train, Epoch : 4, Step : 2280, Loss : 0.36605, Acc : 0.838, Sensitive_Loss : 0.13558, Sensitive_Acc : 21.600, Run Time : 17.13 sec
INFO:root:2024-04-18 11:11:23, Train, Epoch : 4, Step : 2290, Loss : 0.42056, Acc : 0.819, Sensitive_Loss : 0.14728, Sensitive_Acc : 16.400, Run Time : 16.51 sec
INFO:root:2024-04-18 11:11:40, Train, Epoch : 4, Step : 2300, Loss : 0.43497, Acc : 0.816, Sensitive_Loss : 0.18678, Sensitive_Acc : 21.800, Run Time : 17.53 sec
INFO:root:2024-04-18 11:15:30, Dev, Step : 2300, Loss : 0.50342, Acc : 0.785, Auc : 0.858, Sensitive_Loss : 0.18654, Sensitive_Acc : 21.902, Sensitive_Auc : 0.996, Mean auc: 0.858, Run Time : 229.62 sec
INFO:root:2024-04-18 11:15:31, Best, Step : 2300, Loss : 0.50342, Acc : 0.785, Auc : 0.858, Sensitive_Loss : 0.18654, Sensitive_Acc : 21.902, Sensitive_Auc : 0.996, Best Auc : 0.858
INFO:root:2024-04-18 11:15:44, Train, Epoch : 4, Step : 2310, Loss : 0.42421, Acc : 0.812, Sensitive_Loss : 0.16449, Sensitive_Acc : 20.200, Run Time : 243.41 sec
INFO:root:2024-04-18 11:16:03, Train, Epoch : 4, Step : 2320, Loss : 0.44033, Acc : 0.781, Sensitive_Loss : 0.12233, Sensitive_Acc : 22.000, Run Time : 18.83 sec
INFO:root:2024-04-18 11:16:18, Train, Epoch : 4, Step : 2330, Loss : 0.40787, Acc : 0.812, Sensitive_Loss : 0.13179, Sensitive_Acc : 21.700, Run Time : 15.67 sec
INFO:root:2024-04-18 11:16:35, Train, Epoch : 4, Step : 2340, Loss : 0.35872, Acc : 0.828, Sensitive_Loss : 0.11912, Sensitive_Acc : 18.500, Run Time : 16.89 sec
INFO:root:2024-04-18 11:16:52, Train, Epoch : 4, Step : 2350, Loss : 0.42832, Acc : 0.812, Sensitive_Loss : 0.14716, Sensitive_Acc : 22.600, Run Time : 16.82 sec
INFO:root:2024-04-18 11:17:09, Train, Epoch : 4, Step : 2360, Loss : 0.46093, Acc : 0.791, Sensitive_Loss : 0.12810, Sensitive_Acc : 23.000, Run Time : 16.70 sec
INFO:root:2024-04-18 11:17:25, Train, Epoch : 4, Step : 2370, Loss : 0.43671, Acc : 0.769, Sensitive_Loss : 0.13226, Sensitive_Acc : 24.200, Run Time : 16.67 sec
INFO:root:2024-04-18 11:17:43, Train, Epoch : 4, Step : 2380, Loss : 0.38903, Acc : 0.794, Sensitive_Loss : 0.14725, Sensitive_Acc : 19.900, Run Time : 17.76 sec
INFO:root:2024-04-18 11:18:01, Train, Epoch : 4, Step : 2390, Loss : 0.42566, Acc : 0.812, Sensitive_Loss : 0.20206, Sensitive_Acc : 20.000, Run Time : 17.54 sec
INFO:root:2024-04-18 11:18:20, Train, Epoch : 4, Step : 2400, Loss : 0.40329, Acc : 0.806, Sensitive_Loss : 0.17200, Sensitive_Acc : 19.200, Run Time : 19.86 sec
INFO:root:2024-04-18 11:22:24, Dev, Step : 2400, Loss : 0.51153, Acc : 0.783, Auc : 0.858, Sensitive_Loss : 0.21264, Sensitive_Acc : 21.632, Sensitive_Auc : 0.997, Mean auc: 0.858, Run Time : 243.13 sec
INFO:root:2024-04-18 11:22:37, Train, Epoch : 4, Step : 2410, Loss : 0.38562, Acc : 0.834, Sensitive_Loss : 0.10654, Sensitive_Acc : 21.500, Run Time : 256.05 sec
INFO:root:2024-04-18 11:22:54, Train, Epoch : 4, Step : 2420, Loss : 0.41562, Acc : 0.831, Sensitive_Loss : 0.12647, Sensitive_Acc : 23.100, Run Time : 17.86 sec
INFO:root:2024-04-18 11:23:12, Train, Epoch : 4, Step : 2430, Loss : 0.44231, Acc : 0.806, Sensitive_Loss : 0.12587, Sensitive_Acc : 21.700, Run Time : 17.24 sec
INFO:root:2024-04-18 11:23:29, Train, Epoch : 4, Step : 2440, Loss : 0.45056, Acc : 0.819, Sensitive_Loss : 0.10372, Sensitive_Acc : 20.400, Run Time : 17.78 sec
INFO:root:2024-04-18 11:23:46, Train, Epoch : 4, Step : 2450, Loss : 0.42097, Acc : 0.816, Sensitive_Loss : 0.17624, Sensitive_Acc : 25.100, Run Time : 17.05 sec
INFO:root:2024-04-18 11:24:04, Train, Epoch : 4, Step : 2460, Loss : 0.45282, Acc : 0.803, Sensitive_Loss : 0.11796, Sensitive_Acc : 19.500, Run Time : 17.22 sec
INFO:root:2024-04-18 11:24:22, Train, Epoch : 4, Step : 2470, Loss : 0.37579, Acc : 0.831, Sensitive_Loss : 0.14735, Sensitive_Acc : 24.200, Run Time : 18.30 sec
INFO:root:2024-04-18 11:24:39, Train, Epoch : 4, Step : 2480, Loss : 0.43537, Acc : 0.787, Sensitive_Loss : 0.15871, Sensitive_Acc : 24.200, Run Time : 17.33 sec
INFO:root:2024-04-18 11:24:55, Train, Epoch : 4, Step : 2490, Loss : 0.38363, Acc : 0.847, Sensitive_Loss : 0.08413, Sensitive_Acc : 20.700, Run Time : 15.32 sec
INFO:root:2024-04-18 11:25:12, Train, Epoch : 4, Step : 2500, Loss : 0.38292, Acc : 0.841, Sensitive_Loss : 0.17561, Sensitive_Acc : 22.500, Run Time : 17.67 sec
INFO:root:2024-04-18 11:29:20, Dev, Step : 2500, Loss : 0.52648, Acc : 0.774, Auc : 0.858, Sensitive_Loss : 0.18377, Sensitive_Acc : 21.902, Sensitive_Auc : 0.997, Mean auc: 0.858, Run Time : 247.96 sec
INFO:root:2024-04-18 11:29:24, Best, Step : 2500, Loss : 0.52648, Acc : 0.774, Auc : 0.858, Sensitive_Loss : 0.18377, Sensitive_Acc : 21.902, Sensitive_Auc : 0.997, Best Auc : 0.858
INFO:root:2024-04-18 11:29:39, Train, Epoch : 4, Step : 2510, Loss : 0.41542, Acc : 0.825, Sensitive_Loss : 0.11444, Sensitive_Acc : 20.900, Run Time : 266.34 sec
INFO:root:2024-04-18 11:30:03, Train, Epoch : 4, Step : 2520, Loss : 0.46643, Acc : 0.794, Sensitive_Loss : 0.16316, Sensitive_Acc : 16.000, Run Time : 23.83 sec
INFO:root:2024-04-18 11:30:24, Train, Epoch : 4, Step : 2530, Loss : 0.38782, Acc : 0.816, Sensitive_Loss : 0.13142, Sensitive_Acc : 19.600, Run Time : 21.71 sec
INFO:root:2024-04-18 11:34:54
INFO:root:y_pred: [0.22859435 0.00304577 0.07175105 ... 0.310981   0.06691437 0.01233306]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [2.8189430e-03 6.1172083e-02 7.8477740e-04 1.5753354e-01 2.3603209e-03
 5.6375563e-04 2.0161938e-04 2.2479165e-03 1.1846892e-01 9.9959224e-01
 2.3931442e-01 4.5062876e-03 9.6989144e-04 1.8957493e-04 9.9931502e-01
 3.0760407e-02 1.4342948e-02 9.9983633e-01 9.9981076e-01 5.9854113e-03
 9.6135801e-01 1.3144767e-03 2.7923390e-02 2.3911099e-03 1.3887933e-02
 1.3639180e-01 2.0791500e-04 1.9368328e-03 5.2988669e-04 1.4257118e-02
 1.0415507e-02 9.7253543e-01 3.8035908e-03 5.7347566e-01 9.6184958e-04
 3.9104966e-04 1.3932226e-02 5.3964138e-02 1.1897042e-01 7.4421004e-03
 4.5290954e-02 9.9896514e-01 1.7282992e-03 1.4780189e-03 8.2678324e-01
 6.9611412e-03 3.6092496e-01 2.2760722e-01 5.2113479e-01 9.9438381e-01
 9.7802299e-01 9.9971324e-01 9.9373299e-01 6.4971158e-04 2.2576934e-02
 2.8441608e-01 6.9435575e-04 1.9985684e-03 9.9699116e-01 8.5778991e-03
 1.7731294e-05 1.2513805e-03 1.1322814e-02 1.8519042e-04 9.9716043e-01
 7.6053622e-03 1.0641967e-06 2.9650477e-01 8.5638221e-03 9.9548483e-01
 9.9997663e-01 9.9971336e-01 1.4178460e-03 3.5777774e-01 2.0278434e-03
 3.3837783e-01 8.3832353e-02 8.0385558e-05 1.6866706e-04 6.4804679e-04
 1.6901722e-02 1.7570390e-05 9.9302238e-01 9.9591798e-01 1.5198143e-04
 2.6964545e-03 5.9356801e-02 4.9457722e-03 9.6239412e-04 6.7701295e-04
 9.2364280e-03 9.7983949e-02 1.2631067e-04 1.2243210e-05 1.9496830e-02
 4.5930785e-03 2.3706637e-04 3.9758229e-01 1.8967270e-03 2.1826166e-03
 3.5608439e-03 4.2861523e-03 3.9180437e-01 2.0633904e-04 5.4052912e-02
 1.5629566e-03 1.6564243e-02 2.7389741e-01 5.6891394e-01 1.5393403e-01
 2.3573218e-06 9.9988949e-01 9.9909317e-01 1.4844794e-05 9.5668398e-02
 8.1760339e-02 9.8277226e-02 4.1403796e-04 4.1260448e-01 9.5477942e-03
 4.2926162e-03 7.8449259e-04 1.8876888e-03 3.0644336e-05 5.8264746e-03
 7.1084362e-01 2.3087765e-05 9.9517822e-01 9.1901369e-02 2.4655141e-01
 4.0908181e-03 3.1315622e-01 7.0323968e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 11:34:54, Dev, Step : 2536, Loss : 0.51154, Acc : 0.785, Auc : 0.857, Sensitive_Loss : 0.18697, Sensitive_Acc : 21.752, Sensitive_Auc : 0.998, Mean auc: 0.857, Run Time : 257.14 sec
INFO:root:2024-04-18 11:35:07, Train, Epoch : 5, Step : 2540, Loss : 0.14882, Acc : 0.331, Sensitive_Loss : 0.03743, Sensitive_Acc : 8.700, Run Time : 10.74 sec
INFO:root:2024-04-18 11:35:26, Train, Epoch : 5, Step : 2550, Loss : 0.40795, Acc : 0.800, Sensitive_Loss : 0.16781, Sensitive_Acc : 19.300, Run Time : 18.85 sec
INFO:root:2024-04-18 11:35:45, Train, Epoch : 5, Step : 2560, Loss : 0.41878, Acc : 0.831, Sensitive_Loss : 0.11405, Sensitive_Acc : 22.200, Run Time : 19.17 sec
INFO:root:2024-04-18 11:36:04, Train, Epoch : 5, Step : 2570, Loss : 0.38576, Acc : 0.825, Sensitive_Loss : 0.12574, Sensitive_Acc : 21.600, Run Time : 19.14 sec
INFO:root:2024-04-18 11:36:24, Train, Epoch : 5, Step : 2580, Loss : 0.40723, Acc : 0.803, Sensitive_Loss : 0.11889, Sensitive_Acc : 23.500, Run Time : 19.47 sec
INFO:root:2024-04-18 11:36:42, Train, Epoch : 5, Step : 2590, Loss : 0.37715, Acc : 0.819, Sensitive_Loss : 0.09831, Sensitive_Acc : 24.000, Run Time : 18.32 sec
INFO:root:2024-04-18 11:37:04, Train, Epoch : 5, Step : 2600, Loss : 0.41817, Acc : 0.812, Sensitive_Loss : 0.15672, Sensitive_Acc : 25.600, Run Time : 22.15 sec
INFO:root:2024-04-18 11:40:59, Dev, Step : 2600, Loss : 0.51922, Acc : 0.781, Auc : 0.857, Sensitive_Loss : 0.19357, Sensitive_Acc : 21.752, Sensitive_Auc : 0.996, Mean auc: 0.857, Run Time : 234.70 sec
INFO:root:2024-04-18 11:41:11, Train, Epoch : 5, Step : 2610, Loss : 0.43359, Acc : 0.828, Sensitive_Loss : 0.13599, Sensitive_Acc : 21.000, Run Time : 246.61 sec
INFO:root:2024-04-18 11:41:29, Train, Epoch : 5, Step : 2620, Loss : 0.42678, Acc : 0.825, Sensitive_Loss : 0.16231, Sensitive_Acc : 24.700, Run Time : 18.26 sec
INFO:root:2024-04-18 11:41:46, Train, Epoch : 5, Step : 2630, Loss : 0.37494, Acc : 0.816, Sensitive_Loss : 0.07936, Sensitive_Acc : 18.300, Run Time : 16.88 sec
INFO:root:2024-04-18 11:42:03, Train, Epoch : 5, Step : 2640, Loss : 0.45425, Acc : 0.834, Sensitive_Loss : 0.09547, Sensitive_Acc : 17.700, Run Time : 17.12 sec
INFO:root:2024-04-18 11:42:21, Train, Epoch : 5, Step : 2650, Loss : 0.40457, Acc : 0.822, Sensitive_Loss : 0.10416, Sensitive_Acc : 22.800, Run Time : 18.05 sec
INFO:root:2024-04-18 11:42:36, Train, Epoch : 5, Step : 2660, Loss : 0.37751, Acc : 0.816, Sensitive_Loss : 0.09407, Sensitive_Acc : 25.300, Run Time : 15.42 sec
INFO:root:2024-04-18 11:42:54, Train, Epoch : 5, Step : 2670, Loss : 0.48352, Acc : 0.797, Sensitive_Loss : 0.11988, Sensitive_Acc : 18.200, Run Time : 17.45 sec
INFO:root:2024-04-18 11:43:11, Train, Epoch : 5, Step : 2680, Loss : 0.40163, Acc : 0.847, Sensitive_Loss : 0.10656, Sensitive_Acc : 11.700, Run Time : 17.09 sec
INFO:root:2024-04-18 11:43:28, Train, Epoch : 5, Step : 2690, Loss : 0.35497, Acc : 0.869, Sensitive_Loss : 0.12923, Sensitive_Acc : 23.000, Run Time : 16.64 sec
INFO:root:2024-04-18 11:43:45, Train, Epoch : 5, Step : 2700, Loss : 0.42595, Acc : 0.834, Sensitive_Loss : 0.14242, Sensitive_Acc : 21.800, Run Time : 16.93 sec
INFO:root:2024-04-18 11:47:35, Dev, Step : 2700, Loss : 0.51176, Acc : 0.785, Auc : 0.860, Sensitive_Loss : 0.18964, Sensitive_Acc : 21.902, Sensitive_Auc : 0.997, Mean auc: 0.860, Run Time : 230.22 sec
INFO:root:2024-04-18 11:47:36, Best, Step : 2700, Loss : 0.51176, Acc : 0.785, Auc : 0.860, Sensitive_Loss : 0.18964, Sensitive_Acc : 21.902, Sensitive_Auc : 0.997, Best Auc : 0.860
INFO:root:2024-04-18 11:47:48, Train, Epoch : 5, Step : 2710, Loss : 0.38952, Acc : 0.825, Sensitive_Loss : 0.10060, Sensitive_Acc : 20.600, Run Time : 243.34 sec
INFO:root:2024-04-18 11:48:05, Train, Epoch : 5, Step : 2720, Loss : 0.45333, Acc : 0.812, Sensitive_Loss : 0.13179, Sensitive_Acc : 23.400, Run Time : 17.06 sec
INFO:root:2024-04-18 11:48:23, Train, Epoch : 5, Step : 2730, Loss : 0.41748, Acc : 0.822, Sensitive_Loss : 0.11733, Sensitive_Acc : 19.200, Run Time : 17.65 sec
INFO:root:2024-04-18 11:48:39, Train, Epoch : 5, Step : 2740, Loss : 0.40400, Acc : 0.831, Sensitive_Loss : 0.13832, Sensitive_Acc : 16.400, Run Time : 16.42 sec
INFO:root:2024-04-18 11:48:56, Train, Epoch : 5, Step : 2750, Loss : 0.38134, Acc : 0.850, Sensitive_Loss : 0.16126, Sensitive_Acc : 19.900, Run Time : 17.14 sec
INFO:root:2024-04-18 11:49:13, Train, Epoch : 5, Step : 2760, Loss : 0.40412, Acc : 0.809, Sensitive_Loss : 0.13111, Sensitive_Acc : 19.600, Run Time : 17.21 sec
INFO:root:2024-04-18 11:49:32, Train, Epoch : 5, Step : 2770, Loss : 0.32404, Acc : 0.844, Sensitive_Loss : 0.12327, Sensitive_Acc : 20.500, Run Time : 18.53 sec
INFO:root:2024-04-18 11:49:49, Train, Epoch : 5, Step : 2780, Loss : 0.41423, Acc : 0.816, Sensitive_Loss : 0.14628, Sensitive_Acc : 25.200, Run Time : 16.98 sec
INFO:root:2024-04-18 11:50:06, Train, Epoch : 5, Step : 2790, Loss : 0.40883, Acc : 0.803, Sensitive_Loss : 0.10758, Sensitive_Acc : 17.700, Run Time : 17.04 sec
INFO:root:2024-04-18 11:50:24, Train, Epoch : 5, Step : 2800, Loss : 0.43331, Acc : 0.797, Sensitive_Loss : 0.16665, Sensitive_Acc : 24.700, Run Time : 18.45 sec
INFO:root:2024-04-18 11:54:13, Dev, Step : 2800, Loss : 0.53446, Acc : 0.777, Auc : 0.857, Sensitive_Loss : 0.19478, Sensitive_Acc : 21.632, Sensitive_Auc : 0.997, Mean auc: 0.857, Run Time : 228.23 sec
INFO:root:2024-04-18 11:54:26, Train, Epoch : 5, Step : 2810, Loss : 0.43292, Acc : 0.816, Sensitive_Loss : 0.15696, Sensitive_Acc : 22.000, Run Time : 241.34 sec
INFO:root:2024-04-18 11:54:43, Train, Epoch : 5, Step : 2820, Loss : 0.44214, Acc : 0.816, Sensitive_Loss : 0.13102, Sensitive_Acc : 20.800, Run Time : 17.03 sec
INFO:root:2024-04-18 11:55:00, Train, Epoch : 5, Step : 2830, Loss : 0.49343, Acc : 0.772, Sensitive_Loss : 0.15821, Sensitive_Acc : 19.500, Run Time : 17.65 sec
INFO:root:2024-04-18 11:55:17, Train, Epoch : 5, Step : 2840, Loss : 0.40348, Acc : 0.809, Sensitive_Loss : 0.18067, Sensitive_Acc : 22.200, Run Time : 16.91 sec
INFO:root:2024-04-18 11:55:33, Train, Epoch : 5, Step : 2850, Loss : 0.37084, Acc : 0.828, Sensitive_Loss : 0.16812, Sensitive_Acc : 21.500, Run Time : 16.02 sec
INFO:root:2024-04-18 11:55:51, Train, Epoch : 5, Step : 2860, Loss : 0.39960, Acc : 0.831, Sensitive_Loss : 0.09649, Sensitive_Acc : 24.300, Run Time : 17.71 sec
INFO:root:2024-04-18 11:56:08, Train, Epoch : 5, Step : 2870, Loss : 0.42039, Acc : 0.831, Sensitive_Loss : 0.16641, Sensitive_Acc : 25.600, Run Time : 17.38 sec
INFO:root:2024-04-18 11:56:24, Train, Epoch : 5, Step : 2880, Loss : 0.37319, Acc : 0.828, Sensitive_Loss : 0.13474, Sensitive_Acc : 17.800, Run Time : 15.64 sec
INFO:root:2024-04-18 11:56:42, Train, Epoch : 5, Step : 2890, Loss : 0.36258, Acc : 0.809, Sensitive_Loss : 0.13885, Sensitive_Acc : 27.000, Run Time : 17.80 sec
INFO:root:2024-04-18 11:56:58, Train, Epoch : 5, Step : 2900, Loss : 0.41857, Acc : 0.794, Sensitive_Loss : 0.16536, Sensitive_Acc : 22.500, Run Time : 15.72 sec
INFO:root:2024-04-18 12:03:17, Dev, Step : 2900, Loss : 0.51014, Acc : 0.784, Auc : 0.859, Sensitive_Loss : 0.19825, Sensitive_Acc : 21.752, Sensitive_Auc : 0.997, Mean auc: 0.859, Run Time : 379.77 sec
INFO:root:2024-04-18 12:03:43, Train, Epoch : 5, Step : 2910, Loss : 0.47814, Acc : 0.809, Sensitive_Loss : 0.10379, Sensitive_Acc : 18.900, Run Time : 405.29 sec
INFO:root:2024-04-18 12:04:24, Train, Epoch : 5, Step : 2920, Loss : 0.43490, Acc : 0.825, Sensitive_Loss : 0.09931, Sensitive_Acc : 23.900, Run Time : 41.07 sec
INFO:root:2024-04-18 12:05:04, Train, Epoch : 5, Step : 2930, Loss : 0.39587, Acc : 0.831, Sensitive_Loss : 0.11387, Sensitive_Acc : 22.000, Run Time : 40.44 sec
INFO:root:2024-04-18 12:05:46, Train, Epoch : 5, Step : 2940, Loss : 0.36499, Acc : 0.825, Sensitive_Loss : 0.11357, Sensitive_Acc : 23.600, Run Time : 41.30 sec
INFO:root:2024-04-18 12:06:16, Train, Epoch : 5, Step : 2950, Loss : 0.41283, Acc : 0.797, Sensitive_Loss : 0.16285, Sensitive_Acc : 21.600, Run Time : 29.93 sec
INFO:root:2024-04-18 12:06:33, Train, Epoch : 5, Step : 2960, Loss : 0.52478, Acc : 0.784, Sensitive_Loss : 0.16017, Sensitive_Acc : 21.700, Run Time : 17.37 sec
INFO:root:2024-04-18 12:06:50, Train, Epoch : 5, Step : 2970, Loss : 0.41002, Acc : 0.834, Sensitive_Loss : 0.10344, Sensitive_Acc : 17.800, Run Time : 17.08 sec
INFO:root:2024-04-18 12:07:09, Train, Epoch : 5, Step : 2980, Loss : 0.47163, Acc : 0.794, Sensitive_Loss : 0.19087, Sensitive_Acc : 21.800, Run Time : 18.90 sec
INFO:root:2024-04-18 12:07:26, Train, Epoch : 5, Step : 2990, Loss : 0.39049, Acc : 0.806, Sensitive_Loss : 0.13358, Sensitive_Acc : 22.100, Run Time : 16.72 sec
INFO:root:2024-04-18 12:07:42, Train, Epoch : 5, Step : 3000, Loss : 0.40497, Acc : 0.816, Sensitive_Loss : 0.13733, Sensitive_Acc : 23.100, Run Time : 16.81 sec
INFO:root:2024-04-18 12:11:53, Dev, Step : 3000, Loss : 0.52380, Acc : 0.780, Auc : 0.858, Sensitive_Loss : 0.21459, Sensitive_Acc : 21.556, Sensitive_Auc : 0.998, Mean auc: 0.858, Run Time : 250.83 sec
INFO:root:2024-04-18 12:12:07, Train, Epoch : 5, Step : 3010, Loss : 0.35684, Acc : 0.825, Sensitive_Loss : 0.10162, Sensitive_Acc : 21.900, Run Time : 264.43 sec
INFO:root:2024-04-18 12:12:28, Train, Epoch : 5, Step : 3020, Loss : 0.42011, Acc : 0.822, Sensitive_Loss : 0.14727, Sensitive_Acc : 23.300, Run Time : 21.17 sec
INFO:root:2024-04-18 12:12:48, Train, Epoch : 5, Step : 3030, Loss : 0.37655, Acc : 0.831, Sensitive_Loss : 0.09124, Sensitive_Acc : 24.900, Run Time : 20.23 sec
INFO:root:2024-04-18 12:13:09, Train, Epoch : 5, Step : 3040, Loss : 0.39264, Acc : 0.800, Sensitive_Loss : 0.11160, Sensitive_Acc : 21.700, Run Time : 20.61 sec
INFO:root:2024-04-18 12:13:31, Train, Epoch : 5, Step : 3050, Loss : 0.38952, Acc : 0.844, Sensitive_Loss : 0.12474, Sensitive_Acc : 20.500, Run Time : 22.35 sec
INFO:root:2024-04-18 12:13:53, Train, Epoch : 5, Step : 3060, Loss : 0.40232, Acc : 0.825, Sensitive_Loss : 0.11872, Sensitive_Acc : 19.900, Run Time : 21.36 sec
INFO:root:2024-04-18 12:14:16, Train, Epoch : 5, Step : 3070, Loss : 0.38909, Acc : 0.859, Sensitive_Loss : 0.14566, Sensitive_Acc : 22.400, Run Time : 23.05 sec
INFO:root:2024-04-18 12:14:40, Train, Epoch : 5, Step : 3080, Loss : 0.42448, Acc : 0.841, Sensitive_Loss : 0.10799, Sensitive_Acc : 20.500, Run Time : 23.95 sec
INFO:root:2024-04-18 12:15:02, Train, Epoch : 5, Step : 3090, Loss : 0.41841, Acc : 0.844, Sensitive_Loss : 0.07804, Sensitive_Acc : 19.800, Run Time : 22.42 sec
INFO:root:2024-04-18 12:15:27, Train, Epoch : 5, Step : 3100, Loss : 0.43646, Acc : 0.816, Sensitive_Loss : 0.11873, Sensitive_Acc : 19.300, Run Time : 24.70 sec
INFO:root:2024-04-18 12:20:12, Dev, Step : 3100, Loss : 0.51852, Acc : 0.790, Auc : 0.858, Sensitive_Loss : 0.18322, Sensitive_Acc : 21.647, Sensitive_Auc : 0.998, Mean auc: 0.858, Run Time : 285.18 sec
INFO:root:2024-04-18 12:20:25, Train, Epoch : 5, Step : 3110, Loss : 0.39701, Acc : 0.828, Sensitive_Loss : 0.09859, Sensitive_Acc : 19.800, Run Time : 297.80 sec
INFO:root:2024-04-18 12:20:43, Train, Epoch : 5, Step : 3120, Loss : 0.31885, Acc : 0.863, Sensitive_Loss : 0.15113, Sensitive_Acc : 22.000, Run Time : 18.55 sec
INFO:root:2024-04-18 12:21:03, Train, Epoch : 5, Step : 3130, Loss : 0.43350, Acc : 0.816, Sensitive_Loss : 0.17663, Sensitive_Acc : 20.100, Run Time : 19.54 sec
INFO:root:2024-04-18 12:21:21, Train, Epoch : 5, Step : 3140, Loss : 0.35348, Acc : 0.844, Sensitive_Loss : 0.13046, Sensitive_Acc : 18.300, Run Time : 18.48 sec
INFO:root:2024-04-18 12:21:41, Train, Epoch : 5, Step : 3150, Loss : 0.35648, Acc : 0.841, Sensitive_Loss : 0.15847, Sensitive_Acc : 24.000, Run Time : 19.40 sec
INFO:root:2024-04-18 12:21:58, Train, Epoch : 5, Step : 3160, Loss : 0.37477, Acc : 0.834, Sensitive_Loss : 0.15116, Sensitive_Acc : 19.200, Run Time : 17.70 sec
INFO:root:2024-04-18 12:22:16, Train, Epoch : 5, Step : 3170, Loss : 0.42710, Acc : 0.841, Sensitive_Loss : 0.15017, Sensitive_Acc : 15.900, Run Time : 17.73 sec
INFO:root:2024-04-18 12:26:03
INFO:root:y_pred: [0.38547394 0.0028875  0.06800506 ... 0.4186944  0.07918925 0.01297501]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [2.04718206e-02 9.29016098e-02 3.28864064e-03 2.49757916e-01
 6.30142353e-03 6.69390312e-04 4.97511704e-04 4.02631005e-03
 1.01928286e-01 9.99876261e-01 3.08446705e-01 1.06946854e-02
 8.62573681e-04 6.88119675e-04 9.99627471e-01 9.83566791e-02
 1.91593915e-02 9.99935627e-01 9.99896049e-01 7.24441418e-03
 9.73798037e-01 1.25613867e-03 4.49099056e-02 1.87231845e-03
 2.26511396e-02 1.52224079e-01 1.43814087e-03 5.48471045e-03
 1.85758853e-03 1.74004417e-02 1.64775681e-02 9.85412061e-01
 5.42287296e-03 7.35750258e-01 7.02190795e-04 1.17511884e-03
 1.54304923e-02 1.03705361e-01 2.43520349e-01 7.31190294e-03
 6.55417740e-02 9.99658942e-01 2.95955851e-03 2.33470392e-03
 9.54194605e-01 2.05923580e-02 3.63243639e-01 2.63252795e-01
 7.99442708e-01 9.94786382e-01 9.94877934e-01 9.99836206e-01
 9.96013165e-01 1.27435417e-03 3.28222327e-02 3.86182487e-01
 1.91865757e-03 6.32413430e-03 9.97577846e-01 1.10359835e-02
 2.07592784e-05 2.85108457e-03 1.18313031e-02 3.49979382e-04
 9.98825371e-01 1.21273883e-02 9.32491548e-06 5.12122452e-01
 8.18534289e-03 9.98444974e-01 9.99988556e-01 9.99842405e-01
 1.54305785e-03 5.70203543e-01 1.97851402e-03 6.18455589e-01
 1.04784824e-01 2.19189114e-05 6.29743590e-05 9.01294290e-04
 1.69838853e-02 6.47615088e-05 9.98368561e-01 9.98048306e-01
 3.28479015e-04 4.84069623e-03 1.68183967e-01 4.73597134e-03
 1.71131955e-03 5.37248445e-04 9.80429258e-03 6.28074780e-02
 1.37224895e-04 2.20447291e-05 3.67141888e-02 1.45495459e-02
 3.08008719e-04 6.34841800e-01 1.69083290e-03 1.44656859e-02
 1.17729437e-02 5.55370748e-03 6.11522377e-01 3.35232471e-04
 6.33590147e-02 2.81678443e-03 1.78268757e-02 4.97354865e-01
 7.39790738e-01 1.75521493e-01 8.48202217e-06 9.99965787e-01
 9.99497414e-01 2.47800053e-05 1.89947426e-01 4.01512310e-02
 1.49228349e-01 5.81133179e-04 5.25283635e-01 1.11333095e-02
 4.71342495e-03 1.06923690e-03 4.83178487e-03 2.59172339e-05
 1.22146262e-02 7.27856100e-01 1.01980067e-05 9.97099757e-01
 9.70599651e-02 4.50046003e-01 1.81177654e-03 2.10323155e-01
 9.89850287e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 12:26:03, Dev, Step : 3170, Loss : 0.51082, Acc : 0.786, Auc : 0.856, Sensitive_Loss : 0.22655, Sensitive_Acc : 21.015, Sensitive_Auc : 0.998, Mean auc: 0.856, Run Time : 226.50 sec
INFO:root:2024-04-18 12:26:24, Train, Epoch : 6, Step : 3180, Loss : 0.39493, Acc : 0.844, Sensitive_Loss : 0.10849, Sensitive_Acc : 17.700, Run Time : 19.77 sec
INFO:root:2024-04-18 12:26:42, Train, Epoch : 6, Step : 3190, Loss : 0.34821, Acc : 0.850, Sensitive_Loss : 0.10741, Sensitive_Acc : 16.900, Run Time : 18.27 sec
INFO:root:2024-04-18 12:27:00, Train, Epoch : 6, Step : 3200, Loss : 0.36813, Acc : 0.838, Sensitive_Loss : 0.15296, Sensitive_Acc : 21.800, Run Time : 17.82 sec
INFO:root:2024-04-18 12:31:13, Dev, Step : 3200, Loss : 0.50766, Acc : 0.786, Auc : 0.857, Sensitive_Loss : 0.18145, Sensitive_Acc : 21.752, Sensitive_Auc : 0.998, Mean auc: 0.857, Run Time : 253.25 sec
INFO:root:2024-04-18 12:31:26, Train, Epoch : 6, Step : 3210, Loss : 0.41017, Acc : 0.806, Sensitive_Loss : 0.13311, Sensitive_Acc : 19.800, Run Time : 265.81 sec
INFO:root:2024-04-18 12:31:43, Train, Epoch : 6, Step : 3220, Loss : 0.38211, Acc : 0.797, Sensitive_Loss : 0.19439, Sensitive_Acc : 21.400, Run Time : 17.35 sec
INFO:root:2024-04-18 12:32:02, Train, Epoch : 6, Step : 3230, Loss : 0.40524, Acc : 0.800, Sensitive_Loss : 0.12240, Sensitive_Acc : 23.500, Run Time : 18.60 sec
INFO:root:2024-04-18 12:32:20, Train, Epoch : 6, Step : 3240, Loss : 0.31734, Acc : 0.853, Sensitive_Loss : 0.14502, Sensitive_Acc : 15.900, Run Time : 18.24 sec
INFO:root:2024-04-18 12:32:38, Train, Epoch : 6, Step : 3250, Loss : 0.41561, Acc : 0.841, Sensitive_Loss : 0.11802, Sensitive_Acc : 16.500, Run Time : 17.68 sec
INFO:root:2024-04-18 12:32:56, Train, Epoch : 6, Step : 3260, Loss : 0.33600, Acc : 0.847, Sensitive_Loss : 0.11108, Sensitive_Acc : 19.800, Run Time : 17.78 sec
INFO:root:2024-04-18 12:33:14, Train, Epoch : 6, Step : 3270, Loss : 0.38394, Acc : 0.847, Sensitive_Loss : 0.08304, Sensitive_Acc : 23.100, Run Time : 18.17 sec
INFO:root:2024-04-18 12:33:32, Train, Epoch : 6, Step : 3280, Loss : 0.46104, Acc : 0.809, Sensitive_Loss : 0.13904, Sensitive_Acc : 25.000, Run Time : 18.16 sec
INFO:root:2024-04-18 12:33:49, Train, Epoch : 6, Step : 3290, Loss : 0.40888, Acc : 0.819, Sensitive_Loss : 0.08529, Sensitive_Acc : 22.400, Run Time : 17.26 sec
INFO:root:2024-04-18 12:34:07, Train, Epoch : 6, Step : 3300, Loss : 0.36728, Acc : 0.809, Sensitive_Loss : 0.22176, Sensitive_Acc : 20.500, Run Time : 18.27 sec
INFO:root:2024-04-18 12:38:06, Dev, Step : 3300, Loss : 0.50799, Acc : 0.791, Auc : 0.858, Sensitive_Loss : 0.17721, Sensitive_Acc : 22.008, Sensitive_Auc : 0.997, Mean auc: 0.858, Run Time : 238.79 sec
INFO:root:2024-04-18 12:38:19, Train, Epoch : 6, Step : 3310, Loss : 0.42013, Acc : 0.828, Sensitive_Loss : 0.10568, Sensitive_Acc : 20.100, Run Time : 251.31 sec
INFO:root:2024-04-18 12:38:38, Train, Epoch : 6, Step : 3320, Loss : 0.45299, Acc : 0.847, Sensitive_Loss : 0.10368, Sensitive_Acc : 26.600, Run Time : 19.59 sec
INFO:root:2024-04-18 12:39:19, Train, Epoch : 6, Step : 3330, Loss : 0.33876, Acc : 0.863, Sensitive_Loss : 0.13168, Sensitive_Acc : 15.500, Run Time : 41.10 sec
INFO:root:2024-04-18 12:39:37, Train, Epoch : 6, Step : 3340, Loss : 0.36166, Acc : 0.822, Sensitive_Loss : 0.14924, Sensitive_Acc : 19.600, Run Time : 18.02 sec
INFO:root:2024-04-18 12:39:57, Train, Epoch : 6, Step : 3350, Loss : 0.39366, Acc : 0.838, Sensitive_Loss : 0.14030, Sensitive_Acc : 21.200, Run Time : 19.17 sec
INFO:root:2024-04-18 12:40:13, Train, Epoch : 6, Step : 3360, Loss : 0.39922, Acc : 0.816, Sensitive_Loss : 0.10542, Sensitive_Acc : 21.900, Run Time : 16.80 sec
INFO:root:2024-04-18 12:40:31, Train, Epoch : 6, Step : 3370, Loss : 0.44020, Acc : 0.800, Sensitive_Loss : 0.08933, Sensitive_Acc : 22.400, Run Time : 17.80 sec
INFO:root:2024-04-18 12:40:48, Train, Epoch : 6, Step : 3380, Loss : 0.36369, Acc : 0.834, Sensitive_Loss : 0.10512, Sensitive_Acc : 20.400, Run Time : 16.85 sec
INFO:root:2024-04-18 12:41:06, Train, Epoch : 6, Step : 3390, Loss : 0.38458, Acc : 0.850, Sensitive_Loss : 0.15514, Sensitive_Acc : 25.800, Run Time : 17.78 sec
INFO:root:2024-04-18 12:41:23, Train, Epoch : 6, Step : 3400, Loss : 0.42797, Acc : 0.822, Sensitive_Loss : 0.11400, Sensitive_Acc : 21.600, Run Time : 17.25 sec
INFO:root:2024-04-18 12:45:16, Dev, Step : 3400, Loss : 0.52711, Acc : 0.784, Auc : 0.857, Sensitive_Loss : 0.17538, Sensitive_Acc : 21.902, Sensitive_Auc : 0.997, Mean auc: 0.857, Run Time : 233.03 sec
INFO:root:2024-04-18 12:45:29, Train, Epoch : 6, Step : 3410, Loss : 0.39837, Acc : 0.844, Sensitive_Loss : 0.12911, Sensitive_Acc : 23.800, Run Time : 245.53 sec
INFO:root:2024-04-18 12:45:47, Train, Epoch : 6, Step : 3420, Loss : 0.37975, Acc : 0.797, Sensitive_Loss : 0.14041, Sensitive_Acc : 19.400, Run Time : 17.92 sec
INFO:root:2024-04-18 12:46:05, Train, Epoch : 6, Step : 3430, Loss : 0.50418, Acc : 0.822, Sensitive_Loss : 0.10208, Sensitive_Acc : 23.100, Run Time : 18.16 sec
INFO:root:2024-04-18 12:46:22, Train, Epoch : 6, Step : 3440, Loss : 0.39349, Acc : 0.856, Sensitive_Loss : 0.13221, Sensitive_Acc : 23.500, Run Time : 17.54 sec
INFO:root:2024-04-18 12:46:40, Train, Epoch : 6, Step : 3450, Loss : 0.39181, Acc : 0.872, Sensitive_Loss : 0.08243, Sensitive_Acc : 20.100, Run Time : 17.82 sec
INFO:root:2024-04-18 12:46:58, Train, Epoch : 6, Step : 3460, Loss : 0.36057, Acc : 0.819, Sensitive_Loss : 0.09736, Sensitive_Acc : 22.100, Run Time : 18.28 sec
INFO:root:2024-04-18 12:47:16, Train, Epoch : 6, Step : 3470, Loss : 0.37309, Acc : 0.844, Sensitive_Loss : 0.13882, Sensitive_Acc : 19.100, Run Time : 17.34 sec
INFO:root:2024-04-18 12:47:39, Train, Epoch : 6, Step : 3480, Loss : 0.41826, Acc : 0.784, Sensitive_Loss : 0.12118, Sensitive_Acc : 20.300, Run Time : 22.97 sec
INFO:root:2024-04-18 12:47:57, Train, Epoch : 6, Step : 3490, Loss : 0.37811, Acc : 0.822, Sensitive_Loss : 0.14443, Sensitive_Acc : 22.000, Run Time : 18.03 sec
INFO:root:2024-04-18 12:48:14, Train, Epoch : 6, Step : 3500, Loss : 0.36067, Acc : 0.831, Sensitive_Loss : 0.14039, Sensitive_Acc : 21.200, Run Time : 17.59 sec
INFO:root:2024-04-18 12:52:05, Dev, Step : 3500, Loss : 0.52650, Acc : 0.782, Auc : 0.857, Sensitive_Loss : 0.16931, Sensitive_Acc : 22.008, Sensitive_Auc : 0.997, Mean auc: 0.857, Run Time : 231.14 sec
INFO:root:2024-04-18 12:52:19, Train, Epoch : 6, Step : 3510, Loss : 0.40377, Acc : 0.816, Sensitive_Loss : 0.12410, Sensitive_Acc : 23.500, Run Time : 245.01 sec
INFO:root:2024-04-18 12:52:37, Train, Epoch : 6, Step : 3520, Loss : 0.40652, Acc : 0.838, Sensitive_Loss : 0.17945, Sensitive_Acc : 23.900, Run Time : 17.91 sec
INFO:root:2024-04-18 12:52:55, Train, Epoch : 6, Step : 3530, Loss : 0.36477, Acc : 0.850, Sensitive_Loss : 0.09664, Sensitive_Acc : 26.200, Run Time : 17.59 sec
INFO:root:2024-04-18 12:53:12, Train, Epoch : 6, Step : 3540, Loss : 0.41245, Acc : 0.831, Sensitive_Loss : 0.11313, Sensitive_Acc : 19.600, Run Time : 17.20 sec
INFO:root:2024-04-18 12:53:31, Train, Epoch : 6, Step : 3550, Loss : 0.34109, Acc : 0.866, Sensitive_Loss : 0.18729, Sensitive_Acc : 25.100, Run Time : 18.81 sec
INFO:root:2024-04-18 12:53:48, Train, Epoch : 6, Step : 3560, Loss : 0.39037, Acc : 0.831, Sensitive_Loss : 0.09427, Sensitive_Acc : 21.900, Run Time : 16.99 sec
INFO:root:2024-04-18 12:54:05, Train, Epoch : 6, Step : 3570, Loss : 0.28383, Acc : 0.884, Sensitive_Loss : 0.12112, Sensitive_Acc : 18.100, Run Time : 16.97 sec
INFO:root:2024-04-18 12:54:24, Train, Epoch : 6, Step : 3580, Loss : 0.35722, Acc : 0.844, Sensitive_Loss : 0.12582, Sensitive_Acc : 19.100, Run Time : 19.40 sec
INFO:root:2024-04-18 12:54:42, Train, Epoch : 6, Step : 3590, Loss : 0.43420, Acc : 0.838, Sensitive_Loss : 0.09903, Sensitive_Acc : 19.800, Run Time : 17.74 sec
INFO:root:2024-04-18 12:55:01, Train, Epoch : 6, Step : 3600, Loss : 0.39975, Acc : 0.816, Sensitive_Loss : 0.09535, Sensitive_Acc : 21.000, Run Time : 18.92 sec
INFO:root:2024-04-18 12:58:52, Dev, Step : 3600, Loss : 0.51294, Acc : 0.786, Auc : 0.859, Sensitive_Loss : 0.17719, Sensitive_Acc : 21.451, Sensitive_Auc : 0.997, Mean auc: 0.859, Run Time : 230.74 sec
INFO:root:2024-04-18 12:59:05, Train, Epoch : 6, Step : 3610, Loss : 0.45228, Acc : 0.791, Sensitive_Loss : 0.10397, Sensitive_Acc : 23.700, Run Time : 244.03 sec
INFO:root:2024-04-18 12:59:23, Train, Epoch : 6, Step : 3620, Loss : 0.36499, Acc : 0.838, Sensitive_Loss : 0.11575, Sensitive_Acc : 13.800, Run Time : 18.34 sec
INFO:root:2024-04-18 12:59:41, Train, Epoch : 6, Step : 3630, Loss : 0.29766, Acc : 0.859, Sensitive_Loss : 0.16361, Sensitive_Acc : 23.800, Run Time : 18.01 sec
INFO:root:2024-04-18 12:59:59, Train, Epoch : 6, Step : 3640, Loss : 0.38668, Acc : 0.819, Sensitive_Loss : 0.12685, Sensitive_Acc : 20.600, Run Time : 18.06 sec
INFO:root:2024-04-18 13:00:17, Train, Epoch : 6, Step : 3650, Loss : 0.36184, Acc : 0.859, Sensitive_Loss : 0.06122, Sensitive_Acc : 20.000, Run Time : 17.97 sec
INFO:root:2024-04-18 13:00:36, Train, Epoch : 6, Step : 3660, Loss : 0.42506, Acc : 0.847, Sensitive_Loss : 0.12698, Sensitive_Acc : 23.900, Run Time : 18.85 sec
INFO:root:2024-04-18 13:00:54, Train, Epoch : 6, Step : 3670, Loss : 0.39161, Acc : 0.844, Sensitive_Loss : 0.13811, Sensitive_Acc : 21.900, Run Time : 17.54 sec
INFO:root:2024-04-18 13:01:12, Train, Epoch : 6, Step : 3680, Loss : 0.39842, Acc : 0.834, Sensitive_Loss : 0.09439, Sensitive_Acc : 21.400, Run Time : 18.57 sec
INFO:root:2024-04-18 13:01:30, Train, Epoch : 6, Step : 3690, Loss : 0.33065, Acc : 0.847, Sensitive_Loss : 0.12324, Sensitive_Acc : 23.200, Run Time : 17.66 sec
INFO:root:2024-04-18 13:01:46, Train, Epoch : 6, Step : 3700, Loss : 0.40863, Acc : 0.787, Sensitive_Loss : 0.12609, Sensitive_Acc : 21.200, Run Time : 16.51 sec
INFO:root:2024-04-18 13:05:38, Dev, Step : 3700, Loss : 0.55233, Acc : 0.778, Auc : 0.856, Sensitive_Loss : 0.17390, Sensitive_Acc : 21.902, Sensitive_Auc : 0.996, Mean auc: 0.856, Run Time : 231.29 sec
INFO:root:2024-04-18 13:05:50, Train, Epoch : 6, Step : 3710, Loss : 0.35853, Acc : 0.819, Sensitive_Loss : 0.11428, Sensitive_Acc : 24.900, Run Time : 243.33 sec
INFO:root:2024-04-18 13:06:08, Train, Epoch : 6, Step : 3720, Loss : 0.36346, Acc : 0.847, Sensitive_Loss : 0.14577, Sensitive_Acc : 23.800, Run Time : 18.35 sec
INFO:root:2024-04-18 13:06:26, Train, Epoch : 6, Step : 3730, Loss : 0.36171, Acc : 0.847, Sensitive_Loss : 0.12692, Sensitive_Acc : 19.700, Run Time : 18.13 sec
INFO:root:2024-04-18 13:06:43, Train, Epoch : 6, Step : 3740, Loss : 0.43064, Acc : 0.812, Sensitive_Loss : 0.12490, Sensitive_Acc : 22.100, Run Time : 16.90 sec
INFO:root:2024-04-18 13:07:01, Train, Epoch : 6, Step : 3750, Loss : 0.38390, Acc : 0.816, Sensitive_Loss : 0.09245, Sensitive_Acc : 21.800, Run Time : 17.86 sec
INFO:root:2024-04-18 13:07:18, Train, Epoch : 6, Step : 3760, Loss : 0.39668, Acc : 0.838, Sensitive_Loss : 0.09846, Sensitive_Acc : 22.600, Run Time : 17.28 sec
INFO:root:2024-04-18 13:07:37, Train, Epoch : 6, Step : 3770, Loss : 0.42429, Acc : 0.800, Sensitive_Loss : 0.15352, Sensitive_Acc : 21.500, Run Time : 18.29 sec
INFO:root:2024-04-18 13:07:53, Train, Epoch : 6, Step : 3780, Loss : 0.36925, Acc : 0.856, Sensitive_Loss : 0.12596, Sensitive_Acc : 21.300, Run Time : 16.39 sec
INFO:root:2024-04-18 13:08:11, Train, Epoch : 6, Step : 3790, Loss : 0.45652, Acc : 0.806, Sensitive_Loss : 0.18337, Sensitive_Acc : 21.500, Run Time : 17.61 sec
INFO:root:2024-04-18 13:08:29, Train, Epoch : 6, Step : 3800, Loss : 0.38391, Acc : 0.838, Sensitive_Loss : 0.09940, Sensitive_Acc : 20.900, Run Time : 18.86 sec
INFO:root:2024-04-18 13:12:21, Dev, Step : 3800, Loss : 0.53677, Acc : 0.779, Auc : 0.856, Sensitive_Loss : 0.18121, Sensitive_Acc : 21.632, Sensitive_Auc : 0.996, Mean auc: 0.856, Run Time : 231.38 sec
INFO:root:2024-04-18 13:16:29
INFO:root:y_pred: [0.05213039 0.0019725  0.08215581 ... 0.34688905 0.04850757 0.00538818]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.0645854e-02 5.6767248e-02 1.1899167e-03 9.6169546e-02 4.0723244e-03
 2.8985587e-04 1.4253389e-04 5.9002410e-03 7.5021066e-02 9.9981278e-01
 3.1665617e-01 4.5652990e-03 3.2160484e-04 2.1072547e-04 9.9963498e-01
 3.6651127e-02 1.1951139e-02 9.9988711e-01 9.9990165e-01 6.1667585e-03
 9.6774131e-01 2.4231462e-04 3.7693388e-03 1.0358425e-03 7.9564154e-03
 5.4960605e-02 4.4545953e-04 3.4196815e-03 6.1667530e-04 1.2594250e-02
 2.6366473e-03 9.8176944e-01 3.5643850e-03 6.2690228e-01 3.5944558e-04
 4.3971799e-04 1.2131640e-02 3.7879348e-02 1.2891564e-01 3.7533592e-03
 4.8167519e-02 9.9914622e-01 1.0257430e-03 1.4485167e-03 9.3869674e-01
 4.2658639e-03 4.7653729e-01 1.3030757e-01 6.5177423e-01 9.9213034e-01
 9.9092227e-01 9.9978906e-01 9.9526346e-01 7.1701978e-04 4.4060494e-03
 1.5241510e-01 1.2784244e-03 1.3187232e-03 9.9683881e-01 2.7685394e-03
 1.0060395e-05 2.7536282e-03 3.6426121e-03 2.0593946e-04 9.9760520e-01
 7.6349848e-03 5.0359558e-06 1.9890395e-01 7.0000393e-03 9.9727875e-01
 9.9998605e-01 9.9976963e-01 2.8010065e-04 4.6857503e-01 1.4129134e-03
 5.4222685e-01 5.6770388e-02 1.2464327e-05 1.1716243e-05 1.0591708e-03
 1.6913652e-02 6.5235618e-05 9.9797004e-01 9.9710852e-01 4.3145029e-04
 2.1633550e-03 7.1327925e-02 1.8246989e-03 1.7857754e-04 1.3859080e-04
 2.8929282e-03 3.3047345e-02 1.2581002e-04 4.8160750e-06 4.5444700e-03
 5.9357882e-03 5.1164516e-05 3.9540389e-01 4.4842862e-04 1.2821407e-02
 5.1552905e-03 1.9490866e-03 2.7480161e-01 1.4241990e-04 2.9747387e-02
 8.2189002e-04 3.1692644e-03 3.7986282e-01 6.3803649e-01 1.2116195e-01
 6.7040487e-06 9.9995458e-01 9.9946934e-01 2.5374962e-05 6.7411184e-02
 2.7668988e-02 2.8334903e-02 1.9857443e-04 4.1152799e-01 8.8304607e-03
 2.8533617e-03 9.7984588e-04 1.2990891e-03 1.2337009e-05 4.2195190e-03
 6.3961685e-01 1.1548194e-05 9.9706119e-01 3.3011876e-02 4.4653660e-01
 1.8167843e-03 1.2532680e-01 1.1201248e-06]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 13:16:30, Dev, Step : 3804, Loss : 0.55174, Acc : 0.775, Auc : 0.855, Sensitive_Loss : 0.18355, Sensitive_Acc : 21.632, Sensitive_Auc : 0.996, Mean auc: 0.855, Run Time : 247.40 sec
INFO:root:2024-04-18 13:16:43, Train, Epoch : 7, Step : 3810, Loss : 0.27968, Acc : 0.481, Sensitive_Loss : 0.09624, Sensitive_Acc : 10.100, Run Time : 11.97 sec
INFO:root:2024-04-18 13:17:01, Train, Epoch : 7, Step : 3820, Loss : 0.38692, Acc : 0.834, Sensitive_Loss : 0.10160, Sensitive_Acc : 18.700, Run Time : 17.47 sec
INFO:root:2024-04-18 13:17:19, Train, Epoch : 7, Step : 3830, Loss : 0.34609, Acc : 0.844, Sensitive_Loss : 0.13362, Sensitive_Acc : 21.800, Run Time : 18.55 sec
INFO:root:2024-04-18 13:17:37, Train, Epoch : 7, Step : 3840, Loss : 0.31635, Acc : 0.872, Sensitive_Loss : 0.08336, Sensitive_Acc : 21.200, Run Time : 17.83 sec
INFO:root:2024-04-18 13:17:54, Train, Epoch : 7, Step : 3850, Loss : 0.37583, Acc : 0.856, Sensitive_Loss : 0.10820, Sensitive_Acc : 24.300, Run Time : 16.56 sec
INFO:root:2024-04-18 13:18:11, Train, Epoch : 7, Step : 3860, Loss : 0.44234, Acc : 0.825, Sensitive_Loss : 0.19143, Sensitive_Acc : 24.000, Run Time : 17.41 sec
INFO:root:2024-04-18 13:18:29, Train, Epoch : 7, Step : 3870, Loss : 0.41381, Acc : 0.828, Sensitive_Loss : 0.08755, Sensitive_Acc : 23.900, Run Time : 17.91 sec
INFO:root:2024-04-18 13:18:46, Train, Epoch : 7, Step : 3880, Loss : 0.32244, Acc : 0.844, Sensitive_Loss : 0.12156, Sensitive_Acc : 20.700, Run Time : 16.80 sec
INFO:root:2024-04-18 13:19:03, Train, Epoch : 7, Step : 3890, Loss : 0.44224, Acc : 0.806, Sensitive_Loss : 0.08887, Sensitive_Acc : 17.600, Run Time : 17.48 sec
INFO:root:2024-04-18 13:19:20, Train, Epoch : 7, Step : 3900, Loss : 0.32486, Acc : 0.872, Sensitive_Loss : 0.08553, Sensitive_Acc : 22.200, Run Time : 16.89 sec
INFO:root:2024-04-18 13:23:11, Dev, Step : 3900, Loss : 0.53142, Acc : 0.781, Auc : 0.856, Sensitive_Loss : 0.17610, Sensitive_Acc : 21.571, Sensitive_Auc : 0.997, Mean auc: 0.856, Run Time : 231.00 sec
INFO:root:2024-04-18 13:23:23, Train, Epoch : 7, Step : 3910, Loss : 0.38937, Acc : 0.831, Sensitive_Loss : 0.10685, Sensitive_Acc : 22.100, Run Time : 243.20 sec
INFO:root:2024-04-18 13:23:41, Train, Epoch : 7, Step : 3920, Loss : 0.38295, Acc : 0.866, Sensitive_Loss : 0.13851, Sensitive_Acc : 23.700, Run Time : 17.90 sec
INFO:root:2024-04-18 13:23:58, Train, Epoch : 7, Step : 3930, Loss : 0.32129, Acc : 0.866, Sensitive_Loss : 0.11629, Sensitive_Acc : 24.500, Run Time : 17.33 sec
INFO:root:2024-04-18 13:24:15, Train, Epoch : 7, Step : 3940, Loss : 0.44093, Acc : 0.800, Sensitive_Loss : 0.08705, Sensitive_Acc : 24.900, Run Time : 16.86 sec
INFO:root:2024-04-18 13:24:33, Train, Epoch : 7, Step : 3950, Loss : 0.38848, Acc : 0.812, Sensitive_Loss : 0.15375, Sensitive_Acc : 19.800, Run Time : 17.29 sec
INFO:root:2024-04-18 13:24:49, Train, Epoch : 7, Step : 3960, Loss : 0.34862, Acc : 0.828, Sensitive_Loss : 0.09319, Sensitive_Acc : 25.500, Run Time : 16.43 sec
INFO:root:2024-04-18 13:25:07, Train, Epoch : 7, Step : 3970, Loss : 0.36591, Acc : 0.856, Sensitive_Loss : 0.07235, Sensitive_Acc : 23.100, Run Time : 17.85 sec
INFO:root:2024-04-18 13:25:24, Train, Epoch : 7, Step : 3980, Loss : 0.33983, Acc : 0.856, Sensitive_Loss : 0.09223, Sensitive_Acc : 17.900, Run Time : 16.96 sec
INFO:root:2024-04-18 13:25:41, Train, Epoch : 7, Step : 3990, Loss : 0.38227, Acc : 0.825, Sensitive_Loss : 0.19995, Sensitive_Acc : 16.200, Run Time : 17.63 sec
INFO:root:2024-04-18 13:25:59, Train, Epoch : 7, Step : 4000, Loss : 0.45560, Acc : 0.816, Sensitive_Loss : 0.13328, Sensitive_Acc : 21.800, Run Time : 17.85 sec
INFO:root:2024-04-18 13:29:51, Dev, Step : 4000, Loss : 0.52212, Acc : 0.786, Auc : 0.855, Sensitive_Loss : 0.18702, Sensitive_Acc : 21.647, Sensitive_Auc : 0.998, Mean auc: 0.855, Run Time : 231.40 sec
INFO:root:2024-04-18 13:30:02, Train, Epoch : 7, Step : 4010, Loss : 0.34431, Acc : 0.853, Sensitive_Loss : 0.15062, Sensitive_Acc : 23.700, Run Time : 243.17 sec
INFO:root:2024-04-18 13:30:21, Train, Epoch : 7, Step : 4020, Loss : 0.36921, Acc : 0.850, Sensitive_Loss : 0.11209, Sensitive_Acc : 24.700, Run Time : 18.14 sec
INFO:root:2024-04-18 13:30:39, Train, Epoch : 7, Step : 4030, Loss : 0.38671, Acc : 0.856, Sensitive_Loss : 0.07122, Sensitive_Acc : 17.500, Run Time : 18.69 sec
INFO:root:2024-04-18 13:30:58, Train, Epoch : 7, Step : 4040, Loss : 0.33890, Acc : 0.834, Sensitive_Loss : 0.09400, Sensitive_Acc : 21.600, Run Time : 18.42 sec
INFO:root:2024-04-18 13:31:16, Train, Epoch : 7, Step : 4050, Loss : 0.37419, Acc : 0.831, Sensitive_Loss : 0.13432, Sensitive_Acc : 24.100, Run Time : 18.11 sec
INFO:root:2024-04-18 13:31:34, Train, Epoch : 7, Step : 4060, Loss : 0.39749, Acc : 0.834, Sensitive_Loss : 0.17583, Sensitive_Acc : 17.200, Run Time : 18.63 sec
INFO:root:2024-04-18 13:31:53, Train, Epoch : 7, Step : 4070, Loss : 0.33574, Acc : 0.847, Sensitive_Loss : 0.13195, Sensitive_Acc : 17.700, Run Time : 18.56 sec
INFO:root:2024-04-18 13:32:11, Train, Epoch : 7, Step : 4080, Loss : 0.30335, Acc : 0.875, Sensitive_Loss : 0.11362, Sensitive_Acc : 21.600, Run Time : 18.06 sec
INFO:root:2024-04-18 13:32:29, Train, Epoch : 7, Step : 4090, Loss : 0.37962, Acc : 0.841, Sensitive_Loss : 0.10707, Sensitive_Acc : 22.100, Run Time : 17.67 sec
INFO:root:2024-04-18 13:32:48, Train, Epoch : 7, Step : 4100, Loss : 0.33745, Acc : 0.856, Sensitive_Loss : 0.10811, Sensitive_Acc : 20.400, Run Time : 19.21 sec
INFO:root:2024-04-18 13:36:40, Dev, Step : 4100, Loss : 0.53094, Acc : 0.782, Auc : 0.857, Sensitive_Loss : 0.19112, Sensitive_Acc : 20.820, Sensitive_Auc : 0.998, Mean auc: 0.857, Run Time : 231.82 sec
INFO:root:2024-04-18 13:36:52, Train, Epoch : 7, Step : 4110, Loss : 0.38023, Acc : 0.834, Sensitive_Loss : 0.11465, Sensitive_Acc : 21.900, Run Time : 244.42 sec
INFO:root:2024-04-18 13:37:11, Train, Epoch : 7, Step : 4120, Loss : 0.37448, Acc : 0.825, Sensitive_Loss : 0.09646, Sensitive_Acc : 20.500, Run Time : 18.14 sec
INFO:root:2024-04-18 13:37:28, Train, Epoch : 7, Step : 4130, Loss : 0.37041, Acc : 0.816, Sensitive_Loss : 0.12096, Sensitive_Acc : 22.100, Run Time : 17.52 sec
INFO:root:2024-04-18 13:37:47, Train, Epoch : 7, Step : 4140, Loss : 0.37347, Acc : 0.859, Sensitive_Loss : 0.08294, Sensitive_Acc : 24.600, Run Time : 18.71 sec
INFO:root:2024-04-18 13:38:04, Train, Epoch : 7, Step : 4150, Loss : 0.35420, Acc : 0.850, Sensitive_Loss : 0.10206, Sensitive_Acc : 18.300, Run Time : 16.98 sec
INFO:root:2024-04-18 13:38:21, Train, Epoch : 7, Step : 4160, Loss : 0.36833, Acc : 0.819, Sensitive_Loss : 0.13411, Sensitive_Acc : 22.900, Run Time : 17.54 sec
INFO:root:2024-04-18 13:38:41, Train, Epoch : 7, Step : 4170, Loss : 0.33562, Acc : 0.863, Sensitive_Loss : 0.10037, Sensitive_Acc : 24.300, Run Time : 19.31 sec
INFO:root:2024-04-18 13:38:59, Train, Epoch : 7, Step : 4180, Loss : 0.44957, Acc : 0.809, Sensitive_Loss : 0.13576, Sensitive_Acc : 16.200, Run Time : 18.63 sec
INFO:root:2024-04-18 13:39:17, Train, Epoch : 7, Step : 4190, Loss : 0.43933, Acc : 0.797, Sensitive_Loss : 0.09568, Sensitive_Acc : 21.700, Run Time : 18.13 sec
INFO:root:2024-04-18 13:39:35, Train, Epoch : 7, Step : 4200, Loss : 0.38585, Acc : 0.816, Sensitive_Loss : 0.11188, Sensitive_Acc : 20.300, Run Time : 17.65 sec
INFO:root:2024-04-18 13:43:25, Dev, Step : 4200, Loss : 0.52553, Acc : 0.783, Auc : 0.858, Sensitive_Loss : 0.16657, Sensitive_Acc : 21.887, Sensitive_Auc : 0.998, Mean auc: 0.858, Run Time : 229.96 sec
INFO:root:2024-04-18 13:43:37, Train, Epoch : 7, Step : 4210, Loss : 0.30665, Acc : 0.875, Sensitive_Loss : 0.09620, Sensitive_Acc : 23.000, Run Time : 241.93 sec
INFO:root:2024-04-18 13:43:55, Train, Epoch : 7, Step : 4220, Loss : 0.33958, Acc : 0.856, Sensitive_Loss : 0.12902, Sensitive_Acc : 22.300, Run Time : 17.80 sec
INFO:root:2024-04-18 13:44:13, Train, Epoch : 7, Step : 4230, Loss : 0.35209, Acc : 0.831, Sensitive_Loss : 0.11786, Sensitive_Acc : 23.200, Run Time : 17.92 sec
INFO:root:2024-04-18 13:44:30, Train, Epoch : 7, Step : 4240, Loss : 0.38218, Acc : 0.844, Sensitive_Loss : 0.12355, Sensitive_Acc : 16.100, Run Time : 17.79 sec
INFO:root:2024-04-18 13:44:48, Train, Epoch : 7, Step : 4250, Loss : 0.32921, Acc : 0.850, Sensitive_Loss : 0.10481, Sensitive_Acc : 21.100, Run Time : 17.04 sec
INFO:root:2024-04-18 13:45:05, Train, Epoch : 7, Step : 4260, Loss : 0.39107, Acc : 0.822, Sensitive_Loss : 0.09476, Sensitive_Acc : 21.900, Run Time : 17.67 sec
INFO:root:2024-04-18 13:45:23, Train, Epoch : 7, Step : 4270, Loss : 0.36484, Acc : 0.850, Sensitive_Loss : 0.10381, Sensitive_Acc : 18.500, Run Time : 17.50 sec
INFO:root:2024-04-18 13:45:40, Train, Epoch : 7, Step : 4280, Loss : 0.36449, Acc : 0.866, Sensitive_Loss : 0.08826, Sensitive_Acc : 22.300, Run Time : 17.44 sec
INFO:root:2024-04-18 13:45:57, Train, Epoch : 7, Step : 4290, Loss : 0.29751, Acc : 0.897, Sensitive_Loss : 0.15838, Sensitive_Acc : 22.600, Run Time : 16.86 sec
INFO:root:2024-04-18 13:46:15, Train, Epoch : 7, Step : 4300, Loss : 0.34834, Acc : 0.884, Sensitive_Loss : 0.12503, Sensitive_Acc : 21.600, Run Time : 18.05 sec
INFO:root:2024-04-18 13:50:05, Dev, Step : 4300, Loss : 0.53115, Acc : 0.782, Auc : 0.858, Sensitive_Loss : 0.17624, Sensitive_Acc : 21.722, Sensitive_Auc : 0.997, Mean auc: 0.858, Run Time : 230.05 sec
INFO:root:2024-04-18 13:50:17, Train, Epoch : 7, Step : 4310, Loss : 0.31581, Acc : 0.856, Sensitive_Loss : 0.17834, Sensitive_Acc : 19.400, Run Time : 242.29 sec
INFO:root:2024-04-18 13:50:37, Train, Epoch : 7, Step : 4320, Loss : 0.35148, Acc : 0.828, Sensitive_Loss : 0.14466, Sensitive_Acc : 22.600, Run Time : 19.47 sec
INFO:root:2024-04-18 13:50:55, Train, Epoch : 7, Step : 4330, Loss : 0.34313, Acc : 0.872, Sensitive_Loss : 0.14689, Sensitive_Acc : 21.800, Run Time : 18.55 sec
INFO:root:2024-04-18 13:51:14, Train, Epoch : 7, Step : 4340, Loss : 0.36002, Acc : 0.838, Sensitive_Loss : 0.13572, Sensitive_Acc : 17.100, Run Time : 18.62 sec
INFO:root:2024-04-18 13:51:32, Train, Epoch : 7, Step : 4350, Loss : 0.39281, Acc : 0.816, Sensitive_Loss : 0.12725, Sensitive_Acc : 21.700, Run Time : 18.54 sec
INFO:root:2024-04-18 13:51:51, Train, Epoch : 7, Step : 4360, Loss : 0.39820, Acc : 0.834, Sensitive_Loss : 0.13516, Sensitive_Acc : 22.300, Run Time : 18.86 sec
INFO:root:2024-04-18 13:52:12, Train, Epoch : 7, Step : 4370, Loss : 0.37938, Acc : 0.803, Sensitive_Loss : 0.14483, Sensitive_Acc : 25.400, Run Time : 20.88 sec
INFO:root:2024-04-18 13:52:31, Train, Epoch : 7, Step : 4380, Loss : 0.36545, Acc : 0.863, Sensitive_Loss : 0.11510, Sensitive_Acc : 14.700, Run Time : 18.73 sec
INFO:root:2024-04-18 13:52:48, Train, Epoch : 7, Step : 4390, Loss : 0.34363, Acc : 0.844, Sensitive_Loss : 0.08211, Sensitive_Acc : 23.900, Run Time : 17.22 sec
INFO:root:2024-04-18 13:53:07, Train, Epoch : 7, Step : 4400, Loss : 0.36874, Acc : 0.834, Sensitive_Loss : 0.09438, Sensitive_Acc : 16.000, Run Time : 18.47 sec
INFO:root:2024-04-18 13:57:00, Dev, Step : 4400, Loss : 0.54151, Acc : 0.780, Auc : 0.853, Sensitive_Loss : 0.17222, Sensitive_Acc : 22.053, Sensitive_Auc : 0.998, Mean auc: 0.853, Run Time : 233.62 sec
INFO:root:2024-04-18 13:57:13, Train, Epoch : 7, Step : 4410, Loss : 0.36989, Acc : 0.853, Sensitive_Loss : 0.09634, Sensitive_Acc : 22.900, Run Time : 246.47 sec
INFO:root:2024-04-18 13:57:34, Train, Epoch : 7, Step : 4420, Loss : 0.35145, Acc : 0.834, Sensitive_Loss : 0.10907, Sensitive_Acc : 22.200, Run Time : 20.73 sec
INFO:root:2024-04-18 13:57:51, Train, Epoch : 7, Step : 4430, Loss : 0.32517, Acc : 0.869, Sensitive_Loss : 0.14240, Sensitive_Acc : 20.000, Run Time : 17.46 sec
INFO:root:2024-04-18 14:01:51
INFO:root:y_pred: [0.08244067 0.00131827 0.12731872 ... 0.42868012 0.06859648 0.00517611]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.0021535e-02 1.6262388e-02 1.1155402e-03 8.9311250e-02 4.2011663e-03
 1.4062405e-04 1.1622526e-04 1.9343033e-03 4.9548585e-02 9.9986422e-01
 4.6382654e-02 1.4300211e-03 9.6796990e-05 3.8369687e-04 9.9959034e-01
 1.5479482e-02 6.1559379e-03 9.9984789e-01 9.9983621e-01 1.0765964e-02
 9.7073370e-01 2.2109551e-04 2.7246976e-03 3.6640960e-04 9.6854074e-03
 7.3216811e-02 1.3580188e-04 2.2105209e-03 1.4719255e-04 1.2459841e-02
 1.7036678e-03 9.7767627e-01 1.3943613e-03 6.0505986e-01 9.1837348e-05
 2.3129200e-04 8.7535353e-03 2.0871650e-02 6.7094818e-02 2.3015465e-03
 1.3492164e-02 9.9830210e-01 3.9463490e-04 6.9723098e-04 9.0166473e-01
 4.1671032e-03 3.7627342e-01 1.0019891e-01 5.4646891e-01 9.8087823e-01
 9.8921353e-01 9.9987173e-01 9.9463356e-01 5.2939245e-04 7.7526615e-04
 1.3135073e-01 4.2363204e-04 1.4221037e-03 9.9519324e-01 2.1608451e-03
 5.5541750e-06 1.2391945e-03 1.3292528e-03 2.2890311e-04 9.9596858e-01
 3.2284018e-03 3.0885312e-06 2.5384945e-01 3.4548400e-03 9.9665266e-01
 9.9997807e-01 9.9973363e-01 1.8043871e-04 4.2497000e-01 3.8725990e-04
 2.6913092e-01 7.0342630e-02 3.4275445e-06 9.5993973e-06 2.3408113e-04
 2.1300180e-02 5.4784159e-05 9.9666923e-01 9.9781513e-01 3.6653539e-04
 6.9575774e-04 8.3310567e-02 1.0472455e-03 2.2670980e-04 6.5791071e-04
 2.8196482e-03 4.1832633e-02 1.0571316e-04 1.5170161e-06 5.2629039e-03
 3.9119385e-03 1.0677947e-04 4.6181816e-01 1.4121990e-03 2.1255761e-02
 2.2518064e-03 5.7627301e-04 3.2649165e-01 3.8806455e-05 1.5883865e-02
 1.0578865e-03 3.6225424e-03 2.1769449e-01 5.1770699e-01 8.6243294e-02
 2.8886680e-06 9.9996114e-01 9.9876297e-01 4.8870111e-06 4.5896418e-02
 2.5755394e-02 3.7520058e-02 4.0352310e-04 4.0125352e-01 7.1425387e-03
 2.9184560e-03 4.2662755e-04 2.2227755e-03 2.1213868e-06 7.3156720e-03
 5.3043216e-01 6.4347037e-06 9.9578875e-01 1.3693894e-02 2.6133609e-01
 1.8911177e-03 7.9867013e-02 5.0384693e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 14:01:51, Dev, Step : 4438, Loss : 0.52784, Acc : 0.782, Auc : 0.856, Sensitive_Loss : 0.17401, Sensitive_Acc : 21.752, Sensitive_Auc : 0.999, Mean auc: 0.856, Run Time : 225.48 sec
INFO:root:2024-04-18 14:01:58, Train, Epoch : 8, Step : 4440, Loss : 0.04926, Acc : 0.194, Sensitive_Loss : 0.01613, Sensitive_Acc : 3.800, Run Time : 5.80 sec
INFO:root:2024-04-18 14:02:16, Train, Epoch : 8, Step : 4450, Loss : 0.31525, Acc : 0.850, Sensitive_Loss : 0.07973, Sensitive_Acc : 21.900, Run Time : 18.09 sec
INFO:root:2024-04-18 14:02:35, Train, Epoch : 8, Step : 4460, Loss : 0.31605, Acc : 0.853, Sensitive_Loss : 0.08273, Sensitive_Acc : 21.200, Run Time : 18.95 sec
INFO:root:2024-04-18 14:02:52, Train, Epoch : 8, Step : 4470, Loss : 0.37852, Acc : 0.841, Sensitive_Loss : 0.07459, Sensitive_Acc : 22.900, Run Time : 17.35 sec
INFO:root:2024-04-18 14:03:11, Train, Epoch : 8, Step : 4480, Loss : 0.37030, Acc : 0.853, Sensitive_Loss : 0.09628, Sensitive_Acc : 23.500, Run Time : 18.40 sec
INFO:root:2024-04-18 14:03:28, Train, Epoch : 8, Step : 4490, Loss : 0.35865, Acc : 0.847, Sensitive_Loss : 0.10245, Sensitive_Acc : 22.100, Run Time : 17.80 sec
INFO:root:2024-04-18 14:03:47, Train, Epoch : 8, Step : 4500, Loss : 0.37755, Acc : 0.856, Sensitive_Loss : 0.08572, Sensitive_Acc : 19.800, Run Time : 18.65 sec
INFO:root:2024-04-18 14:07:40, Dev, Step : 4500, Loss : 0.52336, Acc : 0.781, Auc : 0.856, Sensitive_Loss : 0.16784, Sensitive_Acc : 21.947, Sensitive_Auc : 1.000, Mean auc: 0.856, Run Time : 232.67 sec
INFO:root:2024-04-18 14:07:52, Train, Epoch : 8, Step : 4510, Loss : 0.32404, Acc : 0.878, Sensitive_Loss : 0.11691, Sensitive_Acc : 20.800, Run Time : 245.11 sec
INFO:root:2024-04-18 14:08:09, Train, Epoch : 8, Step : 4520, Loss : 0.34399, Acc : 0.828, Sensitive_Loss : 0.09173, Sensitive_Acc : 18.500, Run Time : 16.84 sec
INFO:root:2024-04-18 14:08:26, Train, Epoch : 8, Step : 4530, Loss : 0.33462, Acc : 0.859, Sensitive_Loss : 0.13084, Sensitive_Acc : 22.000, Run Time : 16.60 sec
INFO:root:2024-04-18 14:08:44, Train, Epoch : 8, Step : 4540, Loss : 0.35500, Acc : 0.834, Sensitive_Loss : 0.09768, Sensitive_Acc : 22.300, Run Time : 18.30 sec
INFO:root:2024-04-18 14:09:01, Train, Epoch : 8, Step : 4550, Loss : 0.32141, Acc : 0.869, Sensitive_Loss : 0.13441, Sensitive_Acc : 19.400, Run Time : 17.38 sec
INFO:root:2024-04-18 14:09:20, Train, Epoch : 8, Step : 4560, Loss : 0.38458, Acc : 0.825, Sensitive_Loss : 0.12408, Sensitive_Acc : 20.800, Run Time : 19.04 sec
INFO:root:2024-04-18 14:09:36, Train, Epoch : 8, Step : 4570, Loss : 0.30965, Acc : 0.863, Sensitive_Loss : 0.12643, Sensitive_Acc : 20.200, Run Time : 16.15 sec
INFO:root:2024-04-18 14:09:54, Train, Epoch : 8, Step : 4580, Loss : 0.28005, Acc : 0.891, Sensitive_Loss : 0.09906, Sensitive_Acc : 20.000, Run Time : 17.95 sec
INFO:root:2024-04-18 14:10:11, Train, Epoch : 8, Step : 4590, Loss : 0.36297, Acc : 0.822, Sensitive_Loss : 0.11695, Sensitive_Acc : 20.600, Run Time : 16.32 sec
INFO:root:2024-04-18 14:10:27, Train, Epoch : 8, Step : 4600, Loss : 0.30296, Acc : 0.872, Sensitive_Loss : 0.12254, Sensitive_Acc : 22.000, Run Time : 16.62 sec
INFO:root:2024-04-18 14:14:36, Dev, Step : 4600, Loss : 0.56255, Acc : 0.775, Auc : 0.851, Sensitive_Loss : 0.17986, Sensitive_Acc : 21.466, Sensitive_Auc : 0.999, Mean auc: 0.851, Run Time : 248.35 sec
INFO:root:2024-04-18 14:14:48, Train, Epoch : 8, Step : 4610, Loss : 0.38528, Acc : 0.844, Sensitive_Loss : 0.13983, Sensitive_Acc : 17.900, Run Time : 260.24 sec
INFO:root:2024-04-18 14:15:05, Train, Epoch : 8, Step : 4620, Loss : 0.38960, Acc : 0.822, Sensitive_Loss : 0.07551, Sensitive_Acc : 25.700, Run Time : 17.67 sec
INFO:root:2024-04-18 14:15:21, Train, Epoch : 8, Step : 4630, Loss : 0.32236, Acc : 0.834, Sensitive_Loss : 0.14565, Sensitive_Acc : 24.000, Run Time : 16.20 sec
INFO:root:2024-04-18 14:15:37, Train, Epoch : 8, Step : 4640, Loss : 0.32826, Acc : 0.866, Sensitive_Loss : 0.11795, Sensitive_Acc : 22.000, Run Time : 15.90 sec
INFO:root:2024-04-18 14:15:56, Train, Epoch : 8, Step : 4650, Loss : 0.28018, Acc : 0.866, Sensitive_Loss : 0.09559, Sensitive_Acc : 23.100, Run Time : 18.38 sec
INFO:root:2024-04-18 14:16:10, Train, Epoch : 8, Step : 4660, Loss : 0.36254, Acc : 0.853, Sensitive_Loss : 0.07502, Sensitive_Acc : 21.300, Run Time : 14.77 sec
INFO:root:2024-04-18 14:16:29, Train, Epoch : 8, Step : 4670, Loss : 0.33499, Acc : 0.875, Sensitive_Loss : 0.07375, Sensitive_Acc : 24.600, Run Time : 18.14 sec
INFO:root:2024-04-18 14:16:46, Train, Epoch : 8, Step : 4680, Loss : 0.42501, Acc : 0.841, Sensitive_Loss : 0.13989, Sensitive_Acc : 21.800, Run Time : 17.59 sec
INFO:root:2024-04-18 14:17:03, Train, Epoch : 8, Step : 4690, Loss : 0.36546, Acc : 0.847, Sensitive_Loss : 0.06695, Sensitive_Acc : 18.600, Run Time : 16.47 sec
INFO:root:2024-04-18 14:17:18, Train, Epoch : 8, Step : 4700, Loss : 0.32640, Acc : 0.859, Sensitive_Loss : 0.11472, Sensitive_Acc : 18.200, Run Time : 15.13 sec
INFO:root:2024-04-18 14:21:09, Dev, Step : 4700, Loss : 0.52454, Acc : 0.777, Auc : 0.855, Sensitive_Loss : 0.18823, Sensitive_Acc : 21.316, Sensitive_Auc : 1.000, Mean auc: 0.855, Run Time : 230.97 sec
INFO:root:2024-04-18 14:21:20, Train, Epoch : 8, Step : 4710, Loss : 0.38459, Acc : 0.841, Sensitive_Loss : 0.08664, Sensitive_Acc : 23.400, Run Time : 241.71 sec
INFO:root:2024-04-18 14:21:37, Train, Epoch : 8, Step : 4720, Loss : 0.44295, Acc : 0.828, Sensitive_Loss : 0.09110, Sensitive_Acc : 18.300, Run Time : 17.68 sec
INFO:root:2024-04-18 14:21:55, Train, Epoch : 8, Step : 4730, Loss : 0.38226, Acc : 0.828, Sensitive_Loss : 0.10555, Sensitive_Acc : 18.500, Run Time : 17.67 sec
INFO:root:2024-04-18 14:22:11, Train, Epoch : 8, Step : 4740, Loss : 0.36001, Acc : 0.856, Sensitive_Loss : 0.09841, Sensitive_Acc : 24.900, Run Time : 16.48 sec
INFO:root:2024-04-18 14:22:29, Train, Epoch : 8, Step : 4750, Loss : 0.31304, Acc : 0.841, Sensitive_Loss : 0.14303, Sensitive_Acc : 25.100, Run Time : 17.67 sec
INFO:root:2024-04-18 14:22:45, Train, Epoch : 8, Step : 4760, Loss : 0.33068, Acc : 0.838, Sensitive_Loss : 0.09248, Sensitive_Acc : 23.800, Run Time : 15.90 sec
INFO:root:2024-04-18 14:23:02, Train, Epoch : 8, Step : 4770, Loss : 0.35566, Acc : 0.828, Sensitive_Loss : 0.06722, Sensitive_Acc : 19.600, Run Time : 17.34 sec
INFO:root:2024-04-18 14:23:20, Train, Epoch : 8, Step : 4780, Loss : 0.31560, Acc : 0.875, Sensitive_Loss : 0.08544, Sensitive_Acc : 22.700, Run Time : 17.68 sec
INFO:root:2024-04-18 14:23:36, Train, Epoch : 8, Step : 4790, Loss : 0.45124, Acc : 0.787, Sensitive_Loss : 0.10396, Sensitive_Acc : 22.400, Run Time : 16.43 sec
INFO:root:2024-04-18 14:23:53, Train, Epoch : 8, Step : 4800, Loss : 0.35331, Acc : 0.869, Sensitive_Loss : 0.09763, Sensitive_Acc : 22.300, Run Time : 16.81 sec
INFO:root:2024-04-18 14:27:43, Dev, Step : 4800, Loss : 0.52979, Acc : 0.773, Auc : 0.850, Sensitive_Loss : 0.16741, Sensitive_Acc : 22.008, Sensitive_Auc : 0.999, Mean auc: 0.850, Run Time : 229.86 sec
INFO:root:2024-04-18 14:27:55, Train, Epoch : 8, Step : 4810, Loss : 0.33441, Acc : 0.850, Sensitive_Loss : 0.13365, Sensitive_Acc : 21.100, Run Time : 241.92 sec
INFO:root:2024-04-18 14:28:13, Train, Epoch : 8, Step : 4820, Loss : 0.35874, Acc : 0.819, Sensitive_Loss : 0.14701, Sensitive_Acc : 13.100, Run Time : 17.60 sec
INFO:root:2024-04-18 14:28:29, Train, Epoch : 8, Step : 4830, Loss : 0.30069, Acc : 0.869, Sensitive_Loss : 0.10621, Sensitive_Acc : 22.700, Run Time : 16.26 sec
INFO:root:2024-04-18 14:28:47, Train, Epoch : 8, Step : 4840, Loss : 0.37012, Acc : 0.856, Sensitive_Loss : 0.10033, Sensitive_Acc : 18.400, Run Time : 17.76 sec
INFO:root:2024-04-18 14:29:04, Train, Epoch : 8, Step : 4850, Loss : 0.27975, Acc : 0.844, Sensitive_Loss : 0.08989, Sensitive_Acc : 18.200, Run Time : 17.61 sec
INFO:root:2024-04-18 14:29:22, Train, Epoch : 8, Step : 4860, Loss : 0.31633, Acc : 0.841, Sensitive_Loss : 0.09942, Sensitive_Acc : 21.300, Run Time : 17.39 sec
INFO:root:2024-04-18 14:29:39, Train, Epoch : 8, Step : 4870, Loss : 0.31188, Acc : 0.869, Sensitive_Loss : 0.08442, Sensitive_Acc : 19.300, Run Time : 16.90 sec
INFO:root:2024-04-18 14:29:56, Train, Epoch : 8, Step : 4880, Loss : 0.36242, Acc : 0.825, Sensitive_Loss : 0.06736, Sensitive_Acc : 19.900, Run Time : 17.29 sec
INFO:root:2024-04-18 14:30:12, Train, Epoch : 8, Step : 4890, Loss : 0.29278, Acc : 0.856, Sensitive_Loss : 0.08627, Sensitive_Acc : 20.300, Run Time : 16.05 sec
INFO:root:2024-04-18 14:30:29, Train, Epoch : 8, Step : 4900, Loss : 0.38983, Acc : 0.838, Sensitive_Loss : 0.12928, Sensitive_Acc : 24.200, Run Time : 17.16 sec
INFO:root:2024-04-18 14:34:20, Dev, Step : 4900, Loss : 0.55786, Acc : 0.775, Auc : 0.855, Sensitive_Loss : 0.16468, Sensitive_Acc : 21.902, Sensitive_Auc : 1.000, Mean auc: 0.855, Run Time : 231.30 sec
INFO:root:2024-04-18 14:34:33, Train, Epoch : 8, Step : 4910, Loss : 0.34931, Acc : 0.859, Sensitive_Loss : 0.20481, Sensitive_Acc : 22.700, Run Time : 243.53 sec
INFO:root:2024-04-18 14:34:49, Train, Epoch : 8, Step : 4920, Loss : 0.35161, Acc : 0.866, Sensitive_Loss : 0.13477, Sensitive_Acc : 16.500, Run Time : 16.02 sec
INFO:root:2024-04-18 14:35:07, Train, Epoch : 8, Step : 4930, Loss : 0.36005, Acc : 0.816, Sensitive_Loss : 0.12151, Sensitive_Acc : 25.800, Run Time : 17.90 sec
INFO:root:2024-04-18 14:35:23, Train, Epoch : 8, Step : 4940, Loss : 0.39110, Acc : 0.847, Sensitive_Loss : 0.10153, Sensitive_Acc : 22.800, Run Time : 16.47 sec
INFO:root:2024-04-18 14:35:41, Train, Epoch : 8, Step : 4950, Loss : 0.41719, Acc : 0.800, Sensitive_Loss : 0.10154, Sensitive_Acc : 23.000, Run Time : 18.03 sec
INFO:root:2024-04-18 14:35:58, Train, Epoch : 8, Step : 4960, Loss : 0.36164, Acc : 0.841, Sensitive_Loss : 0.12036, Sensitive_Acc : 18.500, Run Time : 17.41 sec
INFO:root:2024-04-18 14:36:15, Train, Epoch : 8, Step : 4970, Loss : 0.32914, Acc : 0.803, Sensitive_Loss : 0.11383, Sensitive_Acc : 23.700, Run Time : 16.85 sec
INFO:root:2024-04-18 14:36:31, Train, Epoch : 8, Step : 4980, Loss : 0.26453, Acc : 0.891, Sensitive_Loss : 0.09276, Sensitive_Acc : 21.700, Run Time : 16.16 sec
INFO:root:2024-04-18 14:36:48, Train, Epoch : 8, Step : 4990, Loss : 0.39601, Acc : 0.844, Sensitive_Loss : 0.08661, Sensitive_Acc : 15.800, Run Time : 16.98 sec
INFO:root:2024-04-18 14:37:06, Train, Epoch : 8, Step : 5000, Loss : 0.34322, Acc : 0.844, Sensitive_Loss : 0.13014, Sensitive_Acc : 23.400, Run Time : 17.09 sec
INFO:root:2024-04-18 14:40:54, Dev, Step : 5000, Loss : 0.53412, Acc : 0.780, Auc : 0.856, Sensitive_Loss : 0.16842, Sensitive_Acc : 22.008, Sensitive_Auc : 1.000, Mean auc: 0.856, Run Time : 227.97 sec
INFO:root:2024-04-18 14:41:06, Train, Epoch : 8, Step : 5010, Loss : 0.31608, Acc : 0.859, Sensitive_Loss : 0.12924, Sensitive_Acc : 20.500, Run Time : 239.98 sec
INFO:root:2024-04-18 14:41:23, Train, Epoch : 8, Step : 5020, Loss : 0.30218, Acc : 0.866, Sensitive_Loss : 0.10052, Sensitive_Acc : 19.400, Run Time : 17.39 sec
INFO:root:2024-04-18 14:41:39, Train, Epoch : 8, Step : 5030, Loss : 0.36001, Acc : 0.825, Sensitive_Loss : 0.08103, Sensitive_Acc : 22.600, Run Time : 16.45 sec
INFO:root:2024-04-18 14:41:55, Train, Epoch : 8, Step : 5040, Loss : 0.36300, Acc : 0.816, Sensitive_Loss : 0.08400, Sensitive_Acc : 22.100, Run Time : 16.02 sec
INFO:root:2024-04-18 14:42:13, Train, Epoch : 8, Step : 5050, Loss : 0.32865, Acc : 0.866, Sensitive_Loss : 0.11749, Sensitive_Acc : 24.500, Run Time : 17.83 sec
INFO:root:2024-04-18 14:42:31, Train, Epoch : 8, Step : 5060, Loss : 0.29325, Acc : 0.875, Sensitive_Loss : 0.07294, Sensitive_Acc : 22.300, Run Time : 17.43 sec
INFO:root:2024-04-18 14:42:48, Train, Epoch : 8, Step : 5070, Loss : 0.33719, Acc : 0.872, Sensitive_Loss : 0.11260, Sensitive_Acc : 23.800, Run Time : 16.85 sec
INFO:root:2024-04-18 14:46:37
INFO:root:y_pred: [0.0894784  0.0008056  0.07890835 ... 0.47508812 0.04187869 0.01419189]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [8.56011081e-03 2.98732594e-02 4.91387793e-04 7.53816739e-02
 1.07784085e-02 9.81341072e-05 3.44639484e-05 3.58155649e-03
 7.77617916e-02 9.99875903e-01 8.28855038e-02 4.03037528e-03
 1.07993219e-04 3.69260495e-04 9.99734104e-01 1.53148510e-02
 1.25487838e-02 9.99814689e-01 9.99891639e-01 5.51307388e-03
 9.62608874e-01 3.77639895e-04 2.32215272e-03 1.17253547e-03
 1.33183217e-02 2.36220062e-02 1.11173948e-04 1.69580069e-03
 3.24373046e-04 2.40308512e-02 2.21054815e-03 9.87939477e-01
 1.75624981e-03 6.79229736e-01 4.71976746e-05 3.91390000e-04
 1.16518792e-02 2.67044865e-02 6.40805140e-02 3.87331122e-03
 5.83025478e-02 9.98919964e-01 5.18556975e-04 3.30176961e-04
 9.53760087e-01 5.82167879e-03 4.92448777e-01 1.66636571e-01
 5.23856044e-01 9.93859529e-01 9.92882371e-01 9.99861360e-01
 9.95254517e-01 3.30209383e-04 3.02792434e-03 1.32523909e-01
 2.77970801e-04 4.05110477e-04 9.97734904e-01 2.77908356e-03
 9.50945650e-06 9.42449376e-04 2.91202171e-03 3.34792712e-04
 9.96738017e-01 3.83644900e-03 3.02203648e-06 2.42039979e-01
 9.27497353e-03 9.96895671e-01 9.99981642e-01 9.99805868e-01
 2.56079191e-04 6.18315995e-01 4.71497857e-04 3.40041935e-01
 4.27922644e-02 2.59587409e-06 8.11888458e-06 7.68234779e-04
 2.60565281e-02 4.79073969e-05 9.97280240e-01 9.98473465e-01
 4.83527023e-04 1.63576566e-03 5.44989780e-02 6.58850302e-04
 4.58811206e-04 1.15804444e-03 5.32168197e-03 6.44113421e-02
 6.75739429e-05 2.44297689e-06 7.02945981e-03 6.00631023e-03
 1.19867924e-04 5.36565602e-01 9.69193992e-04 5.78962825e-03
 5.70113165e-03 1.55073917e-03 4.34552699e-01 1.54191410e-04
 2.42219530e-02 1.42551376e-03 4.87217307e-03 4.15827960e-01
 6.09385908e-01 1.14429802e-01 1.70803605e-06 9.99961019e-01
 9.99115884e-01 1.44113501e-05 1.79315712e-02 4.23124060e-02
 4.58871685e-02 1.23447215e-03 3.64002824e-01 2.88663898e-02
 2.47817719e-03 5.31122729e-04 3.94394901e-03 2.04363278e-06
 5.44541003e-03 7.58916438e-01 6.60764272e-07 9.97605324e-01
 6.60093278e-02 3.31744313e-01 1.39575684e-03 1.30974889e-01
 5.79325047e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 14:46:37, Dev, Step : 5072, Loss : 0.54176, Acc : 0.780, Auc : 0.855, Sensitive_Loss : 0.17979, Sensitive_Acc : 21.556, Sensitive_Auc : 0.998, Mean auc: 0.855, Run Time : 226.54 sec
INFO:root:2024-04-18 14:46:54, Train, Epoch : 9, Step : 5080, Loss : 0.26031, Acc : 0.703, Sensitive_Loss : 0.06295, Sensitive_Acc : 19.300, Run Time : 15.45 sec
INFO:root:2024-04-18 14:47:12, Train, Epoch : 9, Step : 5090, Loss : 0.31640, Acc : 0.859, Sensitive_Loss : 0.05133, Sensitive_Acc : 21.700, Run Time : 18.29 sec
INFO:root:2024-04-18 14:47:31, Train, Epoch : 9, Step : 5100, Loss : 0.27264, Acc : 0.878, Sensitive_Loss : 0.14835, Sensitive_Acc : 21.200, Run Time : 19.09 sec
INFO:root:2024-04-18 14:51:19, Dev, Step : 5100, Loss : 0.52550, Acc : 0.783, Auc : 0.856, Sensitive_Loss : 0.16867, Sensitive_Acc : 21.662, Sensitive_Auc : 0.999, Mean auc: 0.856, Run Time : 227.74 sec
INFO:root:2024-04-18 14:51:31, Train, Epoch : 9, Step : 5110, Loss : 0.31676, Acc : 0.853, Sensitive_Loss : 0.09141, Sensitive_Acc : 26.600, Run Time : 239.88 sec
INFO:root:2024-04-18 14:51:48, Train, Epoch : 9, Step : 5120, Loss : 0.33626, Acc : 0.834, Sensitive_Loss : 0.10232, Sensitive_Acc : 23.300, Run Time : 17.62 sec
INFO:root:2024-04-18 14:52:06, Train, Epoch : 9, Step : 5130, Loss : 0.28459, Acc : 0.869, Sensitive_Loss : 0.11336, Sensitive_Acc : 22.000, Run Time : 17.89 sec
INFO:root:2024-04-18 14:52:25, Train, Epoch : 9, Step : 5140, Loss : 0.32487, Acc : 0.850, Sensitive_Loss : 0.10300, Sensitive_Acc : 18.900, Run Time : 18.65 sec
INFO:root:2024-04-18 14:52:41, Train, Epoch : 9, Step : 5150, Loss : 0.25846, Acc : 0.916, Sensitive_Loss : 0.11048, Sensitive_Acc : 22.700, Run Time : 15.71 sec
INFO:root:2024-04-18 14:52:58, Train, Epoch : 9, Step : 5160, Loss : 0.31272, Acc : 0.875, Sensitive_Loss : 0.09490, Sensitive_Acc : 22.800, Run Time : 16.87 sec
INFO:root:2024-04-18 14:53:14, Train, Epoch : 9, Step : 5170, Loss : 0.28174, Acc : 0.850, Sensitive_Loss : 0.10315, Sensitive_Acc : 25.700, Run Time : 16.92 sec
INFO:root:2024-04-18 14:53:31, Train, Epoch : 9, Step : 5180, Loss : 0.31352, Acc : 0.887, Sensitive_Loss : 0.11345, Sensitive_Acc : 19.300, Run Time : 16.43 sec
INFO:root:2024-04-18 14:53:48, Train, Epoch : 9, Step : 5190, Loss : 0.29799, Acc : 0.859, Sensitive_Loss : 0.09119, Sensitive_Acc : 18.600, Run Time : 17.04 sec
INFO:root:2024-04-18 14:54:03, Train, Epoch : 9, Step : 5200, Loss : 0.36325, Acc : 0.841, Sensitive_Loss : 0.08187, Sensitive_Acc : 22.900, Run Time : 15.17 sec
INFO:root:2024-04-18 14:57:54, Dev, Step : 5200, Loss : 0.53845, Acc : 0.779, Auc : 0.855, Sensitive_Loss : 0.16541, Sensitive_Acc : 21.812, Sensitive_Auc : 0.998, Mean auc: 0.855, Run Time : 230.47 sec
INFO:root:2024-04-18 14:58:06, Train, Epoch : 9, Step : 5210, Loss : 0.31585, Acc : 0.863, Sensitive_Loss : 0.04683, Sensitive_Acc : 21.000, Run Time : 243.16 sec
INFO:root:2024-04-18 14:58:24, Train, Epoch : 9, Step : 5220, Loss : 0.30398, Acc : 0.869, Sensitive_Loss : 0.08145, Sensitive_Acc : 19.700, Run Time : 17.50 sec
INFO:root:2024-04-18 14:58:41, Train, Epoch : 9, Step : 5230, Loss : 0.30095, Acc : 0.884, Sensitive_Loss : 0.10496, Sensitive_Acc : 18.900, Run Time : 17.05 sec
INFO:root:2024-04-18 14:58:58, Train, Epoch : 9, Step : 5240, Loss : 0.30239, Acc : 0.869, Sensitive_Loss : 0.09472, Sensitive_Acc : 24.100, Run Time : 17.37 sec
INFO:root:2024-04-18 14:59:16, Train, Epoch : 9, Step : 5250, Loss : 0.31773, Acc : 0.869, Sensitive_Loss : 0.07691, Sensitive_Acc : 21.800, Run Time : 17.34 sec
INFO:root:2024-04-18 14:59:33, Train, Epoch : 9, Step : 5260, Loss : 0.30767, Acc : 0.850, Sensitive_Loss : 0.10344, Sensitive_Acc : 15.700, Run Time : 17.62 sec
INFO:root:2024-04-18 14:59:50, Train, Epoch : 9, Step : 5270, Loss : 0.45566, Acc : 0.828, Sensitive_Loss : 0.11273, Sensitive_Acc : 20.700, Run Time : 17.03 sec
INFO:root:2024-04-18 15:00:08, Train, Epoch : 9, Step : 5280, Loss : 0.25509, Acc : 0.900, Sensitive_Loss : 0.09804, Sensitive_Acc : 23.000, Run Time : 17.93 sec
INFO:root:2024-04-18 15:00:25, Train, Epoch : 9, Step : 5290, Loss : 0.32975, Acc : 0.834, Sensitive_Loss : 0.13348, Sensitive_Acc : 25.900, Run Time : 17.26 sec
INFO:root:2024-04-18 15:00:42, Train, Epoch : 9, Step : 5300, Loss : 0.27906, Acc : 0.878, Sensitive_Loss : 0.04718, Sensitive_Acc : 21.700, Run Time : 16.46 sec
INFO:root:2024-04-18 15:04:29, Dev, Step : 5300, Loss : 0.54019, Acc : 0.780, Auc : 0.852, Sensitive_Loss : 0.17724, Sensitive_Acc : 21.481, Sensitive_Auc : 0.997, Mean auc: 0.852, Run Time : 226.90 sec
INFO:root:2024-04-18 15:04:41, Train, Epoch : 9, Step : 5310, Loss : 0.35472, Acc : 0.831, Sensitive_Loss : 0.11932, Sensitive_Acc : 21.900, Run Time : 238.73 sec
INFO:root:2024-04-18 15:04:59, Train, Epoch : 9, Step : 5320, Loss : 0.27089, Acc : 0.891, Sensitive_Loss : 0.11113, Sensitive_Acc : 24.700, Run Time : 18.26 sec
INFO:root:2024-04-18 15:05:16, Train, Epoch : 9, Step : 5330, Loss : 0.35440, Acc : 0.859, Sensitive_Loss : 0.08293, Sensitive_Acc : 18.100, Run Time : 17.41 sec
INFO:root:2024-04-18 15:05:34, Train, Epoch : 9, Step : 5340, Loss : 0.38437, Acc : 0.834, Sensitive_Loss : 0.06856, Sensitive_Acc : 23.600, Run Time : 18.20 sec
INFO:root:2024-04-18 15:05:52, Train, Epoch : 9, Step : 5350, Loss : 0.28254, Acc : 0.887, Sensitive_Loss : 0.08581, Sensitive_Acc : 18.900, Run Time : 17.77 sec
INFO:root:2024-04-18 15:06:10, Train, Epoch : 9, Step : 5360, Loss : 0.31661, Acc : 0.872, Sensitive_Loss : 0.14624, Sensitive_Acc : 21.700, Run Time : 17.36 sec
INFO:root:2024-04-18 15:06:27, Train, Epoch : 9, Step : 5370, Loss : 0.35336, Acc : 0.859, Sensitive_Loss : 0.12768, Sensitive_Acc : 22.200, Run Time : 17.49 sec
INFO:root:2024-04-18 15:06:45, Train, Epoch : 9, Step : 5380, Loss : 0.29423, Acc : 0.878, Sensitive_Loss : 0.07799, Sensitive_Acc : 23.500, Run Time : 17.42 sec
INFO:root:2024-04-18 15:07:00, Train, Epoch : 9, Step : 5390, Loss : 0.37076, Acc : 0.853, Sensitive_Loss : 0.06559, Sensitive_Acc : 23.700, Run Time : 15.34 sec
INFO:root:2024-04-18 15:07:17, Train, Epoch : 9, Step : 5400, Loss : 0.35227, Acc : 0.863, Sensitive_Loss : 0.08895, Sensitive_Acc : 18.300, Run Time : 17.28 sec
INFO:root:2024-04-18 15:11:17, Dev, Step : 5400, Loss : 0.54897, Acc : 0.783, Auc : 0.852, Sensitive_Loss : 0.17063, Sensitive_Acc : 21.812, Sensitive_Auc : 0.997, Mean auc: 0.852, Run Time : 239.87 sec
INFO:root:2024-04-18 15:11:32, Train, Epoch : 9, Step : 5410, Loss : 0.37933, Acc : 0.834, Sensitive_Loss : 0.08572, Sensitive_Acc : 19.300, Run Time : 254.60 sec
INFO:root:2024-04-18 15:11:54, Train, Epoch : 9, Step : 5420, Loss : 0.38720, Acc : 0.822, Sensitive_Loss : 0.07737, Sensitive_Acc : 21.900, Run Time : 22.32 sec
INFO:root:2024-04-18 15:12:11, Train, Epoch : 9, Step : 5430, Loss : 0.34755, Acc : 0.853, Sensitive_Loss : 0.10726, Sensitive_Acc : 20.700, Run Time : 16.95 sec
INFO:root:2024-04-18 15:12:28, Train, Epoch : 9, Step : 5440, Loss : 0.34911, Acc : 0.844, Sensitive_Loss : 0.10845, Sensitive_Acc : 18.600, Run Time : 16.94 sec
INFO:root:2024-04-18 15:12:45, Train, Epoch : 9, Step : 5450, Loss : 0.30121, Acc : 0.866, Sensitive_Loss : 0.10559, Sensitive_Acc : 22.200, Run Time : 17.47 sec
INFO:root:2024-04-18 15:13:03, Train, Epoch : 9, Step : 5460, Loss : 0.30073, Acc : 0.872, Sensitive_Loss : 0.11177, Sensitive_Acc : 25.000, Run Time : 18.08 sec
INFO:root:2024-04-18 15:13:21, Train, Epoch : 9, Step : 5470, Loss : 0.31423, Acc : 0.881, Sensitive_Loss : 0.09330, Sensitive_Acc : 15.300, Run Time : 17.24 sec
INFO:root:2024-04-18 15:13:39, Train, Epoch : 9, Step : 5480, Loss : 0.37831, Acc : 0.856, Sensitive_Loss : 0.19086, Sensitive_Acc : 21.900, Run Time : 18.03 sec
INFO:root:2024-04-18 15:13:54, Train, Epoch : 9, Step : 5490, Loss : 0.34378, Acc : 0.853, Sensitive_Loss : 0.10632, Sensitive_Acc : 21.500, Run Time : 15.52 sec
INFO:root:2024-04-18 15:14:11, Train, Epoch : 9, Step : 5500, Loss : 0.32005, Acc : 0.866, Sensitive_Loss : 0.07393, Sensitive_Acc : 18.300, Run Time : 16.30 sec
INFO:root:2024-04-18 15:18:01, Dev, Step : 5500, Loss : 0.54704, Acc : 0.781, Auc : 0.852, Sensitive_Loss : 0.18804, Sensitive_Acc : 20.940, Sensitive_Auc : 0.997, Mean auc: 0.852, Run Time : 230.82 sec
INFO:root:2024-04-18 15:18:14, Train, Epoch : 9, Step : 5510, Loss : 0.33648, Acc : 0.853, Sensitive_Loss : 0.12871, Sensitive_Acc : 24.400, Run Time : 243.27 sec
INFO:root:2024-04-18 15:18:31, Train, Epoch : 9, Step : 5520, Loss : 0.31677, Acc : 0.869, Sensitive_Loss : 0.07505, Sensitive_Acc : 18.700, Run Time : 17.00 sec
INFO:root:2024-04-18 15:18:49, Train, Epoch : 9, Step : 5530, Loss : 0.26565, Acc : 0.872, Sensitive_Loss : 0.09890, Sensitive_Acc : 17.000, Run Time : 18.29 sec
INFO:root:2024-04-18 15:19:05, Train, Epoch : 9, Step : 5540, Loss : 0.30902, Acc : 0.878, Sensitive_Loss : 0.11214, Sensitive_Acc : 23.100, Run Time : 15.37 sec
INFO:root:2024-04-18 15:19:22, Train, Epoch : 9, Step : 5550, Loss : 0.32112, Acc : 0.866, Sensitive_Loss : 0.10598, Sensitive_Acc : 22.100, Run Time : 17.14 sec
INFO:root:2024-04-18 15:19:38, Train, Epoch : 9, Step : 5560, Loss : 0.36879, Acc : 0.841, Sensitive_Loss : 0.12825, Sensitive_Acc : 15.000, Run Time : 16.52 sec
INFO:root:2024-04-18 15:19:55, Train, Epoch : 9, Step : 5570, Loss : 0.30963, Acc : 0.847, Sensitive_Loss : 0.12265, Sensitive_Acc : 25.300, Run Time : 17.03 sec
INFO:root:2024-04-18 15:20:13, Train, Epoch : 9, Step : 5580, Loss : 0.40179, Acc : 0.850, Sensitive_Loss : 0.10444, Sensitive_Acc : 24.300, Run Time : 17.66 sec
INFO:root:2024-04-18 15:20:30, Train, Epoch : 9, Step : 5590, Loss : 0.37210, Acc : 0.859, Sensitive_Loss : 0.10288, Sensitive_Acc : 23.400, Run Time : 17.54 sec
INFO:root:2024-04-18 15:20:48, Train, Epoch : 9, Step : 5600, Loss : 0.32222, Acc : 0.850, Sensitive_Loss : 0.10290, Sensitive_Acc : 23.700, Run Time : 17.33 sec
INFO:root:2024-04-18 15:24:37, Dev, Step : 5600, Loss : 0.53962, Acc : 0.777, Auc : 0.850, Sensitive_Loss : 0.16898, Sensitive_Acc : 21.932, Sensitive_Auc : 0.997, Mean auc: 0.850, Run Time : 228.82 sec
INFO:root:2024-04-18 15:24:48, Train, Epoch : 9, Step : 5610, Loss : 0.29976, Acc : 0.875, Sensitive_Loss : 0.14280, Sensitive_Acc : 21.800, Run Time : 240.08 sec
INFO:root:2024-04-18 15:25:06, Train, Epoch : 9, Step : 5620, Loss : 0.29206, Acc : 0.878, Sensitive_Loss : 0.05977, Sensitive_Acc : 22.000, Run Time : 18.03 sec
INFO:root:2024-04-18 15:25:22, Train, Epoch : 9, Step : 5630, Loss : 0.28018, Acc : 0.866, Sensitive_Loss : 0.13207, Sensitive_Acc : 21.900, Run Time : 16.19 sec
INFO:root:2024-04-18 15:25:40, Train, Epoch : 9, Step : 5640, Loss : 0.32215, Acc : 0.859, Sensitive_Loss : 0.10547, Sensitive_Acc : 23.000, Run Time : 18.22 sec
INFO:root:2024-04-18 15:25:56, Train, Epoch : 9, Step : 5650, Loss : 0.30788, Acc : 0.872, Sensitive_Loss : 0.10575, Sensitive_Acc : 23.400, Run Time : 15.98 sec
INFO:root:2024-04-18 15:26:13, Train, Epoch : 9, Step : 5660, Loss : 0.32317, Acc : 0.881, Sensitive_Loss : 0.11137, Sensitive_Acc : 20.200, Run Time : 16.95 sec
INFO:root:2024-04-18 15:26:31, Train, Epoch : 9, Step : 5670, Loss : 0.30639, Acc : 0.872, Sensitive_Loss : 0.06371, Sensitive_Acc : 18.100, Run Time : 17.89 sec
INFO:root:2024-04-18 15:26:47, Train, Epoch : 9, Step : 5680, Loss : 0.28375, Acc : 0.884, Sensitive_Loss : 0.12558, Sensitive_Acc : 16.700, Run Time : 16.18 sec
INFO:root:2024-04-18 15:27:05, Train, Epoch : 9, Step : 5690, Loss : 0.36062, Acc : 0.853, Sensitive_Loss : 0.08126, Sensitive_Acc : 23.800, Run Time : 17.82 sec
INFO:root:2024-04-18 15:27:21, Train, Epoch : 9, Step : 5700, Loss : 0.28945, Acc : 0.878, Sensitive_Loss : 0.08483, Sensitive_Acc : 20.200, Run Time : 16.35 sec
INFO:root:2024-04-18 15:31:11, Dev, Step : 5700, Loss : 0.55964, Acc : 0.777, Auc : 0.852, Sensitive_Loss : 0.16879, Sensitive_Acc : 21.526, Sensitive_Auc : 1.000, Mean auc: 0.852, Run Time : 229.88 sec
INFO:root:2024-04-18 15:35:03
INFO:root:y_pred: [0.06286374 0.00080544 0.06116327 ... 0.4327251  0.05867067 0.00232409]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [5.2630734e-03 1.8074021e-02 1.9096488e-03 7.9600036e-02 9.7896894e-03
 8.7910732e-05 6.2710897e-06 3.9714943e-03 6.0581047e-02 9.9978977e-01
 1.0359021e-01 2.6689018e-03 1.1179191e-04 2.4782310e-04 9.9940515e-01
 8.2493471e-03 5.6561367e-03 9.9971265e-01 9.9988377e-01 4.0489351e-03
 9.8674184e-01 7.8201592e-05 2.7574846e-03 1.8816825e-03 7.1634059e-03
 7.5108013e-03 4.6676047e-05 1.0995872e-03 2.8400085e-04 1.1311856e-02
 1.6828200e-03 9.5921648e-01 4.1552229e-04 6.5900397e-01 2.7150843e-05
 7.0483465e-04 5.5436045e-03 7.6163034e-03 6.9156028e-02 2.9889785e-03
 3.5947874e-02 9.9891758e-01 6.7241816e-04 1.2596948e-03 9.5070982e-01
 2.4955121e-03 3.9666259e-01 7.6429591e-02 5.5556720e-01 9.9017137e-01
 9.9326247e-01 9.9978238e-01 9.9518663e-01 9.8402299e-05 5.1077083e-04
 1.1248127e-01 2.2493971e-04 5.0733908e-04 9.9639386e-01 1.0213447e-03
 7.9270094e-06 4.8115049e-04 2.3680045e-03 2.7282932e-04 9.9582314e-01
 2.5795077e-03 8.6706132e-06 2.2209552e-01 1.8955934e-03 9.9739218e-01
 9.9998093e-01 9.9977177e-01 7.3032134e-05 3.9791074e-01 3.3939735e-04
 3.7810045e-01 1.3195293e-02 7.0553710e-07 4.0319261e-05 3.5726136e-04
 1.5148751e-02 2.7040507e-05 9.9620974e-01 9.9854749e-01 3.3675964e-04
 1.8182686e-03 6.5983772e-02 7.2386791e-04 8.4564294e-04 2.8118349e-04
 3.3107451e-03 1.2748465e-02 3.6469981e-05 1.0178848e-06 2.3098703e-02
 3.8368900e-03 2.8344659e-05 5.8647364e-01 5.2404637e-04 4.9134912e-03
 1.4638467e-02 1.7770572e-03 4.5473936e-01 1.1593161e-04 1.5650200e-02
 5.6162779e-04 5.9065148e-03 3.3546382e-01 5.0946206e-01 3.1147083e-02
 1.9469262e-06 9.9994910e-01 9.9783665e-01 6.5187360e-06 4.9639132e-02
 5.9968233e-02 3.7763808e-02 2.8794489e-04 3.4498259e-01 1.0968072e-02
 4.4861329e-03 4.2541957e-04 1.5561308e-03 1.5796662e-06 2.0658284e-02
 6.3965046e-01 2.9061042e-07 9.9787426e-01 2.7527912e-02 2.6734695e-01
 1.0426929e-03 8.3447829e-02 1.3200794e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 15:35:03, Dev, Step : 5706, Loss : 0.56000, Acc : 0.778, Auc : 0.851, Sensitive_Loss : 0.16687, Sensitive_Acc : 21.647, Sensitive_Auc : 1.000, Mean auc: 0.851, Run Time : 227.15 sec
INFO:root:2024-04-18 15:35:13, Train, Epoch : 10, Step : 5710, Loss : 0.12745, Acc : 0.341, Sensitive_Loss : 0.02460, Sensitive_Acc : 8.500, Run Time : 9.27 sec
INFO:root:2024-04-18 15:35:32, Train, Epoch : 10, Step : 5720, Loss : 0.34571, Acc : 0.866, Sensitive_Loss : 0.07604, Sensitive_Acc : 22.200, Run Time : 18.62 sec
INFO:root:2024-04-18 15:35:49, Train, Epoch : 10, Step : 5730, Loss : 0.40929, Acc : 0.844, Sensitive_Loss : 0.08704, Sensitive_Acc : 20.600, Run Time : 17.33 sec
INFO:root:2024-04-18 15:36:06, Train, Epoch : 10, Step : 5740, Loss : 0.26919, Acc : 0.900, Sensitive_Loss : 0.09810, Sensitive_Acc : 17.300, Run Time : 16.86 sec
INFO:root:2024-04-18 15:36:24, Train, Epoch : 10, Step : 5750, Loss : 0.31432, Acc : 0.878, Sensitive_Loss : 0.10478, Sensitive_Acc : 18.900, Run Time : 18.15 sec
INFO:root:2024-04-18 15:36:41, Train, Epoch : 10, Step : 5760, Loss : 0.26632, Acc : 0.859, Sensitive_Loss : 0.07559, Sensitive_Acc : 21.400, Run Time : 17.43 sec
INFO:root:2024-04-18 15:36:59, Train, Epoch : 10, Step : 5770, Loss : 0.28596, Acc : 0.881, Sensitive_Loss : 0.13231, Sensitive_Acc : 21.800, Run Time : 17.28 sec
INFO:root:2024-04-18 15:37:18, Train, Epoch : 10, Step : 5780, Loss : 0.31005, Acc : 0.859, Sensitive_Loss : 0.10075, Sensitive_Acc : 23.300, Run Time : 18.90 sec
INFO:root:2024-04-18 15:37:34, Train, Epoch : 10, Step : 5790, Loss : 0.31206, Acc : 0.872, Sensitive_Loss : 0.08996, Sensitive_Acc : 19.400, Run Time : 16.00 sec
INFO:root:2024-04-18 15:37:51, Train, Epoch : 10, Step : 5800, Loss : 0.26443, Acc : 0.900, Sensitive_Loss : 0.14775, Sensitive_Acc : 23.000, Run Time : 17.09 sec
INFO:root:2024-04-18 15:41:40, Dev, Step : 5800, Loss : 0.55550, Acc : 0.774, Auc : 0.851, Sensitive_Loss : 0.16318, Sensitive_Acc : 21.872, Sensitive_Auc : 0.997, Mean auc: 0.851, Run Time : 229.11 sec
INFO:root:2024-04-18 15:41:51, Train, Epoch : 10, Step : 5810, Loss : 0.31251, Acc : 0.866, Sensitive_Loss : 0.10013, Sensitive_Acc : 18.800, Run Time : 240.06 sec
INFO:root:2024-04-18 15:42:09, Train, Epoch : 10, Step : 5820, Loss : 0.29532, Acc : 0.884, Sensitive_Loss : 0.10424, Sensitive_Acc : 25.900, Run Time : 18.36 sec
INFO:root:2024-04-18 15:42:26, Train, Epoch : 10, Step : 5830, Loss : 0.31997, Acc : 0.844, Sensitive_Loss : 0.16215, Sensitive_Acc : 21.700, Run Time : 17.19 sec
INFO:root:2024-04-18 15:42:44, Train, Epoch : 10, Step : 5840, Loss : 0.31261, Acc : 0.863, Sensitive_Loss : 0.09707, Sensitive_Acc : 22.200, Run Time : 17.33 sec
INFO:root:2024-04-18 15:43:01, Train, Epoch : 10, Step : 5850, Loss : 0.28393, Acc : 0.897, Sensitive_Loss : 0.10529, Sensitive_Acc : 21.600, Run Time : 17.88 sec
INFO:root:2024-04-18 15:43:17, Train, Epoch : 10, Step : 5860, Loss : 0.28232, Acc : 0.891, Sensitive_Loss : 0.06865, Sensitive_Acc : 22.400, Run Time : 16.03 sec
INFO:root:2024-04-18 15:43:35, Train, Epoch : 10, Step : 5870, Loss : 0.32209, Acc : 0.872, Sensitive_Loss : 0.10534, Sensitive_Acc : 22.100, Run Time : 17.18 sec
INFO:root:2024-04-18 15:43:52, Train, Epoch : 10, Step : 5880, Loss : 0.32667, Acc : 0.847, Sensitive_Loss : 0.08880, Sensitive_Acc : 26.700, Run Time : 17.11 sec
INFO:root:2024-04-18 15:44:10, Train, Epoch : 10, Step : 5890, Loss : 0.25953, Acc : 0.872, Sensitive_Loss : 0.11126, Sensitive_Acc : 21.000, Run Time : 18.73 sec
INFO:root:2024-04-18 15:44:25, Train, Epoch : 10, Step : 5900, Loss : 0.29108, Acc : 0.863, Sensitive_Loss : 0.08859, Sensitive_Acc : 23.600, Run Time : 14.88 sec
INFO:root:2024-04-18 15:48:16, Dev, Step : 5900, Loss : 0.55913, Acc : 0.776, Auc : 0.850, Sensitive_Loss : 0.15654, Sensitive_Acc : 22.278, Sensitive_Auc : 0.999, Mean auc: 0.850, Run Time : 230.41 sec
INFO:root:2024-04-18 15:48:29, Train, Epoch : 10, Step : 5910, Loss : 0.32365, Acc : 0.856, Sensitive_Loss : 0.06235, Sensitive_Acc : 20.800, Run Time : 243.75 sec
INFO:root:2024-04-18 15:48:44, Train, Epoch : 10, Step : 5920, Loss : 0.29451, Acc : 0.850, Sensitive_Loss : 0.07752, Sensitive_Acc : 23.500, Run Time : 15.01 sec
INFO:root:2024-04-18 15:49:01, Train, Epoch : 10, Step : 5930, Loss : 0.27957, Acc : 0.887, Sensitive_Loss : 0.06123, Sensitive_Acc : 24.000, Run Time : 16.86 sec
INFO:root:2024-04-18 15:49:19, Train, Epoch : 10, Step : 5940, Loss : 0.26669, Acc : 0.878, Sensitive_Loss : 0.07104, Sensitive_Acc : 23.000, Run Time : 17.82 sec
INFO:root:2024-04-18 15:49:36, Train, Epoch : 10, Step : 5950, Loss : 0.24549, Acc : 0.875, Sensitive_Loss : 0.09937, Sensitive_Acc : 22.700, Run Time : 17.33 sec
INFO:root:2024-04-18 15:49:53, Train, Epoch : 10, Step : 5960, Loss : 0.30146, Acc : 0.844, Sensitive_Loss : 0.07922, Sensitive_Acc : 19.600, Run Time : 17.35 sec
INFO:root:2024-04-18 15:50:11, Train, Epoch : 10, Step : 5970, Loss : 0.29159, Acc : 0.884, Sensitive_Loss : 0.09876, Sensitive_Acc : 23.100, Run Time : 17.28 sec
INFO:root:2024-04-18 15:50:28, Train, Epoch : 10, Step : 5980, Loss : 0.34193, Acc : 0.866, Sensitive_Loss : 0.14443, Sensitive_Acc : 22.700, Run Time : 17.51 sec
INFO:root:2024-04-18 15:50:46, Train, Epoch : 10, Step : 5990, Loss : 0.32257, Acc : 0.869, Sensitive_Loss : 0.08908, Sensitive_Acc : 21.900, Run Time : 18.11 sec
INFO:root:2024-04-18 15:51:04, Train, Epoch : 10, Step : 6000, Loss : 0.28148, Acc : 0.887, Sensitive_Loss : 0.08051, Sensitive_Acc : 22.600, Run Time : 17.58 sec
INFO:root:2024-04-18 15:54:54, Dev, Step : 6000, Loss : 0.56456, Acc : 0.769, Auc : 0.847, Sensitive_Loss : 0.16027, Sensitive_Acc : 22.128, Sensitive_Auc : 0.999, Mean auc: 0.847, Run Time : 229.86 sec
INFO:root:2024-04-18 15:55:06, Train, Epoch : 10, Step : 6010, Loss : 0.30237, Acc : 0.859, Sensitive_Loss : 0.16658, Sensitive_Acc : 18.300, Run Time : 242.53 sec
INFO:root:2024-04-18 15:55:24, Train, Epoch : 10, Step : 6020, Loss : 0.29830, Acc : 0.900, Sensitive_Loss : 0.08464, Sensitive_Acc : 21.200, Run Time : 17.38 sec
INFO:root:2024-04-18 15:55:40, Train, Epoch : 10, Step : 6030, Loss : 0.34422, Acc : 0.869, Sensitive_Loss : 0.09094, Sensitive_Acc : 21.000, Run Time : 16.14 sec
INFO:root:2024-04-18 15:55:57, Train, Epoch : 10, Step : 6040, Loss : 0.33397, Acc : 0.841, Sensitive_Loss : 0.11054, Sensitive_Acc : 22.700, Run Time : 17.10 sec
INFO:root:2024-04-18 15:56:13, Train, Epoch : 10, Step : 6050, Loss : 0.29464, Acc : 0.859, Sensitive_Loss : 0.13182, Sensitive_Acc : 22.700, Run Time : 16.04 sec
INFO:root:2024-04-18 15:56:31, Train, Epoch : 10, Step : 6060, Loss : 0.26457, Acc : 0.881, Sensitive_Loss : 0.12370, Sensitive_Acc : 22.200, Run Time : 17.96 sec
INFO:root:2024-04-18 15:56:49, Train, Epoch : 10, Step : 6070, Loss : 0.27356, Acc : 0.884, Sensitive_Loss : 0.06808, Sensitive_Acc : 22.900, Run Time : 17.48 sec
INFO:root:2024-04-18 15:57:06, Train, Epoch : 10, Step : 6080, Loss : 0.24308, Acc : 0.912, Sensitive_Loss : 0.07845, Sensitive_Acc : 16.500, Run Time : 17.16 sec
INFO:root:2024-04-18 15:57:23, Train, Epoch : 10, Step : 6090, Loss : 0.27103, Acc : 0.878, Sensitive_Loss : 0.09742, Sensitive_Acc : 20.500, Run Time : 17.34 sec
INFO:root:2024-04-18 15:57:40, Train, Epoch : 10, Step : 6100, Loss : 0.34933, Acc : 0.869, Sensitive_Loss : 0.14860, Sensitive_Acc : 19.200, Run Time : 16.85 sec
INFO:root:2024-04-18 16:01:29, Dev, Step : 6100, Loss : 0.55837, Acc : 0.772, Auc : 0.847, Sensitive_Loss : 0.15607, Sensitive_Acc : 22.278, Sensitive_Auc : 0.999, Mean auc: 0.847, Run Time : 229.00 sec
INFO:root:2024-04-18 16:01:40, Train, Epoch : 10, Step : 6110, Loss : 0.25995, Acc : 0.875, Sensitive_Loss : 0.11064, Sensitive_Acc : 24.800, Run Time : 240.56 sec
INFO:root:2024-04-18 16:01:58, Train, Epoch : 10, Step : 6120, Loss : 0.32901, Acc : 0.853, Sensitive_Loss : 0.08807, Sensitive_Acc : 24.200, Run Time : 17.59 sec
INFO:root:2024-04-18 16:02:14, Train, Epoch : 10, Step : 6130, Loss : 0.32992, Acc : 0.887, Sensitive_Loss : 0.09407, Sensitive_Acc : 16.700, Run Time : 16.33 sec
INFO:root:2024-04-18 16:02:31, Train, Epoch : 10, Step : 6140, Loss : 0.25765, Acc : 0.897, Sensitive_Loss : 0.11141, Sensitive_Acc : 21.400, Run Time : 16.82 sec
INFO:root:2024-04-18 16:02:49, Train, Epoch : 10, Step : 6150, Loss : 0.28905, Acc : 0.878, Sensitive_Loss : 0.08502, Sensitive_Acc : 20.300, Run Time : 17.76 sec
INFO:root:2024-04-18 16:03:05, Train, Epoch : 10, Step : 6160, Loss : 0.30625, Acc : 0.881, Sensitive_Loss : 0.07180, Sensitive_Acc : 22.200, Run Time : 16.36 sec
INFO:root:2024-04-18 16:03:22, Train, Epoch : 10, Step : 6170, Loss : 0.28995, Acc : 0.887, Sensitive_Loss : 0.09046, Sensitive_Acc : 20.200, Run Time : 17.03 sec
INFO:root:2024-04-18 16:03:41, Train, Epoch : 10, Step : 6180, Loss : 0.34132, Acc : 0.819, Sensitive_Loss : 0.06176, Sensitive_Acc : 19.700, Run Time : 18.87 sec
INFO:root:2024-04-18 16:03:58, Train, Epoch : 10, Step : 6190, Loss : 0.31777, Acc : 0.859, Sensitive_Loss : 0.08403, Sensitive_Acc : 23.500, Run Time : 17.22 sec
INFO:root:2024-04-18 16:04:15, Train, Epoch : 10, Step : 6200, Loss : 0.30398, Acc : 0.869, Sensitive_Loss : 0.10121, Sensitive_Acc : 24.600, Run Time : 16.52 sec
INFO:root:2024-04-18 16:08:05, Dev, Step : 6200, Loss : 0.56975, Acc : 0.770, Auc : 0.846, Sensitive_Loss : 0.16408, Sensitive_Acc : 21.707, Sensitive_Auc : 0.998, Mean auc: 0.846, Run Time : 229.54 sec
INFO:root:2024-04-18 16:08:17, Train, Epoch : 10, Step : 6210, Loss : 0.28510, Acc : 0.866, Sensitive_Loss : 0.16672, Sensitive_Acc : 21.700, Run Time : 241.98 sec
INFO:root:2024-04-18 16:08:39, Train, Epoch : 10, Step : 6220, Loss : 0.26508, Acc : 0.875, Sensitive_Loss : 0.08755, Sensitive_Acc : 26.200, Run Time : 21.65 sec
INFO:root:2024-04-18 16:09:02, Train, Epoch : 10, Step : 6230, Loss : 0.31107, Acc : 0.869, Sensitive_Loss : 0.06880, Sensitive_Acc : 19.200, Run Time : 23.08 sec
INFO:root:2024-04-18 16:09:21, Train, Epoch : 10, Step : 6240, Loss : 0.29475, Acc : 0.856, Sensitive_Loss : 0.08918, Sensitive_Acc : 21.900, Run Time : 19.13 sec
INFO:root:2024-04-18 16:09:42, Train, Epoch : 10, Step : 6250, Loss : 0.34481, Acc : 0.847, Sensitive_Loss : 0.07871, Sensitive_Acc : 23.400, Run Time : 20.69 sec
INFO:root:2024-04-18 16:10:03, Train, Epoch : 10, Step : 6260, Loss : 0.29910, Acc : 0.847, Sensitive_Loss : 0.06757, Sensitive_Acc : 24.000, Run Time : 21.34 sec
INFO:root:2024-04-18 16:10:27, Train, Epoch : 10, Step : 6270, Loss : 0.27660, Acc : 0.875, Sensitive_Loss : 0.09410, Sensitive_Acc : 22.600, Run Time : 23.80 sec
INFO:root:2024-04-18 16:10:48, Train, Epoch : 10, Step : 6280, Loss : 0.28814, Acc : 0.872, Sensitive_Loss : 0.07829, Sensitive_Acc : 24.300, Run Time : 21.11 sec
INFO:root:2024-04-18 16:11:05, Train, Epoch : 10, Step : 6290, Loss : 0.27254, Acc : 0.881, Sensitive_Loss : 0.08803, Sensitive_Acc : 20.600, Run Time : 16.79 sec
INFO:root:2024-04-18 16:11:25, Train, Epoch : 10, Step : 6300, Loss : 0.30709, Acc : 0.853, Sensitive_Loss : 0.08925, Sensitive_Acc : 26.900, Run Time : 20.12 sec
INFO:root:2024-04-18 16:15:18, Dev, Step : 6300, Loss : 0.57623, Acc : 0.776, Auc : 0.850, Sensitive_Loss : 0.16252, Sensitive_Acc : 21.586, Sensitive_Auc : 0.997, Mean auc: 0.850, Run Time : 232.99 sec
INFO:root:2024-04-18 16:15:29, Train, Epoch : 10, Step : 6310, Loss : 0.32236, Acc : 0.850, Sensitive_Loss : 0.10157, Sensitive_Acc : 17.600, Run Time : 244.44 sec
INFO:root:2024-04-18 16:15:47, Train, Epoch : 10, Step : 6320, Loss : 0.30817, Acc : 0.859, Sensitive_Loss : 0.07153, Sensitive_Acc : 24.100, Run Time : 18.21 sec
INFO:root:2024-04-18 16:16:04, Train, Epoch : 10, Step : 6330, Loss : 0.30773, Acc : 0.881, Sensitive_Loss : 0.08007, Sensitive_Acc : 19.700, Run Time : 16.89 sec
INFO:root:2024-04-18 16:16:21, Train, Epoch : 10, Step : 6340, Loss : 0.24409, Acc : 0.906, Sensitive_Loss : 0.14958, Sensitive_Acc : 18.100, Run Time : 17.23 sec
INFO:root:2024-04-18 16:20:09
INFO:root:y_pred: [2.1155370e-02 1.9661222e-04 8.4717207e-02 ... 6.2806737e-01 1.8709237e-02
 1.9687854e-03]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [7.34135276e-03 1.12997238e-02 6.36552053e-04 4.63403538e-02
 3.23398435e-03 9.41855833e-05 1.65111142e-05 2.28175661e-03
 1.65865887e-02 9.99795377e-01 2.67711654e-02 2.11956981e-03
 2.13716139e-05 6.50102680e-04 9.99581754e-01 7.94376060e-03
 4.99120075e-03 9.99632835e-01 9.99815166e-01 3.36128892e-03
 9.89702582e-01 4.12937115e-05 2.65430519e-03 4.27241088e-04
 4.34225192e-03 2.81772129e-02 5.53931823e-05 3.75069038e-04
 3.93792317e-04 9.42336768e-03 5.78049920e-04 9.46373165e-01
 2.59395310e-04 3.38763654e-01 2.38054618e-05 9.68289314e-05
 9.72659735e-04 5.63436188e-03 3.43172625e-02 1.57450349e-03
 3.08874492e-02 9.98648465e-01 4.19399294e-04 2.69452954e-04
 9.70218480e-01 2.47272453e-03 2.86382109e-01 1.04741439e-01
 6.71861768e-01 9.86466944e-01 9.96560037e-01 9.99849439e-01
 9.94791031e-01 3.62405699e-05 7.74151413e-04 7.17991889e-02
 8.42074456e-04 4.75016364e-04 9.96425688e-01 4.92580351e-04
 4.14308079e-06 5.94003650e-04 3.23044462e-03 3.36337369e-04
 9.96722281e-01 1.91368733e-03 1.40374732e-05 1.50589824e-01
 4.37516428e-04 9.97595251e-01 9.99986649e-01 9.99702871e-01
 9.36517245e-05 4.77604151e-01 4.68264247e-04 2.22122803e-01
 3.95018235e-03 2.12394852e-07 3.93771916e-05 9.14777105e-04
 1.05005689e-02 3.10609248e-05 9.92113709e-01 9.97517824e-01
 8.21206195e-04 4.66362835e-04 3.77598479e-02 6.95392489e-04
 3.05241498e-04 1.30605302e-04 2.17521656e-03 7.12997420e-03
 5.15534921e-05 2.76978881e-06 9.03537683e-03 5.97305549e-03
 3.33929711e-05 3.47012430e-01 5.25780255e-04 4.54431865e-03
 3.78798833e-03 1.48170977e-03 4.32250977e-01 1.71709835e-04
 1.22256316e-02 9.04196873e-04 1.96546479e-03 1.73640907e-01
 4.53509063e-01 3.70856673e-02 1.99331862e-06 9.99956012e-01
 9.98596370e-01 3.45605463e-06 3.74458954e-02 2.82349102e-02
 1.62684619e-02 6.04948902e-04 3.95869285e-01 5.69693511e-03
 5.14959509e-04 7.19052157e-04 4.25070617e-03 5.39309951e-07
 3.46408645e-03 2.16091633e-01 2.73139705e-07 9.98302937e-01
 1.49021065e-02 2.30790272e-01 5.12906990e-04 6.66048974e-02
 1.19175333e-07]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-18 16:20:09, Dev, Step : 6340, Loss : 0.58185, Acc : 0.776, Auc : 0.848, Sensitive_Loss : 0.15438, Sensitive_Acc : 22.128, Sensitive_Auc : 0.996, Mean auc: 0.848, Run Time : 227.43 sec
