Running on desktop22:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/random_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/random_dataset_val.csv",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.1,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-03-31 20:46:56, Train, Epoch : 1, Step : 10, Loss : 1.23392, Acc : 0.531, Sensitive_Loss : 0.78389, Sensitive_Acc : 15.000, Run Time : 8.94 sec
INFO:root:2024-03-31 20:47:03, Train, Epoch : 1, Step : 20, Loss : 0.98171, Acc : 0.531, Sensitive_Loss : 0.70781, Sensitive_Acc : 17.300, Run Time : 7.09 sec
INFO:root:2024-03-31 20:47:11, Train, Epoch : 1, Step : 30, Loss : 1.13061, Acc : 0.494, Sensitive_Loss : 0.78804, Sensitive_Acc : 16.400, Run Time : 7.34 sec
INFO:root:2024-03-31 20:47:18, Train, Epoch : 1, Step : 40, Loss : 1.52398, Acc : 0.519, Sensitive_Loss : 0.73140, Sensitive_Acc : 15.100, Run Time : 6.91 sec
INFO:root:2024-03-31 20:47:25, Train, Epoch : 1, Step : 50, Loss : 1.33475, Acc : 0.522, Sensitive_Loss : 0.69560, Sensitive_Acc : 16.200, Run Time : 7.52 sec
INFO:root:2024-03-31 20:47:32, Train, Epoch : 1, Step : 60, Loss : 1.01397, Acc : 0.550, Sensitive_Loss : 0.75491, Sensitive_Acc : 17.900, Run Time : 6.89 sec
INFO:root:2024-03-31 20:47:39, Train, Epoch : 1, Step : 70, Loss : 1.41481, Acc : 0.553, Sensitive_Loss : 0.72866, Sensitive_Acc : 17.300, Run Time : 6.89 sec
INFO:root:2024-03-31 20:47:46, Train, Epoch : 1, Step : 80, Loss : 0.76105, Acc : 0.537, Sensitive_Loss : 0.68199, Sensitive_Acc : 15.600, Run Time : 7.14 sec
INFO:root:2024-03-31 20:47:53, Train, Epoch : 1, Step : 90, Loss : 1.36851, Acc : 0.519, Sensitive_Loss : 0.64321, Sensitive_Acc : 16.500, Run Time : 7.20 sec
INFO:root:2024-03-31 20:48:00, Train, Epoch : 1, Step : 100, Loss : 1.23886, Acc : 0.537, Sensitive_Loss : 0.61735, Sensitive_Acc : 16.500, Run Time : 6.84 sec
INFO:root:2024-03-31 20:49:36, Dev, Step : 100, Loss : 1.13994, Acc : 0.771, Auc : 0.631, Sensitive_Loss : 0.68458, Sensitive_Acc : 15.475, Sensitive_Auc : 0.797, Mean auc: 0.631, Run Time : 95.56 sec
INFO:root:2024-03-31 20:49:36, Best, Step : 100, Loss : 1.13994, Acc : 0.771, Auc : 0.631, Sensitive_Loss : 0.68458, Sensitive_Acc : 15.475, Sensitive_Auc : 0.797, Best Auc : 0.631
INFO:root:2024-03-31 20:49:42, Train, Epoch : 1, Step : 110, Loss : 0.83768, Acc : 0.522, Sensitive_Loss : 0.71838, Sensitive_Acc : 16.200, Run Time : 101.67 sec
INFO:root:2024-03-31 20:49:49, Train, Epoch : 1, Step : 120, Loss : 1.18762, Acc : 0.544, Sensitive_Loss : 0.75458, Sensitive_Acc : 16.100, Run Time : 7.57 sec
INFO:root:2024-03-31 20:49:57, Train, Epoch : 1, Step : 130, Loss : 1.17034, Acc : 0.562, Sensitive_Loss : 0.63548, Sensitive_Acc : 16.800, Run Time : 7.22 sec
INFO:root:2024-03-31 20:50:04, Train, Epoch : 1, Step : 140, Loss : 0.96942, Acc : 0.550, Sensitive_Loss : 0.58866, Sensitive_Acc : 17.400, Run Time : 6.92 sec
INFO:root:2024-03-31 20:50:10, Train, Epoch : 1, Step : 150, Loss : 1.26579, Acc : 0.550, Sensitive_Loss : 0.60084, Sensitive_Acc : 14.600, Run Time : 6.57 sec
INFO:root:2024-03-31 20:50:18, Train, Epoch : 1, Step : 160, Loss : 1.12517, Acc : 0.537, Sensitive_Loss : 0.53769, Sensitive_Acc : 15.500, Run Time : 7.49 sec
INFO:root:2024-03-31 20:50:25, Train, Epoch : 1, Step : 170, Loss : 0.87596, Acc : 0.544, Sensitive_Loss : 0.59146, Sensitive_Acc : 17.800, Run Time : 7.11 sec
INFO:root:2024-03-31 20:50:32, Train, Epoch : 1, Step : 180, Loss : 1.26557, Acc : 0.578, Sensitive_Loss : 0.53405, Sensitive_Acc : 15.500, Run Time : 6.95 sec
INFO:root:2024-03-31 20:50:39, Train, Epoch : 1, Step : 190, Loss : 0.97408, Acc : 0.597, Sensitive_Loss : 0.60047, Sensitive_Acc : 14.900, Run Time : 7.00 sec
INFO:root:2024-03-31 20:50:47, Train, Epoch : 1, Step : 200, Loss : 1.19353, Acc : 0.559, Sensitive_Loss : 0.56124, Sensitive_Acc : 16.400, Run Time : 7.81 sec
INFO:root:2024-03-31 20:52:19, Dev, Step : 200, Loss : 1.21289, Acc : 0.325, Auc : 0.672, Sensitive_Loss : 0.62998, Sensitive_Acc : 15.234, Sensitive_Auc : 0.877, Mean auc: 0.672, Run Time : 92.74 sec
INFO:root:2024-03-31 20:52:20, Best, Step : 200, Loss : 1.21289, Acc : 0.325, Auc : 0.672, Sensitive_Loss : 0.62998, Sensitive_Acc : 15.234, Sensitive_Auc : 0.877, Best Auc : 0.672
INFO:root:2024-03-31 20:52:26, Train, Epoch : 1, Step : 210, Loss : 1.36191, Acc : 0.528, Sensitive_Loss : 0.61899, Sensitive_Acc : 19.100, Run Time : 99.05 sec
INFO:root:2024-03-31 20:52:32, Train, Epoch : 1, Step : 220, Loss : 1.07564, Acc : 0.547, Sensitive_Loss : 0.53339, Sensitive_Acc : 17.000, Run Time : 6.79 sec
INFO:root:2024-03-31 20:52:40, Train, Epoch : 1, Step : 230, Loss : 1.35591, Acc : 0.503, Sensitive_Loss : 0.59424, Sensitive_Acc : 15.600, Run Time : 7.39 sec
INFO:root:2024-03-31 20:52:47, Train, Epoch : 1, Step : 240, Loss : 1.10098, Acc : 0.603, Sensitive_Loss : 0.58071, Sensitive_Acc : 15.100, Run Time : 7.09 sec
INFO:root:2024-03-31 20:52:54, Train, Epoch : 1, Step : 250, Loss : 0.93441, Acc : 0.588, Sensitive_Loss : 0.55391, Sensitive_Acc : 16.400, Run Time : 7.36 sec
INFO:root:2024-03-31 20:53:01, Train, Epoch : 1, Step : 260, Loss : 1.27472, Acc : 0.556, Sensitive_Loss : 0.54228, Sensitive_Acc : 18.000, Run Time : 7.09 sec
INFO:root:2024-03-31 20:53:08, Train, Epoch : 1, Step : 270, Loss : 0.96267, Acc : 0.581, Sensitive_Loss : 0.47025, Sensitive_Acc : 17.400, Run Time : 7.11 sec
INFO:root:2024-03-31 20:53:16, Train, Epoch : 1, Step : 280, Loss : 1.19996, Acc : 0.562, Sensitive_Loss : 0.52878, Sensitive_Acc : 15.500, Run Time : 7.38 sec
INFO:root:2024-03-31 20:53:23, Train, Epoch : 1, Step : 290, Loss : 1.22170, Acc : 0.581, Sensitive_Loss : 0.45666, Sensitive_Acc : 13.900, Run Time : 7.19 sec
INFO:root:2024-03-31 20:53:30, Train, Epoch : 1, Step : 300, Loss : 1.09648, Acc : 0.575, Sensitive_Loss : 0.51643, Sensitive_Acc : 16.600, Run Time : 7.29 sec
INFO:root:2024-03-31 20:55:03, Dev, Step : 300, Loss : 1.21931, Acc : 0.499, Auc : 0.686, Sensitive_Loss : 0.50870, Sensitive_Acc : 17.234, Sensitive_Auc : 0.923, Mean auc: 0.686, Run Time : 93.06 sec
INFO:root:2024-03-31 20:55:04, Best, Step : 300, Loss : 1.21931, Acc : 0.499, Auc : 0.686, Sensitive_Loss : 0.50870, Sensitive_Acc : 17.234, Sensitive_Auc : 0.923, Best Auc : 0.686
INFO:root:2024-03-31 20:55:10, Train, Epoch : 1, Step : 310, Loss : 0.84684, Acc : 0.588, Sensitive_Loss : 0.45919, Sensitive_Acc : 17.100, Run Time : 99.41 sec
INFO:root:2024-03-31 20:55:17, Train, Epoch : 1, Step : 320, Loss : 1.02890, Acc : 0.575, Sensitive_Loss : 0.48801, Sensitive_Acc : 17.500, Run Time : 7.42 sec
INFO:root:2024-03-31 20:55:24, Train, Epoch : 1, Step : 330, Loss : 1.18907, Acc : 0.584, Sensitive_Loss : 0.42517, Sensitive_Acc : 15.800, Run Time : 7.03 sec
INFO:root:2024-03-31 20:55:31, Train, Epoch : 1, Step : 340, Loss : 1.18024, Acc : 0.562, Sensitive_Loss : 0.49496, Sensitive_Acc : 15.400, Run Time : 7.20 sec
INFO:root:2024-03-31 20:55:38, Train, Epoch : 1, Step : 350, Loss : 1.15758, Acc : 0.553, Sensitive_Loss : 0.57397, Sensitive_Acc : 16.400, Run Time : 7.03 sec
INFO:root:2024-03-31 20:55:45, Train, Epoch : 1, Step : 360, Loss : 1.25315, Acc : 0.591, Sensitive_Loss : 0.53592, Sensitive_Acc : 13.200, Run Time : 6.97 sec
INFO:root:2024-03-31 20:55:52, Train, Epoch : 1, Step : 370, Loss : 1.20351, Acc : 0.588, Sensitive_Loss : 0.51316, Sensitive_Acc : 15.100, Run Time : 7.13 sec
INFO:root:2024-03-31 20:56:00, Train, Epoch : 1, Step : 380, Loss : 1.13293, Acc : 0.572, Sensitive_Loss : 0.50785, Sensitive_Acc : 16.200, Run Time : 7.10 sec
INFO:root:2024-03-31 20:56:07, Train, Epoch : 1, Step : 390, Loss : 1.16196, Acc : 0.584, Sensitive_Loss : 0.54473, Sensitive_Acc : 15.000, Run Time : 7.27 sec
INFO:root:2024-03-31 20:56:14, Train, Epoch : 1, Step : 400, Loss : 1.09464, Acc : 0.619, Sensitive_Loss : 0.45639, Sensitive_Acc : 16.600, Run Time : 7.06 sec
INFO:root:2024-03-31 20:57:47, Dev, Step : 400, Loss : 1.16523, Acc : 0.436, Auc : 0.670, Sensitive_Loss : 0.52483, Sensitive_Acc : 16.652, Sensitive_Auc : 0.898, Mean auc: 0.670, Run Time : 93.49 sec
INFO:root:2024-03-31 20:57:53, Train, Epoch : 1, Step : 410, Loss : 1.35671, Acc : 0.594, Sensitive_Loss : 0.48802, Sensitive_Acc : 15.700, Run Time : 98.99 sec
INFO:root:2024-03-31 20:58:00, Train, Epoch : 1, Step : 420, Loss : 0.88158, Acc : 0.616, Sensitive_Loss : 0.48119, Sensitive_Acc : 15.600, Run Time : 7.17 sec
INFO:root:2024-03-31 20:58:07, Train, Epoch : 1, Step : 430, Loss : 1.33902, Acc : 0.550, Sensitive_Loss : 0.44222, Sensitive_Acc : 18.100, Run Time : 7.39 sec
INFO:root:2024-03-31 20:58:15, Train, Epoch : 1, Step : 440, Loss : 1.18371, Acc : 0.581, Sensitive_Loss : 0.51135, Sensitive_Acc : 15.800, Run Time : 7.20 sec
INFO:root:2024-03-31 20:58:21, Train, Epoch : 1, Step : 450, Loss : 1.10906, Acc : 0.547, Sensitive_Loss : 0.44050, Sensitive_Acc : 15.400, Run Time : 6.63 sec
INFO:root:2024-03-31 20:58:29, Train, Epoch : 1, Step : 460, Loss : 0.91872, Acc : 0.572, Sensitive_Loss : 0.44731, Sensitive_Acc : 15.400, Run Time : 7.60 sec
INFO:root:2024-03-31 20:58:36, Train, Epoch : 1, Step : 470, Loss : 1.06272, Acc : 0.559, Sensitive_Loss : 0.44143, Sensitive_Acc : 16.300, Run Time : 6.78 sec
INFO:root:2024-03-31 20:58:43, Train, Epoch : 1, Step : 480, Loss : 1.24914, Acc : 0.597, Sensitive_Loss : 0.42531, Sensitive_Acc : 14.200, Run Time : 7.22 sec
INFO:root:2024-03-31 20:58:50, Train, Epoch : 1, Step : 490, Loss : 1.01937, Acc : 0.634, Sensitive_Loss : 0.43272, Sensitive_Acc : 15.100, Run Time : 6.79 sec
INFO:root:2024-03-31 21:00:23
INFO:root:y_pred: [0.32841358 0.04550307 0.3333407  ... 0.13910471 0.34740165 0.50254625]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [9.28067490e-02 4.23071444e-01 8.05329740e-01 1.79701492e-01
 8.12175333e-01 1.45535069e-02 4.94822323e-01 2.69697607e-01
 4.48054150e-02 1.09976940e-01 1.26570657e-01 3.54818180e-02
 8.41993451e-01 5.86060807e-03 8.69482517e-01 5.58292568e-01
 4.76935022e-02 1.89302742e-01 9.41120505e-01 5.37482738e-01
 3.18667665e-03 1.51458215e-02 9.12499726e-01 6.13522410e-01
 8.76423940e-02 8.09683383e-01 2.20898494e-01 8.65675271e-01
 4.49061655e-02 3.10280114e-01 8.81921053e-01 4.68311638e-01
 1.66262873e-02 1.56782717e-02 2.22797722e-01 5.05555421e-03
 8.97944272e-02 3.96381915e-01 8.46295357e-01 4.58732426e-01
 2.11549597e-03 6.37514964e-02 7.51775086e-01 9.22891438e-01
 3.87059361e-01 2.89899372e-02 5.14568575e-03 1.34560958e-01
 6.24306619e-01 8.02778006e-01 9.01359618e-01 8.66259098e-01
 8.76233637e-01 3.57382238e-01 8.97327721e-01 4.26045448e-01
 5.99115670e-01 8.74714181e-02 9.77078617e-01 2.43132278e-01
 4.91951883e-04 1.91146865e-01 3.09148788e-01 5.42004764e-01
 8.33562851e-01 8.90099168e-01 9.50553566e-02 8.44453275e-01
 9.56662059e-01 9.33537126e-01 7.36950696e-01 5.41976765e-02
 9.91218761e-02 1.27330035e-01 6.19399309e-01 4.11515981e-02
 6.28891826e-01 1.20984931e-02 6.64301515e-01 5.48982203e-01
 8.66186678e-01 2.59471666e-02 1.25069432e-02 2.55103588e-01
 7.22268760e-01 9.00762439e-01 7.66423225e-01 1.78424984e-01
 1.39947077e-02 7.64895260e-01 1.25953898e-01 2.74749458e-01
 3.55734490e-02 4.13030237e-02 7.43768215e-01 5.36285043e-01
 1.10168889e-01 1.61174580e-01 1.19494341e-01 9.06036139e-01
 9.09981191e-01 9.94887669e-03 8.72440159e-01 3.46454754e-02
 2.19678313e-01 8.73153806e-01 8.91784072e-01 7.64169395e-02
 2.40722582e-01 6.50956184e-02 1.21507913e-01 7.47977078e-01
 2.13862091e-01 2.22704843e-01 1.41492402e-02 4.77319444e-03
 9.88192976e-01 1.57228529e-01 7.98316300e-03 4.76179179e-03
 6.17118001e-01 9.57368035e-03 9.84137714e-01 6.79205125e-03
 1.98063165e-01 9.63250339e-01 4.68868241e-02 7.08948970e-01
 8.43055010e-01 8.53111148e-01 2.35999841e-03 7.47912824e-01
 9.81820971e-02 9.75351393e-01 8.08866501e-01 4.68169242e-01
 3.01908165e-01 4.79081959e-01 1.01552501e-01 8.44940484e-01
 9.47978973e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-31 21:00:23, Dev, Step : 492, Loss : 1.01915, Acc : 0.680, Auc : 0.716, Sensitive_Loss : 0.42693, Sensitive_Acc : 17.277, Sensitive_Auc : 0.929, Mean auc: 0.716, Run Time : 92.41 sec
INFO:root:2024-03-31 21:00:24, Best, Step : 492, Loss : 1.01915, Acc : 0.680,Auc : 0.716, Best Auc : 0.716, Sensitive_Loss : 0.42693, Sensitive_Acc : 17.277, Sensitive_Auc : 0.929
INFO:root:2024-03-31 21:00:32, Train, Epoch : 2, Step : 500, Loss : 0.69412, Acc : 0.528, Sensitive_Loss : 0.35295, Sensitive_Acc : 11.100, Run Time : 7.06 sec
INFO:root:2024-03-31 21:02:04, Dev, Step : 500, Loss : 1.03407, Acc : 0.640, Auc : 0.723, Sensitive_Loss : 0.45495, Sensitive_Acc : 16.865, Sensitive_Auc : 0.923, Mean auc: 0.723, Run Time : 92.53 sec
INFO:root:2024-03-31 21:02:05, Best, Step : 500, Loss : 1.03407, Acc : 0.640, Auc : 0.723, Sensitive_Loss : 0.45495, Sensitive_Acc : 16.865, Sensitive_Auc : 0.923, Best Auc : 0.723
INFO:root:2024-03-31 21:02:11, Train, Epoch : 2, Step : 510, Loss : 0.84612, Acc : 0.637, Sensitive_Loss : 0.43014, Sensitive_Acc : 16.400, Run Time : 98.73 sec
INFO:root:2024-03-31 21:02:18, Train, Epoch : 2, Step : 520, Loss : 0.96467, Acc : 0.616, Sensitive_Loss : 0.44010, Sensitive_Acc : 14.700, Run Time : 7.50 sec
INFO:root:2024-03-31 21:02:25, Train, Epoch : 2, Step : 530, Loss : 0.99058, Acc : 0.625, Sensitive_Loss : 0.44617, Sensitive_Acc : 16.600, Run Time : 6.89 sec
INFO:root:2024-03-31 21:02:32, Train, Epoch : 2, Step : 540, Loss : 1.18733, Acc : 0.644, Sensitive_Loss : 0.48206, Sensitive_Acc : 16.200, Run Time : 7.38 sec
INFO:root:2024-03-31 21:02:39, Train, Epoch : 2, Step : 550, Loss : 0.92026, Acc : 0.637, Sensitive_Loss : 0.45765, Sensitive_Acc : 13.800, Run Time : 6.82 sec
INFO:root:2024-03-31 21:02:46, Train, Epoch : 2, Step : 560, Loss : 1.08529, Acc : 0.653, Sensitive_Loss : 0.41453, Sensitive_Acc : 18.200, Run Time : 7.01 sec
INFO:root:2024-03-31 21:02:53, Train, Epoch : 2, Step : 570, Loss : 1.08533, Acc : 0.600, Sensitive_Loss : 0.36224, Sensitive_Acc : 16.000, Run Time : 7.25 sec
INFO:root:2024-03-31 21:03:01, Train, Epoch : 2, Step : 580, Loss : 0.85766, Acc : 0.628, Sensitive_Loss : 0.43694, Sensitive_Acc : 16.500, Run Time : 7.21 sec
INFO:root:2024-03-31 21:03:08, Train, Epoch : 2, Step : 590, Loss : 1.24827, Acc : 0.650, Sensitive_Loss : 0.32986, Sensitive_Acc : 16.900, Run Time : 6.93 sec
INFO:root:2024-03-31 21:03:15, Train, Epoch : 2, Step : 600, Loss : 0.92486, Acc : 0.625, Sensitive_Loss : 0.34408, Sensitive_Acc : 18.400, Run Time : 7.21 sec
INFO:root:2024-03-31 21:04:48, Dev, Step : 600, Loss : 1.03120, Acc : 0.811, Auc : 0.735, Sensitive_Loss : 0.37344, Sensitive_Acc : 16.624, Sensitive_Auc : 0.965, Mean auc: 0.735, Run Time : 93.64 sec
INFO:root:2024-03-31 21:04:49, Best, Step : 600, Loss : 1.03120, Acc : 0.811, Auc : 0.735, Sensitive_Loss : 0.37344, Sensitive_Acc : 16.624, Sensitive_Auc : 0.965, Best Auc : 0.735
INFO:root:2024-03-31 21:04:55, Train, Epoch : 2, Step : 610, Loss : 0.87351, Acc : 0.622, Sensitive_Loss : 0.41185, Sensitive_Acc : 16.500, Run Time : 99.81 sec
INFO:root:2024-03-31 21:05:02, Train, Epoch : 2, Step : 620, Loss : 0.86388, Acc : 0.616, Sensitive_Loss : 0.36019, Sensitive_Acc : 16.400, Run Time : 7.58 sec
INFO:root:2024-03-31 21:05:09, Train, Epoch : 2, Step : 630, Loss : 0.96284, Acc : 0.637, Sensitive_Loss : 0.37105, Sensitive_Acc : 15.800, Run Time : 6.96 sec
INFO:root:2024-03-31 21:05:16, Train, Epoch : 2, Step : 640, Loss : 1.04746, Acc : 0.619, Sensitive_Loss : 0.41561, Sensitive_Acc : 15.600, Run Time : 6.79 sec
INFO:root:2024-03-31 21:05:23, Train, Epoch : 2, Step : 650, Loss : 1.10894, Acc : 0.628, Sensitive_Loss : 0.41853, Sensitive_Acc : 17.100, Run Time : 7.39 sec
INFO:root:2024-03-31 21:05:30, Train, Epoch : 2, Step : 660, Loss : 0.97346, Acc : 0.650, Sensitive_Loss : 0.33564, Sensitive_Acc : 16.900, Run Time : 6.95 sec
INFO:root:2024-03-31 21:05:37, Train, Epoch : 2, Step : 670, Loss : 1.06184, Acc : 0.603, Sensitive_Loss : 0.38255, Sensitive_Acc : 17.400, Run Time : 6.97 sec
INFO:root:2024-03-31 21:05:44, Train, Epoch : 2, Step : 680, Loss : 1.10517, Acc : 0.666, Sensitive_Loss : 0.30866, Sensitive_Acc : 16.100, Run Time : 7.03 sec
INFO:root:2024-03-31 21:05:51, Train, Epoch : 2, Step : 690, Loss : 0.93087, Acc : 0.591, Sensitive_Loss : 0.36466, Sensitive_Acc : 13.200, Run Time : 7.10 sec
INFO:root:2024-03-31 21:05:58, Train, Epoch : 2, Step : 700, Loss : 1.04445, Acc : 0.647, Sensitive_Loss : 0.41123, Sensitive_Acc : 17.300, Run Time : 7.04 sec
INFO:root:2024-03-31 21:07:33, Dev, Step : 700, Loss : 1.02581, Acc : 0.705, Auc : 0.716, Sensitive_Loss : 0.38930, Sensitive_Acc : 16.057, Sensitive_Auc : 0.962, Mean auc: 0.716, Run Time : 94.09 sec
INFO:root:2024-03-31 21:07:38, Train, Epoch : 2, Step : 710, Loss : 0.78568, Acc : 0.644, Sensitive_Loss : 0.36706, Sensitive_Acc : 15.000, Run Time : 99.63 sec
INFO:root:2024-03-31 21:07:45, Train, Epoch : 2, Step : 720, Loss : 0.78535, Acc : 0.609, Sensitive_Loss : 0.31449, Sensitive_Acc : 15.000, Run Time : 7.22 sec
INFO:root:2024-03-31 21:07:52, Train, Epoch : 2, Step : 730, Loss : 0.96895, Acc : 0.637, Sensitive_Loss : 0.31510, Sensitive_Acc : 16.100, Run Time : 6.88 sec
INFO:root:2024-03-31 21:07:59, Train, Epoch : 2, Step : 740, Loss : 1.07862, Acc : 0.675, Sensitive_Loss : 0.35503, Sensitive_Acc : 16.700, Run Time : 6.92 sec
INFO:root:2024-03-31 21:08:07, Train, Epoch : 2, Step : 750, Loss : 1.11541, Acc : 0.669, Sensitive_Loss : 0.39542, Sensitive_Acc : 17.500, Run Time : 7.46 sec
INFO:root:2024-03-31 21:08:13, Train, Epoch : 2, Step : 760, Loss : 0.98503, Acc : 0.681, Sensitive_Loss : 0.32709, Sensitive_Acc : 17.500, Run Time : 6.86 sec
INFO:root:2024-03-31 21:08:21, Train, Epoch : 2, Step : 770, Loss : 1.08097, Acc : 0.691, Sensitive_Loss : 0.33013, Sensitive_Acc : 16.000, Run Time : 7.32 sec
INFO:root:2024-03-31 21:08:28, Train, Epoch : 2, Step : 780, Loss : 1.01226, Acc : 0.634, Sensitive_Loss : 0.35444, Sensitive_Acc : 16.200, Run Time : 7.32 sec
INFO:root:2024-03-31 21:08:35, Train, Epoch : 2, Step : 790, Loss : 0.99510, Acc : 0.616, Sensitive_Loss : 0.36004, Sensitive_Acc : 17.100, Run Time : 6.98 sec
INFO:root:2024-03-31 21:08:42, Train, Epoch : 2, Step : 800, Loss : 1.17885, Acc : 0.631, Sensitive_Loss : 0.34914, Sensitive_Acc : 17.900, Run Time : 6.95 sec
INFO:root:2024-03-31 21:10:16, Dev, Step : 800, Loss : 1.00582, Acc : 0.639, Auc : 0.743, Sensitive_Loss : 0.33836, Sensitive_Acc : 16.780, Sensitive_Auc : 0.982, Mean auc: 0.743, Run Time : 93.94 sec
INFO:root:2024-03-31 21:10:17, Best, Step : 800, Loss : 1.00582, Acc : 0.639, Auc : 0.743, Sensitive_Loss : 0.33836, Sensitive_Acc : 16.780, Sensitive_Auc : 0.982, Best Auc : 0.743
INFO:root:2024-03-31 21:10:22, Train, Epoch : 2, Step : 810, Loss : 1.21547, Acc : 0.637, Sensitive_Loss : 0.37125, Sensitive_Acc : 16.200, Run Time : 100.10 sec
INFO:root:2024-03-31 21:10:29, Train, Epoch : 2, Step : 820, Loss : 0.99617, Acc : 0.669, Sensitive_Loss : 0.34191, Sensitive_Acc : 15.400, Run Time : 7.42 sec
INFO:root:2024-03-31 21:10:36, Train, Epoch : 2, Step : 830, Loss : 1.11532, Acc : 0.662, Sensitive_Loss : 0.32779, Sensitive_Acc : 16.100, Run Time : 6.72 sec
INFO:root:2024-03-31 21:10:44, Train, Epoch : 2, Step : 840, Loss : 1.18047, Acc : 0.631, Sensitive_Loss : 0.36280, Sensitive_Acc : 16.900, Run Time : 7.39 sec
INFO:root:2024-03-31 21:10:51, Train, Epoch : 2, Step : 850, Loss : 0.91544, Acc : 0.644, Sensitive_Loss : 0.33923, Sensitive_Acc : 18.500, Run Time : 7.68 sec
INFO:root:2024-03-31 21:10:58, Train, Epoch : 2, Step : 860, Loss : 0.99409, Acc : 0.675, Sensitive_Loss : 0.37871, Sensitive_Acc : 17.300, Run Time : 6.51 sec
INFO:root:2024-03-31 21:11:05, Train, Epoch : 2, Step : 870, Loss : 1.10749, Acc : 0.659, Sensitive_Loss : 0.28905, Sensitive_Acc : 17.900, Run Time : 7.29 sec
INFO:root:2024-03-31 21:11:12, Train, Epoch : 2, Step : 880, Loss : 1.28292, Acc : 0.613, Sensitive_Loss : 0.36063, Sensitive_Acc : 16.000, Run Time : 7.25 sec
INFO:root:2024-03-31 21:11:19, Train, Epoch : 2, Step : 890, Loss : 1.19828, Acc : 0.662, Sensitive_Loss : 0.33028, Sensitive_Acc : 15.500, Run Time : 6.65 sec
INFO:root:2024-03-31 21:11:26, Train, Epoch : 2, Step : 900, Loss : 0.77577, Acc : 0.700, Sensitive_Loss : 0.38418, Sensitive_Acc : 17.000, Run Time : 7.37 sec
INFO:root:2024-03-31 21:13:00, Dev, Step : 900, Loss : 1.01124, Acc : 0.738, Auc : 0.737, Sensitive_Loss : 0.39604, Sensitive_Acc : 16.227, Sensitive_Auc : 0.980, Mean auc: 0.737, Run Time : 93.55 sec
INFO:root:2024-03-31 21:13:05, Train, Epoch : 2, Step : 910, Loss : 1.09238, Acc : 0.662, Sensitive_Loss : 0.32047, Sensitive_Acc : 15.800, Run Time : 99.16 sec
INFO:root:2024-03-31 21:13:12, Train, Epoch : 2, Step : 920, Loss : 0.92807, Acc : 0.666, Sensitive_Loss : 0.37442, Sensitive_Acc : 16.100, Run Time : 6.96 sec
INFO:root:2024-03-31 21:13:19, Train, Epoch : 2, Step : 930, Loss : 1.01599, Acc : 0.666, Sensitive_Loss : 0.28188, Sensitive_Acc : 17.600, Run Time : 6.94 sec
INFO:root:2024-03-31 21:13:27, Train, Epoch : 2, Step : 940, Loss : 0.96303, Acc : 0.659, Sensitive_Loss : 0.30682, Sensitive_Acc : 14.700, Run Time : 7.32 sec
INFO:root:2024-03-31 21:13:34, Train, Epoch : 2, Step : 950, Loss : 0.98659, Acc : 0.659, Sensitive_Loss : 0.31551, Sensitive_Acc : 17.500, Run Time : 7.09 sec
INFO:root:2024-03-31 21:13:41, Train, Epoch : 2, Step : 960, Loss : 0.96004, Acc : 0.703, Sensitive_Loss : 0.27404, Sensitive_Acc : 17.700, Run Time : 6.93 sec
INFO:root:2024-03-31 21:13:48, Train, Epoch : 2, Step : 970, Loss : 0.99490, Acc : 0.700, Sensitive_Loss : 0.30672, Sensitive_Acc : 17.300, Run Time : 7.16 sec
INFO:root:2024-03-31 21:13:55, Train, Epoch : 2, Step : 980, Loss : 0.89087, Acc : 0.625, Sensitive_Loss : 0.30331, Sensitive_Acc : 15.600, Run Time : 7.29 sec
INFO:root:2024-03-31 21:15:30
INFO:root:y_pred: [0.8447062  0.07081232 0.25720668 ... 0.56108284 0.7016798  0.22749747]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [4.33694746e-04 5.15767634e-01 9.67073679e-01 4.70856726e-02
 9.85114694e-01 4.28145548e-04 6.75197095e-02 2.40142718e-02
 1.88942894e-03 3.19313556e-01 4.08593155e-02 3.10618877e-02
 9.31040823e-01 3.97695377e-02 9.37869966e-01 9.61740911e-01
 2.28502532e-03 6.82097912e-01 8.92713964e-01 5.73679507e-01
 4.46098950e-03 2.31658984e-02 9.93845284e-01 8.53559613e-01
 1.46055296e-01 5.39275825e-01 1.30309433e-01 9.77834880e-01
 7.37109501e-03 9.24339056e-01 9.97273862e-01 5.49823105e-01
 3.72870971e-04 7.19658055e-05 3.37616324e-01 6.38286525e-04
 7.57485107e-02 7.32562542e-01 9.57218587e-01 9.05058980e-01
 1.95185598e-02 9.47124383e-04 9.91205275e-01 9.96681035e-01
 1.25572339e-01 2.37611726e-01 1.12048618e-03 3.86312641e-02
 9.37474370e-01 9.31483328e-01 7.47749269e-01 6.13507628e-01
 9.72669482e-01 5.85841417e-01 9.40046668e-01 4.48911399e-01
 8.65491271e-01 4.01316658e-02 9.90255415e-01 2.27083266e-03
 6.26722304e-03 3.34723055e-01 8.51300061e-01 9.76218581e-01
 9.45691526e-01 9.91644204e-01 1.27004579e-01 9.37647939e-01
 6.83912694e-01 9.49305773e-01 8.87676895e-01 8.37361589e-02
 8.88987537e-03 1.48360848e-01 5.50512493e-01 7.40268806e-05
 8.91669929e-01 7.12686591e-03 8.87975216e-01 2.63694644e-01
 3.02549005e-01 1.44477154e-03 6.63585216e-02 4.19510961e-01
 9.59736764e-01 6.96539044e-01 7.51381516e-01 3.90740126e-01
 1.70763917e-02 1.44481165e-02 3.82564985e-03 1.27078276e-02
 1.82563718e-03 1.87981725e-02 3.41782749e-01 3.63846272e-01
 1.41567394e-01 2.80286698e-03 5.49740136e-01 4.96576309e-01
 9.34145331e-01 9.49604996e-03 9.87407446e-01 1.48697179e-02
 1.87012881e-01 9.17133391e-01 2.66702592e-01 9.92949978e-02
 2.15928722e-02 1.25012025e-02 2.79058628e-02 8.75743091e-01
 1.51632642e-02 3.35574560e-02 6.98863119e-02 9.28229245e-04
 9.96086121e-01 2.94767469e-01 1.13605030e-01 4.92452178e-04
 5.06395578e-01 2.87080882e-04 9.93970752e-01 3.31071834e-03
 2.09187880e-01 9.98246789e-01 9.76490323e-03 8.54806364e-01
 3.98133785e-01 9.71343815e-01 3.21135449e-04 9.42970276e-01
 4.08320921e-03 9.86677170e-01 9.85646069e-01 1.93324909e-01
 4.90424931e-02 4.06078786e-01 4.80246916e-02 9.57665980e-01
 9.87377584e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-31 21:15:30, Dev, Step : 984, Loss : 1.02346, Acc : 0.614, Auc : 0.745, Sensitive_Loss : 0.31575, Sensitive_Acc : 17.248, Sensitive_Auc : 0.985, Mean auc: 0.745, Run Time : 92.53 sec
INFO:root:2024-03-31 21:15:31, Best, Step : 984, Loss : 1.02346, Acc : 0.614,Auc : 0.745, Best Auc : 0.745, Sensitive_Loss : 0.31575, Sensitive_Acc : 17.248, Sensitive_Auc : 0.985
INFO:root:2024-03-31 21:15:37, Train, Epoch : 3, Step : 990, Loss : 0.58743, Acc : 0.431, Sensitive_Loss : 0.18267, Sensitive_Acc : 9.500, Run Time : 5.42 sec
INFO:root:2024-03-31 21:15:44, Train, Epoch : 3, Step : 1000, Loss : 0.79300, Acc : 0.681, Sensitive_Loss : 0.34672, Sensitive_Acc : 18.200, Run Time : 6.74 sec
INFO:root:2024-03-31 21:17:17, Dev, Step : 1000, Loss : 0.96800, Acc : 0.703, Auc : 0.759, Sensitive_Loss : 0.35849, Sensitive_Acc : 16.397, Sensitive_Auc : 0.989, Mean auc: 0.759, Run Time : 93.32 sec
INFO:root:2024-03-31 21:17:18, Best, Step : 1000, Loss : 0.96800, Acc : 0.703, Auc : 0.759, Sensitive_Loss : 0.35849, Sensitive_Acc : 16.397, Sensitive_Auc : 0.989, Best Auc : 0.759
INFO:root:2024-03-31 21:17:23, Train, Epoch : 3, Step : 1010, Loss : 0.79537, Acc : 0.691, Sensitive_Loss : 0.34414, Sensitive_Acc : 15.100, Run Time : 99.48 sec
INFO:root:2024-03-31 21:17:30, Train, Epoch : 3, Step : 1020, Loss : 0.99042, Acc : 0.678, Sensitive_Loss : 0.34129, Sensitive_Acc : 16.500, Run Time : 7.06 sec
INFO:root:2024-03-31 21:17:37, Train, Epoch : 3, Step : 1030, Loss : 1.12617, Acc : 0.631, Sensitive_Loss : 0.31119, Sensitive_Acc : 17.700, Run Time : 6.90 sec
INFO:root:2024-03-31 21:17:44, Train, Epoch : 3, Step : 1040, Loss : 0.98818, Acc : 0.669, Sensitive_Loss : 0.31135, Sensitive_Acc : 16.200, Run Time : 7.43 sec
INFO:root:2024-03-31 21:17:51, Train, Epoch : 3, Step : 1050, Loss : 1.03877, Acc : 0.659, Sensitive_Loss : 0.27576, Sensitive_Acc : 16.400, Run Time : 7.00 sec
INFO:root:2024-03-31 21:17:59, Train, Epoch : 3, Step : 1060, Loss : 0.91523, Acc : 0.703, Sensitive_Loss : 0.27566, Sensitive_Acc : 15.400, Run Time : 7.19 sec
INFO:root:2024-03-31 21:18:06, Train, Epoch : 3, Step : 1070, Loss : 0.84369, Acc : 0.631, Sensitive_Loss : 0.30604, Sensitive_Acc : 17.200, Run Time : 6.94 sec
INFO:root:2024-03-31 21:18:13, Train, Epoch : 3, Step : 1080, Loss : 1.09076, Acc : 0.681, Sensitive_Loss : 0.31232, Sensitive_Acc : 17.000, Run Time : 7.00 sec
INFO:root:2024-03-31 21:18:20, Train, Epoch : 3, Step : 1090, Loss : 1.11022, Acc : 0.669, Sensitive_Loss : 0.30158, Sensitive_Acc : 16.800, Run Time : 7.76 sec
INFO:root:2024-03-31 21:18:28, Train, Epoch : 3, Step : 1100, Loss : 1.01631, Acc : 0.697, Sensitive_Loss : 0.25126, Sensitive_Acc : 18.100, Run Time : 7.36 sec
INFO:root:2024-03-31 21:20:01, Dev, Step : 1100, Loss : 0.93926, Acc : 0.761, Auc : 0.779, Sensitive_Loss : 0.36086, Sensitive_Acc : 16.440, Sensitive_Auc : 0.983, Mean auc: 0.779, Run Time : 93.26 sec
INFO:root:2024-03-31 21:20:02, Best, Step : 1100, Loss : 0.93926, Acc : 0.761, Auc : 0.779, Sensitive_Loss : 0.36086, Sensitive_Acc : 16.440, Sensitive_Auc : 0.983, Best Auc : 0.779
INFO:root:2024-03-31 21:20:07, Train, Epoch : 3, Step : 1110, Loss : 1.05074, Acc : 0.669, Sensitive_Loss : 0.27243, Sensitive_Acc : 16.300, Run Time : 99.75 sec
INFO:root:2024-03-31 21:20:14, Train, Epoch : 3, Step : 1120, Loss : 0.82870, Acc : 0.731, Sensitive_Loss : 0.34837, Sensitive_Acc : 14.600, Run Time : 6.61 sec
INFO:root:2024-03-31 21:20:21, Train, Epoch : 3, Step : 1130, Loss : 0.83271, Acc : 0.691, Sensitive_Loss : 0.31086, Sensitive_Acc : 17.800, Run Time : 7.11 sec
INFO:root:2024-03-31 21:20:29, Train, Epoch : 3, Step : 1140, Loss : 0.71303, Acc : 0.675, Sensitive_Loss : 0.26931, Sensitive_Acc : 14.600, Run Time : 7.46 sec
INFO:root:2024-03-31 21:20:36, Train, Epoch : 3, Step : 1150, Loss : 0.93894, Acc : 0.728, Sensitive_Loss : 0.30292, Sensitive_Acc : 15.700, Run Time : 7.17 sec
INFO:root:2024-03-31 21:20:43, Train, Epoch : 3, Step : 1160, Loss : 0.89583, Acc : 0.734, Sensitive_Loss : 0.27629, Sensitive_Acc : 16.600, Run Time : 7.21 sec
INFO:root:2024-03-31 21:20:50, Train, Epoch : 3, Step : 1170, Loss : 0.87406, Acc : 0.697, Sensitive_Loss : 0.26957, Sensitive_Acc : 17.300, Run Time : 7.05 sec
INFO:root:2024-03-31 21:20:57, Train, Epoch : 3, Step : 1180, Loss : 0.92602, Acc : 0.688, Sensitive_Loss : 0.32915, Sensitive_Acc : 13.000, Run Time : 7.05 sec
INFO:root:2024-03-31 21:21:04, Train, Epoch : 3, Step : 1190, Loss : 0.98207, Acc : 0.703, Sensitive_Loss : 0.27246, Sensitive_Acc : 17.600, Run Time : 6.86 sec
INFO:root:2024-03-31 21:21:11, Train, Epoch : 3, Step : 1200, Loss : 0.89468, Acc : 0.716, Sensitive_Loss : 0.21313, Sensitive_Acc : 16.700, Run Time : 7.22 sec
INFO:root:2024-03-31 21:22:45, Dev, Step : 1200, Loss : 0.94144, Acc : 0.774, Auc : 0.780, Sensitive_Loss : 0.36137, Sensitive_Acc : 16.454, Sensitive_Auc : 0.984, Mean auc: 0.780, Run Time : 93.77 sec
INFO:root:2024-03-31 21:22:46, Best, Step : 1200, Loss : 0.94144, Acc : 0.774, Auc : 0.780, Sensitive_Loss : 0.36137, Sensitive_Acc : 16.454, Sensitive_Auc : 0.984, Best Auc : 0.780
INFO:root:2024-03-31 21:22:51, Train, Epoch : 3, Step : 1210, Loss : 0.99995, Acc : 0.666, Sensitive_Loss : 0.30333, Sensitive_Acc : 15.800, Run Time : 100.14 sec
INFO:root:2024-03-31 21:22:59, Train, Epoch : 3, Step : 1220, Loss : 0.79389, Acc : 0.697, Sensitive_Loss : 0.29486, Sensitive_Acc : 16.300, Run Time : 7.27 sec
INFO:root:2024-03-31 21:23:06, Train, Epoch : 3, Step : 1230, Loss : 0.95981, Acc : 0.666, Sensitive_Loss : 0.24905, Sensitive_Acc : 17.300, Run Time : 7.06 sec
INFO:root:2024-03-31 21:23:13, Train, Epoch : 3, Step : 1240, Loss : 0.94682, Acc : 0.675, Sensitive_Loss : 0.24286, Sensitive_Acc : 16.500, Run Time : 7.34 sec
INFO:root:2024-03-31 21:23:20, Train, Epoch : 3, Step : 1250, Loss : 0.87366, Acc : 0.691, Sensitive_Loss : 0.28268, Sensitive_Acc : 15.600, Run Time : 7.14 sec
INFO:root:2024-03-31 21:23:27, Train, Epoch : 3, Step : 1260, Loss : 0.78609, Acc : 0.700, Sensitive_Loss : 0.23137, Sensitive_Acc : 17.600, Run Time : 7.00 sec
INFO:root:2024-03-31 21:23:34, Train, Epoch : 3, Step : 1270, Loss : 0.97279, Acc : 0.694, Sensitive_Loss : 0.32828, Sensitive_Acc : 15.400, Run Time : 7.01 sec
INFO:root:2024-03-31 21:23:42, Train, Epoch : 3, Step : 1280, Loss : 0.97558, Acc : 0.688, Sensitive_Loss : 0.26928, Sensitive_Acc : 19.900, Run Time : 7.37 sec
INFO:root:2024-03-31 21:23:49, Train, Epoch : 3, Step : 1290, Loss : 0.90711, Acc : 0.700, Sensitive_Loss : 0.26961, Sensitive_Acc : 17.300, Run Time : 7.33 sec
INFO:root:2024-03-31 21:23:55, Train, Epoch : 3, Step : 1300, Loss : 0.86943, Acc : 0.709, Sensitive_Loss : 0.27795, Sensitive_Acc : 16.200, Run Time : 6.65 sec
INFO:root:2024-03-31 21:25:29, Dev, Step : 1300, Loss : 0.93008, Acc : 0.723, Auc : 0.784, Sensitive_Loss : 0.29874, Sensitive_Acc : 16.709, Sensitive_Auc : 0.986, Mean auc: 0.784, Run Time : 93.51 sec
INFO:root:2024-03-31 21:25:30, Best, Step : 1300, Loss : 0.93008, Acc : 0.723, Auc : 0.784, Sensitive_Loss : 0.29874, Sensitive_Acc : 16.709, Sensitive_Auc : 0.986, Best Auc : 0.784
INFO:root:2024-03-31 21:25:35, Train, Epoch : 3, Step : 1310, Loss : 0.84491, Acc : 0.697, Sensitive_Loss : 0.26796, Sensitive_Acc : 15.800, Run Time : 99.61 sec
INFO:root:2024-03-31 21:25:42, Train, Epoch : 3, Step : 1320, Loss : 0.78745, Acc : 0.678, Sensitive_Loss : 0.26136, Sensitive_Acc : 15.300, Run Time : 7.10 sec
INFO:root:2024-03-31 21:25:49, Train, Epoch : 3, Step : 1330, Loss : 0.92587, Acc : 0.703, Sensitive_Loss : 0.33002, Sensitive_Acc : 17.900, Run Time : 7.25 sec
INFO:root:2024-03-31 21:25:57, Train, Epoch : 3, Step : 1340, Loss : 0.79054, Acc : 0.731, Sensitive_Loss : 0.30542, Sensitive_Acc : 18.600, Run Time : 7.22 sec
INFO:root:2024-03-31 21:26:04, Train, Epoch : 3, Step : 1350, Loss : 0.71943, Acc : 0.709, Sensitive_Loss : 0.27043, Sensitive_Acc : 17.300, Run Time : 6.87 sec
INFO:root:2024-03-31 21:26:11, Train, Epoch : 3, Step : 1360, Loss : 0.86143, Acc : 0.694, Sensitive_Loss : 0.24909, Sensitive_Acc : 17.300, Run Time : 7.23 sec
INFO:root:2024-03-31 21:26:18, Train, Epoch : 3, Step : 1370, Loss : 0.75847, Acc : 0.684, Sensitive_Loss : 0.21747, Sensitive_Acc : 15.100, Run Time : 6.86 sec
INFO:root:2024-03-31 21:26:25, Train, Epoch : 3, Step : 1380, Loss : 1.04583, Acc : 0.656, Sensitive_Loss : 0.27615, Sensitive_Acc : 17.300, Run Time : 7.28 sec
INFO:root:2024-03-31 21:26:32, Train, Epoch : 3, Step : 1390, Loss : 0.91285, Acc : 0.719, Sensitive_Loss : 0.21261, Sensitive_Acc : 18.200, Run Time : 6.82 sec
INFO:root:2024-03-31 21:26:39, Train, Epoch : 3, Step : 1400, Loss : 0.90169, Acc : 0.709, Sensitive_Loss : 0.28989, Sensitive_Acc : 15.500, Run Time : 7.40 sec
INFO:root:2024-03-31 21:28:13, Dev, Step : 1400, Loss : 0.93530, Acc : 0.759, Auc : 0.785, Sensitive_Loss : 0.28154, Sensitive_Acc : 16.809, Sensitive_Auc : 0.990, Mean auc: 0.785, Run Time : 93.92 sec
INFO:root:2024-03-31 21:28:14, Best, Step : 1400, Loss : 0.93530, Acc : 0.759, Auc : 0.785, Sensitive_Loss : 0.28154, Sensitive_Acc : 16.809, Sensitive_Auc : 0.990, Best Auc : 0.785
INFO:root:2024-03-31 21:28:19, Train, Epoch : 3, Step : 1410, Loss : 0.95663, Acc : 0.713, Sensitive_Loss : 0.28364, Sensitive_Acc : 16.700, Run Time : 100.29 sec
INFO:root:2024-03-31 21:28:26, Train, Epoch : 3, Step : 1420, Loss : 0.97553, Acc : 0.744, Sensitive_Loss : 0.23868, Sensitive_Acc : 17.400, Run Time : 6.84 sec
INFO:root:2024-03-31 21:28:34, Train, Epoch : 3, Step : 1430, Loss : 0.80923, Acc : 0.684, Sensitive_Loss : 0.35238, Sensitive_Acc : 16.800, Run Time : 7.68 sec
INFO:root:2024-03-31 21:28:41, Train, Epoch : 3, Step : 1440, Loss : 0.77411, Acc : 0.678, Sensitive_Loss : 0.22708, Sensitive_Acc : 17.600, Run Time : 7.10 sec
INFO:root:2024-03-31 21:28:48, Train, Epoch : 3, Step : 1450, Loss : 0.91655, Acc : 0.675, Sensitive_Loss : 0.28886, Sensitive_Acc : 17.300, Run Time : 7.07 sec
INFO:root:2024-03-31 21:28:56, Train, Epoch : 3, Step : 1460, Loss : 0.91220, Acc : 0.678, Sensitive_Loss : 0.24633, Sensitive_Acc : 15.500, Run Time : 7.46 sec
INFO:root:2024-03-31 21:29:03, Train, Epoch : 3, Step : 1470, Loss : 0.80364, Acc : 0.706, Sensitive_Loss : 0.28614, Sensitive_Acc : 16.900, Run Time : 6.97 sec
INFO:root:2024-03-31 21:30:39
INFO:root:y_pred: [0.39832827 0.12091292 0.12610796 ... 0.5270144  0.33498847 0.09752655]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [1.8832909e-03 4.7264919e-01 9.6338928e-01 1.2127726e-01 9.9680603e-01
 1.1462163e-03 1.0347486e-01 1.4787696e-01 6.7404318e-03 1.5315378e-01
 2.3066247e-02 6.9486469e-02 9.7724324e-01 1.2164393e-02 9.6733642e-01
 8.9985484e-01 1.9771548e-03 5.1757473e-01 9.3432790e-01 8.1542027e-01
 1.9456048e-02 8.9726606e-03 9.9799973e-01 9.5537466e-01 5.5640958e-02
 6.5732366e-01 6.2900811e-02 9.8863447e-01 1.9405548e-02 9.2232674e-01
 9.9888641e-01 8.7172204e-01 5.8503598e-03 1.4587077e-04 3.1708539e-01
 8.5952766e-03 1.4777139e-01 5.6124622e-01 9.4009125e-01 9.5332533e-01
 2.7350858e-02 8.4953979e-03 9.8808473e-01 9.9867576e-01 2.0817412e-01
 1.5793511e-01 5.6137480e-03 6.4140797e-02 9.9233973e-01 9.6823311e-01
 9.6016550e-01 8.7009096e-01 9.9464118e-01 7.1770221e-01 9.8440993e-01
 5.7095653e-01 8.9530867e-01 9.2756962e-03 9.9732554e-01 6.3025199e-02
 4.6804850e-03 3.8383636e-01 8.6555529e-01 9.9158889e-01 9.8126054e-01
 9.9493295e-01 4.2257810e-01 9.8018140e-01 9.6689522e-01 9.8703671e-01
 9.1002554e-01 7.7151962e-02 3.0898184e-03 7.6133057e-02 9.0154320e-01
 4.2035375e-04 9.5087957e-01 2.6197510e-03 9.0782517e-01 7.2433454e-01
 1.8639459e-01 6.6954769e-02 6.2473122e-02 7.0253634e-01 9.5001978e-01
 6.0487235e-01 8.1832540e-01 4.7078675e-01 3.9520338e-02 4.1976769e-02
 8.1797794e-02 4.2543504e-02 3.9405376e-03 4.6400014e-02 8.8029009e-01
 7.5720912e-01 6.1432134e-02 7.8307567e-03 7.6412594e-01 9.1101253e-01
 9.7210974e-01 3.2428384e-02 9.9461383e-01 2.2486173e-02 9.5973365e-02
 9.1919643e-01 4.1965130e-01 1.3308877e-01 1.9557321e-02 3.0689834e-02
 6.1552320e-02 9.5818412e-01 5.3144421e-02 7.7477023e-02 3.9866116e-02
 1.3273825e-03 9.9685848e-01 5.4809546e-01 1.5322337e-01 4.4109166e-04
 4.5502549e-01 2.4216659e-03 9.9638569e-01 6.6977949e-03 1.7289802e-01
 9.9871171e-01 4.3596965e-03 9.4728935e-01 8.9811742e-01 9.9484009e-01
 1.4326602e-03 9.8746371e-01 9.3865274e-03 9.9485815e-01 9.9139333e-01
 5.8692569e-01 1.0301519e-01 7.6711386e-01 1.5476022e-02 9.8863888e-01
 9.9272472e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-31 21:30:39, Dev, Step : 1476, Loss : 0.93674, Acc : 0.794, Auc : 0.789, Sensitive_Loss : 0.27580, Sensitive_Acc : 16.794, Sensitive_Auc : 0.993, Mean auc: 0.789, Run Time : 92.67 sec
INFO:root:2024-03-31 21:30:39, Best, Step : 1476, Loss : 0.93674, Acc : 0.794,Auc : 0.789, Best Auc : 0.789, Sensitive_Loss : 0.27580, Sensitive_Acc : 16.794, Sensitive_Auc : 0.993
INFO:root:2024-03-31 21:30:44, Train, Epoch : 4, Step : 1480, Loss : 0.24589, Acc : 0.281, Sensitive_Loss : 0.08667, Sensitive_Acc : 7.100, Run Time : 4.05 sec
INFO:root:2024-03-31 21:30:52, Train, Epoch : 4, Step : 1490, Loss : 0.88953, Acc : 0.697, Sensitive_Loss : 0.25114, Sensitive_Acc : 17.400, Run Time : 7.19 sec
INFO:root:2024-03-31 21:30:59, Train, Epoch : 4, Step : 1500, Loss : 0.82813, Acc : 0.706, Sensitive_Loss : 0.32632, Sensitive_Acc : 17.400, Run Time : 7.07 sec
INFO:root:2024-03-31 21:32:32, Dev, Step : 1500, Loss : 0.93162, Acc : 0.787, Auc : 0.789, Sensitive_Loss : 0.29756, Sensitive_Acc : 16.582, Sensitive_Auc : 0.994, Mean auc: 0.789, Run Time : 93.32 sec
INFO:root:2024-03-31 21:32:33, Best, Step : 1500, Loss : 0.93162, Acc : 0.787, Auc : 0.789, Sensitive_Loss : 0.29756, Sensitive_Acc : 16.582, Sensitive_Auc : 0.994, Best Auc : 0.789
INFO:root:2024-03-31 21:32:38, Train, Epoch : 4, Step : 1510, Loss : 1.00852, Acc : 0.691, Sensitive_Loss : 0.25895, Sensitive_Acc : 18.600, Run Time : 99.79 sec
INFO:root:2024-03-31 21:32:45, Train, Epoch : 4, Step : 1520, Loss : 0.80251, Acc : 0.722, Sensitive_Loss : 0.29746, Sensitive_Acc : 17.600, Run Time : 6.79 sec
INFO:root:2024-03-31 21:32:53, Train, Epoch : 4, Step : 1530, Loss : 0.85610, Acc : 0.719, Sensitive_Loss : 0.28136, Sensitive_Acc : 17.700, Run Time : 7.29 sec
INFO:root:2024-03-31 21:32:59, Train, Epoch : 4, Step : 1540, Loss : 0.78219, Acc : 0.700, Sensitive_Loss : 0.24789, Sensitive_Acc : 16.700, Run Time : 6.82 sec
INFO:root:2024-03-31 21:33:07, Train, Epoch : 4, Step : 1550, Loss : 0.80756, Acc : 0.697, Sensitive_Loss : 0.26526, Sensitive_Acc : 15.700, Run Time : 7.22 sec
INFO:root:2024-03-31 21:33:14, Train, Epoch : 4, Step : 1560, Loss : 0.83547, Acc : 0.694, Sensitive_Loss : 0.24004, Sensitive_Acc : 18.100, Run Time : 7.24 sec
INFO:root:2024-03-31 21:33:21, Train, Epoch : 4, Step : 1570, Loss : 0.89826, Acc : 0.700, Sensitive_Loss : 0.28293, Sensitive_Acc : 17.700, Run Time : 6.97 sec
INFO:root:2024-03-31 21:33:28, Train, Epoch : 4, Step : 1580, Loss : 0.96549, Acc : 0.719, Sensitive_Loss : 0.23514, Sensitive_Acc : 17.000, Run Time : 7.02 sec
INFO:root:2024-03-31 21:33:35, Train, Epoch : 4, Step : 1590, Loss : 1.00806, Acc : 0.713, Sensitive_Loss : 0.17644, Sensitive_Acc : 15.500, Run Time : 7.03 sec
INFO:root:2024-03-31 21:33:42, Train, Epoch : 4, Step : 1600, Loss : 0.83833, Acc : 0.738, Sensitive_Loss : 0.24953, Sensitive_Acc : 15.100, Run Time : 7.36 sec
INFO:root:2024-03-31 21:35:16, Dev, Step : 1600, Loss : 0.94695, Acc : 0.787, Auc : 0.781, Sensitive_Loss : 0.30148, Sensitive_Acc : 16.496, Sensitive_Auc : 0.995, Mean auc: 0.781, Run Time : 93.67 sec
INFO:root:2024-03-31 21:35:21, Train, Epoch : 4, Step : 1610, Loss : 0.88276, Acc : 0.706, Sensitive_Loss : 0.31127, Sensitive_Acc : 16.800, Run Time : 99.26 sec
INFO:root:2024-03-31 21:35:28, Train, Epoch : 4, Step : 1620, Loss : 0.88860, Acc : 0.750, Sensitive_Loss : 0.23434, Sensitive_Acc : 17.300, Run Time : 6.89 sec
INFO:root:2024-03-31 21:35:36, Train, Epoch : 4, Step : 1630, Loss : 1.01674, Acc : 0.706, Sensitive_Loss : 0.26219, Sensitive_Acc : 16.200, Run Time : 7.49 sec
INFO:root:2024-03-31 21:35:43, Train, Epoch : 4, Step : 1640, Loss : 0.81021, Acc : 0.719, Sensitive_Loss : 0.27195, Sensitive_Acc : 13.600, Run Time : 7.33 sec
INFO:root:2024-03-31 21:35:50, Train, Epoch : 4, Step : 1650, Loss : 1.02306, Acc : 0.691, Sensitive_Loss : 0.25466, Sensitive_Acc : 17.200, Run Time : 7.17 sec
INFO:root:2024-03-31 21:35:57, Train, Epoch : 4, Step : 1660, Loss : 0.81568, Acc : 0.697, Sensitive_Loss : 0.26965, Sensitive_Acc : 15.100, Run Time : 6.89 sec
INFO:root:2024-03-31 21:36:04, Train, Epoch : 4, Step : 1670, Loss : 0.76856, Acc : 0.734, Sensitive_Loss : 0.30922, Sensitive_Acc : 17.000, Run Time : 6.95 sec
INFO:root:2024-03-31 21:36:11, Train, Epoch : 4, Step : 1680, Loss : 0.93832, Acc : 0.691, Sensitive_Loss : 0.23510, Sensitive_Acc : 16.900, Run Time : 7.26 sec
INFO:root:2024-03-31 21:36:18, Train, Epoch : 4, Step : 1690, Loss : 0.75770, Acc : 0.722, Sensitive_Loss : 0.26684, Sensitive_Acc : 15.700, Run Time : 7.02 sec
INFO:root:2024-03-31 21:36:26, Train, Epoch : 4, Step : 1700, Loss : 0.90670, Acc : 0.725, Sensitive_Loss : 0.18050, Sensitive_Acc : 17.200, Run Time : 7.65 sec
INFO:root:2024-03-31 21:38:00, Dev, Step : 1700, Loss : 0.92740, Acc : 0.767, Auc : 0.792, Sensitive_Loss : 0.25246, Sensitive_Acc : 17.135, Sensitive_Auc : 0.995, Mean auc: 0.792, Run Time : 93.71 sec
INFO:root:2024-03-31 21:38:00, Best, Step : 1700, Loss : 0.92740, Acc : 0.767, Auc : 0.792, Sensitive_Loss : 0.25246, Sensitive_Acc : 17.135, Sensitive_Auc : 0.995, Best Auc : 0.792
INFO:root:2024-03-31 21:38:06, Train, Epoch : 4, Step : 1710, Loss : 0.76302, Acc : 0.675, Sensitive_Loss : 0.18822, Sensitive_Acc : 18.500, Run Time : 99.79 sec
INFO:root:2024-03-31 21:38:14, Train, Epoch : 4, Step : 1720, Loss : 0.88715, Acc : 0.728, Sensitive_Loss : 0.22811, Sensitive_Acc : 16.000, Run Time : 7.84 sec
INFO:root:2024-03-31 21:38:21, Train, Epoch : 4, Step : 1730, Loss : 0.70988, Acc : 0.694, Sensitive_Loss : 0.29003, Sensitive_Acc : 16.700, Run Time : 7.04 sec
INFO:root:2024-03-31 21:38:27, Train, Epoch : 4, Step : 1740, Loss : 0.89607, Acc : 0.741, Sensitive_Loss : 0.20847, Sensitive_Acc : 15.900, Run Time : 6.71 sec
INFO:root:2024-03-31 21:38:35, Train, Epoch : 4, Step : 1750, Loss : 0.94804, Acc : 0.713, Sensitive_Loss : 0.29048, Sensitive_Acc : 17.100, Run Time : 7.32 sec
INFO:root:2024-03-31 21:38:42, Train, Epoch : 4, Step : 1760, Loss : 0.92563, Acc : 0.709, Sensitive_Loss : 0.29742, Sensitive_Acc : 13.900, Run Time : 7.25 sec
INFO:root:2024-03-31 21:38:49, Train, Epoch : 4, Step : 1770, Loss : 0.85847, Acc : 0.722, Sensitive_Loss : 0.25984, Sensitive_Acc : 17.500, Run Time : 7.11 sec
INFO:root:2024-03-31 21:38:56, Train, Epoch : 4, Step : 1780, Loss : 0.92848, Acc : 0.744, Sensitive_Loss : 0.25344, Sensitive_Acc : 15.000, Run Time : 7.33 sec
INFO:root:2024-03-31 21:39:03, Train, Epoch : 4, Step : 1790, Loss : 0.93120, Acc : 0.700, Sensitive_Loss : 0.23028, Sensitive_Acc : 18.900, Run Time : 6.77 sec
INFO:root:2024-03-31 21:39:11, Train, Epoch : 4, Step : 1800, Loss : 0.82398, Acc : 0.716, Sensitive_Loss : 0.21564, Sensitive_Acc : 15.900, Run Time : 7.63 sec
INFO:root:2024-03-31 21:40:45, Dev, Step : 1800, Loss : 0.93121, Acc : 0.777, Auc : 0.790, Sensitive_Loss : 0.27053, Sensitive_Acc : 16.780, Sensitive_Auc : 0.995, Mean auc: 0.790, Run Time : 93.68 sec
INFO:root:2024-03-31 21:40:50, Train, Epoch : 4, Step : 1810, Loss : 0.83001, Acc : 0.688, Sensitive_Loss : 0.21872, Sensitive_Acc : 16.300, Run Time : 99.28 sec
INFO:root:2024-03-31 21:40:57, Train, Epoch : 4, Step : 1820, Loss : 0.74946, Acc : 0.731, Sensitive_Loss : 0.22416, Sensitive_Acc : 15.500, Run Time : 7.16 sec
INFO:root:2024-03-31 21:41:05, Train, Epoch : 4, Step : 1830, Loss : 0.80294, Acc : 0.725, Sensitive_Loss : 0.32587, Sensitive_Acc : 18.700, Run Time : 7.27 sec
INFO:root:2024-03-31 21:41:12, Train, Epoch : 4, Step : 1840, Loss : 0.88449, Acc : 0.716, Sensitive_Loss : 0.22597, Sensitive_Acc : 16.000, Run Time : 7.37 sec
INFO:root:2024-03-31 21:41:19, Train, Epoch : 4, Step : 1850, Loss : 0.74401, Acc : 0.688, Sensitive_Loss : 0.22395, Sensitive_Acc : 16.900, Run Time : 6.54 sec
INFO:root:2024-03-31 21:41:26, Train, Epoch : 4, Step : 1860, Loss : 0.73014, Acc : 0.722, Sensitive_Loss : 0.23803, Sensitive_Acc : 17.200, Run Time : 7.15 sec
INFO:root:2024-03-31 21:41:33, Train, Epoch : 4, Step : 1870, Loss : 0.77258, Acc : 0.700, Sensitive_Loss : 0.19937, Sensitive_Acc : 16.700, Run Time : 7.31 sec
INFO:root:2024-03-31 21:41:41, Train, Epoch : 4, Step : 1880, Loss : 0.87775, Acc : 0.722, Sensitive_Loss : 0.23288, Sensitive_Acc : 17.700, Run Time : 7.57 sec
INFO:root:2024-03-31 21:41:47, Train, Epoch : 4, Step : 1890, Loss : 0.76394, Acc : 0.694, Sensitive_Loss : 0.26977, Sensitive_Acc : 17.700, Run Time : 6.69 sec
INFO:root:2024-03-31 21:41:54, Train, Epoch : 4, Step : 1900, Loss : 0.77227, Acc : 0.706, Sensitive_Loss : 0.29300, Sensitive_Acc : 15.800, Run Time : 6.94 sec
INFO:root:2024-03-31 21:43:28, Dev, Step : 1900, Loss : 0.92565, Acc : 0.769, Auc : 0.789, Sensitive_Loss : 0.28774, Sensitive_Acc : 16.638, Sensitive_Auc : 0.995, Mean auc: 0.789, Run Time : 93.88 sec
INFO:root:2024-03-31 21:43:34, Train, Epoch : 4, Step : 1910, Loss : 0.71408, Acc : 0.703, Sensitive_Loss : 0.26652, Sensitive_Acc : 16.300, Run Time : 99.51 sec
INFO:root:2024-03-31 21:43:41, Train, Epoch : 4, Step : 1920, Loss : 1.18287, Acc : 0.662, Sensitive_Loss : 0.22739, Sensitive_Acc : 16.300, Run Time : 7.50 sec
INFO:root:2024-03-31 21:43:48, Train, Epoch : 4, Step : 1930, Loss : 0.82508, Acc : 0.731, Sensitive_Loss : 0.31535, Sensitive_Acc : 17.300, Run Time : 6.60 sec
INFO:root:2024-03-31 21:43:55, Train, Epoch : 4, Step : 1940, Loss : 0.83779, Acc : 0.678, Sensitive_Loss : 0.29518, Sensitive_Acc : 16.700, Run Time : 7.20 sec
INFO:root:2024-03-31 21:44:02, Train, Epoch : 4, Step : 1950, Loss : 0.72314, Acc : 0.744, Sensitive_Loss : 0.25328, Sensitive_Acc : 16.000, Run Time : 6.97 sec
INFO:root:2024-03-31 21:44:09, Train, Epoch : 4, Step : 1960, Loss : 0.92062, Acc : 0.694, Sensitive_Loss : 0.31124, Sensitive_Acc : 15.600, Run Time : 7.08 sec
INFO:root:2024-03-31 21:45:47
INFO:root:y_pred: [0.3957269  0.14416228 0.20614724 ... 0.4255821  0.457394   0.12583137]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [2.21475144e-03 2.24130869e-01 9.72850740e-01 1.16704054e-01
 9.98458624e-01 2.25369702e-03 8.45814496e-02 9.82542336e-02
 5.49102714e-03 1.59158006e-01 1.16277924e-02 8.14860761e-02
 9.75959480e-01 1.86061561e-02 9.82005239e-01 9.55090225e-01
 2.26405612e-03 6.26043916e-01 9.40547824e-01 8.44219625e-01
 5.18729202e-02 3.68946195e-02 9.98962641e-01 9.81543005e-01
 1.25593394e-01 4.02804106e-01 2.11375300e-02 9.93332207e-01
 2.62902416e-02 9.34192300e-01 9.99547899e-01 8.74655128e-01
 6.75626658e-03 1.02178376e-04 2.54765958e-01 1.51276384e-02
 5.04891910e-02 5.75815678e-01 9.67234731e-01 9.64576483e-01
 1.32411318e-02 4.77940496e-03 9.93623674e-01 9.99149084e-01
 7.76263326e-02 8.76087472e-02 8.45938642e-03 6.81515783e-02
 9.97766852e-01 9.66742933e-01 9.93136883e-01 8.44003201e-01
 9.96695399e-01 8.91978025e-01 9.92537081e-01 6.59136415e-01
 9.09888923e-01 8.76655243e-03 9.99033809e-01 9.92633924e-02
 3.56918434e-03 5.32845736e-01 9.19682086e-01 9.96919394e-01
 9.96059954e-01 9.96704280e-01 7.06153274e-01 9.82275724e-01
 9.74891186e-01 9.91439581e-01 9.24867034e-01 8.87351409e-02
 1.71609339e-03 5.93831874e-02 8.91967297e-01 4.21473815e-04
 9.79558170e-01 3.34480125e-03 8.95733714e-01 8.36628497e-01
 2.55396843e-01 1.74794972e-01 1.15867555e-01 6.79958105e-01
 9.80499864e-01 6.43404424e-01 8.28324199e-01 2.53301740e-01
 3.66286486e-02 3.61635312e-02 6.56515360e-02 7.87132606e-02
 1.02221733e-02 4.50451449e-02 9.14076030e-01 7.47317135e-01
 2.74876207e-02 6.45556953e-03 8.82312477e-01 9.37994003e-01
 9.77117419e-01 6.28870800e-02 9.98032510e-01 1.85687281e-02
 7.23137110e-02 9.56956625e-01 4.05771881e-01 1.15793511e-01
 1.06418878e-02 2.70803869e-02 4.49149273e-02 9.66226757e-01
 2.27232561e-01 5.57947569e-02 1.81238815e-01 5.31597063e-04
 9.98305559e-01 6.07388020e-01 1.54008836e-01 7.92390434e-04
 5.51319242e-01 3.31356563e-03 9.97452915e-01 8.09028186e-03
 1.96670547e-01 9.99345958e-01 4.90776636e-03 9.41965222e-01
 8.45052242e-01 9.96463001e-01 9.99873038e-04 9.93056655e-01
 5.71193267e-03 9.97572839e-01 9.94650066e-01 5.07451534e-01
 1.43488273e-01 5.98629653e-01 7.60968495e-03 9.95591581e-01
 9.97028768e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-31 21:45:47, Dev, Step : 1968, Loss : 0.91970, Acc : 0.704, Auc : 0.794, Sensitive_Loss : 0.27239, Sensitive_Acc : 16.667, Sensitive_Auc : 0.997, Mean auc: 0.794, Run Time : 92.94 sec
INFO:root:2024-03-31 21:45:48, Best, Step : 1968, Loss : 0.91970, Acc : 0.704,Auc : 0.794, Best Auc : 0.794, Sensitive_Loss : 0.27239, Sensitive_Acc : 16.667, Sensitive_Auc : 0.997
INFO:root:2024-03-31 21:45:51, Train, Epoch : 5, Step : 1970, Loss : 0.17391, Acc : 0.144, Sensitive_Loss : 0.05279, Sensitive_Acc : 3.500, Run Time : 2.63 sec
INFO:root:2024-03-31 21:45:58, Train, Epoch : 5, Step : 1980, Loss : 0.73820, Acc : 0.766, Sensitive_Loss : 0.29760, Sensitive_Acc : 16.900, Run Time : 6.79 sec
INFO:root:2024-03-31 21:46:06, Train, Epoch : 5, Step : 1990, Loss : 0.79520, Acc : 0.725, Sensitive_Loss : 0.26330, Sensitive_Acc : 13.800, Run Time : 7.30 sec
INFO:root:2024-03-31 21:46:13, Train, Epoch : 5, Step : 2000, Loss : 0.74252, Acc : 0.750, Sensitive_Loss : 0.20549, Sensitive_Acc : 17.100, Run Time : 7.65 sec
INFO:root:2024-03-31 21:47:47, Dev, Step : 2000, Loss : 0.91450, Acc : 0.756, Auc : 0.794, Sensitive_Loss : 0.27120, Sensitive_Acc : 16.681, Sensitive_Auc : 0.997, Mean auc: 0.794, Run Time : 93.27 sec
INFO:root:2024-03-31 21:47:52, Train, Epoch : 5, Step : 2010, Loss : 0.85258, Acc : 0.694, Sensitive_Loss : 0.20538, Sensitive_Acc : 17.600, Run Time : 98.78 sec
INFO:root:2024-03-31 21:47:59, Train, Epoch : 5, Step : 2020, Loss : 0.77754, Acc : 0.706, Sensitive_Loss : 0.23743, Sensitive_Acc : 16.100, Run Time : 7.19 sec
INFO:root:2024-03-31 21:48:06, Train, Epoch : 5, Step : 2030, Loss : 1.06550, Acc : 0.684, Sensitive_Loss : 0.21710, Sensitive_Acc : 15.100, Run Time : 6.66 sec
INFO:root:2024-03-31 21:48:13, Train, Epoch : 5, Step : 2040, Loss : 0.79746, Acc : 0.753, Sensitive_Loss : 0.21669, Sensitive_Acc : 17.300, Run Time : 7.43 sec
INFO:root:2024-03-31 21:48:20, Train, Epoch : 5, Step : 2050, Loss : 0.80791, Acc : 0.684, Sensitive_Loss : 0.25698, Sensitive_Acc : 15.800, Run Time : 7.03 sec
INFO:root:2024-03-31 21:48:28, Train, Epoch : 5, Step : 2060, Loss : 0.89184, Acc : 0.722, Sensitive_Loss : 0.20389, Sensitive_Acc : 16.800, Run Time : 7.40 sec
INFO:root:2024-03-31 21:48:35, Train, Epoch : 5, Step : 2070, Loss : 0.71932, Acc : 0.713, Sensitive_Loss : 0.22086, Sensitive_Acc : 14.100, Run Time : 6.89 sec
INFO:root:2024-03-31 21:48:42, Train, Epoch : 5, Step : 2080, Loss : 0.85878, Acc : 0.706, Sensitive_Loss : 0.25286, Sensitive_Acc : 15.900, Run Time : 7.36 sec
INFO:root:2024-03-31 21:48:49, Train, Epoch : 5, Step : 2090, Loss : 0.62987, Acc : 0.738, Sensitive_Loss : 0.26505, Sensitive_Acc : 17.000, Run Time : 6.95 sec
INFO:root:2024-03-31 21:48:56, Train, Epoch : 5, Step : 2100, Loss : 0.68209, Acc : 0.700, Sensitive_Loss : 0.17367, Sensitive_Acc : 15.800, Run Time : 7.25 sec
INFO:root:2024-03-31 21:50:30, Dev, Step : 2100, Loss : 0.92866, Acc : 0.736, Auc : 0.789, Sensitive_Loss : 0.25051, Sensitive_Acc : 16.879, Sensitive_Auc : 0.998, Mean auc: 0.789, Run Time : 93.63 sec
INFO:root:2024-03-31 21:50:35, Train, Epoch : 5, Step : 2110, Loss : 0.89717, Acc : 0.706, Sensitive_Loss : 0.22878, Sensitive_Acc : 15.600, Run Time : 98.97 sec
INFO:root:2024-03-31 21:50:42, Train, Epoch : 5, Step : 2120, Loss : 0.81131, Acc : 0.703, Sensitive_Loss : 0.19235, Sensitive_Acc : 17.500, Run Time : 7.19 sec
INFO:root:2024-03-31 21:50:50, Train, Epoch : 5, Step : 2130, Loss : 0.57326, Acc : 0.762, Sensitive_Loss : 0.29149, Sensitive_Acc : 17.200, Run Time : 7.20 sec
INFO:root:2024-03-31 21:50:57, Train, Epoch : 5, Step : 2140, Loss : 0.81161, Acc : 0.719, Sensitive_Loss : 0.23997, Sensitive_Acc : 14.800, Run Time : 7.45 sec
INFO:root:2024-03-31 21:51:04, Train, Epoch : 5, Step : 2150, Loss : 0.76112, Acc : 0.694, Sensitive_Loss : 0.23897, Sensitive_Acc : 17.600, Run Time : 7.42 sec
INFO:root:2024-03-31 21:51:11, Train, Epoch : 5, Step : 2160, Loss : 0.64233, Acc : 0.706, Sensitive_Loss : 0.21786, Sensitive_Acc : 16.100, Run Time : 6.84 sec
INFO:root:2024-03-31 21:51:18, Train, Epoch : 5, Step : 2170, Loss : 0.83430, Acc : 0.722, Sensitive_Loss : 0.24471, Sensitive_Acc : 19.000, Run Time : 6.84 sec
INFO:root:2024-03-31 21:51:25, Train, Epoch : 5, Step : 2180, Loss : 0.90937, Acc : 0.709, Sensitive_Loss : 0.22087, Sensitive_Acc : 17.300, Run Time : 7.05 sec
INFO:root:2024-03-31 21:51:32, Train, Epoch : 5, Step : 2190, Loss : 1.02305, Acc : 0.684, Sensitive_Loss : 0.27121, Sensitive_Acc : 17.700, Run Time : 7.16 sec
INFO:root:2024-03-31 21:51:39, Train, Epoch : 5, Step : 2200, Loss : 0.79798, Acc : 0.694, Sensitive_Loss : 0.27326, Sensitive_Acc : 14.100, Run Time : 6.91 sec
INFO:root:2024-03-31 21:53:14, Dev, Step : 2200, Loss : 0.94329, Acc : 0.792, Auc : 0.786, Sensitive_Loss : 0.27364, Sensitive_Acc : 16.723, Sensitive_Auc : 0.997, Mean auc: 0.786, Run Time : 94.49 sec
INFO:root:2024-03-31 21:53:20, Train, Epoch : 5, Step : 2210, Loss : 0.96131, Acc : 0.700, Sensitive_Loss : 0.21879, Sensitive_Acc : 16.200, Run Time : 100.56 sec
INFO:root:2024-03-31 21:53:27, Train, Epoch : 5, Step : 2220, Loss : 0.77897, Acc : 0.691, Sensitive_Loss : 0.33022, Sensitive_Acc : 17.500, Run Time : 7.18 sec
INFO:root:2024-03-31 21:53:34, Train, Epoch : 5, Step : 2230, Loss : 0.89281, Acc : 0.722, Sensitive_Loss : 0.21588, Sensitive_Acc : 17.600, Run Time : 7.03 sec
INFO:root:2024-03-31 21:53:41, Train, Epoch : 5, Step : 2240, Loss : 0.80094, Acc : 0.700, Sensitive_Loss : 0.23923, Sensitive_Acc : 18.500, Run Time : 7.41 sec
INFO:root:2024-03-31 21:53:49, Train, Epoch : 5, Step : 2250, Loss : 0.90711, Acc : 0.741, Sensitive_Loss : 0.21350, Sensitive_Acc : 16.300, Run Time : 7.38 sec
INFO:root:2024-03-31 21:53:56, Train, Epoch : 5, Step : 2260, Loss : 0.72906, Acc : 0.734, Sensitive_Loss : 0.23072, Sensitive_Acc : 14.700, Run Time : 7.09 sec
INFO:root:2024-03-31 21:54:03, Train, Epoch : 5, Step : 2270, Loss : 0.81148, Acc : 0.747, Sensitive_Loss : 0.19595, Sensitive_Acc : 17.600, Run Time : 7.03 sec
INFO:root:2024-03-31 21:54:10, Train, Epoch : 5, Step : 2280, Loss : 0.97181, Acc : 0.684, Sensitive_Loss : 0.29354, Sensitive_Acc : 16.500, Run Time : 7.04 sec
INFO:root:2024-03-31 21:54:17, Train, Epoch : 5, Step : 2290, Loss : 0.78204, Acc : 0.750, Sensitive_Loss : 0.22759, Sensitive_Acc : 15.100, Run Time : 7.17 sec
INFO:root:2024-03-31 21:54:25, Train, Epoch : 5, Step : 2300, Loss : 0.76767, Acc : 0.741, Sensitive_Loss : 0.26371, Sensitive_Acc : 16.000, Run Time : 7.53 sec
INFO:root:2024-03-31 21:55:58, Dev, Step : 2300, Loss : 0.93193, Acc : 0.748, Auc : 0.784, Sensitive_Loss : 0.26717, Sensitive_Acc : 16.837, Sensitive_Auc : 0.998, Mean auc: 0.784, Run Time : 93.37 sec
INFO:root:2024-03-31 21:56:04, Train, Epoch : 5, Step : 2310, Loss : 0.80052, Acc : 0.716, Sensitive_Loss : 0.19972, Sensitive_Acc : 16.700, Run Time : 99.04 sec
INFO:root:2024-03-31 21:56:11, Train, Epoch : 5, Step : 2320, Loss : 0.67668, Acc : 0.703, Sensitive_Loss : 0.19885, Sensitive_Acc : 16.400, Run Time : 7.31 sec
INFO:root:2024-03-31 21:56:19, Train, Epoch : 5, Step : 2330, Loss : 0.66795, Acc : 0.731, Sensitive_Loss : 0.20441, Sensitive_Acc : 16.500, Run Time : 7.61 sec
INFO:root:2024-03-31 21:56:25, Train, Epoch : 5, Step : 2340, Loss : 0.88533, Acc : 0.750, Sensitive_Loss : 0.24858, Sensitive_Acc : 14.900, Run Time : 6.46 sec
INFO:root:2024-03-31 21:56:32, Train, Epoch : 5, Step : 2350, Loss : 0.84567, Acc : 0.728, Sensitive_Loss : 0.20456, Sensitive_Acc : 15.700, Run Time : 6.98 sec
INFO:root:2024-03-31 21:56:39, Train, Epoch : 5, Step : 2360, Loss : 0.90705, Acc : 0.747, Sensitive_Loss : 0.24229, Sensitive_Acc : 18.000, Run Time : 7.05 sec
INFO:root:2024-03-31 21:56:46, Train, Epoch : 5, Step : 2370, Loss : 0.87673, Acc : 0.728, Sensitive_Loss : 0.21669, Sensitive_Acc : 16.600, Run Time : 7.06 sec
INFO:root:2024-03-31 21:56:53, Train, Epoch : 5, Step : 2380, Loss : 0.76153, Acc : 0.731, Sensitive_Loss : 0.22607, Sensitive_Acc : 17.200, Run Time : 7.30 sec
INFO:root:2024-03-31 21:57:01, Train, Epoch : 5, Step : 2390, Loss : 0.77394, Acc : 0.750, Sensitive_Loss : 0.26559, Sensitive_Acc : 13.700, Run Time : 7.60 sec
INFO:root:2024-03-31 21:57:08, Train, Epoch : 5, Step : 2400, Loss : 0.83191, Acc : 0.750, Sensitive_Loss : 0.31604, Sensitive_Acc : 17.100, Run Time : 6.84 sec
INFO:root:2024-03-31 21:58:42, Dev, Step : 2400, Loss : 0.91985, Acc : 0.733, Auc : 0.792, Sensitive_Loss : 0.27478, Sensitive_Acc : 16.652, Sensitive_Auc : 0.998, Mean auc: 0.792, Run Time : 93.82 sec
INFO:root:2024-03-31 21:58:47, Train, Epoch : 5, Step : 2410, Loss : 0.86536, Acc : 0.725, Sensitive_Loss : 0.19670, Sensitive_Acc : 14.800, Run Time : 99.37 sec
INFO:root:2024-03-31 21:58:54, Train, Epoch : 5, Step : 2420, Loss : 0.83183, Acc : 0.756, Sensitive_Loss : 0.20970, Sensitive_Acc : 15.900, Run Time : 7.00 sec
INFO:root:2024-03-31 21:59:01, Train, Epoch : 5, Step : 2430, Loss : 0.78984, Acc : 0.734, Sensitive_Loss : 0.21860, Sensitive_Acc : 14.700, Run Time : 7.09 sec
INFO:root:2024-03-31 21:59:09, Train, Epoch : 5, Step : 2440, Loss : 0.75979, Acc : 0.716, Sensitive_Loss : 0.20895, Sensitive_Acc : 15.800, Run Time : 7.24 sec
INFO:root:2024-03-31 21:59:16, Train, Epoch : 5, Step : 2450, Loss : 0.82467, Acc : 0.741, Sensitive_Loss : 0.24918, Sensitive_Acc : 16.100, Run Time : 7.13 sec
INFO:root:2024-03-31 21:59:22, Train, Epoch : 5, Step : 2460, Loss : 0.87971, Acc : 0.747, Sensitive_Loss : 0.30738, Sensitive_Acc : 17.400, Run Time : 6.52 sec
INFO:root:2024-03-31 22:00:55
INFO:root:y_pred: [0.23903085 0.11158041 0.12725934 ... 0.42788    0.26452217 0.068265  ]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [1.49811269e-03 2.76187837e-01 9.89188790e-01 1.69278368e-01
 9.99058664e-01 3.40056093e-03 1.11412451e-01 1.07231148e-01
 6.09869510e-03 2.28015348e-01 1.66809801e-02 5.18294089e-02
 9.86125767e-01 3.92013006e-02 9.91817057e-01 9.57268834e-01
 2.58399686e-03 6.89205706e-01 9.56852376e-01 8.74839246e-01
 6.83618113e-02 3.12246438e-02 9.99383688e-01 9.85916793e-01
 8.24358463e-02 5.61284363e-01 3.26467864e-02 9.95970368e-01
 1.89798772e-02 9.38816309e-01 9.99625206e-01 9.49773312e-01
 6.35903468e-03 1.27160718e-04 2.14898899e-01 1.93319507e-02
 6.01335019e-02 5.34878552e-01 9.83795166e-01 9.77832735e-01
 1.15334978e-02 8.96178279e-03 9.95239735e-01 9.99759257e-01
 1.45491108e-01 1.03797115e-01 1.06520839e-02 5.08905426e-02
 9.99147177e-01 9.75124717e-01 9.90323424e-01 8.46485436e-01
 9.97611642e-01 9.23823059e-01 9.93960738e-01 8.60029995e-01
 9.38821256e-01 1.15186004e-02 9.99214053e-01 9.23472866e-02
 6.52079377e-03 5.46146393e-01 9.50941741e-01 9.97320235e-01
 9.98076439e-01 9.96284485e-01 7.88550615e-01 9.93866503e-01
 9.87655699e-01 9.90883410e-01 9.51992214e-01 8.38324279e-02
 2.01085978e-03 1.04533643e-01 9.38227654e-01 3.76261421e-04
 9.80505764e-01 2.27668602e-03 9.11097646e-01 7.95381367e-01
 2.66416878e-01 1.14712581e-01 1.41253337e-01 7.60275781e-01
 9.70545232e-01 7.27751136e-01 8.74567747e-01 4.01932597e-01
 6.48615658e-02 5.12130782e-02 7.47722536e-02 8.78597721e-02
 1.59765724e-02 3.73497084e-02 9.64512825e-01 8.91630888e-01
 4.35005315e-02 6.94802171e-03 9.51451719e-01 9.25404310e-01
 9.76017177e-01 8.49471539e-02 9.98573780e-01 1.21682789e-02
 9.54589397e-02 9.70643163e-01 4.94771361e-01 8.38678926e-02
 2.18011327e-02 2.69391555e-02 3.47785279e-02 9.71614957e-01
 2.18358845e-01 7.89531022e-02 2.05757558e-01 5.31660859e-04
 9.98635828e-01 5.05061746e-01 4.60233510e-01 4.44053876e-04
 6.37972057e-01 6.05196645e-03 9.98936832e-01 4.24066512e-03
 2.09147468e-01 9.99576032e-01 5.67941926e-03 9.69686568e-01
 9.30178463e-01 9.98855114e-01 2.06168857e-03 9.97649252e-01
 6.31626742e-03 9.99024391e-01 9.96813476e-01 6.68553889e-01
 3.43971372e-01 6.07156456e-01 1.26551269e-02 9.97192204e-01
 9.97981787e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-31 22:00:55, Dev, Step : 2460, Loss : 0.93225, Acc : 0.778, Auc : 0.789, Sensitive_Loss : 0.29242, Sensitive_Acc : 16.624, Sensitive_Auc : 0.999, Mean auc: 0.789, Run Time : 92.97 sec
INFO:root:2024-03-31 22:01:04, Train, Epoch : 6, Step : 2470, Loss : 0.78426, Acc : 0.744, Sensitive_Loss : 0.20013, Sensitive_Acc : 17.600, Run Time : 7.76 sec
INFO:root:2024-03-31 22:01:12, Train, Epoch : 6, Step : 2480, Loss : 0.87383, Acc : 0.741, Sensitive_Loss : 0.19314, Sensitive_Acc : 17.200, Run Time : 7.76 sec
INFO:root:2024-03-31 22:01:19, Train, Epoch : 6, Step : 2490, Loss : 0.61391, Acc : 0.728, Sensitive_Loss : 0.31438, Sensitive_Acc : 18.200, Run Time : 7.16 sec
INFO:root:2024-03-31 22:01:26, Train, Epoch : 6, Step : 2500, Loss : 0.75180, Acc : 0.716, Sensitive_Loss : 0.31873, Sensitive_Acc : 18.000, Run Time : 6.84 sec
INFO:root:2024-03-31 22:03:00, Dev, Step : 2500, Loss : 0.92272, Acc : 0.752, Auc : 0.791, Sensitive_Loss : 0.28023, Sensitive_Acc : 16.681, Sensitive_Auc : 0.999, Mean auc: 0.791, Run Time : 94.10 sec
INFO:root:2024-03-31 22:03:05, Train, Epoch : 6, Step : 2510, Loss : 0.88555, Acc : 0.738, Sensitive_Loss : 0.24654, Sensitive_Acc : 16.600, Run Time : 99.70 sec
INFO:root:2024-03-31 22:03:13, Train, Epoch : 6, Step : 2520, Loss : 0.89738, Acc : 0.725, Sensitive_Loss : 0.20748, Sensitive_Acc : 16.200, Run Time : 7.36 sec
INFO:root:2024-03-31 22:03:19, Train, Epoch : 6, Step : 2530, Loss : 0.66902, Acc : 0.703, Sensitive_Loss : 0.20426, Sensitive_Acc : 17.400, Run Time : 6.59 sec
INFO:root:2024-03-31 22:03:27, Train, Epoch : 6, Step : 2540, Loss : 0.64184, Acc : 0.719, Sensitive_Loss : 0.21310, Sensitive_Acc : 18.300, Run Time : 7.25 sec
INFO:root:2024-03-31 22:03:34, Train, Epoch : 6, Step : 2550, Loss : 0.79308, Acc : 0.706, Sensitive_Loss : 0.24946, Sensitive_Acc : 17.300, Run Time : 7.01 sec
INFO:root:2024-03-31 22:03:41, Train, Epoch : 6, Step : 2560, Loss : 0.72967, Acc : 0.728, Sensitive_Loss : 0.23076, Sensitive_Acc : 16.400, Run Time : 7.29 sec
INFO:root:2024-03-31 22:03:48, Train, Epoch : 6, Step : 2570, Loss : 0.84525, Acc : 0.725, Sensitive_Loss : 0.23424, Sensitive_Acc : 18.200, Run Time : 7.50 sec
INFO:root:2024-03-31 22:03:56, Train, Epoch : 6, Step : 2580, Loss : 0.73720, Acc : 0.719, Sensitive_Loss : 0.27912, Sensitive_Acc : 14.500, Run Time : 7.30 sec
INFO:root:2024-03-31 22:04:03, Train, Epoch : 6, Step : 2590, Loss : 0.61217, Acc : 0.747, Sensitive_Loss : 0.21113, Sensitive_Acc : 16.100, Run Time : 7.68 sec
INFO:root:2024-03-31 22:04:10, Train, Epoch : 6, Step : 2600, Loss : 0.66910, Acc : 0.728, Sensitive_Loss : 0.23104, Sensitive_Acc : 18.300, Run Time : 6.58 sec
INFO:root:2024-03-31 22:05:44, Dev, Step : 2600, Loss : 0.92152, Acc : 0.743, Auc : 0.791, Sensitive_Loss : 0.26866, Sensitive_Acc : 16.723, Sensitive_Auc : 0.998, Mean auc: 0.791, Run Time : 94.07 sec
INFO:root:2024-03-31 22:05:50, Train, Epoch : 6, Step : 2610, Loss : 0.64779, Acc : 0.731, Sensitive_Loss : 0.25231, Sensitive_Acc : 15.600, Run Time : 99.86 sec
INFO:root:2024-03-31 22:05:56, Train, Epoch : 6, Step : 2620, Loss : 0.76710, Acc : 0.734, Sensitive_Loss : 0.23421, Sensitive_Acc : 16.800, Run Time : 6.50 sec
INFO:root:2024-03-31 22:06:03, Train, Epoch : 6, Step : 2630, Loss : 0.71105, Acc : 0.725, Sensitive_Loss : 0.25812, Sensitive_Acc : 17.300, Run Time : 7.25 sec
INFO:root:2024-03-31 22:06:11, Train, Epoch : 6, Step : 2640, Loss : 0.75191, Acc : 0.728, Sensitive_Loss : 0.23489, Sensitive_Acc : 17.000, Run Time : 8.01 sec
INFO:root:2024-03-31 22:06:18, Train, Epoch : 6, Step : 2650, Loss : 0.75944, Acc : 0.731, Sensitive_Loss : 0.27867, Sensitive_Acc : 17.900, Run Time : 6.32 sec
INFO:root:2024-03-31 22:06:25, Train, Epoch : 6, Step : 2660, Loss : 0.87441, Acc : 0.706, Sensitive_Loss : 0.24487, Sensitive_Acc : 16.800, Run Time : 7.36 sec
INFO:root:2024-03-31 22:06:32, Train, Epoch : 6, Step : 2670, Loss : 1.13502, Acc : 0.694, Sensitive_Loss : 0.19269, Sensitive_Acc : 15.500, Run Time : 7.10 sec
INFO:root:2024-03-31 22:06:39, Train, Epoch : 6, Step : 2680, Loss : 0.71803, Acc : 0.734, Sensitive_Loss : 0.21809, Sensitive_Acc : 19.000, Run Time : 6.87 sec
INFO:root:2024-03-31 22:06:46, Train, Epoch : 6, Step : 2690, Loss : 0.77619, Acc : 0.756, Sensitive_Loss : 0.25362, Sensitive_Acc : 16.200, Run Time : 7.16 sec
INFO:root:2024-03-31 22:06:54, Train, Epoch : 6, Step : 2700, Loss : 0.86777, Acc : 0.725, Sensitive_Loss : 0.28392, Sensitive_Acc : 18.400, Run Time : 7.53 sec
INFO:root:2024-03-31 22:08:29, Dev, Step : 2700, Loss : 0.93671, Acc : 0.751, Auc : 0.782, Sensitive_Loss : 0.27123, Sensitive_Acc : 16.794, Sensitive_Auc : 0.998, Mean auc: 0.782, Run Time : 94.73 sec
INFO:root:2024-03-31 22:08:34, Train, Epoch : 6, Step : 2710, Loss : 0.82715, Acc : 0.738, Sensitive_Loss : 0.27787, Sensitive_Acc : 14.700, Run Time : 100.21 sec
INFO:root:2024-03-31 22:08:42, Train, Epoch : 6, Step : 2720, Loss : 0.89027, Acc : 0.741, Sensitive_Loss : 0.19129, Sensitive_Acc : 16.600, Run Time : 7.90 sec
INFO:root:2024-03-31 22:08:50, Train, Epoch : 6, Step : 2730, Loss : 0.80073, Acc : 0.734, Sensitive_Loss : 0.20176, Sensitive_Acc : 18.000, Run Time : 7.59 sec
INFO:root:2024-03-31 22:08:57, Train, Epoch : 6, Step : 2740, Loss : 0.68134, Acc : 0.747, Sensitive_Loss : 0.20692, Sensitive_Acc : 15.900, Run Time : 7.25 sec
INFO:root:2024-03-31 22:09:05, Train, Epoch : 6, Step : 2750, Loss : 0.59330, Acc : 0.756, Sensitive_Loss : 0.22089, Sensitive_Acc : 17.200, Run Time : 7.81 sec
INFO:root:2024-03-31 22:09:13, Train, Epoch : 6, Step : 2760, Loss : 0.80647, Acc : 0.747, Sensitive_Loss : 0.22133, Sensitive_Acc : 16.000, Run Time : 8.23 sec
INFO:root:2024-03-31 22:09:20, Train, Epoch : 6, Step : 2770, Loss : 0.89243, Acc : 0.709, Sensitive_Loss : 0.15959, Sensitive_Acc : 15.400, Run Time : 7.56 sec
INFO:root:2024-03-31 22:09:28, Train, Epoch : 6, Step : 2780, Loss : 0.66620, Acc : 0.716, Sensitive_Loss : 0.26907, Sensitive_Acc : 19.900, Run Time : 7.46 sec
INFO:root:2024-03-31 22:09:35, Train, Epoch : 6, Step : 2790, Loss : 0.62225, Acc : 0.728, Sensitive_Loss : 0.21028, Sensitive_Acc : 15.000, Run Time : 7.17 sec
INFO:root:2024-03-31 22:09:42, Train, Epoch : 6, Step : 2800, Loss : 0.86941, Acc : 0.738, Sensitive_Loss : 0.28716, Sensitive_Acc : 15.700, Run Time : 6.85 sec
INFO:root:2024-03-31 22:11:16, Dev, Step : 2800, Loss : 0.91285, Acc : 0.754, Auc : 0.797, Sensitive_Loss : 0.24559, Sensitive_Acc : 16.794, Sensitive_Auc : 0.998, Mean auc: 0.797, Run Time : 94.48 sec
INFO:root:2024-03-31 22:11:17, Best, Step : 2800, Loss : 0.91285, Acc : 0.754, Auc : 0.797, Sensitive_Loss : 0.24559, Sensitive_Acc : 16.794, Sensitive_Auc : 0.998, Best Auc : 0.797
INFO:root:2024-03-31 22:11:23, Train, Epoch : 6, Step : 2810, Loss : 0.66476, Acc : 0.719, Sensitive_Loss : 0.18851, Sensitive_Acc : 17.400, Run Time : 100.66 sec
INFO:root:2024-03-31 22:11:30, Train, Epoch : 6, Step : 2820, Loss : 0.71737, Acc : 0.769, Sensitive_Loss : 0.20801, Sensitive_Acc : 15.400, Run Time : 7.48 sec
INFO:root:2024-03-31 22:11:37, Train, Epoch : 6, Step : 2830, Loss : 0.84293, Acc : 0.731, Sensitive_Loss : 0.21171, Sensitive_Acc : 14.100, Run Time : 7.19 sec
INFO:root:2024-03-31 22:11:44, Train, Epoch : 6, Step : 2840, Loss : 0.84986, Acc : 0.753, Sensitive_Loss : 0.23675, Sensitive_Acc : 14.300, Run Time : 7.02 sec
INFO:root:2024-03-31 22:11:52, Train, Epoch : 6, Step : 2850, Loss : 0.61727, Acc : 0.744, Sensitive_Loss : 0.17954, Sensitive_Acc : 16.700, Run Time : 7.59 sec
INFO:root:2024-03-31 22:11:59, Train, Epoch : 6, Step : 2860, Loss : 0.61242, Acc : 0.753, Sensitive_Loss : 0.23559, Sensitive_Acc : 16.800, Run Time : 7.10 sec
INFO:root:2024-03-31 22:12:05, Train, Epoch : 6, Step : 2870, Loss : 0.75763, Acc : 0.753, Sensitive_Loss : 0.26330, Sensitive_Acc : 15.000, Run Time : 6.45 sec
INFO:root:2024-03-31 22:12:13, Train, Epoch : 6, Step : 2880, Loss : 0.76914, Acc : 0.706, Sensitive_Loss : 0.22497, Sensitive_Acc : 18.300, Run Time : 7.23 sec
INFO:root:2024-03-31 22:12:20, Train, Epoch : 6, Step : 2890, Loss : 0.89560, Acc : 0.716, Sensitive_Loss : 0.22442, Sensitive_Acc : 14.400, Run Time : 7.85 sec
INFO:root:2024-03-31 22:12:28, Train, Epoch : 6, Step : 2900, Loss : 0.77243, Acc : 0.747, Sensitive_Loss : 0.22238, Sensitive_Acc : 16.100, Run Time : 7.10 sec
INFO:root:2024-03-31 22:14:04, Dev, Step : 2900, Loss : 0.94698, Acc : 0.705, Auc : 0.780, Sensitive_Loss : 0.22291, Sensitive_Acc : 16.979, Sensitive_Auc : 0.999, Mean auc: 0.780, Run Time : 96.41 sec
INFO:root:2024-03-31 22:14:09, Train, Epoch : 6, Step : 2910, Loss : 0.85030, Acc : 0.744, Sensitive_Loss : 0.25239, Sensitive_Acc : 19.300, Run Time : 101.96 sec
INFO:root:2024-03-31 22:14:17, Train, Epoch : 6, Step : 2920, Loss : 0.76605, Acc : 0.734, Sensitive_Loss : 0.21510, Sensitive_Acc : 16.200, Run Time : 7.72 sec
INFO:root:2024-03-31 22:14:24, Train, Epoch : 6, Step : 2930, Loss : 0.77335, Acc : 0.744, Sensitive_Loss : 0.18709, Sensitive_Acc : 17.000, Run Time : 7.18 sec
INFO:root:2024-03-31 22:14:32, Train, Epoch : 6, Step : 2940, Loss : 0.71142, Acc : 0.744, Sensitive_Loss : 0.20315, Sensitive_Acc : 16.500, Run Time : 7.26 sec
INFO:root:2024-03-31 22:14:39, Train, Epoch : 6, Step : 2950, Loss : 0.77538, Acc : 0.747, Sensitive_Loss : 0.21704, Sensitive_Acc : 15.500, Run Time : 7.19 sec
INFO:root:2024-03-31 22:16:14
INFO:root:y_pred: [0.35626382 0.10734304 0.24092817 ... 0.48681438 0.377103   0.089468  ]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [3.1438041e-03 1.2493789e-01 9.8313844e-01 2.0281973e-01 9.9920696e-01
 3.9534876e-03 5.1536214e-02 1.2557152e-01 6.9479593e-03 1.6759428e-01
 1.1436918e-02 2.7473468e-02 9.8238271e-01 2.0054843e-02 9.8981243e-01
 9.3034858e-01 4.5917537e-03 5.5001748e-01 9.6600127e-01 8.4736562e-01
 4.2410150e-02 3.2143567e-02 9.9966812e-01 9.8996818e-01 5.2815281e-02
 3.6353096e-01 6.9963043e-03 9.9497366e-01 1.6373210e-02 9.1175544e-01
 9.9964702e-01 9.1053599e-01 8.3464347e-03 4.9071405e-05 1.2651214e-01
 1.3297797e-02 3.4854420e-02 3.8872302e-01 9.8233312e-01 9.7588819e-01
 3.8838289e-03 3.6850402e-03 9.9172145e-01 9.9953532e-01 8.7611929e-02
 3.5229526e-02 1.8139683e-02 3.9382655e-02 9.9784613e-01 9.5042104e-01
 9.9199390e-01 6.8379450e-01 9.9789512e-01 9.5368540e-01 9.9542505e-01
 8.8001192e-01 9.4849420e-01 1.2929684e-02 9.9931848e-01 1.2829146e-01
 1.7187520e-03 5.1628834e-01 8.9751399e-01 9.9684310e-01 9.9909317e-01
 9.9585223e-01 8.4001744e-01 9.8148525e-01 9.8435348e-01 9.7779191e-01
 9.6379137e-01 5.0194457e-02 3.2632763e-03 8.2604371e-02 8.7288183e-01
 3.9048979e-04 9.8327178e-01 1.4733879e-03 9.0266651e-01 6.1760330e-01
 1.2700453e-01 1.8206073e-01 7.8917138e-02 7.3825133e-01 9.2675436e-01
 7.3454201e-01 8.3810782e-01 1.5483870e-01 2.2479277e-02 2.2326874e-02
 3.9234575e-02 8.5219368e-02 9.4866725e-03 3.7434641e-02 9.5133895e-01
 8.3503211e-01 1.8976137e-02 8.2114087e-03 9.2066836e-01 9.2112517e-01
 9.6316373e-01 6.5604396e-02 9.9905998e-01 1.3572616e-02 3.7828196e-02
 9.6010166e-01 3.6747476e-01 9.6551664e-02 1.5405997e-02 2.0082038e-02
 1.1639745e-02 9.5187050e-01 4.9568254e-01 3.2687388e-02 1.6322264e-01
 3.0556065e-04 9.9819857e-01 2.8093717e-01 1.8158244e-01 3.0791797e-04
 4.5415530e-01 4.7919094e-03 9.9804175e-01 9.3023721e-03 1.3741235e-01
 9.9933940e-01 4.2450163e-03 9.5721674e-01 9.0565085e-01 9.9857354e-01
 9.0477779e-04 9.9720520e-01 9.3597844e-03 9.9886191e-01 9.9630368e-01
 4.4827339e-01 1.9085631e-01 3.8539851e-01 1.4197357e-02 9.9818784e-01
 9.9718446e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-31 22:16:14, Dev, Step : 2952, Loss : 0.91416, Acc : 0.725, Auc : 0.796, Sensitive_Loss : 0.25336, Sensitive_Acc : 16.851, Sensitive_Auc : 0.998, Mean auc: 0.796, Run Time : 93.94 sec
INFO:root:2024-03-31 22:16:21, Train, Epoch : 7, Step : 2960, Loss : 0.61043, Acc : 0.575, Sensitive_Loss : 0.15964, Sensitive_Acc : 13.200, Run Time : 6.54 sec
INFO:root:2024-03-31 22:16:29, Train, Epoch : 7, Step : 2970, Loss : 0.61899, Acc : 0.791, Sensitive_Loss : 0.24315, Sensitive_Acc : 17.500, Run Time : 7.54 sec
INFO:root:2024-03-31 22:16:36, Train, Epoch : 7, Step : 2980, Loss : 0.78837, Acc : 0.744, Sensitive_Loss : 0.28382, Sensitive_Acc : 19.500, Run Time : 7.03 sec
INFO:root:2024-03-31 22:16:43, Train, Epoch : 7, Step : 2990, Loss : 0.62530, Acc : 0.722, Sensitive_Loss : 0.24481, Sensitive_Acc : 16.300, Run Time : 7.40 sec
INFO:root:2024-03-31 22:16:50, Train, Epoch : 7, Step : 3000, Loss : 0.73013, Acc : 0.756, Sensitive_Loss : 0.18408, Sensitive_Acc : 16.900, Run Time : 7.15 sec
INFO:root:2024-03-31 22:18:26, Dev, Step : 3000, Loss : 0.92247, Acc : 0.766, Auc : 0.791, Sensitive_Loss : 0.25035, Sensitive_Acc : 16.851, Sensitive_Auc : 0.997, Mean auc: 0.791, Run Time : 95.05 sec
INFO:root:2024-03-31 22:18:31, Train, Epoch : 7, Step : 3010, Loss : 0.84599, Acc : 0.706, Sensitive_Loss : 0.23838, Sensitive_Acc : 15.900, Run Time : 100.55 sec
INFO:root:2024-03-31 22:18:38, Train, Epoch : 7, Step : 3020, Loss : 0.87582, Acc : 0.697, Sensitive_Loss : 0.21957, Sensitive_Acc : 15.900, Run Time : 7.35 sec
INFO:root:2024-03-31 22:18:46, Train, Epoch : 7, Step : 3030, Loss : 0.80776, Acc : 0.753, Sensitive_Loss : 0.23044, Sensitive_Acc : 14.600, Run Time : 7.40 sec
INFO:root:2024-03-31 22:18:53, Train, Epoch : 7, Step : 3040, Loss : 0.59344, Acc : 0.716, Sensitive_Loss : 0.22558, Sensitive_Acc : 18.500, Run Time : 7.25 sec
INFO:root:2024-03-31 22:19:00, Train, Epoch : 7, Step : 3050, Loss : 0.85687, Acc : 0.716, Sensitive_Loss : 0.20393, Sensitive_Acc : 17.300, Run Time : 7.04 sec
INFO:root:2024-03-31 22:19:07, Train, Epoch : 7, Step : 3060, Loss : 0.61870, Acc : 0.766, Sensitive_Loss : 0.23640, Sensitive_Acc : 18.600, Run Time : 6.74 sec
INFO:root:2024-03-31 22:19:14, Train, Epoch : 7, Step : 3070, Loss : 0.73699, Acc : 0.784, Sensitive_Loss : 0.18419, Sensitive_Acc : 17.300, Run Time : 7.65 sec
INFO:root:2024-03-31 22:19:22, Train, Epoch : 7, Step : 3080, Loss : 0.62834, Acc : 0.728, Sensitive_Loss : 0.21502, Sensitive_Acc : 14.800, Run Time : 7.36 sec
INFO:root:2024-03-31 22:19:29, Train, Epoch : 7, Step : 3090, Loss : 0.83713, Acc : 0.725, Sensitive_Loss : 0.28802, Sensitive_Acc : 17.800, Run Time : 6.90 sec
INFO:root:2024-03-31 22:19:36, Train, Epoch : 7, Step : 3100, Loss : 0.73088, Acc : 0.756, Sensitive_Loss : 0.30847, Sensitive_Acc : 16.400, Run Time : 7.55 sec
INFO:root:2024-03-31 22:21:10, Dev, Step : 3100, Loss : 0.92005, Acc : 0.764, Auc : 0.791, Sensitive_Loss : 0.24091, Sensitive_Acc : 16.894, Sensitive_Auc : 0.997, Mean auc: 0.791, Run Time : 94.11 sec
INFO:root:2024-03-31 22:21:16, Train, Epoch : 7, Step : 3110, Loss : 0.69186, Acc : 0.772, Sensitive_Loss : 0.20897, Sensitive_Acc : 16.500, Run Time : 99.77 sec
INFO:root:2024-03-31 22:21:23, Train, Epoch : 7, Step : 3120, Loss : 0.76171, Acc : 0.747, Sensitive_Loss : 0.21654, Sensitive_Acc : 16.800, Run Time : 6.95 sec
INFO:root:2024-03-31 22:21:30, Train, Epoch : 7, Step : 3130, Loss : 0.93885, Acc : 0.753, Sensitive_Loss : 0.21492, Sensitive_Acc : 16.300, Run Time : 7.04 sec
INFO:root:2024-03-31 22:21:37, Train, Epoch : 7, Step : 3140, Loss : 0.64488, Acc : 0.713, Sensitive_Loss : 0.21703, Sensitive_Acc : 17.600, Run Time : 7.26 sec
INFO:root:2024-03-31 22:21:45, Train, Epoch : 7, Step : 3150, Loss : 0.73176, Acc : 0.769, Sensitive_Loss : 0.21721, Sensitive_Acc : 16.300, Run Time : 7.38 sec
INFO:root:2024-03-31 22:21:52, Train, Epoch : 7, Step : 3160, Loss : 0.81599, Acc : 0.747, Sensitive_Loss : 0.25460, Sensitive_Acc : 15.200, Run Time : 7.00 sec
INFO:root:2024-03-31 22:21:59, Train, Epoch : 7, Step : 3170, Loss : 0.84596, Acc : 0.728, Sensitive_Loss : 0.21074, Sensitive_Acc : 16.200, Run Time : 7.36 sec
INFO:root:2024-03-31 22:22:07, Train, Epoch : 7, Step : 3180, Loss : 0.70484, Acc : 0.741, Sensitive_Loss : 0.20906, Sensitive_Acc : 18.300, Run Time : 7.63 sec
INFO:root:2024-03-31 22:22:14, Train, Epoch : 7, Step : 3190, Loss : 0.86378, Acc : 0.731, Sensitive_Loss : 0.23666, Sensitive_Acc : 16.800, Run Time : 7.07 sec
INFO:root:2024-03-31 22:22:20, Train, Epoch : 7, Step : 3200, Loss : 0.59955, Acc : 0.738, Sensitive_Loss : 0.23607, Sensitive_Acc : 16.600, Run Time : 6.67 sec
INFO:root:2024-03-31 22:23:55, Dev, Step : 3200, Loss : 0.94117, Acc : 0.756, Auc : 0.782, Sensitive_Loss : 0.26059, Sensitive_Acc : 16.738, Sensitive_Auc : 0.997, Mean auc: 0.782, Run Time : 94.71 sec
INFO:root:2024-03-31 22:24:01, Train, Epoch : 7, Step : 3210, Loss : 0.74312, Acc : 0.706, Sensitive_Loss : 0.18804, Sensitive_Acc : 17.000, Run Time : 100.28 sec
INFO:root:2024-03-31 22:24:08, Train, Epoch : 7, Step : 3220, Loss : 0.67279, Acc : 0.734, Sensitive_Loss : 0.22075, Sensitive_Acc : 18.000, Run Time : 7.04 sec
INFO:root:2024-03-31 22:24:15, Train, Epoch : 7, Step : 3230, Loss : 0.80140, Acc : 0.753, Sensitive_Loss : 0.18545, Sensitive_Acc : 17.400, Run Time : 7.40 sec
INFO:root:2024-03-31 22:24:23, Train, Epoch : 7, Step : 3240, Loss : 0.70639, Acc : 0.747, Sensitive_Loss : 0.16925, Sensitive_Acc : 17.300, Run Time : 7.66 sec
INFO:root:2024-03-31 22:24:30, Train, Epoch : 7, Step : 3250, Loss : 0.74045, Acc : 0.716, Sensitive_Loss : 0.24504, Sensitive_Acc : 17.000, Run Time : 7.02 sec
INFO:root:2024-03-31 22:24:37, Train, Epoch : 7, Step : 3260, Loss : 0.71978, Acc : 0.750, Sensitive_Loss : 0.22501, Sensitive_Acc : 15.500, Run Time : 7.52 sec
INFO:root:2024-03-31 22:24:45, Train, Epoch : 7, Step : 3270, Loss : 0.79802, Acc : 0.706, Sensitive_Loss : 0.22099, Sensitive_Acc : 15.600, Run Time : 7.39 sec
INFO:root:2024-03-31 22:24:52, Train, Epoch : 7, Step : 3280, Loss : 0.70509, Acc : 0.719, Sensitive_Loss : 0.18742, Sensitive_Acc : 17.100, Run Time : 6.99 sec
INFO:root:2024-03-31 22:24:59, Train, Epoch : 7, Step : 3290, Loss : 0.70692, Acc : 0.766, Sensitive_Loss : 0.17773, Sensitive_Acc : 16.800, Run Time : 7.32 sec
INFO:root:2024-03-31 22:25:06, Train, Epoch : 7, Step : 3300, Loss : 0.80326, Acc : 0.722, Sensitive_Loss : 0.16286, Sensitive_Acc : 17.100, Run Time : 6.91 sec
INFO:root:2024-03-31 22:26:41, Dev, Step : 3300, Loss : 0.94597, Acc : 0.697, Auc : 0.784, Sensitive_Loss : 0.24172, Sensitive_Acc : 16.865, Sensitive_Auc : 0.998, Mean auc: 0.784, Run Time : 95.27 sec
INFO:root:2024-03-31 22:26:47, Train, Epoch : 7, Step : 3310, Loss : 0.76970, Acc : 0.744, Sensitive_Loss : 0.21561, Sensitive_Acc : 15.000, Run Time : 100.74 sec
INFO:root:2024-03-31 22:26:54, Train, Epoch : 7, Step : 3320, Loss : 0.76216, Acc : 0.787, Sensitive_Loss : 0.19111, Sensitive_Acc : 18.900, Run Time : 7.46 sec
INFO:root:2024-03-31 22:27:01, Train, Epoch : 7, Step : 3330, Loss : 0.66386, Acc : 0.716, Sensitive_Loss : 0.26116, Sensitive_Acc : 16.900, Run Time : 6.57 sec
INFO:root:2024-03-31 22:27:08, Train, Epoch : 7, Step : 3340, Loss : 0.66314, Acc : 0.719, Sensitive_Loss : 0.21215, Sensitive_Acc : 16.700, Run Time : 7.59 sec
INFO:root:2024-03-31 22:27:15, Train, Epoch : 7, Step : 3350, Loss : 0.74262, Acc : 0.762, Sensitive_Loss : 0.23711, Sensitive_Acc : 18.900, Run Time : 7.00 sec
INFO:root:2024-03-31 22:27:23, Train, Epoch : 7, Step : 3360, Loss : 0.53727, Acc : 0.738, Sensitive_Loss : 0.22440, Sensitive_Acc : 17.900, Run Time : 7.34 sec
INFO:root:2024-03-31 22:27:30, Train, Epoch : 7, Step : 3370, Loss : 0.83795, Acc : 0.741, Sensitive_Loss : 0.19334, Sensitive_Acc : 14.800, Run Time : 7.30 sec
INFO:root:2024-03-31 22:27:37, Train, Epoch : 7, Step : 3380, Loss : 0.75854, Acc : 0.747, Sensitive_Loss : 0.16267, Sensitive_Acc : 16.700, Run Time : 6.99 sec
INFO:root:2024-03-31 22:27:44, Train, Epoch : 7, Step : 3390, Loss : 0.76715, Acc : 0.741, Sensitive_Loss : 0.22318, Sensitive_Acc : 16.100, Run Time : 7.25 sec
INFO:root:2024-03-31 22:27:51, Train, Epoch : 7, Step : 3400, Loss : 0.66781, Acc : 0.769, Sensitive_Loss : 0.24749, Sensitive_Acc : 17.000, Run Time : 7.22 sec
INFO:root:2024-03-31 22:29:26, Dev, Step : 3400, Loss : 0.94313, Acc : 0.733, Auc : 0.786, Sensitive_Loss : 0.26555, Sensitive_Acc : 16.652, Sensitive_Auc : 0.999, Mean auc: 0.786, Run Time : 95.09 sec
INFO:root:2024-03-31 22:29:32, Train, Epoch : 7, Step : 3410, Loss : 0.67113, Acc : 0.731, Sensitive_Loss : 0.13395, Sensitive_Acc : 18.000, Run Time : 100.40 sec
INFO:root:2024-03-31 22:29:39, Train, Epoch : 7, Step : 3420, Loss : 0.57997, Acc : 0.703, Sensitive_Loss : 0.19571, Sensitive_Acc : 15.900, Run Time : 7.44 sec
INFO:root:2024-03-31 22:29:47, Train, Epoch : 7, Step : 3430, Loss : 0.95325, Acc : 0.756, Sensitive_Loss : 0.18906, Sensitive_Acc : 16.400, Run Time : 7.38 sec
INFO:root:2024-03-31 22:29:54, Train, Epoch : 7, Step : 3440, Loss : 0.67596, Acc : 0.731, Sensitive_Loss : 0.21175, Sensitive_Acc : 18.500, Run Time : 7.20 sec
INFO:root:2024-03-31 22:31:31
INFO:root:y_pred: [0.32395118 0.13036312 0.24017501 ... 0.45500872 0.39129373 0.09127451]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [3.54960025e-03 7.93592036e-02 9.91740704e-01 1.63000807e-01
 9.99585330e-01 1.08433841e-02 1.24581672e-01 1.28483981e-01
 1.25162154e-02 3.08551639e-01 1.85782928e-02 2.82303523e-02
 9.91152227e-01 1.75675154e-02 9.93784189e-01 9.54255819e-01
 2.73374352e-03 6.15560532e-01 9.55345809e-01 8.61980736e-01
 1.17780551e-01 1.27570853e-01 9.99766290e-01 9.92884815e-01
 7.97412172e-02 7.15827346e-01 1.12106344e-02 9.97848153e-01
 1.95863862e-02 9.60205853e-01 9.99769390e-01 9.47596788e-01
 8.01790413e-03 6.29074348e-05 1.43520311e-01 2.54761986e-02
 5.46074621e-02 4.62234199e-01 9.94100392e-01 9.91774023e-01
 2.48358166e-03 7.80753279e-03 9.89891410e-01 9.99676585e-01
 8.41144547e-02 6.43436164e-02 1.15499273e-02 8.25213045e-02
 9.99148011e-01 9.72924888e-01 9.95302200e-01 7.95083404e-01
 9.98029768e-01 9.59327161e-01 9.97577131e-01 9.36432064e-01
 9.61236060e-01 1.60087645e-02 9.99559343e-01 1.49269536e-01
 3.52926948e-03 7.20646977e-01 9.50484037e-01 9.97076154e-01
 9.98592913e-01 9.97850895e-01 9.03914750e-01 9.93130624e-01
 9.91816580e-01 9.82794940e-01 9.82276499e-01 8.62027109e-02
 4.34379047e-03 1.45792618e-01 9.36659992e-01 1.15051022e-04
 9.91129518e-01 9.23787709e-04 9.19825494e-01 6.78460777e-01
 1.22545086e-01 2.02084884e-01 7.59385601e-02 8.24209332e-01
 9.45905209e-01 7.89358199e-01 8.99138391e-01 3.04197043e-01
 3.61993797e-02 7.29764625e-02 5.54921366e-02 1.23693615e-01
 1.14981029e-02 7.49584362e-02 9.70630169e-01 9.44431722e-01
 3.57112959e-02 8.49911850e-03 9.62872207e-01 9.39716518e-01
 9.68992651e-01 1.46886945e-01 9.99520540e-01 1.71748176e-02
 4.91289273e-02 9.75415468e-01 3.16934258e-01 7.00059235e-02
 2.39058807e-02 3.89176346e-02 3.17708030e-02 9.83510137e-01
 5.80534220e-01 4.49075140e-02 2.70715088e-01 1.85796540e-04
 9.98579979e-01 5.33538640e-01 3.82452875e-01 2.92466342e-04
 5.46997190e-01 5.58866886e-03 9.99323964e-01 7.72618828e-03
 1.28801882e-01 9.99617100e-01 4.20204038e-03 9.84407425e-01
 9.40322757e-01 9.99311686e-01 7.43003679e-04 9.98513520e-01
 4.78565553e-03 9.99446929e-01 9.99014974e-01 6.60533607e-01
 3.36992204e-01 3.80256623e-01 3.44407335e-02 9.98100936e-01
 9.98304129e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-31 22:31:31, Dev, Step : 3444, Loss : 0.95950, Acc : 0.687, Auc : 0.783, Sensitive_Loss : 0.28560, Sensitive_Acc : 16.610, Sensitive_Auc : 0.999, Mean auc: 0.783, Run Time : 94.41 sec
INFO:root:2024-03-31 22:31:38, Train, Epoch : 8, Step : 3450, Loss : 0.48433, Acc : 0.444, Sensitive_Loss : 0.15816, Sensitive_Acc : 8.400, Run Time : 5.36 sec
INFO:root:2024-03-31 22:31:45, Train, Epoch : 8, Step : 3460, Loss : 0.68976, Acc : 0.744, Sensitive_Loss : 0.17228, Sensitive_Acc : 16.700, Run Time : 7.31 sec
INFO:root:2024-03-31 22:31:53, Train, Epoch : 8, Step : 3470, Loss : 0.99396, Acc : 0.713, Sensitive_Loss : 0.22344, Sensitive_Acc : 16.700, Run Time : 7.71 sec
INFO:root:2024-03-31 22:32:00, Train, Epoch : 8, Step : 3480, Loss : 0.71115, Acc : 0.759, Sensitive_Loss : 0.24399, Sensitive_Acc : 16.200, Run Time : 7.14 sec
INFO:root:2024-03-31 22:32:06, Train, Epoch : 8, Step : 3490, Loss : 0.60413, Acc : 0.772, Sensitive_Loss : 0.19193, Sensitive_Acc : 16.600, Run Time : 6.64 sec
INFO:root:2024-03-31 22:32:14, Train, Epoch : 8, Step : 3500, Loss : 0.58977, Acc : 0.747, Sensitive_Loss : 0.23079, Sensitive_Acc : 17.800, Run Time : 7.24 sec
INFO:root:2024-03-31 22:33:49, Dev, Step : 3500, Loss : 0.93786, Acc : 0.735, Auc : 0.790, Sensitive_Loss : 0.23966, Sensitive_Acc : 16.851, Sensitive_Auc : 1.000, Mean auc: 0.790, Run Time : 95.75 sec
INFO:root:2024-03-31 22:33:55, Train, Epoch : 8, Step : 3510, Loss : 0.59390, Acc : 0.738, Sensitive_Loss : 0.26084, Sensitive_Acc : 16.800, Run Time : 101.25 sec
INFO:root:2024-03-31 22:34:02, Train, Epoch : 8, Step : 3520, Loss : 0.71892, Acc : 0.731, Sensitive_Loss : 0.19591, Sensitive_Acc : 17.700, Run Time : 7.09 sec
INFO:root:2024-03-31 22:34:10, Train, Epoch : 8, Step : 3530, Loss : 0.79376, Acc : 0.744, Sensitive_Loss : 0.23313, Sensitive_Acc : 17.800, Run Time : 7.64 sec
INFO:root:2024-03-31 22:34:17, Train, Epoch : 8, Step : 3540, Loss : 0.74456, Acc : 0.766, Sensitive_Loss : 0.20120, Sensitive_Acc : 15.100, Run Time : 7.01 sec
INFO:root:2024-03-31 22:34:24, Train, Epoch : 8, Step : 3550, Loss : 0.68112, Acc : 0.791, Sensitive_Loss : 0.23502, Sensitive_Acc : 16.200, Run Time : 7.08 sec
INFO:root:2024-03-31 22:34:31, Train, Epoch : 8, Step : 3560, Loss : 0.70390, Acc : 0.759, Sensitive_Loss : 0.22562, Sensitive_Acc : 16.500, Run Time : 7.05 sec
INFO:root:2024-03-31 22:34:38, Train, Epoch : 8, Step : 3570, Loss : 0.71298, Acc : 0.747, Sensitive_Loss : 0.22663, Sensitive_Acc : 18.000, Run Time : 7.39 sec
INFO:root:2024-03-31 22:34:46, Train, Epoch : 8, Step : 3580, Loss : 0.67200, Acc : 0.841, Sensitive_Loss : 0.19487, Sensitive_Acc : 14.400, Run Time : 7.41 sec
INFO:root:2024-03-31 22:34:53, Train, Epoch : 8, Step : 3590, Loss : 0.67954, Acc : 0.769, Sensitive_Loss : 0.21943, Sensitive_Acc : 19.300, Run Time : 7.20 sec
INFO:root:2024-03-31 22:35:00, Train, Epoch : 8, Step : 3600, Loss : 0.74125, Acc : 0.778, Sensitive_Loss : 0.18549, Sensitive_Acc : 16.300, Run Time : 7.28 sec
INFO:root:2024-03-31 22:36:36, Dev, Step : 3600, Loss : 0.93315, Acc : 0.726, Auc : 0.794, Sensitive_Loss : 0.24436, Sensitive_Acc : 16.794, Sensitive_Auc : 1.000, Mean auc: 0.794, Run Time : 95.55 sec
INFO:root:2024-03-31 22:36:41, Train, Epoch : 8, Step : 3610, Loss : 0.65006, Acc : 0.753, Sensitive_Loss : 0.18446, Sensitive_Acc : 15.900, Run Time : 101.17 sec
INFO:root:2024-03-31 22:36:48, Train, Epoch : 8, Step : 3620, Loss : 0.66084, Acc : 0.716, Sensitive_Loss : 0.17379, Sensitive_Acc : 17.800, Run Time : 6.99 sec
INFO:root:2024-03-31 22:36:56, Train, Epoch : 8, Step : 3630, Loss : 0.65592, Acc : 0.750, Sensitive_Loss : 0.23868, Sensitive_Acc : 15.900, Run Time : 7.59 sec
INFO:root:2024-03-31 22:37:03, Train, Epoch : 8, Step : 3640, Loss : 0.49864, Acc : 0.753, Sensitive_Loss : 0.20808, Sensitive_Acc : 17.600, Run Time : 7.59 sec
INFO:root:2024-03-31 22:37:10, Train, Epoch : 8, Step : 3650, Loss : 0.70213, Acc : 0.759, Sensitive_Loss : 0.21881, Sensitive_Acc : 16.800, Run Time : 6.85 sec
INFO:root:2024-03-31 22:37:18, Train, Epoch : 8, Step : 3660, Loss : 0.64419, Acc : 0.809, Sensitive_Loss : 0.21309, Sensitive_Acc : 15.600, Run Time : 7.73 sec
INFO:root:2024-03-31 22:37:25, Train, Epoch : 8, Step : 3670, Loss : 0.75779, Acc : 0.750, Sensitive_Loss : 0.23771, Sensitive_Acc : 16.100, Run Time : 6.86 sec
INFO:root:2024-03-31 22:37:32, Train, Epoch : 8, Step : 3680, Loss : 0.68845, Acc : 0.741, Sensitive_Loss : 0.16487, Sensitive_Acc : 16.600, Run Time : 6.92 sec
INFO:root:2024-03-31 22:37:39, Train, Epoch : 8, Step : 3690, Loss : 0.66914, Acc : 0.738, Sensitive_Loss : 0.20084, Sensitive_Acc : 17.300, Run Time : 7.07 sec
INFO:root:2024-03-31 22:37:46, Train, Epoch : 8, Step : 3700, Loss : 0.69133, Acc : 0.753, Sensitive_Loss : 0.20644, Sensitive_Acc : 17.300, Run Time : 7.13 sec
INFO:root:2024-03-31 22:39:21, Dev, Step : 3700, Loss : 0.95027, Acc : 0.780, Auc : 0.785, Sensitive_Loss : 0.24652, Sensitive_Acc : 16.752, Sensitive_Auc : 1.000, Mean auc: 0.785, Run Time : 95.09 sec
INFO:root:2024-03-31 22:39:27, Train, Epoch : 8, Step : 3710, Loss : 0.72458, Acc : 0.772, Sensitive_Loss : 0.22653, Sensitive_Acc : 16.200, Run Time : 100.78 sec
INFO:root:2024-03-31 22:39:34, Train, Epoch : 8, Step : 3720, Loss : 0.54546, Acc : 0.794, Sensitive_Loss : 0.21678, Sensitive_Acc : 16.700, Run Time : 7.05 sec
INFO:root:2024-03-31 22:39:41, Train, Epoch : 8, Step : 3730, Loss : 0.70418, Acc : 0.766, Sensitive_Loss : 0.21064, Sensitive_Acc : 17.400, Run Time : 7.49 sec
INFO:root:2024-03-31 22:39:48, Train, Epoch : 8, Step : 3740, Loss : 0.72818, Acc : 0.756, Sensitive_Loss : 0.22966, Sensitive_Acc : 16.600, Run Time : 6.85 sec
INFO:root:2024-03-31 22:39:55, Train, Epoch : 8, Step : 3750, Loss : 0.70922, Acc : 0.762, Sensitive_Loss : 0.25675, Sensitive_Acc : 17.100, Run Time : 7.20 sec
INFO:root:2024-03-31 22:40:03, Train, Epoch : 8, Step : 3760, Loss : 0.72579, Acc : 0.756, Sensitive_Loss : 0.19638, Sensitive_Acc : 15.300, Run Time : 7.47 sec
INFO:root:2024-03-31 22:40:09, Train, Epoch : 8, Step : 3770, Loss : 0.73178, Acc : 0.800, Sensitive_Loss : 0.25519, Sensitive_Acc : 18.100, Run Time : 6.46 sec
INFO:root:2024-03-31 22:40:17, Train, Epoch : 8, Step : 3780, Loss : 0.77839, Acc : 0.734, Sensitive_Loss : 0.18916, Sensitive_Acc : 16.900, Run Time : 7.42 sec
INFO:root:2024-03-31 22:40:24, Train, Epoch : 8, Step : 3790, Loss : 0.66966, Acc : 0.766, Sensitive_Loss : 0.18540, Sensitive_Acc : 16.600, Run Time : 7.43 sec
INFO:root:2024-03-31 22:40:31, Train, Epoch : 8, Step : 3800, Loss : 0.58103, Acc : 0.759, Sensitive_Loss : 0.14997, Sensitive_Acc : 15.400, Run Time : 6.97 sec
INFO:root:2024-03-31 22:42:05, Dev, Step : 3800, Loss : 0.93802, Acc : 0.757, Auc : 0.785, Sensitive_Loss : 0.24874, Sensitive_Acc : 16.851, Sensitive_Auc : 0.999, Mean auc: 0.785, Run Time : 94.23 sec
INFO:root:2024-03-31 22:42:12, Train, Epoch : 8, Step : 3810, Loss : 0.66616, Acc : 0.725, Sensitive_Loss : 0.25106, Sensitive_Acc : 18.200, Run Time : 100.53 sec
INFO:root:2024-03-31 22:42:18, Train, Epoch : 8, Step : 3820, Loss : 0.71366, Acc : 0.784, Sensitive_Loss : 0.18735, Sensitive_Acc : 16.600, Run Time : 6.41 sec
INFO:root:2024-03-31 22:42:25, Train, Epoch : 8, Step : 3830, Loss : 0.75731, Acc : 0.775, Sensitive_Loss : 0.19799, Sensitive_Acc : 14.700, Run Time : 7.50 sec
INFO:root:2024-03-31 22:42:33, Train, Epoch : 8, Step : 3840, Loss : 0.64839, Acc : 0.731, Sensitive_Loss : 0.19843, Sensitive_Acc : 15.900, Run Time : 7.69 sec
INFO:root:2024-03-31 22:42:40, Train, Epoch : 8, Step : 3850, Loss : 0.72597, Acc : 0.747, Sensitive_Loss : 0.16628, Sensitive_Acc : 17.300, Run Time : 6.49 sec
INFO:root:2024-03-31 22:42:47, Train, Epoch : 8, Step : 3860, Loss : 0.88571, Acc : 0.772, Sensitive_Loss : 0.15107, Sensitive_Acc : 16.600, Run Time : 7.67 sec
INFO:root:2024-03-31 22:42:55, Train, Epoch : 8, Step : 3870, Loss : 0.57435, Acc : 0.753, Sensitive_Loss : 0.19455, Sensitive_Acc : 15.900, Run Time : 7.23 sec
INFO:root:2024-03-31 22:43:02, Train, Epoch : 8, Step : 3880, Loss : 0.74882, Acc : 0.750, Sensitive_Loss : 0.20101, Sensitive_Acc : 15.800, Run Time : 7.08 sec
INFO:root:2024-03-31 22:43:09, Train, Epoch : 8, Step : 3890, Loss : 0.72809, Acc : 0.747, Sensitive_Loss : 0.21509, Sensitive_Acc : 15.600, Run Time : 6.93 sec
INFO:root:2024-03-31 22:43:16, Train, Epoch : 8, Step : 3900, Loss : 0.59576, Acc : 0.791, Sensitive_Loss : 0.20794, Sensitive_Acc : 15.900, Run Time : 7.73 sec
INFO:root:2024-03-31 22:44:51, Dev, Step : 3900, Loss : 0.92078, Acc : 0.746, Auc : 0.793, Sensitive_Loss : 0.25401, Sensitive_Acc : 16.681, Sensitive_Auc : 0.999, Mean auc: 0.793, Run Time : 94.32 sec
INFO:root:2024-03-31 22:44:56, Train, Epoch : 8, Step : 3910, Loss : 0.81978, Acc : 0.762, Sensitive_Loss : 0.14125, Sensitive_Acc : 16.900, Run Time : 99.69 sec
INFO:root:2024-03-31 22:45:03, Train, Epoch : 8, Step : 3920, Loss : 0.70156, Acc : 0.784, Sensitive_Loss : 0.20271, Sensitive_Acc : 17.900, Run Time : 7.33 sec
INFO:root:2024-03-31 22:45:10, Train, Epoch : 8, Step : 3930, Loss : 0.86848, Acc : 0.741, Sensitive_Loss : 0.21732, Sensitive_Acc : 16.600, Run Time : 6.96 sec
INFO:root:2024-03-31 22:46:49
INFO:root:y_pred: [0.2356784  0.07593582 0.18471675 ... 0.24364388 0.18695945 0.0557354 ]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [1.6190360e-03 1.4920801e-01 9.9152243e-01 1.4337197e-01 9.9974185e-01
 8.3965370e-03 9.5124610e-02 7.9244815e-02 6.7815152e-03 3.8025761e-01
 1.1787359e-02 1.5542078e-02 9.8785269e-01 8.0183782e-03 9.9350882e-01
 9.3900007e-01 2.0034753e-03 6.0775530e-01 9.4479746e-01 8.0218285e-01
 4.8023015e-02 1.1730043e-01 9.9976760e-01 9.9334598e-01 3.3407126e-02
 4.8018548e-01 4.6994938e-03 9.9807537e-01 9.0976795e-03 9.6797222e-01
 9.9980766e-01 9.6248263e-01 6.2411171e-03 4.2603562e-05 2.5166631e-01
 1.0659292e-02 4.7014050e-02 4.9209929e-01 9.9347621e-01 9.8687577e-01
 9.9745789e-04 2.5480264e-03 9.8797441e-01 9.9964726e-01 1.2512781e-01
 8.2589820e-02 1.6905947e-02 1.7435466e-01 9.9893290e-01 9.6758872e-01
 9.9236804e-01 6.6840243e-01 9.9869531e-01 9.8120904e-01 9.9779594e-01
 9.3154788e-01 9.5813137e-01 1.3890375e-02 9.9950504e-01 1.6747199e-01
 3.3525703e-03 7.3507679e-01 9.2685974e-01 9.9666280e-01 9.9879491e-01
 9.9737334e-01 9.1134804e-01 9.9616867e-01 9.9327624e-01 9.7379905e-01
 9.8642272e-01 4.7600519e-02 3.7681870e-03 1.1866598e-01 9.2524683e-01
 6.5084932e-05 9.9280775e-01 2.5431541e-04 9.5241773e-01 6.3041353e-01
 1.0003697e-01 1.3675772e-01 5.6444816e-02 6.7570573e-01 9.3214303e-01
 7.2880650e-01 8.6847311e-01 2.5005993e-01 2.9040758e-02 3.7913240e-02
 7.8345604e-02 1.0079562e-01 8.2803844e-03 6.8335682e-02 9.7507942e-01
 8.8273364e-01 5.8252882e-02 4.1234121e-03 9.2920673e-01 9.0869117e-01
 9.4382083e-01 7.6097466e-02 9.9943370e-01 9.1153169e-03 5.2884795e-02
 9.6593428e-01 1.7874701e-01 6.1644897e-02 9.5385090e-03 2.9244652e-02
 1.5517286e-02 9.8543847e-01 5.2954012e-01 6.5653287e-02 1.7220932e-01
 5.7768375e-05 9.9856842e-01 2.4838585e-01 3.3398783e-01 2.7481967e-04
 5.8832234e-01 6.6158413e-03 9.9930823e-01 3.0162712e-03 1.7360424e-01
 9.9914598e-01 4.9384818e-03 9.8755926e-01 9.1428691e-01 9.9940038e-01
 2.1078555e-04 9.9912423e-01 6.4366478e-03 9.9950027e-01 9.9940562e-01
 5.4405200e-01 2.4299766e-01 5.2896392e-01 3.5729382e-02 9.9794084e-01
 9.9756831e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-31 22:46:49, Dev, Step : 3936, Loss : 0.96162, Acc : 0.771, Auc : 0.777, Sensitive_Loss : 0.26240, Sensitive_Acc : 16.652, Sensitive_Auc : 0.999, Mean auc: 0.777, Run Time : 94.57 sec
INFO:root:2024-03-31 22:46:55, Train, Epoch : 9, Step : 3940, Loss : 0.31145, Acc : 0.312, Sensitive_Loss : 0.07844, Sensitive_Acc : 5.800, Run Time : 4.07 sec
INFO:root:2024-03-31 22:47:02, Train, Epoch : 9, Step : 3950, Loss : 0.57570, Acc : 0.787, Sensitive_Loss : 0.24186, Sensitive_Acc : 16.700, Run Time : 6.95 sec
INFO:root:2024-03-31 22:47:09, Train, Epoch : 9, Step : 3960, Loss : 0.75890, Acc : 0.759, Sensitive_Loss : 0.21696, Sensitive_Acc : 18.900, Run Time : 7.15 sec
INFO:root:2024-03-31 22:47:16, Train, Epoch : 9, Step : 3970, Loss : 0.60377, Acc : 0.806, Sensitive_Loss : 0.19914, Sensitive_Acc : 14.900, Run Time : 7.59 sec
INFO:root:2024-03-31 22:47:23, Train, Epoch : 9, Step : 3980, Loss : 0.47603, Acc : 0.772, Sensitive_Loss : 0.19836, Sensitive_Acc : 17.300, Run Time : 7.10 sec
INFO:root:2024-03-31 22:47:31, Train, Epoch : 9, Step : 3990, Loss : 0.78064, Acc : 0.756, Sensitive_Loss : 0.17171, Sensitive_Acc : 16.900, Run Time : 7.46 sec
INFO:root:2024-03-31 22:47:38, Train, Epoch : 9, Step : 4000, Loss : 0.83097, Acc : 0.759, Sensitive_Loss : 0.31052, Sensitive_Acc : 17.400, Run Time : 7.56 sec
INFO:root:2024-03-31 22:49:14, Dev, Step : 4000, Loss : 0.95704, Acc : 0.749, Auc : 0.778, Sensitive_Loss : 0.23367, Sensitive_Acc : 16.851, Sensitive_Auc : 1.000, Mean auc: 0.778, Run Time : 95.38 sec
INFO:root:2024-03-31 22:49:19, Train, Epoch : 9, Step : 4010, Loss : 0.63355, Acc : 0.787, Sensitive_Loss : 0.20618, Sensitive_Acc : 17.000, Run Time : 101.02 sec
INFO:root:2024-03-31 22:49:27, Train, Epoch : 9, Step : 4020, Loss : 0.57487, Acc : 0.797, Sensitive_Loss : 0.21128, Sensitive_Acc : 16.700, Run Time : 7.15 sec
INFO:root:2024-03-31 22:49:34, Train, Epoch : 9, Step : 4030, Loss : 0.51492, Acc : 0.762, Sensitive_Loss : 0.17332, Sensitive_Acc : 14.700, Run Time : 7.21 sec
INFO:root:2024-03-31 22:49:41, Train, Epoch : 9, Step : 4040, Loss : 0.71486, Acc : 0.787, Sensitive_Loss : 0.21817, Sensitive_Acc : 18.100, Run Time : 7.05 sec
INFO:root:2024-03-31 22:49:48, Train, Epoch : 9, Step : 4050, Loss : 0.67623, Acc : 0.762, Sensitive_Loss : 0.22578, Sensitive_Acc : 18.600, Run Time : 7.23 sec
INFO:root:2024-03-31 22:49:56, Train, Epoch : 9, Step : 4060, Loss : 0.64105, Acc : 0.766, Sensitive_Loss : 0.20980, Sensitive_Acc : 17.500, Run Time : 7.61 sec
INFO:root:2024-03-31 22:50:03, Train, Epoch : 9, Step : 4070, Loss : 0.71324, Acc : 0.762, Sensitive_Loss : 0.19388, Sensitive_Acc : 16.400, Run Time : 7.28 sec
INFO:root:2024-03-31 22:50:10, Train, Epoch : 9, Step : 4080, Loss : 0.62403, Acc : 0.769, Sensitive_Loss : 0.19581, Sensitive_Acc : 15.900, Run Time : 6.92 sec
INFO:root:2024-03-31 22:50:17, Train, Epoch : 9, Step : 4090, Loss : 0.67439, Acc : 0.800, Sensitive_Loss : 0.18960, Sensitive_Acc : 13.500, Run Time : 6.97 sec
INFO:root:2024-03-31 22:50:24, Train, Epoch : 9, Step : 4100, Loss : 0.61457, Acc : 0.772, Sensitive_Loss : 0.20371, Sensitive_Acc : 16.200, Run Time : 6.88 sec
INFO:root:2024-03-31 22:52:01, Dev, Step : 4100, Loss : 0.94885, Acc : 0.751, Auc : 0.782, Sensitive_Loss : 0.22261, Sensitive_Acc : 16.851, Sensitive_Auc : 0.999, Mean auc: 0.782, Run Time : 96.72 sec
INFO:root:2024-03-31 22:52:06, Train, Epoch : 9, Step : 4110, Loss : 0.72389, Acc : 0.766, Sensitive_Loss : 0.22575, Sensitive_Acc : 17.700, Run Time : 102.37 sec
INFO:root:2024-03-31 22:52:14, Train, Epoch : 9, Step : 4120, Loss : 0.59718, Acc : 0.778, Sensitive_Loss : 0.24071, Sensitive_Acc : 16.800, Run Time : 7.80 sec
INFO:root:2024-03-31 22:52:21, Train, Epoch : 9, Step : 4130, Loss : 0.82000, Acc : 0.703, Sensitive_Loss : 0.19572, Sensitive_Acc : 18.000, Run Time : 6.84 sec
INFO:root:2024-03-31 22:52:28, Train, Epoch : 9, Step : 4140, Loss : 0.61288, Acc : 0.812, Sensitive_Loss : 0.19206, Sensitive_Acc : 15.800, Run Time : 6.86 sec
INFO:root:2024-03-31 22:52:35, Train, Epoch : 9, Step : 4150, Loss : 0.61141, Acc : 0.769, Sensitive_Loss : 0.24586, Sensitive_Acc : 16.000, Run Time : 7.19 sec
INFO:root:2024-03-31 22:52:42, Train, Epoch : 9, Step : 4160, Loss : 0.53465, Acc : 0.766, Sensitive_Loss : 0.23535, Sensitive_Acc : 17.600, Run Time : 7.41 sec
INFO:root:2024-03-31 22:52:50, Train, Epoch : 9, Step : 4170, Loss : 0.64996, Acc : 0.781, Sensitive_Loss : 0.20725, Sensitive_Acc : 14.400, Run Time : 7.45 sec
INFO:root:2024-03-31 22:52:57, Train, Epoch : 9, Step : 4180, Loss : 0.62453, Acc : 0.812, Sensitive_Loss : 0.26476, Sensitive_Acc : 17.800, Run Time : 7.14 sec
INFO:root:2024-03-31 22:53:04, Train, Epoch : 9, Step : 4190, Loss : 0.72591, Acc : 0.778, Sensitive_Loss : 0.22198, Sensitive_Acc : 17.600, Run Time : 6.85 sec
INFO:root:2024-03-31 22:53:11, Train, Epoch : 9, Step : 4200, Loss : 0.73772, Acc : 0.759, Sensitive_Loss : 0.23599, Sensitive_Acc : 14.500, Run Time : 7.79 sec
INFO:root:2024-03-31 22:54:46, Dev, Step : 4200, Loss : 0.95269, Acc : 0.787, Auc : 0.784, Sensitive_Loss : 0.23766, Sensitive_Acc : 16.681, Sensitive_Auc : 0.997, Mean auc: 0.784, Run Time : 94.55 sec
INFO:root:2024-03-31 22:54:51, Train, Epoch : 9, Step : 4210, Loss : 0.57938, Acc : 0.797, Sensitive_Loss : 0.16239, Sensitive_Acc : 17.500, Run Time : 99.89 sec
INFO:root:2024-03-31 22:54:59, Train, Epoch : 9, Step : 4220, Loss : 0.63583, Acc : 0.769, Sensitive_Loss : 0.22347, Sensitive_Acc : 14.400, Run Time : 7.33 sec
INFO:root:2024-03-31 22:55:06, Train, Epoch : 9, Step : 4230, Loss : 0.72006, Acc : 0.778, Sensitive_Loss : 0.15701, Sensitive_Acc : 15.800, Run Time : 7.15 sec
INFO:root:2024-03-31 22:55:14, Train, Epoch : 9, Step : 4240, Loss : 0.52048, Acc : 0.750, Sensitive_Loss : 0.24175, Sensitive_Acc : 12.500, Run Time : 7.75 sec
INFO:root:2024-03-31 22:55:21, Train, Epoch : 9, Step : 4250, Loss : 0.70458, Acc : 0.741, Sensitive_Loss : 0.24245, Sensitive_Acc : 15.600, Run Time : 6.96 sec
INFO:root:2024-03-31 22:55:27, Train, Epoch : 9, Step : 4260, Loss : 0.62833, Acc : 0.772, Sensitive_Loss : 0.20113, Sensitive_Acc : 14.300, Run Time : 6.79 sec
INFO:root:2024-03-31 22:55:35, Train, Epoch : 9, Step : 4270, Loss : 0.70793, Acc : 0.762, Sensitive_Loss : 0.21010, Sensitive_Acc : 16.500, Run Time : 7.41 sec
INFO:root:2024-03-31 22:55:42, Train, Epoch : 9, Step : 4280, Loss : 0.76959, Acc : 0.775, Sensitive_Loss : 0.17591, Sensitive_Acc : 16.300, Run Time : 7.01 sec
INFO:root:2024-03-31 22:55:49, Train, Epoch : 9, Step : 4290, Loss : 0.67378, Acc : 0.762, Sensitive_Loss : 0.20971, Sensitive_Acc : 16.500, Run Time : 7.38 sec
INFO:root:2024-03-31 22:55:56, Train, Epoch : 9, Step : 4300, Loss : 0.71788, Acc : 0.753, Sensitive_Loss : 0.17649, Sensitive_Acc : 17.400, Run Time : 6.85 sec
INFO:root:2024-03-31 22:57:31, Dev, Step : 4300, Loss : 0.95837, Acc : 0.760, Auc : 0.783, Sensitive_Loss : 0.23142, Sensitive_Acc : 16.837, Sensitive_Auc : 0.998, Mean auc: 0.783, Run Time : 94.59 sec
INFO:root:2024-03-31 22:57:36, Train, Epoch : 9, Step : 4310, Loss : 0.63240, Acc : 0.766, Sensitive_Loss : 0.18007, Sensitive_Acc : 16.900, Run Time : 100.35 sec
INFO:root:2024-03-31 22:57:44, Train, Epoch : 9, Step : 4320, Loss : 0.69936, Acc : 0.759, Sensitive_Loss : 0.16946, Sensitive_Acc : 17.500, Run Time : 7.34 sec
INFO:root:2024-03-31 22:57:51, Train, Epoch : 9, Step : 4330, Loss : 0.61413, Acc : 0.794, Sensitive_Loss : 0.17001, Sensitive_Acc : 17.200, Run Time : 7.70 sec
INFO:root:2024-03-31 22:57:58, Train, Epoch : 9, Step : 4340, Loss : 0.68637, Acc : 0.750, Sensitive_Loss : 0.17710, Sensitive_Acc : 18.100, Run Time : 7.02 sec
INFO:root:2024-03-31 22:58:06, Train, Epoch : 9, Step : 4350, Loss : 0.61136, Acc : 0.741, Sensitive_Loss : 0.20703, Sensitive_Acc : 16.300, Run Time : 7.19 sec
INFO:root:2024-03-31 22:58:12, Train, Epoch : 9, Step : 4360, Loss : 0.64050, Acc : 0.756, Sensitive_Loss : 0.24019, Sensitive_Acc : 17.600, Run Time : 6.70 sec
INFO:root:2024-03-31 22:58:20, Train, Epoch : 9, Step : 4370, Loss : 0.58688, Acc : 0.806, Sensitive_Loss : 0.18258, Sensitive_Acc : 16.600, Run Time : 7.66 sec
INFO:root:2024-03-31 22:58:27, Train, Epoch : 9, Step : 4380, Loss : 0.75913, Acc : 0.762, Sensitive_Loss : 0.18456, Sensitive_Acc : 15.900, Run Time : 7.20 sec
INFO:root:2024-03-31 22:58:34, Train, Epoch : 9, Step : 4390, Loss : 0.52228, Acc : 0.816, Sensitive_Loss : 0.21697, Sensitive_Acc : 15.700, Run Time : 6.95 sec
INFO:root:2024-03-31 22:58:41, Train, Epoch : 9, Step : 4400, Loss : 0.64887, Acc : 0.800, Sensitive_Loss : 0.18412, Sensitive_Acc : 18.700, Run Time : 7.34 sec
INFO:root:2024-03-31 23:00:17, Dev, Step : 4400, Loss : 0.96096, Acc : 0.795, Auc : 0.783, Sensitive_Loss : 0.23205, Sensitive_Acc : 16.709, Sensitive_Auc : 0.999, Mean auc: 0.783, Run Time : 95.55 sec
INFO:root:2024-03-31 23:00:22, Train, Epoch : 9, Step : 4410, Loss : 0.65794, Acc : 0.794, Sensitive_Loss : 0.21058, Sensitive_Acc : 13.900, Run Time : 100.91 sec
INFO:root:2024-03-31 23:00:30, Train, Epoch : 9, Step : 4420, Loss : 0.75279, Acc : 0.759, Sensitive_Loss : 0.17212, Sensitive_Acc : 17.100, Run Time : 7.44 sec
INFO:root:2024-03-31 23:02:09
INFO:root:y_pred: [0.19591689 0.11591137 0.18167503 ... 0.20413977 0.21898584 0.05897214]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [1.20168237e-03 7.65921623e-02 9.86867189e-01 3.70180197e-02
 9.99682665e-01 3.89121519e-03 3.38942371e-02 3.94104198e-02
 2.27948441e-03 3.02306831e-01 6.10901415e-03 1.29062431e-02
 9.65734482e-01 1.16686784e-02 9.80289161e-01 8.65764856e-01
 7.59644143e-04 4.73150551e-01 9.49777007e-01 6.74193680e-01
 2.15154588e-02 3.29598114e-02 9.99697924e-01 9.90342021e-01
 1.23188458e-02 3.56032580e-01 9.39432182e-04 9.95638847e-01
 5.22444304e-03 9.24587011e-01 9.99750435e-01 9.45325613e-01
 4.75311745e-03 2.15215914e-05 1.47624165e-01 3.06611788e-03
 1.45687712e-02 2.78480589e-01 9.88259315e-01 9.89499629e-01
 3.51166324e-04 1.33295287e-03 9.64089155e-01 9.99496937e-01
 7.21731633e-02 5.55695407e-02 1.06321070e-02 1.07693106e-01
 9.98085499e-01 9.43397880e-01 9.85637248e-01 4.32908118e-01
 9.96311963e-01 9.81663346e-01 9.96123016e-01 9.14475083e-01
 9.47090328e-01 7.99314305e-03 9.99263823e-01 1.58660144e-01
 2.33323826e-03 5.69161952e-01 8.96198869e-01 9.92409110e-01
 9.97607589e-01 9.96433973e-01 8.70333910e-01 9.88361537e-01
 9.86712933e-01 9.31832552e-01 9.74745929e-01 2.11910978e-02
 1.17899780e-03 7.54383057e-02 8.85621905e-01 1.23608319e-04
 9.89622593e-01 2.11649109e-04 9.01041031e-01 5.28249085e-01
 7.15133846e-02 4.99370843e-02 6.13897666e-02 5.58291912e-01
 8.72311890e-01 6.51864052e-01 7.23211706e-01 9.39836875e-02
 1.08455839e-02 2.12898087e-02 3.34563740e-02 5.57032302e-02
 4.15597530e-03 4.49326448e-02 9.68037784e-01 6.87554181e-01
 3.59781869e-02 2.05130386e-03 8.94449770e-01 8.28738928e-01
 8.73646796e-01 5.88997640e-02 9.99312997e-01 2.90681375e-03
 1.70630384e-02 9.52800393e-01 1.31547526e-01 2.66622622e-02
 8.02099239e-03 2.65234504e-02 6.85398467e-03 9.69093621e-01
 3.95637989e-01 1.71267018e-02 1.01877786e-01 3.15332909e-05
 9.97792244e-01 2.79862434e-01 1.61320835e-01 2.31072889e-04
 5.41270912e-01 3.02074826e-03 9.98635948e-01 2.37314892e-03
 7.44028166e-02 9.96895194e-01 3.46815842e-03 9.82897341e-01
 8.34090054e-01 9.98814344e-01 8.97664358e-05 9.98437822e-01
 2.05626059e-03 9.99090075e-01 9.99185979e-01 4.10026044e-01
 1.35637924e-01 2.71499962e-01 2.53000557e-02 9.93483484e-01
 9.96752679e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-31 23:02:09, Dev, Step : 4428, Loss : 0.96504, Acc : 0.761, Auc : 0.777, Sensitive_Loss : 0.21549, Sensitive_Acc : 16.823, Sensitive_Auc : 0.999, Mean auc: 0.777, Run Time : 93.83 sec
INFO:root:2024-03-31 23:02:13, Train, Epoch : 10, Step : 4430, Loss : 0.11031, Acc : 0.156, Sensitive_Loss : 0.03302, Sensitive_Acc : 2.700, Run Time : 2.67 sec
INFO:root:2024-03-31 23:02:20, Train, Epoch : 10, Step : 4440, Loss : 0.65224, Acc : 0.772, Sensitive_Loss : 0.20623, Sensitive_Acc : 16.600, Run Time : 7.21 sec
INFO:root:2024-03-31 23:02:27, Train, Epoch : 10, Step : 4450, Loss : 0.66436, Acc : 0.781, Sensitive_Loss : 0.19170, Sensitive_Acc : 16.800, Run Time : 7.31 sec
INFO:root:2024-03-31 23:02:34, Train, Epoch : 10, Step : 4460, Loss : 0.65983, Acc : 0.787, Sensitive_Loss : 0.20743, Sensitive_Acc : 17.100, Run Time : 6.92 sec
INFO:root:2024-03-31 23:02:41, Train, Epoch : 10, Step : 4470, Loss : 0.64345, Acc : 0.787, Sensitive_Loss : 0.20724, Sensitive_Acc : 16.800, Run Time : 7.32 sec
INFO:root:2024-03-31 23:02:48, Train, Epoch : 10, Step : 4480, Loss : 0.51844, Acc : 0.803, Sensitive_Loss : 0.21256, Sensitive_Acc : 15.500, Run Time : 7.01 sec
INFO:root:2024-03-31 23:02:55, Train, Epoch : 10, Step : 4490, Loss : 0.55872, Acc : 0.800, Sensitive_Loss : 0.18677, Sensitive_Acc : 14.200, Run Time : 7.02 sec
INFO:root:2024-03-31 23:03:02, Train, Epoch : 10, Step : 4500, Loss : 0.62427, Acc : 0.787, Sensitive_Loss : 0.17856, Sensitive_Acc : 14.500, Run Time : 7.07 sec
INFO:root:2024-03-31 23:04:37, Dev, Step : 4500, Loss : 0.95607, Acc : 0.801, Auc : 0.788, Sensitive_Loss : 0.23371, Sensitive_Acc : 16.709, Sensitive_Auc : 0.999, Mean auc: 0.788, Run Time : 94.80 sec
INFO:root:2024-03-31 23:04:43, Train, Epoch : 10, Step : 4510, Loss : 0.60661, Acc : 0.759, Sensitive_Loss : 0.15067, Sensitive_Acc : 16.300, Run Time : 100.72 sec
INFO:root:2024-03-31 23:04:50, Train, Epoch : 10, Step : 4520, Loss : 0.59445, Acc : 0.797, Sensitive_Loss : 0.21400, Sensitive_Acc : 14.700, Run Time : 7.03 sec
INFO:root:2024-03-31 23:04:58, Train, Epoch : 10, Step : 4530, Loss : 0.42041, Acc : 0.794, Sensitive_Loss : 0.17267, Sensitive_Acc : 17.200, Run Time : 7.42 sec
INFO:root:2024-03-31 23:05:05, Train, Epoch : 10, Step : 4540, Loss : 0.53223, Acc : 0.775, Sensitive_Loss : 0.19320, Sensitive_Acc : 16.000, Run Time : 7.13 sec
INFO:root:2024-03-31 23:05:12, Train, Epoch : 10, Step : 4550, Loss : 0.56607, Acc : 0.753, Sensitive_Loss : 0.23972, Sensitive_Acc : 16.200, Run Time : 6.98 sec
INFO:root:2024-03-31 23:05:19, Train, Epoch : 10, Step : 4560, Loss : 0.66286, Acc : 0.812, Sensitive_Loss : 0.19470, Sensitive_Acc : 17.300, Run Time : 7.62 sec
INFO:root:2024-03-31 23:05:26, Train, Epoch : 10, Step : 4570, Loss : 0.63988, Acc : 0.778, Sensitive_Loss : 0.17406, Sensitive_Acc : 14.900, Run Time : 6.96 sec
INFO:root:2024-03-31 23:05:34, Train, Epoch : 10, Step : 4580, Loss : 0.52826, Acc : 0.791, Sensitive_Loss : 0.16396, Sensitive_Acc : 15.400, Run Time : 7.40 sec
INFO:root:2024-03-31 23:05:41, Train, Epoch : 10, Step : 4590, Loss : 0.61046, Acc : 0.803, Sensitive_Loss : 0.20147, Sensitive_Acc : 15.100, Run Time : 6.96 sec
INFO:root:2024-03-31 23:05:48, Train, Epoch : 10, Step : 4600, Loss : 0.66321, Acc : 0.769, Sensitive_Loss : 0.16474, Sensitive_Acc : 16.300, Run Time : 7.76 sec
INFO:root:2024-03-31 23:07:23, Dev, Step : 4600, Loss : 1.00994, Acc : 0.808, Auc : 0.772, Sensitive_Loss : 0.21758, Sensitive_Acc : 16.823, Sensitive_Auc : 0.999, Mean auc: 0.772, Run Time : 94.78 sec
INFO:root:2024-03-31 23:07:28, Train, Epoch : 10, Step : 4610, Loss : 0.45816, Acc : 0.797, Sensitive_Loss : 0.17883, Sensitive_Acc : 17.000, Run Time : 100.04 sec
INFO:root:2024-03-31 23:07:36, Train, Epoch : 10, Step : 4620, Loss : 0.64651, Acc : 0.797, Sensitive_Loss : 0.31181, Sensitive_Acc : 15.800, Run Time : 7.53 sec
INFO:root:2024-03-31 23:07:43, Train, Epoch : 10, Step : 4630, Loss : 0.75441, Acc : 0.766, Sensitive_Loss : 0.17532, Sensitive_Acc : 17.400, Run Time : 6.85 sec
INFO:root:2024-03-31 23:07:50, Train, Epoch : 10, Step : 4640, Loss : 0.49982, Acc : 0.772, Sensitive_Loss : 0.20841, Sensitive_Acc : 17.100, Run Time : 7.14 sec
INFO:root:2024-03-31 23:07:57, Train, Epoch : 10, Step : 4650, Loss : 0.75166, Acc : 0.803, Sensitive_Loss : 0.21169, Sensitive_Acc : 17.300, Run Time : 7.33 sec
INFO:root:2024-03-31 23:08:05, Train, Epoch : 10, Step : 4660, Loss : 0.65933, Acc : 0.778, Sensitive_Loss : 0.18654, Sensitive_Acc : 17.800, Run Time : 7.48 sec
INFO:root:2024-03-31 23:08:12, Train, Epoch : 10, Step : 4670, Loss : 0.68240, Acc : 0.800, Sensitive_Loss : 0.15033, Sensitive_Acc : 18.100, Run Time : 7.00 sec
INFO:root:2024-03-31 23:08:19, Train, Epoch : 10, Step : 4680, Loss : 0.63157, Acc : 0.816, Sensitive_Loss : 0.18519, Sensitive_Acc : 16.400, Run Time : 7.43 sec
INFO:root:2024-03-31 23:08:27, Train, Epoch : 10, Step : 4690, Loss : 0.73039, Acc : 0.778, Sensitive_Loss : 0.17559, Sensitive_Acc : 15.100, Run Time : 7.25 sec
INFO:root:2024-03-31 23:08:34, Train, Epoch : 10, Step : 4700, Loss : 0.54394, Acc : 0.806, Sensitive_Loss : 0.22020, Sensitive_Acc : 15.500, Run Time : 7.33 sec
INFO:root:2024-03-31 23:10:09, Dev, Step : 4700, Loss : 0.95064, Acc : 0.747, Auc : 0.784, Sensitive_Loss : 0.24448, Sensitive_Acc : 16.695, Sensitive_Auc : 0.998, Mean auc: 0.784, Run Time : 95.38 sec
INFO:root:2024-03-31 23:10:15, Train, Epoch : 10, Step : 4710, Loss : 0.63317, Acc : 0.806, Sensitive_Loss : 0.18195, Sensitive_Acc : 14.700, Run Time : 100.83 sec
INFO:root:2024-03-31 23:10:22, Train, Epoch : 10, Step : 4720, Loss : 0.58057, Acc : 0.800, Sensitive_Loss : 0.17490, Sensitive_Acc : 17.200, Run Time : 7.35 sec
INFO:root:2024-03-31 23:10:29, Train, Epoch : 10, Step : 4730, Loss : 0.68848, Acc : 0.781, Sensitive_Loss : 0.15517, Sensitive_Acc : 14.700, Run Time : 7.11 sec
INFO:root:2024-03-31 23:10:36, Train, Epoch : 10, Step : 4740, Loss : 0.46017, Acc : 0.725, Sensitive_Loss : 0.19404, Sensitive_Acc : 17.000, Run Time : 7.17 sec
INFO:root:2024-03-31 23:10:44, Train, Epoch : 10, Step : 4750, Loss : 0.63837, Acc : 0.812, Sensitive_Loss : 0.24585, Sensitive_Acc : 17.000, Run Time : 7.55 sec
INFO:root:2024-03-31 23:10:51, Train, Epoch : 10, Step : 4760, Loss : 0.50175, Acc : 0.812, Sensitive_Loss : 0.16804, Sensitive_Acc : 16.700, Run Time : 7.06 sec
INFO:root:2024-03-31 23:10:58, Train, Epoch : 10, Step : 4770, Loss : 0.69699, Acc : 0.800, Sensitive_Loss : 0.21454, Sensitive_Acc : 14.800, Run Time : 7.52 sec
INFO:root:2024-03-31 23:11:06, Train, Epoch : 10, Step : 4780, Loss : 0.54164, Acc : 0.772, Sensitive_Loss : 0.15081, Sensitive_Acc : 14.900, Run Time : 7.34 sec
INFO:root:2024-03-31 23:11:13, Train, Epoch : 10, Step : 4790, Loss : 0.61538, Acc : 0.791, Sensitive_Loss : 0.16851, Sensitive_Acc : 15.300, Run Time : 6.97 sec
INFO:root:2024-03-31 23:11:20, Train, Epoch : 10, Step : 4800, Loss : 0.66363, Acc : 0.769, Sensitive_Loss : 0.23790, Sensitive_Acc : 19.800, Run Time : 7.26 sec
INFO:root:2024-03-31 23:12:56, Dev, Step : 4800, Loss : 0.99895, Acc : 0.808, Auc : 0.775, Sensitive_Loss : 0.22663, Sensitive_Acc : 16.865, Sensitive_Auc : 0.998, Mean auc: 0.775, Run Time : 95.41 sec
INFO:root:2024-03-31 23:13:01, Train, Epoch : 10, Step : 4810, Loss : 0.56042, Acc : 0.784, Sensitive_Loss : 0.15636, Sensitive_Acc : 17.000, Run Time : 100.94 sec
INFO:root:2024-03-31 23:13:08, Train, Epoch : 10, Step : 4820, Loss : 0.71843, Acc : 0.787, Sensitive_Loss : 0.16959, Sensitive_Acc : 17.600, Run Time : 7.32 sec
INFO:root:2024-03-31 23:13:16, Train, Epoch : 10, Step : 4830, Loss : 0.59047, Acc : 0.816, Sensitive_Loss : 0.19758, Sensitive_Acc : 16.500, Run Time : 7.24 sec
INFO:root:2024-03-31 23:13:23, Train, Epoch : 10, Step : 4840, Loss : 0.69588, Acc : 0.797, Sensitive_Loss : 0.27035, Sensitive_Acc : 16.300, Run Time : 7.02 sec
INFO:root:2024-03-31 23:13:30, Train, Epoch : 10, Step : 4850, Loss : 0.71484, Acc : 0.784, Sensitive_Loss : 0.17521, Sensitive_Acc : 14.800, Run Time : 7.38 sec
INFO:root:2024-03-31 23:13:38, Train, Epoch : 10, Step : 4860, Loss : 0.59153, Acc : 0.841, Sensitive_Loss : 0.22772, Sensitive_Acc : 15.200, Run Time : 7.57 sec
INFO:root:2024-03-31 23:13:45, Train, Epoch : 10, Step : 4870, Loss : 0.75406, Acc : 0.778, Sensitive_Loss : 0.20000, Sensitive_Acc : 16.800, Run Time : 7.16 sec
INFO:root:2024-03-31 23:13:51, Train, Epoch : 10, Step : 4880, Loss : 0.50530, Acc : 0.800, Sensitive_Loss : 0.22783, Sensitive_Acc : 16.800, Run Time : 6.07 sec
INFO:root:2024-03-31 23:13:58, Train, Epoch : 10, Step : 4890, Loss : 0.63037, Acc : 0.800, Sensitive_Loss : 0.16835, Sensitive_Acc : 17.300, Run Time : 7.37 sec
INFO:root:2024-03-31 23:14:06, Train, Epoch : 10, Step : 4900, Loss : 0.61799, Acc : 0.775, Sensitive_Loss : 0.20848, Sensitive_Acc : 15.900, Run Time : 7.34 sec
INFO:root:2024-03-31 23:15:41, Dev, Step : 4900, Loss : 0.94876, Acc : 0.749, Auc : 0.789, Sensitive_Loss : 0.26256, Sensitive_Acc : 16.667, Sensitive_Auc : 0.998, Mean auc: 0.789, Run Time : 95.07 sec
INFO:root:2024-03-31 23:15:46, Train, Epoch : 10, Step : 4910, Loss : 0.55973, Acc : 0.778, Sensitive_Loss : 0.18101, Sensitive_Acc : 17.900, Run Time : 100.75 sec
INFO:root:2024-03-31 23:15:53, Train, Epoch : 10, Step : 4920, Loss : 0.60435, Acc : 0.781, Sensitive_Loss : 0.15283, Sensitive_Acc : 17.800, Run Time : 6.56 sec
INFO:root:2024-03-31 23:17:27
INFO:root:y_pred: [0.14982536 0.05704655 0.27790704 ... 0.17810717 0.25893322 0.0836174 ]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [9.43235878e-04 1.61640614e-01 9.89024818e-01 5.46537302e-02
 9.99860048e-01 6.80438569e-03 4.46101874e-02 1.33047774e-01
 5.00294659e-03 1.76114082e-01 1.79351009e-02 2.09868178e-02
 9.80801404e-01 7.81876221e-03 9.89449680e-01 8.89179647e-01
 1.16546056e-03 6.37327075e-01 9.54246342e-01 6.97648942e-01
 4.72368672e-02 5.01853824e-02 9.99923110e-01 9.92038131e-01
 2.51228362e-02 6.37223005e-01 3.43848625e-03 9.98069584e-01
 1.50145162e-02 9.60358858e-01 9.99869347e-01 9.78973091e-01
 9.23543796e-03 2.20518996e-05 1.69239104e-01 1.28912535e-02
 4.75343354e-02 3.20320547e-01 9.95777607e-01 9.86980855e-01
 8.00578564e-04 3.51412874e-03 9.75883663e-01 9.99476373e-01
 9.38607380e-02 5.31442240e-02 1.45921055e-02 2.36333966e-01
 9.98450279e-01 9.69033837e-01 9.94945705e-01 5.90183020e-01
 9.97900128e-01 9.83817101e-01 9.97100770e-01 9.63333964e-01
 9.61767256e-01 3.31355929e-02 9.99638081e-01 3.31474960e-01
 1.21532474e-03 8.36501539e-01 9.27258909e-01 9.94951606e-01
 9.98955369e-01 9.97821450e-01 8.88735831e-01 9.96255279e-01
 9.91171777e-01 9.76351380e-01 9.64943528e-01 2.17587743e-02
 1.78735203e-03 9.49059278e-02 9.43703949e-01 8.78169740e-05
 9.92542267e-01 3.22208187e-04 9.64143872e-01 6.13719583e-01
 9.97169018e-02 7.52622932e-02 1.28540471e-01 7.47030854e-01
 9.27999735e-01 8.17681789e-01 8.69935572e-01 7.23180100e-02
 2.52825189e-02 9.29941610e-02 2.45966259e-02 1.12271689e-01
 3.73254274e-03 9.21147987e-02 9.78148818e-01 7.90238798e-01
 3.37429643e-02 3.70869040e-03 9.60455179e-01 8.78696144e-01
 9.29943442e-01 1.16976686e-01 9.99552786e-01 1.02409972e-02
 4.00712341e-02 9.73119915e-01 2.70604402e-01 3.02107334e-02
 8.78873281e-03 1.34207621e-01 1.14006335e-02 9.80427146e-01
 5.79221845e-01 1.67103913e-02 2.15005711e-01 6.41790539e-05
 9.98951435e-01 4.23265398e-01 1.56239688e-01 2.82815454e-04
 5.74164629e-01 4.36084112e-03 9.99147177e-01 5.52445510e-03
 9.97346193e-02 9.97953057e-01 3.62616545e-03 9.90663886e-01
 8.93969178e-01 9.99612272e-01 4.62050491e-04 9.98981178e-01
 2.71113031e-03 9.99574721e-01 9.99320865e-01 4.56107169e-01
 2.29062155e-01 4.39490587e-01 2.93753352e-02 9.96558487e-01
 9.97880578e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-31 23:17:27, Dev, Step : 4920, Loss : 0.95823, Acc : 0.792, Auc : 0.789, Sensitive_Loss : 0.24871, Sensitive_Acc : 16.695, Sensitive_Auc : 0.998, Mean auc: 0.789, Run Time : 93.86 sec
