Running on desktop22:
stdin: is not a tty
/home/pmen/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
0
Using the specified args:
Namespace(cfg_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_pmen.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_train.csv",
    "dev_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_val.csv",
    "pred_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Biased_Sex_1_pos01.csv",
    "pred_model": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Biased_Sex_1_pos011.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.1,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-27 12:39:46, Train, Epoch : 1, Step : 10, Loss : 0.65815, Acc : 0.556, Sensitive_Loss : 0.69394, Sensitive_Acc : 16.000, Run Time : 9.97 sec
INFO:root:2024-04-27 12:39:54, Train, Epoch : 1, Step : 20, Loss : 0.63192, Acc : 0.691, Sensitive_Loss : 0.60588, Sensitive_Acc : 15.800, Run Time : 7.75 sec
INFO:root:2024-04-27 12:40:02, Train, Epoch : 1, Step : 30, Loss : 0.56184, Acc : 0.713, Sensitive_Loss : 0.61386, Sensitive_Acc : 15.300, Run Time : 7.98 sec
INFO:root:2024-04-27 12:40:10, Train, Epoch : 1, Step : 40, Loss : 0.61854, Acc : 0.697, Sensitive_Loss : 0.47863, Sensitive_Acc : 17.000, Run Time : 8.01 sec
INFO:root:2024-04-27 12:40:18, Train, Epoch : 1, Step : 50, Loss : 0.54373, Acc : 0.731, Sensitive_Loss : 0.42463, Sensitive_Acc : 16.100, Run Time : 8.10 sec
INFO:root:2024-04-27 12:40:26, Train, Epoch : 1, Step : 60, Loss : 0.57745, Acc : 0.728, Sensitive_Loss : 0.43879, Sensitive_Acc : 18.300, Run Time : 7.74 sec
INFO:root:2024-04-27 12:40:34, Train, Epoch : 1, Step : 70, Loss : 0.59424, Acc : 0.741, Sensitive_Loss : 0.46644, Sensitive_Acc : 14.900, Run Time : 7.85 sec
INFO:root:2024-04-27 12:40:42, Train, Epoch : 1, Step : 80, Loss : 0.48064, Acc : 0.787, Sensitive_Loss : 0.35542, Sensitive_Acc : 15.900, Run Time : 8.17 sec
INFO:root:2024-04-27 12:40:50, Train, Epoch : 1, Step : 90, Loss : 0.41819, Acc : 0.781, Sensitive_Loss : 0.34582, Sensitive_Acc : 16.500, Run Time : 7.96 sec
INFO:root:2024-04-27 12:40:58, Train, Epoch : 1, Step : 100, Loss : 0.57011, Acc : 0.747, Sensitive_Loss : 0.34334, Sensitive_Acc : 16.100, Run Time : 7.98 sec
INFO:root:2024-04-27 12:42:47, Dev, Step : 100, Loss : 0.50061, Acc : 0.776, Auc : 0.856, Sensitive_Loss : 0.33672, Sensitive_Acc : 16.693, Sensitive_Auc : 0.946, Mean auc: 0.856, Run Time : 109.77 sec
INFO:root:2024-04-27 12:42:48, Best, Step : 100, Loss : 0.50061, Acc : 0.776, Auc : 0.856, Sensitive_Loss : 0.33672, Sensitive_Acc : 16.693, Sensitive_Auc : 0.946, Best Auc : 0.856
INFO:root:2024-04-27 12:42:54, Train, Epoch : 1, Step : 110, Loss : 0.49799, Acc : 0.756, Sensitive_Loss : 0.30865, Sensitive_Acc : 15.400, Run Time : 116.51 sec
INFO:root:2024-04-27 12:43:02, Train, Epoch : 1, Step : 120, Loss : 0.50397, Acc : 0.762, Sensitive_Loss : 0.34328, Sensitive_Acc : 15.900, Run Time : 7.98 sec
INFO:root:2024-04-27 12:43:10, Train, Epoch : 1, Step : 130, Loss : 0.49641, Acc : 0.747, Sensitive_Loss : 0.34754, Sensitive_Acc : 17.200, Run Time : 7.82 sec
INFO:root:2024-04-27 12:43:18, Train, Epoch : 1, Step : 140, Loss : 0.43374, Acc : 0.791, Sensitive_Loss : 0.31250, Sensitive_Acc : 14.700, Run Time : 8.33 sec
INFO:root:2024-04-27 12:43:27, Train, Epoch : 1, Step : 150, Loss : 0.55274, Acc : 0.753, Sensitive_Loss : 0.32434, Sensitive_Acc : 15.900, Run Time : 8.22 sec
INFO:root:2024-04-27 12:43:35, Train, Epoch : 1, Step : 160, Loss : 0.50688, Acc : 0.772, Sensitive_Loss : 0.26866, Sensitive_Acc : 15.700, Run Time : 8.16 sec
INFO:root:2024-04-27 12:43:43, Train, Epoch : 1, Step : 170, Loss : 0.53089, Acc : 0.766, Sensitive_Loss : 0.33172, Sensitive_Acc : 15.300, Run Time : 8.34 sec
INFO:root:2024-04-27 12:43:51, Train, Epoch : 1, Step : 180, Loss : 0.55774, Acc : 0.772, Sensitive_Loss : 0.27491, Sensitive_Acc : 16.200, Run Time : 8.04 sec
INFO:root:2024-04-27 12:43:59, Train, Epoch : 1, Step : 190, Loss : 0.48670, Acc : 0.806, Sensitive_Loss : 0.29786, Sensitive_Acc : 16.100, Run Time : 8.06 sec
INFO:root:2024-04-27 12:44:07, Train, Epoch : 1, Step : 200, Loss : 0.48287, Acc : 0.778, Sensitive_Loss : 0.35355, Sensitive_Acc : 15.800, Run Time : 8.31 sec
INFO:root:2024-04-27 12:45:42, Dev, Step : 200, Loss : 0.50307, Acc : 0.781, Auc : 0.869, Sensitive_Loss : 0.28030, Sensitive_Acc : 16.850, Sensitive_Auc : 0.971, Mean auc: 0.869, Run Time : 94.37 sec
INFO:root:2024-04-27 12:45:42, Best, Step : 200, Loss : 0.50307, Acc : 0.781, Auc : 0.869, Sensitive_Loss : 0.28030, Sensitive_Acc : 16.850, Sensitive_Auc : 0.971, Best Auc : 0.869
INFO:root:2024-04-27 12:45:49, Train, Epoch : 1, Step : 210, Loss : 0.46035, Acc : 0.787, Sensitive_Loss : 0.28529, Sensitive_Acc : 15.600, Run Time : 101.13 sec
INFO:root:2024-04-27 12:45:56, Train, Epoch : 1, Step : 220, Loss : 0.49302, Acc : 0.778, Sensitive_Loss : 0.30389, Sensitive_Acc : 15.000, Run Time : 7.81 sec
INFO:root:2024-04-27 12:46:04, Train, Epoch : 1, Step : 230, Loss : 0.56959, Acc : 0.753, Sensitive_Loss : 0.25016, Sensitive_Acc : 16.800, Run Time : 8.11 sec
INFO:root:2024-04-27 12:46:12, Train, Epoch : 1, Step : 240, Loss : 0.42298, Acc : 0.778, Sensitive_Loss : 0.27758, Sensitive_Acc : 16.800, Run Time : 7.98 sec
INFO:root:2024-04-27 12:46:21, Train, Epoch : 1, Step : 250, Loss : 0.46945, Acc : 0.759, Sensitive_Loss : 0.23451, Sensitive_Acc : 15.500, Run Time : 8.27 sec
INFO:root:2024-04-27 12:46:29, Train, Epoch : 1, Step : 260, Loss : 0.44964, Acc : 0.791, Sensitive_Loss : 0.23790, Sensitive_Acc : 15.800, Run Time : 8.08 sec
INFO:root:2024-04-27 12:46:37, Train, Epoch : 1, Step : 270, Loss : 0.52867, Acc : 0.762, Sensitive_Loss : 0.25654, Sensitive_Acc : 15.100, Run Time : 8.26 sec
INFO:root:2024-04-27 12:46:45, Train, Epoch : 1, Step : 280, Loss : 0.43900, Acc : 0.766, Sensitive_Loss : 0.24316, Sensitive_Acc : 16.500, Run Time : 7.79 sec
INFO:root:2024-04-27 12:46:53, Train, Epoch : 1, Step : 290, Loss : 0.59063, Acc : 0.753, Sensitive_Loss : 0.26360, Sensitive_Acc : 17.600, Run Time : 7.81 sec
INFO:root:2024-04-27 12:47:01, Train, Epoch : 1, Step : 300, Loss : 0.47836, Acc : 0.787, Sensitive_Loss : 0.30804, Sensitive_Acc : 14.800, Run Time : 8.44 sec
INFO:root:2024-04-27 12:48:36, Dev, Step : 300, Loss : 0.47405, Acc : 0.784, Auc : 0.873, Sensitive_Loss : 0.25680, Sensitive_Acc : 16.807, Sensitive_Auc : 0.980, Mean auc: 0.873, Run Time : 94.41 sec
INFO:root:2024-04-27 12:48:37, Best, Step : 300, Loss : 0.47405, Acc : 0.784, Auc : 0.873, Sensitive_Loss : 0.25680, Sensitive_Acc : 16.807, Sensitive_Auc : 0.980, Best Auc : 0.873
INFO:root:2024-04-27 12:48:42, Train, Epoch : 1, Step : 310, Loss : 0.50111, Acc : 0.753, Sensitive_Loss : 0.19890, Sensitive_Acc : 17.000, Run Time : 101.22 sec
INFO:root:2024-04-27 12:48:51, Train, Epoch : 1, Step : 320, Loss : 0.48162, Acc : 0.797, Sensitive_Loss : 0.19742, Sensitive_Acc : 16.100, Run Time : 8.47 sec
INFO:root:2024-04-27 12:48:59, Train, Epoch : 1, Step : 330, Loss : 0.44595, Acc : 0.800, Sensitive_Loss : 0.23318, Sensitive_Acc : 16.600, Run Time : 8.10 sec
INFO:root:2024-04-27 12:49:07, Train, Epoch : 1, Step : 340, Loss : 0.50354, Acc : 0.787, Sensitive_Loss : 0.20683, Sensitive_Acc : 16.600, Run Time : 8.25 sec
INFO:root:2024-04-27 12:49:15, Train, Epoch : 1, Step : 350, Loss : 0.44571, Acc : 0.806, Sensitive_Loss : 0.20337, Sensitive_Acc : 15.700, Run Time : 7.74 sec
INFO:root:2024-04-27 12:49:23, Train, Epoch : 1, Step : 360, Loss : 0.46270, Acc : 0.803, Sensitive_Loss : 0.26036, Sensitive_Acc : 17.700, Run Time : 7.99 sec
INFO:root:2024-04-27 12:49:31, Train, Epoch : 1, Step : 370, Loss : 0.41435, Acc : 0.822, Sensitive_Loss : 0.14694, Sensitive_Acc : 17.200, Run Time : 8.03 sec
INFO:root:2024-04-27 12:49:39, Train, Epoch : 1, Step : 380, Loss : 0.46897, Acc : 0.819, Sensitive_Loss : 0.19391, Sensitive_Acc : 16.700, Run Time : 7.99 sec
INFO:root:2024-04-27 12:49:47, Train, Epoch : 1, Step : 390, Loss : 0.45127, Acc : 0.784, Sensitive_Loss : 0.24313, Sensitive_Acc : 14.600, Run Time : 8.12 sec
INFO:root:2024-04-27 12:49:55, Train, Epoch : 1, Step : 400, Loss : 0.47633, Acc : 0.784, Sensitive_Loss : 0.20844, Sensitive_Acc : 15.900, Run Time : 7.77 sec
INFO:root:2024-04-27 12:51:30, Dev, Step : 400, Loss : 0.52267, Acc : 0.758, Auc : 0.864, Sensitive_Loss : 0.25931, Sensitive_Acc : 16.550, Sensitive_Auc : 0.980, Mean auc: 0.864, Run Time : 95.46 sec
INFO:root:2024-04-27 12:51:36, Train, Epoch : 1, Step : 410, Loss : 0.47911, Acc : 0.797, Sensitive_Loss : 0.24032, Sensitive_Acc : 16.000, Run Time : 101.20 sec
INFO:root:2024-04-27 12:51:44, Train, Epoch : 1, Step : 420, Loss : 0.41268, Acc : 0.838, Sensitive_Loss : 0.15093, Sensitive_Acc : 14.600, Run Time : 8.22 sec
INFO:root:2024-04-27 12:51:52, Train, Epoch : 1, Step : 430, Loss : 0.38892, Acc : 0.828, Sensitive_Loss : 0.18832, Sensitive_Acc : 17.700, Run Time : 8.01 sec
INFO:root:2024-04-27 12:52:01, Train, Epoch : 1, Step : 440, Loss : 0.42292, Acc : 0.784, Sensitive_Loss : 0.15697, Sensitive_Acc : 16.600, Run Time : 8.69 sec
INFO:root:2024-04-27 12:52:09, Train, Epoch : 1, Step : 450, Loss : 0.53542, Acc : 0.772, Sensitive_Loss : 0.23442, Sensitive_Acc : 16.300, Run Time : 8.53 sec
INFO:root:2024-04-27 12:52:18, Train, Epoch : 1, Step : 460, Loss : 0.45821, Acc : 0.803, Sensitive_Loss : 0.22112, Sensitive_Acc : 14.500, Run Time : 8.27 sec
INFO:root:2024-04-27 12:52:26, Train, Epoch : 1, Step : 470, Loss : 0.51418, Acc : 0.800, Sensitive_Loss : 0.21536, Sensitive_Acc : 16.100, Run Time : 8.22 sec
INFO:root:2024-04-27 12:52:34, Train, Epoch : 1, Step : 480, Loss : 0.55960, Acc : 0.734, Sensitive_Loss : 0.22674, Sensitive_Acc : 16.000, Run Time : 7.84 sec
INFO:root:2024-04-27 12:52:42, Train, Epoch : 1, Step : 490, Loss : 0.48072, Acc : 0.734, Sensitive_Loss : 0.16565, Sensitive_Acc : 17.400, Run Time : 8.68 sec
INFO:root:2024-04-27 12:52:51, Train, Epoch : 1, Step : 500, Loss : 0.41229, Acc : 0.797, Sensitive_Loss : 0.17102, Sensitive_Acc : 15.700, Run Time : 8.18 sec
INFO:root:2024-04-27 12:54:26, Dev, Step : 500, Loss : 0.45400, Acc : 0.800, Auc : 0.884, Sensitive_Loss : 0.23672, Sensitive_Acc : 16.721, Sensitive_Auc : 0.977, Mean auc: 0.884, Run Time : 95.43 sec
INFO:root:2024-04-27 12:54:27, Best, Step : 500, Loss : 0.45400, Acc : 0.800, Auc : 0.884, Sensitive_Loss : 0.23672, Sensitive_Acc : 16.721, Sensitive_Auc : 0.977, Best Auc : 0.884
INFO:root:2024-04-27 12:54:33, Train, Epoch : 1, Step : 510, Loss : 0.46939, Acc : 0.794, Sensitive_Loss : 0.22079, Sensitive_Acc : 17.000, Run Time : 102.19 sec
INFO:root:2024-04-27 12:54:41, Train, Epoch : 1, Step : 520, Loss : 0.48961, Acc : 0.778, Sensitive_Loss : 0.19435, Sensitive_Acc : 15.800, Run Time : 8.55 sec
INFO:root:2024-04-27 12:54:50, Train, Epoch : 1, Step : 530, Loss : 0.46202, Acc : 0.766, Sensitive_Loss : 0.16888, Sensitive_Acc : 16.000, Run Time : 8.15 sec
INFO:root:2024-04-27 12:54:58, Train, Epoch : 1, Step : 540, Loss : 0.54305, Acc : 0.756, Sensitive_Loss : 0.20532, Sensitive_Acc : 15.400, Run Time : 8.05 sec
INFO:root:2024-04-27 12:55:06, Train, Epoch : 1, Step : 550, Loss : 0.44572, Acc : 0.791, Sensitive_Loss : 0.18958, Sensitive_Acc : 15.700, Run Time : 7.96 sec
INFO:root:2024-04-27 12:55:14, Train, Epoch : 1, Step : 560, Loss : 0.45793, Acc : 0.806, Sensitive_Loss : 0.17165, Sensitive_Acc : 16.500, Run Time : 8.49 sec
INFO:root:2024-04-27 12:55:22, Train, Epoch : 1, Step : 570, Loss : 0.38875, Acc : 0.819, Sensitive_Loss : 0.16684, Sensitive_Acc : 17.400, Run Time : 8.22 sec
INFO:root:2024-04-27 12:55:30, Train, Epoch : 1, Step : 580, Loss : 0.40570, Acc : 0.822, Sensitive_Loss : 0.21522, Sensitive_Acc : 16.000, Run Time : 8.07 sec
INFO:root:2024-04-27 12:55:38, Train, Epoch : 1, Step : 590, Loss : 0.55410, Acc : 0.722, Sensitive_Loss : 0.18329, Sensitive_Acc : 14.300, Run Time : 7.66 sec
INFO:root:2024-04-27 12:55:46, Train, Epoch : 1, Step : 600, Loss : 0.45307, Acc : 0.791, Sensitive_Loss : 0.19432, Sensitive_Acc : 16.600, Run Time : 8.36 sec
INFO:root:2024-04-27 12:57:21, Dev, Step : 600, Loss : 0.45627, Acc : 0.797, Auc : 0.883, Sensitive_Loss : 0.15868, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.883, Run Time : 94.60 sec
INFO:root:2024-04-27 12:57:27, Train, Epoch : 1, Step : 610, Loss : 0.40365, Acc : 0.831, Sensitive_Loss : 0.19388, Sensitive_Acc : 16.300, Run Time : 100.43 sec
INFO:root:2024-04-27 12:57:35, Train, Epoch : 1, Step : 620, Loss : 0.45615, Acc : 0.803, Sensitive_Loss : 0.17961, Sensitive_Acc : 14.800, Run Time : 8.04 sec
INFO:root:2024-04-27 12:59:12
INFO:root:y_pred: [0.03017781 0.9589212  0.02190518 ... 0.8741439  0.02527343 0.7218209 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.83100176e-01 2.14905548e-03 1.15263812e-01 4.81120951e-05
 9.99855280e-01 1.59880538e-02 9.99995470e-01 9.97601807e-01
 8.17304128e-04 4.27543610e-01 9.95346487e-01 9.98921633e-01
 9.92148876e-01 9.47904825e-01 8.17072839e-02 8.17342877e-01
 9.97672141e-01 3.21638351e-03 3.51619571e-01 7.14585245e-01
 9.98427153e-01 3.92323583e-02 9.94332671e-01 9.36265647e-01
 9.93768454e-01 9.91933525e-01 3.75240366e-03 9.99564707e-01
 9.95059371e-01 4.67147142e-01 6.70704097e-02 1.00373574e-01
 2.41812795e-01 6.72831805e-03 6.83855489e-02 3.13639306e-02
 3.69974822e-01 7.73070008e-02 9.97094393e-01 9.95709181e-01
 8.00247188e-04 1.96780264e-03 9.70753253e-01 2.71301460e-03
 9.99981046e-01 9.95149553e-01 9.94704902e-01 9.31332111e-01
 3.30070667e-02 8.79117072e-01 9.85179543e-01 2.96473224e-02
 3.30987275e-01 1.26997652e-02 8.86586320e-04 1.69292524e-01
 3.68864201e-02 3.82503937e-03 1.33354701e-02 6.67878613e-02
 3.79987694e-02 3.95352960e-01 5.81422932e-02 7.41333902e-01
 6.56511486e-02 9.99982238e-01 7.44325221e-02 9.98375535e-01
 9.77687538e-01 6.88038826e-01 3.49834502e-01 4.55960780e-01
 6.61479402e-03 9.75338072e-02 3.33989531e-01 5.86927356e-03
 6.42440021e-02 2.01450408e-01 1.58712957e-02 9.97543514e-01
 9.96965468e-01 2.48950603e-03 2.72805601e-01 1.33810146e-02
 8.34485114e-01 9.57331836e-01 1.88837908e-02 1.88779924e-02
 9.71825361e-01 9.85530257e-01 9.99972582e-01 1.69809386e-02
 2.47539859e-02 9.96919632e-01 4.16058660e-01 9.32802111e-02
 9.78589356e-01 9.99207079e-01 2.32283212e-03 2.35238105e-01
 9.92526650e-01 8.60684752e-01 9.78446126e-01 9.91554081e-01
 2.42815651e-02 2.35278338e-01 9.88341868e-01 9.91891742e-01
 9.81908917e-01 1.42003316e-03 8.30184400e-01 9.91553962e-01
 2.57950779e-02 9.98493075e-01 9.95849967e-01 9.89602745e-01
 6.52931213e-01 9.99930143e-01 4.72155809e-02 6.25378668e-01
 9.98543143e-01 9.99546349e-01 5.18759945e-04 9.88552809e-01
 9.99920845e-01 4.85748589e-01 9.94698048e-01 3.88004072e-02
 2.17780963e-01 9.74515736e-01 9.91193593e-01 1.49437174e-01
 1.16417870e-01 2.79389843e-02 9.99785483e-01 9.83404875e-01
 9.63660181e-01 2.51635141e-03 1.13859564e-01 9.57181454e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 12:59:12, Dev, Step : 626, Loss : 0.45576, Acc : 0.805, Auc : 0.881, Sensitive_Loss : 0.15565, Sensitive_Acc : 16.964, Sensitive_Auc : 0.992, Mean auc: 0.881, Run Time : 92.83 sec
INFO:root:2024-04-27 12:59:17, Train, Epoch : 2, Step : 630, Loss : 0.24973, Acc : 0.300, Sensitive_Loss : 0.06715, Sensitive_Acc : 6.000, Run Time : 3.98 sec
INFO:root:2024-04-27 12:59:24, Train, Epoch : 2, Step : 640, Loss : 0.38780, Acc : 0.847, Sensitive_Loss : 0.13375, Sensitive_Acc : 16.700, Run Time : 6.94 sec
INFO:root:2024-04-27 12:59:32, Train, Epoch : 2, Step : 650, Loss : 0.36983, Acc : 0.803, Sensitive_Loss : 0.15824, Sensitive_Acc : 17.300, Run Time : 7.33 sec
INFO:root:2024-04-27 12:59:39, Train, Epoch : 2, Step : 660, Loss : 0.40676, Acc : 0.822, Sensitive_Loss : 0.18738, Sensitive_Acc : 17.600, Run Time : 7.09 sec
INFO:root:2024-04-27 12:59:46, Train, Epoch : 2, Step : 670, Loss : 0.47964, Acc : 0.762, Sensitive_Loss : 0.17249, Sensitive_Acc : 15.000, Run Time : 7.23 sec
INFO:root:2024-04-27 12:59:53, Train, Epoch : 2, Step : 680, Loss : 0.42106, Acc : 0.822, Sensitive_Loss : 0.14493, Sensitive_Acc : 16.200, Run Time : 7.11 sec
INFO:root:2024-04-27 13:00:00, Train, Epoch : 2, Step : 690, Loss : 0.50054, Acc : 0.781, Sensitive_Loss : 0.15494, Sensitive_Acc : 17.500, Run Time : 7.24 sec
INFO:root:2024-04-27 13:00:08, Train, Epoch : 2, Step : 700, Loss : 0.50091, Acc : 0.787, Sensitive_Loss : 0.14466, Sensitive_Acc : 14.800, Run Time : 7.56 sec
INFO:root:2024-04-27 13:01:41, Dev, Step : 700, Loss : 0.46274, Acc : 0.794, Auc : 0.887, Sensitive_Loss : 0.16357, Sensitive_Acc : 16.864, Sensitive_Auc : 0.980, Mean auc: 0.887, Run Time : 92.94 sec
INFO:root:2024-04-27 13:01:41, Best, Step : 700, Loss : 0.46274, Acc : 0.794, Auc : 0.887, Sensitive_Loss : 0.16357, Sensitive_Acc : 16.864, Sensitive_Auc : 0.980, Best Auc : 0.887
INFO:root:2024-04-27 13:01:47, Train, Epoch : 2, Step : 710, Loss : 0.42189, Acc : 0.822, Sensitive_Loss : 0.17538, Sensitive_Acc : 16.100, Run Time : 99.71 sec
INFO:root:2024-04-27 13:01:54, Train, Epoch : 2, Step : 720, Loss : 0.36219, Acc : 0.831, Sensitive_Loss : 0.16776, Sensitive_Acc : 16.500, Run Time : 6.82 sec
INFO:root:2024-04-27 13:02:01, Train, Epoch : 2, Step : 730, Loss : 0.50351, Acc : 0.797, Sensitive_Loss : 0.12467, Sensitive_Acc : 15.400, Run Time : 6.93 sec
INFO:root:2024-04-27 13:02:09, Train, Epoch : 2, Step : 740, Loss : 0.49823, Acc : 0.769, Sensitive_Loss : 0.13617, Sensitive_Acc : 14.100, Run Time : 7.72 sec
INFO:root:2024-04-27 13:02:16, Train, Epoch : 2, Step : 750, Loss : 0.42720, Acc : 0.812, Sensitive_Loss : 0.16571, Sensitive_Acc : 15.700, Run Time : 7.24 sec
INFO:root:2024-04-27 13:02:23, Train, Epoch : 2, Step : 760, Loss : 0.41933, Acc : 0.812, Sensitive_Loss : 0.17964, Sensitive_Acc : 14.600, Run Time : 6.42 sec
INFO:root:2024-04-27 13:02:30, Train, Epoch : 2, Step : 770, Loss : 0.47500, Acc : 0.800, Sensitive_Loss : 0.17443, Sensitive_Acc : 16.400, Run Time : 7.59 sec
INFO:root:2024-04-27 13:02:37, Train, Epoch : 2, Step : 780, Loss : 0.41921, Acc : 0.812, Sensitive_Loss : 0.18552, Sensitive_Acc : 16.400, Run Time : 6.82 sec
INFO:root:2024-04-27 13:02:44, Train, Epoch : 2, Step : 790, Loss : 0.43934, Acc : 0.819, Sensitive_Loss : 0.17663, Sensitive_Acc : 17.400, Run Time : 7.46 sec
INFO:root:2024-04-27 13:02:52, Train, Epoch : 2, Step : 800, Loss : 0.39912, Acc : 0.809, Sensitive_Loss : 0.23098, Sensitive_Acc : 16.800, Run Time : 7.26 sec
INFO:root:2024-04-27 13:04:25, Dev, Step : 800, Loss : 0.46523, Acc : 0.796, Auc : 0.890, Sensitive_Loss : 0.15049, Sensitive_Acc : 16.836, Sensitive_Auc : 0.984, Mean auc: 0.890, Run Time : 93.41 sec
INFO:root:2024-04-27 13:04:26, Best, Step : 800, Loss : 0.46523, Acc : 0.796, Auc : 0.890, Sensitive_Loss : 0.15049, Sensitive_Acc : 16.836, Sensitive_Auc : 0.984, Best Auc : 0.890
INFO:root:2024-04-27 13:04:32, Train, Epoch : 2, Step : 810, Loss : 0.45097, Acc : 0.797, Sensitive_Loss : 0.16236, Sensitive_Acc : 17.000, Run Time : 99.77 sec
INFO:root:2024-04-27 13:04:38, Train, Epoch : 2, Step : 820, Loss : 0.44398, Acc : 0.797, Sensitive_Loss : 0.16483, Sensitive_Acc : 15.500, Run Time : 6.82 sec
INFO:root:2024-04-27 13:04:46, Train, Epoch : 2, Step : 830, Loss : 0.36510, Acc : 0.856, Sensitive_Loss : 0.17889, Sensitive_Acc : 15.900, Run Time : 7.24 sec
INFO:root:2024-04-27 13:04:53, Train, Epoch : 2, Step : 840, Loss : 0.43052, Acc : 0.806, Sensitive_Loss : 0.15452, Sensitive_Acc : 14.900, Run Time : 7.46 sec
INFO:root:2024-04-27 13:05:00, Train, Epoch : 2, Step : 850, Loss : 0.42323, Acc : 0.806, Sensitive_Loss : 0.19366, Sensitive_Acc : 14.100, Run Time : 7.27 sec
INFO:root:2024-04-27 13:05:08, Train, Epoch : 2, Step : 860, Loss : 0.43759, Acc : 0.759, Sensitive_Loss : 0.14860, Sensitive_Acc : 15.500, Run Time : 7.36 sec
INFO:root:2024-04-27 13:05:14, Train, Epoch : 2, Step : 870, Loss : 0.40416, Acc : 0.847, Sensitive_Loss : 0.09626, Sensitive_Acc : 16.800, Run Time : 6.75 sec
INFO:root:2024-04-27 13:05:22, Train, Epoch : 2, Step : 880, Loss : 0.38656, Acc : 0.825, Sensitive_Loss : 0.13582, Sensitive_Acc : 16.800, Run Time : 7.16 sec
INFO:root:2024-04-27 13:05:29, Train, Epoch : 2, Step : 890, Loss : 0.40001, Acc : 0.819, Sensitive_Loss : 0.16349, Sensitive_Acc : 17.100, Run Time : 7.19 sec
INFO:root:2024-04-27 13:05:36, Train, Epoch : 2, Step : 900, Loss : 0.47928, Acc : 0.806, Sensitive_Loss : 0.13688, Sensitive_Acc : 15.600, Run Time : 7.21 sec
INFO:root:2024-04-27 13:07:10, Dev, Step : 900, Loss : 0.59241, Acc : 0.748, Auc : 0.897, Sensitive_Loss : 0.22146, Sensitive_Acc : 16.636, Sensitive_Auc : 0.985, Mean auc: 0.897, Run Time : 93.91 sec
INFO:root:2024-04-27 13:07:11, Best, Step : 900, Loss : 0.59241, Acc : 0.748, Auc : 0.897, Sensitive_Loss : 0.22146, Sensitive_Acc : 16.636, Sensitive_Auc : 0.985, Best Auc : 0.897
INFO:root:2024-04-27 13:07:16, Train, Epoch : 2, Step : 910, Loss : 0.33227, Acc : 0.859, Sensitive_Loss : 0.16204, Sensitive_Acc : 16.700, Run Time : 100.15 sec
INFO:root:2024-04-27 13:07:23, Train, Epoch : 2, Step : 920, Loss : 0.48943, Acc : 0.778, Sensitive_Loss : 0.15696, Sensitive_Acc : 17.100, Run Time : 7.32 sec
INFO:root:2024-04-27 13:07:31, Train, Epoch : 2, Step : 930, Loss : 0.45768, Acc : 0.819, Sensitive_Loss : 0.11267, Sensitive_Acc : 15.500, Run Time : 7.48 sec
INFO:root:2024-04-27 13:07:38, Train, Epoch : 2, Step : 940, Loss : 0.46768, Acc : 0.816, Sensitive_Loss : 0.16136, Sensitive_Acc : 15.500, Run Time : 7.51 sec
INFO:root:2024-04-27 13:07:45, Train, Epoch : 2, Step : 950, Loss : 0.42393, Acc : 0.797, Sensitive_Loss : 0.17888, Sensitive_Acc : 16.100, Run Time : 6.75 sec
INFO:root:2024-04-27 13:07:52, Train, Epoch : 2, Step : 960, Loss : 0.49456, Acc : 0.759, Sensitive_Loss : 0.14956, Sensitive_Acc : 16.900, Run Time : 7.28 sec
INFO:root:2024-04-27 13:07:59, Train, Epoch : 2, Step : 970, Loss : 0.41296, Acc : 0.831, Sensitive_Loss : 0.13035, Sensitive_Acc : 15.600, Run Time : 6.67 sec
INFO:root:2024-04-27 13:08:07, Train, Epoch : 2, Step : 980, Loss : 0.44027, Acc : 0.800, Sensitive_Loss : 0.11685, Sensitive_Acc : 16.400, Run Time : 7.47 sec
INFO:root:2024-04-27 13:08:13, Train, Epoch : 2, Step : 990, Loss : 0.50111, Acc : 0.775, Sensitive_Loss : 0.12076, Sensitive_Acc : 15.000, Run Time : 6.85 sec
INFO:root:2024-04-27 13:08:21, Train, Epoch : 2, Step : 1000, Loss : 0.37868, Acc : 0.816, Sensitive_Loss : 0.20027, Sensitive_Acc : 17.500, Run Time : 7.55 sec
INFO:root:2024-04-27 13:09:55, Dev, Step : 1000, Loss : 0.42843, Acc : 0.812, Auc : 0.893, Sensitive_Loss : 0.13605, Sensitive_Acc : 16.936, Sensitive_Auc : 0.990, Mean auc: 0.893, Run Time : 93.75 sec
INFO:root:2024-04-27 13:10:01, Train, Epoch : 2, Step : 1010, Loss : 0.40695, Acc : 0.828, Sensitive_Loss : 0.14924, Sensitive_Acc : 16.300, Run Time : 99.54 sec
INFO:root:2024-04-27 13:10:08, Train, Epoch : 2, Step : 1020, Loss : 0.38511, Acc : 0.825, Sensitive_Loss : 0.15376, Sensitive_Acc : 16.500, Run Time : 7.03 sec
INFO:root:2024-04-27 13:10:15, Train, Epoch : 2, Step : 1030, Loss : 0.49867, Acc : 0.778, Sensitive_Loss : 0.13175, Sensitive_Acc : 16.100, Run Time : 7.00 sec
INFO:root:2024-04-27 13:10:22, Train, Epoch : 2, Step : 1040, Loss : 0.41058, Acc : 0.822, Sensitive_Loss : 0.18021, Sensitive_Acc : 15.100, Run Time : 7.04 sec
INFO:root:2024-04-27 13:10:29, Train, Epoch : 2, Step : 1050, Loss : 0.47568, Acc : 0.806, Sensitive_Loss : 0.11745, Sensitive_Acc : 15.100, Run Time : 7.52 sec
INFO:root:2024-04-27 13:10:36, Train, Epoch : 2, Step : 1060, Loss : 0.48416, Acc : 0.794, Sensitive_Loss : 0.13394, Sensitive_Acc : 17.000, Run Time : 7.30 sec
INFO:root:2024-04-27 13:10:43, Train, Epoch : 2, Step : 1070, Loss : 0.36030, Acc : 0.853, Sensitive_Loss : 0.17782, Sensitive_Acc : 14.900, Run Time : 6.84 sec
INFO:root:2024-04-27 13:10:50, Train, Epoch : 2, Step : 1080, Loss : 0.35924, Acc : 0.838, Sensitive_Loss : 0.13581, Sensitive_Acc : 15.300, Run Time : 7.15 sec
INFO:root:2024-04-27 13:10:58, Train, Epoch : 2, Step : 1090, Loss : 0.36924, Acc : 0.838, Sensitive_Loss : 0.12056, Sensitive_Acc : 16.200, Run Time : 7.30 sec
INFO:root:2024-04-27 13:11:05, Train, Epoch : 2, Step : 1100, Loss : 0.37601, Acc : 0.828, Sensitive_Loss : 0.12872, Sensitive_Acc : 17.500, Run Time : 7.33 sec
INFO:root:2024-04-27 13:12:38, Dev, Step : 1100, Loss : 0.45060, Acc : 0.800, Auc : 0.898, Sensitive_Loss : 0.13093, Sensitive_Acc : 16.979, Sensitive_Auc : 0.993, Mean auc: 0.898, Run Time : 93.32 sec
INFO:root:2024-04-27 13:12:39, Best, Step : 1100, Loss : 0.45060, Acc : 0.800, Auc : 0.898, Sensitive_Loss : 0.13093, Sensitive_Acc : 16.979, Sensitive_Auc : 0.993, Best Auc : 0.898
INFO:root:2024-04-27 13:12:45, Train, Epoch : 2, Step : 1110, Loss : 0.35391, Acc : 0.828, Sensitive_Loss : 0.15456, Sensitive_Acc : 17.000, Run Time : 100.15 sec
INFO:root:2024-04-27 13:12:52, Train, Epoch : 2, Step : 1120, Loss : 0.35610, Acc : 0.838, Sensitive_Loss : 0.12770, Sensitive_Acc : 16.600, Run Time : 6.65 sec
INFO:root:2024-04-27 13:12:59, Train, Epoch : 2, Step : 1130, Loss : 0.42149, Acc : 0.838, Sensitive_Loss : 0.12032, Sensitive_Acc : 16.100, Run Time : 7.19 sec
INFO:root:2024-04-27 13:13:06, Train, Epoch : 2, Step : 1140, Loss : 0.41400, Acc : 0.812, Sensitive_Loss : 0.16198, Sensitive_Acc : 16.000, Run Time : 6.91 sec
INFO:root:2024-04-27 13:13:13, Train, Epoch : 2, Step : 1150, Loss : 0.40899, Acc : 0.828, Sensitive_Loss : 0.18859, Sensitive_Acc : 14.300, Run Time : 7.50 sec
INFO:root:2024-04-27 13:13:21, Train, Epoch : 2, Step : 1160, Loss : 0.49185, Acc : 0.797, Sensitive_Loss : 0.09739, Sensitive_Acc : 15.700, Run Time : 7.18 sec
INFO:root:2024-04-27 13:13:28, Train, Epoch : 2, Step : 1170, Loss : 0.36824, Acc : 0.844, Sensitive_Loss : 0.12790, Sensitive_Acc : 15.200, Run Time : 6.89 sec
INFO:root:2024-04-27 13:13:35, Train, Epoch : 2, Step : 1180, Loss : 0.34318, Acc : 0.856, Sensitive_Loss : 0.12255, Sensitive_Acc : 16.300, Run Time : 7.87 sec
INFO:root:2024-04-27 13:13:43, Train, Epoch : 2, Step : 1190, Loss : 0.42491, Acc : 0.800, Sensitive_Loss : 0.17123, Sensitive_Acc : 17.200, Run Time : 7.23 sec
INFO:root:2024-04-27 13:13:50, Train, Epoch : 2, Step : 1200, Loss : 0.38705, Acc : 0.850, Sensitive_Loss : 0.17027, Sensitive_Acc : 15.500, Run Time : 7.60 sec
INFO:root:2024-04-27 13:15:23, Dev, Step : 1200, Loss : 0.46405, Acc : 0.800, Auc : 0.899, Sensitive_Loss : 0.21527, Sensitive_Acc : 16.807, Sensitive_Auc : 0.992, Mean auc: 0.899, Run Time : 93.20 sec
INFO:root:2024-04-27 13:15:24, Best, Step : 1200, Loss : 0.46405, Acc : 0.800, Auc : 0.899, Sensitive_Loss : 0.21527, Sensitive_Acc : 16.807, Sensitive_Auc : 0.992, Best Auc : 0.899
INFO:root:2024-04-27 13:15:30, Train, Epoch : 2, Step : 1210, Loss : 0.39723, Acc : 0.831, Sensitive_Loss : 0.15329, Sensitive_Acc : 16.200, Run Time : 99.69 sec
INFO:root:2024-04-27 13:15:38, Train, Epoch : 2, Step : 1220, Loss : 0.37936, Acc : 0.812, Sensitive_Loss : 0.12861, Sensitive_Acc : 14.900, Run Time : 7.67 sec
INFO:root:2024-04-27 13:15:44, Train, Epoch : 2, Step : 1230, Loss : 0.36747, Acc : 0.838, Sensitive_Loss : 0.12821, Sensitive_Acc : 14.900, Run Time : 6.21 sec
INFO:root:2024-04-27 13:15:51, Train, Epoch : 2, Step : 1240, Loss : 0.47701, Acc : 0.819, Sensitive_Loss : 0.13860, Sensitive_Acc : 15.800, Run Time : 7.34 sec
INFO:root:2024-04-27 13:15:59, Train, Epoch : 2, Step : 1250, Loss : 0.42299, Acc : 0.809, Sensitive_Loss : 0.12984, Sensitive_Acc : 15.400, Run Time : 7.37 sec
INFO:root:2024-04-27 13:17:32
INFO:root:y_pred: [0.01186546 0.84169215 0.01412941 ... 0.6880514  0.01324823 0.5576451 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.9636239e-01 1.4984116e-03 2.4610558e-01 4.9319206e-05 9.9999535e-01
 3.8107398e-03 1.0000000e+00 9.9999404e-01 1.4756457e-02 9.1508621e-01
 9.9837732e-01 9.9997759e-01 9.9911255e-01 9.9626559e-01 2.7067345e-01
 9.4617450e-01 9.9988914e-01 1.6882578e-02 3.8131514e-01 9.1396445e-01
 9.9992383e-01 2.6287475e-01 9.9984097e-01 9.8894638e-01 9.9980205e-01
 9.9969375e-01 1.8105290e-03 9.9999464e-01 9.9990034e-01 9.4229937e-01
 2.1129761e-02 4.1791955e-01 6.8721557e-01 1.0707185e-02 1.6906774e-01
 6.9193423e-02 8.1152433e-01 8.8936463e-03 9.9989355e-01 9.9996614e-01
 1.1469306e-03 4.5690625e-03 9.9112809e-01 9.8424149e-04 9.9999988e-01
 9.9950695e-01 9.9997020e-01 9.8963064e-01 5.5119945e-03 9.9818558e-01
 9.9950361e-01 2.7057815e-01 9.1810691e-01 1.4771980e-03 5.8519962e-04
 6.2296800e-02 9.2317417e-02 2.2780080e-01 4.2307479e-03 1.3941343e-01
 5.4104370e-03 2.3082004e-01 9.1107070e-02 9.7498214e-01 6.4049673e-01
 9.9999905e-01 1.1938571e-02 9.9992633e-01 9.9931908e-01 9.7078133e-01
 8.6778468e-01 7.2255516e-01 6.9774603e-03 6.8339610e-01 1.1902583e-01
 1.2892215e-03 6.1273694e-01 1.7802285e-01 2.3782675e-03 9.9997497e-01
 9.9996829e-01 1.4731319e-03 7.7242456e-02 5.0148271e-02 9.8451024e-01
 9.9240029e-01 3.7633494e-02 2.2217087e-02 9.8888260e-01 9.9949503e-01
 9.9999964e-01 1.9708181e-02 4.6016281e-03 9.9992096e-01 3.6969638e-01
 1.8759843e-02 9.9896586e-01 9.9998665e-01 7.6870818e-04 1.7295042e-01
 9.9989927e-01 9.9896693e-01 9.9904698e-01 9.9783945e-01 9.3662411e-02
 1.2899350e-01 9.9882084e-01 9.9914861e-01 9.9159765e-01 1.5983672e-03
 9.9954104e-01 9.9971801e-01 6.2699407e-02 9.9996114e-01 9.9992514e-01
 9.9978191e-01 5.9390366e-01 9.9999440e-01 2.4051531e-01 9.6433371e-01
 9.9992561e-01 9.9992836e-01 4.5320409e-04 9.9977714e-01 9.9999976e-01
 2.3730795e-01 9.9993801e-01 1.4699301e-01 2.0932971e-02 9.8321044e-01
 9.9984539e-01 9.5597738e-03 6.6452318e-01 1.4937751e-02 9.9999511e-01
 9.9975830e-01 9.9452317e-01 6.3931338e-02 1.4712137e-01 9.9681818e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 13:17:32, Dev, Step : 1252, Loss : 0.52656, Acc : 0.770, Auc : 0.898, Sensitive_Loss : 0.24125, Sensitive_Acc : 16.650, Sensitive_Auc : 0.987, Mean auc: 0.898, Run Time : 92.91 sec
INFO:root:2024-04-27 13:17:40, Train, Epoch : 3, Step : 1260, Loss : 0.32437, Acc : 0.672, Sensitive_Loss : 0.08407, Sensitive_Acc : 13.900, Run Time : 6.85 sec
INFO:root:2024-04-27 13:17:47, Train, Epoch : 3, Step : 1270, Loss : 0.35134, Acc : 0.859, Sensitive_Loss : 0.13544, Sensitive_Acc : 17.400, Run Time : 7.14 sec
INFO:root:2024-04-27 13:17:55, Train, Epoch : 3, Step : 1280, Loss : 0.38327, Acc : 0.838, Sensitive_Loss : 0.10832, Sensitive_Acc : 16.400, Run Time : 7.28 sec
INFO:root:2024-04-27 13:18:02, Train, Epoch : 3, Step : 1290, Loss : 0.37693, Acc : 0.844, Sensitive_Loss : 0.12557, Sensitive_Acc : 17.000, Run Time : 7.31 sec
INFO:root:2024-04-27 13:18:09, Train, Epoch : 3, Step : 1300, Loss : 0.34781, Acc : 0.831, Sensitive_Loss : 0.14757, Sensitive_Acc : 17.300, Run Time : 7.16 sec
INFO:root:2024-04-27 13:19:44, Dev, Step : 1300, Loss : 0.40770, Acc : 0.822, Auc : 0.906, Sensitive_Loss : 0.13702, Sensitive_Acc : 16.793, Sensitive_Auc : 0.990, Mean auc: 0.906, Run Time : 94.67 sec
INFO:root:2024-04-27 13:19:44, Best, Step : 1300, Loss : 0.40770, Acc : 0.822, Auc : 0.906, Sensitive_Loss : 0.13702, Sensitive_Acc : 16.793, Sensitive_Auc : 0.990, Best Auc : 0.906
INFO:root:2024-04-27 13:19:50, Train, Epoch : 3, Step : 1310, Loss : 0.36029, Acc : 0.816, Sensitive_Loss : 0.12731, Sensitive_Acc : 16.300, Run Time : 101.04 sec
INFO:root:2024-04-27 13:19:57, Train, Epoch : 3, Step : 1320, Loss : 0.39213, Acc : 0.825, Sensitive_Loss : 0.12017, Sensitive_Acc : 14.900, Run Time : 7.28 sec
INFO:root:2024-04-27 13:20:04, Train, Epoch : 3, Step : 1330, Loss : 0.35866, Acc : 0.838, Sensitive_Loss : 0.09387, Sensitive_Acc : 17.100, Run Time : 6.73 sec
INFO:root:2024-04-27 13:20:12, Train, Epoch : 3, Step : 1340, Loss : 0.32502, Acc : 0.844, Sensitive_Loss : 0.11829, Sensitive_Acc : 15.300, Run Time : 7.70 sec
INFO:root:2024-04-27 13:20:19, Train, Epoch : 3, Step : 1350, Loss : 0.29761, Acc : 0.872, Sensitive_Loss : 0.11064, Sensitive_Acc : 17.100, Run Time : 7.50 sec
INFO:root:2024-04-27 13:20:26, Train, Epoch : 3, Step : 1360, Loss : 0.37602, Acc : 0.853, Sensitive_Loss : 0.10748, Sensitive_Acc : 15.300, Run Time : 6.64 sec
INFO:root:2024-04-27 13:20:33, Train, Epoch : 3, Step : 1370, Loss : 0.39174, Acc : 0.797, Sensitive_Loss : 0.09663, Sensitive_Acc : 16.700, Run Time : 7.18 sec
INFO:root:2024-04-27 13:20:41, Train, Epoch : 3, Step : 1380, Loss : 0.34322, Acc : 0.875, Sensitive_Loss : 0.11796, Sensitive_Acc : 15.300, Run Time : 7.53 sec
INFO:root:2024-04-27 13:20:48, Train, Epoch : 3, Step : 1390, Loss : 0.35832, Acc : 0.816, Sensitive_Loss : 0.14340, Sensitive_Acc : 15.400, Run Time : 7.08 sec
INFO:root:2024-04-27 13:20:54, Train, Epoch : 3, Step : 1400, Loss : 0.38538, Acc : 0.831, Sensitive_Loss : 0.07931, Sensitive_Acc : 16.600, Run Time : 6.57 sec
INFO:root:2024-04-27 13:22:28, Dev, Step : 1400, Loss : 0.41421, Acc : 0.822, Auc : 0.909, Sensitive_Loss : 0.13971, Sensitive_Acc : 16.721, Sensitive_Auc : 0.991, Mean auc: 0.909, Run Time : 93.39 sec
INFO:root:2024-04-27 13:22:28, Best, Step : 1400, Loss : 0.41421, Acc : 0.822, Auc : 0.909, Sensitive_Loss : 0.13971, Sensitive_Acc : 16.721, Sensitive_Auc : 0.991, Best Auc : 0.909
INFO:root:2024-04-27 13:22:34, Train, Epoch : 3, Step : 1410, Loss : 0.36425, Acc : 0.847, Sensitive_Loss : 0.11255, Sensitive_Acc : 16.300, Run Time : 99.51 sec
INFO:root:2024-04-27 13:22:41, Train, Epoch : 3, Step : 1420, Loss : 0.33823, Acc : 0.838, Sensitive_Loss : 0.10492, Sensitive_Acc : 15.100, Run Time : 7.03 sec
INFO:root:2024-04-27 13:22:48, Train, Epoch : 3, Step : 1430, Loss : 0.36051, Acc : 0.838, Sensitive_Loss : 0.12288, Sensitive_Acc : 16.200, Run Time : 7.35 sec
INFO:root:2024-04-27 13:22:55, Train, Epoch : 3, Step : 1440, Loss : 0.29488, Acc : 0.875, Sensitive_Loss : 0.13091, Sensitive_Acc : 16.400, Run Time : 6.84 sec
INFO:root:2024-04-27 13:23:03, Train, Epoch : 3, Step : 1450, Loss : 0.33741, Acc : 0.859, Sensitive_Loss : 0.10719, Sensitive_Acc : 16.700, Run Time : 7.91 sec
INFO:root:2024-04-27 13:23:10, Train, Epoch : 3, Step : 1460, Loss : 0.36524, Acc : 0.838, Sensitive_Loss : 0.11403, Sensitive_Acc : 17.700, Run Time : 7.10 sec
INFO:root:2024-04-27 13:23:17, Train, Epoch : 3, Step : 1470, Loss : 0.36616, Acc : 0.853, Sensitive_Loss : 0.10461, Sensitive_Acc : 17.700, Run Time : 6.52 sec
INFO:root:2024-04-27 13:23:24, Train, Epoch : 3, Step : 1480, Loss : 0.32595, Acc : 0.856, Sensitive_Loss : 0.16995, Sensitive_Acc : 17.600, Run Time : 7.51 sec
INFO:root:2024-04-27 13:23:31, Train, Epoch : 3, Step : 1490, Loss : 0.32320, Acc : 0.872, Sensitive_Loss : 0.12141, Sensitive_Acc : 16.500, Run Time : 6.87 sec
INFO:root:2024-04-27 13:23:38, Train, Epoch : 3, Step : 1500, Loss : 0.43308, Acc : 0.831, Sensitive_Loss : 0.08449, Sensitive_Acc : 15.000, Run Time : 7.47 sec
INFO:root:2024-04-27 13:25:12, Dev, Step : 1500, Loss : 0.40543, Acc : 0.826, Auc : 0.912, Sensitive_Loss : 0.13036, Sensitive_Acc : 16.750, Sensitive_Auc : 0.991, Mean auc: 0.912, Run Time : 93.48 sec
INFO:root:2024-04-27 13:25:13, Best, Step : 1500, Loss : 0.40543, Acc : 0.826, Auc : 0.912, Sensitive_Loss : 0.13036, Sensitive_Acc : 16.750, Sensitive_Auc : 0.991, Best Auc : 0.912
INFO:root:2024-04-27 13:25:18, Train, Epoch : 3, Step : 1510, Loss : 0.34857, Acc : 0.844, Sensitive_Loss : 0.09298, Sensitive_Acc : 15.400, Run Time : 99.71 sec
INFO:root:2024-04-27 13:25:26, Train, Epoch : 3, Step : 1520, Loss : 0.36053, Acc : 0.841, Sensitive_Loss : 0.10565, Sensitive_Acc : 15.600, Run Time : 7.42 sec
INFO:root:2024-04-27 13:25:33, Train, Epoch : 3, Step : 1530, Loss : 0.35679, Acc : 0.850, Sensitive_Loss : 0.12654, Sensitive_Acc : 16.900, Run Time : 7.27 sec
INFO:root:2024-04-27 13:25:40, Train, Epoch : 3, Step : 1540, Loss : 0.38357, Acc : 0.844, Sensitive_Loss : 0.11005, Sensitive_Acc : 14.700, Run Time : 7.28 sec
INFO:root:2024-04-27 13:25:47, Train, Epoch : 3, Step : 1550, Loss : 0.41974, Acc : 0.831, Sensitive_Loss : 0.10504, Sensitive_Acc : 15.600, Run Time : 7.21 sec
INFO:root:2024-04-27 13:25:54, Train, Epoch : 3, Step : 1560, Loss : 0.40349, Acc : 0.806, Sensitive_Loss : 0.11499, Sensitive_Acc : 15.900, Run Time : 7.11 sec
INFO:root:2024-04-27 13:26:01, Train, Epoch : 3, Step : 1570, Loss : 0.25059, Acc : 0.887, Sensitive_Loss : 0.09647, Sensitive_Acc : 16.900, Run Time : 6.75 sec
INFO:root:2024-04-27 13:26:09, Train, Epoch : 3, Step : 1580, Loss : 0.36768, Acc : 0.841, Sensitive_Loss : 0.09521, Sensitive_Acc : 15.300, Run Time : 7.43 sec
INFO:root:2024-04-27 13:26:16, Train, Epoch : 3, Step : 1590, Loss : 0.34565, Acc : 0.863, Sensitive_Loss : 0.14876, Sensitive_Acc : 16.100, Run Time : 7.20 sec
INFO:root:2024-04-27 13:26:23, Train, Epoch : 3, Step : 1600, Loss : 0.38485, Acc : 0.819, Sensitive_Loss : 0.11224, Sensitive_Acc : 16.200, Run Time : 6.76 sec
INFO:root:2024-04-27 13:27:56, Dev, Step : 1600, Loss : 0.40204, Acc : 0.827, Auc : 0.912, Sensitive_Loss : 0.12222, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992, Mean auc: 0.912, Run Time : 93.76 sec
INFO:root:2024-04-27 13:27:57, Best, Step : 1600, Loss : 0.40204, Acc : 0.827, Auc : 0.912, Sensitive_Loss : 0.12222, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992, Best Auc : 0.912
INFO:root:2024-04-27 13:28:03, Train, Epoch : 3, Step : 1610, Loss : 0.33213, Acc : 0.850, Sensitive_Loss : 0.06960, Sensitive_Acc : 14.900, Run Time : 100.28 sec
INFO:root:2024-04-27 13:28:10, Train, Epoch : 3, Step : 1620, Loss : 0.30438, Acc : 0.853, Sensitive_Loss : 0.11975, Sensitive_Acc : 16.500, Run Time : 7.28 sec
INFO:root:2024-04-27 13:28:17, Train, Epoch : 3, Step : 1630, Loss : 0.27664, Acc : 0.863, Sensitive_Loss : 0.12301, Sensitive_Acc : 17.400, Run Time : 7.15 sec
INFO:root:2024-04-27 13:28:25, Train, Epoch : 3, Step : 1640, Loss : 0.35357, Acc : 0.869, Sensitive_Loss : 0.10549, Sensitive_Acc : 17.700, Run Time : 7.48 sec
INFO:root:2024-04-27 13:28:31, Train, Epoch : 3, Step : 1650, Loss : 0.36482, Acc : 0.828, Sensitive_Loss : 0.13982, Sensitive_Acc : 15.800, Run Time : 6.54 sec
INFO:root:2024-04-27 13:28:39, Train, Epoch : 3, Step : 1660, Loss : 0.35194, Acc : 0.863, Sensitive_Loss : 0.07569, Sensitive_Acc : 15.700, Run Time : 7.67 sec
INFO:root:2024-04-27 13:28:46, Train, Epoch : 3, Step : 1670, Loss : 0.36388, Acc : 0.850, Sensitive_Loss : 0.13295, Sensitive_Acc : 16.100, Run Time : 7.22 sec
INFO:root:2024-04-27 13:28:53, Train, Epoch : 3, Step : 1680, Loss : 0.41870, Acc : 0.819, Sensitive_Loss : 0.13414, Sensitive_Acc : 15.500, Run Time : 6.92 sec
INFO:root:2024-04-27 13:29:00, Train, Epoch : 3, Step : 1690, Loss : 0.33499, Acc : 0.844, Sensitive_Loss : 0.13987, Sensitive_Acc : 15.800, Run Time : 6.90 sec
INFO:root:2024-04-27 13:29:07, Train, Epoch : 3, Step : 1700, Loss : 0.31677, Acc : 0.863, Sensitive_Loss : 0.07386, Sensitive_Acc : 15.700, Run Time : 7.42 sec
INFO:root:2024-04-27 13:30:41, Dev, Step : 1700, Loss : 0.40994, Acc : 0.822, Auc : 0.913, Sensitive_Loss : 0.13606, Sensitive_Acc : 16.850, Sensitive_Auc : 0.991, Mean auc: 0.913, Run Time : 93.36 sec
INFO:root:2024-04-27 13:30:42, Best, Step : 1700, Loss : 0.40994, Acc : 0.822, Auc : 0.913, Sensitive_Loss : 0.13606, Sensitive_Acc : 16.850, Sensitive_Auc : 0.991, Best Auc : 0.913
INFO:root:2024-04-27 13:30:47, Train, Epoch : 3, Step : 1710, Loss : 0.35301, Acc : 0.847, Sensitive_Loss : 0.08520, Sensitive_Acc : 15.400, Run Time : 99.92 sec
INFO:root:2024-04-27 13:30:55, Train, Epoch : 3, Step : 1720, Loss : 0.30706, Acc : 0.863, Sensitive_Loss : 0.12375, Sensitive_Acc : 15.400, Run Time : 7.25 sec
INFO:root:2024-04-27 13:31:02, Train, Epoch : 3, Step : 1730, Loss : 0.33991, Acc : 0.828, Sensitive_Loss : 0.08537, Sensitive_Acc : 16.400, Run Time : 7.00 sec
INFO:root:2024-04-27 13:31:09, Train, Epoch : 3, Step : 1740, Loss : 0.37814, Acc : 0.847, Sensitive_Loss : 0.10687, Sensitive_Acc : 15.800, Run Time : 7.16 sec
INFO:root:2024-04-27 13:31:16, Train, Epoch : 3, Step : 1750, Loss : 0.36166, Acc : 0.844, Sensitive_Loss : 0.13180, Sensitive_Acc : 16.700, Run Time : 7.27 sec
INFO:root:2024-04-27 13:31:23, Train, Epoch : 3, Step : 1760, Loss : 0.34453, Acc : 0.847, Sensitive_Loss : 0.08740, Sensitive_Acc : 16.100, Run Time : 7.18 sec
INFO:root:2024-04-27 13:31:30, Train, Epoch : 3, Step : 1770, Loss : 0.34789, Acc : 0.841, Sensitive_Loss : 0.07097, Sensitive_Acc : 15.800, Run Time : 6.95 sec
INFO:root:2024-04-27 13:31:38, Train, Epoch : 3, Step : 1780, Loss : 0.30109, Acc : 0.872, Sensitive_Loss : 0.08971, Sensitive_Acc : 17.000, Run Time : 7.36 sec
INFO:root:2024-04-27 13:31:44, Train, Epoch : 3, Step : 1790, Loss : 0.35393, Acc : 0.875, Sensitive_Loss : 0.12301, Sensitive_Acc : 16.200, Run Time : 6.80 sec
INFO:root:2024-04-27 13:31:52, Train, Epoch : 3, Step : 1800, Loss : 0.31999, Acc : 0.869, Sensitive_Loss : 0.08703, Sensitive_Acc : 14.000, Run Time : 7.92 sec
INFO:root:2024-04-27 13:33:26, Dev, Step : 1800, Loss : 0.40602, Acc : 0.826, Auc : 0.913, Sensitive_Loss : 0.13254, Sensitive_Acc : 16.736, Sensitive_Auc : 0.990, Mean auc: 0.913, Run Time : 93.40 sec
INFO:root:2024-04-27 13:33:31, Train, Epoch : 3, Step : 1810, Loss : 0.32748, Acc : 0.863, Sensitive_Loss : 0.07143, Sensitive_Acc : 16.600, Run Time : 98.73 sec
INFO:root:2024-04-27 13:33:38, Train, Epoch : 3, Step : 1820, Loss : 0.33307, Acc : 0.856, Sensitive_Loss : 0.11474, Sensitive_Acc : 15.700, Run Time : 7.08 sec
INFO:root:2024-04-27 13:33:45, Train, Epoch : 3, Step : 1830, Loss : 0.35004, Acc : 0.856, Sensitive_Loss : 0.10594, Sensitive_Acc : 17.100, Run Time : 7.44 sec
INFO:root:2024-04-27 13:33:53, Train, Epoch : 3, Step : 1840, Loss : 0.35290, Acc : 0.834, Sensitive_Loss : 0.07772, Sensitive_Acc : 16.900, Run Time : 7.36 sec
INFO:root:2024-04-27 13:34:00, Train, Epoch : 3, Step : 1850, Loss : 0.31863, Acc : 0.869, Sensitive_Loss : 0.08746, Sensitive_Acc : 15.900, Run Time : 7.42 sec
INFO:root:2024-04-27 13:34:07, Train, Epoch : 3, Step : 1860, Loss : 0.33775, Acc : 0.844, Sensitive_Loss : 0.08682, Sensitive_Acc : 15.600, Run Time : 7.05 sec
INFO:root:2024-04-27 13:34:14, Train, Epoch : 3, Step : 1870, Loss : 0.41487, Acc : 0.834, Sensitive_Loss : 0.11046, Sensitive_Acc : 15.500, Run Time : 6.86 sec
INFO:root:2024-04-27 13:35:52
INFO:root:y_pred: [0.0319919  0.9698635  0.03788588 ... 0.74575984 0.00623429 0.8408042 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.73555505e-01 1.22995640e-03 1.06210947e-01 2.31273625e-05
 9.99800146e-01 7.12301000e-04 9.99997973e-01 9.99979854e-01
 5.48910256e-03 8.66097748e-01 9.98075366e-01 9.99882579e-01
 9.95408118e-01 9.96865809e-01 5.78678846e-02 9.18132722e-01
 9.99203861e-01 2.14389767e-02 3.59212726e-01 8.39141011e-01
 9.99616146e-01 3.02570220e-02 9.99202073e-01 9.95171487e-01
 9.99446809e-01 9.98631895e-01 5.13351333e-05 9.99875188e-01
 9.99537587e-01 5.44216871e-01 1.14632184e-02 1.13189720e-01
 4.21094298e-01 6.56049373e-03 7.09057227e-02 1.98533107e-02
 4.43092614e-01 6.81625772e-03 9.98887360e-01 9.99686003e-01
 2.14075466e-04 4.19849588e-04 9.60355043e-01 3.47568566e-04
 9.99995112e-01 9.98859406e-01 9.99658704e-01 9.80220079e-01
 9.13207885e-04 9.95856345e-01 9.97176886e-01 1.15717910e-01
 4.17935610e-01 2.40080277e-04 1.78187518e-04 1.56723186e-02
 5.86975273e-03 1.22364387e-02 6.15881756e-04 9.64348614e-02
 4.61289892e-03 7.70548508e-02 1.69931147e-02 9.55452442e-01
 6.50346279e-01 9.99970198e-01 2.73916172e-03 9.99800861e-01
 9.99029517e-01 9.51129794e-01 4.18134362e-01 3.61623347e-01
 1.58002810e-03 1.25340611e-01 3.44315954e-02 4.33577545e-04
 1.43921673e-01 1.17622934e-01 6.80428173e-04 9.99919295e-01
 9.99861836e-01 2.72068486e-04 2.21392885e-02 6.06918940e-03
 9.60475981e-01 9.44793940e-01 1.36931678e-02 2.46719122e-02
 9.50905740e-01 9.98877108e-01 9.99987006e-01 7.27734528e-03
 1.90349482e-03 9.98577237e-01 3.05000573e-01 2.05138437e-02
 9.92164016e-01 9.99825180e-01 5.45655421e-05 2.86541153e-02
 9.99342859e-01 9.97888267e-01 9.98941123e-01 9.93423462e-01
 6.65257312e-03 5.11427298e-02 9.95538116e-01 9.98555362e-01
 9.66126978e-01 5.57017920e-05 9.98270750e-01 9.94624078e-01
 1.63019430e-02 9.99644995e-01 9.99721229e-01 9.98724282e-01
 3.96914065e-01 9.99939084e-01 1.45518258e-01 5.73730648e-01
 9.99376476e-01 9.99534011e-01 1.20456854e-04 9.98744488e-01
 9.99990106e-01 2.17859626e-01 9.99242067e-01 1.44748101e-02
 8.11422803e-03 9.61198270e-01 9.98910069e-01 1.03046000e-03
 5.58561869e-02 5.34489611e-03 9.99832869e-01 9.97542739e-01
 9.91048396e-01 3.53437918e-03 2.29099449e-02 9.94024932e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 13:35:53, Dev, Step : 1878, Loss : 0.40466, Acc : 0.827, Auc : 0.914, Sensitive_Loss : 0.13575, Sensitive_Acc : 16.793, Sensitive_Auc : 0.991, Mean auc: 0.914, Run Time : 92.99 sec
INFO:root:2024-04-27 13:35:53, Best, Step : 1878, Loss : 0.40466, Acc : 0.827,Auc : 0.914, Best Auc : 0.914, Sensitive_Loss : 0.13575, Sensitive_Acc : 16.793, Sensitive_Auc : 0.991
INFO:root:2024-04-27 13:35:57, Train, Epoch : 4, Step : 1880, Loss : 0.08599, Acc : 0.153, Sensitive_Loss : 0.01035, Sensitive_Acc : 2.800, Run Time : 2.91 sec
INFO:root:2024-04-27 13:36:04, Train, Epoch : 4, Step : 1890, Loss : 0.27552, Acc : 0.887, Sensitive_Loss : 0.09783, Sensitive_Acc : 15.400, Run Time : 7.04 sec
INFO:root:2024-04-27 13:36:11, Train, Epoch : 4, Step : 1900, Loss : 0.27585, Acc : 0.875, Sensitive_Loss : 0.13856, Sensitive_Acc : 17.800, Run Time : 7.21 sec
INFO:root:2024-04-27 13:37:44, Dev, Step : 1900, Loss : 0.40824, Acc : 0.828, Auc : 0.914, Sensitive_Loss : 0.13670, Sensitive_Acc : 16.793, Sensitive_Auc : 0.991, Mean auc: 0.914, Run Time : 93.00 sec
INFO:root:2024-04-27 13:37:50, Train, Epoch : 4, Step : 1910, Loss : 0.33180, Acc : 0.859, Sensitive_Loss : 0.07504, Sensitive_Acc : 16.100, Run Time : 98.99 sec
INFO:root:2024-04-27 13:37:58, Train, Epoch : 4, Step : 1920, Loss : 0.34309, Acc : 0.834, Sensitive_Loss : 0.10322, Sensitive_Acc : 16.800, Run Time : 7.49 sec
INFO:root:2024-04-27 13:38:05, Train, Epoch : 4, Step : 1930, Loss : 0.25593, Acc : 0.894, Sensitive_Loss : 0.11591, Sensitive_Acc : 15.700, Run Time : 6.83 sec
INFO:root:2024-04-27 13:38:12, Train, Epoch : 4, Step : 1940, Loss : 0.25146, Acc : 0.909, Sensitive_Loss : 0.09455, Sensitive_Acc : 16.100, Run Time : 7.53 sec
INFO:root:2024-04-27 13:38:19, Train, Epoch : 4, Step : 1950, Loss : 0.30328, Acc : 0.859, Sensitive_Loss : 0.08294, Sensitive_Acc : 17.200, Run Time : 7.22 sec
INFO:root:2024-04-27 13:38:27, Train, Epoch : 4, Step : 1960, Loss : 0.31786, Acc : 0.841, Sensitive_Loss : 0.12686, Sensitive_Acc : 15.900, Run Time : 7.33 sec
INFO:root:2024-04-27 13:38:33, Train, Epoch : 4, Step : 1970, Loss : 0.26110, Acc : 0.891, Sensitive_Loss : 0.10038, Sensitive_Acc : 17.600, Run Time : 6.37 sec
INFO:root:2024-04-27 13:38:41, Train, Epoch : 4, Step : 1980, Loss : 0.26225, Acc : 0.906, Sensitive_Loss : 0.13775, Sensitive_Acc : 15.600, Run Time : 7.66 sec
INFO:root:2024-04-27 13:38:48, Train, Epoch : 4, Step : 1990, Loss : 0.33759, Acc : 0.844, Sensitive_Loss : 0.11519, Sensitive_Acc : 17.300, Run Time : 7.18 sec
INFO:root:2024-04-27 13:38:55, Train, Epoch : 4, Step : 2000, Loss : 0.30273, Acc : 0.891, Sensitive_Loss : 0.07447, Sensitive_Acc : 17.200, Run Time : 7.24 sec
INFO:root:2024-04-27 13:40:29, Dev, Step : 2000, Loss : 0.39727, Acc : 0.826, Auc : 0.913, Sensitive_Loss : 0.12007, Sensitive_Acc : 16.821, Sensitive_Auc : 0.991, Mean auc: 0.913, Run Time : 93.39 sec
INFO:root:2024-04-27 13:40:34, Train, Epoch : 4, Step : 2010, Loss : 0.31389, Acc : 0.856, Sensitive_Loss : 0.10226, Sensitive_Acc : 15.900, Run Time : 98.79 sec
INFO:root:2024-04-27 13:40:41, Train, Epoch : 4, Step : 2020, Loss : 0.27465, Acc : 0.881, Sensitive_Loss : 0.10191, Sensitive_Acc : 17.300, Run Time : 7.38 sec
INFO:root:2024-04-27 13:40:48, Train, Epoch : 4, Step : 2030, Loss : 0.37941, Acc : 0.838, Sensitive_Loss : 0.08786, Sensitive_Acc : 16.000, Run Time : 7.16 sec
INFO:root:2024-04-27 13:40:56, Train, Epoch : 4, Step : 2040, Loss : 0.38127, Acc : 0.841, Sensitive_Loss : 0.09053, Sensitive_Acc : 17.000, Run Time : 7.75 sec
INFO:root:2024-04-27 13:41:04, Train, Epoch : 4, Step : 2050, Loss : 0.31381, Acc : 0.866, Sensitive_Loss : 0.13094, Sensitive_Acc : 15.900, Run Time : 7.32 sec
INFO:root:2024-04-27 13:41:11, Train, Epoch : 4, Step : 2060, Loss : 0.31724, Acc : 0.841, Sensitive_Loss : 0.07307, Sensitive_Acc : 16.800, Run Time : 7.01 sec
INFO:root:2024-04-27 13:41:18, Train, Epoch : 4, Step : 2070, Loss : 0.36201, Acc : 0.828, Sensitive_Loss : 0.09709, Sensitive_Acc : 17.700, Run Time : 6.97 sec
INFO:root:2024-04-27 13:41:25, Train, Epoch : 4, Step : 2080, Loss : 0.39028, Acc : 0.844, Sensitive_Loss : 0.10717, Sensitive_Acc : 16.000, Run Time : 7.58 sec
INFO:root:2024-04-27 13:41:32, Train, Epoch : 4, Step : 2090, Loss : 0.28933, Acc : 0.878, Sensitive_Loss : 0.08080, Sensitive_Acc : 16.700, Run Time : 6.91 sec
INFO:root:2024-04-27 13:41:40, Train, Epoch : 4, Step : 2100, Loss : 0.35837, Acc : 0.838, Sensitive_Loss : 0.11366, Sensitive_Acc : 16.600, Run Time : 7.95 sec
INFO:root:2024-04-27 13:43:13, Dev, Step : 2100, Loss : 0.39810, Acc : 0.830, Auc : 0.913, Sensitive_Loss : 0.12723, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992, Mean auc: 0.913, Run Time : 93.54 sec
INFO:root:2024-04-27 13:43:19, Train, Epoch : 4, Step : 2110, Loss : 0.35853, Acc : 0.838, Sensitive_Loss : 0.08209, Sensitive_Acc : 14.700, Run Time : 99.17 sec
INFO:root:2024-04-27 13:43:27, Train, Epoch : 4, Step : 2120, Loss : 0.32053, Acc : 0.881, Sensitive_Loss : 0.08721, Sensitive_Acc : 17.400, Run Time : 7.38 sec
INFO:root:2024-04-27 13:43:34, Train, Epoch : 4, Step : 2130, Loss : 0.29797, Acc : 0.850, Sensitive_Loss : 0.11585, Sensitive_Acc : 17.200, Run Time : 7.26 sec
INFO:root:2024-04-27 13:43:41, Train, Epoch : 4, Step : 2140, Loss : 0.31564, Acc : 0.859, Sensitive_Loss : 0.08648, Sensitive_Acc : 16.000, Run Time : 7.67 sec
INFO:root:2024-04-27 13:43:48, Train, Epoch : 4, Step : 2150, Loss : 0.38715, Acc : 0.847, Sensitive_Loss : 0.10402, Sensitive_Acc : 17.200, Run Time : 7.06 sec
INFO:root:2024-04-27 13:43:56, Train, Epoch : 4, Step : 2160, Loss : 0.34088, Acc : 0.847, Sensitive_Loss : 0.08560, Sensitive_Acc : 16.400, Run Time : 7.19 sec
INFO:root:2024-04-27 13:44:02, Train, Epoch : 4, Step : 2170, Loss : 0.43112, Acc : 0.831, Sensitive_Loss : 0.07794, Sensitive_Acc : 16.300, Run Time : 6.62 sec
INFO:root:2024-04-27 13:44:10, Train, Epoch : 4, Step : 2180, Loss : 0.30619, Acc : 0.875, Sensitive_Loss : 0.12436, Sensitive_Acc : 17.700, Run Time : 7.34 sec
INFO:root:2024-04-27 13:44:17, Train, Epoch : 4, Step : 2190, Loss : 0.32891, Acc : 0.844, Sensitive_Loss : 0.12495, Sensitive_Acc : 15.600, Run Time : 7.37 sec
INFO:root:2024-04-27 13:44:24, Train, Epoch : 4, Step : 2200, Loss : 0.29240, Acc : 0.875, Sensitive_Loss : 0.09279, Sensitive_Acc : 17.000, Run Time : 7.28 sec
INFO:root:2024-04-27 13:45:58, Dev, Step : 2200, Loss : 0.39942, Acc : 0.832, Auc : 0.914, Sensitive_Loss : 0.13641, Sensitive_Acc : 16.850, Sensitive_Auc : 0.991, Mean auc: 0.914, Run Time : 93.39 sec
INFO:root:2024-04-27 13:46:03, Train, Epoch : 4, Step : 2210, Loss : 0.32655, Acc : 0.856, Sensitive_Loss : 0.07892, Sensitive_Acc : 16.500, Run Time : 99.02 sec
INFO:root:2024-04-27 13:46:11, Train, Epoch : 4, Step : 2220, Loss : 0.31234, Acc : 0.866, Sensitive_Loss : 0.09761, Sensitive_Acc : 17.100, Run Time : 7.28 sec
INFO:root:2024-04-27 13:46:18, Train, Epoch : 4, Step : 2230, Loss : 0.31538, Acc : 0.850, Sensitive_Loss : 0.08806, Sensitive_Acc : 16.100, Run Time : 7.11 sec
INFO:root:2024-04-27 13:46:25, Train, Epoch : 4, Step : 2240, Loss : 0.33242, Acc : 0.853, Sensitive_Loss : 0.12423, Sensitive_Acc : 15.100, Run Time : 6.86 sec
INFO:root:2024-04-27 13:46:32, Train, Epoch : 4, Step : 2250, Loss : 0.22453, Acc : 0.922, Sensitive_Loss : 0.09938, Sensitive_Acc : 15.900, Run Time : 7.47 sec
INFO:root:2024-04-27 13:46:40, Train, Epoch : 4, Step : 2260, Loss : 0.33794, Acc : 0.828, Sensitive_Loss : 0.08321, Sensitive_Acc : 16.200, Run Time : 7.53 sec
INFO:root:2024-04-27 13:46:47, Train, Epoch : 4, Step : 2270, Loss : 0.37030, Acc : 0.853, Sensitive_Loss : 0.09438, Sensitive_Acc : 16.500, Run Time : 7.58 sec
INFO:root:2024-04-27 13:46:54, Train, Epoch : 4, Step : 2280, Loss : 0.30315, Acc : 0.869, Sensitive_Loss : 0.11866, Sensitive_Acc : 16.100, Run Time : 7.18 sec
INFO:root:2024-04-27 13:47:02, Train, Epoch : 4, Step : 2290, Loss : 0.39726, Acc : 0.834, Sensitive_Loss : 0.10330, Sensitive_Acc : 17.500, Run Time : 7.21 sec
INFO:root:2024-04-27 13:47:08, Train, Epoch : 4, Step : 2300, Loss : 0.35041, Acc : 0.825, Sensitive_Loss : 0.08497, Sensitive_Acc : 17.400, Run Time : 6.97 sec
INFO:root:2024-04-27 13:48:42, Dev, Step : 2300, Loss : 0.41064, Acc : 0.824, Auc : 0.913, Sensitive_Loss : 0.13767, Sensitive_Acc : 16.821, Sensitive_Auc : 0.991, Mean auc: 0.913, Run Time : 93.30 sec
INFO:root:2024-04-27 13:48:47, Train, Epoch : 4, Step : 2310, Loss : 0.31901, Acc : 0.856, Sensitive_Loss : 0.12112, Sensitive_Acc : 16.400, Run Time : 98.87 sec
INFO:root:2024-04-27 13:48:54, Train, Epoch : 4, Step : 2320, Loss : 0.31177, Acc : 0.872, Sensitive_Loss : 0.10298, Sensitive_Acc : 16.000, Run Time : 7.01 sec
INFO:root:2024-04-27 13:49:02, Train, Epoch : 4, Step : 2330, Loss : 0.37809, Acc : 0.850, Sensitive_Loss : 0.08104, Sensitive_Acc : 17.500, Run Time : 7.72 sec
INFO:root:2024-04-27 13:49:09, Train, Epoch : 4, Step : 2340, Loss : 0.34954, Acc : 0.831, Sensitive_Loss : 0.09054, Sensitive_Acc : 16.000, Run Time : 7.10 sec
INFO:root:2024-04-27 13:49:16, Train, Epoch : 4, Step : 2350, Loss : 0.33209, Acc : 0.859, Sensitive_Loss : 0.07186, Sensitive_Acc : 16.500, Run Time : 7.07 sec
INFO:root:2024-04-27 13:49:24, Train, Epoch : 4, Step : 2360, Loss : 0.32451, Acc : 0.863, Sensitive_Loss : 0.08477, Sensitive_Acc : 15.300, Run Time : 7.64 sec
INFO:root:2024-04-27 13:49:31, Train, Epoch : 4, Step : 2370, Loss : 0.31305, Acc : 0.869, Sensitive_Loss : 0.09382, Sensitive_Acc : 15.700, Run Time : 7.16 sec
INFO:root:2024-04-27 13:49:38, Train, Epoch : 4, Step : 2380, Loss : 0.33657, Acc : 0.875, Sensitive_Loss : 0.10352, Sensitive_Acc : 17.200, Run Time : 7.12 sec
INFO:root:2024-04-27 13:49:45, Train, Epoch : 4, Step : 2390, Loss : 0.34981, Acc : 0.875, Sensitive_Loss : 0.12823, Sensitive_Acc : 16.600, Run Time : 7.26 sec
INFO:root:2024-04-27 13:49:53, Train, Epoch : 4, Step : 2400, Loss : 0.34096, Acc : 0.844, Sensitive_Loss : 0.06482, Sensitive_Acc : 15.900, Run Time : 7.34 sec
INFO:root:2024-04-27 13:51:26, Dev, Step : 2400, Loss : 0.40385, Acc : 0.830, Auc : 0.914, Sensitive_Loss : 0.12386, Sensitive_Acc : 16.821, Sensitive_Auc : 0.993, Mean auc: 0.914, Run Time : 93.41 sec
INFO:root:2024-04-27 13:51:32, Train, Epoch : 4, Step : 2410, Loss : 0.31263, Acc : 0.869, Sensitive_Loss : 0.07202, Sensitive_Acc : 16.300, Run Time : 99.41 sec
INFO:root:2024-04-27 13:51:39, Train, Epoch : 4, Step : 2420, Loss : 0.33121, Acc : 0.878, Sensitive_Loss : 0.06993, Sensitive_Acc : 15.900, Run Time : 6.97 sec
INFO:root:2024-04-27 13:51:46, Train, Epoch : 4, Step : 2430, Loss : 0.35096, Acc : 0.872, Sensitive_Loss : 0.12488, Sensitive_Acc : 15.400, Run Time : 6.96 sec
INFO:root:2024-04-27 13:51:54, Train, Epoch : 4, Step : 2440, Loss : 0.34662, Acc : 0.831, Sensitive_Loss : 0.14164, Sensitive_Acc : 16.100, Run Time : 7.56 sec
INFO:root:2024-04-27 13:52:01, Train, Epoch : 4, Step : 2450, Loss : 0.39398, Acc : 0.841, Sensitive_Loss : 0.08373, Sensitive_Acc : 16.700, Run Time : 7.39 sec
INFO:root:2024-04-27 13:52:09, Train, Epoch : 4, Step : 2460, Loss : 0.33278, Acc : 0.853, Sensitive_Loss : 0.15467, Sensitive_Acc : 15.700, Run Time : 7.63 sec
INFO:root:2024-04-27 13:52:16, Train, Epoch : 4, Step : 2470, Loss : 0.28851, Acc : 0.869, Sensitive_Loss : 0.08266, Sensitive_Acc : 15.500, Run Time : 7.15 sec
INFO:root:2024-04-27 13:52:23, Train, Epoch : 4, Step : 2480, Loss : 0.32963, Acc : 0.872, Sensitive_Loss : 0.09804, Sensitive_Acc : 17.700, Run Time : 6.75 sec
INFO:root:2024-04-27 13:52:30, Train, Epoch : 4, Step : 2490, Loss : 0.39394, Acc : 0.825, Sensitive_Loss : 0.10925, Sensitive_Acc : 15.000, Run Time : 7.48 sec
INFO:root:2024-04-27 13:52:37, Train, Epoch : 4, Step : 2500, Loss : 0.36878, Acc : 0.831, Sensitive_Loss : 0.13419, Sensitive_Acc : 16.100, Run Time : 6.89 sec
INFO:root:2024-04-27 13:54:11, Dev, Step : 2500, Loss : 0.41336, Acc : 0.825, Auc : 0.913, Sensitive_Loss : 0.12581, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.913, Run Time : 93.63 sec
INFO:root:2024-04-27 13:55:44
INFO:root:y_pred: [0.01526598 0.9545484  0.03530179 ... 0.7177384  0.00542019 0.74912137]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.85745549e-01 1.22357812e-03 2.30759516e-01 1.04648225e-05
 9.99823987e-01 3.46470537e-04 9.99998093e-01 9.99992251e-01
 3.95792164e-03 8.20707560e-01 9.98072147e-01 9.99913931e-01
 9.97584343e-01 9.98379111e-01 4.63960730e-02 9.76537883e-01
 9.99500871e-01 2.83520781e-02 5.33218563e-01 7.09433317e-01
 9.99837637e-01 2.82474812e-02 9.99701917e-01 9.96581614e-01
 9.99764025e-01 9.99184072e-01 1.87327641e-05 9.99854803e-01
 9.99822676e-01 7.71226287e-01 1.48776071e-02 1.44731551e-01
 3.05763036e-01 9.07375477e-03 1.46075189e-01 1.32033797e-02
 2.70963550e-01 2.21060379e-03 9.98985589e-01 9.99863029e-01
 1.14413640e-04 1.36184943e-04 9.49071407e-01 2.15989348e-04
 9.99994993e-01 9.99034405e-01 9.99724090e-01 9.88285959e-01
 1.28301024e-03 9.95473444e-01 9.98735249e-01 7.63988495e-02
 5.35684288e-01 9.89008840e-05 7.69177277e-05 9.70265642e-03
 1.75692944e-03 2.08706222e-02 5.88915194e-04 1.65455475e-01
 5.97983366e-03 1.04635343e-01 7.82408752e-03 9.63205516e-01
 7.10515141e-01 9.99973536e-01 5.55806048e-03 9.99872446e-01
 9.99558985e-01 9.39745367e-01 4.33246911e-01 4.31418031e-01
 1.98363629e-03 1.11441143e-01 3.06190345e-02 3.12455522e-04
 1.65505961e-01 2.21249789e-01 3.63717467e-04 9.99935389e-01
 9.99928594e-01 1.64348821e-04 2.08043829e-02 4.77780588e-03
 9.56069052e-01 8.88067186e-01 1.92899145e-02 1.73184667e-02
 9.65707242e-01 9.99002516e-01 9.99990582e-01 2.88865287e-02
 4.19335160e-03 9.99173701e-01 5.22564888e-01 2.17903387e-02
 9.92376387e-01 9.99922156e-01 3.92898328e-05 1.02747250e-02
 9.99797285e-01 9.99105036e-01 9.97903585e-01 9.95855987e-01
 3.42803495e-03 4.14549597e-02 9.97328997e-01 9.99377728e-01
 9.65754449e-01 1.61680327e-05 9.97784972e-01 9.95241284e-01
 2.49023978e-02 9.99771893e-01 9.99884367e-01 9.98317838e-01
 3.90681624e-01 9.99958158e-01 1.84462547e-01 4.88289684e-01
 9.99491692e-01 9.99558747e-01 1.14546703e-04 9.98972535e-01
 9.99993324e-01 2.45285273e-01 9.99180853e-01 3.02155390e-02
 1.74335635e-03 9.75234449e-01 9.99382973e-01 9.87332780e-04
 8.03428665e-02 2.81835184e-03 9.99787033e-01 9.98238802e-01
 9.92411792e-01 3.10237217e-03 1.80767793e-02 9.98154104e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 13:55:44, Dev, Step : 2504, Loss : 0.42348, Acc : 0.823, Auc : 0.913, Sensitive_Loss : 0.13064, Sensitive_Acc : 16.879, Sensitive_Auc : 0.993, Mean auc: 0.913, Run Time : 92.05 sec
INFO:root:2024-04-27 13:55:51, Train, Epoch : 5, Step : 2510, Loss : 0.22097, Acc : 0.512, Sensitive_Loss : 0.05153, Sensitive_Acc : 9.500, Run Time : 5.55 sec
INFO:root:2024-04-27 13:55:58, Train, Epoch : 5, Step : 2520, Loss : 0.34952, Acc : 0.850, Sensitive_Loss : 0.10869, Sensitive_Acc : 16.000, Run Time : 7.05 sec
INFO:root:2024-04-27 13:56:05, Train, Epoch : 5, Step : 2530, Loss : 0.35703, Acc : 0.850, Sensitive_Loss : 0.08022, Sensitive_Acc : 15.900, Run Time : 6.77 sec
INFO:root:2024-04-27 13:56:12, Train, Epoch : 5, Step : 2540, Loss : 0.35891, Acc : 0.841, Sensitive_Loss : 0.08912, Sensitive_Acc : 16.700, Run Time : 7.52 sec
INFO:root:2024-04-27 13:56:20, Train, Epoch : 5, Step : 2550, Loss : 0.34440, Acc : 0.850, Sensitive_Loss : 0.08240, Sensitive_Acc : 16.200, Run Time : 7.51 sec
INFO:root:2024-04-27 13:56:27, Train, Epoch : 5, Step : 2560, Loss : 0.26807, Acc : 0.884, Sensitive_Loss : 0.17981, Sensitive_Acc : 16.200, Run Time : 6.99 sec
INFO:root:2024-04-27 13:56:34, Train, Epoch : 5, Step : 2570, Loss : 0.31324, Acc : 0.850, Sensitive_Loss : 0.09895, Sensitive_Acc : 16.600, Run Time : 7.25 sec
INFO:root:2024-04-27 13:56:41, Train, Epoch : 5, Step : 2580, Loss : 0.32477, Acc : 0.844, Sensitive_Loss : 0.12478, Sensitive_Acc : 16.000, Run Time : 6.73 sec
INFO:root:2024-04-27 13:56:48, Train, Epoch : 5, Step : 2590, Loss : 0.29391, Acc : 0.909, Sensitive_Loss : 0.11420, Sensitive_Acc : 17.900, Run Time : 7.29 sec
INFO:root:2024-04-27 13:56:55, Train, Epoch : 5, Step : 2600, Loss : 0.30834, Acc : 0.881, Sensitive_Loss : 0.10895, Sensitive_Acc : 15.600, Run Time : 7.24 sec
INFO:root:2024-04-27 13:58:29, Dev, Step : 2600, Loss : 0.40572, Acc : 0.826, Auc : 0.914, Sensitive_Loss : 0.12315, Sensitive_Acc : 16.793, Sensitive_Auc : 0.994, Mean auc: 0.914, Run Time : 93.87 sec
INFO:root:2024-04-27 13:58:34, Train, Epoch : 5, Step : 2610, Loss : 0.36245, Acc : 0.866, Sensitive_Loss : 0.08068, Sensitive_Acc : 15.900, Run Time : 99.26 sec
INFO:root:2024-04-27 13:58:42, Train, Epoch : 5, Step : 2620, Loss : 0.30334, Acc : 0.878, Sensitive_Loss : 0.10093, Sensitive_Acc : 15.600, Run Time : 7.31 sec
INFO:root:2024-04-27 13:58:49, Train, Epoch : 5, Step : 2630, Loss : 0.30096, Acc : 0.863, Sensitive_Loss : 0.07632, Sensitive_Acc : 15.500, Run Time : 7.49 sec
INFO:root:2024-04-27 13:58:56, Train, Epoch : 5, Step : 2640, Loss : 0.32719, Acc : 0.838, Sensitive_Loss : 0.08039, Sensitive_Acc : 15.300, Run Time : 7.03 sec
INFO:root:2024-04-27 13:59:03, Train, Epoch : 5, Step : 2650, Loss : 0.31533, Acc : 0.856, Sensitive_Loss : 0.09874, Sensitive_Acc : 17.400, Run Time : 7.21 sec
INFO:root:2024-04-27 13:59:10, Train, Epoch : 5, Step : 2660, Loss : 0.33137, Acc : 0.869, Sensitive_Loss : 0.07102, Sensitive_Acc : 16.500, Run Time : 7.03 sec
INFO:root:2024-04-27 13:59:18, Train, Epoch : 5, Step : 2670, Loss : 0.30220, Acc : 0.875, Sensitive_Loss : 0.11448, Sensitive_Acc : 16.300, Run Time : 7.50 sec
INFO:root:2024-04-27 13:59:25, Train, Epoch : 5, Step : 2680, Loss : 0.35749, Acc : 0.863, Sensitive_Loss : 0.08329, Sensitive_Acc : 15.800, Run Time : 6.56 sec
INFO:root:2024-04-27 13:59:32, Train, Epoch : 5, Step : 2690, Loss : 0.26706, Acc : 0.881, Sensitive_Loss : 0.06399, Sensitive_Acc : 15.900, Run Time : 7.36 sec
INFO:root:2024-04-27 13:59:39, Train, Epoch : 5, Step : 2700, Loss : 0.30426, Acc : 0.853, Sensitive_Loss : 0.10163, Sensitive_Acc : 16.300, Run Time : 7.41 sec
INFO:root:2024-04-27 14:01:13, Dev, Step : 2700, Loss : 0.41570, Acc : 0.823, Auc : 0.913, Sensitive_Loss : 0.13257, Sensitive_Acc : 16.850, Sensitive_Auc : 0.992, Mean auc: 0.913, Run Time : 93.56 sec
INFO:root:2024-04-27 14:01:18, Train, Epoch : 5, Step : 2710, Loss : 0.31203, Acc : 0.863, Sensitive_Loss : 0.05554, Sensitive_Acc : 16.100, Run Time : 99.05 sec
INFO:root:2024-04-27 14:01:26, Train, Epoch : 5, Step : 2720, Loss : 0.34723, Acc : 0.869, Sensitive_Loss : 0.08873, Sensitive_Acc : 16.000, Run Time : 7.24 sec
INFO:root:2024-04-27 14:01:33, Train, Epoch : 5, Step : 2730, Loss : 0.35111, Acc : 0.847, Sensitive_Loss : 0.08372, Sensitive_Acc : 16.900, Run Time : 7.11 sec
INFO:root:2024-04-27 14:01:40, Train, Epoch : 5, Step : 2740, Loss : 0.27532, Acc : 0.887, Sensitive_Loss : 0.08794, Sensitive_Acc : 16.700, Run Time : 7.32 sec
INFO:root:2024-04-27 14:01:47, Train, Epoch : 5, Step : 2750, Loss : 0.28490, Acc : 0.897, Sensitive_Loss : 0.12095, Sensitive_Acc : 17.700, Run Time : 7.35 sec
INFO:root:2024-04-27 14:01:55, Train, Epoch : 5, Step : 2760, Loss : 0.28908, Acc : 0.866, Sensitive_Loss : 0.07000, Sensitive_Acc : 16.700, Run Time : 7.34 sec
INFO:root:2024-04-27 14:02:02, Train, Epoch : 5, Step : 2770, Loss : 0.31346, Acc : 0.863, Sensitive_Loss : 0.08496, Sensitive_Acc : 15.900, Run Time : 7.12 sec
INFO:root:2024-04-27 14:02:09, Train, Epoch : 5, Step : 2780, Loss : 0.31761, Acc : 0.866, Sensitive_Loss : 0.10192, Sensitive_Acc : 17.100, Run Time : 7.31 sec
INFO:root:2024-04-27 14:02:16, Train, Epoch : 5, Step : 2790, Loss : 0.28046, Acc : 0.875, Sensitive_Loss : 0.09803, Sensitive_Acc : 16.700, Run Time : 6.94 sec
INFO:root:2024-04-27 14:02:23, Train, Epoch : 5, Step : 2800, Loss : 0.27990, Acc : 0.878, Sensitive_Loss : 0.09877, Sensitive_Acc : 16.000, Run Time : 7.22 sec
INFO:root:2024-04-27 14:03:57, Dev, Step : 2800, Loss : 0.42467, Acc : 0.820, Auc : 0.914, Sensitive_Loss : 0.15178, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992, Mean auc: 0.914, Run Time : 93.35 sec
INFO:root:2024-04-27 14:04:02, Train, Epoch : 5, Step : 2810, Loss : 0.30964, Acc : 0.878, Sensitive_Loss : 0.07869, Sensitive_Acc : 17.800, Run Time : 98.91 sec
INFO:root:2024-04-27 14:04:09, Train, Epoch : 5, Step : 2820, Loss : 0.30009, Acc : 0.863, Sensitive_Loss : 0.11015, Sensitive_Acc : 16.500, Run Time : 7.05 sec
INFO:root:2024-04-27 14:04:16, Train, Epoch : 5, Step : 2830, Loss : 0.29074, Acc : 0.866, Sensitive_Loss : 0.11433, Sensitive_Acc : 16.700, Run Time : 7.10 sec
INFO:root:2024-04-27 14:04:24, Train, Epoch : 5, Step : 2840, Loss : 0.27739, Acc : 0.884, Sensitive_Loss : 0.13469, Sensitive_Acc : 14.500, Run Time : 7.68 sec
INFO:root:2024-04-27 14:04:31, Train, Epoch : 5, Step : 2850, Loss : 0.27293, Acc : 0.887, Sensitive_Loss : 0.09652, Sensitive_Acc : 14.900, Run Time : 6.81 sec
INFO:root:2024-04-27 14:04:38, Train, Epoch : 5, Step : 2860, Loss : 0.30534, Acc : 0.881, Sensitive_Loss : 0.07627, Sensitive_Acc : 17.400, Run Time : 7.16 sec
INFO:root:2024-04-27 14:04:45, Train, Epoch : 5, Step : 2870, Loss : 0.32341, Acc : 0.878, Sensitive_Loss : 0.09554, Sensitive_Acc : 16.100, Run Time : 7.41 sec
INFO:root:2024-04-27 14:04:52, Train, Epoch : 5, Step : 2880, Loss : 0.34545, Acc : 0.847, Sensitive_Loss : 0.06239, Sensitive_Acc : 15.500, Run Time : 7.03 sec
INFO:root:2024-04-27 14:04:59, Train, Epoch : 5, Step : 2890, Loss : 0.31716, Acc : 0.869, Sensitive_Loss : 0.09087, Sensitive_Acc : 16.800, Run Time : 7.00 sec
INFO:root:2024-04-27 14:05:07, Train, Epoch : 5, Step : 2900, Loss : 0.30151, Acc : 0.875, Sensitive_Loss : 0.11336, Sensitive_Acc : 16.100, Run Time : 7.51 sec
INFO:root:2024-04-27 14:06:41, Dev, Step : 2900, Loss : 0.42157, Acc : 0.824, Auc : 0.915, Sensitive_Loss : 0.13939, Sensitive_Acc : 16.793, Sensitive_Auc : 0.993, Mean auc: 0.915, Run Time : 94.01 sec
INFO:root:2024-04-27 14:06:42, Best, Step : 2900, Loss : 0.42157, Acc : 0.824, Auc : 0.915, Sensitive_Loss : 0.13939, Sensitive_Acc : 16.793, Sensitive_Auc : 0.993, Best Auc : 0.915
INFO:root:2024-04-27 14:06:47, Train, Epoch : 5, Step : 2910, Loss : 0.27597, Acc : 0.900, Sensitive_Loss : 0.06239, Sensitive_Acc : 16.900, Run Time : 100.21 sec
INFO:root:2024-04-27 14:06:54, Train, Epoch : 5, Step : 2920, Loss : 0.38938, Acc : 0.859, Sensitive_Loss : 0.11351, Sensitive_Acc : 16.000, Run Time : 7.21 sec
INFO:root:2024-04-27 14:07:02, Train, Epoch : 5, Step : 2930, Loss : 0.26465, Acc : 0.891, Sensitive_Loss : 0.08630, Sensitive_Acc : 15.000, Run Time : 7.71 sec
INFO:root:2024-04-27 14:07:09, Train, Epoch : 5, Step : 2940, Loss : 0.25073, Acc : 0.856, Sensitive_Loss : 0.09945, Sensitive_Acc : 16.200, Run Time : 6.86 sec
INFO:root:2024-04-27 14:07:16, Train, Epoch : 5, Step : 2950, Loss : 0.29076, Acc : 0.878, Sensitive_Loss : 0.07831, Sensitive_Acc : 15.600, Run Time : 7.18 sec
INFO:root:2024-04-27 14:07:24, Train, Epoch : 5, Step : 2960, Loss : 0.29197, Acc : 0.859, Sensitive_Loss : 0.08477, Sensitive_Acc : 16.100, Run Time : 7.75 sec
INFO:root:2024-04-27 14:07:31, Train, Epoch : 5, Step : 2970, Loss : 0.34284, Acc : 0.863, Sensitive_Loss : 0.08610, Sensitive_Acc : 15.400, Run Time : 6.93 sec
INFO:root:2024-04-27 14:07:37, Train, Epoch : 5, Step : 2980, Loss : 0.27147, Acc : 0.878, Sensitive_Loss : 0.10009, Sensitive_Acc : 16.400, Run Time : 6.61 sec
INFO:root:2024-04-27 14:07:45, Train, Epoch : 5, Step : 2990, Loss : 0.32064, Acc : 0.847, Sensitive_Loss : 0.07192, Sensitive_Acc : 15.700, Run Time : 7.76 sec
INFO:root:2024-04-27 14:07:52, Train, Epoch : 5, Step : 3000, Loss : 0.34075, Acc : 0.847, Sensitive_Loss : 0.08875, Sensitive_Acc : 16.300, Run Time : 6.43 sec
INFO:root:2024-04-27 14:09:26, Dev, Step : 3000, Loss : 0.42420, Acc : 0.822, Auc : 0.912, Sensitive_Loss : 0.13752, Sensitive_Acc : 16.793, Sensitive_Auc : 0.993, Mean auc: 0.912, Run Time : 94.26 sec
INFO:root:2024-04-27 14:09:32, Train, Epoch : 5, Step : 3010, Loss : 0.33915, Acc : 0.841, Sensitive_Loss : 0.09033, Sensitive_Acc : 16.200, Run Time : 100.04 sec
INFO:root:2024-04-27 14:09:39, Train, Epoch : 5, Step : 3020, Loss : 0.27944, Acc : 0.881, Sensitive_Loss : 0.07711, Sensitive_Acc : 17.300, Run Time : 7.02 sec
INFO:root:2024-04-27 14:09:46, Train, Epoch : 5, Step : 3030, Loss : 0.31704, Acc : 0.859, Sensitive_Loss : 0.09758, Sensitive_Acc : 17.000, Run Time : 7.57 sec
INFO:root:2024-04-27 14:09:53, Train, Epoch : 5, Step : 3040, Loss : 0.36234, Acc : 0.819, Sensitive_Loss : 0.07495, Sensitive_Acc : 16.000, Run Time : 7.19 sec
INFO:root:2024-04-27 14:10:01, Train, Epoch : 5, Step : 3050, Loss : 0.28355, Acc : 0.909, Sensitive_Loss : 0.09271, Sensitive_Acc : 15.700, Run Time : 7.13 sec
INFO:root:2024-04-27 14:10:08, Train, Epoch : 5, Step : 3060, Loss : 0.34081, Acc : 0.863, Sensitive_Loss : 0.09443, Sensitive_Acc : 14.800, Run Time : 7.17 sec
INFO:root:2024-04-27 14:10:14, Train, Epoch : 5, Step : 3070, Loss : 0.28386, Acc : 0.869, Sensitive_Loss : 0.07705, Sensitive_Acc : 17.000, Run Time : 6.55 sec
INFO:root:2024-04-27 14:10:22, Train, Epoch : 5, Step : 3080, Loss : 0.27628, Acc : 0.894, Sensitive_Loss : 0.08804, Sensitive_Acc : 14.200, Run Time : 7.50 sec
INFO:root:2024-04-27 14:10:29, Train, Epoch : 5, Step : 3090, Loss : 0.29427, Acc : 0.875, Sensitive_Loss : 0.08094, Sensitive_Acc : 17.100, Run Time : 7.41 sec
INFO:root:2024-04-27 14:10:36, Train, Epoch : 5, Step : 3100, Loss : 0.27044, Acc : 0.894, Sensitive_Loss : 0.05721, Sensitive_Acc : 17.200, Run Time : 7.06 sec
INFO:root:2024-04-27 14:12:10, Dev, Step : 3100, Loss : 0.40717, Acc : 0.823, Auc : 0.915, Sensitive_Loss : 0.13575, Sensitive_Acc : 16.821, Sensitive_Auc : 0.992, Mean auc: 0.915, Run Time : 93.69 sec
INFO:root:2024-04-27 14:12:16, Train, Epoch : 5, Step : 3110, Loss : 0.29278, Acc : 0.859, Sensitive_Loss : 0.10519, Sensitive_Acc : 15.800, Run Time : 99.29 sec
INFO:root:2024-04-27 14:12:23, Train, Epoch : 5, Step : 3120, Loss : 0.31304, Acc : 0.872, Sensitive_Loss : 0.05282, Sensitive_Acc : 15.800, Run Time : 7.39 sec
INFO:root:2024-04-27 14:12:29, Train, Epoch : 5, Step : 3130, Loss : 0.31497, Acc : 0.856, Sensitive_Loss : 0.10537, Sensitive_Acc : 16.100, Run Time : 6.35 sec
INFO:root:2024-04-27 14:14:02
INFO:root:y_pred: [0.01960089 0.96200216 0.03854772 ... 0.7973596  0.00366668 0.8604694 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.8963416e-01 4.8154918e-03 1.9733950e-01 1.7260256e-05 9.9989045e-01
 8.0233091e-04 9.9999821e-01 9.9999368e-01 2.1168988e-03 8.4913033e-01
 9.9932373e-01 9.9990404e-01 9.9790359e-01 9.9861717e-01 2.9675720e-02
 9.7967142e-01 9.9954885e-01 1.4335218e-02 5.0368243e-01 8.9899242e-01
 9.9990714e-01 6.8096086e-02 9.9980885e-01 9.9793833e-01 9.9989140e-01
 9.9919552e-01 3.9219678e-05 9.9986756e-01 9.9975055e-01 7.9935628e-01
 1.7330237e-02 2.9918483e-01 4.8670691e-01 1.1021063e-02 2.6279351e-01
 2.7978353e-02 1.5380913e-01 3.5944956e-03 9.9927109e-01 9.9984789e-01
 2.7503280e-04 7.9486973e-04 9.7837263e-01 3.0574334e-04 9.9999654e-01
 9.9967551e-01 9.9963415e-01 9.8858029e-01 1.6186953e-03 9.9562621e-01
 9.9825245e-01 6.6387020e-02 7.3715514e-01 3.8193364e-04 1.6621254e-04
 9.9682650e-03 2.5713656e-03 3.4435723e-02 3.1601484e-03 1.8670624e-01
 2.0794570e-02 1.9358684e-01 9.1582481e-03 9.8889089e-01 5.6673723e-01
 9.9996662e-01 1.1559741e-02 9.9980181e-01 9.9971086e-01 9.6601951e-01
 5.7431513e-01 6.4227033e-01 2.8984658e-03 6.7875616e-02 3.0228786e-02
 8.3175761e-04 1.5037605e-01 3.3677423e-01 5.5171381e-04 9.9996924e-01
 9.9993265e-01 1.1065654e-04 2.6134012e-02 6.0712718e-03 9.6761638e-01
 9.5536184e-01 8.9870812e-03 2.8836783e-02 9.8550653e-01 9.9904925e-01
 9.9998939e-01 2.1778066e-02 2.6744034e-03 9.9938703e-01 5.0902599e-01
 3.1619616e-02 9.9108034e-01 9.9988389e-01 6.9234055e-05 2.3597877e-02
 9.9986851e-01 9.9914479e-01 9.9927980e-01 9.9735510e-01 8.6826645e-03
 6.5681525e-02 9.9340135e-01 9.9913210e-01 9.8895556e-01 1.5890575e-05
 9.9732310e-01 9.9696296e-01 1.5965916e-02 9.9975926e-01 9.9984050e-01
 9.9910271e-01 3.6583188e-01 9.9994779e-01 1.4930129e-01 3.5816273e-01
 9.9956137e-01 9.9890327e-01 3.3954415e-04 9.9959821e-01 9.9999332e-01
 3.5631207e-01 9.9890041e-01 7.7676699e-02 3.4858070e-03 9.9209589e-01
 9.9961853e-01 1.2432134e-03 4.1184444e-02 1.5239147e-03 9.9986398e-01
 9.9917561e-01 9.9085641e-01 2.5944728e-03 1.1915465e-02 9.9795985e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 14:14:02, Dev, Step : 3130, Loss : 0.41763, Acc : 0.823, Auc : 0.914, Sensitive_Loss : 0.13958, Sensitive_Acc : 16.793, Sensitive_Auc : 0.993, Mean auc: 0.914, Run Time : 92.19 sec
INFO:root:2024-04-27 14:14:11, Train, Epoch : 6, Step : 3140, Loss : 0.29055, Acc : 0.859, Sensitive_Loss : 0.09193, Sensitive_Acc : 16.600, Run Time : 8.21 sec
INFO:root:2024-04-27 14:14:18, Train, Epoch : 6, Step : 3150, Loss : 0.33270, Acc : 0.859, Sensitive_Loss : 0.12708, Sensitive_Acc : 15.800, Run Time : 7.23 sec
INFO:root:2024-04-27 14:14:25, Train, Epoch : 6, Step : 3160, Loss : 0.35689, Acc : 0.869, Sensitive_Loss : 0.14778, Sensitive_Acc : 16.800, Run Time : 7.38 sec
INFO:root:2024-04-27 14:14:33, Train, Epoch : 6, Step : 3170, Loss : 0.33281, Acc : 0.844, Sensitive_Loss : 0.07071, Sensitive_Acc : 16.800, Run Time : 7.19 sec
INFO:root:2024-04-27 14:14:39, Train, Epoch : 6, Step : 3180, Loss : 0.29451, Acc : 0.856, Sensitive_Loss : 0.09529, Sensitive_Acc : 15.200, Run Time : 6.91 sec
INFO:root:2024-04-27 14:14:47, Train, Epoch : 6, Step : 3190, Loss : 0.31088, Acc : 0.859, Sensitive_Loss : 0.06580, Sensitive_Acc : 16.500, Run Time : 7.34 sec
INFO:root:2024-04-27 14:14:54, Train, Epoch : 6, Step : 3200, Loss : 0.32345, Acc : 0.856, Sensitive_Loss : 0.06966, Sensitive_Acc : 17.600, Run Time : 7.52 sec
INFO:root:2024-04-27 14:16:27, Dev, Step : 3200, Loss : 0.40940, Acc : 0.828, Auc : 0.913, Sensitive_Loss : 0.12330, Sensitive_Acc : 16.864, Sensitive_Auc : 0.992, Mean auc: 0.913, Run Time : 93.06 sec
INFO:root:2024-04-27 14:16:33, Train, Epoch : 6, Step : 3210, Loss : 0.29351, Acc : 0.869, Sensitive_Loss : 0.12213, Sensitive_Acc : 15.100, Run Time : 98.74 sec
INFO:root:2024-04-27 14:16:40, Train, Epoch : 6, Step : 3220, Loss : 0.27041, Acc : 0.878, Sensitive_Loss : 0.10562, Sensitive_Acc : 16.600, Run Time : 7.18 sec
INFO:root:2024-04-27 14:16:48, Train, Epoch : 6, Step : 3230, Loss : 0.28347, Acc : 0.891, Sensitive_Loss : 0.07378, Sensitive_Acc : 15.600, Run Time : 7.38 sec
INFO:root:2024-04-27 14:16:55, Train, Epoch : 6, Step : 3240, Loss : 0.27411, Acc : 0.887, Sensitive_Loss : 0.08228, Sensitive_Acc : 17.600, Run Time : 7.15 sec
INFO:root:2024-04-27 14:17:02, Train, Epoch : 6, Step : 3250, Loss : 0.28679, Acc : 0.891, Sensitive_Loss : 0.15228, Sensitive_Acc : 16.000, Run Time : 6.80 sec
INFO:root:2024-04-27 14:17:09, Train, Epoch : 6, Step : 3260, Loss : 0.28555, Acc : 0.869, Sensitive_Loss : 0.10667, Sensitive_Acc : 16.300, Run Time : 7.95 sec
INFO:root:2024-04-27 14:17:16, Train, Epoch : 6, Step : 3270, Loss : 0.29980, Acc : 0.881, Sensitive_Loss : 0.06562, Sensitive_Acc : 16.300, Run Time : 6.97 sec
INFO:root:2024-04-27 14:17:23, Train, Epoch : 6, Step : 3280, Loss : 0.27076, Acc : 0.909, Sensitive_Loss : 0.08358, Sensitive_Acc : 18.700, Run Time : 7.04 sec
INFO:root:2024-04-27 14:17:31, Train, Epoch : 6, Step : 3290, Loss : 0.30470, Acc : 0.884, Sensitive_Loss : 0.06165, Sensitive_Acc : 17.600, Run Time : 7.20 sec
INFO:root:2024-04-27 14:17:38, Train, Epoch : 6, Step : 3300, Loss : 0.29107, Acc : 0.881, Sensitive_Loss : 0.06880, Sensitive_Acc : 15.900, Run Time : 7.19 sec
INFO:root:2024-04-27 14:19:11, Dev, Step : 3300, Loss : 0.42013, Acc : 0.827, Auc : 0.910, Sensitive_Loss : 0.13467, Sensitive_Acc : 16.821, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 93.59 sec
INFO:root:2024-04-27 14:19:17, Train, Epoch : 6, Step : 3310, Loss : 0.29148, Acc : 0.856, Sensitive_Loss : 0.08556, Sensitive_Acc : 16.300, Run Time : 99.15 sec
INFO:root:2024-04-27 14:19:25, Train, Epoch : 6, Step : 3320, Loss : 0.28250, Acc : 0.881, Sensitive_Loss : 0.08360, Sensitive_Acc : 16.000, Run Time : 7.76 sec
INFO:root:2024-04-27 14:19:32, Train, Epoch : 6, Step : 3330, Loss : 0.27226, Acc : 0.866, Sensitive_Loss : 0.06326, Sensitive_Acc : 15.400, Run Time : 7.19 sec
INFO:root:2024-04-27 14:19:39, Train, Epoch : 6, Step : 3340, Loss : 0.30670, Acc : 0.853, Sensitive_Loss : 0.10643, Sensitive_Acc : 15.700, Run Time : 6.80 sec
INFO:root:2024-04-27 14:19:46, Train, Epoch : 6, Step : 3350, Loss : 0.28498, Acc : 0.875, Sensitive_Loss : 0.11340, Sensitive_Acc : 15.400, Run Time : 7.34 sec
INFO:root:2024-04-27 14:19:54, Train, Epoch : 6, Step : 3360, Loss : 0.31189, Acc : 0.859, Sensitive_Loss : 0.11980, Sensitive_Acc : 15.800, Run Time : 7.84 sec
INFO:root:2024-04-27 14:20:01, Train, Epoch : 6, Step : 3370, Loss : 0.30403, Acc : 0.869, Sensitive_Loss : 0.09348, Sensitive_Acc : 17.100, Run Time : 6.88 sec
INFO:root:2024-04-27 14:20:08, Train, Epoch : 6, Step : 3380, Loss : 0.31607, Acc : 0.897, Sensitive_Loss : 0.08648, Sensitive_Acc : 17.200, Run Time : 7.09 sec
INFO:root:2024-04-27 14:20:15, Train, Epoch : 6, Step : 3390, Loss : 0.28201, Acc : 0.881, Sensitive_Loss : 0.07320, Sensitive_Acc : 16.600, Run Time : 7.16 sec
INFO:root:2024-04-27 14:20:22, Train, Epoch : 6, Step : 3400, Loss : 0.27361, Acc : 0.872, Sensitive_Loss : 0.09924, Sensitive_Acc : 17.200, Run Time : 7.31 sec
INFO:root:2024-04-27 14:21:56, Dev, Step : 3400, Loss : 0.43086, Acc : 0.823, Auc : 0.911, Sensitive_Loss : 0.13641, Sensitive_Acc : 16.821, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 93.40 sec
INFO:root:2024-04-27 14:22:01, Train, Epoch : 6, Step : 3410, Loss : 0.26705, Acc : 0.869, Sensitive_Loss : 0.11485, Sensitive_Acc : 15.100, Run Time : 98.77 sec
INFO:root:2024-04-27 14:22:09, Train, Epoch : 6, Step : 3420, Loss : 0.27505, Acc : 0.884, Sensitive_Loss : 0.09655, Sensitive_Acc : 16.600, Run Time : 7.59 sec
INFO:root:2024-04-27 14:22:16, Train, Epoch : 6, Step : 3430, Loss : 0.26506, Acc : 0.850, Sensitive_Loss : 0.06660, Sensitive_Acc : 17.600, Run Time : 6.99 sec
INFO:root:2024-04-27 14:22:23, Train, Epoch : 6, Step : 3440, Loss : 0.24418, Acc : 0.897, Sensitive_Loss : 0.07368, Sensitive_Acc : 16.100, Run Time : 7.20 sec
INFO:root:2024-04-27 14:22:31, Train, Epoch : 6, Step : 3450, Loss : 0.24691, Acc : 0.900, Sensitive_Loss : 0.06685, Sensitive_Acc : 16.700, Run Time : 7.63 sec
INFO:root:2024-04-27 14:22:38, Train, Epoch : 6, Step : 3460, Loss : 0.28714, Acc : 0.894, Sensitive_Loss : 0.12766, Sensitive_Acc : 17.000, Run Time : 7.48 sec
INFO:root:2024-04-27 14:22:45, Train, Epoch : 6, Step : 3470, Loss : 0.30677, Acc : 0.866, Sensitive_Loss : 0.09595, Sensitive_Acc : 16.900, Run Time : 7.28 sec
INFO:root:2024-04-27 14:22:52, Train, Epoch : 6, Step : 3480, Loss : 0.28649, Acc : 0.878, Sensitive_Loss : 0.13388, Sensitive_Acc : 16.300, Run Time : 6.39 sec
INFO:root:2024-04-27 14:22:59, Train, Epoch : 6, Step : 3490, Loss : 0.27167, Acc : 0.863, Sensitive_Loss : 0.13284, Sensitive_Acc : 16.700, Run Time : 7.16 sec
INFO:root:2024-04-27 14:23:06, Train, Epoch : 6, Step : 3500, Loss : 0.31428, Acc : 0.869, Sensitive_Loss : 0.08166, Sensitive_Acc : 16.400, Run Time : 7.25 sec
INFO:root:2024-04-27 14:24:40, Dev, Step : 3500, Loss : 0.44021, Acc : 0.819, Auc : 0.909, Sensitive_Loss : 0.12593, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992, Mean auc: 0.909, Run Time : 94.05 sec
INFO:root:2024-04-27 14:24:46, Train, Epoch : 6, Step : 3510, Loss : 0.31137, Acc : 0.875, Sensitive_Loss : 0.12514, Sensitive_Acc : 17.100, Run Time : 99.72 sec
INFO:root:2024-04-27 14:24:53, Train, Epoch : 6, Step : 3520, Loss : 0.32845, Acc : 0.847, Sensitive_Loss : 0.08321, Sensitive_Acc : 16.400, Run Time : 7.47 sec
INFO:root:2024-04-27 14:25:01, Train, Epoch : 6, Step : 3530, Loss : 0.28446, Acc : 0.869, Sensitive_Loss : 0.08689, Sensitive_Acc : 17.600, Run Time : 7.20 sec
INFO:root:2024-04-27 14:25:08, Train, Epoch : 6, Step : 3540, Loss : 0.30708, Acc : 0.856, Sensitive_Loss : 0.08311, Sensitive_Acc : 16.700, Run Time : 7.27 sec
INFO:root:2024-04-27 14:25:15, Train, Epoch : 6, Step : 3550, Loss : 0.24363, Acc : 0.891, Sensitive_Loss : 0.09561, Sensitive_Acc : 16.800, Run Time : 6.90 sec
INFO:root:2024-04-27 14:25:22, Train, Epoch : 6, Step : 3560, Loss : 0.30514, Acc : 0.866, Sensitive_Loss : 0.10334, Sensitive_Acc : 17.900, Run Time : 7.35 sec
INFO:root:2024-04-27 14:25:29, Train, Epoch : 6, Step : 3570, Loss : 0.27166, Acc : 0.887, Sensitive_Loss : 0.07787, Sensitive_Acc : 15.800, Run Time : 7.12 sec
INFO:root:2024-04-27 14:25:37, Train, Epoch : 6, Step : 3580, Loss : 0.30500, Acc : 0.878, Sensitive_Loss : 0.08373, Sensitive_Acc : 16.400, Run Time : 7.54 sec
INFO:root:2024-04-27 14:25:44, Train, Epoch : 6, Step : 3590, Loss : 0.25923, Acc : 0.878, Sensitive_Loss : 0.07858, Sensitive_Acc : 15.400, Run Time : 7.33 sec
INFO:root:2024-04-27 14:25:51, Train, Epoch : 6, Step : 3600, Loss : 0.30718, Acc : 0.863, Sensitive_Loss : 0.07585, Sensitive_Acc : 16.200, Run Time : 6.94 sec
INFO:root:2024-04-27 14:29:46, Dev, Step : 3600, Loss : 0.43591, Acc : 0.822, Auc : 0.913, Sensitive_Loss : 0.14047, Sensitive_Acc : 16.721, Sensitive_Auc : 0.991, Mean auc: 0.913, Run Time : 234.94 sec
INFO:root:2024-04-27 14:29:56, Train, Epoch : 6, Step : 3610, Loss : 0.33032, Acc : 0.866, Sensitive_Loss : 0.08700, Sensitive_Acc : 16.200, Run Time : 245.41 sec
INFO:root:2024-04-27 14:30:04, Train, Epoch : 6, Step : 3620, Loss : 0.30218, Acc : 0.869, Sensitive_Loss : 0.09002, Sensitive_Acc : 16.100, Run Time : 7.75 sec
INFO:root:2024-04-27 14:30:11, Train, Epoch : 6, Step : 3630, Loss : 0.31999, Acc : 0.875, Sensitive_Loss : 0.07881, Sensitive_Acc : 16.300, Run Time : 6.70 sec
INFO:root:2024-04-27 14:30:18, Train, Epoch : 6, Step : 3640, Loss : 0.27629, Acc : 0.872, Sensitive_Loss : 0.09107, Sensitive_Acc : 16.100, Run Time : 7.01 sec
INFO:root:2024-04-27 14:30:25, Train, Epoch : 6, Step : 3650, Loss : 0.26065, Acc : 0.866, Sensitive_Loss : 0.06566, Sensitive_Acc : 17.000, Run Time : 7.23 sec
INFO:root:2024-04-27 14:30:32, Train, Epoch : 6, Step : 3660, Loss : 0.25447, Acc : 0.894, Sensitive_Loss : 0.08675, Sensitive_Acc : 15.900, Run Time : 7.06 sec
INFO:root:2024-04-27 14:30:40, Train, Epoch : 6, Step : 3670, Loss : 0.25636, Acc : 0.884, Sensitive_Loss : 0.06789, Sensitive_Acc : 15.600, Run Time : 8.04 sec
INFO:root:2024-04-27 14:30:47, Train, Epoch : 6, Step : 3680, Loss : 0.30517, Acc : 0.869, Sensitive_Loss : 0.06438, Sensitive_Acc : 15.600, Run Time : 6.54 sec
INFO:root:2024-04-27 14:30:55, Train, Epoch : 6, Step : 3690, Loss : 0.30898, Acc : 0.853, Sensitive_Loss : 0.06914, Sensitive_Acc : 17.200, Run Time : 7.80 sec
INFO:root:2024-04-27 14:31:02, Train, Epoch : 6, Step : 3700, Loss : 0.33212, Acc : 0.841, Sensitive_Loss : 0.09742, Sensitive_Acc : 16.400, Run Time : 7.41 sec
INFO:root:2024-04-27 14:32:37, Dev, Step : 3700, Loss : 0.43038, Acc : 0.820, Auc : 0.911, Sensitive_Loss : 0.12954, Sensitive_Acc : 16.807, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 94.93 sec
INFO:root:2024-04-27 14:32:42, Train, Epoch : 6, Step : 3710, Loss : 0.27889, Acc : 0.891, Sensitive_Loss : 0.07207, Sensitive_Acc : 17.000, Run Time : 100.28 sec
INFO:root:2024-04-27 14:32:49, Train, Epoch : 6, Step : 3720, Loss : 0.28643, Acc : 0.897, Sensitive_Loss : 0.06188, Sensitive_Acc : 17.700, Run Time : 7.05 sec
INFO:root:2024-04-27 14:32:57, Train, Epoch : 6, Step : 3730, Loss : 0.22343, Acc : 0.900, Sensitive_Loss : 0.05896, Sensitive_Acc : 16.100, Run Time : 7.37 sec
INFO:root:2024-04-27 14:33:04, Train, Epoch : 6, Step : 3740, Loss : 0.28533, Acc : 0.869, Sensitive_Loss : 0.07543, Sensitive_Acc : 14.900, Run Time : 7.18 sec
INFO:root:2024-04-27 14:33:11, Train, Epoch : 6, Step : 3750, Loss : 0.31679, Acc : 0.887, Sensitive_Loss : 0.09232, Sensitive_Acc : 17.300, Run Time : 6.96 sec
INFO:root:2024-04-27 14:34:47
INFO:root:y_pred: [0.01971215 0.98139566 0.02348698 ... 0.8334681  0.00216144 0.7873142 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.5590222e-01 1.7069725e-03 6.8704545e-02 2.0359999e-05 9.9981648e-01
 1.3662004e-03 9.9999654e-01 9.9999428e-01 1.7265313e-03 8.5392278e-01
 9.9934870e-01 9.9989772e-01 9.9774265e-01 9.9937695e-01 2.7300853e-02
 9.9244529e-01 9.9948019e-01 2.4347467e-02 6.6526091e-01 8.3366764e-01
 9.9987733e-01 1.2235697e-01 9.9982220e-01 9.9834144e-01 9.9986267e-01
 9.9938154e-01 3.8208666e-05 9.9980301e-01 9.9965656e-01 5.9045208e-01
 2.7941884e-02 2.0491517e-01 5.8101147e-01 1.7754942e-02 1.2046713e-01
 4.2248119e-02 2.2743843e-01 2.8009340e-03 9.9960130e-01 9.9985576e-01
 2.1198680e-04 1.8458538e-04 9.7907674e-01 3.1243527e-04 9.9999273e-01
 9.9958295e-01 9.9977070e-01 9.8864579e-01 3.8220373e-03 9.9773824e-01
 9.9869508e-01 3.7363201e-02 4.9634784e-01 5.7950930e-04 8.5887143e-05
 1.3367721e-02 8.4937681e-03 1.9371061e-02 2.1706892e-03 1.6114417e-01
 1.0341370e-02 3.6682492e-01 2.1716699e-02 9.8932087e-01 5.5057305e-01
 9.9997520e-01 4.5852303e-03 9.9987805e-01 9.9981195e-01 9.5968825e-01
 2.7067623e-01 5.7512993e-01 2.9728822e-03 1.1085848e-01 2.4050053e-02
 2.2313664e-04 1.5763605e-01 2.0219423e-01 3.5217343e-04 9.9995947e-01
 9.9991727e-01 9.5576914e-05 1.5228772e-02 7.1907714e-03 9.7071159e-01
 9.4943643e-01 1.1745603e-02 2.3261717e-02 9.8135144e-01 9.9884439e-01
 9.9998689e-01 1.0537614e-02 3.6420329e-03 9.9939919e-01 4.3923792e-01
 2.4416281e-02 9.9370182e-01 9.9986553e-01 4.4847602e-05 1.0785086e-02
 9.9982303e-01 9.9931157e-01 9.9906570e-01 9.9532431e-01 1.2055971e-02
 3.1568732e-02 9.9702674e-01 9.9898905e-01 9.7478652e-01 1.8228167e-05
 9.9824667e-01 9.9591297e-01 1.3803918e-02 9.9957210e-01 9.9963391e-01
 9.9954945e-01 3.8387743e-01 9.9993181e-01 1.5637307e-01 4.7626123e-01
 9.9967396e-01 9.9943787e-01 2.7007973e-04 9.9942195e-01 9.9999201e-01
 5.1700985e-01 9.9893969e-01 7.7331327e-02 1.2943606e-03 9.8801440e-01
 9.9953544e-01 7.4918795e-04 2.9853826e-02 9.7291317e-04 9.9986541e-01
 9.9940145e-01 9.8488683e-01 1.0511406e-03 2.2495881e-02 9.9825627e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 14:34:47, Dev, Step : 3756, Loss : 0.42763, Acc : 0.826, Auc : 0.913, Sensitive_Loss : 0.13504, Sensitive_Acc : 16.721, Sensitive_Auc : 0.993, Mean auc: 0.913, Run Time : 91.97 sec
INFO:root:2024-04-27 14:34:52, Train, Epoch : 7, Step : 3760, Loss : 0.10338, Acc : 0.356, Sensitive_Loss : 0.05008, Sensitive_Acc : 6.900, Run Time : 4.06 sec
INFO:root:2024-04-27 14:34:59, Train, Epoch : 7, Step : 3770, Loss : 0.30086, Acc : 0.878, Sensitive_Loss : 0.07325, Sensitive_Acc : 17.300, Run Time : 6.94 sec
INFO:root:2024-04-27 14:35:06, Train, Epoch : 7, Step : 3780, Loss : 0.25251, Acc : 0.884, Sensitive_Loss : 0.07463, Sensitive_Acc : 16.600, Run Time : 7.56 sec
INFO:root:2024-04-27 14:35:13, Train, Epoch : 7, Step : 3790, Loss : 0.25655, Acc : 0.891, Sensitive_Loss : 0.10368, Sensitive_Acc : 16.500, Run Time : 7.13 sec
INFO:root:2024-04-27 14:35:20, Train, Epoch : 7, Step : 3800, Loss : 0.27461, Acc : 0.887, Sensitive_Loss : 0.12220, Sensitive_Acc : 18.400, Run Time : 6.65 sec
INFO:root:2024-04-27 14:36:54, Dev, Step : 3800, Loss : 0.42390, Acc : 0.826, Auc : 0.914, Sensitive_Loss : 0.13151, Sensitive_Acc : 16.821, Sensitive_Auc : 0.994, Mean auc: 0.914, Run Time : 93.56 sec
INFO:root:2024-04-27 14:36:59, Train, Epoch : 7, Step : 3810, Loss : 0.30323, Acc : 0.878, Sensitive_Loss : 0.09243, Sensitive_Acc : 16.100, Run Time : 99.17 sec
INFO:root:2024-04-27 14:37:06, Train, Epoch : 7, Step : 3820, Loss : 0.27420, Acc : 0.878, Sensitive_Loss : 0.09626, Sensitive_Acc : 16.600, Run Time : 6.87 sec
INFO:root:2024-04-27 14:37:13, Train, Epoch : 7, Step : 3830, Loss : 0.25838, Acc : 0.878, Sensitive_Loss : 0.06365, Sensitive_Acc : 16.200, Run Time : 7.25 sec
INFO:root:2024-04-27 14:37:20, Train, Epoch : 7, Step : 3840, Loss : 0.27206, Acc : 0.881, Sensitive_Loss : 0.06891, Sensitive_Acc : 16.700, Run Time : 7.20 sec
INFO:root:2024-04-27 14:37:28, Train, Epoch : 7, Step : 3850, Loss : 0.22924, Acc : 0.912, Sensitive_Loss : 0.07165, Sensitive_Acc : 16.300, Run Time : 7.52 sec
INFO:root:2024-04-27 14:37:35, Train, Epoch : 7, Step : 3860, Loss : 0.29457, Acc : 0.897, Sensitive_Loss : 0.10458, Sensitive_Acc : 16.600, Run Time : 6.88 sec
INFO:root:2024-04-27 14:37:42, Train, Epoch : 7, Step : 3870, Loss : 0.25194, Acc : 0.897, Sensitive_Loss : 0.08628, Sensitive_Acc : 16.500, Run Time : 7.32 sec
INFO:root:2024-04-27 14:37:49, Train, Epoch : 7, Step : 3880, Loss : 0.23257, Acc : 0.903, Sensitive_Loss : 0.07840, Sensitive_Acc : 15.500, Run Time : 7.21 sec
INFO:root:2024-04-27 14:37:56, Train, Epoch : 7, Step : 3890, Loss : 0.32759, Acc : 0.875, Sensitive_Loss : 0.07500, Sensitive_Acc : 15.400, Run Time : 6.98 sec
INFO:root:2024-04-27 14:38:04, Train, Epoch : 7, Step : 3900, Loss : 0.24326, Acc : 0.875, Sensitive_Loss : 0.10347, Sensitive_Acc : 17.300, Run Time : 7.27 sec
INFO:root:2024-04-27 14:39:37, Dev, Step : 3900, Loss : 0.43364, Acc : 0.823, Auc : 0.912, Sensitive_Loss : 0.14667, Sensitive_Acc : 16.750, Sensitive_Auc : 0.992, Mean auc: 0.912, Run Time : 93.09 sec
INFO:root:2024-04-27 14:39:43, Train, Epoch : 7, Step : 3910, Loss : 0.26306, Acc : 0.900, Sensitive_Loss : 0.08612, Sensitive_Acc : 14.700, Run Time : 98.86 sec
INFO:root:2024-04-27 14:39:49, Train, Epoch : 7, Step : 3920, Loss : 0.28500, Acc : 0.875, Sensitive_Loss : 0.07178, Sensitive_Acc : 16.000, Run Time : 6.75 sec
INFO:root:2024-04-27 14:39:57, Train, Epoch : 7, Step : 3930, Loss : 0.22768, Acc : 0.881, Sensitive_Loss : 0.08659, Sensitive_Acc : 16.200, Run Time : 7.60 sec
INFO:root:2024-04-27 14:40:04, Train, Epoch : 7, Step : 3940, Loss : 0.24606, Acc : 0.891, Sensitive_Loss : 0.07231, Sensitive_Acc : 16.900, Run Time : 7.04 sec
INFO:root:2024-04-27 14:40:11, Train, Epoch : 7, Step : 3950, Loss : 0.26042, Acc : 0.887, Sensitive_Loss : 0.09974, Sensitive_Acc : 14.800, Run Time : 7.05 sec
INFO:root:2024-04-27 14:40:18, Train, Epoch : 7, Step : 3960, Loss : 0.25931, Acc : 0.897, Sensitive_Loss : 0.11054, Sensitive_Acc : 16.300, Run Time : 7.49 sec
INFO:root:2024-04-27 14:40:25, Train, Epoch : 7, Step : 3970, Loss : 0.33839, Acc : 0.875, Sensitive_Loss : 0.11055, Sensitive_Acc : 17.200, Run Time : 6.70 sec
INFO:root:2024-04-27 14:40:33, Train, Epoch : 7, Step : 3980, Loss : 0.27269, Acc : 0.875, Sensitive_Loss : 0.14479, Sensitive_Acc : 16.700, Run Time : 7.80 sec
INFO:root:2024-04-27 14:40:40, Train, Epoch : 7, Step : 3990, Loss : 0.25773, Acc : 0.878, Sensitive_Loss : 0.09605, Sensitive_Acc : 15.700, Run Time : 6.82 sec
INFO:root:2024-04-27 14:40:47, Train, Epoch : 7, Step : 4000, Loss : 0.30424, Acc : 0.887, Sensitive_Loss : 0.09054, Sensitive_Acc : 15.900, Run Time : 6.82 sec
INFO:root:2024-04-27 14:42:21, Dev, Step : 4000, Loss : 0.44556, Acc : 0.819, Auc : 0.910, Sensitive_Loss : 0.12104, Sensitive_Acc : 16.807, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 94.79 sec
INFO:root:2024-04-27 14:42:27, Train, Epoch : 7, Step : 4010, Loss : 0.30807, Acc : 0.872, Sensitive_Loss : 0.07564, Sensitive_Acc : 16.500, Run Time : 99.98 sec
INFO:root:2024-04-27 14:42:34, Train, Epoch : 7, Step : 4020, Loss : 0.22891, Acc : 0.897, Sensitive_Loss : 0.09937, Sensitive_Acc : 17.700, Run Time : 7.33 sec
INFO:root:2024-04-27 14:42:41, Train, Epoch : 7, Step : 4030, Loss : 0.28032, Acc : 0.897, Sensitive_Loss : 0.08047, Sensitive_Acc : 15.000, Run Time : 7.12 sec
INFO:root:2024-04-27 14:42:48, Train, Epoch : 7, Step : 4040, Loss : 0.29850, Acc : 0.866, Sensitive_Loss : 0.09039, Sensitive_Acc : 16.300, Run Time : 7.27 sec
INFO:root:2024-04-27 14:42:56, Train, Epoch : 7, Step : 4050, Loss : 0.26076, Acc : 0.884, Sensitive_Loss : 0.10191, Sensitive_Acc : 17.600, Run Time : 7.57 sec
INFO:root:2024-04-27 14:43:03, Train, Epoch : 7, Step : 4060, Loss : 0.33748, Acc : 0.859, Sensitive_Loss : 0.09091, Sensitive_Acc : 15.700, Run Time : 7.25 sec
INFO:root:2024-04-27 14:43:10, Train, Epoch : 7, Step : 4070, Loss : 0.24831, Acc : 0.903, Sensitive_Loss : 0.09272, Sensitive_Acc : 15.400, Run Time : 6.79 sec
INFO:root:2024-04-27 14:43:17, Train, Epoch : 7, Step : 4080, Loss : 0.29647, Acc : 0.878, Sensitive_Loss : 0.09983, Sensitive_Acc : 16.600, Run Time : 7.53 sec
INFO:root:2024-04-27 14:43:25, Train, Epoch : 7, Step : 4090, Loss : 0.25823, Acc : 0.897, Sensitive_Loss : 0.09628, Sensitive_Acc : 17.700, Run Time : 7.35 sec
INFO:root:2024-04-27 14:43:32, Train, Epoch : 7, Step : 4100, Loss : 0.28083, Acc : 0.884, Sensitive_Loss : 0.07197, Sensitive_Acc : 16.400, Run Time : 7.30 sec
INFO:root:2024-04-27 14:45:06, Dev, Step : 4100, Loss : 0.44590, Acc : 0.818, Auc : 0.908, Sensitive_Loss : 0.13087, Sensitive_Acc : 16.821, Sensitive_Auc : 0.993, Mean auc: 0.908, Run Time : 93.78 sec
INFO:root:2024-04-27 14:45:12, Train, Epoch : 7, Step : 4110, Loss : 0.25841, Acc : 0.875, Sensitive_Loss : 0.07029, Sensitive_Acc : 16.400, Run Time : 99.48 sec
INFO:root:2024-04-27 14:45:19, Train, Epoch : 7, Step : 4120, Loss : 0.26873, Acc : 0.878, Sensitive_Loss : 0.06977, Sensitive_Acc : 16.300, Run Time : 7.18 sec
INFO:root:2024-04-27 14:45:26, Train, Epoch : 7, Step : 4130, Loss : 0.28582, Acc : 0.897, Sensitive_Loss : 0.07164, Sensitive_Acc : 15.800, Run Time : 7.35 sec
INFO:root:2024-04-27 14:45:35, Train, Epoch : 7, Step : 4140, Loss : 0.29624, Acc : 0.881, Sensitive_Loss : 0.08321, Sensitive_Acc : 18.500, Run Time : 8.69 sec
INFO:root:2024-04-27 14:45:42, Train, Epoch : 7, Step : 4150, Loss : 0.28067, Acc : 0.881, Sensitive_Loss : 0.08655, Sensitive_Acc : 17.200, Run Time : 7.21 sec
INFO:root:2024-04-27 14:45:49, Train, Epoch : 7, Step : 4160, Loss : 0.33771, Acc : 0.859, Sensitive_Loss : 0.05913, Sensitive_Acc : 16.200, Run Time : 7.05 sec
INFO:root:2024-04-27 14:45:56, Train, Epoch : 7, Step : 4170, Loss : 0.25868, Acc : 0.903, Sensitive_Loss : 0.05766, Sensitive_Acc : 16.200, Run Time : 6.81 sec
INFO:root:2024-04-27 14:46:03, Train, Epoch : 7, Step : 4180, Loss : 0.24039, Acc : 0.897, Sensitive_Loss : 0.07261, Sensitive_Acc : 16.900, Run Time : 7.41 sec
INFO:root:2024-04-27 14:46:10, Train, Epoch : 7, Step : 4190, Loss : 0.24513, Acc : 0.912, Sensitive_Loss : 0.07287, Sensitive_Acc : 16.800, Run Time : 7.10 sec
INFO:root:2024-04-27 14:46:18, Train, Epoch : 7, Step : 4200, Loss : 0.26373, Acc : 0.903, Sensitive_Loss : 0.10489, Sensitive_Acc : 16.000, Run Time : 7.28 sec
INFO:root:2024-04-27 14:47:51, Dev, Step : 4200, Loss : 0.42064, Acc : 0.822, Auc : 0.911, Sensitive_Loss : 0.12218, Sensitive_Acc : 16.807, Sensitive_Auc : 0.994, Mean auc: 0.911, Run Time : 93.41 sec
INFO:root:2024-04-27 14:47:57, Train, Epoch : 7, Step : 4210, Loss : 0.25282, Acc : 0.894, Sensitive_Loss : 0.08337, Sensitive_Acc : 16.500, Run Time : 98.93 sec
INFO:root:2024-04-27 14:48:04, Train, Epoch : 7, Step : 4220, Loss : 0.26483, Acc : 0.859, Sensitive_Loss : 0.06917, Sensitive_Acc : 17.200, Run Time : 7.13 sec
INFO:root:2024-04-27 14:48:11, Train, Epoch : 7, Step : 4230, Loss : 0.25222, Acc : 0.887, Sensitive_Loss : 0.07002, Sensitive_Acc : 17.000, Run Time : 7.27 sec
INFO:root:2024-04-27 14:48:19, Train, Epoch : 7, Step : 4240, Loss : 0.32369, Acc : 0.856, Sensitive_Loss : 0.07574, Sensitive_Acc : 16.200, Run Time : 7.59 sec
INFO:root:2024-04-27 14:48:26, Train, Epoch : 7, Step : 4250, Loss : 0.28492, Acc : 0.863, Sensitive_Loss : 0.08787, Sensitive_Acc : 16.400, Run Time : 7.63 sec
INFO:root:2024-04-27 14:48:33, Train, Epoch : 7, Step : 4260, Loss : 0.30871, Acc : 0.863, Sensitive_Loss : 0.06276, Sensitive_Acc : 17.000, Run Time : 6.85 sec
INFO:root:2024-04-27 14:48:40, Train, Epoch : 7, Step : 4270, Loss : 0.26840, Acc : 0.897, Sensitive_Loss : 0.08294, Sensitive_Acc : 17.800, Run Time : 7.33 sec
INFO:root:2024-04-27 14:48:47, Train, Epoch : 7, Step : 4280, Loss : 0.29036, Acc : 0.869, Sensitive_Loss : 0.10498, Sensitive_Acc : 17.600, Run Time : 6.96 sec
INFO:root:2024-04-27 14:48:54, Train, Epoch : 7, Step : 4290, Loss : 0.24601, Acc : 0.912, Sensitive_Loss : 0.10563, Sensitive_Acc : 15.400, Run Time : 7.00 sec
INFO:root:2024-04-27 14:49:02, Train, Epoch : 7, Step : 4300, Loss : 0.28537, Acc : 0.878, Sensitive_Loss : 0.07204, Sensitive_Acc : 15.400, Run Time : 7.22 sec
INFO:root:2024-04-27 14:50:35, Dev, Step : 4300, Loss : 0.44162, Acc : 0.820, Auc : 0.912, Sensitive_Loss : 0.12748, Sensitive_Acc : 16.850, Sensitive_Auc : 0.994, Mean auc: 0.912, Run Time : 93.64 sec
INFO:root:2024-04-27 14:50:41, Train, Epoch : 7, Step : 4310, Loss : 0.26364, Acc : 0.881, Sensitive_Loss : 0.05605, Sensitive_Acc : 15.900, Run Time : 99.18 sec
INFO:root:2024-04-27 14:50:48, Train, Epoch : 7, Step : 4320, Loss : 0.30960, Acc : 0.863, Sensitive_Loss : 0.10321, Sensitive_Acc : 15.600, Run Time : 7.18 sec
INFO:root:2024-04-27 14:50:55, Train, Epoch : 7, Step : 4330, Loss : 0.23753, Acc : 0.900, Sensitive_Loss : 0.06857, Sensitive_Acc : 17.600, Run Time : 6.99 sec
INFO:root:2024-04-27 14:51:04, Train, Epoch : 7, Step : 4340, Loss : 0.25421, Acc : 0.894, Sensitive_Loss : 0.06746, Sensitive_Acc : 15.700, Run Time : 9.19 sec
INFO:root:2024-04-27 14:51:12, Train, Epoch : 7, Step : 4350, Loss : 0.27957, Acc : 0.884, Sensitive_Loss : 0.06632, Sensitive_Acc : 15.800, Run Time : 7.41 sec
INFO:root:2024-04-27 14:51:19, Train, Epoch : 7, Step : 4360, Loss : 0.26966, Acc : 0.881, Sensitive_Loss : 0.07782, Sensitive_Acc : 16.100, Run Time : 7.46 sec
INFO:root:2024-04-27 14:51:26, Train, Epoch : 7, Step : 4370, Loss : 0.31349, Acc : 0.834, Sensitive_Loss : 0.06239, Sensitive_Acc : 15.100, Run Time : 7.33 sec
INFO:root:2024-04-27 14:51:33, Train, Epoch : 7, Step : 4380, Loss : 0.21799, Acc : 0.909, Sensitive_Loss : 0.12806, Sensitive_Acc : 14.900, Run Time : 6.92 sec
INFO:root:2024-04-27 14:53:07
INFO:root:y_pred: [0.00533126 0.9796196  0.01593265 ... 0.6999196  0.00275644 0.7296348 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.83844697e-01 2.06071860e-03 9.02629644e-02 1.41768624e-05
 9.99860644e-01 2.79919337e-03 9.99998212e-01 9.99995232e-01
 1.88739237e-03 9.15172875e-01 9.99556959e-01 9.99929309e-01
 9.98112440e-01 9.99465406e-01 2.33368222e-02 9.94368613e-01
 9.99713957e-01 1.90566257e-02 7.14638293e-01 8.92590106e-01
 9.99898911e-01 7.72822276e-02 9.99836087e-01 9.98478115e-01
 9.99920249e-01 9.99655843e-01 7.88012767e-05 9.99750197e-01
 9.99363601e-01 3.39969516e-01 3.16163115e-02 2.90630341e-01
 7.00558960e-01 1.58985052e-02 1.26963675e-01 6.18797205e-02
 2.93419361e-01 6.43792609e-03 9.99732316e-01 9.99873281e-01
 1.02508435e-04 1.86055026e-04 9.83668387e-01 5.73788071e-04
 9.99998450e-01 9.99584258e-01 9.99886632e-01 9.89584267e-01
 4.40341886e-03 9.98873174e-01 9.98408139e-01 4.64185327e-02
 6.61132097e-01 4.35852999e-04 2.33399856e-04 1.76175646e-02
 6.32714108e-03 9.03781503e-03 1.50343624e-03 1.08556367e-01
 9.97019839e-03 3.02291334e-01 1.77955274e-02 9.95296180e-01
 5.26250184e-01 9.99990106e-01 5.08211972e-03 9.99917746e-01
 9.99813735e-01 9.47723746e-01 5.92495680e-01 6.67441905e-01
 4.96706273e-03 1.18855186e-01 2.99917907e-02 9.61084326e-04
 1.80264980e-01 2.47875065e-01 2.24641582e-04 9.99970317e-01
 9.99969840e-01 4.34227295e-05 1.19554633e-02 1.05151553e-02
 9.53852713e-01 9.79662597e-01 1.15038492e-02 2.38040518e-02
 9.88784909e-01 9.98838723e-01 9.99994755e-01 2.71616294e-03
 4.95281257e-03 9.99266326e-01 5.61053634e-01 4.67738323e-02
 9.90143061e-01 9.99952197e-01 6.96843199e-05 2.70315763e-02
 9.99819696e-01 9.99590576e-01 9.99708235e-01 9.97716188e-01
 2.08739992e-02 9.26648453e-02 9.97678101e-01 9.99295354e-01
 9.88238275e-01 1.12474154e-05 9.98358309e-01 9.99228716e-01
 2.26489715e-02 9.99771535e-01 9.99672771e-01 9.99772489e-01
 2.81269401e-01 9.99956489e-01 1.62615404e-01 5.51325023e-01
 9.99873757e-01 9.99511003e-01 1.08193170e-04 9.99813974e-01
 9.99996781e-01 4.77123827e-01 9.99604642e-01 9.01606604e-02
 2.58761970e-03 9.95553315e-01 9.99720395e-01 6.74296869e-04
 2.67163496e-02 8.70894175e-04 9.99886036e-01 9.99507308e-01
 9.88837123e-01 4.64356446e-04 2.95225997e-02 9.98833716e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 14:53:07, Dev, Step : 4382, Loss : 0.46066, Acc : 0.812, Auc : 0.911, Sensitive_Loss : 0.14384, Sensitive_Acc : 16.779, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 92.64 sec
INFO:root:2024-04-27 14:53:15, Train, Epoch : 8, Step : 4390, Loss : 0.24427, Acc : 0.684, Sensitive_Loss : 0.07267, Sensitive_Acc : 13.200, Run Time : 6.90 sec
INFO:root:2024-04-27 14:53:22, Train, Epoch : 8, Step : 4400, Loss : 0.22097, Acc : 0.900, Sensitive_Loss : 0.09388, Sensitive_Acc : 15.500, Run Time : 7.58 sec
INFO:root:2024-04-27 14:54:58, Dev, Step : 4400, Loss : 0.43438, Acc : 0.820, Auc : 0.911, Sensitive_Loss : 0.12523, Sensitive_Acc : 16.879, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 95.89 sec
INFO:root:2024-04-27 14:55:04, Train, Epoch : 8, Step : 4410, Loss : 0.23948, Acc : 0.900, Sensitive_Loss : 0.07318, Sensitive_Acc : 15.100, Run Time : 101.62 sec
INFO:root:2024-04-27 14:55:12, Train, Epoch : 8, Step : 4420, Loss : 0.22733, Acc : 0.900, Sensitive_Loss : 0.11630, Sensitive_Acc : 16.900, Run Time : 8.75 sec
INFO:root:2024-04-27 14:55:20, Train, Epoch : 8, Step : 4430, Loss : 0.23494, Acc : 0.925, Sensitive_Loss : 0.07912, Sensitive_Acc : 16.000, Run Time : 7.28 sec
INFO:root:2024-04-27 14:55:27, Train, Epoch : 8, Step : 4440, Loss : 0.27773, Acc : 0.891, Sensitive_Loss : 0.07696, Sensitive_Acc : 15.700, Run Time : 7.12 sec
INFO:root:2024-04-27 14:55:34, Train, Epoch : 8, Step : 4450, Loss : 0.24461, Acc : 0.912, Sensitive_Loss : 0.07432, Sensitive_Acc : 16.800, Run Time : 7.38 sec
INFO:root:2024-04-27 14:55:44, Train, Epoch : 8, Step : 4460, Loss : 0.27621, Acc : 0.881, Sensitive_Loss : 0.06968, Sensitive_Acc : 16.400, Run Time : 9.54 sec
INFO:root:2024-04-27 14:55:51, Train, Epoch : 8, Step : 4470, Loss : 0.30804, Acc : 0.894, Sensitive_Loss : 0.10559, Sensitive_Acc : 18.200, Run Time : 6.99 sec
INFO:root:2024-04-27 14:55:58, Train, Epoch : 8, Step : 4480, Loss : 0.28581, Acc : 0.853, Sensitive_Loss : 0.10514, Sensitive_Acc : 17.800, Run Time : 7.17 sec
INFO:root:2024-04-27 14:56:05, Train, Epoch : 8, Step : 4490, Loss : 0.21506, Acc : 0.909, Sensitive_Loss : 0.06328, Sensitive_Acc : 17.700, Run Time : 6.71 sec
INFO:root:2024-04-27 14:56:12, Train, Epoch : 8, Step : 4500, Loss : 0.25582, Acc : 0.906, Sensitive_Loss : 0.11940, Sensitive_Acc : 16.600, Run Time : 7.02 sec
INFO:root:2024-04-27 14:57:49, Dev, Step : 4500, Loss : 0.41604, Acc : 0.828, Auc : 0.909, Sensitive_Loss : 0.13069, Sensitive_Acc : 16.864, Sensitive_Auc : 0.992, Mean auc: 0.909, Run Time : 97.47 sec
INFO:root:2024-04-27 14:57:55, Train, Epoch : 8, Step : 4510, Loss : 0.29785, Acc : 0.878, Sensitive_Loss : 0.05249, Sensitive_Acc : 16.000, Run Time : 102.87 sec
INFO:root:2024-04-27 14:58:03, Train, Epoch : 8, Step : 4520, Loss : 0.24748, Acc : 0.891, Sensitive_Loss : 0.07092, Sensitive_Acc : 16.500, Run Time : 8.66 sec
INFO:root:2024-04-27 14:58:11, Train, Epoch : 8, Step : 4530, Loss : 0.24814, Acc : 0.891, Sensitive_Loss : 0.08510, Sensitive_Acc : 15.700, Run Time : 7.39 sec
INFO:root:2024-04-27 14:58:18, Train, Epoch : 8, Step : 4540, Loss : 0.26600, Acc : 0.900, Sensitive_Loss : 0.08883, Sensitive_Acc : 15.400, Run Time : 7.48 sec
INFO:root:2024-04-27 14:58:25, Train, Epoch : 8, Step : 4550, Loss : 0.24476, Acc : 0.900, Sensitive_Loss : 0.10837, Sensitive_Acc : 17.900, Run Time : 7.08 sec
INFO:root:2024-04-27 14:58:32, Train, Epoch : 8, Step : 4560, Loss : 0.22551, Acc : 0.909, Sensitive_Loss : 0.06609, Sensitive_Acc : 16.900, Run Time : 7.14 sec
INFO:root:2024-04-27 14:58:39, Train, Epoch : 8, Step : 4570, Loss : 0.24551, Acc : 0.900, Sensitive_Loss : 0.07089, Sensitive_Acc : 17.200, Run Time : 6.76 sec
INFO:root:2024-04-27 14:58:48, Train, Epoch : 8, Step : 4580, Loss : 0.23799, Acc : 0.897, Sensitive_Loss : 0.08963, Sensitive_Acc : 17.900, Run Time : 8.69 sec
INFO:root:2024-04-27 14:58:55, Train, Epoch : 8, Step : 4590, Loss : 0.27647, Acc : 0.866, Sensitive_Loss : 0.08933, Sensitive_Acc : 15.700, Run Time : 7.51 sec
INFO:root:2024-04-27 14:59:03, Train, Epoch : 8, Step : 4600, Loss : 0.32378, Acc : 0.884, Sensitive_Loss : 0.08674, Sensitive_Acc : 16.700, Run Time : 7.73 sec
INFO:root:2024-04-27 15:00:42, Dev, Step : 4600, Loss : 0.41976, Acc : 0.824, Auc : 0.906, Sensitive_Loss : 0.11501, Sensitive_Acc : 16.879, Sensitive_Auc : 0.993, Mean auc: 0.906, Run Time : 98.61 sec
INFO:root:2024-04-27 15:00:47, Train, Epoch : 8, Step : 4610, Loss : 0.26989, Acc : 0.881, Sensitive_Loss : 0.07174, Sensitive_Acc : 16.400, Run Time : 104.25 sec
INFO:root:2024-04-27 15:00:56, Train, Epoch : 8, Step : 4620, Loss : 0.26512, Acc : 0.878, Sensitive_Loss : 0.09293, Sensitive_Acc : 15.800, Run Time : 8.53 sec
INFO:root:2024-04-27 15:01:03, Train, Epoch : 8, Step : 4630, Loss : 0.21609, Acc : 0.922, Sensitive_Loss : 0.08733, Sensitive_Acc : 15.100, Run Time : 7.09 sec
INFO:root:2024-04-27 15:01:10, Train, Epoch : 8, Step : 4640, Loss : 0.31363, Acc : 0.875, Sensitive_Loss : 0.05653, Sensitive_Acc : 16.100, Run Time : 7.40 sec
INFO:root:2024-04-27 15:01:18, Train, Epoch : 8, Step : 4650, Loss : 0.33251, Acc : 0.872, Sensitive_Loss : 0.09117, Sensitive_Acc : 16.700, Run Time : 7.42 sec
INFO:root:2024-04-27 15:01:25, Train, Epoch : 8, Step : 4660, Loss : 0.24083, Acc : 0.878, Sensitive_Loss : 0.11109, Sensitive_Acc : 16.200, Run Time : 7.55 sec
INFO:root:2024-04-27 15:01:33, Train, Epoch : 8, Step : 4670, Loss : 0.25333, Acc : 0.887, Sensitive_Loss : 0.09428, Sensitive_Acc : 17.700, Run Time : 8.16 sec
INFO:root:2024-04-27 15:01:41, Train, Epoch : 8, Step : 4680, Loss : 0.24072, Acc : 0.894, Sensitive_Loss : 0.07077, Sensitive_Acc : 16.600, Run Time : 7.61 sec
INFO:root:2024-04-27 15:01:48, Train, Epoch : 8, Step : 4690, Loss : 0.25163, Acc : 0.916, Sensitive_Loss : 0.09654, Sensitive_Acc : 16.800, Run Time : 7.14 sec
INFO:root:2024-04-27 15:01:55, Train, Epoch : 8, Step : 4700, Loss : 0.25480, Acc : 0.900, Sensitive_Loss : 0.08235, Sensitive_Acc : 18.000, Run Time : 6.75 sec
INFO:root:2024-04-27 15:03:31, Dev, Step : 4700, Loss : 0.44102, Acc : 0.820, Auc : 0.909, Sensitive_Loss : 0.12616, Sensitive_Acc : 16.864, Sensitive_Auc : 0.993, Mean auc: 0.909, Run Time : 95.96 sec
INFO:root:2024-04-27 15:03:39, Train, Epoch : 8, Step : 4710, Loss : 0.22921, Acc : 0.916, Sensitive_Loss : 0.06589, Sensitive_Acc : 15.900, Run Time : 104.43 sec
INFO:root:2024-04-27 15:03:46, Train, Epoch : 8, Step : 4720, Loss : 0.23919, Acc : 0.919, Sensitive_Loss : 0.09071, Sensitive_Acc : 15.800, Run Time : 7.04 sec
INFO:root:2024-04-27 15:03:53, Train, Epoch : 8, Step : 4730, Loss : 0.22162, Acc : 0.881, Sensitive_Loss : 0.10546, Sensitive_Acc : 16.100, Run Time : 7.10 sec
INFO:root:2024-04-27 15:04:04, Train, Epoch : 8, Step : 4740, Loss : 0.24862, Acc : 0.891, Sensitive_Loss : 0.05337, Sensitive_Acc : 15.900, Run Time : 10.76 sec
INFO:root:2024-04-27 15:04:12, Train, Epoch : 8, Step : 4750, Loss : 0.24530, Acc : 0.872, Sensitive_Loss : 0.12343, Sensitive_Acc : 15.400, Run Time : 7.34 sec
INFO:root:2024-04-27 15:04:21, Train, Epoch : 8, Step : 4760, Loss : 0.22457, Acc : 0.909, Sensitive_Loss : 0.04415, Sensitive_Acc : 15.800, Run Time : 9.18 sec
INFO:root:2024-04-27 15:04:28, Train, Epoch : 8, Step : 4770, Loss : 0.27556, Acc : 0.884, Sensitive_Loss : 0.08720, Sensitive_Acc : 15.700, Run Time : 7.55 sec
INFO:root:2024-04-27 15:04:35, Train, Epoch : 8, Step : 4780, Loss : 0.22763, Acc : 0.900, Sensitive_Loss : 0.07336, Sensitive_Acc : 16.700, Run Time : 7.18 sec
INFO:root:2024-04-27 15:04:44, Train, Epoch : 8, Step : 4790, Loss : 0.26528, Acc : 0.875, Sensitive_Loss : 0.10956, Sensitive_Acc : 17.600, Run Time : 8.75 sec
INFO:root:2024-04-27 15:04:51, Train, Epoch : 8, Step : 4800, Loss : 0.24909, Acc : 0.875, Sensitive_Loss : 0.11132, Sensitive_Acc : 16.400, Run Time : 6.89 sec
INFO:root:2024-04-27 15:06:26, Dev, Step : 4800, Loss : 0.44520, Acc : 0.821, Auc : 0.908, Sensitive_Loss : 0.12888, Sensitive_Acc : 16.850, Sensitive_Auc : 0.994, Mean auc: 0.908, Run Time : 94.85 sec
INFO:root:2024-04-27 15:06:33, Train, Epoch : 8, Step : 4810, Loss : 0.28615, Acc : 0.884, Sensitive_Loss : 0.09360, Sensitive_Acc : 17.700, Run Time : 102.02 sec
INFO:root:2024-04-27 15:06:40, Train, Epoch : 8, Step : 4820, Loss : 0.24541, Acc : 0.894, Sensitive_Loss : 0.10171, Sensitive_Acc : 15.100, Run Time : 6.95 sec
INFO:root:2024-04-27 15:06:47, Train, Epoch : 8, Step : 4830, Loss : 0.18966, Acc : 0.925, Sensitive_Loss : 0.10405, Sensitive_Acc : 16.400, Run Time : 6.83 sec
INFO:root:2024-04-27 15:06:54, Train, Epoch : 8, Step : 4840, Loss : 0.22770, Acc : 0.912, Sensitive_Loss : 0.09004, Sensitive_Acc : 16.000, Run Time : 7.48 sec
INFO:root:2024-04-27 15:07:01, Train, Epoch : 8, Step : 4850, Loss : 0.23993, Acc : 0.872, Sensitive_Loss : 0.09935, Sensitive_Acc : 15.700, Run Time : 7.01 sec
INFO:root:2024-04-27 15:07:09, Train, Epoch : 8, Step : 4860, Loss : 0.25100, Acc : 0.884, Sensitive_Loss : 0.07701, Sensitive_Acc : 16.500, Run Time : 7.81 sec
INFO:root:2024-04-27 15:07:16, Train, Epoch : 8, Step : 4870, Loss : 0.29965, Acc : 0.891, Sensitive_Loss : 0.08680, Sensitive_Acc : 17.300, Run Time : 6.43 sec
INFO:root:2024-04-27 15:07:24, Train, Epoch : 8, Step : 4880, Loss : 0.25092, Acc : 0.894, Sensitive_Loss : 0.11444, Sensitive_Acc : 16.400, Run Time : 8.47 sec
INFO:root:2024-04-27 15:07:31, Train, Epoch : 8, Step : 4890, Loss : 0.22346, Acc : 0.912, Sensitive_Loss : 0.08777, Sensitive_Acc : 16.200, Run Time : 7.24 sec
INFO:root:2024-04-27 15:07:39, Train, Epoch : 8, Step : 4900, Loss : 0.22180, Acc : 0.919, Sensitive_Loss : 0.07775, Sensitive_Acc : 16.800, Run Time : 7.50 sec
INFO:root:2024-04-27 15:09:19, Dev, Step : 4900, Loss : 0.41609, Acc : 0.827, Auc : 0.911, Sensitive_Loss : 0.11363, Sensitive_Acc : 16.879, Sensitive_Auc : 0.992, Mean auc: 0.911, Run Time : 99.69 sec
INFO:root:2024-04-27 15:09:24, Train, Epoch : 8, Step : 4910, Loss : 0.28099, Acc : 0.894, Sensitive_Loss : 0.08836, Sensitive_Acc : 17.300, Run Time : 105.13 sec
INFO:root:2024-04-27 15:09:31, Train, Epoch : 8, Step : 4920, Loss : 0.30896, Acc : 0.859, Sensitive_Loss : 0.08664, Sensitive_Acc : 18.200, Run Time : 7.21 sec
INFO:root:2024-04-27 15:09:38, Train, Epoch : 8, Step : 4930, Loss : 0.26032, Acc : 0.897, Sensitive_Loss : 0.05757, Sensitive_Acc : 15.900, Run Time : 7.15 sec
INFO:root:2024-04-27 15:09:46, Train, Epoch : 8, Step : 4940, Loss : 0.24078, Acc : 0.900, Sensitive_Loss : 0.05558, Sensitive_Acc : 15.800, Run Time : 7.40 sec
INFO:root:2024-04-27 15:09:53, Train, Epoch : 8, Step : 4950, Loss : 0.24229, Acc : 0.881, Sensitive_Loss : 0.09128, Sensitive_Acc : 16.500, Run Time : 6.94 sec
INFO:root:2024-04-27 15:10:00, Train, Epoch : 8, Step : 4960, Loss : 0.23203, Acc : 0.903, Sensitive_Loss : 0.07971, Sensitive_Acc : 16.800, Run Time : 6.95 sec
INFO:root:2024-04-27 15:10:07, Train, Epoch : 8, Step : 4970, Loss : 0.24385, Acc : 0.884, Sensitive_Loss : 0.08231, Sensitive_Acc : 16.100, Run Time : 7.41 sec
INFO:root:2024-04-27 15:10:14, Train, Epoch : 8, Step : 4980, Loss : 0.21847, Acc : 0.894, Sensitive_Loss : 0.09283, Sensitive_Acc : 14.000, Run Time : 7.18 sec
INFO:root:2024-04-27 15:10:21, Train, Epoch : 8, Step : 4990, Loss : 0.22917, Acc : 0.916, Sensitive_Loss : 0.06152, Sensitive_Acc : 16.600, Run Time : 7.07 sec
INFO:root:2024-04-27 15:10:28, Train, Epoch : 8, Step : 5000, Loss : 0.23662, Acc : 0.897, Sensitive_Loss : 0.07786, Sensitive_Acc : 17.200, Run Time : 6.91 sec
INFO:root:2024-04-27 15:12:01, Dev, Step : 5000, Loss : 0.44710, Acc : 0.824, Auc : 0.911, Sensitive_Loss : 0.12194, Sensitive_Acc : 16.750, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 93.22 sec
INFO:root:2024-04-27 15:13:37
INFO:root:y_pred: [5.345261e-03 9.932166e-01 8.364020e-03 ... 8.430594e-01 8.279498e-04
 8.769523e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.70987320e-01 1.02269847e-03 3.00866961e-02 7.83488940e-06
 9.99682546e-01 2.39271182e-03 9.99977589e-01 9.99993443e-01
 1.33243634e-03 8.82817328e-01 9.99273956e-01 9.99850154e-01
 9.98158157e-01 9.99030352e-01 1.33804921e-02 9.91976082e-01
 9.99427438e-01 1.13736009e-02 4.82673913e-01 8.28573406e-01
 9.99627233e-01 7.23733082e-02 9.99614835e-01 9.96896744e-01
 9.99906778e-01 9.98746872e-01 4.17708507e-05 9.99645233e-01
 9.99113381e-01 4.17808384e-01 1.45148970e-02 6.16337955e-02
 6.11458540e-01 5.69453510e-03 1.34145454e-01 4.93028276e-02
 1.56289428e-01 4.27287351e-03 9.99095798e-01 9.99784648e-01
 7.87726531e-05 3.18343838e-04 9.64543283e-01 3.68043286e-04
 9.99987245e-01 9.98670220e-01 9.99368250e-01 9.87594724e-01
 4.93643200e-03 9.98263657e-01 9.96481895e-01 1.04731675e-02
 4.22763199e-01 1.82544565e-04 2.80564709e-05 5.22712525e-03
 7.55537115e-03 1.09069105e-02 1.12109212e-03 6.76233321e-02
 5.54180937e-03 1.16616949e-01 6.85454346e-03 9.88340616e-01
 3.11567932e-01 9.99958158e-01 6.22417359e-03 9.99783814e-01
 9.99608815e-01 9.13917243e-01 4.46984172e-01 4.76838201e-01
 1.42189919e-03 3.09401117e-02 1.88505966e-02 3.97347409e-04
 1.32098079e-01 1.66066974e-01 1.27219420e-04 9.99933362e-01
 9.99915719e-01 2.63268412e-05 2.47347471e-03 3.70415626e-03
 8.95322621e-01 8.81993651e-01 4.19603102e-03 9.38829221e-03
 9.79049742e-01 9.98054147e-01 9.99985933e-01 3.79086309e-03
 4.49692365e-03 9.99218941e-01 3.46875101e-01 2.75962278e-02
 9.77078855e-01 9.99872923e-01 2.31287759e-05 1.37022631e-02
 9.99667168e-01 9.98162568e-01 9.99095798e-01 9.89283323e-01
 2.10105628e-02 4.67063002e-02 9.97673929e-01 9.98721421e-01
 9.81651306e-01 8.07316883e-06 9.96675730e-01 9.97760773e-01
 1.43234450e-02 9.99284327e-01 9.99131143e-01 9.99498844e-01
 1.28010273e-01 9.99912024e-01 1.33029342e-01 2.25127622e-01
 9.99454439e-01 9.98855710e-01 7.71678897e-05 9.99425054e-01
 9.99976993e-01 4.84214187e-01 9.98604596e-01 3.13121043e-02
 1.80316030e-03 9.72552001e-01 9.99597013e-01 3.92989401e-04
 6.42022630e-03 6.46201428e-04 9.99843478e-01 9.98784482e-01
 9.81072545e-01 2.75856321e-04 1.71405952e-02 9.97511744e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 15:13:37, Dev, Step : 5008, Loss : 0.43662, Acc : 0.826, Auc : 0.910, Sensitive_Loss : 0.11626, Sensitive_Acc : 16.879, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 91.55 sec
INFO:root:2024-04-27 15:13:41, Train, Epoch : 9, Step : 5010, Loss : 0.02981, Acc : 0.188, Sensitive_Loss : 0.01821, Sensitive_Acc : 3.300, Run Time : 2.90 sec
INFO:root:2024-04-27 15:13:48, Train, Epoch : 9, Step : 5020, Loss : 0.20698, Acc : 0.922, Sensitive_Loss : 0.10811, Sensitive_Acc : 16.600, Run Time : 7.04 sec
INFO:root:2024-04-27 15:13:55, Train, Epoch : 9, Step : 5030, Loss : 0.24352, Acc : 0.891, Sensitive_Loss : 0.08498, Sensitive_Acc : 15.900, Run Time : 6.87 sec
INFO:root:2024-04-27 15:14:02, Train, Epoch : 9, Step : 5040, Loss : 0.26732, Acc : 0.884, Sensitive_Loss : 0.07552, Sensitive_Acc : 16.100, Run Time : 7.02 sec
INFO:root:2024-04-27 15:14:09, Train, Epoch : 9, Step : 5050, Loss : 0.23928, Acc : 0.912, Sensitive_Loss : 0.08053, Sensitive_Acc : 16.800, Run Time : 7.04 sec
INFO:root:2024-04-27 15:14:16, Train, Epoch : 9, Step : 5060, Loss : 0.24559, Acc : 0.894, Sensitive_Loss : 0.06236, Sensitive_Acc : 16.400, Run Time : 7.11 sec
INFO:root:2024-04-27 15:14:22, Train, Epoch : 9, Step : 5070, Loss : 0.20306, Acc : 0.900, Sensitive_Loss : 0.10519, Sensitive_Acc : 17.000, Run Time : 6.71 sec
INFO:root:2024-04-27 15:14:30, Train, Epoch : 9, Step : 5080, Loss : 0.20417, Acc : 0.928, Sensitive_Loss : 0.06286, Sensitive_Acc : 17.400, Run Time : 7.40 sec
INFO:root:2024-04-27 15:14:37, Train, Epoch : 9, Step : 5090, Loss : 0.20835, Acc : 0.928, Sensitive_Loss : 0.07646, Sensitive_Acc : 16.500, Run Time : 7.67 sec
INFO:root:2024-04-27 15:14:45, Train, Epoch : 9, Step : 5100, Loss : 0.21448, Acc : 0.906, Sensitive_Loss : 0.08051, Sensitive_Acc : 17.200, Run Time : 7.04 sec
INFO:root:2024-04-27 15:16:17, Dev, Step : 5100, Loss : 0.44493, Acc : 0.823, Auc : 0.909, Sensitive_Loss : 0.12887, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.909, Run Time : 92.78 sec
INFO:root:2024-04-27 15:16:23, Train, Epoch : 9, Step : 5110, Loss : 0.19406, Acc : 0.909, Sensitive_Loss : 0.06806, Sensitive_Acc : 15.700, Run Time : 98.24 sec
INFO:root:2024-04-27 15:16:30, Train, Epoch : 9, Step : 5120, Loss : 0.25538, Acc : 0.887, Sensitive_Loss : 0.10087, Sensitive_Acc : 15.000, Run Time : 7.18 sec
INFO:root:2024-04-27 15:16:38, Train, Epoch : 9, Step : 5130, Loss : 0.17367, Acc : 0.934, Sensitive_Loss : 0.10179, Sensitive_Acc : 17.700, Run Time : 7.65 sec
INFO:root:2024-04-27 15:16:44, Train, Epoch : 9, Step : 5140, Loss : 0.22152, Acc : 0.884, Sensitive_Loss : 0.06742, Sensitive_Acc : 16.000, Run Time : 6.79 sec
INFO:root:2024-04-27 15:16:52, Train, Epoch : 9, Step : 5150, Loss : 0.19161, Acc : 0.912, Sensitive_Loss : 0.04111, Sensitive_Acc : 15.600, Run Time : 7.52 sec
INFO:root:2024-04-27 15:16:59, Train, Epoch : 9, Step : 5160, Loss : 0.21875, Acc : 0.919, Sensitive_Loss : 0.09884, Sensitive_Acc : 16.700, Run Time : 6.76 sec
INFO:root:2024-04-27 15:17:06, Train, Epoch : 9, Step : 5170, Loss : 0.23730, Acc : 0.894, Sensitive_Loss : 0.07146, Sensitive_Acc : 16.600, Run Time : 7.20 sec
INFO:root:2024-04-27 15:17:13, Train, Epoch : 9, Step : 5180, Loss : 0.25876, Acc : 0.891, Sensitive_Loss : 0.06152, Sensitive_Acc : 16.700, Run Time : 6.93 sec
INFO:root:2024-04-27 15:17:20, Train, Epoch : 9, Step : 5190, Loss : 0.28562, Acc : 0.869, Sensitive_Loss : 0.09156, Sensitive_Acc : 18.100, Run Time : 7.28 sec
INFO:root:2024-04-27 15:17:27, Train, Epoch : 9, Step : 5200, Loss : 0.18348, Acc : 0.950, Sensitive_Loss : 0.06399, Sensitive_Acc : 15.200, Run Time : 7.30 sec
INFO:root:2024-04-27 15:19:00, Dev, Step : 5200, Loss : 0.45214, Acc : 0.821, Auc : 0.906, Sensitive_Loss : 0.12763, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.906, Run Time : 92.94 sec
INFO:root:2024-04-27 15:19:06, Train, Epoch : 9, Step : 5210, Loss : 0.31425, Acc : 0.878, Sensitive_Loss : 0.08734, Sensitive_Acc : 15.800, Run Time : 98.52 sec
INFO:root:2024-04-27 15:19:13, Train, Epoch : 9, Step : 5220, Loss : 0.28002, Acc : 0.891, Sensitive_Loss : 0.06273, Sensitive_Acc : 15.300, Run Time : 6.95 sec
INFO:root:2024-04-27 15:19:20, Train, Epoch : 9, Step : 5230, Loss : 0.18012, Acc : 0.944, Sensitive_Loss : 0.09059, Sensitive_Acc : 15.900, Run Time : 7.23 sec
INFO:root:2024-04-27 15:19:27, Train, Epoch : 9, Step : 5240, Loss : 0.26829, Acc : 0.909, Sensitive_Loss : 0.06480, Sensitive_Acc : 15.500, Run Time : 7.14 sec
INFO:root:2024-04-27 15:19:35, Train, Epoch : 9, Step : 5250, Loss : 0.22830, Acc : 0.925, Sensitive_Loss : 0.07948, Sensitive_Acc : 16.900, Run Time : 7.37 sec
INFO:root:2024-04-27 15:19:42, Train, Epoch : 9, Step : 5260, Loss : 0.19207, Acc : 0.900, Sensitive_Loss : 0.08022, Sensitive_Acc : 15.100, Run Time : 7.47 sec
INFO:root:2024-04-27 15:19:49, Train, Epoch : 9, Step : 5270, Loss : 0.20247, Acc : 0.938, Sensitive_Loss : 0.05377, Sensitive_Acc : 16.700, Run Time : 6.49 sec
INFO:root:2024-04-27 15:19:56, Train, Epoch : 9, Step : 5280, Loss : 0.23554, Acc : 0.903, Sensitive_Loss : 0.08411, Sensitive_Acc : 16.900, Run Time : 7.23 sec
INFO:root:2024-04-27 15:20:03, Train, Epoch : 9, Step : 5290, Loss : 0.21335, Acc : 0.903, Sensitive_Loss : 0.07462, Sensitive_Acc : 17.400, Run Time : 7.20 sec
INFO:root:2024-04-27 15:20:10, Train, Epoch : 9, Step : 5300, Loss : 0.20414, Acc : 0.916, Sensitive_Loss : 0.04937, Sensitive_Acc : 18.000, Run Time : 7.07 sec
INFO:root:2024-04-27 15:21:43, Dev, Step : 5300, Loss : 0.46954, Acc : 0.818, Auc : 0.908, Sensitive_Loss : 0.13926, Sensitive_Acc : 16.736, Sensitive_Auc : 0.993, Mean auc: 0.908, Run Time : 93.27 sec
INFO:root:2024-04-27 15:21:49, Train, Epoch : 9, Step : 5310, Loss : 0.23560, Acc : 0.922, Sensitive_Loss : 0.06575, Sensitive_Acc : 15.600, Run Time : 98.78 sec
INFO:root:2024-04-27 15:21:56, Train, Epoch : 9, Step : 5320, Loss : 0.24051, Acc : 0.916, Sensitive_Loss : 0.06575, Sensitive_Acc : 16.300, Run Time : 7.42 sec
INFO:root:2024-04-27 15:22:03, Train, Epoch : 9, Step : 5330, Loss : 0.21528, Acc : 0.891, Sensitive_Loss : 0.11777, Sensitive_Acc : 15.300, Run Time : 7.04 sec
INFO:root:2024-04-27 15:22:10, Train, Epoch : 9, Step : 5340, Loss : 0.24549, Acc : 0.894, Sensitive_Loss : 0.07018, Sensitive_Acc : 16.600, Run Time : 6.97 sec
INFO:root:2024-04-27 15:22:17, Train, Epoch : 9, Step : 5350, Loss : 0.23704, Acc : 0.891, Sensitive_Loss : 0.10320, Sensitive_Acc : 16.300, Run Time : 7.02 sec
INFO:root:2024-04-27 15:22:25, Train, Epoch : 9, Step : 5360, Loss : 0.23102, Acc : 0.909, Sensitive_Loss : 0.06583, Sensitive_Acc : 16.500, Run Time : 7.28 sec
INFO:root:2024-04-27 15:22:32, Train, Epoch : 9, Step : 5370, Loss : 0.29224, Acc : 0.872, Sensitive_Loss : 0.09050, Sensitive_Acc : 18.100, Run Time : 7.06 sec
INFO:root:2024-04-27 15:22:39, Train, Epoch : 9, Step : 5380, Loss : 0.21745, Acc : 0.909, Sensitive_Loss : 0.06476, Sensitive_Acc : 15.400, Run Time : 7.11 sec
INFO:root:2024-04-27 15:22:47, Train, Epoch : 9, Step : 5390, Loss : 0.21026, Acc : 0.906, Sensitive_Loss : 0.06180, Sensitive_Acc : 18.200, Run Time : 8.07 sec
INFO:root:2024-04-27 15:22:54, Train, Epoch : 9, Step : 5400, Loss : 0.25529, Acc : 0.897, Sensitive_Loss : 0.08609, Sensitive_Acc : 16.400, Run Time : 6.95 sec
INFO:root:2024-04-27 15:24:27, Dev, Step : 5400, Loss : 0.46981, Acc : 0.815, Auc : 0.906, Sensitive_Loss : 0.12166, Sensitive_Acc : 16.793, Sensitive_Auc : 0.993, Mean auc: 0.906, Run Time : 92.85 sec
INFO:root:2024-04-27 15:24:32, Train, Epoch : 9, Step : 5410, Loss : 0.23049, Acc : 0.909, Sensitive_Loss : 0.09416, Sensitive_Acc : 16.200, Run Time : 98.42 sec
INFO:root:2024-04-27 15:24:40, Train, Epoch : 9, Step : 5420, Loss : 0.22338, Acc : 0.897, Sensitive_Loss : 0.09989, Sensitive_Acc : 17.100, Run Time : 7.91 sec
INFO:root:2024-04-27 15:24:47, Train, Epoch : 9, Step : 5430, Loss : 0.23060, Acc : 0.912, Sensitive_Loss : 0.11838, Sensitive_Acc : 16.600, Run Time : 7.33 sec
INFO:root:2024-04-27 15:24:54, Train, Epoch : 9, Step : 5440, Loss : 0.19971, Acc : 0.919, Sensitive_Loss : 0.06267, Sensitive_Acc : 16.900, Run Time : 6.90 sec
INFO:root:2024-04-27 15:25:02, Train, Epoch : 9, Step : 5450, Loss : 0.25070, Acc : 0.881, Sensitive_Loss : 0.06304, Sensitive_Acc : 16.300, Run Time : 7.44 sec
INFO:root:2024-04-27 15:25:09, Train, Epoch : 9, Step : 5460, Loss : 0.18547, Acc : 0.906, Sensitive_Loss : 0.06939, Sensitive_Acc : 15.900, Run Time : 6.93 sec
INFO:root:2024-04-27 15:25:16, Train, Epoch : 9, Step : 5470, Loss : 0.19780, Acc : 0.906, Sensitive_Loss : 0.09568, Sensitive_Acc : 16.100, Run Time : 6.98 sec
INFO:root:2024-04-27 15:25:23, Train, Epoch : 9, Step : 5480, Loss : 0.29996, Acc : 0.866, Sensitive_Loss : 0.09042, Sensitive_Acc : 16.700, Run Time : 7.42 sec
INFO:root:2024-04-27 15:25:30, Train, Epoch : 9, Step : 5490, Loss : 0.15400, Acc : 0.938, Sensitive_Loss : 0.07903, Sensitive_Acc : 14.700, Run Time : 7.33 sec
INFO:root:2024-04-27 15:25:38, Train, Epoch : 9, Step : 5500, Loss : 0.25134, Acc : 0.884, Sensitive_Loss : 0.12274, Sensitive_Acc : 16.600, Run Time : 7.43 sec
INFO:root:2024-04-27 15:27:11, Dev, Step : 5500, Loss : 0.43580, Acc : 0.821, Auc : 0.907, Sensitive_Loss : 0.11475, Sensitive_Acc : 16.864, Sensitive_Auc : 0.994, Mean auc: 0.907, Run Time : 93.20 sec
INFO:root:2024-04-27 15:27:17, Train, Epoch : 9, Step : 5510, Loss : 0.29577, Acc : 0.897, Sensitive_Loss : 0.06498, Sensitive_Acc : 16.600, Run Time : 98.64 sec
INFO:root:2024-04-27 15:27:24, Train, Epoch : 9, Step : 5520, Loss : 0.25579, Acc : 0.903, Sensitive_Loss : 0.08309, Sensitive_Acc : 15.100, Run Time : 7.35 sec
INFO:root:2024-04-27 15:27:31, Train, Epoch : 9, Step : 5530, Loss : 0.19182, Acc : 0.909, Sensitive_Loss : 0.05807, Sensitive_Acc : 17.700, Run Time : 6.88 sec
INFO:root:2024-04-27 15:27:38, Train, Epoch : 9, Step : 5540, Loss : 0.20261, Acc : 0.903, Sensitive_Loss : 0.04675, Sensitive_Acc : 15.300, Run Time : 7.23 sec
INFO:root:2024-04-27 15:27:46, Train, Epoch : 9, Step : 5550, Loss : 0.19895, Acc : 0.912, Sensitive_Loss : 0.07195, Sensitive_Acc : 15.600, Run Time : 7.77 sec
INFO:root:2024-04-27 15:27:53, Train, Epoch : 9, Step : 5560, Loss : 0.29540, Acc : 0.878, Sensitive_Loss : 0.09440, Sensitive_Acc : 15.800, Run Time : 7.18 sec
INFO:root:2024-04-27 15:27:59, Train, Epoch : 9, Step : 5570, Loss : 0.22708, Acc : 0.894, Sensitive_Loss : 0.10153, Sensitive_Acc : 16.700, Run Time : 6.56 sec
INFO:root:2024-04-27 15:28:07, Train, Epoch : 9, Step : 5580, Loss : 0.27300, Acc : 0.903, Sensitive_Loss : 0.08111, Sensitive_Acc : 16.500, Run Time : 7.09 sec
INFO:root:2024-04-27 15:28:14, Train, Epoch : 9, Step : 5590, Loss : 0.29043, Acc : 0.863, Sensitive_Loss : 0.09071, Sensitive_Acc : 16.600, Run Time : 7.27 sec
INFO:root:2024-04-27 15:28:21, Train, Epoch : 9, Step : 5600, Loss : 0.28500, Acc : 0.869, Sensitive_Loss : 0.09383, Sensitive_Acc : 17.400, Run Time : 7.44 sec
INFO:root:2024-04-27 15:29:55, Dev, Step : 5600, Loss : 0.45002, Acc : 0.817, Auc : 0.903, Sensitive_Loss : 0.11202, Sensitive_Acc : 16.964, Sensitive_Auc : 0.993, Mean auc: 0.903, Run Time : 93.38 sec
INFO:root:2024-04-27 15:30:00, Train, Epoch : 9, Step : 5610, Loss : 0.23387, Acc : 0.900, Sensitive_Loss : 0.11026, Sensitive_Acc : 16.800, Run Time : 99.15 sec
INFO:root:2024-04-27 15:30:08, Train, Epoch : 9, Step : 5620, Loss : 0.24325, Acc : 0.878, Sensitive_Loss : 0.07436, Sensitive_Acc : 16.800, Run Time : 7.21 sec
INFO:root:2024-04-27 15:30:15, Train, Epoch : 9, Step : 5630, Loss : 0.23577, Acc : 0.909, Sensitive_Loss : 0.09670, Sensitive_Acc : 16.300, Run Time : 7.29 sec
INFO:root:2024-04-27 15:31:49
INFO:root:y_pred: [0.00416943 0.9952931  0.01103032 ... 0.88842064 0.00188138 0.9698281 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.82051492e-01 5.88387251e-04 3.20064165e-02 4.05881019e-06
 9.99546468e-01 8.07675184e-04 9.99986768e-01 9.99994040e-01
 1.11392234e-03 8.82364094e-01 9.99240279e-01 9.99908924e-01
 9.97744799e-01 9.97125328e-01 3.87506513e-03 9.92495179e-01
 9.99487996e-01 1.20726507e-02 4.01163965e-01 7.96414077e-01
 9.99613702e-01 5.09989485e-02 9.99771774e-01 9.97931838e-01
 9.99772012e-01 9.96730208e-01 2.99416788e-05 9.99594152e-01
 9.98530507e-01 2.92815685e-01 1.01271896e-02 9.19508114e-02
 4.40020531e-01 7.73846311e-03 9.31125209e-02 5.52104935e-02
 1.57447562e-01 1.07710797e-03 9.99377847e-01 9.99783099e-01
 6.40772923e-05 1.36150673e-04 9.50792134e-01 1.37376512e-04
 9.99994874e-01 9.97959614e-01 9.99583542e-01 9.89413738e-01
 1.17797160e-03 9.98704314e-01 9.98350739e-01 3.99205834e-03
 3.46048862e-01 8.67027993e-05 4.64277036e-05 1.15408935e-02
 4.84754145e-03 5.96390897e-03 1.22254947e-03 1.07443236e-01
 3.16404109e-03 1.10921457e-01 1.95624027e-03 9.94588017e-01
 3.19909185e-01 9.99967694e-01 5.27219102e-03 9.99921322e-01
 9.99674082e-01 8.70573699e-01 2.62058079e-01 3.38102221e-01
 2.16646283e-03 3.34320180e-02 2.28924900e-02 1.63183606e-04
 1.06076449e-01 8.94389004e-02 2.60452973e-04 9.99925017e-01
 9.99953985e-01 4.36814771e-05 1.25266220e-02 3.96912033e-03
 8.55536938e-01 9.42966163e-01 4.52779746e-03 9.21259914e-03
 9.90296304e-01 9.98045802e-01 9.99990582e-01 2.11919262e-03
 2.72637326e-03 9.99209762e-01 2.41473779e-01 4.17576395e-02
 9.71211910e-01 9.99946833e-01 1.22904466e-05 1.37866810e-02
 9.99461472e-01 9.98034656e-01 9.99661922e-01 9.89189386e-01
 7.89958891e-03 2.46341098e-02 9.96800303e-01 9.98081326e-01
 9.73476112e-01 6.79505774e-06 9.96431828e-01 9.96426523e-01
 5.26458630e-03 9.99672532e-01 9.98382568e-01 9.99448717e-01
 1.22592777e-01 9.99901295e-01 9.80202407e-02 1.15484275e-01
 9.99661088e-01 9.98932302e-01 1.06301974e-04 9.99592006e-01
 9.99988079e-01 3.44650924e-01 9.98463750e-01 2.82961354e-02
 4.80933784e-04 9.72706497e-01 9.99471962e-01 6.49127527e-04
 3.19925183e-03 4.45237296e-04 9.99851346e-01 9.97827470e-01
 9.82296526e-01 4.52892040e-04 1.83979366e-02 9.97898579e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 15:31:49, Dev, Step : 5634, Loss : 0.43470, Acc : 0.822, Auc : 0.906, Sensitive_Loss : 0.10621, Sensitive_Acc : 16.907, Sensitive_Auc : 0.993, Mean auc: 0.906, Run Time : 92.47 sec
INFO:root:2024-04-27 15:31:56, Train, Epoch : 10, Step : 5640, Loss : 0.12594, Acc : 0.553, Sensitive_Loss : 0.03836, Sensitive_Acc : 10.100, Run Time : 5.31 sec
INFO:root:2024-04-27 15:32:03, Train, Epoch : 10, Step : 5650, Loss : 0.23876, Acc : 0.916, Sensitive_Loss : 0.09153, Sensitive_Acc : 16.100, Run Time : 7.02 sec
INFO:root:2024-04-27 15:32:10, Train, Epoch : 10, Step : 5660, Loss : 0.27300, Acc : 0.891, Sensitive_Loss : 0.11383, Sensitive_Acc : 16.700, Run Time : 7.37 sec
INFO:root:2024-04-27 15:32:17, Train, Epoch : 10, Step : 5670, Loss : 0.16821, Acc : 0.925, Sensitive_Loss : 0.07674, Sensitive_Acc : 15.200, Run Time : 7.08 sec
INFO:root:2024-04-27 15:32:24, Train, Epoch : 10, Step : 5680, Loss : 0.20204, Acc : 0.887, Sensitive_Loss : 0.10288, Sensitive_Acc : 17.800, Run Time : 7.34 sec
INFO:root:2024-04-27 15:32:32, Train, Epoch : 10, Step : 5690, Loss : 0.21710, Acc : 0.912, Sensitive_Loss : 0.08724, Sensitive_Acc : 15.900, Run Time : 7.44 sec
INFO:root:2024-04-27 15:32:39, Train, Epoch : 10, Step : 5700, Loss : 0.16949, Acc : 0.941, Sensitive_Loss : 0.08693, Sensitive_Acc : 18.600, Run Time : 7.40 sec
INFO:root:2024-04-27 15:34:12, Dev, Step : 5700, Loss : 0.46008, Acc : 0.819, Auc : 0.904, Sensitive_Loss : 0.11836, Sensitive_Acc : 16.879, Sensitive_Auc : 0.993, Mean auc: 0.904, Run Time : 92.73 sec
INFO:root:2024-04-27 15:34:18, Train, Epoch : 10, Step : 5710, Loss : 0.20827, Acc : 0.919, Sensitive_Loss : 0.08081, Sensitive_Acc : 16.200, Run Time : 98.56 sec
INFO:root:2024-04-27 15:34:25, Train, Epoch : 10, Step : 5720, Loss : 0.23550, Acc : 0.887, Sensitive_Loss : 0.07462, Sensitive_Acc : 16.700, Run Time : 6.79 sec
INFO:root:2024-04-27 15:34:32, Train, Epoch : 10, Step : 5730, Loss : 0.23648, Acc : 0.906, Sensitive_Loss : 0.05597, Sensitive_Acc : 16.300, Run Time : 7.27 sec
INFO:root:2024-04-27 15:34:40, Train, Epoch : 10, Step : 5740, Loss : 0.21317, Acc : 0.916, Sensitive_Loss : 0.06124, Sensitive_Acc : 16.400, Run Time : 7.75 sec
INFO:root:2024-04-27 15:34:47, Train, Epoch : 10, Step : 5750, Loss : 0.22599, Acc : 0.900, Sensitive_Loss : 0.07844, Sensitive_Acc : 16.700, Run Time : 6.95 sec
INFO:root:2024-04-27 15:34:54, Train, Epoch : 10, Step : 5760, Loss : 0.18338, Acc : 0.928, Sensitive_Loss : 0.06334, Sensitive_Acc : 17.000, Run Time : 7.36 sec
INFO:root:2024-04-27 15:35:01, Train, Epoch : 10, Step : 5770, Loss : 0.23459, Acc : 0.897, Sensitive_Loss : 0.05941, Sensitive_Acc : 15.800, Run Time : 6.94 sec
INFO:root:2024-04-27 15:35:08, Train, Epoch : 10, Step : 5780, Loss : 0.21917, Acc : 0.897, Sensitive_Loss : 0.07970, Sensitive_Acc : 16.800, Run Time : 7.10 sec
INFO:root:2024-04-27 15:35:15, Train, Epoch : 10, Step : 5790, Loss : 0.24249, Acc : 0.912, Sensitive_Loss : 0.08999, Sensitive_Acc : 18.000, Run Time : 7.44 sec
INFO:root:2024-04-27 15:35:22, Train, Epoch : 10, Step : 5800, Loss : 0.24194, Acc : 0.894, Sensitive_Loss : 0.11313, Sensitive_Acc : 16.300, Run Time : 6.83 sec
INFO:root:2024-04-27 15:36:55, Dev, Step : 5800, Loss : 0.45448, Acc : 0.817, Auc : 0.905, Sensitive_Loss : 0.12574, Sensitive_Acc : 16.807, Sensitive_Auc : 0.992, Mean auc: 0.905, Run Time : 92.86 sec
INFO:root:2024-04-27 15:37:01, Train, Epoch : 10, Step : 5810, Loss : 0.18722, Acc : 0.916, Sensitive_Loss : 0.06605, Sensitive_Acc : 16.800, Run Time : 98.35 sec
INFO:root:2024-04-27 15:37:08, Train, Epoch : 10, Step : 5820, Loss : 0.20927, Acc : 0.931, Sensitive_Loss : 0.09238, Sensitive_Acc : 16.200, Run Time : 7.34 sec
INFO:root:2024-04-27 15:37:15, Train, Epoch : 10, Step : 5830, Loss : 0.21167, Acc : 0.903, Sensitive_Loss : 0.08609, Sensitive_Acc : 16.900, Run Time : 6.88 sec
INFO:root:2024-04-27 15:37:22, Train, Epoch : 10, Step : 5840, Loss : 0.20425, Acc : 0.912, Sensitive_Loss : 0.05710, Sensitive_Acc : 16.400, Run Time : 6.91 sec
INFO:root:2024-04-27 15:37:29, Train, Epoch : 10, Step : 5850, Loss : 0.19916, Acc : 0.931, Sensitive_Loss : 0.05574, Sensitive_Acc : 17.600, Run Time : 7.59 sec
INFO:root:2024-04-27 15:37:36, Train, Epoch : 10, Step : 5860, Loss : 0.18829, Acc : 0.934, Sensitive_Loss : 0.09167, Sensitive_Acc : 16.600, Run Time : 7.04 sec
INFO:root:2024-04-27 15:37:43, Train, Epoch : 10, Step : 5870, Loss : 0.22472, Acc : 0.897, Sensitive_Loss : 0.06530, Sensitive_Acc : 15.900, Run Time : 7.05 sec
INFO:root:2024-04-27 15:37:51, Train, Epoch : 10, Step : 5880, Loss : 0.20838, Acc : 0.919, Sensitive_Loss : 0.06460, Sensitive_Acc : 16.700, Run Time : 7.17 sec
INFO:root:2024-04-27 15:37:58, Train, Epoch : 10, Step : 5890, Loss : 0.22513, Acc : 0.900, Sensitive_Loss : 0.06069, Sensitive_Acc : 16.100, Run Time : 7.00 sec
INFO:root:2024-04-27 15:38:05, Train, Epoch : 10, Step : 5900, Loss : 0.20101, Acc : 0.909, Sensitive_Loss : 0.06984, Sensitive_Acc : 15.900, Run Time : 7.12 sec
INFO:root:2024-04-27 15:39:40, Dev, Step : 5900, Loss : 0.47010, Acc : 0.818, Auc : 0.904, Sensitive_Loss : 0.12239, Sensitive_Acc : 16.879, Sensitive_Auc : 0.992, Mean auc: 0.904, Run Time : 95.38 sec
INFO:root:2024-04-27 15:39:46, Train, Epoch : 10, Step : 5910, Loss : 0.18817, Acc : 0.912, Sensitive_Loss : 0.07205, Sensitive_Acc : 16.300, Run Time : 100.83 sec
INFO:root:2024-04-27 15:39:55, Train, Epoch : 10, Step : 5920, Loss : 0.16580, Acc : 0.928, Sensitive_Loss : 0.06571, Sensitive_Acc : 17.800, Run Time : 9.23 sec
INFO:root:2024-04-27 15:40:02, Train, Epoch : 10, Step : 5930, Loss : 0.23121, Acc : 0.894, Sensitive_Loss : 0.07683, Sensitive_Acc : 17.300, Run Time : 7.00 sec
INFO:root:2024-04-27 15:40:11, Train, Epoch : 10, Step : 5940, Loss : 0.15685, Acc : 0.934, Sensitive_Loss : 0.10223, Sensitive_Acc : 16.500, Run Time : 9.05 sec
INFO:root:2024-04-27 15:40:18, Train, Epoch : 10, Step : 5950, Loss : 0.23332, Acc : 0.894, Sensitive_Loss : 0.05801, Sensitive_Acc : 16.700, Run Time : 6.78 sec
INFO:root:2024-04-27 15:40:25, Train, Epoch : 10, Step : 5960, Loss : 0.21612, Acc : 0.897, Sensitive_Loss : 0.08761, Sensitive_Acc : 16.500, Run Time : 7.28 sec
INFO:root:2024-04-27 15:40:33, Train, Epoch : 10, Step : 5970, Loss : 0.24105, Acc : 0.934, Sensitive_Loss : 0.07075, Sensitive_Acc : 15.000, Run Time : 7.66 sec
INFO:root:2024-04-27 15:40:43, Train, Epoch : 10, Step : 5980, Loss : 0.25583, Acc : 0.919, Sensitive_Loss : 0.13470, Sensitive_Acc : 16.900, Run Time : 10.00 sec
INFO:root:2024-04-27 15:40:51, Train, Epoch : 10, Step : 5990, Loss : 0.19554, Acc : 0.897, Sensitive_Loss : 0.08714, Sensitive_Acc : 18.600, Run Time : 8.02 sec
INFO:root:2024-04-27 15:40:58, Train, Epoch : 10, Step : 6000, Loss : 0.25570, Acc : 0.891, Sensitive_Loss : 0.09970, Sensitive_Acc : 15.500, Run Time : 7.60 sec
INFO:root:2024-04-27 15:42:51, Dev, Step : 6000, Loss : 0.47910, Acc : 0.815, Auc : 0.906, Sensitive_Loss : 0.13541, Sensitive_Acc : 16.864, Sensitive_Auc : 0.993, Mean auc: 0.906, Run Time : 112.62 sec
INFO:root:2024-04-27 15:42:57, Train, Epoch : 10, Step : 6010, Loss : 0.18992, Acc : 0.903, Sensitive_Loss : 0.04326, Sensitive_Acc : 16.900, Run Time : 118.52 sec
INFO:root:2024-04-27 15:43:05, Train, Epoch : 10, Step : 6020, Loss : 0.23378, Acc : 0.903, Sensitive_Loss : 0.08718, Sensitive_Acc : 16.700, Run Time : 8.15 sec
INFO:root:2024-04-27 15:43:12, Train, Epoch : 10, Step : 6030, Loss : 0.18891, Acc : 0.912, Sensitive_Loss : 0.08770, Sensitive_Acc : 15.500, Run Time : 7.41 sec
INFO:root:2024-04-27 15:43:20, Train, Epoch : 10, Step : 6040, Loss : 0.20504, Acc : 0.894, Sensitive_Loss : 0.05951, Sensitive_Acc : 17.100, Run Time : 8.16 sec
INFO:root:2024-04-27 15:43:29, Train, Epoch : 10, Step : 6050, Loss : 0.22864, Acc : 0.881, Sensitive_Loss : 0.08113, Sensitive_Acc : 17.800, Run Time : 8.10 sec
INFO:root:2024-04-27 15:43:36, Train, Epoch : 10, Step : 6060, Loss : 0.23506, Acc : 0.891, Sensitive_Loss : 0.08607, Sensitive_Acc : 15.300, Run Time : 7.96 sec
INFO:root:2024-04-27 15:43:45, Train, Epoch : 10, Step : 6070, Loss : 0.24415, Acc : 0.881, Sensitive_Loss : 0.07793, Sensitive_Acc : 15.900, Run Time : 8.08 sec
INFO:root:2024-04-27 15:43:52, Train, Epoch : 10, Step : 6080, Loss : 0.20271, Acc : 0.903, Sensitive_Loss : 0.07319, Sensitive_Acc : 16.500, Run Time : 7.94 sec
INFO:root:2024-04-27 15:44:01, Train, Epoch : 10, Step : 6090, Loss : 0.21785, Acc : 0.900, Sensitive_Loss : 0.05560, Sensitive_Acc : 17.300, Run Time : 8.08 sec
INFO:root:2024-04-27 15:44:09, Train, Epoch : 10, Step : 6100, Loss : 0.18736, Acc : 0.916, Sensitive_Loss : 0.14319, Sensitive_Acc : 15.400, Run Time : 8.22 sec
INFO:root:2024-04-27 15:45:54, Dev, Step : 6100, Loss : 0.46745, Acc : 0.817, Auc : 0.907, Sensitive_Loss : 0.13571, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.907, Run Time : 105.44 sec
INFO:root:2024-04-27 15:46:00, Train, Epoch : 10, Step : 6110, Loss : 0.25815, Acc : 0.891, Sensitive_Loss : 0.07602, Sensitive_Acc : 17.200, Run Time : 111.65 sec
INFO:root:2024-04-27 15:46:08, Train, Epoch : 10, Step : 6120, Loss : 0.20903, Acc : 0.897, Sensitive_Loss : 0.06928, Sensitive_Acc : 15.300, Run Time : 7.92 sec
INFO:root:2024-04-27 15:46:16, Train, Epoch : 10, Step : 6130, Loss : 0.19747, Acc : 0.922, Sensitive_Loss : 0.08056, Sensitive_Acc : 18.000, Run Time : 7.89 sec
INFO:root:2024-04-27 15:46:24, Train, Epoch : 10, Step : 6140, Loss : 0.24316, Acc : 0.906, Sensitive_Loss : 0.06962, Sensitive_Acc : 17.000, Run Time : 8.15 sec
INFO:root:2024-04-27 15:46:41, Train, Epoch : 10, Step : 6150, Loss : 0.22155, Acc : 0.900, Sensitive_Loss : 0.07262, Sensitive_Acc : 16.000, Run Time : 17.07 sec
INFO:root:2024-04-27 15:46:58, Train, Epoch : 10, Step : 6160, Loss : 0.20814, Acc : 0.909, Sensitive_Loss : 0.12187, Sensitive_Acc : 15.600, Run Time : 16.15 sec
INFO:root:2024-04-27 15:47:11, Train, Epoch : 10, Step : 6170, Loss : 0.27548, Acc : 0.872, Sensitive_Loss : 0.11061, Sensitive_Acc : 16.800, Run Time : 13.52 sec
INFO:root:2024-04-27 15:47:24, Train, Epoch : 10, Step : 6180, Loss : 0.18283, Acc : 0.919, Sensitive_Loss : 0.05615, Sensitive_Acc : 15.600, Run Time : 12.76 sec
INFO:root:2024-04-27 15:47:37, Train, Epoch : 10, Step : 6190, Loss : 0.25321, Acc : 0.887, Sensitive_Loss : 0.08027, Sensitive_Acc : 16.100, Run Time : 13.18 sec
INFO:root:2024-04-27 15:47:50, Train, Epoch : 10, Step : 6200, Loss : 0.20948, Acc : 0.919, Sensitive_Loss : 0.06039, Sensitive_Acc : 15.800, Run Time : 13.12 sec
INFO:root:2024-04-27 15:49:33, Dev, Step : 6200, Loss : 0.48318, Acc : 0.817, Auc : 0.905, Sensitive_Loss : 0.11702, Sensitive_Acc : 16.836, Sensitive_Auc : 0.994, Mean auc: 0.905, Run Time : 103.10 sec
INFO:root:2024-04-27 15:49:42, Train, Epoch : 10, Step : 6210, Loss : 0.22177, Acc : 0.922, Sensitive_Loss : 0.06647, Sensitive_Acc : 16.300, Run Time : 111.52 sec
INFO:root:2024-04-27 15:49:50, Train, Epoch : 10, Step : 6220, Loss : 0.25361, Acc : 0.894, Sensitive_Loss : 0.07596, Sensitive_Acc : 16.700, Run Time : 8.51 sec
INFO:root:2024-04-27 15:50:01, Train, Epoch : 10, Step : 6230, Loss : 0.19964, Acc : 0.909, Sensitive_Loss : 0.11245, Sensitive_Acc : 18.000, Run Time : 11.22 sec
INFO:root:2024-04-27 15:50:09, Train, Epoch : 10, Step : 6240, Loss : 0.15321, Acc : 0.938, Sensitive_Loss : 0.05838, Sensitive_Acc : 15.600, Run Time : 8.03 sec
INFO:root:2024-04-27 15:50:20, Train, Epoch : 10, Step : 6250, Loss : 0.24047, Acc : 0.900, Sensitive_Loss : 0.05867, Sensitive_Acc : 16.500, Run Time : 10.64 sec
INFO:root:2024-04-27 15:50:28, Train, Epoch : 10, Step : 6260, Loss : 0.19617, Acc : 0.916, Sensitive_Loss : 0.10688, Sensitive_Acc : 15.700, Run Time : 7.69 sec
INFO:root:2024-04-27 15:52:06
INFO:root:y_pred: [0.00220149 0.9923248  0.0048539  ... 0.67908823 0.00116529 0.92601675]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.91805315e-01 3.84920440e-03 6.03714846e-02 7.55097653e-06
 9.99613106e-01 5.57467062e-03 9.99981761e-01 9.99995470e-01
 1.33216276e-03 8.80898774e-01 9.99671578e-01 9.99866486e-01
 9.98894513e-01 9.98209715e-01 4.03865660e-03 9.93788540e-01
 9.99349058e-01 1.25429826e-02 4.30101663e-01 8.92269909e-01
 9.99815881e-01 6.77658543e-02 9.99752820e-01 9.97334898e-01
 9.99863505e-01 9.97518301e-01 6.23483138e-05 9.99697804e-01
 9.97148931e-01 2.49984801e-01 2.20165811e-02 7.38268718e-02
 4.90942806e-01 1.54147875e-02 1.11009911e-01 5.61027862e-02
 1.07371852e-01 4.19400493e-03 9.99594986e-01 9.99714553e-01
 1.94121327e-04 4.83364362e-04 9.68008757e-01 9.51682508e-04
 9.99995708e-01 9.96889055e-01 9.99798954e-01 9.95078802e-01
 1.29190343e-03 9.98852730e-01 9.97080147e-01 9.36734769e-03
 3.53859812e-01 5.08005214e-05 6.08547525e-05 2.41458118e-02
 4.42391308e-03 6.61145803e-03 2.18690885e-03 5.15072085e-02
 7.40189338e-03 1.49013057e-01 3.75913549e-03 9.95909452e-01
 2.88093865e-01 9.99966741e-01 5.01226494e-03 9.99846220e-01
 9.99458611e-01 8.58225167e-01 3.65309507e-01 4.68011171e-01
 3.07153142e-03 2.09090002e-02 1.34542426e-02 2.11565377e-04
 2.70473123e-01 2.53094018e-01 1.52294044e-04 9.99823749e-01
 9.99942899e-01 5.66028248e-05 6.46513095e-03 1.16954017e-02
 9.00822043e-01 9.25002992e-01 5.65282814e-03 3.00385542e-02
 9.89917099e-01 9.97761846e-01 9.99986291e-01 1.45172118e-03
 1.93712593e-03 9.99001086e-01 4.24026668e-01 3.63458395e-02
 9.68474984e-01 9.99939561e-01 1.80011066e-05 1.05813909e-02
 9.98646438e-01 9.97894108e-01 9.98844981e-01 9.94216025e-01
 1.23895826e-02 1.16333246e-01 9.98093307e-01 9.97464180e-01
 9.81547117e-01 5.85886801e-06 9.97811258e-01 9.98070180e-01
 3.24772741e-03 9.99678969e-01 9.99049485e-01 9.99541879e-01
 2.84122020e-01 9.99920607e-01 7.13933036e-02 1.82542771e-01
 9.99808013e-01 9.99211073e-01 1.89288112e-04 9.99654770e-01
 9.99992967e-01 5.07281840e-01 9.99505639e-01 3.67663391e-02
 3.56770889e-03 9.87270117e-01 9.99516487e-01 5.52267302e-04
 4.41646157e-03 3.51338967e-04 9.99792159e-01 9.98108387e-01
 9.84719515e-01 2.18408182e-04 1.36662740e-02 9.98700857e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 15:52:06, Dev, Step : 6260, Loss : 0.47729, Acc : 0.816, Auc : 0.903, Sensitive_Loss : 0.11283, Sensitive_Acc : 16.893, Sensitive_Auc : 0.995, Mean auc: 0.903, Run Time : 98.06 sec
