Running on desktop22:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/balanced_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/balanced_dataset_val.csv",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.1,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-10 15:36:45, Train, Epoch : 1, Step : 10, Loss : 0.74578, Acc : 0.525, Sensitive_Loss : 0.74329, Sensitive_Acc : 15.900, Run Time : 10.28 sec
INFO:root:2024-04-10 15:36:54, Train, Epoch : 1, Step : 20, Loss : 0.70854, Acc : 0.591, Sensitive_Loss : 0.66310, Sensitive_Acc : 15.800, Run Time : 8.10 sec
INFO:root:2024-04-10 15:37:02, Train, Epoch : 1, Step : 30, Loss : 0.69473, Acc : 0.622, Sensitive_Loss : 0.66051, Sensitive_Acc : 15.900, Run Time : 8.29 sec
INFO:root:2024-04-10 15:37:11, Train, Epoch : 1, Step : 40, Loss : 0.68821, Acc : 0.644, Sensitive_Loss : 0.68049, Sensitive_Acc : 17.600, Run Time : 9.04 sec
INFO:root:2024-04-10 15:37:21, Train, Epoch : 1, Step : 50, Loss : 0.69254, Acc : 0.684, Sensitive_Loss : 0.60149, Sensitive_Acc : 15.100, Run Time : 9.84 sec
INFO:root:2024-04-10 15:37:29, Train, Epoch : 1, Step : 60, Loss : 0.65809, Acc : 0.672, Sensitive_Loss : 0.57569, Sensitive_Acc : 14.900, Run Time : 8.26 sec
INFO:root:2024-04-10 15:37:38, Train, Epoch : 1, Step : 70, Loss : 0.57048, Acc : 0.653, Sensitive_Loss : 0.48913, Sensitive_Acc : 16.100, Run Time : 9.20 sec
INFO:root:2024-04-10 15:37:47, Train, Epoch : 1, Step : 80, Loss : 0.57830, Acc : 0.681, Sensitive_Loss : 0.53289, Sensitive_Acc : 16.000, Run Time : 9.28 sec
INFO:root:2024-04-10 15:37:56, Train, Epoch : 1, Step : 90, Loss : 0.56292, Acc : 0.669, Sensitive_Loss : 0.48605, Sensitive_Acc : 15.000, Run Time : 8.92 sec
INFO:root:2024-04-10 15:38:05, Train, Epoch : 1, Step : 100, Loss : 0.51668, Acc : 0.688, Sensitive_Loss : 0.41344, Sensitive_Acc : 14.900, Run Time : 8.45 sec
INFO:root:2024-04-10 15:39:44, Dev, Step : 100, Loss : 0.60523, Acc : 0.687, Auc : 0.756, Sensitive_Loss : 0.45502, Sensitive_Acc : 15.979, Sensitive_Auc : 0.917, Mean auc: 0.756, Run Time : 99.08 sec
INFO:root:2024-04-10 15:39:45, Best, Step : 100, Loss : 0.60523, Acc : 0.687, Auc : 0.756, Sensitive_Loss : 0.45502, Sensitive_Acc : 15.979, Sensitive_Auc : 0.917, Best Auc : 0.756
INFO:root:2024-04-10 15:39:51, Train, Epoch : 1, Step : 110, Loss : 0.58289, Acc : 0.703, Sensitive_Loss : 0.46685, Sensitive_Acc : 14.600, Run Time : 106.59 sec
INFO:root:2024-04-10 15:40:01, Train, Epoch : 1, Step : 120, Loss : 0.60528, Acc : 0.675, Sensitive_Loss : 0.45785, Sensitive_Acc : 16.100, Run Time : 9.63 sec
INFO:root:2024-04-10 15:40:13, Train, Epoch : 1, Step : 130, Loss : 0.58723, Acc : 0.713, Sensitive_Loss : 0.40508, Sensitive_Acc : 16.400, Run Time : 11.64 sec
INFO:root:2024-04-10 15:40:23, Train, Epoch : 1, Step : 140, Loss : 0.62520, Acc : 0.697, Sensitive_Loss : 0.37565, Sensitive_Acc : 15.000, Run Time : 10.21 sec
INFO:root:2024-04-10 15:40:34, Train, Epoch : 1, Step : 150, Loss : 0.55319, Acc : 0.706, Sensitive_Loss : 0.41525, Sensitive_Acc : 15.700, Run Time : 11.14 sec
INFO:root:2024-04-10 15:40:49, Train, Epoch : 1, Step : 160, Loss : 0.61059, Acc : 0.678, Sensitive_Loss : 0.38319, Sensitive_Acc : 16.200, Run Time : 14.86 sec
INFO:root:2024-04-10 15:41:01, Train, Epoch : 1, Step : 170, Loss : 0.63128, Acc : 0.659, Sensitive_Loss : 0.38140, Sensitive_Acc : 15.400, Run Time : 12.02 sec
INFO:root:2024-04-10 15:41:12, Train, Epoch : 1, Step : 180, Loss : 0.65936, Acc : 0.622, Sensitive_Loss : 0.39390, Sensitive_Acc : 16.500, Run Time : 11.31 sec
INFO:root:2024-04-10 15:41:28, Train, Epoch : 1, Step : 190, Loss : 0.63701, Acc : 0.653, Sensitive_Loss : 0.43415, Sensitive_Acc : 17.700, Run Time : 15.49 sec
INFO:root:2024-04-10 15:41:41, Train, Epoch : 1, Step : 200, Loss : 0.55407, Acc : 0.713, Sensitive_Loss : 0.29970, Sensitive_Acc : 15.700, Run Time : 13.17 sec
INFO:root:2024-04-10 15:43:14, Dev, Step : 200, Loss : 0.60940, Acc : 0.686, Auc : 0.751, Sensitive_Loss : 0.34336, Sensitive_Acc : 16.107, Sensitive_Auc : 0.960, Mean auc: 0.751, Run Time : 93.19 sec
INFO:root:2024-04-10 15:43:21, Train, Epoch : 1, Step : 210, Loss : 0.56361, Acc : 0.659, Sensitive_Loss : 0.32471, Sensitive_Acc : 17.600, Run Time : 100.26 sec
INFO:root:2024-04-10 15:43:32, Train, Epoch : 1, Step : 220, Loss : 0.60219, Acc : 0.678, Sensitive_Loss : 0.34523, Sensitive_Acc : 14.500, Run Time : 11.29 sec
INFO:root:2024-04-10 15:43:46, Train, Epoch : 1, Step : 230, Loss : 0.56831, Acc : 0.725, Sensitive_Loss : 0.31268, Sensitive_Acc : 14.600, Run Time : 13.19 sec
INFO:root:2024-04-10 15:43:56, Train, Epoch : 1, Step : 240, Loss : 0.59874, Acc : 0.697, Sensitive_Loss : 0.38361, Sensitive_Acc : 16.300, Run Time : 10.32 sec
INFO:root:2024-04-10 15:44:08, Train, Epoch : 1, Step : 250, Loss : 0.56688, Acc : 0.722, Sensitive_Loss : 0.30700, Sensitive_Acc : 15.300, Run Time : 11.74 sec
INFO:root:2024-04-10 15:44:20, Train, Epoch : 1, Step : 260, Loss : 0.56261, Acc : 0.725, Sensitive_Loss : 0.31976, Sensitive_Acc : 17.300, Run Time : 12.04 sec
INFO:root:2024-04-10 15:44:30, Train, Epoch : 1, Step : 270, Loss : 0.57815, Acc : 0.691, Sensitive_Loss : 0.31272, Sensitive_Acc : 16.700, Run Time : 10.59 sec
INFO:root:2024-04-10 15:44:41, Train, Epoch : 1, Step : 280, Loss : 0.65165, Acc : 0.688, Sensitive_Loss : 0.31396, Sensitive_Acc : 15.800, Run Time : 11.14 sec
INFO:root:2024-04-10 15:44:51, Train, Epoch : 1, Step : 290, Loss : 0.58166, Acc : 0.675, Sensitive_Loss : 0.19367, Sensitive_Acc : 16.400, Run Time : 9.33 sec
INFO:root:2024-04-10 15:45:00, Train, Epoch : 1, Step : 300, Loss : 0.57282, Acc : 0.688, Sensitive_Loss : 0.28883, Sensitive_Acc : 16.500, Run Time : 9.19 sec
INFO:root:2024-04-10 15:46:37, Dev, Step : 300, Loss : 0.60275, Acc : 0.693, Auc : 0.786, Sensitive_Loss : 0.40540, Sensitive_Acc : 16.250, Sensitive_Auc : 0.974, Mean auc: 0.786, Run Time : 97.42 sec
INFO:root:2024-04-10 15:46:38, Best, Step : 300, Loss : 0.60275, Acc : 0.693, Auc : 0.786, Sensitive_Loss : 0.40540, Sensitive_Acc : 16.250, Sensitive_Auc : 0.974, Best Auc : 0.786
INFO:root:2024-04-10 15:46:45, Train, Epoch : 1, Step : 310, Loss : 0.49440, Acc : 0.766, Sensitive_Loss : 0.31858, Sensitive_Acc : 16.600, Run Time : 104.74 sec
INFO:root:2024-04-10 15:46:57, Train, Epoch : 1, Step : 320, Loss : 0.72095, Acc : 0.691, Sensitive_Loss : 0.27214, Sensitive_Acc : 16.600, Run Time : 12.07 sec
INFO:root:2024-04-10 15:47:09, Train, Epoch : 1, Step : 330, Loss : 0.58557, Acc : 0.691, Sensitive_Loss : 0.28452, Sensitive_Acc : 16.600, Run Time : 12.14 sec
INFO:root:2024-04-10 15:47:18, Train, Epoch : 1, Step : 340, Loss : 0.50195, Acc : 0.697, Sensitive_Loss : 0.27394, Sensitive_Acc : 16.100, Run Time : 9.23 sec
INFO:root:2024-04-10 15:47:28, Train, Epoch : 1, Step : 350, Loss : 0.55601, Acc : 0.716, Sensitive_Loss : 0.29444, Sensitive_Acc : 16.900, Run Time : 9.74 sec
INFO:root:2024-04-10 15:47:41, Train, Epoch : 1, Step : 360, Loss : 0.54372, Acc : 0.716, Sensitive_Loss : 0.24819, Sensitive_Acc : 15.600, Run Time : 12.91 sec
INFO:root:2024-04-10 15:47:50, Train, Epoch : 1, Step : 370, Loss : 0.55313, Acc : 0.669, Sensitive_Loss : 0.30471, Sensitive_Acc : 15.800, Run Time : 9.12 sec
INFO:root:2024-04-10 15:48:00, Train, Epoch : 1, Step : 380, Loss : 0.53279, Acc : 0.681, Sensitive_Loss : 0.26155, Sensitive_Acc : 16.900, Run Time : 9.71 sec
INFO:root:2024-04-10 15:48:12, Train, Epoch : 1, Step : 390, Loss : 0.52419, Acc : 0.756, Sensitive_Loss : 0.22840, Sensitive_Acc : 17.900, Run Time : 12.67 sec
INFO:root:2024-04-10 15:48:21, Train, Epoch : 1, Step : 400, Loss : 0.70582, Acc : 0.666, Sensitive_Loss : 0.25918, Sensitive_Acc : 15.500, Run Time : 9.07 sec
INFO:root:2024-04-10 15:50:28, Dev, Step : 400, Loss : 0.60193, Acc : 0.692, Auc : 0.770, Sensitive_Loss : 0.28529, Sensitive_Acc : 16.164, Sensitive_Auc : 0.947, Mean auc: 0.770, Run Time : 126.63 sec
INFO:root:2024-04-10 15:50:35, Train, Epoch : 1, Step : 410, Loss : 0.61032, Acc : 0.719, Sensitive_Loss : 0.27801, Sensitive_Acc : 15.200, Run Time : 133.44 sec
INFO:root:2024-04-10 15:50:45, Train, Epoch : 1, Step : 420, Loss : 0.50774, Acc : 0.719, Sensitive_Loss : 0.25518, Sensitive_Acc : 16.000, Run Time : 9.88 sec
INFO:root:2024-04-10 15:50:57, Train, Epoch : 1, Step : 430, Loss : 0.51807, Acc : 0.744, Sensitive_Loss : 0.32654, Sensitive_Acc : 15.600, Run Time : 11.93 sec
INFO:root:2024-04-10 15:51:08, Train, Epoch : 1, Step : 440, Loss : 0.49247, Acc : 0.728, Sensitive_Loss : 0.23634, Sensitive_Acc : 14.200, Run Time : 11.67 sec
INFO:root:2024-04-10 15:51:19, Train, Epoch : 1, Step : 450, Loss : 0.53232, Acc : 0.744, Sensitive_Loss : 0.29001, Sensitive_Acc : 17.200, Run Time : 10.60 sec
INFO:root:2024-04-10 15:51:29, Train, Epoch : 1, Step : 460, Loss : 0.55714, Acc : 0.716, Sensitive_Loss : 0.22006, Sensitive_Acc : 16.000, Run Time : 10.62 sec
INFO:root:2024-04-10 15:51:39, Train, Epoch : 1, Step : 470, Loss : 0.50044, Acc : 0.741, Sensitive_Loss : 0.21239, Sensitive_Acc : 15.600, Run Time : 9.09 sec
INFO:root:2024-04-10 15:51:47, Train, Epoch : 1, Step : 480, Loss : 0.49290, Acc : 0.750, Sensitive_Loss : 0.35139, Sensitive_Acc : 17.700, Run Time : 8.80 sec
INFO:root:2024-04-10 15:51:57, Train, Epoch : 1, Step : 490, Loss : 0.57379, Acc : 0.731, Sensitive_Loss : 0.27457, Sensitive_Acc : 16.100, Run Time : 9.76 sec
INFO:root:2024-04-10 15:52:07, Train, Epoch : 1, Step : 500, Loss : 0.54868, Acc : 0.731, Sensitive_Loss : 0.22914, Sensitive_Acc : 16.400, Run Time : 10.10 sec
INFO:root:2024-04-10 15:54:16, Dev, Step : 500, Loss : 0.59037, Acc : 0.709, Auc : 0.790, Sensitive_Loss : 0.43766, Sensitive_Acc : 16.179, Sensitive_Auc : 0.988, Mean auc: 0.790, Run Time : 128.65 sec
INFO:root:2024-04-10 15:54:17, Best, Step : 500, Loss : 0.59037, Acc : 0.709, Auc : 0.790, Sensitive_Loss : 0.43766, Sensitive_Acc : 16.179, Sensitive_Auc : 0.988, Best Auc : 0.790
INFO:root:2024-04-10 15:54:25, Train, Epoch : 1, Step : 510, Loss : 0.54466, Acc : 0.719, Sensitive_Loss : 0.21289, Sensitive_Acc : 18.000, Run Time : 137.60 sec
INFO:root:2024-04-10 15:54:35, Train, Epoch : 1, Step : 520, Loss : 0.63208, Acc : 0.725, Sensitive_Loss : 0.24117, Sensitive_Acc : 16.900, Run Time : 9.71 sec
INFO:root:2024-04-10 15:54:43, Train, Epoch : 1, Step : 530, Loss : 0.59296, Acc : 0.709, Sensitive_Loss : 0.23644, Sensitive_Acc : 16.600, Run Time : 8.88 sec
INFO:root:2024-04-10 15:54:54, Train, Epoch : 1, Step : 540, Loss : 0.58203, Acc : 0.722, Sensitive_Loss : 0.20485, Sensitive_Acc : 15.500, Run Time : 10.21 sec
INFO:root:2024-04-10 15:55:03, Train, Epoch : 1, Step : 550, Loss : 0.44576, Acc : 0.769, Sensitive_Loss : 0.24375, Sensitive_Acc : 16.000, Run Time : 9.26 sec
INFO:root:2024-04-10 15:55:12, Train, Epoch : 1, Step : 560, Loss : 0.58988, Acc : 0.691, Sensitive_Loss : 0.25295, Sensitive_Acc : 18.200, Run Time : 9.29 sec
INFO:root:2024-04-10 15:55:22, Train, Epoch : 1, Step : 570, Loss : 0.52936, Acc : 0.650, Sensitive_Loss : 0.22654, Sensitive_Acc : 15.800, Run Time : 10.01 sec
INFO:root:2024-04-10 15:55:33, Train, Epoch : 1, Step : 580, Loss : 0.51374, Acc : 0.731, Sensitive_Loss : 0.22601, Sensitive_Acc : 16.800, Run Time : 11.29 sec
INFO:root:2024-04-10 15:55:43, Train, Epoch : 1, Step : 590, Loss : 0.56428, Acc : 0.738, Sensitive_Loss : 0.19722, Sensitive_Acc : 15.300, Run Time : 9.80 sec
INFO:root:2024-04-10 15:55:52, Train, Epoch : 1, Step : 600, Loss : 0.68549, Acc : 0.656, Sensitive_Loss : 0.21054, Sensitive_Acc : 17.000, Run Time : 8.70 sec
INFO:root:2024-04-10 15:57:44, Dev, Step : 600, Loss : 0.70576, Acc : 0.603, Auc : 0.788, Sensitive_Loss : 0.47760, Sensitive_Acc : 16.107, Sensitive_Auc : 0.963, Mean auc: 0.788, Run Time : 111.64 sec
INFO:root:2024-04-10 15:57:52, Train, Epoch : 1, Step : 610, Loss : 0.54412, Acc : 0.725, Sensitive_Loss : 0.21759, Sensitive_Acc : 16.200, Run Time : 120.13 sec
INFO:root:2024-04-10 15:58:03, Train, Epoch : 1, Step : 620, Loss : 0.52963, Acc : 0.725, Sensitive_Loss : 0.17579, Sensitive_Acc : 16.300, Run Time : 11.00 sec
INFO:root:2024-04-10 15:58:12, Train, Epoch : 1, Step : 630, Loss : 0.48659, Acc : 0.744, Sensitive_Loss : 0.22259, Sensitive_Acc : 14.400, Run Time : 8.83 sec
INFO:root:2024-04-10 15:58:22, Train, Epoch : 1, Step : 640, Loss : 0.54827, Acc : 0.722, Sensitive_Loss : 0.17954, Sensitive_Acc : 15.200, Run Time : 9.69 sec
INFO:root:2024-04-10 16:00:00
INFO:root:y_pred: [0.24451223 0.08934077 0.465564   ... 0.40386644 0.31696868 0.4248432 ]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9470550e-01 8.1401672e-03 1.6230261e-01 9.9921215e-01 9.9546450e-01
 9.7399533e-01 9.9790430e-01 1.0504234e-03 8.8745648e-01 9.9550843e-01
 1.9220915e-01 1.4408721e-01 5.9445639e-04 9.7589195e-01 9.9355525e-01
 9.9153978e-01 9.8440373e-01 9.0202713e-01 8.6842960e-01 9.9782002e-01
 9.7314394e-01 2.4982709e-01 9.7078305e-01 9.3453127e-01 6.7954391e-01
 6.7451231e-02 9.5468706e-01 4.2397618e-01 9.9777788e-01 2.1514468e-01
 4.8468508e-02 3.7657470e-01 4.9166158e-01 9.9565107e-01 1.4553456e-04
 9.9911124e-01 8.0859318e-04 9.9962854e-01 1.5025064e-01 9.6346730e-01
 9.9737000e-01 4.0366944e-02 3.7808439e-01 3.8446416e-03 5.8124518e-01
 6.7451030e-01 9.7561103e-01 3.9637420e-01 8.9846253e-01 9.8674273e-01
 4.0267655e-03 7.0209557e-01 2.1352032e-02 9.0815187e-01 9.9630272e-01
 5.6519783e-03 9.8784024e-01 9.9357718e-01 8.2859427e-01 7.0876561e-02
 5.5361101e-03 9.9329412e-01 5.1753975e-02 9.9846929e-01 9.1301537e-01
 2.6870000e-01 6.2234187e-01 5.9056771e-01 9.9894983e-01 9.8655868e-01
 4.3946789e-03 8.8970935e-01 9.4775856e-01 9.8582065e-01 9.9169427e-01
 1.3338853e-06 3.4337444e-03 6.8815358e-02 2.6409519e-03 9.9264586e-01
 1.0159837e-02 8.8441211e-01 9.9617392e-01 9.9700719e-01 2.1986045e-02
 9.9990964e-01 1.6340565e-02 2.5114678e-02 9.9896944e-01 9.1254306e-01
 3.6523178e-02 9.4663918e-01 6.0958746e-03 4.8349854e-01 6.1460012e-01
 9.9956745e-01 2.9480057e-02 9.9930310e-01 9.1429967e-01 1.6091041e-02
 2.2487822e-03 2.5245509e-01 9.6398896e-01 9.8862350e-01 7.5107300e-01
 8.2303840e-01 9.9963915e-01 2.7668895e-02 4.3096638e-01 9.8216635e-01
 1.0202193e-04 7.4117333e-03 4.9930525e-01 9.9860919e-01 9.3253601e-01
 1.4951229e-02 9.1852403e-01 1.6395234e-02 9.9976939e-01 9.1891682e-01
 9.9499136e-01 9.9839044e-01 2.8330719e-02 7.4860424e-01 3.7046471e-01
 3.5764139e-02 1.6738228e-02 9.4127469e-04 9.5771539e-01 9.9905080e-01
 5.4157991e-04 7.7291159e-04 2.5881366e-03 4.4005927e-01 9.9537671e-01
 9.9509996e-01 9.7913605e-01 5.3752351e-01 6.0647127e-04 7.2251755e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 16:00:00, Dev, Step : 644, Loss : 0.56347, Acc : 0.725, Auc : 0.817, Sensitive_Loss : 0.20995, Sensitive_Acc : 16.164, Sensitive_Auc : 0.980, Mean auc: 0.817, Run Time : 94.75 sec
INFO:root:2024-04-10 16:00:00, Best, Step : 644, Loss : 0.56347, Acc : 0.725,Auc : 0.817, Best Auc : 0.817, Sensitive_Loss : 0.20995, Sensitive_Acc : 16.164, Sensitive_Auc : 0.980
INFO:root:2024-04-10 16:00:10, Train, Epoch : 2, Step : 650, Loss : 0.24498, Acc : 0.459, Sensitive_Loss : 0.12561, Sensitive_Acc : 10.300, Run Time : 8.27 sec
INFO:root:2024-04-10 16:00:21, Train, Epoch : 2, Step : 660, Loss : 0.49910, Acc : 0.750, Sensitive_Loss : 0.20602, Sensitive_Acc : 17.400, Run Time : 11.56 sec
INFO:root:2024-04-10 16:00:30, Train, Epoch : 2, Step : 670, Loss : 0.49279, Acc : 0.772, Sensitive_Loss : 0.17412, Sensitive_Acc : 16.300, Run Time : 8.65 sec
INFO:root:2024-04-10 16:00:41, Train, Epoch : 2, Step : 680, Loss : 0.50940, Acc : 0.772, Sensitive_Loss : 0.21356, Sensitive_Acc : 16.800, Run Time : 11.05 sec
INFO:root:2024-04-10 16:00:54, Train, Epoch : 2, Step : 690, Loss : 0.53382, Acc : 0.759, Sensitive_Loss : 0.18068, Sensitive_Acc : 16.000, Run Time : 13.10 sec
INFO:root:2024-04-10 16:01:08, Train, Epoch : 2, Step : 700, Loss : 0.52168, Acc : 0.772, Sensitive_Loss : 0.18991, Sensitive_Acc : 16.200, Run Time : 13.78 sec
INFO:root:2024-04-10 16:02:44, Dev, Step : 700, Loss : 0.55423, Acc : 0.737, Auc : 0.813, Sensitive_Loss : 0.18620, Sensitive_Acc : 16.221, Sensitive_Auc : 0.982, Mean auc: 0.813, Run Time : 95.75 sec
INFO:root:2024-04-10 16:02:51, Train, Epoch : 2, Step : 710, Loss : 0.48349, Acc : 0.803, Sensitive_Loss : 0.19323, Sensitive_Acc : 16.400, Run Time : 102.89 sec
INFO:root:2024-04-10 16:03:02, Train, Epoch : 2, Step : 720, Loss : 0.43608, Acc : 0.713, Sensitive_Loss : 0.21497, Sensitive_Acc : 18.400, Run Time : 11.66 sec
INFO:root:2024-04-10 16:03:11, Train, Epoch : 2, Step : 730, Loss : 0.53533, Acc : 0.738, Sensitive_Loss : 0.14316, Sensitive_Acc : 15.400, Run Time : 8.81 sec
INFO:root:2024-04-10 16:03:21, Train, Epoch : 2, Step : 740, Loss : 0.50073, Acc : 0.769, Sensitive_Loss : 0.13602, Sensitive_Acc : 16.200, Run Time : 9.60 sec
INFO:root:2024-04-10 16:03:33, Train, Epoch : 2, Step : 750, Loss : 0.44939, Acc : 0.772, Sensitive_Loss : 0.21850, Sensitive_Acc : 15.900, Run Time : 12.31 sec
INFO:root:2024-04-10 16:03:43, Train, Epoch : 2, Step : 760, Loss : 0.54907, Acc : 0.713, Sensitive_Loss : 0.20672, Sensitive_Acc : 15.400, Run Time : 9.65 sec
INFO:root:2024-04-10 16:03:51, Train, Epoch : 2, Step : 770, Loss : 0.49069, Acc : 0.775, Sensitive_Loss : 0.21660, Sensitive_Acc : 17.000, Run Time : 8.59 sec
INFO:root:2024-04-10 16:04:02, Train, Epoch : 2, Step : 780, Loss : 0.52079, Acc : 0.762, Sensitive_Loss : 0.16349, Sensitive_Acc : 15.300, Run Time : 10.49 sec
INFO:root:2024-04-10 16:04:12, Train, Epoch : 2, Step : 790, Loss : 0.50537, Acc : 0.753, Sensitive_Loss : 0.17904, Sensitive_Acc : 14.000, Run Time : 10.65 sec
INFO:root:2024-04-10 16:04:22, Train, Epoch : 2, Step : 800, Loss : 0.56290, Acc : 0.709, Sensitive_Loss : 0.23411, Sensitive_Acc : 15.300, Run Time : 9.13 sec
INFO:root:2024-04-10 16:05:57, Dev, Step : 800, Loss : 0.58858, Acc : 0.710, Auc : 0.818, Sensitive_Loss : 0.21867, Sensitive_Acc : 16.407, Sensitive_Auc : 0.987, Mean auc: 0.818, Run Time : 95.49 sec
INFO:root:2024-04-10 16:05:58, Best, Step : 800, Loss : 0.58858, Acc : 0.710, Auc : 0.818, Sensitive_Loss : 0.21867, Sensitive_Acc : 16.407, Sensitive_Auc : 0.987, Best Auc : 0.818
INFO:root:2024-04-10 16:06:05, Train, Epoch : 2, Step : 810, Loss : 0.48161, Acc : 0.728, Sensitive_Loss : 0.20043, Sensitive_Acc : 16.800, Run Time : 103.34 sec
INFO:root:2024-04-10 16:06:16, Train, Epoch : 2, Step : 820, Loss : 0.49022, Acc : 0.738, Sensitive_Loss : 0.18122, Sensitive_Acc : 16.300, Run Time : 11.28 sec
INFO:root:2024-04-10 16:06:31, Train, Epoch : 2, Step : 830, Loss : 0.48750, Acc : 0.744, Sensitive_Loss : 0.19294, Sensitive_Acc : 16.900, Run Time : 14.47 sec
INFO:root:2024-04-10 16:06:42, Train, Epoch : 2, Step : 840, Loss : 0.51278, Acc : 0.734, Sensitive_Loss : 0.18245, Sensitive_Acc : 16.300, Run Time : 11.80 sec
INFO:root:2024-04-10 16:06:57, Train, Epoch : 2, Step : 850, Loss : 0.53323, Acc : 0.725, Sensitive_Loss : 0.14999, Sensitive_Acc : 15.300, Run Time : 14.97 sec
INFO:root:2024-04-10 16:07:11, Train, Epoch : 2, Step : 860, Loss : 0.45480, Acc : 0.784, Sensitive_Loss : 0.19618, Sensitive_Acc : 17.400, Run Time : 13.63 sec
INFO:root:2024-04-10 16:07:23, Train, Epoch : 2, Step : 870, Loss : 0.51963, Acc : 0.728, Sensitive_Loss : 0.16088, Sensitive_Acc : 16.700, Run Time : 12.24 sec
INFO:root:2024-04-10 16:07:35, Train, Epoch : 2, Step : 880, Loss : 0.48944, Acc : 0.756, Sensitive_Loss : 0.14233, Sensitive_Acc : 16.400, Run Time : 11.87 sec
INFO:root:2024-04-10 16:07:47, Train, Epoch : 2, Step : 890, Loss : 0.53912, Acc : 0.744, Sensitive_Loss : 0.12394, Sensitive_Acc : 15.500, Run Time : 11.65 sec
INFO:root:2024-04-10 16:07:58, Train, Epoch : 2, Step : 900, Loss : 0.54368, Acc : 0.741, Sensitive_Loss : 0.17442, Sensitive_Acc : 16.700, Run Time : 11.10 sec
INFO:root:2024-04-10 16:10:37, Dev, Step : 900, Loss : 0.53781, Acc : 0.749, Auc : 0.821, Sensitive_Loss : 0.19235, Sensitive_Acc : 16.264, Sensitive_Auc : 0.984, Mean auc: 0.821, Run Time : 158.91 sec
INFO:root:2024-04-10 16:10:38, Best, Step : 900, Loss : 0.53781, Acc : 0.749, Auc : 0.821, Sensitive_Loss : 0.19235, Sensitive_Acc : 16.264, Sensitive_Auc : 0.984, Best Auc : 0.821
INFO:root:2024-04-10 16:10:46, Train, Epoch : 2, Step : 910, Loss : 0.53702, Acc : 0.716, Sensitive_Loss : 0.19590, Sensitive_Acc : 16.700, Run Time : 168.01 sec
INFO:root:2024-04-10 16:10:58, Train, Epoch : 2, Step : 920, Loss : 0.59597, Acc : 0.731, Sensitive_Loss : 0.14529, Sensitive_Acc : 15.800, Run Time : 11.94 sec
INFO:root:2024-04-10 16:11:10, Train, Epoch : 2, Step : 930, Loss : 0.54037, Acc : 0.731, Sensitive_Loss : 0.18014, Sensitive_Acc : 16.000, Run Time : 12.18 sec
INFO:root:2024-04-10 16:11:22, Train, Epoch : 2, Step : 940, Loss : 0.42422, Acc : 0.803, Sensitive_Loss : 0.20356, Sensitive_Acc : 18.100, Run Time : 11.73 sec
INFO:root:2024-04-10 16:11:34, Train, Epoch : 2, Step : 950, Loss : 0.53120, Acc : 0.716, Sensitive_Loss : 0.19447, Sensitive_Acc : 16.200, Run Time : 12.49 sec
INFO:root:2024-04-10 16:11:46, Train, Epoch : 2, Step : 960, Loss : 0.54447, Acc : 0.716, Sensitive_Loss : 0.19368, Sensitive_Acc : 15.900, Run Time : 11.43 sec
INFO:root:2024-04-10 16:11:57, Train, Epoch : 2, Step : 970, Loss : 0.50891, Acc : 0.750, Sensitive_Loss : 0.16183, Sensitive_Acc : 16.500, Run Time : 11.62 sec
INFO:root:2024-04-10 16:12:14, Train, Epoch : 2, Step : 980, Loss : 0.53576, Acc : 0.709, Sensitive_Loss : 0.11404, Sensitive_Acc : 15.100, Run Time : 16.98 sec
INFO:root:2024-04-10 16:12:29, Train, Epoch : 2, Step : 990, Loss : 0.62218, Acc : 0.741, Sensitive_Loss : 0.18242, Sensitive_Acc : 16.000, Run Time : 14.80 sec
INFO:root:2024-04-10 16:12:42, Train, Epoch : 2, Step : 1000, Loss : 0.55291, Acc : 0.709, Sensitive_Loss : 0.14595, Sensitive_Acc : 17.700, Run Time : 13.00 sec
INFO:root:2024-04-10 16:14:29, Dev, Step : 1000, Loss : 0.53639, Acc : 0.743, Auc : 0.833, Sensitive_Loss : 0.15185, Sensitive_Acc : 16.207, Sensitive_Auc : 0.983, Mean auc: 0.833, Run Time : 107.05 sec
INFO:root:2024-04-10 16:14:32, Best, Step : 1000, Loss : 0.53639, Acc : 0.743, Auc : 0.833, Sensitive_Loss : 0.15185, Sensitive_Acc : 16.207, Sensitive_Auc : 0.983, Best Auc : 0.833
INFO:root:2024-04-10 16:14:44, Train, Epoch : 2, Step : 1010, Loss : 0.49856, Acc : 0.728, Sensitive_Loss : 0.18650, Sensitive_Acc : 15.200, Run Time : 121.44 sec
INFO:root:2024-04-10 16:14:56, Train, Epoch : 2, Step : 1020, Loss : 0.52574, Acc : 0.753, Sensitive_Loss : 0.19775, Sensitive_Acc : 15.800, Run Time : 12.10 sec
INFO:root:2024-04-10 16:15:08, Train, Epoch : 2, Step : 1030, Loss : 0.46493, Acc : 0.744, Sensitive_Loss : 0.17679, Sensitive_Acc : 17.400, Run Time : 12.28 sec
INFO:root:2024-04-10 16:15:21, Train, Epoch : 2, Step : 1040, Loss : 0.51013, Acc : 0.756, Sensitive_Loss : 0.17495, Sensitive_Acc : 15.600, Run Time : 13.30 sec
INFO:root:2024-04-10 16:15:35, Train, Epoch : 2, Step : 1050, Loss : 0.48008, Acc : 0.731, Sensitive_Loss : 0.16447, Sensitive_Acc : 16.200, Run Time : 13.55 sec
INFO:root:2024-04-10 16:15:52, Train, Epoch : 2, Step : 1060, Loss : 0.55917, Acc : 0.784, Sensitive_Loss : 0.16490, Sensitive_Acc : 15.500, Run Time : 16.88 sec
INFO:root:2024-04-10 16:16:05, Train, Epoch : 2, Step : 1070, Loss : 0.52344, Acc : 0.766, Sensitive_Loss : 0.16779, Sensitive_Acc : 16.200, Run Time : 13.19 sec
INFO:root:2024-04-10 16:16:19, Train, Epoch : 2, Step : 1080, Loss : 0.49178, Acc : 0.741, Sensitive_Loss : 0.14040, Sensitive_Acc : 16.900, Run Time : 14.48 sec
INFO:root:2024-04-10 16:16:31, Train, Epoch : 2, Step : 1090, Loss : 0.49177, Acc : 0.722, Sensitive_Loss : 0.28098, Sensitive_Acc : 18.200, Run Time : 11.80 sec
INFO:root:2024-04-10 16:16:44, Train, Epoch : 2, Step : 1100, Loss : 0.44753, Acc : 0.738, Sensitive_Loss : 0.15488, Sensitive_Acc : 16.200, Run Time : 12.56 sec
INFO:root:2024-04-10 16:19:03, Dev, Step : 1100, Loss : 0.53770, Acc : 0.747, Auc : 0.831, Sensitive_Loss : 0.55319, Sensitive_Acc : 16.079, Sensitive_Auc : 0.985, Mean auc: 0.831, Run Time : 139.45 sec
INFO:root:2024-04-10 16:19:12, Train, Epoch : 2, Step : 1110, Loss : 0.53340, Acc : 0.738, Sensitive_Loss : 0.18558, Sensitive_Acc : 15.700, Run Time : 148.31 sec
INFO:root:2024-04-10 16:19:24, Train, Epoch : 2, Step : 1120, Loss : 0.53925, Acc : 0.709, Sensitive_Loss : 0.19931, Sensitive_Acc : 16.200, Run Time : 12.14 sec
INFO:root:2024-04-10 16:19:39, Train, Epoch : 2, Step : 1130, Loss : 0.51033, Acc : 0.769, Sensitive_Loss : 0.18426, Sensitive_Acc : 18.600, Run Time : 14.45 sec
INFO:root:2024-04-10 16:19:54, Train, Epoch : 2, Step : 1140, Loss : 0.49253, Acc : 0.753, Sensitive_Loss : 0.23801, Sensitive_Acc : 16.000, Run Time : 15.51 sec
INFO:root:2024-04-10 16:20:06, Train, Epoch : 2, Step : 1150, Loss : 0.53102, Acc : 0.750, Sensitive_Loss : 0.15546, Sensitive_Acc : 16.100, Run Time : 12.22 sec
INFO:root:2024-04-10 16:20:20, Train, Epoch : 2, Step : 1160, Loss : 0.54393, Acc : 0.713, Sensitive_Loss : 0.15701, Sensitive_Acc : 16.900, Run Time : 14.10 sec
INFO:root:2024-04-10 16:20:32, Train, Epoch : 2, Step : 1170, Loss : 0.43276, Acc : 0.791, Sensitive_Loss : 0.14694, Sensitive_Acc : 16.900, Run Time : 11.38 sec
INFO:root:2024-04-10 16:20:43, Train, Epoch : 2, Step : 1180, Loss : 0.56880, Acc : 0.722, Sensitive_Loss : 0.15400, Sensitive_Acc : 17.200, Run Time : 11.06 sec
INFO:root:2024-04-10 16:20:55, Train, Epoch : 2, Step : 1190, Loss : 0.49292, Acc : 0.756, Sensitive_Loss : 0.17825, Sensitive_Acc : 17.100, Run Time : 11.99 sec
INFO:root:2024-04-10 16:21:07, Train, Epoch : 2, Step : 1200, Loss : 0.57742, Acc : 0.753, Sensitive_Loss : 0.15573, Sensitive_Acc : 16.400, Run Time : 11.66 sec
INFO:root:2024-04-10 16:23:28, Dev, Step : 1200, Loss : 0.56224, Acc : 0.738, Auc : 0.828, Sensitive_Loss : 0.14392, Sensitive_Acc : 16.207, Sensitive_Auc : 0.985, Mean auc: 0.828, Run Time : 141.09 sec
INFO:root:2024-04-10 16:23:42, Train, Epoch : 2, Step : 1210, Loss : 0.49864, Acc : 0.781, Sensitive_Loss : 0.13106, Sensitive_Acc : 16.600, Run Time : 155.92 sec
INFO:root:2024-04-10 16:24:00, Train, Epoch : 2, Step : 1220, Loss : 0.54430, Acc : 0.713, Sensitive_Loss : 0.17515, Sensitive_Acc : 16.700, Run Time : 17.44 sec
INFO:root:2024-04-10 16:24:14, Train, Epoch : 2, Step : 1230, Loss : 0.53673, Acc : 0.744, Sensitive_Loss : 0.19526, Sensitive_Acc : 16.500, Run Time : 14.51 sec
INFO:root:2024-04-10 16:24:29, Train, Epoch : 2, Step : 1240, Loss : 0.48142, Acc : 0.725, Sensitive_Loss : 0.13260, Sensitive_Acc : 17.000, Run Time : 14.79 sec
INFO:root:2024-04-10 16:24:41, Train, Epoch : 2, Step : 1250, Loss : 0.39815, Acc : 0.787, Sensitive_Loss : 0.19783, Sensitive_Acc : 15.600, Run Time : 12.14 sec
INFO:root:2024-04-10 16:24:54, Train, Epoch : 2, Step : 1260, Loss : 0.51811, Acc : 0.725, Sensitive_Loss : 0.14958, Sensitive_Acc : 17.700, Run Time : 12.58 sec
INFO:root:2024-04-10 16:25:09, Train, Epoch : 2, Step : 1270, Loss : 0.53008, Acc : 0.756, Sensitive_Loss : 0.19168, Sensitive_Acc : 16.700, Run Time : 15.38 sec
INFO:root:2024-04-10 16:25:21, Train, Epoch : 2, Step : 1280, Loss : 0.60417, Acc : 0.697, Sensitive_Loss : 0.12658, Sensitive_Acc : 16.000, Run Time : 11.84 sec
INFO:root:2024-04-10 16:27:41
INFO:root:y_pred: [0.13734724 0.54913557 0.52120686 ... 0.5959508  0.38274035 0.47035056]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.97916043e-01 1.41538694e-04 8.71721566e-01 9.99258459e-01
 9.98692930e-01 9.79828894e-01 9.99709547e-01 1.57557151e-04
 9.91208434e-01 9.95877266e-01 1.16847783e-01 2.75971144e-01
 3.65682557e-04 9.90402699e-01 9.99361455e-01 9.99932766e-01
 9.95003164e-01 9.48693752e-01 9.77688313e-01 9.98773873e-01
 9.95138109e-01 1.23303859e-02 9.99080181e-01 9.27845061e-01
 8.52696419e-01 1.74927320e-02 9.91861820e-01 1.96966454e-01
 9.99782145e-01 1.37779489e-02 1.79262483e-04 8.71770382e-01
 1.06757529e-01 9.95085299e-01 3.52042443e-05 9.99707878e-01
 1.20054279e-03 9.99986887e-01 4.01368877e-03 9.95742977e-01
 9.98628855e-01 6.43458730e-03 1.52887860e-02 1.03076658e-04
 8.42449144e-02 1.00447953e-01 9.99228358e-01 8.87438595e-01
 9.97062862e-01 9.95947540e-01 2.47557345e-03 9.05355275e-01
 3.13816182e-02 8.50453675e-01 9.99518871e-01 4.72266227e-02
 9.52959597e-01 9.99451458e-01 9.82765615e-01 1.09274127e-02
 5.00405300e-03 9.84506905e-01 1.51695654e-01 9.99946952e-01
 9.93930578e-01 1.90535590e-01 8.56574059e-01 6.11886144e-01
 9.99080777e-01 9.73255992e-01 6.56120479e-04 9.81850684e-01
 8.61350298e-01 9.96458828e-01 9.96727824e-01 3.81387122e-06
 3.44569832e-02 3.86350527e-02 6.98810327e-05 9.96427357e-01
 7.24286493e-03 9.91831243e-01 9.98348236e-01 9.98628259e-01
 3.45890939e-01 9.99998093e-01 5.89088944e-04 9.36172158e-02
 9.99767482e-01 9.76449788e-01 1.52609974e-01 8.88556778e-01
 1.25191109e-02 8.58123004e-01 4.27585781e-01 9.99983311e-01
 3.91067192e-02 9.99809325e-01 9.85339284e-01 8.40766449e-03
 3.19778366e-04 2.49478951e-01 9.96792138e-01 9.99106467e-01
 7.76238739e-01 7.85687506e-01 9.99842286e-01 6.05751537e-02
 1.40948713e-01 9.97953415e-01 1.56480382e-04 4.24800557e-04
 1.22293212e-01 9.99891162e-01 9.97926950e-01 2.87274923e-03
 9.81850982e-01 6.68376824e-03 9.99546230e-01 7.87640333e-01
 9.99541879e-01 9.99391317e-01 3.33424568e-01 3.23878407e-01
 4.88563269e-01 7.82660693e-02 1.49003565e-02 6.38371130e-05
 9.97212112e-01 9.99931216e-01 4.78748116e-04 5.36158797e-04
 6.37897290e-04 5.41276298e-02 9.96642947e-01 9.97501433e-01
 9.46606696e-01 2.66485214e-01 1.70610882e-02 9.75410223e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 16:27:41, Dev, Step : 1288, Loss : 0.54923, Acc : 0.744, Auc : 0.814, Sensitive_Loss : 0.18907, Sensitive_Acc : 16.264, Sensitive_Auc : 0.986, Mean auc: 0.814, Run Time : 130.74 sec
INFO:root:2024-04-10 16:27:45, Train, Epoch : 3, Step : 1290, Loss : 0.09864, Acc : 0.172, Sensitive_Loss : 0.01435, Sensitive_Acc : 2.700, Run Time : 3.02 sec
INFO:root:2024-04-10 16:27:55, Train, Epoch : 3, Step : 1300, Loss : 0.45853, Acc : 0.778, Sensitive_Loss : 0.12544, Sensitive_Acc : 15.900, Run Time : 10.12 sec
INFO:root:2024-04-10 16:29:30, Dev, Step : 1300, Loss : 0.54144, Acc : 0.746, Auc : 0.824, Sensitive_Loss : 0.18164, Sensitive_Acc : 16.193, Sensitive_Auc : 0.987, Mean auc: 0.824, Run Time : 94.68 sec
INFO:root:2024-04-10 16:29:39, Train, Epoch : 3, Step : 1310, Loss : 0.48960, Acc : 0.769, Sensitive_Loss : 0.14631, Sensitive_Acc : 16.800, Run Time : 103.55 sec
INFO:root:2024-04-10 16:29:49, Train, Epoch : 3, Step : 1320, Loss : 0.48710, Acc : 0.759, Sensitive_Loss : 0.14882, Sensitive_Acc : 15.000, Run Time : 10.66 sec
INFO:root:2024-04-10 16:30:01, Train, Epoch : 3, Step : 1330, Loss : 0.51814, Acc : 0.787, Sensitive_Loss : 0.12034, Sensitive_Acc : 16.300, Run Time : 11.63 sec
INFO:root:2024-04-10 16:30:14, Train, Epoch : 3, Step : 1340, Loss : 0.47527, Acc : 0.794, Sensitive_Loss : 0.14832, Sensitive_Acc : 16.300, Run Time : 12.96 sec
INFO:root:2024-04-10 16:30:25, Train, Epoch : 3, Step : 1350, Loss : 0.44683, Acc : 0.806, Sensitive_Loss : 0.13133, Sensitive_Acc : 17.400, Run Time : 11.10 sec
INFO:root:2024-04-10 16:30:37, Train, Epoch : 3, Step : 1360, Loss : 0.45057, Acc : 0.822, Sensitive_Loss : 0.13188, Sensitive_Acc : 16.800, Run Time : 12.19 sec
INFO:root:2024-04-10 16:30:49, Train, Epoch : 3, Step : 1370, Loss : 0.37850, Acc : 0.819, Sensitive_Loss : 0.11287, Sensitive_Acc : 16.500, Run Time : 11.73 sec
INFO:root:2024-04-10 16:31:00, Train, Epoch : 3, Step : 1380, Loss : 0.42454, Acc : 0.816, Sensitive_Loss : 0.10720, Sensitive_Acc : 16.400, Run Time : 11.01 sec
INFO:root:2024-04-10 16:31:11, Train, Epoch : 3, Step : 1390, Loss : 0.39968, Acc : 0.806, Sensitive_Loss : 0.15390, Sensitive_Acc : 17.000, Run Time : 11.29 sec
INFO:root:2024-04-10 16:31:22, Train, Epoch : 3, Step : 1400, Loss : 0.39967, Acc : 0.806, Sensitive_Loss : 0.12351, Sensitive_Acc : 15.800, Run Time : 11.08 sec
INFO:root:2024-04-10 16:33:26, Dev, Step : 1400, Loss : 0.51758, Acc : 0.758, Auc : 0.844, Sensitive_Loss : 0.21177, Sensitive_Acc : 16.164, Sensitive_Auc : 0.992, Mean auc: 0.844, Run Time : 124.10 sec
INFO:root:2024-04-10 16:33:27, Best, Step : 1400, Loss : 0.51758, Acc : 0.758, Auc : 0.844, Sensitive_Loss : 0.21177, Sensitive_Acc : 16.164, Sensitive_Auc : 0.992, Best Auc : 0.844
INFO:root:2024-04-10 16:33:35, Train, Epoch : 3, Step : 1410, Loss : 0.41870, Acc : 0.841, Sensitive_Loss : 0.13536, Sensitive_Acc : 15.800, Run Time : 132.33 sec
INFO:root:2024-04-10 16:33:45, Train, Epoch : 3, Step : 1420, Loss : 0.45597, Acc : 0.794, Sensitive_Loss : 0.14903, Sensitive_Acc : 18.400, Run Time : 10.75 sec
INFO:root:2024-04-10 16:33:55, Train, Epoch : 3, Step : 1430, Loss : 0.41603, Acc : 0.816, Sensitive_Loss : 0.11102, Sensitive_Acc : 15.800, Run Time : 9.91 sec
INFO:root:2024-04-10 16:34:07, Train, Epoch : 3, Step : 1440, Loss : 0.41801, Acc : 0.812, Sensitive_Loss : 0.14178, Sensitive_Acc : 17.700, Run Time : 11.41 sec
INFO:root:2024-04-10 16:34:17, Train, Epoch : 3, Step : 1450, Loss : 0.40356, Acc : 0.787, Sensitive_Loss : 0.15702, Sensitive_Acc : 15.600, Run Time : 10.25 sec
INFO:root:2024-04-10 16:34:27, Train, Epoch : 3, Step : 1460, Loss : 0.41557, Acc : 0.794, Sensitive_Loss : 0.12167, Sensitive_Acc : 16.500, Run Time : 10.08 sec
INFO:root:2024-04-10 16:34:40, Train, Epoch : 3, Step : 1470, Loss : 0.39119, Acc : 0.819, Sensitive_Loss : 0.12078, Sensitive_Acc : 15.600, Run Time : 12.51 sec
INFO:root:2024-04-10 16:34:49, Train, Epoch : 3, Step : 1480, Loss : 0.48037, Acc : 0.797, Sensitive_Loss : 0.13531, Sensitive_Acc : 16.000, Run Time : 9.72 sec
INFO:root:2024-04-10 16:34:59, Train, Epoch : 3, Step : 1490, Loss : 0.43508, Acc : 0.825, Sensitive_Loss : 0.14717, Sensitive_Acc : 16.500, Run Time : 10.03 sec
INFO:root:2024-04-10 16:35:09, Train, Epoch : 3, Step : 1500, Loss : 0.45220, Acc : 0.800, Sensitive_Loss : 0.11963, Sensitive_Acc : 16.200, Run Time : 9.67 sec
INFO:root:2024-04-10 16:37:13, Dev, Step : 1500, Loss : 0.50629, Acc : 0.767, Auc : 0.849, Sensitive_Loss : 0.17538, Sensitive_Acc : 16.193, Sensitive_Auc : 0.992, Mean auc: 0.849, Run Time : 123.95 sec
INFO:root:2024-04-10 16:37:14, Best, Step : 1500, Loss : 0.50629, Acc : 0.767, Auc : 0.849, Sensitive_Loss : 0.17538, Sensitive_Acc : 16.193, Sensitive_Auc : 0.992, Best Auc : 0.849
INFO:root:2024-04-10 16:37:21, Train, Epoch : 3, Step : 1510, Loss : 0.39528, Acc : 0.812, Sensitive_Loss : 0.13844, Sensitive_Acc : 15.200, Run Time : 132.46 sec
INFO:root:2024-04-10 16:37:33, Train, Epoch : 3, Step : 1520, Loss : 0.47161, Acc : 0.772, Sensitive_Loss : 0.14547, Sensitive_Acc : 15.700, Run Time : 11.53 sec
INFO:root:2024-04-10 16:37:44, Train, Epoch : 3, Step : 1530, Loss : 0.48918, Acc : 0.772, Sensitive_Loss : 0.12534, Sensitive_Acc : 17.000, Run Time : 11.24 sec
INFO:root:2024-04-10 16:37:54, Train, Epoch : 3, Step : 1540, Loss : 0.44137, Acc : 0.778, Sensitive_Loss : 0.10292, Sensitive_Acc : 15.600, Run Time : 10.13 sec
INFO:root:2024-04-10 16:38:06, Train, Epoch : 3, Step : 1550, Loss : 0.39021, Acc : 0.822, Sensitive_Loss : 0.13247, Sensitive_Acc : 15.100, Run Time : 12.02 sec
INFO:root:2024-04-10 16:38:17, Train, Epoch : 3, Step : 1560, Loss : 0.41575, Acc : 0.797, Sensitive_Loss : 0.13492, Sensitive_Acc : 16.600, Run Time : 10.42 sec
INFO:root:2024-04-10 16:38:26, Train, Epoch : 3, Step : 1570, Loss : 0.44189, Acc : 0.806, Sensitive_Loss : 0.15487, Sensitive_Acc : 17.600, Run Time : 9.64 sec
INFO:root:2024-04-10 16:38:37, Train, Epoch : 3, Step : 1580, Loss : 0.47258, Acc : 0.747, Sensitive_Loss : 0.11947, Sensitive_Acc : 15.400, Run Time : 10.22 sec
INFO:root:2024-04-10 16:38:47, Train, Epoch : 3, Step : 1590, Loss : 0.41062, Acc : 0.819, Sensitive_Loss : 0.17762, Sensitive_Acc : 15.400, Run Time : 10.15 sec
INFO:root:2024-04-10 16:38:57, Train, Epoch : 3, Step : 1600, Loss : 0.43235, Acc : 0.816, Sensitive_Loss : 0.15445, Sensitive_Acc : 17.100, Run Time : 10.42 sec
INFO:root:2024-04-10 16:41:00, Dev, Step : 1600, Loss : 0.50981, Acc : 0.769, Auc : 0.850, Sensitive_Loss : 0.17106, Sensitive_Acc : 16.250, Sensitive_Auc : 0.992, Mean auc: 0.850, Run Time : 123.05 sec
INFO:root:2024-04-10 16:41:01, Best, Step : 1600, Loss : 0.50981, Acc : 0.769, Auc : 0.850, Sensitive_Loss : 0.17106, Sensitive_Acc : 16.250, Sensitive_Auc : 0.992, Best Auc : 0.850
INFO:root:2024-04-10 16:41:08, Train, Epoch : 3, Step : 1610, Loss : 0.40404, Acc : 0.816, Sensitive_Loss : 0.09955, Sensitive_Acc : 15.800, Run Time : 130.73 sec
INFO:root:2024-04-10 16:41:20, Train, Epoch : 3, Step : 1620, Loss : 0.42421, Acc : 0.828, Sensitive_Loss : 0.09022, Sensitive_Acc : 17.200, Run Time : 12.42 sec
INFO:root:2024-04-10 16:41:32, Train, Epoch : 3, Step : 1630, Loss : 0.42802, Acc : 0.803, Sensitive_Loss : 0.10906, Sensitive_Acc : 16.900, Run Time : 11.36 sec
INFO:root:2024-04-10 16:41:50, Train, Epoch : 3, Step : 1640, Loss : 0.40126, Acc : 0.825, Sensitive_Loss : 0.12044, Sensitive_Acc : 16.300, Run Time : 18.71 sec
INFO:root:2024-04-10 16:42:02, Train, Epoch : 3, Step : 1650, Loss : 0.43752, Acc : 0.791, Sensitive_Loss : 0.18570, Sensitive_Acc : 15.100, Run Time : 11.96 sec
INFO:root:2024-04-10 16:42:37, Train, Epoch : 3, Step : 1660, Loss : 0.40408, Acc : 0.803, Sensitive_Loss : 0.15386, Sensitive_Acc : 16.700, Run Time : 34.58 sec
INFO:root:2024-04-10 16:42:53, Train, Epoch : 3, Step : 1670, Loss : 0.39205, Acc : 0.797, Sensitive_Loss : 0.19253, Sensitive_Acc : 16.000, Run Time : 16.15 sec
INFO:root:2024-04-10 16:43:04, Train, Epoch : 3, Step : 1680, Loss : 0.43839, Acc : 0.806, Sensitive_Loss : 0.11995, Sensitive_Acc : 16.400, Run Time : 10.65 sec
INFO:root:2024-04-10 16:43:19, Train, Epoch : 3, Step : 1690, Loss : 0.39131, Acc : 0.816, Sensitive_Loss : 0.13369, Sensitive_Acc : 17.100, Run Time : 15.71 sec
INFO:root:2024-04-10 16:43:33, Train, Epoch : 3, Step : 1700, Loss : 0.36035, Acc : 0.784, Sensitive_Loss : 0.13602, Sensitive_Acc : 15.000, Run Time : 13.95 sec
INFO:root:2024-04-10 16:45:55, Dev, Step : 1700, Loss : 0.51155, Acc : 0.771, Auc : 0.854, Sensitive_Loss : 0.16735, Sensitive_Acc : 16.250, Sensitive_Auc : 0.991, Mean auc: 0.854, Run Time : 142.03 sec
INFO:root:2024-04-10 16:45:56, Best, Step : 1700, Loss : 0.51155, Acc : 0.771, Auc : 0.854, Sensitive_Loss : 0.16735, Sensitive_Acc : 16.250, Sensitive_Auc : 0.991, Best Auc : 0.854
INFO:root:2024-04-10 16:46:03, Train, Epoch : 3, Step : 1710, Loss : 0.48386, Acc : 0.794, Sensitive_Loss : 0.19747, Sensitive_Acc : 16.400, Run Time : 149.35 sec
INFO:root:2024-04-10 16:46:12, Train, Epoch : 3, Step : 1720, Loss : 0.42886, Acc : 0.816, Sensitive_Loss : 0.14899, Sensitive_Acc : 16.000, Run Time : 9.42 sec
INFO:root:2024-04-10 16:46:22, Train, Epoch : 3, Step : 1730, Loss : 0.41571, Acc : 0.803, Sensitive_Loss : 0.10815, Sensitive_Acc : 17.900, Run Time : 9.89 sec
INFO:root:2024-04-10 16:46:31, Train, Epoch : 3, Step : 1740, Loss : 0.39336, Acc : 0.828, Sensitive_Loss : 0.08020, Sensitive_Acc : 16.300, Run Time : 8.91 sec
INFO:root:2024-04-10 16:46:41, Train, Epoch : 3, Step : 1750, Loss : 0.46121, Acc : 0.806, Sensitive_Loss : 0.10117, Sensitive_Acc : 17.300, Run Time : 10.14 sec
INFO:root:2024-04-10 16:46:52, Train, Epoch : 3, Step : 1760, Loss : 0.44029, Acc : 0.812, Sensitive_Loss : 0.11557, Sensitive_Acc : 15.000, Run Time : 10.70 sec
INFO:root:2024-04-10 16:47:00, Train, Epoch : 3, Step : 1770, Loss : 0.44202, Acc : 0.794, Sensitive_Loss : 0.09824, Sensitive_Acc : 16.600, Run Time : 8.63 sec
INFO:root:2024-04-10 16:47:09, Train, Epoch : 3, Step : 1780, Loss : 0.41352, Acc : 0.775, Sensitive_Loss : 0.15370, Sensitive_Acc : 14.200, Run Time : 8.96 sec
INFO:root:2024-04-10 16:47:17, Train, Epoch : 3, Step : 1790, Loss : 0.41221, Acc : 0.816, Sensitive_Loss : 0.13679, Sensitive_Acc : 16.900, Run Time : 7.95 sec
INFO:root:2024-04-10 16:47:26, Train, Epoch : 3, Step : 1800, Loss : 0.45810, Acc : 0.816, Sensitive_Loss : 0.15297, Sensitive_Acc : 16.800, Run Time : 8.55 sec
INFO:root:2024-04-10 16:49:09, Dev, Step : 1800, Loss : 0.50841, Acc : 0.766, Auc : 0.853, Sensitive_Loss : 0.15443, Sensitive_Acc : 16.250, Sensitive_Auc : 0.992, Mean auc: 0.853, Run Time : 102.62 sec
INFO:root:2024-04-10 16:49:15, Train, Epoch : 3, Step : 1810, Loss : 0.46506, Acc : 0.781, Sensitive_Loss : 0.10241, Sensitive_Acc : 17.100, Run Time : 108.74 sec
INFO:root:2024-04-10 16:49:23, Train, Epoch : 3, Step : 1820, Loss : 0.45134, Acc : 0.800, Sensitive_Loss : 0.13122, Sensitive_Acc : 16.400, Run Time : 8.26 sec
INFO:root:2024-04-10 16:49:33, Train, Epoch : 3, Step : 1830, Loss : 0.40175, Acc : 0.806, Sensitive_Loss : 0.11480, Sensitive_Acc : 16.300, Run Time : 10.30 sec
INFO:root:2024-04-10 16:49:43, Train, Epoch : 3, Step : 1840, Loss : 0.41139, Acc : 0.816, Sensitive_Loss : 0.09096, Sensitive_Acc : 16.200, Run Time : 9.56 sec
INFO:root:2024-04-10 16:49:51, Train, Epoch : 3, Step : 1850, Loss : 0.45930, Acc : 0.784, Sensitive_Loss : 0.13000, Sensitive_Acc : 16.800, Run Time : 8.33 sec
INFO:root:2024-04-10 16:49:59, Train, Epoch : 3, Step : 1860, Loss : 0.34362, Acc : 0.828, Sensitive_Loss : 0.07890, Sensitive_Acc : 16.000, Run Time : 8.03 sec
INFO:root:2024-04-10 16:50:09, Train, Epoch : 3, Step : 1870, Loss : 0.43070, Acc : 0.825, Sensitive_Loss : 0.16557, Sensitive_Acc : 16.100, Run Time : 10.23 sec
INFO:root:2024-04-10 16:50:18, Train, Epoch : 3, Step : 1880, Loss : 0.45189, Acc : 0.775, Sensitive_Loss : 0.13544, Sensitive_Acc : 17.000, Run Time : 8.83 sec
INFO:root:2024-04-10 16:50:26, Train, Epoch : 3, Step : 1890, Loss : 0.43501, Acc : 0.775, Sensitive_Loss : 0.09256, Sensitive_Acc : 16.400, Run Time : 8.14 sec
INFO:root:2024-04-10 16:50:35, Train, Epoch : 3, Step : 1900, Loss : 0.41035, Acc : 0.825, Sensitive_Loss : 0.08902, Sensitive_Acc : 15.200, Run Time : 8.78 sec
INFO:root:2024-04-10 16:52:09, Dev, Step : 1900, Loss : 0.52053, Acc : 0.761, Auc : 0.852, Sensitive_Loss : 0.13754, Sensitive_Acc : 16.236, Sensitive_Auc : 0.991, Mean auc: 0.852, Run Time : 94.01 sec
INFO:root:2024-04-10 16:52:16, Train, Epoch : 3, Step : 1910, Loss : 0.35457, Acc : 0.856, Sensitive_Loss : 0.10293, Sensitive_Acc : 17.500, Run Time : 100.69 sec
INFO:root:2024-04-10 16:52:27, Train, Epoch : 3, Step : 1920, Loss : 0.42021, Acc : 0.831, Sensitive_Loss : 0.13002, Sensitive_Acc : 15.500, Run Time : 10.80 sec
INFO:root:2024-04-10 16:52:42, Train, Epoch : 3, Step : 1930, Loss : 0.44669, Acc : 0.797, Sensitive_Loss : 0.10367, Sensitive_Acc : 16.000, Run Time : 15.38 sec
INFO:root:2024-04-10 16:54:19
INFO:root:y_pred: [0.18638372 0.19519535 0.56267357 ... 0.5128475  0.27655366 0.30537158]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9763441e-01 4.5612411e-04 7.1492106e-01 9.9926645e-01 9.9643731e-01
 9.9263728e-01 9.9923384e-01 3.4332403e-04 9.7645330e-01 9.9688941e-01
 3.1757998e-01 4.4347885e-01 5.0561223e-04 9.9411088e-01 9.9777287e-01
 9.9984097e-01 9.8391771e-01 9.6495271e-01 9.8495317e-01 9.9823892e-01
 9.9470693e-01 1.1493154e-02 9.9671882e-01 8.9812821e-01 9.5037127e-01
 2.2656403e-02 9.8442245e-01 1.2852912e-01 9.9966919e-01 6.7334980e-02
 9.3844900e-04 7.2766364e-01 1.1932988e-02 9.9619114e-01 1.1426246e-05
 9.9925858e-01 4.0297053e-04 9.9998510e-01 2.5995886e-02 9.9758422e-01
 9.9933016e-01 7.9399059e-03 6.4321660e-02 2.5932185e-04 1.3122702e-01
 1.9434389e-01 9.9920183e-01 9.3348020e-01 9.8332226e-01 9.9714357e-01
 2.9944612e-03 8.6147326e-01 2.0608675e-02 5.8337319e-01 9.9942517e-01
 3.6017898e-02 8.3622712e-01 9.9935538e-01 9.8623556e-01 1.1083584e-02
 1.4653463e-02 9.9395245e-01 2.4933141e-01 9.9981397e-01 9.7199237e-01
 2.8092143e-01 8.5258079e-01 7.7097338e-01 9.9908876e-01 9.8316228e-01
 9.6440385e-04 9.4975966e-01 9.5073414e-01 9.9492550e-01 9.9840814e-01
 5.0304840e-05 1.2990044e-01 1.8764792e-02 7.3106617e-05 9.8907286e-01
 1.7905349e-02 9.9649125e-01 9.9890292e-01 9.9778736e-01 1.0312509e-01
 9.9999321e-01 1.2524519e-03 1.3238546e-02 9.9937999e-01 9.8950231e-01
 8.2579479e-02 8.3366078e-01 1.0753837e-01 6.8544561e-01 6.4278120e-01
 9.9989593e-01 3.6185563e-02 9.9961960e-01 9.8398954e-01 8.1631877e-03
 1.3378168e-04 3.0101591e-01 9.9586934e-01 9.9913329e-01 9.0930140e-01
 6.9625735e-01 9.9953592e-01 7.0303209e-02 3.6846441e-01 9.9435103e-01
 1.0381457e-03 1.2267523e-03 6.5811574e-02 9.9915481e-01 9.9769777e-01
 3.9578895e-03 9.8097140e-01 4.5793885e-03 9.9946934e-01 5.1817828e-01
 9.9897897e-01 9.9854517e-01 3.4517145e-01 2.8047040e-01 4.4623688e-01
 6.5475225e-02 1.5676054e-01 4.1648615e-04 9.9055427e-01 9.9983716e-01
 3.3109105e-04 1.4064646e-04 1.2267862e-03 1.0156997e-01 9.9664140e-01
 9.9787867e-01 9.6034104e-01 6.3184780e-01 2.2579779e-01 9.9643916e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 16:54:19, Dev, Step : 1932, Loss : 0.53064, Acc : 0.755, Auc : 0.855, Sensitive_Loss : 0.16008, Sensitive_Acc : 16.250, Sensitive_Auc : 0.992, Mean auc: 0.855, Run Time : 94.91 sec
INFO:root:2024-04-10 16:54:20, Best, Step : 1932, Loss : 0.53064, Acc : 0.755,Auc : 0.855, Best Auc : 0.855, Sensitive_Loss : 0.16008, Sensitive_Acc : 16.250, Sensitive_Auc : 0.992
INFO:root:2024-04-10 16:54:32, Train, Epoch : 4, Step : 1940, Loss : 0.35555, Acc : 0.641, Sensitive_Loss : 0.06556, Sensitive_Acc : 12.700, Run Time : 11.33 sec
INFO:root:2024-04-10 16:54:44, Train, Epoch : 4, Step : 1950, Loss : 0.43329, Acc : 0.791, Sensitive_Loss : 0.12329, Sensitive_Acc : 17.200, Run Time : 12.10 sec
INFO:root:2024-04-10 16:54:57, Train, Epoch : 4, Step : 1960, Loss : 0.41381, Acc : 0.800, Sensitive_Loss : 0.08726, Sensitive_Acc : 16.100, Run Time : 12.91 sec
INFO:root:2024-04-10 16:55:07, Train, Epoch : 4, Step : 1970, Loss : 0.37403, Acc : 0.844, Sensitive_Loss : 0.11823, Sensitive_Acc : 15.200, Run Time : 9.73 sec
INFO:root:2024-04-10 16:55:18, Train, Epoch : 4, Step : 1980, Loss : 0.38258, Acc : 0.781, Sensitive_Loss : 0.11347, Sensitive_Acc : 15.300, Run Time : 10.99 sec
INFO:root:2024-04-10 16:55:30, Train, Epoch : 4, Step : 1990, Loss : 0.44437, Acc : 0.778, Sensitive_Loss : 0.11100, Sensitive_Acc : 16.400, Run Time : 12.41 sec
INFO:root:2024-04-10 16:55:40, Train, Epoch : 4, Step : 2000, Loss : 0.34616, Acc : 0.847, Sensitive_Loss : 0.15215, Sensitive_Acc : 16.300, Run Time : 9.34 sec
INFO:root:2024-04-10 16:57:15, Dev, Step : 2000, Loss : 0.54559, Acc : 0.752, Auc : 0.857, Sensitive_Loss : 0.19188, Sensitive_Acc : 16.179, Sensitive_Auc : 0.991, Mean auc: 0.857, Run Time : 95.05 sec
INFO:root:2024-04-10 16:57:15, Best, Step : 2000, Loss : 0.54559, Acc : 0.752, Auc : 0.857, Sensitive_Loss : 0.19188, Sensitive_Acc : 16.179, Sensitive_Auc : 0.991, Best Auc : 0.857
INFO:root:2024-04-10 16:57:22, Train, Epoch : 4, Step : 2010, Loss : 0.38900, Acc : 0.838, Sensitive_Loss : 0.09441, Sensitive_Acc : 16.700, Run Time : 102.19 sec
INFO:root:2024-04-10 16:57:32, Train, Epoch : 4, Step : 2020, Loss : 0.43735, Acc : 0.787, Sensitive_Loss : 0.10820, Sensitive_Acc : 17.000, Run Time : 10.38 sec
INFO:root:2024-04-10 16:57:45, Train, Epoch : 4, Step : 2030, Loss : 0.40991, Acc : 0.831, Sensitive_Loss : 0.13593, Sensitive_Acc : 15.100, Run Time : 12.73 sec
INFO:root:2024-04-10 16:57:56, Train, Epoch : 4, Step : 2040, Loss : 0.37374, Acc : 0.825, Sensitive_Loss : 0.14594, Sensitive_Acc : 15.700, Run Time : 10.70 sec
INFO:root:2024-04-10 16:58:05, Train, Epoch : 4, Step : 2050, Loss : 0.44012, Acc : 0.772, Sensitive_Loss : 0.16706, Sensitive_Acc : 16.200, Run Time : 9.07 sec
INFO:root:2024-04-10 16:58:14, Train, Epoch : 4, Step : 2060, Loss : 0.42267, Acc : 0.822, Sensitive_Loss : 0.21311, Sensitive_Acc : 16.100, Run Time : 9.75 sec
INFO:root:2024-04-10 16:58:24, Train, Epoch : 4, Step : 2070, Loss : 0.36782, Acc : 0.831, Sensitive_Loss : 0.09912, Sensitive_Acc : 15.300, Run Time : 9.49 sec
INFO:root:2024-04-10 16:58:33, Train, Epoch : 4, Step : 2080, Loss : 0.40163, Acc : 0.816, Sensitive_Loss : 0.10874, Sensitive_Acc : 15.900, Run Time : 9.07 sec
INFO:root:2024-04-10 16:58:43, Train, Epoch : 4, Step : 2090, Loss : 0.46034, Acc : 0.778, Sensitive_Loss : 0.11694, Sensitive_Acc : 16.400, Run Time : 10.15 sec
INFO:root:2024-04-10 16:58:55, Train, Epoch : 4, Step : 2100, Loss : 0.38460, Acc : 0.844, Sensitive_Loss : 0.13040, Sensitive_Acc : 15.600, Run Time : 11.99 sec
INFO:root:2024-04-10 17:00:30, Dev, Step : 2100, Loss : 0.55080, Acc : 0.752, Auc : 0.852, Sensitive_Loss : 0.15945, Sensitive_Acc : 16.293, Sensitive_Auc : 0.992, Mean auc: 0.852, Run Time : 95.23 sec
INFO:root:2024-04-10 17:00:37, Train, Epoch : 4, Step : 2110, Loss : 0.44718, Acc : 0.787, Sensitive_Loss : 0.12999, Sensitive_Acc : 16.400, Run Time : 102.25 sec
INFO:root:2024-04-10 17:00:47, Train, Epoch : 4, Step : 2120, Loss : 0.32384, Acc : 0.875, Sensitive_Loss : 0.10444, Sensitive_Acc : 17.200, Run Time : 9.55 sec
INFO:root:2024-04-10 17:00:57, Train, Epoch : 4, Step : 2130, Loss : 0.38489, Acc : 0.800, Sensitive_Loss : 0.09612, Sensitive_Acc : 17.700, Run Time : 9.85 sec
INFO:root:2024-04-10 17:01:08, Train, Epoch : 4, Step : 2140, Loss : 0.42702, Acc : 0.812, Sensitive_Loss : 0.07938, Sensitive_Acc : 16.200, Run Time : 10.69 sec
INFO:root:2024-04-10 17:01:18, Train, Epoch : 4, Step : 2150, Loss : 0.46411, Acc : 0.784, Sensitive_Loss : 0.08734, Sensitive_Acc : 15.200, Run Time : 10.24 sec
INFO:root:2024-04-10 17:01:27, Train, Epoch : 4, Step : 2160, Loss : 0.38201, Acc : 0.825, Sensitive_Loss : 0.10507, Sensitive_Acc : 17.300, Run Time : 9.24 sec
INFO:root:2024-04-10 17:01:40, Train, Epoch : 4, Step : 2170, Loss : 0.37553, Acc : 0.822, Sensitive_Loss : 0.11538, Sensitive_Acc : 17.000, Run Time : 13.09 sec
INFO:root:2024-04-10 17:01:51, Train, Epoch : 4, Step : 2180, Loss : 0.37445, Acc : 0.847, Sensitive_Loss : 0.08475, Sensitive_Acc : 17.500, Run Time : 10.90 sec
INFO:root:2024-04-10 17:02:00, Train, Epoch : 4, Step : 2190, Loss : 0.34455, Acc : 0.850, Sensitive_Loss : 0.10121, Sensitive_Acc : 15.800, Run Time : 9.26 sec
INFO:root:2024-04-10 17:02:13, Train, Epoch : 4, Step : 2200, Loss : 0.37734, Acc : 0.847, Sensitive_Loss : 0.09745, Sensitive_Acc : 16.200, Run Time : 12.32 sec
INFO:root:2024-04-10 17:03:55, Dev, Step : 2200, Loss : 0.50596, Acc : 0.774, Auc : 0.857, Sensitive_Loss : 0.13824, Sensitive_Acc : 16.421, Sensitive_Auc : 0.991, Mean auc: 0.857, Run Time : 102.91 sec
INFO:root:2024-04-10 17:03:57, Best, Step : 2200, Loss : 0.50596, Acc : 0.774, Auc : 0.857, Sensitive_Loss : 0.13824, Sensitive_Acc : 16.421, Sensitive_Auc : 0.991, Best Auc : 0.857
INFO:root:2024-04-10 17:04:06, Train, Epoch : 4, Step : 2210, Loss : 0.34118, Acc : 0.863, Sensitive_Loss : 0.10517, Sensitive_Acc : 16.800, Run Time : 113.70 sec
INFO:root:2024-04-10 17:04:18, Train, Epoch : 4, Step : 2220, Loss : 0.40572, Acc : 0.822, Sensitive_Loss : 0.12141, Sensitive_Acc : 15.100, Run Time : 11.87 sec
INFO:root:2024-04-10 17:04:31, Train, Epoch : 4, Step : 2230, Loss : 0.41395, Acc : 0.828, Sensitive_Loss : 0.15253, Sensitive_Acc : 17.100, Run Time : 13.08 sec
INFO:root:2024-04-10 17:04:44, Train, Epoch : 4, Step : 2240, Loss : 0.46414, Acc : 0.794, Sensitive_Loss : 0.15503, Sensitive_Acc : 15.300, Run Time : 12.37 sec
INFO:root:2024-04-10 17:04:54, Train, Epoch : 4, Step : 2250, Loss : 0.37465, Acc : 0.809, Sensitive_Loss : 0.12578, Sensitive_Acc : 15.100, Run Time : 10.51 sec
INFO:root:2024-04-10 17:05:05, Train, Epoch : 4, Step : 2260, Loss : 0.36859, Acc : 0.803, Sensitive_Loss : 0.10925, Sensitive_Acc : 15.800, Run Time : 10.70 sec
INFO:root:2024-04-10 17:05:17, Train, Epoch : 4, Step : 2270, Loss : 0.36662, Acc : 0.863, Sensitive_Loss : 0.09908, Sensitive_Acc : 17.200, Run Time : 12.18 sec
INFO:root:2024-04-10 17:05:26, Train, Epoch : 4, Step : 2280, Loss : 0.37598, Acc : 0.838, Sensitive_Loss : 0.09752, Sensitive_Acc : 15.200, Run Time : 9.37 sec
INFO:root:2024-04-10 17:05:37, Train, Epoch : 4, Step : 2290, Loss : 0.45873, Acc : 0.809, Sensitive_Loss : 0.12779, Sensitive_Acc : 15.300, Run Time : 10.60 sec
INFO:root:2024-04-10 17:05:47, Train, Epoch : 4, Step : 2300, Loss : 0.31479, Acc : 0.875, Sensitive_Loss : 0.15307, Sensitive_Acc : 17.000, Run Time : 9.98 sec
INFO:root:2024-04-10 17:07:40, Dev, Step : 2300, Loss : 0.50737, Acc : 0.773, Auc : 0.858, Sensitive_Loss : 0.14113, Sensitive_Acc : 16.407, Sensitive_Auc : 0.991, Mean auc: 0.858, Run Time : 113.13 sec
INFO:root:2024-04-10 17:07:41, Best, Step : 2300, Loss : 0.50737, Acc : 0.773, Auc : 0.858, Sensitive_Loss : 0.14113, Sensitive_Acc : 16.407, Sensitive_Auc : 0.991, Best Auc : 0.858
INFO:root:2024-04-10 17:07:49, Train, Epoch : 4, Step : 2310, Loss : 0.40697, Acc : 0.853, Sensitive_Loss : 0.09915, Sensitive_Acc : 15.200, Run Time : 122.29 sec
INFO:root:2024-04-10 17:08:01, Train, Epoch : 4, Step : 2320, Loss : 0.50961, Acc : 0.772, Sensitive_Loss : 0.11208, Sensitive_Acc : 15.700, Run Time : 12.17 sec
INFO:root:2024-04-10 17:08:14, Train, Epoch : 4, Step : 2330, Loss : 0.39480, Acc : 0.781, Sensitive_Loss : 0.12618, Sensitive_Acc : 16.900, Run Time : 12.33 sec
INFO:root:2024-04-10 17:08:26, Train, Epoch : 4, Step : 2340, Loss : 0.41940, Acc : 0.787, Sensitive_Loss : 0.12276, Sensitive_Acc : 14.800, Run Time : 12.04 sec
INFO:root:2024-04-10 17:08:37, Train, Epoch : 4, Step : 2350, Loss : 0.43944, Acc : 0.825, Sensitive_Loss : 0.10503, Sensitive_Acc : 17.100, Run Time : 11.71 sec
INFO:root:2024-04-10 17:08:48, Train, Epoch : 4, Step : 2360, Loss : 0.32385, Acc : 0.822, Sensitive_Loss : 0.09475, Sensitive_Acc : 16.800, Run Time : 10.59 sec
INFO:root:2024-04-10 17:08:59, Train, Epoch : 4, Step : 2370, Loss : 0.45032, Acc : 0.803, Sensitive_Loss : 0.09851, Sensitive_Acc : 15.600, Run Time : 11.38 sec
INFO:root:2024-04-10 17:09:11, Train, Epoch : 4, Step : 2380, Loss : 0.45038, Acc : 0.787, Sensitive_Loss : 0.12315, Sensitive_Acc : 14.700, Run Time : 11.68 sec
INFO:root:2024-04-10 17:09:22, Train, Epoch : 4, Step : 2390, Loss : 0.48825, Acc : 0.797, Sensitive_Loss : 0.09363, Sensitive_Acc : 16.600, Run Time : 10.99 sec
INFO:root:2024-04-10 17:09:33, Train, Epoch : 4, Step : 2400, Loss : 0.40332, Acc : 0.819, Sensitive_Loss : 0.12103, Sensitive_Acc : 16.100, Run Time : 11.12 sec
INFO:root:2024-04-10 17:11:52, Dev, Step : 2400, Loss : 0.51192, Acc : 0.770, Auc : 0.857, Sensitive_Loss : 0.14788, Sensitive_Acc : 16.364, Sensitive_Auc : 0.993, Mean auc: 0.857, Run Time : 138.93 sec
INFO:root:2024-04-10 17:12:01, Train, Epoch : 4, Step : 2410, Loss : 0.42468, Acc : 0.766, Sensitive_Loss : 0.10972, Sensitive_Acc : 16.700, Run Time : 148.24 sec
INFO:root:2024-04-10 17:12:12, Train, Epoch : 4, Step : 2420, Loss : 0.36875, Acc : 0.822, Sensitive_Loss : 0.10306, Sensitive_Acc : 17.700, Run Time : 10.90 sec
INFO:root:2024-04-10 17:12:25, Train, Epoch : 4, Step : 2430, Loss : 0.40445, Acc : 0.828, Sensitive_Loss : 0.10402, Sensitive_Acc : 16.600, Run Time : 12.55 sec
INFO:root:2024-04-10 17:12:35, Train, Epoch : 4, Step : 2440, Loss : 0.38338, Acc : 0.850, Sensitive_Loss : 0.09399, Sensitive_Acc : 16.100, Run Time : 10.33 sec
INFO:root:2024-04-10 17:12:46, Train, Epoch : 4, Step : 2450, Loss : 0.43472, Acc : 0.787, Sensitive_Loss : 0.14397, Sensitive_Acc : 15.900, Run Time : 10.67 sec
INFO:root:2024-04-10 17:12:55, Train, Epoch : 4, Step : 2460, Loss : 0.34986, Acc : 0.831, Sensitive_Loss : 0.12831, Sensitive_Acc : 17.600, Run Time : 9.47 sec
INFO:root:2024-04-10 17:13:07, Train, Epoch : 4, Step : 2470, Loss : 0.45946, Acc : 0.803, Sensitive_Loss : 0.11505, Sensitive_Acc : 16.900, Run Time : 11.77 sec
INFO:root:2024-04-10 17:13:19, Train, Epoch : 4, Step : 2480, Loss : 0.40603, Acc : 0.809, Sensitive_Loss : 0.15397, Sensitive_Acc : 17.700, Run Time : 12.28 sec
INFO:root:2024-04-10 17:13:29, Train, Epoch : 4, Step : 2490, Loss : 0.44627, Acc : 0.806, Sensitive_Loss : 0.11683, Sensitive_Acc : 16.600, Run Time : 10.05 sec
INFO:root:2024-04-10 17:13:40, Train, Epoch : 4, Step : 2500, Loss : 0.43523, Acc : 0.806, Sensitive_Loss : 0.11403, Sensitive_Acc : 16.000, Run Time : 10.33 sec
INFO:root:2024-04-10 17:15:12, Dev, Step : 2500, Loss : 0.49797, Acc : 0.776, Auc : 0.858, Sensitive_Loss : 0.15220, Sensitive_Acc : 16.364, Sensitive_Auc : 0.992, Mean auc: 0.858, Run Time : 92.42 sec
INFO:root:2024-04-10 17:15:13, Best, Step : 2500, Loss : 0.49797, Acc : 0.776, Auc : 0.858, Sensitive_Loss : 0.15220, Sensitive_Acc : 16.364, Sensitive_Auc : 0.992, Best Auc : 0.858
INFO:root:2024-04-10 17:15:20, Train, Epoch : 4, Step : 2510, Loss : 0.35124, Acc : 0.787, Sensitive_Loss : 0.09857, Sensitive_Acc : 17.200, Run Time : 100.54 sec
INFO:root:2024-04-10 17:15:34, Train, Epoch : 4, Step : 2520, Loss : 0.30733, Acc : 0.834, Sensitive_Loss : 0.09053, Sensitive_Acc : 17.100, Run Time : 13.62 sec
INFO:root:2024-04-10 17:15:44, Train, Epoch : 4, Step : 2530, Loss : 0.34149, Acc : 0.853, Sensitive_Loss : 0.11505, Sensitive_Acc : 16.000, Run Time : 9.83 sec
INFO:root:2024-04-10 17:15:54, Train, Epoch : 4, Step : 2540, Loss : 0.40026, Acc : 0.825, Sensitive_Loss : 0.09265, Sensitive_Acc : 17.300, Run Time : 9.89 sec
INFO:root:2024-04-10 17:16:03, Train, Epoch : 4, Step : 2550, Loss : 0.44140, Acc : 0.812, Sensitive_Loss : 0.13144, Sensitive_Acc : 15.400, Run Time : 9.28 sec
INFO:root:2024-04-10 17:16:12, Train, Epoch : 4, Step : 2560, Loss : 0.44709, Acc : 0.766, Sensitive_Loss : 0.11776, Sensitive_Acc : 17.900, Run Time : 9.23 sec
INFO:root:2024-04-10 17:16:22, Train, Epoch : 4, Step : 2570, Loss : 0.45079, Acc : 0.819, Sensitive_Loss : 0.15059, Sensitive_Acc : 16.800, Run Time : 9.56 sec
INFO:root:2024-04-10 17:18:03
INFO:root:y_pred: [0.17225292 0.16538234 0.75605196 ... 0.72581196 0.4727064  0.40321746]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9877447e-01 2.0606318e-04 4.2050475e-01 9.9948668e-01 9.9731666e-01
 9.9465656e-01 9.9944311e-01 5.1277649e-04 9.7686476e-01 9.9479634e-01
 1.2040643e-01 3.7279278e-01 1.0330544e-04 9.9360812e-01 9.9862564e-01
 9.9984431e-01 9.8145562e-01 9.8523360e-01 9.8765743e-01 9.9762267e-01
 9.9194568e-01 7.4253106e-03 9.9483204e-01 9.0547818e-01 9.4737428e-01
 1.3757342e-02 9.8457158e-01 2.5263993e-02 9.9962175e-01 2.3527883e-02
 5.8771938e-04 5.6075376e-01 8.1295064e-03 9.9610573e-01 2.5070640e-06
 9.9903369e-01 1.1810961e-04 9.9997795e-01 3.7482094e-02 9.9769658e-01
 9.9955171e-01 3.5509011e-03 4.1108854e-02 1.0277898e-04 1.1244127e-01
 2.0784009e-01 9.9946874e-01 9.5217168e-01 9.7521007e-01 9.9745220e-01
 4.5302312e-04 8.3007002e-01 3.0420499e-02 1.5726939e-01 9.9922061e-01
 1.5438954e-02 9.0364480e-01 9.9944109e-01 9.8214495e-01 7.6974132e-03
 1.5623264e-02 9.9717331e-01 1.8392114e-01 9.9980801e-01 9.6912986e-01
 1.9775270e-01 8.5546005e-01 7.1550125e-01 9.9962664e-01 9.8296750e-01
 1.1138846e-03 9.5024759e-01 9.5084494e-01 9.9743575e-01 9.9890697e-01
 3.5402878e-05 2.1374877e-01 6.0926457e-03 1.6930955e-05 9.8933280e-01
 9.8015582e-03 9.9774313e-01 9.9890089e-01 9.9836665e-01 3.4383733e-02
 9.9999344e-01 5.4639910e-04 7.1652750e-03 9.9917608e-01 9.9238694e-01
 7.3331356e-02 7.3738939e-01 4.2489909e-02 4.4300359e-01 3.9640582e-01
 9.9994576e-01 1.3393598e-02 9.9972147e-01 9.7666544e-01 1.0966925e-03
 3.6937108e-05 1.3418019e-01 9.9670070e-01 9.9900454e-01 9.3847990e-01
 3.9449760e-01 9.9962270e-01 2.7369836e-02 2.8091025e-01 9.9381870e-01
 3.4726268e-04 1.3089504e-03 3.0120304e-02 9.9830097e-01 9.9726117e-01
 2.0578867e-03 9.9042457e-01 9.6522504e-04 9.9828762e-01 1.9929874e-01
 9.9892479e-01 9.9900973e-01 3.2121345e-01 2.0803288e-01 2.6950225e-01
 2.5801035e-02 2.3515627e-01 1.8800668e-04 9.7727656e-01 9.9983084e-01
 2.4919119e-04 3.5729841e-05 3.3964257e-04 4.7311667e-02 9.9428552e-01
 9.9677038e-01 9.7696519e-01 5.8302200e-01 1.4132476e-01 9.9082690e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 17:18:03, Dev, Step : 2576, Loss : 0.50159, Acc : 0.776, Auc : 0.857, Sensitive_Loss : 0.13052, Sensitive_Acc : 16.379, Sensitive_Auc : 0.993, Mean auc: 0.857, Run Time : 93.56 sec
INFO:root:2024-04-10 17:18:09, Train, Epoch : 5, Step : 2580, Loss : 0.12262, Acc : 0.334, Sensitive_Loss : 0.02855, Sensitive_Acc : 6.900, Run Time : 4.56 sec
INFO:root:2024-04-10 17:18:19, Train, Epoch : 5, Step : 2590, Loss : 0.32423, Acc : 0.825, Sensitive_Loss : 0.12031, Sensitive_Acc : 15.800, Run Time : 10.28 sec
INFO:root:2024-04-10 17:18:29, Train, Epoch : 5, Step : 2600, Loss : 0.34806, Acc : 0.838, Sensitive_Loss : 0.11779, Sensitive_Acc : 17.700, Run Time : 9.90 sec
INFO:root:2024-04-10 17:20:05, Dev, Step : 2600, Loss : 0.51929, Acc : 0.769, Auc : 0.858, Sensitive_Loss : 0.14296, Sensitive_Acc : 16.350, Sensitive_Auc : 0.993, Mean auc: 0.858, Run Time : 96.05 sec
INFO:root:2024-04-10 17:20:12, Train, Epoch : 5, Step : 2610, Loss : 0.40276, Acc : 0.828, Sensitive_Loss : 0.08689, Sensitive_Acc : 15.500, Run Time : 102.88 sec
INFO:root:2024-04-10 17:20:22, Train, Epoch : 5, Step : 2620, Loss : 0.42484, Acc : 0.828, Sensitive_Loss : 0.08156, Sensitive_Acc : 15.000, Run Time : 9.98 sec
INFO:root:2024-04-10 17:20:32, Train, Epoch : 5, Step : 2630, Loss : 0.44450, Acc : 0.794, Sensitive_Loss : 0.13152, Sensitive_Acc : 15.100, Run Time : 10.19 sec
INFO:root:2024-04-10 17:20:44, Train, Epoch : 5, Step : 2640, Loss : 0.42278, Acc : 0.806, Sensitive_Loss : 0.12398, Sensitive_Acc : 16.200, Run Time : 11.73 sec
INFO:root:2024-04-10 17:20:54, Train, Epoch : 5, Step : 2650, Loss : 0.37746, Acc : 0.859, Sensitive_Loss : 0.12420, Sensitive_Acc : 16.100, Run Time : 10.82 sec
INFO:root:2024-04-10 17:21:04, Train, Epoch : 5, Step : 2660, Loss : 0.39726, Acc : 0.816, Sensitive_Loss : 0.14297, Sensitive_Acc : 17.600, Run Time : 9.67 sec
INFO:root:2024-04-10 17:21:14, Train, Epoch : 5, Step : 2670, Loss : 0.39056, Acc : 0.816, Sensitive_Loss : 0.09908, Sensitive_Acc : 16.800, Run Time : 10.05 sec
INFO:root:2024-04-10 17:21:27, Train, Epoch : 5, Step : 2680, Loss : 0.33030, Acc : 0.866, Sensitive_Loss : 0.12834, Sensitive_Acc : 15.600, Run Time : 13.17 sec
INFO:root:2024-04-10 17:21:37, Train, Epoch : 5, Step : 2690, Loss : 0.37394, Acc : 0.844, Sensitive_Loss : 0.13528, Sensitive_Acc : 15.300, Run Time : 9.47 sec
INFO:root:2024-04-10 17:21:45, Train, Epoch : 5, Step : 2700, Loss : 0.36236, Acc : 0.834, Sensitive_Loss : 0.09713, Sensitive_Acc : 16.600, Run Time : 8.66 sec
INFO:root:2024-04-10 17:23:38, Dev, Step : 2700, Loss : 0.52059, Acc : 0.769, Auc : 0.860, Sensitive_Loss : 0.15278, Sensitive_Acc : 16.321, Sensitive_Auc : 0.992, Mean auc: 0.860, Run Time : 113.02 sec
INFO:root:2024-04-10 17:23:39, Best, Step : 2700, Loss : 0.52059, Acc : 0.769, Auc : 0.860, Sensitive_Loss : 0.15278, Sensitive_Acc : 16.321, Sensitive_Auc : 0.992, Best Auc : 0.860
INFO:root:2024-04-10 17:23:48, Train, Epoch : 5, Step : 2710, Loss : 0.39732, Acc : 0.838, Sensitive_Loss : 0.13174, Sensitive_Acc : 17.000, Run Time : 122.14 sec
INFO:root:2024-04-10 17:24:01, Train, Epoch : 5, Step : 2720, Loss : 0.40188, Acc : 0.822, Sensitive_Loss : 0.09953, Sensitive_Acc : 16.600, Run Time : 13.07 sec
INFO:root:2024-04-10 17:24:11, Train, Epoch : 5, Step : 2730, Loss : 0.35736, Acc : 0.828, Sensitive_Loss : 0.12698, Sensitive_Acc : 17.100, Run Time : 10.08 sec
INFO:root:2024-04-10 17:24:23, Train, Epoch : 5, Step : 2740, Loss : 0.33473, Acc : 0.841, Sensitive_Loss : 0.11368, Sensitive_Acc : 15.800, Run Time : 12.48 sec
INFO:root:2024-04-10 17:24:36, Train, Epoch : 5, Step : 2750, Loss : 0.39373, Acc : 0.806, Sensitive_Loss : 0.08760, Sensitive_Acc : 16.800, Run Time : 13.11 sec
INFO:root:2024-04-10 17:24:46, Train, Epoch : 5, Step : 2760, Loss : 0.39200, Acc : 0.834, Sensitive_Loss : 0.11403, Sensitive_Acc : 15.900, Run Time : 9.29 sec
INFO:root:2024-04-10 17:24:56, Train, Epoch : 5, Step : 2770, Loss : 0.34178, Acc : 0.838, Sensitive_Loss : 0.11842, Sensitive_Acc : 14.900, Run Time : 10.30 sec
INFO:root:2024-04-10 17:25:09, Train, Epoch : 5, Step : 2780, Loss : 0.36453, Acc : 0.844, Sensitive_Loss : 0.15594, Sensitive_Acc : 17.700, Run Time : 13.13 sec
INFO:root:2024-04-10 17:25:19, Train, Epoch : 5, Step : 2790, Loss : 0.42154, Acc : 0.809, Sensitive_Loss : 0.09715, Sensitive_Acc : 15.900, Run Time : 9.64 sec
INFO:root:2024-04-10 17:25:29, Train, Epoch : 5, Step : 2800, Loss : 0.41379, Acc : 0.806, Sensitive_Loss : 0.11363, Sensitive_Acc : 15.900, Run Time : 9.92 sec
INFO:root:2024-04-10 17:27:22, Dev, Step : 2800, Loss : 0.49905, Acc : 0.774, Auc : 0.860, Sensitive_Loss : 0.13736, Sensitive_Acc : 16.421, Sensitive_Auc : 0.994, Mean auc: 0.860, Run Time : 113.85 sec
INFO:root:2024-04-10 17:27:29, Train, Epoch : 5, Step : 2810, Loss : 0.37503, Acc : 0.838, Sensitive_Loss : 0.11245, Sensitive_Acc : 17.600, Run Time : 120.23 sec
INFO:root:2024-04-10 17:27:38, Train, Epoch : 5, Step : 2820, Loss : 0.42441, Acc : 0.816, Sensitive_Loss : 0.11805, Sensitive_Acc : 16.100, Run Time : 8.79 sec
INFO:root:2024-04-10 17:27:48, Train, Epoch : 5, Step : 2830, Loss : 0.37660, Acc : 0.850, Sensitive_Loss : 0.11038, Sensitive_Acc : 16.800, Run Time : 10.72 sec
INFO:root:2024-04-10 17:28:01, Train, Epoch : 5, Step : 2840, Loss : 0.37899, Acc : 0.800, Sensitive_Loss : 0.09960, Sensitive_Acc : 16.300, Run Time : 12.66 sec
INFO:root:2024-04-10 17:28:11, Train, Epoch : 5, Step : 2850, Loss : 0.31753, Acc : 0.825, Sensitive_Loss : 0.11889, Sensitive_Acc : 16.700, Run Time : 9.77 sec
INFO:root:2024-04-10 17:28:20, Train, Epoch : 5, Step : 2860, Loss : 0.39139, Acc : 0.825, Sensitive_Loss : 0.10633, Sensitive_Acc : 16.500, Run Time : 9.57 sec
INFO:root:2024-04-10 17:28:30, Train, Epoch : 5, Step : 2870, Loss : 0.36001, Acc : 0.834, Sensitive_Loss : 0.08468, Sensitive_Acc : 16.600, Run Time : 9.28 sec
INFO:root:2024-04-10 17:28:39, Train, Epoch : 5, Step : 2880, Loss : 0.32679, Acc : 0.863, Sensitive_Loss : 0.13467, Sensitive_Acc : 16.400, Run Time : 8.90 sec
INFO:root:2024-04-10 17:28:47, Train, Epoch : 5, Step : 2890, Loss : 0.40559, Acc : 0.834, Sensitive_Loss : 0.09928, Sensitive_Acc : 15.900, Run Time : 8.86 sec
INFO:root:2024-04-10 17:28:56, Train, Epoch : 5, Step : 2900, Loss : 0.44094, Acc : 0.806, Sensitive_Loss : 0.11689, Sensitive_Acc : 16.500, Run Time : 8.98 sec
INFO:root:2024-04-10 17:30:29, Dev, Step : 2900, Loss : 0.51077, Acc : 0.776, Auc : 0.858, Sensitive_Loss : 0.14497, Sensitive_Acc : 16.393, Sensitive_Auc : 0.993, Mean auc: 0.858, Run Time : 93.00 sec
INFO:root:2024-04-10 17:30:38, Train, Epoch : 5, Step : 2910, Loss : 0.40143, Acc : 0.834, Sensitive_Loss : 0.11072, Sensitive_Acc : 15.800, Run Time : 101.79 sec
INFO:root:2024-04-10 17:30:51, Train, Epoch : 5, Step : 2920, Loss : 0.34388, Acc : 0.856, Sensitive_Loss : 0.09546, Sensitive_Acc : 14.900, Run Time : 13.14 sec
INFO:root:2024-04-10 17:31:02, Train, Epoch : 5, Step : 2930, Loss : 0.37619, Acc : 0.822, Sensitive_Loss : 0.12526, Sensitive_Acc : 15.400, Run Time : 10.29 sec
INFO:root:2024-04-10 17:31:13, Train, Epoch : 5, Step : 2940, Loss : 0.36275, Acc : 0.828, Sensitive_Loss : 0.14270, Sensitive_Acc : 15.600, Run Time : 11.26 sec
INFO:root:2024-04-10 17:31:27, Train, Epoch : 5, Step : 2950, Loss : 0.31964, Acc : 0.856, Sensitive_Loss : 0.11136, Sensitive_Acc : 17.000, Run Time : 14.50 sec
INFO:root:2024-04-10 17:31:37, Train, Epoch : 5, Step : 2960, Loss : 0.37019, Acc : 0.812, Sensitive_Loss : 0.11168, Sensitive_Acc : 17.500, Run Time : 9.98 sec
INFO:root:2024-04-10 17:31:49, Train, Epoch : 5, Step : 2970, Loss : 0.37833, Acc : 0.834, Sensitive_Loss : 0.13716, Sensitive_Acc : 17.900, Run Time : 11.27 sec
INFO:root:2024-04-10 17:32:02, Train, Epoch : 5, Step : 2980, Loss : 0.37228, Acc : 0.831, Sensitive_Loss : 0.11518, Sensitive_Acc : 16.800, Run Time : 13.42 sec
INFO:root:2024-04-10 17:32:12, Train, Epoch : 5, Step : 2990, Loss : 0.36383, Acc : 0.863, Sensitive_Loss : 0.09851, Sensitive_Acc : 15.800, Run Time : 10.40 sec
INFO:root:2024-04-10 17:32:22, Train, Epoch : 5, Step : 3000, Loss : 0.40954, Acc : 0.828, Sensitive_Loss : 0.12128, Sensitive_Acc : 17.500, Run Time : 9.82 sec
INFO:root:2024-04-10 17:34:24, Dev, Step : 3000, Loss : 0.51737, Acc : 0.777, Auc : 0.861, Sensitive_Loss : 0.16209, Sensitive_Acc : 16.307, Sensitive_Auc : 0.993, Mean auc: 0.861, Run Time : 121.30 sec
INFO:root:2024-04-10 17:34:24, Best, Step : 3000, Loss : 0.51737, Acc : 0.777, Auc : 0.861, Sensitive_Loss : 0.16209, Sensitive_Acc : 16.307, Sensitive_Auc : 0.993, Best Auc : 0.861
INFO:root:2024-04-10 17:34:31, Train, Epoch : 5, Step : 3010, Loss : 0.39098, Acc : 0.803, Sensitive_Loss : 0.09935, Sensitive_Acc : 15.600, Run Time : 128.73 sec
INFO:root:2024-04-10 17:34:41, Train, Epoch : 5, Step : 3020, Loss : 0.35299, Acc : 0.856, Sensitive_Loss : 0.12543, Sensitive_Acc : 15.900, Run Time : 9.88 sec
INFO:root:2024-04-10 17:34:55, Train, Epoch : 5, Step : 3030, Loss : 0.39981, Acc : 0.822, Sensitive_Loss : 0.11307, Sensitive_Acc : 14.700, Run Time : 13.85 sec
INFO:root:2024-04-10 17:35:05, Train, Epoch : 5, Step : 3040, Loss : 0.37158, Acc : 0.831, Sensitive_Loss : 0.08605, Sensitive_Acc : 17.400, Run Time : 10.10 sec
INFO:root:2024-04-10 17:35:14, Train, Epoch : 5, Step : 3050, Loss : 0.38002, Acc : 0.838, Sensitive_Loss : 0.13336, Sensitive_Acc : 15.800, Run Time : 9.02 sec
INFO:root:2024-04-10 17:35:28, Train, Epoch : 5, Step : 3060, Loss : 0.33615, Acc : 0.838, Sensitive_Loss : 0.12934, Sensitive_Acc : 16.800, Run Time : 13.92 sec
INFO:root:2024-04-10 17:35:38, Train, Epoch : 5, Step : 3070, Loss : 0.37125, Acc : 0.831, Sensitive_Loss : 0.12457, Sensitive_Acc : 17.200, Run Time : 10.34 sec
INFO:root:2024-04-10 17:35:48, Train, Epoch : 5, Step : 3080, Loss : 0.38744, Acc : 0.841, Sensitive_Loss : 0.11213, Sensitive_Acc : 15.900, Run Time : 9.97 sec
INFO:root:2024-04-10 17:35:58, Train, Epoch : 5, Step : 3090, Loss : 0.37704, Acc : 0.819, Sensitive_Loss : 0.10981, Sensitive_Acc : 17.100, Run Time : 9.77 sec
INFO:root:2024-04-10 17:36:09, Train, Epoch : 5, Step : 3100, Loss : 0.37186, Acc : 0.838, Sensitive_Loss : 0.10471, Sensitive_Acc : 16.200, Run Time : 11.43 sec
INFO:root:2024-04-10 17:37:42, Dev, Step : 3100, Loss : 0.51314, Acc : 0.778, Auc : 0.862, Sensitive_Loss : 0.14909, Sensitive_Acc : 16.307, Sensitive_Auc : 0.993, Mean auc: 0.862, Run Time : 93.20 sec
INFO:root:2024-04-10 17:37:44, Best, Step : 3100, Loss : 0.51314, Acc : 0.778, Auc : 0.862, Sensitive_Loss : 0.14909, Sensitive_Acc : 16.307, Sensitive_Auc : 0.993, Best Auc : 0.862
INFO:root:2024-04-10 17:37:53, Train, Epoch : 5, Step : 3110, Loss : 0.39112, Acc : 0.806, Sensitive_Loss : 0.12829, Sensitive_Acc : 17.200, Run Time : 103.72 sec
INFO:root:2024-04-10 17:38:03, Train, Epoch : 5, Step : 3120, Loss : 0.36135, Acc : 0.819, Sensitive_Loss : 0.09433, Sensitive_Acc : 15.800, Run Time : 10.40 sec
INFO:root:2024-04-10 17:38:14, Train, Epoch : 5, Step : 3130, Loss : 0.40771, Acc : 0.806, Sensitive_Loss : 0.11871, Sensitive_Acc : 14.900, Run Time : 10.14 sec
INFO:root:2024-04-10 17:38:26, Train, Epoch : 5, Step : 3140, Loss : 0.37474, Acc : 0.850, Sensitive_Loss : 0.10209, Sensitive_Acc : 15.300, Run Time : 12.23 sec
INFO:root:2024-04-10 17:38:36, Train, Epoch : 5, Step : 3150, Loss : 0.39604, Acc : 0.809, Sensitive_Loss : 0.08377, Sensitive_Acc : 16.700, Run Time : 10.71 sec
INFO:root:2024-04-10 17:38:46, Train, Epoch : 5, Step : 3160, Loss : 0.35696, Acc : 0.844, Sensitive_Loss : 0.10836, Sensitive_Acc : 16.900, Run Time : 9.77 sec
INFO:root:2024-04-10 17:38:57, Train, Epoch : 5, Step : 3170, Loss : 0.37311, Acc : 0.841, Sensitive_Loss : 0.10085, Sensitive_Acc : 17.100, Run Time : 10.51 sec
INFO:root:2024-04-10 17:39:07, Train, Epoch : 5, Step : 3180, Loss : 0.36863, Acc : 0.822, Sensitive_Loss : 0.09833, Sensitive_Acc : 15.400, Run Time : 10.55 sec
INFO:root:2024-04-10 17:39:17, Train, Epoch : 5, Step : 3190, Loss : 0.35302, Acc : 0.828, Sensitive_Loss : 0.09362, Sensitive_Acc : 16.800, Run Time : 9.86 sec
INFO:root:2024-04-10 17:39:31, Train, Epoch : 5, Step : 3200, Loss : 0.30046, Acc : 0.869, Sensitive_Loss : 0.11035, Sensitive_Acc : 15.600, Run Time : 14.29 sec
INFO:root:2024-04-10 17:41:06, Dev, Step : 3200, Loss : 0.50993, Acc : 0.770, Auc : 0.861, Sensitive_Loss : 0.13657, Sensitive_Acc : 16.279, Sensitive_Auc : 0.992, Mean auc: 0.861, Run Time : 94.68 sec
INFO:root:2024-04-10 17:41:12, Train, Epoch : 5, Step : 3210, Loss : 0.37898, Acc : 0.825, Sensitive_Loss : 0.08664, Sensitive_Acc : 15.000, Run Time : 100.84 sec
INFO:root:2024-04-10 17:41:23, Train, Epoch : 5, Step : 3220, Loss : 0.37679, Acc : 0.859, Sensitive_Loss : 0.11058, Sensitive_Acc : 14.500, Run Time : 10.42 sec
INFO:root:2024-04-10 17:42:55
INFO:root:y_pred: [0.36751133 0.10874196 0.79663897 ... 0.57499623 0.26634237 0.50974005]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9911648e-01 4.2052270e-04 4.4987214e-01 9.9967098e-01 9.9826968e-01
 9.9550492e-01 9.9941123e-01 4.3375962e-04 9.8017657e-01 9.9431157e-01
 1.2229686e-01 5.4363161e-01 1.1776188e-04 9.8947704e-01 9.9849010e-01
 9.9987710e-01 9.9402797e-01 9.8595119e-01 9.9326706e-01 9.9852222e-01
 9.9496007e-01 1.9734900e-02 9.9711007e-01 8.6538458e-01 9.7245908e-01
 1.0119305e-02 9.9298662e-01 6.9772914e-02 9.9973518e-01 2.0891964e-02
 1.5359744e-03 6.6812760e-01 1.5279958e-02 9.9749559e-01 3.4917682e-06
 9.9954128e-01 2.2737439e-04 9.9998605e-01 5.3586941e-02 9.9849272e-01
 9.9978274e-01 5.6537236e-03 8.2602285e-02 6.6975714e-05 1.4709948e-01
 2.0984484e-01 9.9955314e-01 9.7370297e-01 9.8930347e-01 9.9931228e-01
 3.7696623e-04 8.5214949e-01 2.0621220e-02 1.6609122e-01 9.9943513e-01
 1.1868891e-02 9.4866133e-01 9.9969566e-01 9.9246913e-01 9.3262773e-03
 1.8248644e-02 9.9875760e-01 3.1062090e-01 9.9990141e-01 9.7872829e-01
 2.8255185e-01 9.0287751e-01 8.8706368e-01 9.9971122e-01 9.9127519e-01
 1.7883062e-03 9.4155508e-01 9.7470969e-01 9.9896359e-01 9.9892396e-01
 8.1605664e-05 3.4113911e-01 7.9354933e-03 2.5963667e-05 9.9183863e-01
 9.3432674e-03 9.9864393e-01 9.9939811e-01 9.9903178e-01 6.1689578e-02
 9.9999642e-01 3.5052147e-04 1.6007172e-02 9.9885190e-01 9.9543601e-01
 5.5181410e-02 8.5678577e-01 2.8260959e-02 4.2108223e-01 4.7774827e-01
 9.9993014e-01 1.1270595e-02 9.9957114e-01 9.7407246e-01 9.2931540e-04
 2.3973776e-05 2.3473111e-01 9.9723083e-01 9.9956983e-01 9.6239930e-01
 4.0850502e-01 9.9981171e-01 3.9649013e-02 4.0517560e-01 9.9549699e-01
 2.9605703e-04 1.0783642e-03 8.7064758e-02 9.9865401e-01 9.9696630e-01
 3.7077160e-03 9.9601305e-01 6.5981015e-04 9.9828929e-01 3.8285851e-01
 9.9828088e-01 9.9909019e-01 3.6186466e-01 3.6541995e-01 1.8354347e-01
 2.4484528e-02 3.5178101e-01 3.0603993e-04 9.9029154e-01 9.9988425e-01
 5.8381160e-04 2.5011555e-05 6.7728199e-04 6.2064014e-02 9.9639988e-01
 9.9715072e-01 9.8704278e-01 4.4717005e-01 2.1878164e-01 9.9491185e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 17:42:55, Dev, Step : 3220, Loss : 0.49934, Acc : 0.777, Auc : 0.862, Sensitive_Loss : 0.13879, Sensitive_Acc : 16.336, Sensitive_Auc : 0.994, Mean auc: 0.862, Run Time : 92.62 sec
INFO:root:2024-04-10 17:42:56, Best, Step : 3220, Loss : 0.49934, Acc : 0.777,Auc : 0.862, Best Auc : 0.862, Sensitive_Loss : 0.13879, Sensitive_Acc : 16.336, Sensitive_Auc : 0.994
INFO:root:2024-04-10 17:43:11, Train, Epoch : 6, Step : 3230, Loss : 0.30737, Acc : 0.856, Sensitive_Loss : 0.07757, Sensitive_Acc : 15.600, Run Time : 14.22 sec
INFO:root:2024-04-10 17:43:21, Train, Epoch : 6, Step : 3240, Loss : 0.31023, Acc : 0.828, Sensitive_Loss : 0.11870, Sensitive_Acc : 16.900, Run Time : 9.75 sec
INFO:root:2024-04-10 17:43:32, Train, Epoch : 6, Step : 3250, Loss : 0.38324, Acc : 0.834, Sensitive_Loss : 0.10871, Sensitive_Acc : 15.100, Run Time : 10.54 sec
INFO:root:2024-04-10 17:43:42, Train, Epoch : 6, Step : 3260, Loss : 0.32194, Acc : 0.856, Sensitive_Loss : 0.08521, Sensitive_Acc : 17.200, Run Time : 10.79 sec
INFO:root:2024-04-10 17:43:52, Train, Epoch : 6, Step : 3270, Loss : 0.38234, Acc : 0.825, Sensitive_Loss : 0.12188, Sensitive_Acc : 15.600, Run Time : 9.96 sec
INFO:root:2024-04-10 17:44:02, Train, Epoch : 6, Step : 3280, Loss : 0.37598, Acc : 0.853, Sensitive_Loss : 0.10839, Sensitive_Acc : 17.200, Run Time : 9.47 sec
INFO:root:2024-04-10 17:44:14, Train, Epoch : 6, Step : 3290, Loss : 0.30570, Acc : 0.844, Sensitive_Loss : 0.10166, Sensitive_Acc : 17.400, Run Time : 12.01 sec
INFO:root:2024-04-10 17:44:25, Train, Epoch : 6, Step : 3300, Loss : 0.36011, Acc : 0.847, Sensitive_Loss : 0.08384, Sensitive_Acc : 15.000, Run Time : 10.95 sec
INFO:root:2024-04-10 17:46:01, Dev, Step : 3300, Loss : 0.52220, Acc : 0.769, Auc : 0.859, Sensitive_Loss : 0.13345, Sensitive_Acc : 16.293, Sensitive_Auc : 0.993, Mean auc: 0.859, Run Time : 96.04 sec
INFO:root:2024-04-10 17:46:07, Train, Epoch : 6, Step : 3310, Loss : 0.28869, Acc : 0.878, Sensitive_Loss : 0.10406, Sensitive_Acc : 17.000, Run Time : 102.59 sec
INFO:root:2024-04-10 17:46:18, Train, Epoch : 6, Step : 3320, Loss : 0.35188, Acc : 0.859, Sensitive_Loss : 0.13408, Sensitive_Acc : 15.700, Run Time : 10.14 sec
INFO:root:2024-04-10 17:46:28, Train, Epoch : 6, Step : 3330, Loss : 0.29736, Acc : 0.866, Sensitive_Loss : 0.12176, Sensitive_Acc : 15.800, Run Time : 10.13 sec
INFO:root:2024-04-10 17:46:40, Train, Epoch : 6, Step : 3340, Loss : 0.35225, Acc : 0.850, Sensitive_Loss : 0.11737, Sensitive_Acc : 18.000, Run Time : 12.56 sec
INFO:root:2024-04-10 17:46:50, Train, Epoch : 6, Step : 3350, Loss : 0.31747, Acc : 0.831, Sensitive_Loss : 0.10566, Sensitive_Acc : 17.100, Run Time : 9.46 sec
INFO:root:2024-04-10 17:46:59, Train, Epoch : 6, Step : 3360, Loss : 0.31120, Acc : 0.831, Sensitive_Loss : 0.09559, Sensitive_Acc : 15.600, Run Time : 9.14 sec
INFO:root:2024-04-10 17:47:08, Train, Epoch : 6, Step : 3370, Loss : 0.41070, Acc : 0.825, Sensitive_Loss : 0.10150, Sensitive_Acc : 15.700, Run Time : 9.62 sec
INFO:root:2024-04-10 17:47:19, Train, Epoch : 6, Step : 3380, Loss : 0.34814, Acc : 0.816, Sensitive_Loss : 0.14030, Sensitive_Acc : 16.800, Run Time : 10.94 sec
INFO:root:2024-04-10 17:47:31, Train, Epoch : 6, Step : 3390, Loss : 0.42684, Acc : 0.825, Sensitive_Loss : 0.13458, Sensitive_Acc : 16.100, Run Time : 11.92 sec
INFO:root:2024-04-10 17:47:40, Train, Epoch : 6, Step : 3400, Loss : 0.34304, Acc : 0.850, Sensitive_Loss : 0.11802, Sensitive_Acc : 15.900, Run Time : 8.36 sec
INFO:root:2024-04-10 17:49:15, Dev, Step : 3400, Loss : 0.51028, Acc : 0.776, Auc : 0.861, Sensitive_Loss : 0.15272, Sensitive_Acc : 16.264, Sensitive_Auc : 0.994, Mean auc: 0.861, Run Time : 95.26 sec
INFO:root:2024-04-10 17:49:22, Train, Epoch : 6, Step : 3410, Loss : 0.33960, Acc : 0.853, Sensitive_Loss : 0.18860, Sensitive_Acc : 15.700, Run Time : 102.63 sec
INFO:root:2024-04-10 17:49:32, Train, Epoch : 6, Step : 3420, Loss : 0.41115, Acc : 0.819, Sensitive_Loss : 0.12453, Sensitive_Acc : 16.700, Run Time : 9.64 sec
INFO:root:2024-04-10 17:49:43, Train, Epoch : 6, Step : 3430, Loss : 0.35211, Acc : 0.816, Sensitive_Loss : 0.10644, Sensitive_Acc : 18.500, Run Time : 10.81 sec
INFO:root:2024-04-10 17:49:53, Train, Epoch : 6, Step : 3440, Loss : 0.34807, Acc : 0.841, Sensitive_Loss : 0.09526, Sensitive_Acc : 16.700, Run Time : 9.98 sec
INFO:root:2024-04-10 17:50:03, Train, Epoch : 6, Step : 3450, Loss : 0.34530, Acc : 0.838, Sensitive_Loss : 0.09332, Sensitive_Acc : 16.200, Run Time : 10.11 sec
INFO:root:2024-04-10 17:50:15, Train, Epoch : 6, Step : 3460, Loss : 0.36689, Acc : 0.847, Sensitive_Loss : 0.08703, Sensitive_Acc : 17.600, Run Time : 12.27 sec
INFO:root:2024-04-10 17:50:26, Train, Epoch : 6, Step : 3470, Loss : 0.39511, Acc : 0.809, Sensitive_Loss : 0.12935, Sensitive_Acc : 16.800, Run Time : 10.48 sec
INFO:root:2024-04-10 17:50:35, Train, Epoch : 6, Step : 3480, Loss : 0.30300, Acc : 0.853, Sensitive_Loss : 0.10678, Sensitive_Acc : 16.500, Run Time : 9.57 sec
INFO:root:2024-04-10 17:50:49, Train, Epoch : 6, Step : 3490, Loss : 0.36202, Acc : 0.841, Sensitive_Loss : 0.13908, Sensitive_Acc : 17.600, Run Time : 13.41 sec
INFO:root:2024-04-10 17:51:00, Train, Epoch : 6, Step : 3500, Loss : 0.35487, Acc : 0.841, Sensitive_Loss : 0.08700, Sensitive_Acc : 15.400, Run Time : 11.24 sec
INFO:root:2024-04-10 17:52:33, Dev, Step : 3500, Loss : 0.54545, Acc : 0.761, Auc : 0.862, Sensitive_Loss : 0.12580, Sensitive_Acc : 16.364, Sensitive_Auc : 0.993, Mean auc: 0.862, Run Time : 93.26 sec
INFO:root:2024-04-10 17:52:34, Best, Step : 3500, Loss : 0.54545, Acc : 0.761, Auc : 0.862, Sensitive_Loss : 0.12580, Sensitive_Acc : 16.364, Sensitive_Auc : 0.993, Best Auc : 0.862
INFO:root:2024-04-10 17:52:41, Train, Epoch : 6, Step : 3510, Loss : 0.30038, Acc : 0.853, Sensitive_Loss : 0.12325, Sensitive_Acc : 15.800, Run Time : 100.87 sec
INFO:root:2024-04-10 17:52:51, Train, Epoch : 6, Step : 3520, Loss : 0.33832, Acc : 0.850, Sensitive_Loss : 0.10987, Sensitive_Acc : 15.900, Run Time : 9.77 sec
INFO:root:2024-04-10 17:53:00, Train, Epoch : 6, Step : 3530, Loss : 0.40490, Acc : 0.822, Sensitive_Loss : 0.10312, Sensitive_Acc : 16.500, Run Time : 9.69 sec
INFO:root:2024-04-10 17:53:11, Train, Epoch : 6, Step : 3540, Loss : 0.37686, Acc : 0.825, Sensitive_Loss : 0.13040, Sensitive_Acc : 15.000, Run Time : 10.55 sec
INFO:root:2024-04-10 17:53:22, Train, Epoch : 6, Step : 3550, Loss : 0.33878, Acc : 0.859, Sensitive_Loss : 0.09668, Sensitive_Acc : 14.500, Run Time : 11.25 sec
INFO:root:2024-04-10 17:53:31, Train, Epoch : 6, Step : 3560, Loss : 0.31548, Acc : 0.856, Sensitive_Loss : 0.12337, Sensitive_Acc : 16.300, Run Time : 9.24 sec
INFO:root:2024-04-10 17:53:41, Train, Epoch : 6, Step : 3570, Loss : 0.33762, Acc : 0.869, Sensitive_Loss : 0.09849, Sensitive_Acc : 17.400, Run Time : 9.27 sec
INFO:root:2024-04-10 17:53:50, Train, Epoch : 6, Step : 3580, Loss : 0.31915, Acc : 0.834, Sensitive_Loss : 0.10068, Sensitive_Acc : 15.800, Run Time : 9.22 sec
INFO:root:2024-04-10 17:54:01, Train, Epoch : 6, Step : 3590, Loss : 0.33683, Acc : 0.819, Sensitive_Loss : 0.07380, Sensitive_Acc : 16.000, Run Time : 10.90 sec
INFO:root:2024-04-10 17:54:10, Train, Epoch : 6, Step : 3600, Loss : 0.28726, Acc : 0.869, Sensitive_Loss : 0.11592, Sensitive_Acc : 17.300, Run Time : 9.51 sec
INFO:root:2024-04-10 17:55:45, Dev, Step : 3600, Loss : 0.50727, Acc : 0.775, Auc : 0.861, Sensitive_Loss : 0.16708, Sensitive_Acc : 16.236, Sensitive_Auc : 0.992, Mean auc: 0.861, Run Time : 94.75 sec
INFO:root:2024-04-10 17:55:52, Train, Epoch : 6, Step : 3610, Loss : 0.38085, Acc : 0.850, Sensitive_Loss : 0.08515, Sensitive_Acc : 16.500, Run Time : 101.63 sec
INFO:root:2024-04-10 17:56:02, Train, Epoch : 6, Step : 3620, Loss : 0.35207, Acc : 0.847, Sensitive_Loss : 0.11982, Sensitive_Acc : 16.800, Run Time : 10.46 sec
INFO:root:2024-04-10 17:56:12, Train, Epoch : 6, Step : 3630, Loss : 0.36085, Acc : 0.828, Sensitive_Loss : 0.12646, Sensitive_Acc : 16.800, Run Time : 9.87 sec
INFO:root:2024-04-10 17:56:21, Train, Epoch : 6, Step : 3640, Loss : 0.35335, Acc : 0.859, Sensitive_Loss : 0.07819, Sensitive_Acc : 17.100, Run Time : 9.29 sec
INFO:root:2024-04-10 17:56:31, Train, Epoch : 6, Step : 3650, Loss : 0.42861, Acc : 0.834, Sensitive_Loss : 0.11840, Sensitive_Acc : 17.400, Run Time : 9.14 sec
INFO:root:2024-04-10 17:56:40, Train, Epoch : 6, Step : 3660, Loss : 0.36585, Acc : 0.847, Sensitive_Loss : 0.07526, Sensitive_Acc : 17.200, Run Time : 9.45 sec
INFO:root:2024-04-10 17:56:52, Train, Epoch : 6, Step : 3670, Loss : 0.38178, Acc : 0.834, Sensitive_Loss : 0.09701, Sensitive_Acc : 18.300, Run Time : 12.14 sec
INFO:root:2024-04-10 17:57:02, Train, Epoch : 6, Step : 3680, Loss : 0.40349, Acc : 0.850, Sensitive_Loss : 0.08636, Sensitive_Acc : 16.800, Run Time : 10.00 sec
INFO:root:2024-04-10 17:57:12, Train, Epoch : 6, Step : 3690, Loss : 0.36298, Acc : 0.859, Sensitive_Loss : 0.11116, Sensitive_Acc : 17.600, Run Time : 9.88 sec
INFO:root:2024-04-10 17:57:21, Train, Epoch : 6, Step : 3700, Loss : 0.35643, Acc : 0.856, Sensitive_Loss : 0.11654, Sensitive_Acc : 14.400, Run Time : 9.42 sec
INFO:root:2024-04-10 17:59:07, Dev, Step : 3700, Loss : 0.52597, Acc : 0.774, Auc : 0.858, Sensitive_Loss : 0.16086, Sensitive_Acc : 16.336, Sensitive_Auc : 0.993, Mean auc: 0.858, Run Time : 105.94 sec
INFO:root:2024-04-10 17:59:15, Train, Epoch : 6, Step : 3710, Loss : 0.35561, Acc : 0.844, Sensitive_Loss : 0.08898, Sensitive_Acc : 16.500, Run Time : 113.10 sec
INFO:root:2024-04-10 17:59:24, Train, Epoch : 6, Step : 3720, Loss : 0.31613, Acc : 0.850, Sensitive_Loss : 0.07661, Sensitive_Acc : 15.300, Run Time : 9.23 sec
INFO:root:2024-04-10 17:59:33, Train, Epoch : 6, Step : 3730, Loss : 0.38837, Acc : 0.847, Sensitive_Loss : 0.08764, Sensitive_Acc : 16.900, Run Time : 9.61 sec
INFO:root:2024-04-10 17:59:42, Train, Epoch : 6, Step : 3740, Loss : 0.44074, Acc : 0.844, Sensitive_Loss : 0.09775, Sensitive_Acc : 16.100, Run Time : 9.01 sec
INFO:root:2024-04-10 17:59:51, Train, Epoch : 6, Step : 3750, Loss : 0.32131, Acc : 0.850, Sensitive_Loss : 0.13967, Sensitive_Acc : 17.100, Run Time : 9.04 sec
INFO:root:2024-04-10 18:00:03, Train, Epoch : 6, Step : 3760, Loss : 0.38873, Acc : 0.838, Sensitive_Loss : 0.11734, Sensitive_Acc : 14.300, Run Time : 11.41 sec
INFO:root:2024-04-10 18:00:12, Train, Epoch : 6, Step : 3770, Loss : 0.42786, Acc : 0.781, Sensitive_Loss : 0.10168, Sensitive_Acc : 16.600, Run Time : 9.04 sec
INFO:root:2024-04-10 18:00:21, Train, Epoch : 6, Step : 3780, Loss : 0.33911, Acc : 0.866, Sensitive_Loss : 0.10485, Sensitive_Acc : 17.400, Run Time : 9.17 sec
INFO:root:2024-04-10 18:00:31, Train, Epoch : 6, Step : 3790, Loss : 0.37556, Acc : 0.834, Sensitive_Loss : 0.12617, Sensitive_Acc : 18.400, Run Time : 9.59 sec
INFO:root:2024-04-10 18:00:40, Train, Epoch : 6, Step : 3800, Loss : 0.39166, Acc : 0.847, Sensitive_Loss : 0.10832, Sensitive_Acc : 18.000, Run Time : 9.47 sec
INFO:root:2024-04-10 18:02:14, Dev, Step : 3800, Loss : 0.52035, Acc : 0.773, Auc : 0.858, Sensitive_Loss : 0.15963, Sensitive_Acc : 16.350, Sensitive_Auc : 0.992, Mean auc: 0.858, Run Time : 94.18 sec
INFO:root:2024-04-10 18:02:21, Train, Epoch : 6, Step : 3810, Loss : 0.31764, Acc : 0.869, Sensitive_Loss : 0.07751, Sensitive_Acc : 14.700, Run Time : 100.97 sec
INFO:root:2024-04-10 18:02:31, Train, Epoch : 6, Step : 3820, Loss : 0.35107, Acc : 0.850, Sensitive_Loss : 0.08840, Sensitive_Acc : 15.600, Run Time : 10.00 sec
INFO:root:2024-04-10 18:02:40, Train, Epoch : 6, Step : 3830, Loss : 0.40758, Acc : 0.822, Sensitive_Loss : 0.09111, Sensitive_Acc : 16.300, Run Time : 9.15 sec
INFO:root:2024-04-10 18:02:50, Train, Epoch : 6, Step : 3840, Loss : 0.37907, Acc : 0.825, Sensitive_Loss : 0.13541, Sensitive_Acc : 16.100, Run Time : 9.47 sec
INFO:root:2024-04-10 18:03:00, Train, Epoch : 6, Step : 3850, Loss : 0.39758, Acc : 0.800, Sensitive_Loss : 0.09927, Sensitive_Acc : 17.300, Run Time : 10.35 sec
INFO:root:2024-04-10 18:03:10, Train, Epoch : 6, Step : 3860, Loss : 0.39790, Acc : 0.812, Sensitive_Loss : 0.10501, Sensitive_Acc : 16.600, Run Time : 10.26 sec
INFO:root:2024-04-10 18:04:47
INFO:root:y_pred: [0.28395522 0.1553095  0.731744   ... 0.653435   0.30088124 0.41460302]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9913222e-01 1.8381122e-04 4.1022632e-01 9.9980646e-01 9.9724042e-01
 9.9756134e-01 9.9974281e-01 1.8317610e-04 9.9266022e-01 9.9446553e-01
 1.3338381e-01 6.5732223e-01 1.5379856e-05 9.9247837e-01 9.9877781e-01
 9.9977702e-01 9.9047613e-01 9.8921520e-01 9.9431193e-01 9.9927884e-01
 9.9726832e-01 2.4377659e-02 9.9431473e-01 7.9755974e-01 9.7390616e-01
 7.5911949e-03 9.9508566e-01 3.3589762e-02 9.9983919e-01 8.4135029e-03
 8.5772946e-04 5.2115250e-01 2.6155766e-03 9.9703908e-01 9.3071037e-07
 9.9946362e-01 1.8052166e-04 9.9997413e-01 3.9605062e-02 9.9775749e-01
 9.9988604e-01 4.8364368e-03 3.6616690e-02 7.6985241e-05 2.2116239e-01
 1.7993443e-01 9.9961948e-01 9.7001439e-01 9.8647320e-01 9.9927777e-01
 3.0268703e-04 8.2947743e-01 4.5334711e-03 2.2576961e-01 9.9837899e-01
 6.7688241e-03 8.8986975e-01 9.9985766e-01 9.9694365e-01 9.4963061e-03
 8.7977350e-03 9.9738759e-01 3.3144259e-01 9.9995351e-01 9.7685516e-01
 2.2655131e-01 8.7764847e-01 8.0681485e-01 9.9978822e-01 9.9374938e-01
 5.6055957e-04 9.5214057e-01 9.8294199e-01 9.9863344e-01 9.9898940e-01
 1.2116186e-05 4.1896442e-01 5.7811644e-03 9.1442171e-06 9.8882550e-01
 5.9636012e-03 9.9875808e-01 9.9896061e-01 9.9903738e-01 3.0378278e-02
 9.9999678e-01 1.9439546e-04 6.0919300e-03 9.9958020e-01 9.9674302e-01
 5.5813413e-02 9.2302519e-01 1.5885692e-02 4.4994476e-01 4.0487280e-01
 9.9996817e-01 6.2349420e-03 9.9957973e-01 9.8193157e-01 1.8616121e-03
 6.8609224e-06 1.4123769e-01 9.9780780e-01 9.9985039e-01 9.6437389e-01
 4.5033404e-01 9.9976975e-01 5.1509328e-02 3.3272639e-01 9.9688655e-01
 5.5350992e-04 5.6836894e-04 3.6879577e-02 9.9932003e-01 9.9660683e-01
 2.2678131e-03 9.9639708e-01 7.8812958e-04 9.9874628e-01 3.0138266e-01
 9.9910563e-01 9.9962187e-01 3.9491102e-01 3.2245299e-01 1.5839523e-01
 5.9845638e-03 4.2721349e-01 3.7371952e-04 9.9325609e-01 9.9990606e-01
 2.1142060e-04 3.7862017e-05 3.6027012e-04 5.8164738e-02 9.9697876e-01
 9.9790633e-01 9.9268556e-01 3.4959474e-01 1.1284428e-01 9.8808527e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 18:04:47, Dev, Step : 3864, Loss : 0.51961, Acc : 0.775, Auc : 0.857, Sensitive_Loss : 0.12825, Sensitive_Acc : 16.336, Sensitive_Auc : 0.992, Mean auc: 0.857, Run Time : 92.66 sec
INFO:root:2024-04-10 18:04:54, Train, Epoch : 7, Step : 3870, Loss : 0.23370, Acc : 0.503, Sensitive_Loss : 0.09415, Sensitive_Acc : 9.600, Run Time : 6.16 sec
INFO:root:2024-04-10 18:05:03, Train, Epoch : 7, Step : 3880, Loss : 0.36483, Acc : 0.850, Sensitive_Loss : 0.09250, Sensitive_Acc : 16.300, Run Time : 8.73 sec
INFO:root:2024-04-10 18:05:12, Train, Epoch : 7, Step : 3890, Loss : 0.35727, Acc : 0.841, Sensitive_Loss : 0.09709, Sensitive_Acc : 15.400, Run Time : 9.80 sec
INFO:root:2024-04-10 18:05:22, Train, Epoch : 7, Step : 3900, Loss : 0.28452, Acc : 0.853, Sensitive_Loss : 0.13001, Sensitive_Acc : 14.900, Run Time : 9.23 sec
INFO:root:2024-04-10 18:06:58, Dev, Step : 3900, Loss : 0.51536, Acc : 0.775, Auc : 0.857, Sensitive_Loss : 0.12293, Sensitive_Acc : 16.307, Sensitive_Auc : 0.993, Mean auc: 0.857, Run Time : 96.28 sec
INFO:root:2024-04-10 18:07:05, Train, Epoch : 7, Step : 3910, Loss : 0.34874, Acc : 0.834, Sensitive_Loss : 0.08342, Sensitive_Acc : 16.700, Run Time : 103.26 sec
INFO:root:2024-04-10 18:07:14, Train, Epoch : 7, Step : 3920, Loss : 0.31861, Acc : 0.834, Sensitive_Loss : 0.10653, Sensitive_Acc : 16.400, Run Time : 9.41 sec
INFO:root:2024-04-10 18:07:23, Train, Epoch : 7, Step : 3930, Loss : 0.28761, Acc : 0.887, Sensitive_Loss : 0.09429, Sensitive_Acc : 15.800, Run Time : 9.22 sec
INFO:root:2024-04-10 18:07:36, Train, Epoch : 7, Step : 3940, Loss : 0.31636, Acc : 0.844, Sensitive_Loss : 0.08513, Sensitive_Acc : 16.700, Run Time : 12.28 sec
INFO:root:2024-04-10 18:07:46, Train, Epoch : 7, Step : 3950, Loss : 0.26076, Acc : 0.866, Sensitive_Loss : 0.07212, Sensitive_Acc : 16.500, Run Time : 10.10 sec
INFO:root:2024-04-10 18:07:55, Train, Epoch : 7, Step : 3960, Loss : 0.39181, Acc : 0.825, Sensitive_Loss : 0.09335, Sensitive_Acc : 16.800, Run Time : 9.24 sec
INFO:root:2024-04-10 18:08:04, Train, Epoch : 7, Step : 3970, Loss : 0.34231, Acc : 0.866, Sensitive_Loss : 0.10401, Sensitive_Acc : 17.600, Run Time : 9.07 sec
INFO:root:2024-04-10 18:08:13, Train, Epoch : 7, Step : 3980, Loss : 0.33577, Acc : 0.838, Sensitive_Loss : 0.06304, Sensitive_Acc : 16.200, Run Time : 9.08 sec
INFO:root:2024-04-10 18:08:22, Train, Epoch : 7, Step : 3990, Loss : 0.33606, Acc : 0.841, Sensitive_Loss : 0.12812, Sensitive_Acc : 14.900, Run Time : 9.13 sec
INFO:root:2024-04-10 18:08:31, Train, Epoch : 7, Step : 4000, Loss : 0.35343, Acc : 0.863, Sensitive_Loss : 0.09156, Sensitive_Acc : 15.900, Run Time : 8.98 sec
INFO:root:2024-04-10 18:10:07, Dev, Step : 4000, Loss : 0.53317, Acc : 0.770, Auc : 0.859, Sensitive_Loss : 0.13776, Sensitive_Acc : 16.236, Sensitive_Auc : 0.994, Mean auc: 0.859, Run Time : 96.02 sec
INFO:root:2024-04-10 18:10:14, Train, Epoch : 7, Step : 4010, Loss : 0.39620, Acc : 0.812, Sensitive_Loss : 0.09546, Sensitive_Acc : 15.800, Run Time : 102.69 sec
INFO:root:2024-04-10 18:10:27, Train, Epoch : 7, Step : 4020, Loss : 0.28176, Acc : 0.912, Sensitive_Loss : 0.13431, Sensitive_Acc : 14.700, Run Time : 13.07 sec
INFO:root:2024-04-10 18:10:37, Train, Epoch : 7, Step : 4030, Loss : 0.26963, Acc : 0.875, Sensitive_Loss : 0.07802, Sensitive_Acc : 15.800, Run Time : 9.44 sec
INFO:root:2024-04-10 18:10:46, Train, Epoch : 7, Step : 4040, Loss : 0.32275, Acc : 0.847, Sensitive_Loss : 0.10125, Sensitive_Acc : 15.400, Run Time : 9.49 sec
INFO:root:2024-04-10 18:10:58, Train, Epoch : 7, Step : 4050, Loss : 0.25426, Acc : 0.891, Sensitive_Loss : 0.11763, Sensitive_Acc : 16.400, Run Time : 12.39 sec
INFO:root:2024-04-10 18:11:08, Train, Epoch : 7, Step : 4060, Loss : 0.27259, Acc : 0.875, Sensitive_Loss : 0.10495, Sensitive_Acc : 15.700, Run Time : 10.05 sec
INFO:root:2024-04-10 18:11:17, Train, Epoch : 7, Step : 4070, Loss : 0.30499, Acc : 0.881, Sensitive_Loss : 0.06901, Sensitive_Acc : 17.800, Run Time : 8.70 sec
INFO:root:2024-04-10 18:11:27, Train, Epoch : 7, Step : 4080, Loss : 0.33368, Acc : 0.869, Sensitive_Loss : 0.10011, Sensitive_Acc : 15.900, Run Time : 9.69 sec
INFO:root:2024-04-10 18:11:37, Train, Epoch : 7, Step : 4090, Loss : 0.36098, Acc : 0.828, Sensitive_Loss : 0.05903, Sensitive_Acc : 17.000, Run Time : 9.94 sec
INFO:root:2024-04-10 18:11:47, Train, Epoch : 7, Step : 4100, Loss : 0.37061, Acc : 0.844, Sensitive_Loss : 0.14793, Sensitive_Acc : 18.100, Run Time : 10.01 sec
INFO:root:2024-04-10 18:13:22, Dev, Step : 4100, Loss : 0.52071, Acc : 0.775, Auc : 0.861, Sensitive_Loss : 0.15964, Sensitive_Acc : 16.364, Sensitive_Auc : 0.994, Mean auc: 0.861, Run Time : 94.95 sec
INFO:root:2024-04-10 18:13:29, Train, Epoch : 7, Step : 4110, Loss : 0.43250, Acc : 0.825, Sensitive_Loss : 0.10905, Sensitive_Acc : 16.700, Run Time : 102.35 sec
INFO:root:2024-04-10 18:13:39, Train, Epoch : 7, Step : 4120, Loss : 0.29651, Acc : 0.853, Sensitive_Loss : 0.08921, Sensitive_Acc : 17.500, Run Time : 9.94 sec
INFO:root:2024-04-10 18:13:50, Train, Epoch : 7, Step : 4130, Loss : 0.33889, Acc : 0.872, Sensitive_Loss : 0.12554, Sensitive_Acc : 16.300, Run Time : 10.75 sec
INFO:root:2024-04-10 18:13:59, Train, Epoch : 7, Step : 4140, Loss : 0.36490, Acc : 0.825, Sensitive_Loss : 0.09233, Sensitive_Acc : 16.900, Run Time : 9.56 sec
INFO:root:2024-04-10 18:14:09, Train, Epoch : 7, Step : 4150, Loss : 0.31461, Acc : 0.834, Sensitive_Loss : 0.12257, Sensitive_Acc : 15.700, Run Time : 9.56 sec
INFO:root:2024-04-10 18:14:18, Train, Epoch : 7, Step : 4160, Loss : 0.35253, Acc : 0.841, Sensitive_Loss : 0.07572, Sensitive_Acc : 16.400, Run Time : 9.05 sec
INFO:root:2024-04-10 18:14:30, Train, Epoch : 7, Step : 4170, Loss : 0.30522, Acc : 0.863, Sensitive_Loss : 0.08179, Sensitive_Acc : 17.100, Run Time : 11.83 sec
INFO:root:2024-04-10 18:14:39, Train, Epoch : 7, Step : 4180, Loss : 0.34995, Acc : 0.847, Sensitive_Loss : 0.08918, Sensitive_Acc : 15.700, Run Time : 9.26 sec
INFO:root:2024-04-10 18:14:48, Train, Epoch : 7, Step : 4190, Loss : 0.38836, Acc : 0.838, Sensitive_Loss : 0.14695, Sensitive_Acc : 16.300, Run Time : 9.27 sec
INFO:root:2024-04-10 18:15:00, Train, Epoch : 7, Step : 4200, Loss : 0.31902, Acc : 0.856, Sensitive_Loss : 0.09329, Sensitive_Acc : 17.500, Run Time : 11.89 sec
INFO:root:2024-04-10 18:16:37, Dev, Step : 4200, Loss : 0.51526, Acc : 0.777, Auc : 0.861, Sensitive_Loss : 0.14137, Sensitive_Acc : 16.264, Sensitive_Auc : 0.991, Mean auc: 0.861, Run Time : 96.64 sec
INFO:root:2024-04-10 18:16:45, Train, Epoch : 7, Step : 4210, Loss : 0.34455, Acc : 0.863, Sensitive_Loss : 0.11690, Sensitive_Acc : 15.400, Run Time : 105.12 sec
INFO:root:2024-04-10 18:16:55, Train, Epoch : 7, Step : 4220, Loss : 0.38765, Acc : 0.841, Sensitive_Loss : 0.10687, Sensitive_Acc : 16.500, Run Time : 10.08 sec
INFO:root:2024-04-10 18:17:05, Train, Epoch : 7, Step : 4230, Loss : 0.33729, Acc : 0.831, Sensitive_Loss : 0.11610, Sensitive_Acc : 16.200, Run Time : 9.28 sec
INFO:root:2024-04-10 18:17:15, Train, Epoch : 7, Step : 4240, Loss : 0.38808, Acc : 0.841, Sensitive_Loss : 0.08489, Sensitive_Acc : 16.500, Run Time : 10.33 sec
INFO:root:2024-04-10 18:17:24, Train, Epoch : 7, Step : 4250, Loss : 0.31967, Acc : 0.875, Sensitive_Loss : 0.12292, Sensitive_Acc : 17.600, Run Time : 9.42 sec
INFO:root:2024-04-10 18:17:33, Train, Epoch : 7, Step : 4260, Loss : 0.31271, Acc : 0.859, Sensitive_Loss : 0.10805, Sensitive_Acc : 14.700, Run Time : 8.77 sec
INFO:root:2024-04-10 18:17:42, Train, Epoch : 7, Step : 4270, Loss : 0.34874, Acc : 0.869, Sensitive_Loss : 0.13306, Sensitive_Acc : 16.300, Run Time : 8.99 sec
INFO:root:2024-04-10 18:17:52, Train, Epoch : 7, Step : 4280, Loss : 0.30882, Acc : 0.853, Sensitive_Loss : 0.10420, Sensitive_Acc : 16.300, Run Time : 9.46 sec
INFO:root:2024-04-10 18:18:02, Train, Epoch : 7, Step : 4290, Loss : 0.36391, Acc : 0.828, Sensitive_Loss : 0.12373, Sensitive_Acc : 17.700, Run Time : 10.61 sec
INFO:root:2024-04-10 18:18:11, Train, Epoch : 7, Step : 4300, Loss : 0.32806, Acc : 0.850, Sensitive_Loss : 0.08644, Sensitive_Acc : 15.400, Run Time : 9.06 sec
INFO:root:2024-04-10 18:19:58, Dev, Step : 4300, Loss : 0.53240, Acc : 0.777, Auc : 0.858, Sensitive_Loss : 0.15507, Sensitive_Acc : 16.279, Sensitive_Auc : 0.996, Mean auc: 0.858, Run Time : 106.24 sec
INFO:root:2024-04-10 18:20:04, Train, Epoch : 7, Step : 4310, Loss : 0.29990, Acc : 0.900, Sensitive_Loss : 0.11090, Sensitive_Acc : 16.800, Run Time : 112.78 sec
INFO:root:2024-04-10 18:20:14, Train, Epoch : 7, Step : 4320, Loss : 0.32092, Acc : 0.866, Sensitive_Loss : 0.10362, Sensitive_Acc : 16.000, Run Time : 10.24 sec
INFO:root:2024-04-10 18:20:26, Train, Epoch : 7, Step : 4330, Loss : 0.42413, Acc : 0.772, Sensitive_Loss : 0.10230, Sensitive_Acc : 15.200, Run Time : 11.31 sec
INFO:root:2024-04-10 18:20:35, Train, Epoch : 7, Step : 4340, Loss : 0.34991, Acc : 0.847, Sensitive_Loss : 0.15452, Sensitive_Acc : 16.800, Run Time : 8.80 sec
INFO:root:2024-04-10 18:20:44, Train, Epoch : 7, Step : 4350, Loss : 0.29698, Acc : 0.847, Sensitive_Loss : 0.09208, Sensitive_Acc : 15.700, Run Time : 9.01 sec
INFO:root:2024-04-10 18:20:53, Train, Epoch : 7, Step : 4360, Loss : 0.34289, Acc : 0.847, Sensitive_Loss : 0.11273, Sensitive_Acc : 14.500, Run Time : 9.74 sec
INFO:root:2024-04-10 18:21:02, Train, Epoch : 7, Step : 4370, Loss : 0.36032, Acc : 0.847, Sensitive_Loss : 0.15007, Sensitive_Acc : 17.500, Run Time : 9.19 sec
INFO:root:2024-04-10 18:21:11, Train, Epoch : 7, Step : 4380, Loss : 0.31230, Acc : 0.850, Sensitive_Loss : 0.11408, Sensitive_Acc : 16.400, Run Time : 8.40 sec
INFO:root:2024-04-10 18:21:20, Train, Epoch : 7, Step : 4390, Loss : 0.31778, Acc : 0.850, Sensitive_Loss : 0.08636, Sensitive_Acc : 16.900, Run Time : 9.15 sec
INFO:root:2024-04-10 18:21:31, Train, Epoch : 7, Step : 4400, Loss : 0.31703, Acc : 0.872, Sensitive_Loss : 0.11031, Sensitive_Acc : 16.000, Run Time : 10.96 sec
INFO:root:2024-04-10 18:23:05, Dev, Step : 4400, Loss : 0.52814, Acc : 0.772, Auc : 0.858, Sensitive_Loss : 0.13625, Sensitive_Acc : 16.264, Sensitive_Auc : 0.993, Mean auc: 0.858, Run Time : 93.66 sec
INFO:root:2024-04-10 18:23:11, Train, Epoch : 7, Step : 4410, Loss : 0.31669, Acc : 0.872, Sensitive_Loss : 0.10055, Sensitive_Acc : 17.000, Run Time : 100.48 sec
INFO:root:2024-04-10 18:23:21, Train, Epoch : 7, Step : 4420, Loss : 0.30518, Acc : 0.875, Sensitive_Loss : 0.10626, Sensitive_Acc : 17.200, Run Time : 9.68 sec
INFO:root:2024-04-10 18:23:30, Train, Epoch : 7, Step : 4430, Loss : 0.32867, Acc : 0.825, Sensitive_Loss : 0.15060, Sensitive_Acc : 15.300, Run Time : 8.96 sec
INFO:root:2024-04-10 18:23:39, Train, Epoch : 7, Step : 4440, Loss : 0.32230, Acc : 0.850, Sensitive_Loss : 0.09788, Sensitive_Acc : 15.200, Run Time : 9.03 sec
INFO:root:2024-04-10 18:23:49, Train, Epoch : 7, Step : 4450, Loss : 0.30336, Acc : 0.875, Sensitive_Loss : 0.10964, Sensitive_Acc : 16.000, Run Time : 9.97 sec
INFO:root:2024-04-10 18:23:59, Train, Epoch : 7, Step : 4460, Loss : 0.35542, Acc : 0.834, Sensitive_Loss : 0.08431, Sensitive_Acc : 15.300, Run Time : 9.55 sec
INFO:root:2024-04-10 18:24:08, Train, Epoch : 7, Step : 4470, Loss : 0.36111, Acc : 0.856, Sensitive_Loss : 0.11075, Sensitive_Acc : 15.800, Run Time : 9.39 sec
INFO:root:2024-04-10 18:24:18, Train, Epoch : 7, Step : 4480, Loss : 0.33941, Acc : 0.856, Sensitive_Loss : 0.11887, Sensitive_Acc : 16.500, Run Time : 9.92 sec
INFO:root:2024-04-10 18:24:30, Train, Epoch : 7, Step : 4490, Loss : 0.33998, Acc : 0.863, Sensitive_Loss : 0.13017, Sensitive_Acc : 15.400, Run Time : 12.43 sec
INFO:root:2024-04-10 18:24:40, Train, Epoch : 7, Step : 4500, Loss : 0.35330, Acc : 0.819, Sensitive_Loss : 0.08642, Sensitive_Acc : 16.400, Run Time : 9.86 sec
INFO:root:2024-04-10 18:26:15, Dev, Step : 4500, Loss : 0.55476, Acc : 0.771, Auc : 0.856, Sensitive_Loss : 0.14675, Sensitive_Acc : 16.279, Sensitive_Auc : 0.993, Mean auc: 0.856, Run Time : 95.13 sec
INFO:root:2024-04-10 18:27:52
INFO:root:y_pred: [0.10755958 0.11418281 0.6555513  ... 0.6174364  0.35163736 0.24958849]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9904436e-01 3.3108096e-04 3.2908630e-01 9.9988413e-01 9.9897325e-01
 9.9870038e-01 9.9979395e-01 4.1628911e-04 9.9227685e-01 9.9093854e-01
 1.4988172e-01 7.3837721e-01 5.4163083e-05 9.9036825e-01 9.9919051e-01
 9.9973553e-01 9.9252349e-01 9.8895264e-01 9.9587101e-01 9.9951935e-01
 9.9578875e-01 5.4308943e-02 9.9769670e-01 7.9582620e-01 9.7522551e-01
 1.1401424e-02 9.9639374e-01 6.1763771e-02 9.9990416e-01 1.4715050e-02
 1.7092515e-03 5.3456986e-01 4.0323897e-03 9.9592978e-01 1.4563328e-06
 9.9969554e-01 2.7906083e-04 9.9995482e-01 2.8900219e-02 9.9894398e-01
 9.9991512e-01 6.4885342e-03 3.9001737e-02 8.7515262e-05 2.0741034e-01
 1.4162239e-01 9.9969900e-01 9.7638583e-01 9.9016869e-01 9.9932194e-01
 4.4065475e-04 8.4393418e-01 1.7936049e-02 2.4901204e-01 9.9900264e-01
 4.2371224e-03 9.4406325e-01 9.9987555e-01 9.9684048e-01 7.3511102e-03
 1.0290437e-02 9.9793768e-01 3.5858110e-01 9.9994755e-01 9.8003089e-01
 2.7312639e-01 9.2018199e-01 8.6010551e-01 9.9970406e-01 9.9642342e-01
 8.7322458e-04 9.5894456e-01 9.8767686e-01 9.9938107e-01 9.9855608e-01
 3.9002607e-05 5.9774506e-01 8.1153568e-03 2.1439733e-05 9.8871535e-01
 1.3489917e-02 9.9855083e-01 9.9883848e-01 9.9921799e-01 2.5971141e-02
 9.9999857e-01 2.8866148e-04 1.5138775e-02 9.9966145e-01 9.9755257e-01
 6.3567378e-02 9.3616444e-01 3.5442561e-02 6.6109520e-01 5.6673968e-01
 9.9995935e-01 7.7401842e-03 9.9983871e-01 9.7886813e-01 2.5821517e-03
 7.5285454e-05 2.6922953e-01 9.9756312e-01 9.9989116e-01 9.8343247e-01
 4.7227687e-01 9.9979013e-01 7.1672916e-02 2.7610451e-01 9.9723202e-01
 5.2019540e-04 7.5363251e-04 3.4990303e-02 9.9936277e-01 9.9794990e-01
 2.6911779e-03 9.9735701e-01 1.7650317e-03 9.9703836e-01 5.0959533e-01
 9.9838614e-01 9.9967861e-01 4.0665665e-01 3.5219228e-01 1.9638585e-01
 1.1993573e-02 4.2161691e-01 2.0667159e-03 9.8833406e-01 9.9994159e-01
 4.5663334e-04 5.6754594e-05 4.5317510e-04 6.0132541e-02 9.9715304e-01
 9.9742883e-01 9.9486285e-01 4.1573319e-01 1.3864076e-01 9.9330842e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 18:27:52, Dev, Step : 4508, Loss : 0.55636, Acc : 0.769, Auc : 0.857, Sensitive_Loss : 0.14314, Sensitive_Acc : 16.279, Sensitive_Auc : 0.993, Mean auc: 0.857, Run Time : 92.49 sec
INFO:root:2024-04-10 18:27:56, Train, Epoch : 8, Step : 4510, Loss : 0.09803, Acc : 0.159, Sensitive_Loss : 0.01536, Sensitive_Acc : 3.400, Run Time : 2.74 sec
INFO:root:2024-04-10 18:28:06, Train, Epoch : 8, Step : 4520, Loss : 0.29943, Acc : 0.856, Sensitive_Loss : 0.13313, Sensitive_Acc : 17.200, Run Time : 9.70 sec
INFO:root:2024-04-10 18:28:15, Train, Epoch : 8, Step : 4530, Loss : 0.33765, Acc : 0.853, Sensitive_Loss : 0.14691, Sensitive_Acc : 16.900, Run Time : 9.50 sec
INFO:root:2024-04-10 18:28:27, Train, Epoch : 8, Step : 4540, Loss : 0.27794, Acc : 0.891, Sensitive_Loss : 0.13534, Sensitive_Acc : 13.200, Run Time : 11.57 sec
INFO:root:2024-04-10 18:28:38, Train, Epoch : 8, Step : 4550, Loss : 0.30807, Acc : 0.853, Sensitive_Loss : 0.09153, Sensitive_Acc : 15.800, Run Time : 10.70 sec
INFO:root:2024-04-10 18:28:47, Train, Epoch : 8, Step : 4560, Loss : 0.29119, Acc : 0.878, Sensitive_Loss : 0.13215, Sensitive_Acc : 15.400, Run Time : 9.07 sec
INFO:root:2024-04-10 18:28:57, Train, Epoch : 8, Step : 4570, Loss : 0.31158, Acc : 0.869, Sensitive_Loss : 0.09889, Sensitive_Acc : 16.900, Run Time : 10.70 sec
INFO:root:2024-04-10 18:29:09, Train, Epoch : 8, Step : 4580, Loss : 0.31548, Acc : 0.847, Sensitive_Loss : 0.08547, Sensitive_Acc : 16.700, Run Time : 11.62 sec
INFO:root:2024-04-10 18:29:18, Train, Epoch : 8, Step : 4590, Loss : 0.30369, Acc : 0.859, Sensitive_Loss : 0.13496, Sensitive_Acc : 14.800, Run Time : 8.91 sec
INFO:root:2024-04-10 18:29:27, Train, Epoch : 8, Step : 4600, Loss : 0.31012, Acc : 0.844, Sensitive_Loss : 0.08035, Sensitive_Acc : 15.700, Run Time : 9.04 sec
INFO:root:2024-04-10 18:31:03, Dev, Step : 4600, Loss : 0.53599, Acc : 0.774, Auc : 0.860, Sensitive_Loss : 0.16614, Sensitive_Acc : 16.279, Sensitive_Auc : 0.994, Mean auc: 0.860, Run Time : 95.77 sec
INFO:root:2024-04-10 18:31:10, Train, Epoch : 8, Step : 4610, Loss : 0.26327, Acc : 0.850, Sensitive_Loss : 0.09634, Sensitive_Acc : 15.500, Run Time : 102.81 sec
INFO:root:2024-04-10 18:31:20, Train, Epoch : 8, Step : 4620, Loss : 0.28431, Acc : 0.856, Sensitive_Loss : 0.09450, Sensitive_Acc : 16.300, Run Time : 9.72 sec
INFO:root:2024-04-10 18:31:29, Train, Epoch : 8, Step : 4630, Loss : 0.28519, Acc : 0.872, Sensitive_Loss : 0.08669, Sensitive_Acc : 15.500, Run Time : 9.24 sec
INFO:root:2024-04-10 18:31:38, Train, Epoch : 8, Step : 4640, Loss : 0.28039, Acc : 0.878, Sensitive_Loss : 0.13138, Sensitive_Acc : 16.500, Run Time : 8.86 sec
INFO:root:2024-04-10 18:31:47, Train, Epoch : 8, Step : 4650, Loss : 0.34601, Acc : 0.847, Sensitive_Loss : 0.07092, Sensitive_Acc : 17.200, Run Time : 8.98 sec
INFO:root:2024-04-10 18:31:56, Train, Epoch : 8, Step : 4660, Loss : 0.30552, Acc : 0.822, Sensitive_Loss : 0.08199, Sensitive_Acc : 16.900, Run Time : 9.52 sec
INFO:root:2024-04-10 18:32:06, Train, Epoch : 8, Step : 4670, Loss : 0.34922, Acc : 0.869, Sensitive_Loss : 0.14111, Sensitive_Acc : 15.300, Run Time : 10.16 sec
INFO:root:2024-04-10 18:32:15, Train, Epoch : 8, Step : 4680, Loss : 0.35785, Acc : 0.831, Sensitive_Loss : 0.09012, Sensitive_Acc : 15.300, Run Time : 9.05 sec
INFO:root:2024-04-10 18:32:24, Train, Epoch : 8, Step : 4690, Loss : 0.26279, Acc : 0.878, Sensitive_Loss : 0.12268, Sensitive_Acc : 15.800, Run Time : 8.85 sec
INFO:root:2024-04-10 18:32:34, Train, Epoch : 8, Step : 4700, Loss : 0.30083, Acc : 0.881, Sensitive_Loss : 0.10112, Sensitive_Acc : 17.900, Run Time : 9.32 sec
INFO:root:2024-04-10 18:34:16, Dev, Step : 4700, Loss : 0.53101, Acc : 0.779, Auc : 0.860, Sensitive_Loss : 0.13888, Sensitive_Acc : 16.221, Sensitive_Auc : 0.993, Mean auc: 0.860, Run Time : 102.74 sec
INFO:root:2024-04-10 18:34:24, Train, Epoch : 8, Step : 4710, Loss : 0.32724, Acc : 0.828, Sensitive_Loss : 0.13290, Sensitive_Acc : 16.300, Run Time : 110.10 sec
INFO:root:2024-04-10 18:34:33, Train, Epoch : 8, Step : 4720, Loss : 0.34541, Acc : 0.841, Sensitive_Loss : 0.08553, Sensitive_Acc : 17.500, Run Time : 9.57 sec
INFO:root:2024-04-10 18:34:44, Train, Epoch : 8, Step : 4730, Loss : 0.31444, Acc : 0.863, Sensitive_Loss : 0.12291, Sensitive_Acc : 16.600, Run Time : 11.02 sec
INFO:root:2024-04-10 18:34:57, Train, Epoch : 8, Step : 4740, Loss : 0.31496, Acc : 0.859, Sensitive_Loss : 0.11158, Sensitive_Acc : 17.500, Run Time : 12.67 sec
INFO:root:2024-04-10 18:35:07, Train, Epoch : 8, Step : 4750, Loss : 0.36339, Acc : 0.828, Sensitive_Loss : 0.09496, Sensitive_Acc : 15.900, Run Time : 10.50 sec
INFO:root:2024-04-10 18:35:18, Train, Epoch : 8, Step : 4760, Loss : 0.24785, Acc : 0.875, Sensitive_Loss : 0.06876, Sensitive_Acc : 15.700, Run Time : 10.76 sec
INFO:root:2024-04-10 18:35:28, Train, Epoch : 8, Step : 4770, Loss : 0.33857, Acc : 0.863, Sensitive_Loss : 0.08229, Sensitive_Acc : 15.600, Run Time : 9.84 sec
INFO:root:2024-04-10 18:35:38, Train, Epoch : 8, Step : 4780, Loss : 0.29682, Acc : 0.856, Sensitive_Loss : 0.09241, Sensitive_Acc : 14.400, Run Time : 9.77 sec
INFO:root:2024-04-10 18:35:47, Train, Epoch : 8, Step : 4790, Loss : 0.33106, Acc : 0.841, Sensitive_Loss : 0.09455, Sensitive_Acc : 15.700, Run Time : 8.96 sec
INFO:root:2024-04-10 18:35:58, Train, Epoch : 8, Step : 4800, Loss : 0.24582, Acc : 0.884, Sensitive_Loss : 0.08995, Sensitive_Acc : 17.900, Run Time : 11.14 sec
INFO:root:2024-04-10 18:37:59, Dev, Step : 4800, Loss : 0.56362, Acc : 0.766, Auc : 0.856, Sensitive_Loss : 0.13610, Sensitive_Acc : 16.264, Sensitive_Auc : 0.993, Mean auc: 0.856, Run Time : 121.05 sec
INFO:root:2024-04-10 18:38:06, Train, Epoch : 8, Step : 4810, Loss : 0.29234, Acc : 0.872, Sensitive_Loss : 0.08023, Sensitive_Acc : 15.000, Run Time : 127.78 sec
INFO:root:2024-04-10 18:38:15, Train, Epoch : 8, Step : 4820, Loss : 0.36310, Acc : 0.866, Sensitive_Loss : 0.10613, Sensitive_Acc : 16.300, Run Time : 9.58 sec
INFO:root:2024-04-10 18:38:27, Train, Epoch : 8, Step : 4830, Loss : 0.27610, Acc : 0.863, Sensitive_Loss : 0.11620, Sensitive_Acc : 15.700, Run Time : 11.73 sec
INFO:root:2024-04-10 18:38:36, Train, Epoch : 8, Step : 4840, Loss : 0.38763, Acc : 0.847, Sensitive_Loss : 0.07004, Sensitive_Acc : 15.200, Run Time : 9.20 sec
INFO:root:2024-04-10 18:38:45, Train, Epoch : 8, Step : 4850, Loss : 0.28599, Acc : 0.875, Sensitive_Loss : 0.06723, Sensitive_Acc : 16.700, Run Time : 9.02 sec
INFO:root:2024-04-10 18:38:54, Train, Epoch : 8, Step : 4860, Loss : 0.33957, Acc : 0.881, Sensitive_Loss : 0.09177, Sensitive_Acc : 16.500, Run Time : 8.84 sec
INFO:root:2024-04-10 18:39:03, Train, Epoch : 8, Step : 4870, Loss : 0.31840, Acc : 0.844, Sensitive_Loss : 0.14110, Sensitive_Acc : 15.200, Run Time : 9.07 sec
INFO:root:2024-04-10 18:39:13, Train, Epoch : 8, Step : 4880, Loss : 0.32695, Acc : 0.875, Sensitive_Loss : 0.10454, Sensitive_Acc : 17.200, Run Time : 9.74 sec
INFO:root:2024-04-10 18:39:22, Train, Epoch : 8, Step : 4890, Loss : 0.32023, Acc : 0.856, Sensitive_Loss : 0.10550, Sensitive_Acc : 16.200, Run Time : 9.50 sec
INFO:root:2024-04-10 18:39:32, Train, Epoch : 8, Step : 4900, Loss : 0.36167, Acc : 0.853, Sensitive_Loss : 0.10498, Sensitive_Acc : 17.000, Run Time : 9.19 sec
INFO:root:2024-04-10 18:41:15, Dev, Step : 4900, Loss : 0.54022, Acc : 0.773, Auc : 0.856, Sensitive_Loss : 0.13278, Sensitive_Acc : 16.236, Sensitive_Auc : 0.992, Mean auc: 0.856, Run Time : 103.62 sec
INFO:root:2024-04-10 18:41:21, Train, Epoch : 8, Step : 4910, Loss : 0.31617, Acc : 0.872, Sensitive_Loss : 0.07993, Sensitive_Acc : 15.300, Run Time : 109.90 sec
INFO:root:2024-04-10 18:41:31, Train, Epoch : 8, Step : 4920, Loss : 0.28201, Acc : 0.875, Sensitive_Loss : 0.08628, Sensitive_Acc : 16.100, Run Time : 9.22 sec
INFO:root:2024-04-10 18:41:42, Train, Epoch : 8, Step : 4930, Loss : 0.34415, Acc : 0.841, Sensitive_Loss : 0.08130, Sensitive_Acc : 17.200, Run Time : 11.08 sec
INFO:root:2024-04-10 18:41:51, Train, Epoch : 8, Step : 4940, Loss : 0.32904, Acc : 0.887, Sensitive_Loss : 0.09074, Sensitive_Acc : 16.900, Run Time : 9.40 sec
INFO:root:2024-04-10 18:42:00, Train, Epoch : 8, Step : 4950, Loss : 0.27465, Acc : 0.878, Sensitive_Loss : 0.12900, Sensitive_Acc : 17.500, Run Time : 8.91 sec
INFO:root:2024-04-10 18:42:08, Train, Epoch : 8, Step : 4960, Loss : 0.31021, Acc : 0.847, Sensitive_Loss : 0.09012, Sensitive_Acc : 16.500, Run Time : 8.45 sec
INFO:root:2024-04-10 18:42:18, Train, Epoch : 8, Step : 4970, Loss : 0.31258, Acc : 0.850, Sensitive_Loss : 0.10013, Sensitive_Acc : 14.900, Run Time : 9.17 sec
INFO:root:2024-04-10 18:42:27, Train, Epoch : 8, Step : 4980, Loss : 0.26814, Acc : 0.878, Sensitive_Loss : 0.11370, Sensitive_Acc : 16.800, Run Time : 9.82 sec
INFO:root:2024-04-10 18:42:36, Train, Epoch : 8, Step : 4990, Loss : 0.35534, Acc : 0.838, Sensitive_Loss : 0.12054, Sensitive_Acc : 15.700, Run Time : 8.60 sec
INFO:root:2024-04-10 18:42:45, Train, Epoch : 8, Step : 5000, Loss : 0.31822, Acc : 0.884, Sensitive_Loss : 0.11874, Sensitive_Acc : 16.800, Run Time : 8.88 sec
INFO:root:2024-04-10 18:44:18, Dev, Step : 5000, Loss : 0.53395, Acc : 0.779, Auc : 0.860, Sensitive_Loss : 0.13784, Sensitive_Acc : 16.236, Sensitive_Auc : 0.992, Mean auc: 0.860, Run Time : 93.05 sec
INFO:root:2024-04-10 18:44:25, Train, Epoch : 8, Step : 5010, Loss : 0.31752, Acc : 0.881, Sensitive_Loss : 0.13345, Sensitive_Acc : 15.700, Run Time : 100.05 sec
INFO:root:2024-04-10 18:44:34, Train, Epoch : 8, Step : 5020, Loss : 0.34759, Acc : 0.838, Sensitive_Loss : 0.10501, Sensitive_Acc : 17.200, Run Time : 9.42 sec
INFO:root:2024-04-10 18:44:45, Train, Epoch : 8, Step : 5030, Loss : 0.29749, Acc : 0.856, Sensitive_Loss : 0.07066, Sensitive_Acc : 15.500, Run Time : 10.70 sec
INFO:root:2024-04-10 18:44:56, Train, Epoch : 8, Step : 5040, Loss : 0.28986, Acc : 0.881, Sensitive_Loss : 0.09172, Sensitive_Acc : 15.800, Run Time : 10.94 sec
INFO:root:2024-04-10 18:45:05, Train, Epoch : 8, Step : 5050, Loss : 0.26344, Acc : 0.881, Sensitive_Loss : 0.09382, Sensitive_Acc : 16.800, Run Time : 8.80 sec
INFO:root:2024-04-10 18:45:14, Train, Epoch : 8, Step : 5060, Loss : 0.27873, Acc : 0.878, Sensitive_Loss : 0.09797, Sensitive_Acc : 14.600, Run Time : 9.52 sec
INFO:root:2024-04-10 18:45:28, Train, Epoch : 8, Step : 5070, Loss : 0.26870, Acc : 0.872, Sensitive_Loss : 0.15925, Sensitive_Acc : 14.800, Run Time : 13.67 sec
INFO:root:2024-04-10 18:45:39, Train, Epoch : 8, Step : 5080, Loss : 0.26547, Acc : 0.897, Sensitive_Loss : 0.09021, Sensitive_Acc : 16.400, Run Time : 10.60 sec
INFO:root:2024-04-10 18:45:50, Train, Epoch : 8, Step : 5090, Loss : 0.27497, Acc : 0.884, Sensitive_Loss : 0.07125, Sensitive_Acc : 15.000, Run Time : 11.43 sec
INFO:root:2024-04-10 18:46:04, Train, Epoch : 8, Step : 5100, Loss : 0.31517, Acc : 0.863, Sensitive_Loss : 0.08726, Sensitive_Acc : 15.800, Run Time : 14.08 sec
INFO:root:2024-04-10 18:47:39, Dev, Step : 5100, Loss : 0.54230, Acc : 0.779, Auc : 0.860, Sensitive_Loss : 0.12450, Sensitive_Acc : 16.293, Sensitive_Auc : 0.993, Mean auc: 0.860, Run Time : 94.35 sec
INFO:root:2024-04-10 18:47:45, Train, Epoch : 8, Step : 5110, Loss : 0.33093, Acc : 0.844, Sensitive_Loss : 0.06456, Sensitive_Acc : 16.000, Run Time : 101.16 sec
INFO:root:2024-04-10 18:47:55, Train, Epoch : 8, Step : 5120, Loss : 0.30089, Acc : 0.878, Sensitive_Loss : 0.10130, Sensitive_Acc : 17.000, Run Time : 9.93 sec
INFO:root:2024-04-10 18:48:05, Train, Epoch : 8, Step : 5130, Loss : 0.35478, Acc : 0.847, Sensitive_Loss : 0.10058, Sensitive_Acc : 16.900, Run Time : 9.39 sec
INFO:root:2024-04-10 18:48:14, Train, Epoch : 8, Step : 5140, Loss : 0.30950, Acc : 0.866, Sensitive_Loss : 0.09773, Sensitive_Acc : 14.900, Run Time : 9.46 sec
INFO:root:2024-04-10 18:48:24, Train, Epoch : 8, Step : 5150, Loss : 0.33710, Acc : 0.866, Sensitive_Loss : 0.08696, Sensitive_Acc : 15.200, Run Time : 9.75 sec
INFO:root:2024-04-10 18:49:59
INFO:root:y_pred: [0.07848992 0.07278135 0.54281914 ... 0.49909213 0.28299743 0.14772503]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9909520e-01 4.4877696e-04 3.5150337e-01 9.9983525e-01 9.9864489e-01
 9.9945170e-01 9.9976081e-01 3.9607403e-04 9.9555677e-01 9.9700230e-01
 1.9917072e-01 7.2503477e-01 4.1507974e-05 9.9415439e-01 9.9895811e-01
 9.9980205e-01 9.8764056e-01 9.9447000e-01 9.9169523e-01 9.9964941e-01
 9.9841392e-01 2.1312131e-01 9.9699926e-01 7.4489802e-01 9.7897905e-01
 9.0443380e-03 9.9897087e-01 8.2219973e-02 9.9989951e-01 1.7895827e-02
 1.7142033e-03 5.4167408e-01 5.1370296e-03 9.9257666e-01 3.0216415e-06
 9.9943620e-01 3.9666312e-04 9.9996293e-01 2.7522024e-02 9.9851292e-01
 9.9989355e-01 7.1169799e-03 2.4052389e-02 6.6549655e-05 2.6933071e-01
 2.6178309e-01 9.9985671e-01 9.8204905e-01 9.9347186e-01 9.9963570e-01
 4.6519292e-04 8.7696266e-01 8.8306572e-03 3.4019393e-01 9.9925119e-01
 7.5564687e-03 9.3981183e-01 9.9990070e-01 9.9841142e-01 8.9755282e-03
 2.3235541e-02 9.9826729e-01 4.3226394e-01 9.9995017e-01 9.7347564e-01
 3.6367315e-01 9.4837821e-01 9.2176020e-01 9.9979001e-01 9.9810350e-01
 4.3418698e-04 9.2020953e-01 9.8937654e-01 9.9962389e-01 9.9877256e-01
 3.4790100e-05 4.9911731e-01 3.4436139e-03 6.2815365e-05 9.9616230e-01
 2.6099734e-02 9.9931729e-01 9.9857342e-01 9.9963856e-01 1.7722158e-02
 9.9999905e-01 3.9815807e-04 1.5124063e-02 9.9977404e-01 9.9909413e-01
 4.1615684e-02 9.1736221e-01 2.8368674e-02 6.9933933e-01 7.3071980e-01
 9.9995065e-01 1.2323262e-02 9.9983943e-01 9.9097437e-01 2.7227921e-03
 1.0766255e-04 3.3642101e-01 9.9875367e-01 9.9993789e-01 9.6056432e-01
 6.4871967e-01 9.9968493e-01 7.7283390e-02 3.7853780e-01 9.9771619e-01
 5.6070409e-04 1.1966761e-03 2.4580896e-02 9.9948633e-01 9.9845529e-01
 1.8764668e-03 9.9782038e-01 3.6920283e-03 9.9821955e-01 5.3483701e-01
 9.9881685e-01 9.9975854e-01 3.7938911e-01 3.6403358e-01 3.3729589e-01
 1.6336795e-02 4.2357004e-01 2.4043864e-03 9.9414730e-01 9.9992561e-01
 1.7527115e-04 2.6105266e-04 9.5764198e-04 9.0698749e-02 9.9806207e-01
 9.9582279e-01 9.9696583e-01 2.1784921e-01 2.8059793e-01 9.9252993e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 18:49:59, Dev, Step : 5152, Loss : 0.59140, Acc : 0.763, Auc : 0.857, Sensitive_Loss : 0.15392, Sensitive_Acc : 16.250, Sensitive_Auc : 0.992, Mean auc: 0.857, Run Time : 92.94 sec
INFO:root:2024-04-10 18:50:07, Train, Epoch : 9, Step : 5160, Loss : 0.26332, Acc : 0.694, Sensitive_Loss : 0.05319, Sensitive_Acc : 13.200, Run Time : 7.79 sec
INFO:root:2024-04-10 18:50:17, Train, Epoch : 9, Step : 5170, Loss : 0.30639, Acc : 0.859, Sensitive_Loss : 0.11197, Sensitive_Acc : 17.000, Run Time : 9.28 sec
INFO:root:2024-04-10 18:50:28, Train, Epoch : 9, Step : 5180, Loss : 0.23543, Acc : 0.912, Sensitive_Loss : 0.11044, Sensitive_Acc : 15.300, Run Time : 11.19 sec
INFO:root:2024-04-10 18:50:36, Train, Epoch : 9, Step : 5190, Loss : 0.24202, Acc : 0.897, Sensitive_Loss : 0.11170, Sensitive_Acc : 16.600, Run Time : 8.62 sec
INFO:root:2024-04-10 18:50:45, Train, Epoch : 9, Step : 5200, Loss : 0.32395, Acc : 0.875, Sensitive_Loss : 0.07494, Sensitive_Acc : 15.500, Run Time : 8.66 sec
INFO:root:2024-04-10 18:52:18, Dev, Step : 5200, Loss : 0.54019, Acc : 0.776, Auc : 0.857, Sensitive_Loss : 0.12071, Sensitive_Acc : 16.264, Sensitive_Auc : 0.991, Mean auc: 0.857, Run Time : 92.82 sec
INFO:root:2024-04-10 18:52:25, Train, Epoch : 9, Step : 5210, Loss : 0.31440, Acc : 0.866, Sensitive_Loss : 0.10258, Sensitive_Acc : 16.400, Run Time : 99.57 sec
INFO:root:2024-04-10 18:52:34, Train, Epoch : 9, Step : 5220, Loss : 0.25358, Acc : 0.869, Sensitive_Loss : 0.06688, Sensitive_Acc : 16.600, Run Time : 9.53 sec
INFO:root:2024-04-10 18:52:47, Train, Epoch : 9, Step : 5230, Loss : 0.28290, Acc : 0.878, Sensitive_Loss : 0.08125, Sensitive_Acc : 17.000, Run Time : 12.42 sec
INFO:root:2024-04-10 18:52:57, Train, Epoch : 9, Step : 5240, Loss : 0.27398, Acc : 0.881, Sensitive_Loss : 0.10295, Sensitive_Acc : 16.100, Run Time : 10.38 sec
INFO:root:2024-04-10 18:53:05, Train, Epoch : 9, Step : 5250, Loss : 0.22116, Acc : 0.903, Sensitive_Loss : 0.11074, Sensitive_Acc : 16.700, Run Time : 8.37 sec
INFO:root:2024-04-10 18:53:15, Train, Epoch : 9, Step : 5260, Loss : 0.29681, Acc : 0.872, Sensitive_Loss : 0.11137, Sensitive_Acc : 17.000, Run Time : 9.41 sec
INFO:root:2024-04-10 18:53:23, Train, Epoch : 9, Step : 5270, Loss : 0.31130, Acc : 0.881, Sensitive_Loss : 0.07251, Sensitive_Acc : 16.800, Run Time : 8.32 sec
INFO:root:2024-04-10 18:53:31, Train, Epoch : 9, Step : 5280, Loss : 0.28460, Acc : 0.838, Sensitive_Loss : 0.09031, Sensitive_Acc : 15.800, Run Time : 8.37 sec
INFO:root:2024-04-10 18:53:41, Train, Epoch : 9, Step : 5290, Loss : 0.29342, Acc : 0.875, Sensitive_Loss : 0.10259, Sensitive_Acc : 17.400, Run Time : 9.06 sec
INFO:root:2024-04-10 18:53:50, Train, Epoch : 9, Step : 5300, Loss : 0.30121, Acc : 0.878, Sensitive_Loss : 0.08052, Sensitive_Acc : 15.500, Run Time : 9.34 sec
INFO:root:2024-04-10 18:55:24, Dev, Step : 5300, Loss : 0.53984, Acc : 0.777, Auc : 0.858, Sensitive_Loss : 0.12174, Sensitive_Acc : 16.336, Sensitive_Auc : 0.992, Mean auc: 0.858, Run Time : 94.34 sec
INFO:root:2024-04-10 18:55:34, Train, Epoch : 9, Step : 5310, Loss : 0.26722, Acc : 0.872, Sensitive_Loss : 0.09992, Sensitive_Acc : 15.900, Run Time : 103.88 sec
INFO:root:2024-04-10 18:55:45, Train, Epoch : 9, Step : 5320, Loss : 0.27406, Acc : 0.881, Sensitive_Loss : 0.11917, Sensitive_Acc : 15.600, Run Time : 11.13 sec
INFO:root:2024-04-10 18:55:54, Train, Epoch : 9, Step : 5330, Loss : 0.29473, Acc : 0.869, Sensitive_Loss : 0.08160, Sensitive_Acc : 16.500, Run Time : 9.09 sec
INFO:root:2024-04-10 18:56:04, Train, Epoch : 9, Step : 5340, Loss : 0.31534, Acc : 0.866, Sensitive_Loss : 0.10566, Sensitive_Acc : 17.000, Run Time : 10.05 sec
INFO:root:2024-04-10 18:56:14, Train, Epoch : 9, Step : 5350, Loss : 0.26226, Acc : 0.891, Sensitive_Loss : 0.09614, Sensitive_Acc : 15.300, Run Time : 9.83 sec
INFO:root:2024-04-10 18:56:22, Train, Epoch : 9, Step : 5360, Loss : 0.29046, Acc : 0.869, Sensitive_Loss : 0.10290, Sensitive_Acc : 16.100, Run Time : 8.49 sec
INFO:root:2024-04-10 18:56:33, Train, Epoch : 9, Step : 5370, Loss : 0.29149, Acc : 0.869, Sensitive_Loss : 0.11548, Sensitive_Acc : 15.500, Run Time : 11.14 sec
INFO:root:2024-04-10 18:56:42, Train, Epoch : 9, Step : 5380, Loss : 0.28255, Acc : 0.875, Sensitive_Loss : 0.07831, Sensitive_Acc : 15.000, Run Time : 8.33 sec
INFO:root:2024-04-10 18:56:50, Train, Epoch : 9, Step : 5390, Loss : 0.23757, Acc : 0.897, Sensitive_Loss : 0.07377, Sensitive_Acc : 17.400, Run Time : 8.21 sec
INFO:root:2024-04-10 18:57:00, Train, Epoch : 9, Step : 5400, Loss : 0.27335, Acc : 0.884, Sensitive_Loss : 0.10505, Sensitive_Acc : 16.300, Run Time : 9.91 sec
INFO:root:2024-04-10 18:58:44, Dev, Step : 5400, Loss : 0.55255, Acc : 0.776, Auc : 0.859, Sensitive_Loss : 0.17313, Sensitive_Acc : 16.079, Sensitive_Auc : 0.991, Mean auc: 0.859, Run Time : 104.42 sec
INFO:root:2024-04-10 18:58:51, Train, Epoch : 9, Step : 5410, Loss : 0.27707, Acc : 0.891, Sensitive_Loss : 0.09262, Sensitive_Acc : 16.300, Run Time : 110.89 sec
INFO:root:2024-04-10 18:59:03, Train, Epoch : 9, Step : 5420, Loss : 0.32186, Acc : 0.875, Sensitive_Loss : 0.14417, Sensitive_Acc : 16.100, Run Time : 12.07 sec
INFO:root:2024-04-10 18:59:13, Train, Epoch : 9, Step : 5430, Loss : 0.26025, Acc : 0.859, Sensitive_Loss : 0.11160, Sensitive_Acc : 14.800, Run Time : 10.40 sec
INFO:root:2024-04-10 18:59:22, Train, Epoch : 9, Step : 5440, Loss : 0.36539, Acc : 0.850, Sensitive_Loss : 0.13109, Sensitive_Acc : 18.300, Run Time : 8.96 sec
INFO:root:2024-04-10 18:59:32, Train, Epoch : 9, Step : 5450, Loss : 0.25732, Acc : 0.856, Sensitive_Loss : 0.09754, Sensitive_Acc : 16.800, Run Time : 9.52 sec
INFO:root:2024-04-10 18:59:42, Train, Epoch : 9, Step : 5460, Loss : 0.24586, Acc : 0.900, Sensitive_Loss : 0.10401, Sensitive_Acc : 15.200, Run Time : 9.74 sec
INFO:root:2024-04-10 18:59:50, Train, Epoch : 9, Step : 5470, Loss : 0.33826, Acc : 0.847, Sensitive_Loss : 0.10410, Sensitive_Acc : 16.100, Run Time : 8.40 sec
INFO:root:2024-04-10 18:59:58, Train, Epoch : 9, Step : 5480, Loss : 0.26686, Acc : 0.881, Sensitive_Loss : 0.10941, Sensitive_Acc : 15.700, Run Time : 8.12 sec
INFO:root:2024-04-10 19:00:09, Train, Epoch : 9, Step : 5490, Loss : 0.30574, Acc : 0.878, Sensitive_Loss : 0.08467, Sensitive_Acc : 17.000, Run Time : 11.32 sec
INFO:root:2024-04-10 19:00:17, Train, Epoch : 9, Step : 5500, Loss : 0.25204, Acc : 0.887, Sensitive_Loss : 0.08397, Sensitive_Acc : 17.000, Run Time : 8.01 sec
INFO:root:2024-04-10 19:02:03, Dev, Step : 5500, Loss : 0.57258, Acc : 0.773, Auc : 0.856, Sensitive_Loss : 0.13402, Sensitive_Acc : 16.279, Sensitive_Auc : 0.993, Mean auc: 0.856, Run Time : 105.86 sec
INFO:root:2024-04-10 19:02:10, Train, Epoch : 9, Step : 5510, Loss : 0.34638, Acc : 0.831, Sensitive_Loss : 0.10909, Sensitive_Acc : 17.100, Run Time : 112.27 sec
INFO:root:2024-04-10 19:02:22, Train, Epoch : 9, Step : 5520, Loss : 0.32189, Acc : 0.822, Sensitive_Loss : 0.10104, Sensitive_Acc : 16.800, Run Time : 11.89 sec
INFO:root:2024-04-10 19:02:30, Train, Epoch : 9, Step : 5530, Loss : 0.25790, Acc : 0.887, Sensitive_Loss : 0.07758, Sensitive_Acc : 15.400, Run Time : 8.63 sec
INFO:root:2024-04-10 19:02:39, Train, Epoch : 9, Step : 5540, Loss : 0.32411, Acc : 0.866, Sensitive_Loss : 0.10344, Sensitive_Acc : 16.100, Run Time : 8.61 sec
INFO:root:2024-04-10 19:02:52, Train, Epoch : 9, Step : 5550, Loss : 0.27410, Acc : 0.887, Sensitive_Loss : 0.09584, Sensitive_Acc : 15.000, Run Time : 13.00 sec
INFO:root:2024-04-10 19:03:03, Train, Epoch : 9, Step : 5560, Loss : 0.27405, Acc : 0.872, Sensitive_Loss : 0.10200, Sensitive_Acc : 14.900, Run Time : 11.34 sec
INFO:root:2024-04-10 19:03:12, Train, Epoch : 9, Step : 5570, Loss : 0.28609, Acc : 0.869, Sensitive_Loss : 0.11844, Sensitive_Acc : 17.600, Run Time : 9.23 sec
INFO:root:2024-04-10 19:03:27, Train, Epoch : 9, Step : 5580, Loss : 0.29310, Acc : 0.878, Sensitive_Loss : 0.10600, Sensitive_Acc : 16.900, Run Time : 14.51 sec
INFO:root:2024-04-10 19:03:36, Train, Epoch : 9, Step : 5590, Loss : 0.28180, Acc : 0.887, Sensitive_Loss : 0.08262, Sensitive_Acc : 16.200, Run Time : 8.80 sec
INFO:root:2024-04-10 19:03:45, Train, Epoch : 9, Step : 5600, Loss : 0.34063, Acc : 0.866, Sensitive_Loss : 0.12581, Sensitive_Acc : 17.100, Run Time : 8.91 sec
INFO:root:2024-04-10 19:05:27, Dev, Step : 5600, Loss : 0.54555, Acc : 0.779, Auc : 0.859, Sensitive_Loss : 0.14280, Sensitive_Acc : 16.279, Sensitive_Auc : 0.993, Mean auc: 0.859, Run Time : 102.23 sec
INFO:root:2024-04-10 19:05:33, Train, Epoch : 9, Step : 5610, Loss : 0.32451, Acc : 0.859, Sensitive_Loss : 0.05828, Sensitive_Acc : 14.600, Run Time : 108.81 sec
INFO:root:2024-04-10 19:05:48, Train, Epoch : 9, Step : 5620, Loss : 0.33086, Acc : 0.875, Sensitive_Loss : 0.06709, Sensitive_Acc : 16.200, Run Time : 14.57 sec
INFO:root:2024-04-10 19:05:57, Train, Epoch : 9, Step : 5630, Loss : 0.33130, Acc : 0.825, Sensitive_Loss : 0.08006, Sensitive_Acc : 16.800, Run Time : 8.79 sec
INFO:root:2024-04-10 19:06:07, Train, Epoch : 9, Step : 5640, Loss : 0.27274, Acc : 0.881, Sensitive_Loss : 0.08432, Sensitive_Acc : 16.700, Run Time : 10.18 sec
INFO:root:2024-04-10 19:06:17, Train, Epoch : 9, Step : 5650, Loss : 0.26586, Acc : 0.884, Sensitive_Loss : 0.12038, Sensitive_Acc : 17.300, Run Time : 10.31 sec
INFO:root:2024-04-10 19:06:26, Train, Epoch : 9, Step : 5660, Loss : 0.23481, Acc : 0.900, Sensitive_Loss : 0.09621, Sensitive_Acc : 15.900, Run Time : 9.02 sec
INFO:root:2024-04-10 19:06:35, Train, Epoch : 9, Step : 5670, Loss : 0.24759, Acc : 0.916, Sensitive_Loss : 0.10281, Sensitive_Acc : 16.000, Run Time : 8.82 sec
INFO:root:2024-04-10 19:06:44, Train, Epoch : 9, Step : 5680, Loss : 0.34095, Acc : 0.844, Sensitive_Loss : 0.11857, Sensitive_Acc : 16.500, Run Time : 9.22 sec
INFO:root:2024-04-10 19:06:55, Train, Epoch : 9, Step : 5690, Loss : 0.33441, Acc : 0.863, Sensitive_Loss : 0.08755, Sensitive_Acc : 16.900, Run Time : 10.41 sec
INFO:root:2024-04-10 19:07:03, Train, Epoch : 9, Step : 5700, Loss : 0.27229, Acc : 0.878, Sensitive_Loss : 0.07215, Sensitive_Acc : 16.500, Run Time : 8.29 sec
INFO:root:2024-04-10 19:08:41, Dev, Step : 5700, Loss : 0.55939, Acc : 0.775, Auc : 0.857, Sensitive_Loss : 0.15547, Sensitive_Acc : 16.179, Sensitive_Auc : 0.993, Mean auc: 0.857, Run Time : 98.34 sec
INFO:root:2024-04-10 19:08:50, Train, Epoch : 9, Step : 5710, Loss : 0.35309, Acc : 0.863, Sensitive_Loss : 0.06510, Sensitive_Acc : 15.400, Run Time : 107.08 sec
INFO:root:2024-04-10 19:08:59, Train, Epoch : 9, Step : 5720, Loss : 0.33999, Acc : 0.887, Sensitive_Loss : 0.08417, Sensitive_Acc : 15.600, Run Time : 8.68 sec
INFO:root:2024-04-10 19:09:08, Train, Epoch : 9, Step : 5730, Loss : 0.32389, Acc : 0.847, Sensitive_Loss : 0.07599, Sensitive_Acc : 15.900, Run Time : 9.25 sec
INFO:root:2024-04-10 19:09:21, Train, Epoch : 9, Step : 5740, Loss : 0.29576, Acc : 0.878, Sensitive_Loss : 0.11410, Sensitive_Acc : 15.800, Run Time : 12.71 sec
INFO:root:2024-04-10 19:09:30, Train, Epoch : 9, Step : 5750, Loss : 0.32057, Acc : 0.863, Sensitive_Loss : 0.07495, Sensitive_Acc : 15.500, Run Time : 9.56 sec
INFO:root:2024-04-10 19:09:39, Train, Epoch : 9, Step : 5760, Loss : 0.26454, Acc : 0.875, Sensitive_Loss : 0.15958, Sensitive_Acc : 17.100, Run Time : 9.11 sec
INFO:root:2024-04-10 19:09:50, Train, Epoch : 9, Step : 5770, Loss : 0.28234, Acc : 0.875, Sensitive_Loss : 0.05880, Sensitive_Acc : 16.200, Run Time : 10.74 sec
INFO:root:2024-04-10 19:10:00, Train, Epoch : 9, Step : 5780, Loss : 0.29112, Acc : 0.863, Sensitive_Loss : 0.08884, Sensitive_Acc : 17.200, Run Time : 9.60 sec
INFO:root:2024-04-10 19:10:10, Train, Epoch : 9, Step : 5790, Loss : 0.28343, Acc : 0.891, Sensitive_Loss : 0.08769, Sensitive_Acc : 15.800, Run Time : 9.82 sec
INFO:root:2024-04-10 19:12:01
INFO:root:y_pred: [0.03739976 0.0808368  0.50381875 ... 0.6535888  0.20397292 0.24185497]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9837506e-01 2.7471854e-04 9.0378143e-02 9.9984288e-01 9.9798763e-01
 9.9884880e-01 9.9964488e-01 1.9883760e-04 9.8599237e-01 9.9370158e-01
 7.7652141e-02 6.9754040e-01 1.5395601e-05 9.8287106e-01 9.9827862e-01
 9.9965954e-01 9.8678881e-01 9.8894823e-01 9.7545809e-01 9.9940133e-01
 9.9575752e-01 1.0324991e-01 9.9367625e-01 5.5523145e-01 9.3380773e-01
 1.4953079e-03 9.9613720e-01 5.8075517e-02 9.9981886e-01 5.8587166e-03
 1.2641540e-03 1.9358398e-01 4.7100578e-03 9.8058814e-01 1.8859715e-06
 9.9863976e-01 1.7769411e-04 9.9991381e-01 1.1711180e-02 9.9736446e-01
 9.9986303e-01 2.9981893e-03 1.6055461e-02 2.6251957e-05 8.0339499e-02
 7.2461978e-02 9.9974483e-01 9.7366285e-01 9.8334366e-01 9.9940562e-01
 1.2223075e-04 7.8753304e-01 2.5074445e-03 3.2006261e-01 9.9846846e-01
 1.4689309e-03 8.7935144e-01 9.9983180e-01 9.9431783e-01 3.6312754e-03
 5.1089162e-03 9.9687529e-01 3.2174274e-01 9.9989641e-01 9.4545507e-01
 2.3729661e-01 8.9771843e-01 8.6325884e-01 9.9986339e-01 9.9590474e-01
 1.3682978e-04 7.9750293e-01 9.8459446e-01 9.9925166e-01 9.9716800e-01
 9.1421844e-06 2.1927278e-01 2.5638079e-03 2.4567176e-05 9.8876035e-01
 1.0305882e-02 9.9802148e-01 9.9704570e-01 9.9916911e-01 1.3171614e-02
 9.9999845e-01 8.5697095e-05 4.8061726e-03 9.9978179e-01 9.9794477e-01
 5.0153930e-02 8.8288897e-01 7.3564034e-03 3.9760929e-01 4.0808633e-01
 9.9989462e-01 2.4374896e-03 9.9946159e-01 9.8579037e-01 6.5480889e-04
 2.0438865e-05 1.8732324e-01 9.9758351e-01 9.9988735e-01 9.3693244e-01
 4.8511654e-01 9.9948514e-01 2.0036817e-02 2.3770757e-01 9.9641895e-01
 4.9063517e-04 7.2404556e-04 1.0436303e-02 9.9909997e-01 9.9756962e-01
 8.5445668e-04 9.9607372e-01 1.5363782e-03 9.9601525e-01 4.6555138e-01
 9.9737620e-01 9.9976808e-01 2.4774131e-01 3.3357719e-01 1.4180297e-01
 5.0701555e-03 1.7335658e-01 1.6001780e-03 9.9084443e-01 9.9990582e-01
 6.1424667e-05 1.2510682e-04 6.6661782e-04 6.1803494e-02 9.9527138e-01
 9.9449378e-01 9.9166858e-01 1.6461264e-01 9.6936442e-02 9.7424781e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 19:12:01, Dev, Step : 5796, Loss : 0.57361, Acc : 0.772, Auc : 0.858, Sensitive_Loss : 0.11669, Sensitive_Acc : 16.307, Sensitive_Auc : 0.994, Mean auc: 0.858, Run Time : 103.97 sec
INFO:root:2024-04-10 19:12:07, Train, Epoch : 10, Step : 5800, Loss : 0.08556, Acc : 0.372, Sensitive_Loss : 0.05376, Sensitive_Acc : 6.500, Run Time : 4.48 sec
INFO:root:2024-04-10 19:13:47, Dev, Step : 5800, Loss : 0.56598, Acc : 0.773, Auc : 0.858, Sensitive_Loss : 0.11795, Sensitive_Acc : 16.250, Sensitive_Auc : 0.993, Mean auc: 0.858, Run Time : 100.57 sec
INFO:root:2024-04-10 19:13:59, Train, Epoch : 10, Step : 5810, Loss : 0.24649, Acc : 0.894, Sensitive_Loss : 0.07652, Sensitive_Acc : 14.700, Run Time : 111.93 sec
INFO:root:2024-04-10 19:14:08, Train, Epoch : 10, Step : 5820, Loss : 0.26357, Acc : 0.875, Sensitive_Loss : 0.09077, Sensitive_Acc : 16.600, Run Time : 9.41 sec
INFO:root:2024-04-10 19:14:20, Train, Epoch : 10, Step : 5830, Loss : 0.23485, Acc : 0.903, Sensitive_Loss : 0.09511, Sensitive_Acc : 14.900, Run Time : 11.48 sec
INFO:root:2024-04-10 19:14:30, Train, Epoch : 10, Step : 5840, Loss : 0.29854, Acc : 0.878, Sensitive_Loss : 0.07212, Sensitive_Acc : 15.900, Run Time : 10.37 sec
INFO:root:2024-04-10 19:14:39, Train, Epoch : 10, Step : 5850, Loss : 0.21654, Acc : 0.919, Sensitive_Loss : 0.09098, Sensitive_Acc : 17.000, Run Time : 8.90 sec
INFO:root:2024-04-10 19:14:47, Train, Epoch : 10, Step : 5860, Loss : 0.27008, Acc : 0.887, Sensitive_Loss : 0.08819, Sensitive_Acc : 15.500, Run Time : 8.57 sec
INFO:root:2024-04-10 19:14:57, Train, Epoch : 10, Step : 5870, Loss : 0.28523, Acc : 0.906, Sensitive_Loss : 0.10514, Sensitive_Acc : 15.700, Run Time : 9.93 sec
INFO:root:2024-04-10 19:15:06, Train, Epoch : 10, Step : 5880, Loss : 0.30098, Acc : 0.869, Sensitive_Loss : 0.07767, Sensitive_Acc : 16.900, Run Time : 8.49 sec
INFO:root:2024-04-10 19:15:14, Train, Epoch : 10, Step : 5890, Loss : 0.27217, Acc : 0.909, Sensitive_Loss : 0.08421, Sensitive_Acc : 15.800, Run Time : 8.38 sec
INFO:root:2024-04-10 19:15:23, Train, Epoch : 10, Step : 5900, Loss : 0.28966, Acc : 0.891, Sensitive_Loss : 0.06802, Sensitive_Acc : 16.800, Run Time : 8.46 sec
INFO:root:2024-04-10 19:17:06, Dev, Step : 5900, Loss : 0.54865, Acc : 0.772, Auc : 0.856, Sensitive_Loss : 0.12552, Sensitive_Acc : 16.264, Sensitive_Auc : 0.994, Mean auc: 0.856, Run Time : 103.09 sec
INFO:root:2024-04-10 19:17:13, Train, Epoch : 10, Step : 5910, Loss : 0.20292, Acc : 0.894, Sensitive_Loss : 0.10099, Sensitive_Acc : 16.900, Run Time : 110.04 sec
INFO:root:2024-04-10 19:17:25, Train, Epoch : 10, Step : 5920, Loss : 0.26984, Acc : 0.894, Sensitive_Loss : 0.12621, Sensitive_Acc : 16.900, Run Time : 12.60 sec
INFO:root:2024-04-10 19:17:34, Train, Epoch : 10, Step : 5930, Loss : 0.25468, Acc : 0.891, Sensitive_Loss : 0.12928, Sensitive_Acc : 14.500, Run Time : 8.99 sec
INFO:root:2024-04-10 19:17:43, Train, Epoch : 10, Step : 5940, Loss : 0.30076, Acc : 0.844, Sensitive_Loss : 0.10595, Sensitive_Acc : 14.500, Run Time : 9.14 sec
INFO:root:2024-04-10 19:17:55, Train, Epoch : 10, Step : 5950, Loss : 0.30296, Acc : 0.884, Sensitive_Loss : 0.06585, Sensitive_Acc : 17.400, Run Time : 11.21 sec
INFO:root:2024-04-10 19:18:03, Train, Epoch : 10, Step : 5960, Loss : 0.25007, Acc : 0.909, Sensitive_Loss : 0.08460, Sensitive_Acc : 16.600, Run Time : 8.42 sec
INFO:root:2024-04-10 19:18:12, Train, Epoch : 10, Step : 5970, Loss : 0.23799, Acc : 0.863, Sensitive_Loss : 0.08037, Sensitive_Acc : 16.300, Run Time : 8.67 sec
INFO:root:2024-04-10 19:18:23, Train, Epoch : 10, Step : 5980, Loss : 0.25248, Acc : 0.900, Sensitive_Loss : 0.06589, Sensitive_Acc : 16.100, Run Time : 11.69 sec
INFO:root:2024-04-10 19:18:33, Train, Epoch : 10, Step : 5990, Loss : 0.31462, Acc : 0.878, Sensitive_Loss : 0.13189, Sensitive_Acc : 16.100, Run Time : 9.85 sec
INFO:root:2024-04-10 19:18:42, Train, Epoch : 10, Step : 6000, Loss : 0.23210, Acc : 0.897, Sensitive_Loss : 0.13788, Sensitive_Acc : 17.300, Run Time : 9.13 sec
INFO:root:2024-04-10 19:20:30, Dev, Step : 6000, Loss : 0.56716, Acc : 0.776, Auc : 0.857, Sensitive_Loss : 0.13574, Sensitive_Acc : 16.264, Sensitive_Auc : 0.993, Mean auc: 0.857, Run Time : 107.26 sec
INFO:root:2024-04-10 19:20:36, Train, Epoch : 10, Step : 6010, Loss : 0.22766, Acc : 0.887, Sensitive_Loss : 0.14592, Sensitive_Acc : 17.000, Run Time : 113.75 sec
INFO:root:2024-04-10 19:20:48, Train, Epoch : 10, Step : 6020, Loss : 0.30861, Acc : 0.872, Sensitive_Loss : 0.09324, Sensitive_Acc : 14.900, Run Time : 11.89 sec
INFO:root:2024-04-10 19:21:01, Train, Epoch : 10, Step : 6030, Loss : 0.25358, Acc : 0.878, Sensitive_Loss : 0.07865, Sensitive_Acc : 15.900, Run Time : 12.76 sec
INFO:root:2024-04-10 19:21:10, Train, Epoch : 10, Step : 6040, Loss : 0.31445, Acc : 0.869, Sensitive_Loss : 0.07484, Sensitive_Acc : 15.400, Run Time : 9.02 sec
INFO:root:2024-04-10 19:21:22, Train, Epoch : 10, Step : 6050, Loss : 0.26695, Acc : 0.869, Sensitive_Loss : 0.11766, Sensitive_Acc : 15.700, Run Time : 12.32 sec
INFO:root:2024-04-10 19:21:31, Train, Epoch : 10, Step : 6060, Loss : 0.31105, Acc : 0.872, Sensitive_Loss : 0.07776, Sensitive_Acc : 15.600, Run Time : 8.71 sec
INFO:root:2024-04-10 19:21:39, Train, Epoch : 10, Step : 6070, Loss : 0.28278, Acc : 0.856, Sensitive_Loss : 0.08015, Sensitive_Acc : 15.500, Run Time : 7.98 sec
INFO:root:2024-04-10 19:21:50, Train, Epoch : 10, Step : 6080, Loss : 0.26610, Acc : 0.872, Sensitive_Loss : 0.10478, Sensitive_Acc : 17.900, Run Time : 10.69 sec
INFO:root:2024-04-10 19:21:58, Train, Epoch : 10, Step : 6090, Loss : 0.26476, Acc : 0.884, Sensitive_Loss : 0.09965, Sensitive_Acc : 16.600, Run Time : 8.32 sec
INFO:root:2024-04-10 19:22:06, Train, Epoch : 10, Step : 6100, Loss : 0.31011, Acc : 0.872, Sensitive_Loss : 0.12155, Sensitive_Acc : 15.100, Run Time : 8.20 sec
INFO:root:2024-04-10 19:23:55, Dev, Step : 6100, Loss : 0.55846, Acc : 0.773, Auc : 0.857, Sensitive_Loss : 0.14640, Sensitive_Acc : 16.179, Sensitive_Auc : 0.993, Mean auc: 0.857, Run Time : 108.70 sec
INFO:root:2024-04-10 19:24:03, Train, Epoch : 10, Step : 6110, Loss : 0.24394, Acc : 0.906, Sensitive_Loss : 0.12135, Sensitive_Acc : 17.100, Run Time : 116.62 sec
INFO:root:2024-04-10 19:24:12, Train, Epoch : 10, Step : 6120, Loss : 0.23265, Acc : 0.894, Sensitive_Loss : 0.12815, Sensitive_Acc : 16.800, Run Time : 9.24 sec
INFO:root:2024-04-10 19:24:24, Train, Epoch : 10, Step : 6130, Loss : 0.26050, Acc : 0.891, Sensitive_Loss : 0.09389, Sensitive_Acc : 16.600, Run Time : 11.91 sec
INFO:root:2024-04-10 19:24:33, Train, Epoch : 10, Step : 6140, Loss : 0.19076, Acc : 0.912, Sensitive_Loss : 0.07409, Sensitive_Acc : 14.800, Run Time : 9.04 sec
INFO:root:2024-04-10 19:24:43, Train, Epoch : 10, Step : 6150, Loss : 0.26033, Acc : 0.909, Sensitive_Loss : 0.11172, Sensitive_Acc : 16.200, Run Time : 9.87 sec
INFO:root:2024-04-10 19:24:56, Train, Epoch : 10, Step : 6160, Loss : 0.24002, Acc : 0.894, Sensitive_Loss : 0.10555, Sensitive_Acc : 16.400, Run Time : 13.22 sec
INFO:root:2024-04-10 19:25:04, Train, Epoch : 10, Step : 6170, Loss : 0.27097, Acc : 0.887, Sensitive_Loss : 0.17014, Sensitive_Acc : 14.700, Run Time : 8.40 sec
INFO:root:2024-04-10 19:25:13, Train, Epoch : 10, Step : 6180, Loss : 0.33844, Acc : 0.887, Sensitive_Loss : 0.07292, Sensitive_Acc : 17.200, Run Time : 8.57 sec
INFO:root:2024-04-10 19:25:24, Train, Epoch : 10, Step : 6190, Loss : 0.26794, Acc : 0.891, Sensitive_Loss : 0.07317, Sensitive_Acc : 15.500, Run Time : 10.69 sec
INFO:root:2024-04-10 19:25:32, Train, Epoch : 10, Step : 6200, Loss : 0.22503, Acc : 0.900, Sensitive_Loss : 0.11237, Sensitive_Acc : 18.300, Run Time : 8.05 sec
INFO:root:2024-04-10 19:27:15, Dev, Step : 6200, Loss : 0.57206, Acc : 0.773, Auc : 0.854, Sensitive_Loss : 0.15489, Sensitive_Acc : 16.179, Sensitive_Auc : 0.993, Mean auc: 0.854, Run Time : 103.16 sec
INFO:root:2024-04-10 19:27:24, Train, Epoch : 10, Step : 6210, Loss : 0.26949, Acc : 0.875, Sensitive_Loss : 0.10939, Sensitive_Acc : 15.600, Run Time : 112.50 sec
INFO:root:2024-04-10 19:27:33, Train, Epoch : 10, Step : 6220, Loss : 0.28355, Acc : 0.894, Sensitive_Loss : 0.08462, Sensitive_Acc : 17.900, Run Time : 8.56 sec
INFO:root:2024-04-10 19:27:42, Train, Epoch : 10, Step : 6230, Loss : 0.25875, Acc : 0.906, Sensitive_Loss : 0.10412, Sensitive_Acc : 16.600, Run Time : 9.13 sec
INFO:root:2024-04-10 19:27:53, Train, Epoch : 10, Step : 6240, Loss : 0.28348, Acc : 0.872, Sensitive_Loss : 0.09860, Sensitive_Acc : 14.500, Run Time : 11.28 sec
INFO:root:2024-04-10 19:28:02, Train, Epoch : 10, Step : 6250, Loss : 0.25353, Acc : 0.872, Sensitive_Loss : 0.14492, Sensitive_Acc : 15.500, Run Time : 8.47 sec
INFO:root:2024-04-10 19:28:10, Train, Epoch : 10, Step : 6260, Loss : 0.23503, Acc : 0.887, Sensitive_Loss : 0.10001, Sensitive_Acc : 16.400, Run Time : 8.50 sec
INFO:root:2024-04-10 19:28:21, Train, Epoch : 10, Step : 6270, Loss : 0.23589, Acc : 0.897, Sensitive_Loss : 0.12349, Sensitive_Acc : 15.500, Run Time : 10.95 sec
INFO:root:2024-04-10 19:28:30, Train, Epoch : 10, Step : 6280, Loss : 0.27578, Acc : 0.909, Sensitive_Loss : 0.09864, Sensitive_Acc : 16.800, Run Time : 8.56 sec
INFO:root:2024-04-10 19:28:41, Train, Epoch : 10, Step : 6290, Loss : 0.28988, Acc : 0.869, Sensitive_Loss : 0.09732, Sensitive_Acc : 15.600, Run Time : 11.43 sec
INFO:root:2024-04-10 19:28:52, Train, Epoch : 10, Step : 6300, Loss : 0.30509, Acc : 0.850, Sensitive_Loss : 0.09999, Sensitive_Acc : 16.100, Run Time : 10.79 sec
INFO:root:2024-04-10 19:30:35, Dev, Step : 6300, Loss : 0.57770, Acc : 0.772, Auc : 0.855, Sensitive_Loss : 0.13913, Sensitive_Acc : 16.221, Sensitive_Auc : 0.993, Mean auc: 0.855, Run Time : 103.26 sec
INFO:root:2024-04-10 19:30:42, Train, Epoch : 10, Step : 6310, Loss : 0.27289, Acc : 0.900, Sensitive_Loss : 0.07557, Sensitive_Acc : 16.600, Run Time : 109.98 sec
INFO:root:2024-04-10 19:30:54, Train, Epoch : 10, Step : 6320, Loss : 0.32969, Acc : 0.856, Sensitive_Loss : 0.10947, Sensitive_Acc : 16.700, Run Time : 12.41 sec
INFO:root:2024-04-10 19:31:03, Train, Epoch : 10, Step : 6330, Loss : 0.24110, Acc : 0.891, Sensitive_Loss : 0.11967, Sensitive_Acc : 15.900, Run Time : 8.77 sec
INFO:root:2024-04-10 19:31:12, Train, Epoch : 10, Step : 6340, Loss : 0.25515, Acc : 0.884, Sensitive_Loss : 0.10297, Sensitive_Acc : 18.300, Run Time : 8.58 sec
INFO:root:2024-04-10 19:31:23, Train, Epoch : 10, Step : 6350, Loss : 0.24531, Acc : 0.897, Sensitive_Loss : 0.08260, Sensitive_Acc : 16.800, Run Time : 11.36 sec
INFO:root:2024-04-10 19:31:34, Train, Epoch : 10, Step : 6360, Loss : 0.32198, Acc : 0.850, Sensitive_Loss : 0.08813, Sensitive_Acc : 14.200, Run Time : 10.82 sec
INFO:root:2024-04-10 19:31:43, Train, Epoch : 10, Step : 6370, Loss : 0.32641, Acc : 0.853, Sensitive_Loss : 0.13771, Sensitive_Acc : 18.700, Run Time : 9.14 sec
INFO:root:2024-04-10 19:31:54, Train, Epoch : 10, Step : 6380, Loss : 0.25558, Acc : 0.894, Sensitive_Loss : 0.10136, Sensitive_Acc : 16.800, Run Time : 10.84 sec
INFO:root:2024-04-10 19:32:03, Train, Epoch : 10, Step : 6390, Loss : 0.25844, Acc : 0.866, Sensitive_Loss : 0.10006, Sensitive_Acc : 14.700, Run Time : 8.80 sec
INFO:root:2024-04-10 19:32:12, Train, Epoch : 10, Step : 6400, Loss : 0.32714, Acc : 0.878, Sensitive_Loss : 0.10599, Sensitive_Acc : 18.300, Run Time : 9.08 sec
INFO:root:2024-04-10 19:33:59, Dev, Step : 6400, Loss : 0.57444, Acc : 0.769, Auc : 0.852, Sensitive_Loss : 0.12852, Sensitive_Acc : 16.264, Sensitive_Auc : 0.992, Mean auc: 0.852, Run Time : 107.53 sec
INFO:root:2024-04-10 19:34:06, Train, Epoch : 10, Step : 6410, Loss : 0.25965, Acc : 0.891, Sensitive_Loss : 0.08729, Sensitive_Acc : 17.100, Run Time : 114.06 sec
INFO:root:2024-04-10 19:34:15, Train, Epoch : 10, Step : 6420, Loss : 0.26132, Acc : 0.884, Sensitive_Loss : 0.08436, Sensitive_Acc : 15.800, Run Time : 8.84 sec
INFO:root:2024-04-10 19:34:26, Train, Epoch : 10, Step : 6430, Loss : 0.26229, Acc : 0.875, Sensitive_Loss : 0.10702, Sensitive_Acc : 16.700, Run Time : 11.29 sec
INFO:root:2024-04-10 19:34:36, Train, Epoch : 10, Step : 6440, Loss : 0.24972, Acc : 0.891, Sensitive_Loss : 0.07939, Sensitive_Acc : 15.700, Run Time : 10.22 sec
INFO:root:2024-04-10 19:36:16
INFO:root:y_pred: [0.06049588 0.1444441  0.77815497 ... 0.74346095 0.43460205 0.08809488]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.98464823e-01 3.56441480e-04 2.05389962e-01 9.99954700e-01
 9.98936355e-01 9.99584734e-01 9.99868512e-01 4.27113438e-04
 9.88659978e-01 9.92882609e-01 1.95137963e-01 7.41126359e-01
 1.97768750e-05 9.91589010e-01 9.98868644e-01 9.99850392e-01
 9.88186657e-01 9.93061483e-01 9.92462873e-01 9.99523401e-01
 9.97249424e-01 9.20547619e-02 9.96152818e-01 6.72731936e-01
 9.61380124e-01 6.84518786e-03 9.98602211e-01 4.11843881e-02
 9.99858856e-01 1.92943066e-02 2.96249893e-03 2.59842187e-01
 1.59282088e-02 9.90053773e-01 2.42566762e-06 9.98754382e-01
 4.37564158e-04 9.99962449e-01 3.82045321e-02 9.98159230e-01
 9.99874830e-01 4.16664593e-03 5.01347147e-02 5.60869994e-05
 1.77250743e-01 3.18166912e-01 9.99874115e-01 9.89757538e-01
 9.90823388e-01 9.99615431e-01 4.30506421e-04 8.64216328e-01
 5.82870143e-03 4.30151284e-01 9.98510659e-01 6.59542996e-03
 9.70928431e-01 9.99896646e-01 9.97283340e-01 9.19965468e-03
 1.38614718e-02 9.97428954e-01 5.89057565e-01 9.99931097e-01
 9.66506422e-01 5.04588842e-01 8.86310697e-01 9.29975331e-01
 9.99836087e-01 9.97919381e-01 4.37158102e-04 9.63154435e-01
 9.94568825e-01 9.99688864e-01 9.98561561e-01 7.38059025e-05
 4.85149533e-01 4.74278815e-03 8.28436314e-05 9.94561136e-01
 6.39569536e-02 9.98465896e-01 9.98508036e-01 9.99589860e-01
 3.87069918e-02 9.99999046e-01 2.37758562e-04 7.79125188e-03
 9.99796689e-01 9.98610139e-01 1.42540663e-01 8.51185799e-01
 4.35246862e-02 6.17642045e-01 6.93619490e-01 9.99917865e-01
 1.27191329e-02 9.99681234e-01 9.88482893e-01 4.27259551e-03
 5.64986840e-05 4.51838762e-01 9.95967269e-01 9.99962807e-01
 9.52193439e-01 4.86929327e-01 9.99618173e-01 4.70345654e-02
 2.53261775e-01 9.96857882e-01 8.98915285e-04 8.13983846e-04
 1.73534118e-02 9.99058902e-01 9.98205662e-01 1.90295139e-03
 9.97976243e-01 2.91094207e-03 9.98156965e-01 4.28941667e-01
 9.98510182e-01 9.99881983e-01 4.31850404e-01 5.03871679e-01
 1.75520629e-01 5.52226044e-03 3.67296547e-01 5.34625212e-03
 9.91637647e-01 9.99932766e-01 3.72058217e-04 4.21664095e-04
 8.87301750e-04 1.16880685e-01 9.97937202e-01 9.97101724e-01
 9.95421827e-01 2.80240744e-01 2.95361906e-01 9.92291510e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 19:36:16, Dev, Step : 6440, Loss : 0.57988, Acc : 0.770, Auc : 0.848, Sensitive_Loss : 0.14013, Sensitive_Acc : 16.193, Sensitive_Auc : 0.992, Mean auc: 0.848, Run Time : 100.04 sec
