Running on desktop22:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_val.csv",
    "pred_model": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Balanced_Sex_0_01.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.1,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-12 16:10:56, Train, Epoch : 1, Step : 10, Loss : 0.71518, Acc : 0.525, Sensitive_Loss : 1.10731, Sensitive_Acc : 18.700, Run Time : 8.92 sec
INFO:root:2024-04-12 16:11:04, Train, Epoch : 1, Step : 20, Loss : 0.73594, Acc : 0.544, Sensitive_Loss : 0.96869, Sensitive_Acc : 16.400, Run Time : 7.34 sec
INFO:root:2024-04-12 16:11:11, Train, Epoch : 1, Step : 30, Loss : 0.71849, Acc : 0.616, Sensitive_Loss : 0.95863, Sensitive_Acc : 18.000, Run Time : 7.42 sec
INFO:root:2024-04-12 16:11:18, Train, Epoch : 1, Step : 40, Loss : 0.68449, Acc : 0.628, Sensitive_Loss : 0.96534, Sensitive_Acc : 18.900, Run Time : 7.36 sec
INFO:root:2024-04-12 16:11:26, Train, Epoch : 1, Step : 50, Loss : 0.58903, Acc : 0.678, Sensitive_Loss : 0.88411, Sensitive_Acc : 16.200, Run Time : 7.50 sec
INFO:root:2024-04-12 16:11:33, Train, Epoch : 1, Step : 60, Loss : 0.63243, Acc : 0.600, Sensitive_Loss : 0.79823, Sensitive_Acc : 14.500, Run Time : 7.06 sec
INFO:root:2024-04-12 16:11:40, Train, Epoch : 1, Step : 70, Loss : 0.65755, Acc : 0.613, Sensitive_Loss : 0.81286, Sensitive_Acc : 17.200, Run Time : 7.21 sec
INFO:root:2024-04-12 16:11:47, Train, Epoch : 1, Step : 80, Loss : 0.59051, Acc : 0.694, Sensitive_Loss : 0.74669, Sensitive_Acc : 14.000, Run Time : 7.32 sec
INFO:root:2024-04-12 16:11:55, Train, Epoch : 1, Step : 90, Loss : 0.66126, Acc : 0.641, Sensitive_Loss : 0.67864, Sensitive_Acc : 19.900, Run Time : 7.21 sec
INFO:root:2024-04-12 16:12:02, Train, Epoch : 1, Step : 100, Loss : 0.75806, Acc : 0.625, Sensitive_Loss : 0.65016, Sensitive_Acc : 19.500, Run Time : 7.43 sec
INFO:root:2024-04-12 16:13:38, Dev, Step : 100, Loss : 0.76670, Acc : 0.598, Auc : 0.742, Sensitive_Loss : 0.64684, Sensitive_Acc : 16.895, Sensitive_Auc : 0.912, Mean auc: 0.742, Run Time : 96.24 sec
INFO:root:2024-04-12 16:13:39, Best, Step : 100, Loss : 0.76670, Acc : 0.598, Auc : 0.742, Sensitive_Loss : 0.64684, Sensitive_Acc : 16.895, Sensitive_Auc : 0.912, Best Auc : 0.742
INFO:root:2024-04-12 16:13:45, Train, Epoch : 1, Step : 110, Loss : 0.68110, Acc : 0.681, Sensitive_Loss : 0.63130, Sensitive_Acc : 17.900, Run Time : 102.91 sec
INFO:root:2024-04-12 16:13:52, Train, Epoch : 1, Step : 120, Loss : 0.58202, Acc : 0.681, Sensitive_Loss : 0.65961, Sensitive_Acc : 19.500, Run Time : 7.11 sec
INFO:root:2024-04-12 16:13:59, Train, Epoch : 1, Step : 130, Loss : 0.64994, Acc : 0.641, Sensitive_Loss : 0.53797, Sensitive_Acc : 21.500, Run Time : 6.71 sec
INFO:root:2024-04-12 16:14:06, Train, Epoch : 1, Step : 140, Loss : 0.64975, Acc : 0.669, Sensitive_Loss : 0.63025, Sensitive_Acc : 20.800, Run Time : 7.65 sec
INFO:root:2024-04-12 16:14:14, Train, Epoch : 1, Step : 150, Loss : 0.61919, Acc : 0.650, Sensitive_Loss : 0.60153, Sensitive_Acc : 23.200, Run Time : 7.43 sec
INFO:root:2024-04-12 16:14:21, Train, Epoch : 1, Step : 160, Loss : 0.62668, Acc : 0.694, Sensitive_Loss : 0.53241, Sensitive_Acc : 18.300, Run Time : 7.22 sec
INFO:root:2024-04-12 16:14:28, Train, Epoch : 1, Step : 170, Loss : 0.57792, Acc : 0.694, Sensitive_Loss : 0.48176, Sensitive_Acc : 18.700, Run Time : 7.07 sec
INFO:root:2024-04-12 16:14:36, Train, Epoch : 1, Step : 180, Loss : 0.62352, Acc : 0.703, Sensitive_Loss : 0.52865, Sensitive_Acc : 18.500, Run Time : 7.47 sec
INFO:root:2024-04-12 16:14:42, Train, Epoch : 1, Step : 190, Loss : 0.56047, Acc : 0.725, Sensitive_Loss : 0.45188, Sensitive_Acc : 21.900, Run Time : 6.72 sec
INFO:root:2024-04-12 16:14:50, Train, Epoch : 1, Step : 200, Loss : 0.64303, Acc : 0.688, Sensitive_Loss : 0.49333, Sensitive_Acc : 19.100, Run Time : 7.29 sec
INFO:root:2024-04-12 16:16:17, Dev, Step : 200, Loss : 0.66958, Acc : 0.674, Auc : 0.781, Sensitive_Loss : 0.49957, Sensitive_Acc : 19.030, Sensitive_Auc : 0.947, Mean auc: 0.781, Run Time : 87.71 sec
INFO:root:2024-04-12 16:16:18, Best, Step : 200, Loss : 0.66958, Acc : 0.674, Auc : 0.781, Sensitive_Loss : 0.49957, Sensitive_Acc : 19.030, Sensitive_Auc : 0.947, Best Auc : 0.781
INFO:root:2024-04-12 16:16:24, Train, Epoch : 1, Step : 210, Loss : 0.60337, Acc : 0.644, Sensitive_Loss : 0.39833, Sensitive_Acc : 17.400, Run Time : 94.02 sec
INFO:root:2024-04-12 16:16:31, Train, Epoch : 1, Step : 220, Loss : 0.60017, Acc : 0.672, Sensitive_Loss : 0.44416, Sensitive_Acc : 19.200, Run Time : 7.47 sec
INFO:root:2024-04-12 16:16:38, Train, Epoch : 1, Step : 230, Loss : 0.54969, Acc : 0.734, Sensitive_Loss : 0.44311, Sensitive_Acc : 18.200, Run Time : 6.79 sec
INFO:root:2024-04-12 16:16:45, Train, Epoch : 1, Step : 240, Loss : 0.58513, Acc : 0.728, Sensitive_Loss : 0.51333, Sensitive_Acc : 24.900, Run Time : 7.18 sec
INFO:root:2024-04-12 16:16:53, Train, Epoch : 1, Step : 250, Loss : 0.56676, Acc : 0.669, Sensitive_Loss : 0.40697, Sensitive_Acc : 23.900, Run Time : 7.75 sec
INFO:root:2024-04-12 16:17:00, Train, Epoch : 1, Step : 260, Loss : 0.65545, Acc : 0.653, Sensitive_Loss : 0.42198, Sensitive_Acc : 20.500, Run Time : 7.30 sec
INFO:root:2024-04-12 16:17:07, Train, Epoch : 1, Step : 270, Loss : 0.60767, Acc : 0.681, Sensitive_Loss : 0.42619, Sensitive_Acc : 25.100, Run Time : 6.52 sec
INFO:root:2024-04-12 16:17:14, Train, Epoch : 1, Step : 280, Loss : 0.62054, Acc : 0.688, Sensitive_Loss : 0.41432, Sensitive_Acc : 20.000, Run Time : 7.60 sec
INFO:root:2024-04-12 16:17:21, Train, Epoch : 1, Step : 290, Loss : 0.61403, Acc : 0.738, Sensitive_Loss : 0.48414, Sensitive_Acc : 23.200, Run Time : 6.81 sec
INFO:root:2024-04-12 16:17:29, Train, Epoch : 1, Step : 300, Loss : 0.55519, Acc : 0.700, Sensitive_Loss : 0.39818, Sensitive_Acc : 21.400, Run Time : 7.51 sec
INFO:root:2024-04-12 16:18:59, Dev, Step : 300, Loss : 0.60093, Acc : 0.712, Auc : 0.780, Sensitive_Loss : 0.44183, Sensitive_Acc : 20.729, Sensitive_Auc : 0.960, Mean auc: 0.780, Run Time : 90.03 sec
INFO:root:2024-04-12 16:19:04, Train, Epoch : 1, Step : 310, Loss : 0.55015, Acc : 0.725, Sensitive_Loss : 0.34912, Sensitive_Acc : 24.400, Run Time : 95.50 sec
INFO:root:2024-04-12 16:19:11, Train, Epoch : 1, Step : 320, Loss : 0.56754, Acc : 0.709, Sensitive_Loss : 0.45024, Sensitive_Acc : 22.800, Run Time : 7.15 sec
INFO:root:2024-04-12 16:19:19, Train, Epoch : 1, Step : 330, Loss : 0.58156, Acc : 0.700, Sensitive_Loss : 0.45373, Sensitive_Acc : 23.400, Run Time : 7.31 sec
INFO:root:2024-04-12 16:19:26, Train, Epoch : 1, Step : 340, Loss : 0.57475, Acc : 0.700, Sensitive_Loss : 0.42330, Sensitive_Acc : 18.200, Run Time : 7.09 sec
INFO:root:2024-04-12 16:19:33, Train, Epoch : 1, Step : 350, Loss : 0.53709, Acc : 0.719, Sensitive_Loss : 0.36388, Sensitive_Acc : 21.300, Run Time : 6.94 sec
INFO:root:2024-04-12 16:19:40, Train, Epoch : 1, Step : 360, Loss : 0.62131, Acc : 0.672, Sensitive_Loss : 0.35216, Sensitive_Acc : 18.200, Run Time : 7.56 sec
INFO:root:2024-04-12 16:19:47, Train, Epoch : 1, Step : 370, Loss : 0.58429, Acc : 0.734, Sensitive_Loss : 0.31920, Sensitive_Acc : 20.200, Run Time : 6.93 sec
INFO:root:2024-04-12 16:19:55, Train, Epoch : 1, Step : 380, Loss : 0.53931, Acc : 0.722, Sensitive_Loss : 0.30343, Sensitive_Acc : 19.500, Run Time : 7.51 sec
INFO:root:2024-04-12 16:20:02, Train, Epoch : 1, Step : 390, Loss : 0.64000, Acc : 0.666, Sensitive_Loss : 0.36057, Sensitive_Acc : 22.200, Run Time : 7.09 sec
INFO:root:2024-04-12 16:20:09, Train, Epoch : 1, Step : 400, Loss : 0.55262, Acc : 0.738, Sensitive_Loss : 0.35337, Sensitive_Acc : 20.800, Run Time : 7.60 sec
INFO:root:2024-04-12 16:21:38, Dev, Step : 400, Loss : 0.58201, Acc : 0.728, Auc : 0.798, Sensitive_Loss : 0.42303, Sensitive_Acc : 19.316, Sensitive_Auc : 0.980, Mean auc: 0.798, Run Time : 88.63 sec
INFO:root:2024-04-12 16:21:39, Best, Step : 400, Loss : 0.58201, Acc : 0.728, Auc : 0.798, Sensitive_Loss : 0.42303, Sensitive_Acc : 19.316, Sensitive_Auc : 0.980, Best Auc : 0.798
INFO:root:2024-04-12 16:21:44, Train, Epoch : 1, Step : 410, Loss : 0.52242, Acc : 0.731, Sensitive_Loss : 0.47254, Sensitive_Acc : 24.500, Run Time : 94.78 sec
INFO:root:2024-04-12 16:21:51, Train, Epoch : 1, Step : 420, Loss : 0.62401, Acc : 0.703, Sensitive_Loss : 0.32809, Sensitive_Acc : 19.800, Run Time : 7.09 sec
INFO:root:2024-04-12 16:21:59, Train, Epoch : 1, Step : 430, Loss : 0.57013, Acc : 0.738, Sensitive_Loss : 0.25505, Sensitive_Acc : 19.700, Run Time : 7.52 sec
INFO:root:2024-04-12 16:22:06, Train, Epoch : 1, Step : 440, Loss : 0.48945, Acc : 0.738, Sensitive_Loss : 0.27614, Sensitive_Acc : 23.700, Run Time : 7.47 sec
INFO:root:2024-04-12 16:22:14, Train, Epoch : 1, Step : 450, Loss : 0.62359, Acc : 0.738, Sensitive_Loss : 0.36847, Sensitive_Acc : 16.800, Run Time : 7.44 sec
INFO:root:2024-04-12 16:22:20, Train, Epoch : 1, Step : 460, Loss : 0.54344, Acc : 0.738, Sensitive_Loss : 0.28589, Sensitive_Acc : 22.700, Run Time : 6.30 sec
INFO:root:2024-04-12 16:22:27, Train, Epoch : 1, Step : 470, Loss : 0.58577, Acc : 0.697, Sensitive_Loss : 0.31060, Sensitive_Acc : 22.100, Run Time : 7.14 sec
INFO:root:2024-04-12 16:22:34, Train, Epoch : 1, Step : 480, Loss : 0.62948, Acc : 0.716, Sensitive_Loss : 0.30234, Sensitive_Acc : 19.400, Run Time : 7.24 sec
INFO:root:2024-04-12 16:22:41, Train, Epoch : 1, Step : 490, Loss : 0.59406, Acc : 0.672, Sensitive_Loss : 0.29903, Sensitive_Acc : 24.500, Run Time : 7.21 sec
INFO:root:2024-04-12 16:22:49, Train, Epoch : 1, Step : 500, Loss : 0.60185, Acc : 0.716, Sensitive_Loss : 0.27568, Sensitive_Acc : 17.900, Run Time : 7.37 sec
INFO:root:2024-04-12 16:24:17, Dev, Step : 500, Loss : 0.56731, Acc : 0.733, Auc : 0.808, Sensitive_Loss : 0.26620, Sensitive_Acc : 21.677, Sensitive_Auc : 0.989, Mean auc: 0.808, Run Time : 88.13 sec
INFO:root:2024-04-12 16:24:18, Best, Step : 500, Loss : 0.56731, Acc : 0.733, Auc : 0.808, Sensitive_Loss : 0.26620, Sensitive_Acc : 21.677, Sensitive_Auc : 0.989, Best Auc : 0.808
INFO:root:2024-04-12 16:24:23, Train, Epoch : 1, Step : 510, Loss : 0.61707, Acc : 0.641, Sensitive_Loss : 0.30130, Sensitive_Acc : 18.400, Run Time : 94.57 sec
INFO:root:2024-04-12 16:24:31, Train, Epoch : 1, Step : 520, Loss : 0.57168, Acc : 0.738, Sensitive_Loss : 0.34843, Sensitive_Acc : 18.300, Run Time : 7.19 sec
INFO:root:2024-04-12 16:24:38, Train, Epoch : 1, Step : 530, Loss : 0.67056, Acc : 0.700, Sensitive_Loss : 0.39040, Sensitive_Acc : 17.300, Run Time : 6.99 sec
INFO:root:2024-04-12 16:24:45, Train, Epoch : 1, Step : 540, Loss : 0.60077, Acc : 0.744, Sensitive_Loss : 0.30085, Sensitive_Acc : 19.000, Run Time : 7.84 sec
INFO:root:2024-04-12 16:24:53, Train, Epoch : 1, Step : 550, Loss : 0.58302, Acc : 0.697, Sensitive_Loss : 0.27415, Sensitive_Acc : 19.100, Run Time : 7.45 sec
INFO:root:2024-04-12 16:25:00, Train, Epoch : 1, Step : 560, Loss : 0.60196, Acc : 0.722, Sensitive_Loss : 0.36115, Sensitive_Acc : 21.300, Run Time : 6.68 sec
INFO:root:2024-04-12 16:25:06, Train, Epoch : 1, Step : 570, Loss : 0.48061, Acc : 0.728, Sensitive_Loss : 0.37237, Sensitive_Acc : 21.600, Run Time : 6.90 sec
INFO:root:2024-04-12 16:25:14, Train, Epoch : 1, Step : 580, Loss : 0.54656, Acc : 0.738, Sensitive_Loss : 0.33367, Sensitive_Acc : 17.300, Run Time : 7.30 sec
INFO:root:2024-04-12 16:25:21, Train, Epoch : 1, Step : 590, Loss : 0.59181, Acc : 0.734, Sensitive_Loss : 0.32215, Sensitive_Acc : 20.100, Run Time : 7.08 sec
INFO:root:2024-04-12 16:25:28, Train, Epoch : 1, Step : 600, Loss : 0.61813, Acc : 0.709, Sensitive_Loss : 0.27336, Sensitive_Acc : 21.400, Run Time : 7.05 sec
INFO:root:2024-04-12 16:26:57, Dev, Step : 600, Loss : 0.56870, Acc : 0.741, Auc : 0.807, Sensitive_Loss : 0.24660, Sensitive_Acc : 21.346, Sensitive_Auc : 0.991, Mean auc: 0.807, Run Time : 88.61 sec
INFO:root:2024-04-12 16:27:02, Train, Epoch : 1, Step : 610, Loss : 0.53887, Acc : 0.684, Sensitive_Loss : 0.21302, Sensitive_Acc : 21.900, Run Time : 94.12 sec
INFO:root:2024-04-12 16:27:09, Train, Epoch : 1, Step : 620, Loss : 0.53370, Acc : 0.719, Sensitive_Loss : 0.35617, Sensitive_Acc : 22.000, Run Time : 7.22 sec
INFO:root:2024-04-12 16:27:16, Train, Epoch : 1, Step : 630, Loss : 0.58162, Acc : 0.753, Sensitive_Loss : 0.28171, Sensitive_Acc : 22.200, Run Time : 6.96 sec
INFO:root:2024-04-12 16:28:45
INFO:root:y_pred: [0.32329392 0.17435211 0.5913848  ... 0.52303755 0.48579144 0.40653235]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [5.51024936e-02 5.35098324e-03 5.45983016e-02 3.84442300e-01
 4.91039753e-02 4.67738323e-02 1.33724779e-01 9.67078805e-02
 3.88472706e-01 9.99582350e-01 3.87616396e-01 1.67459194e-02
 2.03292906e-01 8.86638809e-05 9.99695778e-01 1.03356898e-01
 6.06423430e-02 9.85026538e-01 9.44396973e-01 6.29956275e-02
 9.66975927e-01 2.05587354e-02 1.86757892e-02 7.88042322e-02
 1.66930348e-01 3.17965806e-01 7.05023622e-03 6.32367507e-02
 3.84864165e-03 9.29385889e-03 2.68647134e-01 9.71561551e-01
 8.98221582e-02 9.82799351e-01 4.16075550e-02 6.40869886e-03
 5.79900155e-03 3.46008658e-01 5.36594093e-02 3.21985148e-02
 3.34401190e-01 9.80458498e-01 7.99255222e-02 7.20501458e-03
 9.94840682e-01 7.69715786e-01 9.02959168e-01 7.69317329e-01
 2.66618341e-01 9.98400152e-01 9.47106957e-01 9.89403486e-01
 9.90878880e-01 3.77086215e-02 1.12698413e-01 8.98282349e-01
 2.69611180e-02 6.60975650e-02 9.91384387e-01 4.48258407e-03
 1.89508982e-02 3.10009383e-02 2.68778410e-02 3.43594793e-03
 9.93247747e-01 9.35759544e-02 1.54451860e-04 5.67414224e-01
 1.06246611e-02 9.07763481e-01 9.99438107e-01 9.96020854e-01
 3.18642496e-03 6.80886328e-01 2.03082506e-02 9.46539402e-01
 1.07987821e-01 8.22132046e-04 2.76177051e-03 1.69491135e-02
 8.71663243e-02 2.56469590e-03 9.96909916e-01 9.87471998e-01
 1.20745627e-02 2.01558411e-01 1.84876382e-01 5.30063221e-03
 8.35570544e-02 3.71980830e-03 1.18807815e-02 9.07629907e-01
 2.33901828e-03 2.32234271e-03 1.87331349e-01 3.57179693e-03
 6.46857661e-04 8.59827697e-01 6.10109530e-02 3.82951647e-02
 1.02254692e-02 3.83980453e-01 1.88923120e-01 3.85666937e-02
 3.20528671e-02 6.30211597e-03 2.26749122e-01 5.09106100e-01
 5.82585633e-01 8.90167594e-01 2.06546494e-04 9.97309208e-01
 9.99491334e-01 1.32811881e-04 6.03455365e-01 5.51439285e-01
 8.32979605e-02 3.32713313e-02 1.41125351e-01 1.17366388e-02
 9.46890470e-03 1.11147331e-03 2.73473151e-02 7.43051816e-04
 1.34400057e-03 9.48308051e-01 2.12048762e-03 9.95623052e-01
 1.10681094e-02 4.88122046e-01 4.19298038e-02 1.79876402e-01
 1.03969232e-03]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 16:28:45, Dev, Step : 634, Loss : 0.64025, Acc : 0.670, Auc : 0.805, Sensitive_Loss : 0.33625, Sensitive_Acc : 20.263, Sensitive_Auc : 0.996, Mean auc: 0.805, Run Time : 86.54 sec
INFO:root:2024-04-12 16:28:52, Train, Epoch : 2, Step : 640, Loss : 0.36380, Acc : 0.416, Sensitive_Loss : 0.14708, Sensitive_Acc : 15.200, Run Time : 5.56 sec
INFO:root:2024-04-12 16:28:58, Train, Epoch : 2, Step : 650, Loss : 0.58500, Acc : 0.706, Sensitive_Loss : 0.29045, Sensitive_Acc : 19.600, Run Time : 6.60 sec
INFO:root:2024-04-12 16:29:05, Train, Epoch : 2, Step : 660, Loss : 0.58945, Acc : 0.738, Sensitive_Loss : 0.27085, Sensitive_Acc : 22.900, Run Time : 7.34 sec
INFO:root:2024-04-12 16:29:13, Train, Epoch : 2, Step : 670, Loss : 0.55490, Acc : 0.731, Sensitive_Loss : 0.25774, Sensitive_Acc : 19.800, Run Time : 7.44 sec
INFO:root:2024-04-12 16:29:20, Train, Epoch : 2, Step : 680, Loss : 0.43127, Acc : 0.791, Sensitive_Loss : 0.21577, Sensitive_Acc : 23.800, Run Time : 7.13 sec
INFO:root:2024-04-12 16:29:27, Train, Epoch : 2, Step : 690, Loss : 0.51452, Acc : 0.744, Sensitive_Loss : 0.28273, Sensitive_Acc : 24.700, Run Time : 6.80 sec
INFO:root:2024-04-12 16:29:34, Train, Epoch : 2, Step : 700, Loss : 0.59718, Acc : 0.734, Sensitive_Loss : 0.23526, Sensitive_Acc : 21.400, Run Time : 7.31 sec
INFO:root:2024-04-12 16:31:02, Dev, Step : 700, Loss : 0.59202, Acc : 0.722, Auc : 0.816, Sensitive_Loss : 0.26821, Sensitive_Acc : 21.301, Sensitive_Auc : 0.989, Mean auc: 0.816, Run Time : 88.23 sec
INFO:root:2024-04-12 16:31:03, Best, Step : 700, Loss : 0.59202, Acc : 0.722, Auc : 0.816, Sensitive_Loss : 0.26821, Sensitive_Acc : 21.301, Sensitive_Auc : 0.989, Best Auc : 0.816
INFO:root:2024-04-12 16:31:08, Train, Epoch : 2, Step : 710, Loss : 0.48790, Acc : 0.759, Sensitive_Loss : 0.22168, Sensitive_Acc : 23.500, Run Time : 94.20 sec
INFO:root:2024-04-12 16:31:15, Train, Epoch : 2, Step : 720, Loss : 0.52379, Acc : 0.756, Sensitive_Loss : 0.22214, Sensitive_Acc : 24.000, Run Time : 7.11 sec
INFO:root:2024-04-12 16:31:23, Train, Epoch : 2, Step : 730, Loss : 0.49185, Acc : 0.731, Sensitive_Loss : 0.18937, Sensitive_Acc : 19.400, Run Time : 7.62 sec
INFO:root:2024-04-12 16:31:30, Train, Epoch : 2, Step : 740, Loss : 0.51324, Acc : 0.753, Sensitive_Loss : 0.26133, Sensitive_Acc : 21.300, Run Time : 7.21 sec
INFO:root:2024-04-12 16:31:37, Train, Epoch : 2, Step : 750, Loss : 0.49000, Acc : 0.766, Sensitive_Loss : 0.27940, Sensitive_Acc : 27.100, Run Time : 6.98 sec
INFO:root:2024-04-12 16:31:44, Train, Epoch : 2, Step : 760, Loss : 0.55233, Acc : 0.753, Sensitive_Loss : 0.27313, Sensitive_Acc : 22.800, Run Time : 6.99 sec
INFO:root:2024-04-12 16:31:51, Train, Epoch : 2, Step : 770, Loss : 0.53045, Acc : 0.750, Sensitive_Loss : 0.43413, Sensitive_Acc : 16.300, Run Time : 7.06 sec
INFO:root:2024-04-12 16:31:58, Train, Epoch : 2, Step : 780, Loss : 0.61343, Acc : 0.731, Sensitive_Loss : 0.19678, Sensitive_Acc : 17.300, Run Time : 6.74 sec
INFO:root:2024-04-12 16:32:05, Train, Epoch : 2, Step : 790, Loss : 0.54838, Acc : 0.728, Sensitive_Loss : 0.28549, Sensitive_Acc : 24.900, Run Time : 7.33 sec
INFO:root:2024-04-12 16:32:13, Train, Epoch : 2, Step : 800, Loss : 0.57142, Acc : 0.719, Sensitive_Loss : 0.28486, Sensitive_Acc : 19.200, Run Time : 7.61 sec
INFO:root:2024-04-12 16:33:41, Dev, Step : 800, Loss : 0.61521, Acc : 0.716, Auc : 0.805, Sensitive_Loss : 0.23817, Sensitive_Acc : 21.571, Sensitive_Auc : 0.991, Mean auc: 0.805, Run Time : 87.66 sec
INFO:root:2024-04-12 16:33:46, Train, Epoch : 2, Step : 810, Loss : 0.54222, Acc : 0.769, Sensitive_Loss : 0.19972, Sensitive_Acc : 19.600, Run Time : 92.83 sec
INFO:root:2024-04-12 16:33:53, Train, Epoch : 2, Step : 820, Loss : 0.55975, Acc : 0.769, Sensitive_Loss : 0.27123, Sensitive_Acc : 21.800, Run Time : 7.24 sec
INFO:root:2024-04-12 16:34:00, Train, Epoch : 2, Step : 830, Loss : 0.47837, Acc : 0.775, Sensitive_Loss : 0.20854, Sensitive_Acc : 25.400, Run Time : 6.93 sec
INFO:root:2024-04-12 16:34:07, Train, Epoch : 2, Step : 840, Loss : 0.55797, Acc : 0.747, Sensitive_Loss : 0.19840, Sensitive_Acc : 19.900, Run Time : 7.11 sec
INFO:root:2024-04-12 16:34:14, Train, Epoch : 2, Step : 850, Loss : 0.55027, Acc : 0.738, Sensitive_Loss : 0.31728, Sensitive_Acc : 23.800, Run Time : 7.20 sec
INFO:root:2024-04-12 16:34:22, Train, Epoch : 2, Step : 860, Loss : 0.47195, Acc : 0.772, Sensitive_Loss : 0.28714, Sensitive_Acc : 20.400, Run Time : 7.42 sec
INFO:root:2024-04-12 16:34:29, Train, Epoch : 2, Step : 870, Loss : 0.49511, Acc : 0.750, Sensitive_Loss : 0.24730, Sensitive_Acc : 22.000, Run Time : 7.35 sec
INFO:root:2024-04-12 16:34:35, Train, Epoch : 2, Step : 880, Loss : 0.54812, Acc : 0.738, Sensitive_Loss : 0.23642, Sensitive_Acc : 21.700, Run Time : 6.34 sec
INFO:root:2024-04-12 16:34:43, Train, Epoch : 2, Step : 890, Loss : 0.51643, Acc : 0.756, Sensitive_Loss : 0.23677, Sensitive_Acc : 17.600, Run Time : 7.49 sec
INFO:root:2024-04-12 16:34:50, Train, Epoch : 2, Step : 900, Loss : 0.55686, Acc : 0.709, Sensitive_Loss : 0.18262, Sensitive_Acc : 24.700, Run Time : 7.57 sec
INFO:root:2024-04-12 16:36:18, Dev, Step : 900, Loss : 0.61188, Acc : 0.726, Auc : 0.817, Sensitive_Loss : 0.22524, Sensitive_Acc : 21.887, Sensitive_Auc : 0.996, Mean auc: 0.817, Run Time : 87.61 sec
INFO:root:2024-04-12 16:36:19, Best, Step : 900, Loss : 0.61188, Acc : 0.726, Auc : 0.817, Sensitive_Loss : 0.22524, Sensitive_Acc : 21.887, Sensitive_Auc : 0.996, Best Auc : 0.817
INFO:root:2024-04-12 16:36:24, Train, Epoch : 2, Step : 910, Loss : 0.52850, Acc : 0.744, Sensitive_Loss : 0.20212, Sensitive_Acc : 19.600, Run Time : 93.88 sec
INFO:root:2024-04-12 16:36:31, Train, Epoch : 2, Step : 920, Loss : 0.51818, Acc : 0.744, Sensitive_Loss : 0.29192, Sensitive_Acc : 22.200, Run Time : 6.84 sec
INFO:root:2024-04-12 16:36:39, Train, Epoch : 2, Step : 930, Loss : 0.50394, Acc : 0.753, Sensitive_Loss : 0.24499, Sensitive_Acc : 19.000, Run Time : 7.57 sec
INFO:root:2024-04-12 16:36:46, Train, Epoch : 2, Step : 940, Loss : 0.57042, Acc : 0.738, Sensitive_Loss : 0.17417, Sensitive_Acc : 20.400, Run Time : 7.55 sec
INFO:root:2024-04-12 16:36:53, Train, Epoch : 2, Step : 950, Loss : 0.66606, Acc : 0.694, Sensitive_Loss : 0.21627, Sensitive_Acc : 23.600, Run Time : 6.94 sec
INFO:root:2024-04-12 16:37:00, Train, Epoch : 2, Step : 960, Loss : 0.56569, Acc : 0.756, Sensitive_Loss : 0.26463, Sensitive_Acc : 23.500, Run Time : 7.10 sec
INFO:root:2024-04-12 16:37:07, Train, Epoch : 2, Step : 970, Loss : 0.57165, Acc : 0.728, Sensitive_Loss : 0.21512, Sensitive_Acc : 24.300, Run Time : 7.04 sec
INFO:root:2024-04-12 16:37:15, Train, Epoch : 2, Step : 980, Loss : 0.52798, Acc : 0.719, Sensitive_Loss : 0.22543, Sensitive_Acc : 24.100, Run Time : 7.32 sec
INFO:root:2024-04-12 16:37:21, Train, Epoch : 2, Step : 990, Loss : 0.50528, Acc : 0.753, Sensitive_Loss : 0.20918, Sensitive_Acc : 17.900, Run Time : 6.62 sec
INFO:root:2024-04-12 16:37:28, Train, Epoch : 2, Step : 1000, Loss : 0.54938, Acc : 0.731, Sensitive_Loss : 0.35226, Sensitive_Acc : 17.900, Run Time : 7.08 sec
INFO:root:2024-04-12 16:38:56, Dev, Step : 1000, Loss : 0.54830, Acc : 0.751, Auc : 0.824, Sensitive_Loss : 0.26234, Sensitive_Acc : 21.030, Sensitive_Auc : 0.983, Mean auc: 0.824, Run Time : 87.75 sec
INFO:root:2024-04-12 16:38:57, Best, Step : 1000, Loss : 0.54830, Acc : 0.751, Auc : 0.824, Sensitive_Loss : 0.26234, Sensitive_Acc : 21.030, Sensitive_Auc : 0.983, Best Auc : 0.824
INFO:root:2024-04-12 16:39:02, Train, Epoch : 2, Step : 1010, Loss : 0.61824, Acc : 0.738, Sensitive_Loss : 0.23347, Sensitive_Acc : 20.000, Run Time : 93.69 sec
INFO:root:2024-04-12 16:39:09, Train, Epoch : 2, Step : 1020, Loss : 0.48264, Acc : 0.738, Sensitive_Loss : 0.19786, Sensitive_Acc : 20.000, Run Time : 7.24 sec
INFO:root:2024-04-12 16:39:17, Train, Epoch : 2, Step : 1030, Loss : 0.49700, Acc : 0.784, Sensitive_Loss : 0.21959, Sensitive_Acc : 17.300, Run Time : 7.51 sec
INFO:root:2024-04-12 16:39:24, Train, Epoch : 2, Step : 1040, Loss : 0.55133, Acc : 0.719, Sensitive_Loss : 0.18897, Sensitive_Acc : 16.400, Run Time : 7.35 sec
INFO:root:2024-04-12 16:39:31, Train, Epoch : 2, Step : 1050, Loss : 0.54758, Acc : 0.744, Sensitive_Loss : 0.20464, Sensitive_Acc : 23.100, Run Time : 6.89 sec
INFO:root:2024-04-12 16:39:38, Train, Epoch : 2, Step : 1060, Loss : 0.47545, Acc : 0.762, Sensitive_Loss : 0.27405, Sensitive_Acc : 22.800, Run Time : 7.00 sec
INFO:root:2024-04-12 16:39:45, Train, Epoch : 2, Step : 1070, Loss : 0.48440, Acc : 0.791, Sensitive_Loss : 0.28179, Sensitive_Acc : 24.400, Run Time : 7.13 sec
INFO:root:2024-04-12 16:39:53, Train, Epoch : 2, Step : 1080, Loss : 0.63676, Acc : 0.700, Sensitive_Loss : 0.25031, Sensitive_Acc : 18.600, Run Time : 7.42 sec
INFO:root:2024-04-12 16:40:00, Train, Epoch : 2, Step : 1090, Loss : 0.52532, Acc : 0.753, Sensitive_Loss : 0.17162, Sensitive_Acc : 25.400, Run Time : 6.92 sec
INFO:root:2024-04-12 16:40:07, Train, Epoch : 2, Step : 1100, Loss : 0.52203, Acc : 0.759, Sensitive_Loss : 0.17456, Sensitive_Acc : 20.000, Run Time : 7.43 sec
INFO:root:2024-04-12 16:41:35, Dev, Step : 1100, Loss : 0.58530, Acc : 0.731, Auc : 0.831, Sensitive_Loss : 0.23475, Sensitive_Acc : 21.737, Sensitive_Auc : 0.997, Mean auc: 0.831, Run Time : 87.90 sec
INFO:root:2024-04-12 16:41:36, Best, Step : 1100, Loss : 0.58530, Acc : 0.731, Auc : 0.831, Sensitive_Loss : 0.23475, Sensitive_Acc : 21.737, Sensitive_Auc : 0.997, Best Auc : 0.831
INFO:root:2024-04-12 16:41:41, Train, Epoch : 2, Step : 1110, Loss : 0.54450, Acc : 0.734, Sensitive_Loss : 0.17438, Sensitive_Acc : 20.700, Run Time : 94.39 sec
INFO:root:2024-04-12 16:41:49, Train, Epoch : 2, Step : 1120, Loss : 0.44120, Acc : 0.772, Sensitive_Loss : 0.21630, Sensitive_Acc : 21.400, Run Time : 7.12 sec
INFO:root:2024-04-12 16:41:56, Train, Epoch : 2, Step : 1130, Loss : 0.53877, Acc : 0.744, Sensitive_Loss : 0.16971, Sensitive_Acc : 17.200, Run Time : 7.05 sec
INFO:root:2024-04-12 16:42:03, Train, Epoch : 2, Step : 1140, Loss : 0.51514, Acc : 0.791, Sensitive_Loss : 0.20381, Sensitive_Acc : 21.800, Run Time : 7.35 sec
INFO:root:2024-04-12 16:42:10, Train, Epoch : 2, Step : 1150, Loss : 0.52985, Acc : 0.759, Sensitive_Loss : 0.19347, Sensitive_Acc : 19.800, Run Time : 6.86 sec
INFO:root:2024-04-12 16:42:17, Train, Epoch : 2, Step : 1160, Loss : 0.48371, Acc : 0.762, Sensitive_Loss : 0.18124, Sensitive_Acc : 26.000, Run Time : 7.44 sec
INFO:root:2024-04-12 16:42:24, Train, Epoch : 2, Step : 1170, Loss : 0.44225, Acc : 0.816, Sensitive_Loss : 0.15840, Sensitive_Acc : 20.700, Run Time : 6.89 sec
INFO:root:2024-04-12 16:42:31, Train, Epoch : 2, Step : 1180, Loss : 0.53056, Acc : 0.759, Sensitive_Loss : 0.23773, Sensitive_Acc : 20.800, Run Time : 7.20 sec
INFO:root:2024-04-12 16:42:39, Train, Epoch : 2, Step : 1190, Loss : 0.54937, Acc : 0.762, Sensitive_Loss : 0.15979, Sensitive_Acc : 19.300, Run Time : 7.39 sec
INFO:root:2024-04-12 16:42:45, Train, Epoch : 2, Step : 1200, Loss : 0.58568, Acc : 0.697, Sensitive_Loss : 0.14224, Sensitive_Acc : 25.900, Run Time : 6.77 sec
INFO:root:2024-04-12 16:44:14, Dev, Step : 1200, Loss : 0.57546, Acc : 0.720, Auc : 0.811, Sensitive_Loss : 0.30479, Sensitive_Acc : 20.684, Sensitive_Auc : 0.995, Mean auc: 0.811, Run Time : 88.51 sec
INFO:root:2024-04-12 16:44:19, Train, Epoch : 2, Step : 1210, Loss : 0.49972, Acc : 0.797, Sensitive_Loss : 0.14914, Sensitive_Acc : 22.900, Run Time : 93.97 sec
INFO:root:2024-04-12 16:44:26, Train, Epoch : 2, Step : 1220, Loss : 0.53481, Acc : 0.741, Sensitive_Loss : 0.20274, Sensitive_Acc : 23.000, Run Time : 7.02 sec
INFO:root:2024-04-12 16:44:34, Train, Epoch : 2, Step : 1230, Loss : 0.58534, Acc : 0.703, Sensitive_Loss : 0.19572, Sensitive_Acc : 19.600, Run Time : 7.20 sec
INFO:root:2024-04-12 16:44:41, Train, Epoch : 2, Step : 1240, Loss : 0.52326, Acc : 0.759, Sensitive_Loss : 0.19575, Sensitive_Acc : 17.800, Run Time : 7.37 sec
INFO:root:2024-04-12 16:44:48, Train, Epoch : 2, Step : 1250, Loss : 0.50128, Acc : 0.747, Sensitive_Loss : 0.18218, Sensitive_Acc : 21.700, Run Time : 6.84 sec
INFO:root:2024-04-12 16:44:55, Train, Epoch : 2, Step : 1260, Loss : 0.42786, Acc : 0.762, Sensitive_Loss : 0.23880, Sensitive_Acc : 18.700, Run Time : 7.18 sec
INFO:root:2024-04-12 16:46:28
INFO:root:y_pred: [0.10741135 0.01649453 0.43980327 ... 0.4240343  0.1470175  0.14105777]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [4.91377804e-03 1.11831134e-04 1.15898356e-01 5.89142274e-03
 3.21319749e-05 2.69108173e-03 3.20999861e-01 4.05015983e-03
 8.43637139e-02 9.99991536e-01 1.28932986e-02 1.62693585e-04
 1.73287862e-03 1.12433772e-04 9.99167919e-01 7.06517650e-03
 3.06298002e-03 9.99288797e-01 9.98320401e-01 2.12935568e-03
 8.06889117e-01 8.68731411e-04 4.32385458e-03 3.95612307e-02
 3.30448866e-01 3.26969743e-01 1.19277718e-06 1.32173867e-04
 2.16788219e-04 3.25643877e-03 3.63819860e-03 8.94335449e-01
 2.26073116e-02 6.85972810e-01 2.20537375e-04 1.02336155e-04
 3.20385536e-03 1.60968006e-02 5.66652743e-03 4.47664298e-02
 2.49994476e-03 9.96106327e-01 4.78386588e-04 8.71561002e-03
 9.93765473e-01 7.91895151e-01 4.57865447e-01 6.54977709e-02
 1.99646521e-02 9.98462558e-01 9.58985865e-01 9.97572124e-01
 9.96119142e-01 2.43269769e-03 2.50528812e-01 6.99349940e-01
 7.96892401e-03 5.65296151e-02 9.99339998e-01 4.62343509e-04
 3.56741832e-04 3.30792594e-04 2.95622274e-03 1.06754934e-03
 9.98124540e-01 2.45693564e-01 3.82937687e-05 3.01012278e-01
 4.99974762e-04 9.74498510e-01 9.98824775e-01 9.99602973e-01
 1.32723508e-04 1.27093717e-01 1.13067008e-03 8.80052567e-01
 8.33576247e-02 3.45286280e-05 2.10291566e-03 1.78773403e-02
 7.62787536e-02 7.44162127e-03 9.93469357e-01 9.97788906e-01
 5.41355636e-04 1.40467510e-02 3.92342806e-01 1.57452931e-04
 1.96188278e-02 1.43883721e-04 1.35961629e-03 1.07078090e-01
 6.79421792e-05 1.93769229e-04 1.67368539e-02 3.89380730e-03
 6.52062663e-05 4.57944334e-01 2.02535372e-02 3.82659100e-02
 1.75536778e-02 4.68246453e-03 3.58494669e-02 4.01890883e-03
 6.29590591e-04 2.41684713e-04 6.41145650e-03 1.44037381e-01
 5.26278257e-01 2.94201379e-03 1.19694982e-04 9.99900937e-01
 9.99086142e-01 2.06855126e-04 5.17777979e-01 1.90900862e-01
 1.84292942e-02 1.23844575e-03 1.52508561e-02 1.26692615e-02
 1.42357284e-02 1.82861448e-04 8.93686432e-03 1.05026586e-04
 3.06185056e-03 8.81191313e-01 5.89008618e-04 9.95011806e-01
 8.86591431e-03 1.60500228e-01 1.76334735e-02 1.29342303e-02
 1.05385239e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 16:46:28, Dev, Step : 1268, Loss : 0.54840, Acc : 0.744, Auc : 0.834, Sensitive_Loss : 0.19723, Sensitive_Acc : 21.406, Sensitive_Auc : 0.995, Mean auc: 0.834, Run Time : 87.39 sec
INFO:root:2024-04-12 16:46:28, Best, Step : 1268, Loss : 0.54840, Acc : 0.744,Auc : 0.834, Best Auc : 0.834, Sensitive_Loss : 0.19723, Sensitive_Acc : 21.406, Sensitive_Auc : 0.995
INFO:root:2024-04-12 16:46:32, Train, Epoch : 3, Step : 1270, Loss : 0.08485, Acc : 0.147, Sensitive_Loss : 0.04641, Sensitive_Acc : 5.200, Run Time : 2.56 sec
INFO:root:2024-04-12 16:46:39, Train, Epoch : 3, Step : 1280, Loss : 0.52488, Acc : 0.781, Sensitive_Loss : 0.15705, Sensitive_Acc : 19.900, Run Time : 7.40 sec
INFO:root:2024-04-12 16:46:46, Train, Epoch : 3, Step : 1290, Loss : 0.49994, Acc : 0.781, Sensitive_Loss : 0.19537, Sensitive_Acc : 23.100, Run Time : 7.04 sec
INFO:root:2024-04-12 16:46:53, Train, Epoch : 3, Step : 1300, Loss : 0.43001, Acc : 0.794, Sensitive_Loss : 0.19059, Sensitive_Acc : 25.300, Run Time : 6.52 sec
INFO:root:2024-04-12 16:48:21, Dev, Step : 1300, Loss : 0.51951, Acc : 0.771, Auc : 0.844, Sensitive_Loss : 0.24453, Sensitive_Acc : 21.000, Sensitive_Auc : 0.997, Mean auc: 0.844, Run Time : 88.44 sec
INFO:root:2024-04-12 16:48:22, Best, Step : 1300, Loss : 0.51951, Acc : 0.771, Auc : 0.844, Sensitive_Loss : 0.24453, Sensitive_Acc : 21.000, Sensitive_Auc : 0.997, Best Auc : 0.844
INFO:root:2024-04-12 16:48:27, Train, Epoch : 3, Step : 1310, Loss : 0.46838, Acc : 0.750, Sensitive_Loss : 0.18739, Sensitive_Acc : 23.300, Run Time : 94.55 sec
INFO:root:2024-04-12 16:48:34, Train, Epoch : 3, Step : 1320, Loss : 0.43994, Acc : 0.781, Sensitive_Loss : 0.15757, Sensitive_Acc : 22.300, Run Time : 7.00 sec
INFO:root:2024-04-12 16:48:41, Train, Epoch : 3, Step : 1330, Loss : 0.47422, Acc : 0.772, Sensitive_Loss : 0.20290, Sensitive_Acc : 20.100, Run Time : 7.27 sec
INFO:root:2024-04-12 16:48:49, Train, Epoch : 3, Step : 1340, Loss : 0.43738, Acc : 0.797, Sensitive_Loss : 0.15846, Sensitive_Acc : 24.200, Run Time : 7.55 sec
INFO:root:2024-04-12 16:48:56, Train, Epoch : 3, Step : 1350, Loss : 0.54346, Acc : 0.750, Sensitive_Loss : 0.15233, Sensitive_Acc : 21.600, Run Time : 6.70 sec
INFO:root:2024-04-12 16:49:03, Train, Epoch : 3, Step : 1360, Loss : 0.42368, Acc : 0.819, Sensitive_Loss : 0.13489, Sensitive_Acc : 18.800, Run Time : 7.20 sec
INFO:root:2024-04-12 16:49:10, Train, Epoch : 3, Step : 1370, Loss : 0.54231, Acc : 0.766, Sensitive_Loss : 0.23856, Sensitive_Acc : 24.800, Run Time : 7.29 sec
INFO:root:2024-04-12 16:49:17, Train, Epoch : 3, Step : 1380, Loss : 0.43090, Acc : 0.791, Sensitive_Loss : 0.22175, Sensitive_Acc : 21.000, Run Time : 7.07 sec
INFO:root:2024-04-12 16:49:24, Train, Epoch : 3, Step : 1390, Loss : 0.41553, Acc : 0.819, Sensitive_Loss : 0.16393, Sensitive_Acc : 21.700, Run Time : 6.97 sec
INFO:root:2024-04-12 16:49:31, Train, Epoch : 3, Step : 1400, Loss : 0.48402, Acc : 0.822, Sensitive_Loss : 0.15203, Sensitive_Acc : 21.300, Run Time : 7.00 sec
INFO:root:2024-04-12 16:50:59, Dev, Step : 1400, Loss : 0.50949, Acc : 0.775, Auc : 0.850, Sensitive_Loss : 0.24246, Sensitive_Acc : 21.165, Sensitive_Auc : 0.997, Mean auc: 0.850, Run Time : 87.93 sec
INFO:root:2024-04-12 16:51:00, Best, Step : 1400, Loss : 0.50949, Acc : 0.775, Auc : 0.850, Sensitive_Loss : 0.24246, Sensitive_Acc : 21.165, Sensitive_Auc : 0.997, Best Auc : 0.850
INFO:root:2024-04-12 16:51:05, Train, Epoch : 3, Step : 1410, Loss : 0.42830, Acc : 0.797, Sensitive_Loss : 0.17217, Sensitive_Acc : 20.700, Run Time : 94.13 sec
INFO:root:2024-04-12 16:51:12, Train, Epoch : 3, Step : 1420, Loss : 0.51833, Acc : 0.766, Sensitive_Loss : 0.13753, Sensitive_Acc : 26.100, Run Time : 7.11 sec
INFO:root:2024-04-12 16:51:20, Train, Epoch : 3, Step : 1430, Loss : 0.41912, Acc : 0.819, Sensitive_Loss : 0.15771, Sensitive_Acc : 21.100, Run Time : 7.65 sec
INFO:root:2024-04-12 16:51:27, Train, Epoch : 3, Step : 1440, Loss : 0.40348, Acc : 0.819, Sensitive_Loss : 0.13561, Sensitive_Acc : 20.100, Run Time : 6.91 sec
INFO:root:2024-04-12 16:51:34, Train, Epoch : 3, Step : 1450, Loss : 0.43385, Acc : 0.775, Sensitive_Loss : 0.14628, Sensitive_Acc : 18.100, Run Time : 6.86 sec
INFO:root:2024-04-12 16:51:41, Train, Epoch : 3, Step : 1460, Loss : 0.44512, Acc : 0.819, Sensitive_Loss : 0.15572, Sensitive_Acc : 20.100, Run Time : 7.12 sec
INFO:root:2024-04-12 16:51:48, Train, Epoch : 3, Step : 1470, Loss : 0.52011, Acc : 0.762, Sensitive_Loss : 0.12989, Sensitive_Acc : 23.500, Run Time : 7.37 sec
INFO:root:2024-04-12 16:51:55, Train, Epoch : 3, Step : 1480, Loss : 0.40775, Acc : 0.819, Sensitive_Loss : 0.15109, Sensitive_Acc : 20.900, Run Time : 7.00 sec
INFO:root:2024-04-12 16:52:02, Train, Epoch : 3, Step : 1490, Loss : 0.47859, Acc : 0.775, Sensitive_Loss : 0.15096, Sensitive_Acc : 20.000, Run Time : 6.98 sec
INFO:root:2024-04-12 16:52:10, Train, Epoch : 3, Step : 1500, Loss : 0.41791, Acc : 0.797, Sensitive_Loss : 0.13547, Sensitive_Acc : 26.500, Run Time : 7.29 sec
INFO:root:2024-04-12 16:53:37, Dev, Step : 1500, Loss : 0.51343, Acc : 0.775, Auc : 0.853, Sensitive_Loss : 0.25675, Sensitive_Acc : 21.135, Sensitive_Auc : 0.997, Mean auc: 0.853, Run Time : 87.82 sec
INFO:root:2024-04-12 16:53:38, Best, Step : 1500, Loss : 0.51343, Acc : 0.775, Auc : 0.853, Sensitive_Loss : 0.25675, Sensitive_Acc : 21.135, Sensitive_Auc : 0.997, Best Auc : 0.853
INFO:root:2024-04-12 16:53:44, Train, Epoch : 3, Step : 1510, Loss : 0.48351, Acc : 0.784, Sensitive_Loss : 0.17667, Sensitive_Acc : 21.800, Run Time : 94.12 sec
INFO:root:2024-04-12 16:53:51, Train, Epoch : 3, Step : 1520, Loss : 0.47858, Acc : 0.806, Sensitive_Loss : 0.09178, Sensitive_Acc : 19.700, Run Time : 7.50 sec
INFO:root:2024-04-12 16:53:58, Train, Epoch : 3, Step : 1530, Loss : 0.44700, Acc : 0.781, Sensitive_Loss : 0.16704, Sensitive_Acc : 23.300, Run Time : 6.74 sec
INFO:root:2024-04-12 16:54:05, Train, Epoch : 3, Step : 1540, Loss : 0.46547, Acc : 0.803, Sensitive_Loss : 0.14381, Sensitive_Acc : 22.100, Run Time : 6.90 sec
INFO:root:2024-04-12 16:54:12, Train, Epoch : 3, Step : 1550, Loss : 0.39123, Acc : 0.819, Sensitive_Loss : 0.18484, Sensitive_Acc : 25.800, Run Time : 7.11 sec
INFO:root:2024-04-12 16:54:20, Train, Epoch : 3, Step : 1560, Loss : 0.45446, Acc : 0.784, Sensitive_Loss : 0.14998, Sensitive_Acc : 20.300, Run Time : 7.84 sec
INFO:root:2024-04-12 16:54:26, Train, Epoch : 3, Step : 1570, Loss : 0.48754, Acc : 0.794, Sensitive_Loss : 0.15687, Sensitive_Acc : 25.500, Run Time : 6.63 sec
INFO:root:2024-04-12 16:54:33, Train, Epoch : 3, Step : 1580, Loss : 0.42391, Acc : 0.762, Sensitive_Loss : 0.15555, Sensitive_Acc : 25.000, Run Time : 7.06 sec
INFO:root:2024-04-12 16:54:40, Train, Epoch : 3, Step : 1590, Loss : 0.49515, Acc : 0.759, Sensitive_Loss : 0.10037, Sensitive_Acc : 24.200, Run Time : 6.58 sec
INFO:root:2024-04-12 16:54:48, Train, Epoch : 3, Step : 1600, Loss : 0.41961, Acc : 0.791, Sensitive_Loss : 0.14826, Sensitive_Acc : 17.000, Run Time : 7.46 sec
INFO:root:2024-04-12 16:56:17, Dev, Step : 1600, Loss : 0.51578, Acc : 0.775, Auc : 0.853, Sensitive_Loss : 0.24739, Sensitive_Acc : 21.135, Sensitive_Auc : 0.997, Mean auc: 0.853, Run Time : 89.69 sec
INFO:root:2024-04-12 16:56:18, Best, Step : 1600, Loss : 0.51578, Acc : 0.775, Auc : 0.853, Sensitive_Loss : 0.24739, Sensitive_Acc : 21.135, Sensitive_Auc : 0.997, Best Auc : 0.853
INFO:root:2024-04-12 16:56:23, Train, Epoch : 3, Step : 1610, Loss : 0.38867, Acc : 0.850, Sensitive_Loss : 0.12469, Sensitive_Acc : 26.600, Run Time : 95.85 sec
INFO:root:2024-04-12 16:56:31, Train, Epoch : 3, Step : 1620, Loss : 0.41567, Acc : 0.803, Sensitive_Loss : 0.19265, Sensitive_Acc : 24.500, Run Time : 7.63 sec
INFO:root:2024-04-12 16:56:38, Train, Epoch : 3, Step : 1630, Loss : 0.47271, Acc : 0.797, Sensitive_Loss : 0.09597, Sensitive_Acc : 21.400, Run Time : 7.22 sec
INFO:root:2024-04-12 16:56:45, Train, Epoch : 3, Step : 1640, Loss : 0.54920, Acc : 0.741, Sensitive_Loss : 0.11690, Sensitive_Acc : 19.500, Run Time : 6.50 sec
INFO:root:2024-04-12 16:56:52, Train, Epoch : 3, Step : 1650, Loss : 0.46435, Acc : 0.816, Sensitive_Loss : 0.16987, Sensitive_Acc : 20.400, Run Time : 7.23 sec
INFO:root:2024-04-12 16:56:59, Train, Epoch : 3, Step : 1660, Loss : 0.40187, Acc : 0.806, Sensitive_Loss : 0.20566, Sensitive_Acc : 18.100, Run Time : 7.25 sec
INFO:root:2024-04-12 16:57:06, Train, Epoch : 3, Step : 1670, Loss : 0.42195, Acc : 0.834, Sensitive_Loss : 0.12036, Sensitive_Acc : 20.400, Run Time : 7.10 sec
INFO:root:2024-04-12 16:57:13, Train, Epoch : 3, Step : 1680, Loss : 0.35337, Acc : 0.822, Sensitive_Loss : 0.12293, Sensitive_Acc : 24.200, Run Time : 7.02 sec
INFO:root:2024-04-12 16:57:21, Train, Epoch : 3, Step : 1690, Loss : 0.43792, Acc : 0.803, Sensitive_Loss : 0.11222, Sensitive_Acc : 20.300, Run Time : 7.29 sec
INFO:root:2024-04-12 16:57:28, Train, Epoch : 3, Step : 1700, Loss : 0.39396, Acc : 0.866, Sensitive_Loss : 0.15043, Sensitive_Acc : 23.400, Run Time : 7.23 sec
INFO:root:2024-04-12 16:58:57, Dev, Step : 1700, Loss : 0.51012, Acc : 0.777, Auc : 0.855, Sensitive_Loss : 0.21830, Sensitive_Acc : 21.286, Sensitive_Auc : 0.997, Mean auc: 0.855, Run Time : 89.00 sec
INFO:root:2024-04-12 16:58:58, Best, Step : 1700, Loss : 0.51012, Acc : 0.777, Auc : 0.855, Sensitive_Loss : 0.21830, Sensitive_Acc : 21.286, Sensitive_Auc : 0.997, Best Auc : 0.855
INFO:root:2024-04-12 16:59:03, Train, Epoch : 3, Step : 1710, Loss : 0.47448, Acc : 0.800, Sensitive_Loss : 0.11838, Sensitive_Acc : 24.500, Run Time : 95.27 sec
INFO:root:2024-04-12 16:59:10, Train, Epoch : 3, Step : 1720, Loss : 0.46045, Acc : 0.812, Sensitive_Loss : 0.10904, Sensitive_Acc : 24.300, Run Time : 7.11 sec
INFO:root:2024-04-12 16:59:17, Train, Epoch : 3, Step : 1730, Loss : 0.37298, Acc : 0.847, Sensitive_Loss : 0.16805, Sensitive_Acc : 20.100, Run Time : 7.12 sec
INFO:root:2024-04-12 16:59:25, Train, Epoch : 3, Step : 1740, Loss : 0.40793, Acc : 0.806, Sensitive_Loss : 0.13612, Sensitive_Acc : 18.000, Run Time : 7.29 sec
INFO:root:2024-04-12 16:59:31, Train, Epoch : 3, Step : 1750, Loss : 0.41340, Acc : 0.822, Sensitive_Loss : 0.22311, Sensitive_Acc : 25.900, Run Time : 6.68 sec
INFO:root:2024-04-12 16:59:39, Train, Epoch : 3, Step : 1760, Loss : 0.45396, Acc : 0.812, Sensitive_Loss : 0.09940, Sensitive_Acc : 19.700, Run Time : 7.60 sec
INFO:root:2024-04-12 16:59:46, Train, Epoch : 3, Step : 1770, Loss : 0.44705, Acc : 0.791, Sensitive_Loss : 0.14433, Sensitive_Acc : 18.300, Run Time : 7.17 sec
INFO:root:2024-04-12 16:59:53, Train, Epoch : 3, Step : 1780, Loss : 0.43777, Acc : 0.816, Sensitive_Loss : 0.13863, Sensitive_Acc : 19.500, Run Time : 7.14 sec
INFO:root:2024-04-12 17:00:00, Train, Epoch : 3, Step : 1790, Loss : 0.45520, Acc : 0.800, Sensitive_Loss : 0.15397, Sensitive_Acc : 20.000, Run Time : 7.10 sec
INFO:root:2024-04-12 17:00:08, Train, Epoch : 3, Step : 1800, Loss : 0.43291, Acc : 0.787, Sensitive_Loss : 0.12284, Sensitive_Acc : 19.000, Run Time : 7.37 sec
INFO:root:2024-04-12 17:01:36, Dev, Step : 1800, Loss : 0.50548, Acc : 0.781, Auc : 0.855, Sensitive_Loss : 0.20937, Sensitive_Acc : 21.391, Sensitive_Auc : 0.996, Mean auc: 0.855, Run Time : 87.86 sec
INFO:root:2024-04-12 17:01:36, Best, Step : 1800, Loss : 0.50548, Acc : 0.781, Auc : 0.855, Sensitive_Loss : 0.20937, Sensitive_Acc : 21.391, Sensitive_Auc : 0.996, Best Auc : 0.855
INFO:root:2024-04-12 17:01:42, Train, Epoch : 3, Step : 1810, Loss : 0.42860, Acc : 0.775, Sensitive_Loss : 0.14083, Sensitive_Acc : 21.100, Run Time : 93.88 sec
INFO:root:2024-04-12 17:01:49, Train, Epoch : 3, Step : 1820, Loss : 0.44484, Acc : 0.794, Sensitive_Loss : 0.24531, Sensitive_Acc : 20.200, Run Time : 7.09 sec
INFO:root:2024-04-12 17:01:56, Train, Epoch : 3, Step : 1830, Loss : 0.41783, Acc : 0.825, Sensitive_Loss : 0.23912, Sensitive_Acc : 20.400, Run Time : 7.53 sec
INFO:root:2024-04-12 17:02:03, Train, Epoch : 3, Step : 1840, Loss : 0.54568, Acc : 0.775, Sensitive_Loss : 0.17419, Sensitive_Acc : 17.800, Run Time : 7.10 sec
INFO:root:2024-04-12 17:02:10, Train, Epoch : 3, Step : 1850, Loss : 0.44257, Acc : 0.803, Sensitive_Loss : 0.18820, Sensitive_Acc : 19.200, Run Time : 6.83 sec
INFO:root:2024-04-12 17:02:18, Train, Epoch : 3, Step : 1860, Loss : 0.46030, Acc : 0.794, Sensitive_Loss : 0.13187, Sensitive_Acc : 21.200, Run Time : 7.69 sec
INFO:root:2024-04-12 17:02:24, Train, Epoch : 3, Step : 1870, Loss : 0.45586, Acc : 0.775, Sensitive_Loss : 0.08830, Sensitive_Acc : 24.500, Run Time : 6.40 sec
INFO:root:2024-04-12 17:02:31, Train, Epoch : 3, Step : 1880, Loss : 0.43408, Acc : 0.778, Sensitive_Loss : 0.21075, Sensitive_Acc : 19.900, Run Time : 7.08 sec
INFO:root:2024-04-12 17:02:39, Train, Epoch : 3, Step : 1890, Loss : 0.42488, Acc : 0.787, Sensitive_Loss : 0.14412, Sensitive_Acc : 20.700, Run Time : 7.64 sec
INFO:root:2024-04-12 17:02:46, Train, Epoch : 3, Step : 1900, Loss : 0.49954, Acc : 0.787, Sensitive_Loss : 0.14442, Sensitive_Acc : 22.400, Run Time : 6.82 sec
INFO:root:2024-04-12 17:04:13, Dev, Step : 1900, Loss : 0.51301, Acc : 0.778, Auc : 0.858, Sensitive_Loss : 0.22520, Sensitive_Acc : 21.211, Sensitive_Auc : 0.997, Mean auc: 0.858, Run Time : 87.32 sec
INFO:root:2024-04-12 17:04:14, Best, Step : 1900, Loss : 0.51301, Acc : 0.778, Auc : 0.858, Sensitive_Loss : 0.22520, Sensitive_Acc : 21.211, Sensitive_Auc : 0.997, Best Auc : 0.858
INFO:root:2024-04-12 17:05:42
INFO:root:y_pred: [0.09644648 0.00730819 0.11097818 ... 0.17636566 0.06642097 0.12159405]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [2.5136743e-03 2.0572671e-04 3.3097237e-02 3.9748795e-02 1.4270007e-04
 5.9828050e-03 8.5283421e-02 2.7070038e-02 2.7430388e-01 9.9998188e-01
 8.0312863e-02 6.1836763e-04 9.4251931e-03 1.7353421e-04 9.9960023e-01
 1.1642747e-02 4.9802843e-03 9.9942267e-01 9.9890172e-01 7.9911081e-03
 9.8131198e-01 1.4757462e-03 4.6088872e-03 3.0473208e-02 5.7208925e-01
 5.6612962e-01 4.3076038e-06 6.1749486e-04 5.7920360e-04 2.1499492e-02
 1.6724484e-02 9.1330105e-01 5.6353774e-02 8.5283250e-01 2.5205524e-03
 1.0700187e-04 1.1059475e-02 3.8996466e-02 2.2242134e-02 8.5433185e-02
 2.0343712e-02 9.9713671e-01 1.5351688e-03 2.6044336e-03 9.9743974e-01
 4.2943147e-01 6.8862921e-01 3.7343547e-01 1.7802067e-01 9.9916482e-01
 9.9518061e-01 9.9961168e-01 9.9671054e-01 4.7871633e-03 3.2895702e-01
 9.1540504e-01 1.1147557e-02 1.7279960e-02 9.9969769e-01 1.9428318e-03
 5.1662722e-04 6.2893541e-03 6.5942495e-03 9.2722243e-04 9.9909282e-01
 2.4065296e-01 5.3809996e-04 3.1075767e-01 1.0966609e-02 9.9712700e-01
 9.9979502e-01 9.9989283e-01 3.9532073e-04 4.9760139e-01 1.2208581e-02
 9.3719435e-01 4.8730101e-02 5.0915951e-05 6.1131263e-04 4.6554785e-02
 1.6361782e-01 2.7971989e-03 9.9987698e-01 9.9961829e-01 4.2591444e-03
 1.4805649e-02 8.8366061e-02 6.1056262e-04 8.5798793e-02 4.0357752e-04
 2.7032798e-02 3.3425233e-01 6.7167974e-04 3.3878203e-04 5.3394632e-03
 1.8152146e-02 3.5817875e-04 8.6234117e-01 4.6848405e-02 7.4835382e-03
 2.0959157e-02 6.4524878e-03 2.5149378e-01 4.2540114e-03 6.8495558e-03
 1.3585575e-03 3.6036018e-02 1.7175040e-01 5.3472602e-01 9.9315131e-03
 5.5870297e-04 9.9993646e-01 9.9986970e-01 1.2877176e-04 1.5720135e-01
 2.6804447e-01 1.0407469e-02 4.9195169e-03 2.7272031e-02 1.1500069e-02
 2.8255852e-02 3.5904985e-04 7.6195620e-02 4.1968573e-04 4.6939347e-03
 9.7251421e-01 2.1355881e-03 9.9834311e-01 1.9655131e-02 1.0524414e-01
 2.8913217e-02 1.8507736e-02 3.7275229e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 17:05:42, Dev, Step : 1902, Loss : 0.50905, Acc : 0.782, Auc : 0.858, Sensitive_Loss : 0.22882, Sensitive_Acc : 21.211, Sensitive_Auc : 0.997, Mean auc: 0.858, Run Time : 86.60 sec
INFO:root:2024-04-12 17:05:49, Train, Epoch : 4, Step : 1910, Loss : 0.33631, Acc : 0.656, Sensitive_Loss : 0.13261, Sensitive_Acc : 18.500, Run Time : 6.83 sec
INFO:root:2024-04-12 17:05:56, Train, Epoch : 4, Step : 1920, Loss : 0.40196, Acc : 0.809, Sensitive_Loss : 0.15865, Sensitive_Acc : 20.400, Run Time : 7.15 sec
INFO:root:2024-04-12 17:06:03, Train, Epoch : 4, Step : 1930, Loss : 0.45135, Acc : 0.784, Sensitive_Loss : 0.19624, Sensitive_Acc : 23.100, Run Time : 6.96 sec
INFO:root:2024-04-12 17:06:11, Train, Epoch : 4, Step : 1940, Loss : 0.40776, Acc : 0.784, Sensitive_Loss : 0.16474, Sensitive_Acc : 22.600, Run Time : 7.23 sec
INFO:root:2024-04-12 17:06:18, Train, Epoch : 4, Step : 1950, Loss : 0.42954, Acc : 0.831, Sensitive_Loss : 0.13534, Sensitive_Acc : 22.500, Run Time : 7.12 sec
INFO:root:2024-04-12 17:06:25, Train, Epoch : 4, Step : 1960, Loss : 0.42840, Acc : 0.819, Sensitive_Loss : 0.16540, Sensitive_Acc : 23.700, Run Time : 7.04 sec
INFO:root:2024-04-12 17:06:32, Train, Epoch : 4, Step : 1970, Loss : 0.43981, Acc : 0.819, Sensitive_Loss : 0.08401, Sensitive_Acc : 19.800, Run Time : 7.45 sec
INFO:root:2024-04-12 17:06:39, Train, Epoch : 4, Step : 1980, Loss : 0.40554, Acc : 0.809, Sensitive_Loss : 0.19212, Sensitive_Acc : 19.100, Run Time : 6.91 sec
INFO:root:2024-04-12 17:06:46, Train, Epoch : 4, Step : 1990, Loss : 0.39791, Acc : 0.834, Sensitive_Loss : 0.15005, Sensitive_Acc : 20.400, Run Time : 7.19 sec
INFO:root:2024-04-12 17:06:54, Train, Epoch : 4, Step : 2000, Loss : 0.37408, Acc : 0.825, Sensitive_Loss : 0.15107, Sensitive_Acc : 11.500, Run Time : 7.42 sec
INFO:root:2024-04-12 17:08:22, Dev, Step : 2000, Loss : 0.50727, Acc : 0.780, Auc : 0.855, Sensitive_Loss : 0.22824, Sensitive_Acc : 21.331, Sensitive_Auc : 0.997, Mean auc: 0.855, Run Time : 87.91 sec
INFO:root:2024-04-12 17:08:28, Train, Epoch : 4, Step : 2010, Loss : 0.47587, Acc : 0.806, Sensitive_Loss : 0.12565, Sensitive_Acc : 20.900, Run Time : 93.88 sec
INFO:root:2024-04-12 17:08:35, Train, Epoch : 4, Step : 2020, Loss : 0.38925, Acc : 0.825, Sensitive_Loss : 0.08665, Sensitive_Acc : 17.200, Run Time : 7.11 sec
INFO:root:2024-04-12 17:08:41, Train, Epoch : 4, Step : 2030, Loss : 0.38508, Acc : 0.819, Sensitive_Loss : 0.12110, Sensitive_Acc : 22.700, Run Time : 6.65 sec
INFO:root:2024-04-12 17:08:48, Train, Epoch : 4, Step : 2040, Loss : 0.39521, Acc : 0.812, Sensitive_Loss : 0.14679, Sensitive_Acc : 21.600, Run Time : 6.92 sec
INFO:root:2024-04-12 17:08:56, Train, Epoch : 4, Step : 2050, Loss : 0.43990, Acc : 0.794, Sensitive_Loss : 0.15188, Sensitive_Acc : 25.500, Run Time : 7.50 sec
INFO:root:2024-04-12 17:09:03, Train, Epoch : 4, Step : 2060, Loss : 0.39191, Acc : 0.838, Sensitive_Loss : 0.12611, Sensitive_Acc : 23.100, Run Time : 6.88 sec
INFO:root:2024-04-12 17:09:10, Train, Epoch : 4, Step : 2070, Loss : 0.39212, Acc : 0.825, Sensitive_Loss : 0.12791, Sensitive_Acc : 21.100, Run Time : 7.03 sec
INFO:root:2024-04-12 17:09:17, Train, Epoch : 4, Step : 2080, Loss : 0.37352, Acc : 0.812, Sensitive_Loss : 0.14729, Sensitive_Acc : 21.800, Run Time : 7.33 sec
INFO:root:2024-04-12 17:09:24, Train, Epoch : 4, Step : 2090, Loss : 0.43463, Acc : 0.819, Sensitive_Loss : 0.18768, Sensitive_Acc : 25.600, Run Time : 7.05 sec
INFO:root:2024-04-12 17:09:32, Train, Epoch : 4, Step : 2100, Loss : 0.39634, Acc : 0.812, Sensitive_Loss : 0.09655, Sensitive_Acc : 25.200, Run Time : 7.48 sec
INFO:root:2024-04-12 17:10:59, Dev, Step : 2100, Loss : 0.50626, Acc : 0.778, Auc : 0.857, Sensitive_Loss : 0.20531, Sensitive_Acc : 21.391, Sensitive_Auc : 0.997, Mean auc: 0.857, Run Time : 87.37 sec
INFO:root:2024-04-12 17:11:05, Train, Epoch : 4, Step : 2110, Loss : 0.45219, Acc : 0.794, Sensitive_Loss : 0.13489, Sensitive_Acc : 24.800, Run Time : 93.08 sec
INFO:root:2024-04-12 17:11:12, Train, Epoch : 4, Step : 2120, Loss : 0.43957, Acc : 0.756, Sensitive_Loss : 0.08858, Sensitive_Acc : 17.700, Run Time : 7.10 sec
INFO:root:2024-04-12 17:11:19, Train, Epoch : 4, Step : 2130, Loss : 0.32896, Acc : 0.831, Sensitive_Loss : 0.20596, Sensitive_Acc : 21.700, Run Time : 6.77 sec
INFO:root:2024-04-12 17:11:26, Train, Epoch : 4, Step : 2140, Loss : 0.43978, Acc : 0.781, Sensitive_Loss : 0.17465, Sensitive_Acc : 18.000, Run Time : 7.94 sec
INFO:root:2024-04-12 17:11:33, Train, Epoch : 4, Step : 2150, Loss : 0.46399, Acc : 0.781, Sensitive_Loss : 0.08682, Sensitive_Acc : 19.800, Run Time : 6.64 sec
INFO:root:2024-04-12 17:11:40, Train, Epoch : 4, Step : 2160, Loss : 0.44819, Acc : 0.800, Sensitive_Loss : 0.16427, Sensitive_Acc : 20.600, Run Time : 7.22 sec
INFO:root:2024-04-12 17:11:47, Train, Epoch : 4, Step : 2170, Loss : 0.39006, Acc : 0.819, Sensitive_Loss : 0.15294, Sensitive_Acc : 23.600, Run Time : 6.25 sec
INFO:root:2024-04-12 17:11:54, Train, Epoch : 4, Step : 2180, Loss : 0.39848, Acc : 0.800, Sensitive_Loss : 0.18217, Sensitive_Acc : 26.100, Run Time : 7.72 sec
INFO:root:2024-04-12 17:12:01, Train, Epoch : 4, Step : 2190, Loss : 0.46069, Acc : 0.812, Sensitive_Loss : 0.15657, Sensitive_Acc : 18.100, Run Time : 7.07 sec
INFO:root:2024-04-12 17:12:08, Train, Epoch : 4, Step : 2200, Loss : 0.42631, Acc : 0.806, Sensitive_Loss : 0.15530, Sensitive_Acc : 24.600, Run Time : 6.94 sec
INFO:root:2024-04-12 17:13:36, Dev, Step : 2200, Loss : 0.51771, Acc : 0.773, Auc : 0.857, Sensitive_Loss : 0.20765, Sensitive_Acc : 21.511, Sensitive_Auc : 0.997, Mean auc: 0.857, Run Time : 87.89 sec
INFO:root:2024-04-12 17:13:42, Train, Epoch : 4, Step : 2210, Loss : 0.42250, Acc : 0.831, Sensitive_Loss : 0.16919, Sensitive_Acc : 20.400, Run Time : 93.85 sec
INFO:root:2024-04-12 17:13:49, Train, Epoch : 4, Step : 2220, Loss : 0.48278, Acc : 0.772, Sensitive_Loss : 0.17119, Sensitive_Acc : 20.700, Run Time : 7.11 sec
INFO:root:2024-04-12 17:13:56, Train, Epoch : 4, Step : 2230, Loss : 0.43535, Acc : 0.781, Sensitive_Loss : 0.10763, Sensitive_Acc : 21.600, Run Time : 6.80 sec
INFO:root:2024-04-12 17:14:03, Train, Epoch : 4, Step : 2240, Loss : 0.41920, Acc : 0.781, Sensitive_Loss : 0.17950, Sensitive_Acc : 21.000, Run Time : 7.11 sec
INFO:root:2024-04-12 17:14:10, Train, Epoch : 4, Step : 2250, Loss : 0.39703, Acc : 0.847, Sensitive_Loss : 0.10139, Sensitive_Acc : 21.600, Run Time : 6.88 sec
INFO:root:2024-04-12 17:14:18, Train, Epoch : 4, Step : 2260, Loss : 0.48477, Acc : 0.806, Sensitive_Loss : 0.09984, Sensitive_Acc : 22.100, Run Time : 7.49 sec
INFO:root:2024-04-12 17:14:25, Train, Epoch : 4, Step : 2270, Loss : 0.40837, Acc : 0.847, Sensitive_Loss : 0.09885, Sensitive_Acc : 22.100, Run Time : 7.05 sec
INFO:root:2024-04-12 17:14:32, Train, Epoch : 4, Step : 2280, Loss : 0.39729, Acc : 0.828, Sensitive_Loss : 0.09385, Sensitive_Acc : 21.600, Run Time : 7.28 sec
INFO:root:2024-04-12 17:14:39, Train, Epoch : 4, Step : 2290, Loss : 0.42263, Acc : 0.822, Sensitive_Loss : 0.13900, Sensitive_Acc : 22.500, Run Time : 7.23 sec
INFO:root:2024-04-12 17:14:46, Train, Epoch : 4, Step : 2300, Loss : 0.40980, Acc : 0.816, Sensitive_Loss : 0.11079, Sensitive_Acc : 23.700, Run Time : 7.25 sec
INFO:root:2024-04-12 17:16:14, Dev, Step : 2300, Loss : 0.50153, Acc : 0.785, Auc : 0.860, Sensitive_Loss : 0.18970, Sensitive_Acc : 21.571, Sensitive_Auc : 0.997, Mean auc: 0.860, Run Time : 87.75 sec
INFO:root:2024-04-12 17:16:15, Best, Step : 2300, Loss : 0.50153, Acc : 0.785, Auc : 0.860, Sensitive_Loss : 0.18970, Sensitive_Acc : 21.571, Sensitive_Auc : 0.997, Best Auc : 0.860
INFO:root:2024-04-12 17:16:20, Train, Epoch : 4, Step : 2310, Loss : 0.42586, Acc : 0.831, Sensitive_Loss : 0.16272, Sensitive_Acc : 16.600, Run Time : 93.83 sec
INFO:root:2024-04-12 17:16:27, Train, Epoch : 4, Step : 2320, Loss : 0.37798, Acc : 0.809, Sensitive_Loss : 0.13165, Sensitive_Acc : 21.700, Run Time : 7.24 sec
INFO:root:2024-04-12 17:16:35, Train, Epoch : 4, Step : 2330, Loss : 0.41387, Acc : 0.844, Sensitive_Loss : 0.13090, Sensitive_Acc : 19.100, Run Time : 7.55 sec
INFO:root:2024-04-12 17:16:42, Train, Epoch : 4, Step : 2340, Loss : 0.51132, Acc : 0.784, Sensitive_Loss : 0.15058, Sensitive_Acc : 24.900, Run Time : 6.94 sec
INFO:root:2024-04-12 17:16:49, Train, Epoch : 4, Step : 2350, Loss : 0.40864, Acc : 0.809, Sensitive_Loss : 0.10346, Sensitive_Acc : 24.600, Run Time : 7.29 sec
INFO:root:2024-04-12 17:16:57, Train, Epoch : 4, Step : 2360, Loss : 0.41341, Acc : 0.812, Sensitive_Loss : 0.10363, Sensitive_Acc : 19.100, Run Time : 7.55 sec
INFO:root:2024-04-12 17:17:04, Train, Epoch : 4, Step : 2370, Loss : 0.42227, Acc : 0.822, Sensitive_Loss : 0.11106, Sensitive_Acc : 21.500, Run Time : 6.84 sec
INFO:root:2024-04-12 17:17:10, Train, Epoch : 4, Step : 2380, Loss : 0.38726, Acc : 0.816, Sensitive_Loss : 0.09592, Sensitive_Acc : 21.800, Run Time : 6.68 sec
INFO:root:2024-04-12 17:17:18, Train, Epoch : 4, Step : 2390, Loss : 0.39090, Acc : 0.850, Sensitive_Loss : 0.11626, Sensitive_Acc : 21.600, Run Time : 7.45 sec
INFO:root:2024-04-12 17:17:25, Train, Epoch : 4, Step : 2400, Loss : 0.45058, Acc : 0.819, Sensitive_Loss : 0.16883, Sensitive_Acc : 20.800, Run Time : 7.21 sec
INFO:root:2024-04-12 17:18:53, Dev, Step : 2400, Loss : 0.50380, Acc : 0.780, Auc : 0.859, Sensitive_Loss : 0.17695, Sensitive_Acc : 21.511, Sensitive_Auc : 0.997, Mean auc: 0.859, Run Time : 88.07 sec
INFO:root:2024-04-12 17:18:59, Train, Epoch : 4, Step : 2410, Loss : 0.41242, Acc : 0.812, Sensitive_Loss : 0.13221, Sensitive_Acc : 20.200, Run Time : 94.30 sec
INFO:root:2024-04-12 17:19:06, Train, Epoch : 4, Step : 2420, Loss : 0.41614, Acc : 0.800, Sensitive_Loss : 0.11902, Sensitive_Acc : 22.600, Run Time : 6.96 sec
INFO:root:2024-04-12 17:19:13, Train, Epoch : 4, Step : 2430, Loss : 0.47159, Acc : 0.812, Sensitive_Loss : 0.13139, Sensitive_Acc : 22.400, Run Time : 6.86 sec
INFO:root:2024-04-12 17:19:20, Train, Epoch : 4, Step : 2440, Loss : 0.39604, Acc : 0.847, Sensitive_Loss : 0.10702, Sensitive_Acc : 19.500, Run Time : 7.33 sec
INFO:root:2024-04-12 17:19:27, Train, Epoch : 4, Step : 2450, Loss : 0.44372, Acc : 0.803, Sensitive_Loss : 0.11584, Sensitive_Acc : 24.700, Run Time : 6.75 sec
INFO:root:2024-04-12 17:19:35, Train, Epoch : 4, Step : 2460, Loss : 0.42832, Acc : 0.822, Sensitive_Loss : 0.09894, Sensitive_Acc : 17.200, Run Time : 7.66 sec
INFO:root:2024-04-12 17:19:42, Train, Epoch : 4, Step : 2470, Loss : 0.47778, Acc : 0.797, Sensitive_Loss : 0.12283, Sensitive_Acc : 20.100, Run Time : 6.88 sec
INFO:root:2024-04-12 17:19:49, Train, Epoch : 4, Step : 2480, Loss : 0.41315, Acc : 0.809, Sensitive_Loss : 0.09770, Sensitive_Acc : 18.600, Run Time : 6.90 sec
INFO:root:2024-04-12 17:19:56, Train, Epoch : 4, Step : 2490, Loss : 0.44139, Acc : 0.812, Sensitive_Loss : 0.11500, Sensitive_Acc : 22.900, Run Time : 7.08 sec
INFO:root:2024-04-12 17:20:03, Train, Epoch : 4, Step : 2500, Loss : 0.36522, Acc : 0.834, Sensitive_Loss : 0.10393, Sensitive_Acc : 23.300, Run Time : 7.71 sec
INFO:root:2024-04-12 17:21:31, Dev, Step : 2500, Loss : 0.51693, Acc : 0.777, Auc : 0.859, Sensitive_Loss : 0.16861, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.859, Run Time : 87.47 sec
INFO:root:2024-04-12 17:21:36, Train, Epoch : 4, Step : 2510, Loss : 0.41640, Acc : 0.850, Sensitive_Loss : 0.08668, Sensitive_Acc : 22.100, Run Time : 92.92 sec
INFO:root:2024-04-12 17:21:43, Train, Epoch : 4, Step : 2520, Loss : 0.36416, Acc : 0.828, Sensitive_Loss : 0.14411, Sensitive_Acc : 23.200, Run Time : 7.01 sec
INFO:root:2024-04-12 17:21:51, Train, Epoch : 4, Step : 2530, Loss : 0.44128, Acc : 0.809, Sensitive_Loss : 0.08845, Sensitive_Acc : 21.500, Run Time : 7.50 sec
INFO:root:2024-04-12 17:23:22
INFO:root:y_pred: [0.15321076 0.00612607 0.15331875 ... 0.15580069 0.0542734  0.09971637]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [9.79842036e-04 8.01580682e-05 1.27723906e-02 3.53029417e-03
 2.56924104e-05 1.34368439e-03 3.60534042e-02 3.40756564e-03
 7.42204413e-02 9.99933720e-01 1.68812722e-02 2.02073948e-04
 1.08432805e-03 3.28429087e-05 9.98838007e-01 6.28076168e-03
 3.26248677e-03 9.98905897e-01 9.97431219e-01 1.86593272e-03
 9.12284195e-01 2.86219729e-04 1.87147025e-03 6.60623051e-03
 3.50382537e-01 3.17992717e-01 8.28674104e-07 3.67928937e-04
 3.59175843e-04 5.37490752e-03 1.29437372e-02 8.97500813e-01
 1.22156907e-02 9.14471090e-01 1.12773024e-03 1.82050626e-05
 3.71959805e-03 1.99243482e-02 1.44569529e-02 5.65297082e-02
 1.61206685e-02 9.89316702e-01 2.32866383e-04 2.90707714e-04
 9.79319394e-01 8.37676451e-02 4.53662604e-01 1.24292023e-01
 1.65449567e-02 9.98880804e-01 9.86706793e-01 9.98641431e-01
 9.29141462e-01 3.65631492e-03 1.32877424e-01 6.46125257e-01
 2.62581068e-03 4.17432003e-03 9.99114573e-01 6.46233500e-04
 4.41220727e-05 1.91512972e-03 5.86593617e-03 1.83626922e-04
 9.94827807e-01 1.70993119e-01 1.17244781e-04 4.26330306e-02
 2.08224147e-03 9.84904528e-01 9.99412537e-01 9.99675632e-01
 4.55487352e-05 1.58058837e-01 1.42582133e-03 9.07843351e-01
 1.80153381e-02 9.27318706e-06 1.22211059e-04 6.38291752e-03
 2.56678611e-02 1.11660117e-03 9.98705626e-01 9.99117196e-01
 1.28366612e-03 2.66296114e-03 2.34751478e-02 1.19715014e-05
 1.02380197e-02 4.70434134e-05 5.12228999e-03 1.35820240e-01
 1.38427291e-04 6.50799630e-05 5.17254230e-04 3.29020224e-03
 1.83148150e-04 5.31841874e-01 1.47178099e-02 1.66601129e-03
 4.02013259e-03 8.67197104e-03 3.21344733e-02 1.20367808e-03
 2.73648626e-03 7.71872292e-04 1.10659124e-02 7.40977898e-02
 3.73541236e-01 2.77739111e-03 1.89272207e-04 9.99773085e-01
 9.99459207e-01 5.84951849e-05 6.69848621e-02 1.27935767e-01
 9.82021634e-03 1.55790907e-03 1.36190541e-02 8.17792490e-03
 3.14775459e-03 5.56958948e-05 1.04450621e-02 4.88554979e-05
 7.09265412e-04 9.35751915e-01 5.30379068e-04 9.97945845e-01
 9.78570618e-03 1.84256844e-02 5.18309465e-03 1.23608755e-02
 4.86166391e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 17:23:22, Dev, Step : 2536, Loss : 0.51271, Acc : 0.779, Auc : 0.860, Sensitive_Loss : 0.16813, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.860, Run Time : 87.44 sec
INFO:root:2024-04-12 17:23:23, Best, Step : 2536, Loss : 0.51271, Acc : 0.779,Auc : 0.860, Best Auc : 0.860, Sensitive_Loss : 0.16813, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997
INFO:root:2024-04-12 17:23:27, Train, Epoch : 5, Step : 2540, Loss : 0.13030, Acc : 0.350, Sensitive_Loss : 0.04781, Sensitive_Acc : 8.600, Run Time : 3.89 sec
INFO:root:2024-04-12 17:23:35, Train, Epoch : 5, Step : 2550, Loss : 0.35622, Acc : 0.816, Sensitive_Loss : 0.10342, Sensitive_Acc : 22.200, Run Time : 7.10 sec
INFO:root:2024-04-12 17:23:42, Train, Epoch : 5, Step : 2560, Loss : 0.40442, Acc : 0.822, Sensitive_Loss : 0.13487, Sensitive_Acc : 22.100, Run Time : 6.99 sec
INFO:root:2024-04-12 17:23:49, Train, Epoch : 5, Step : 2570, Loss : 0.41191, Acc : 0.809, Sensitive_Loss : 0.13899, Sensitive_Acc : 24.700, Run Time : 7.14 sec
INFO:root:2024-04-12 17:23:56, Train, Epoch : 5, Step : 2580, Loss : 0.37484, Acc : 0.881, Sensitive_Loss : 0.14383, Sensitive_Acc : 17.900, Run Time : 7.19 sec
INFO:root:2024-04-12 17:24:03, Train, Epoch : 5, Step : 2590, Loss : 0.40177, Acc : 0.838, Sensitive_Loss : 0.13886, Sensitive_Acc : 22.500, Run Time : 6.90 sec
INFO:root:2024-04-12 17:24:10, Train, Epoch : 5, Step : 2600, Loss : 0.39685, Acc : 0.825, Sensitive_Loss : 0.13498, Sensitive_Acc : 22.800, Run Time : 7.50 sec
INFO:root:2024-04-12 17:25:38, Dev, Step : 2600, Loss : 0.51066, Acc : 0.777, Auc : 0.859, Sensitive_Loss : 0.20567, Sensitive_Acc : 21.211, Sensitive_Auc : 0.998, Mean auc: 0.859, Run Time : 88.15 sec
INFO:root:2024-04-12 17:25:44, Train, Epoch : 5, Step : 2610, Loss : 0.44252, Acc : 0.803, Sensitive_Loss : 0.05853, Sensitive_Acc : 20.500, Run Time : 93.74 sec
INFO:root:2024-04-12 17:25:51, Train, Epoch : 5, Step : 2620, Loss : 0.35304, Acc : 0.834, Sensitive_Loss : 0.14153, Sensitive_Acc : 20.500, Run Time : 7.28 sec
INFO:root:2024-04-12 17:25:58, Train, Epoch : 5, Step : 2630, Loss : 0.37847, Acc : 0.831, Sensitive_Loss : 0.15787, Sensitive_Acc : 19.200, Run Time : 7.05 sec
INFO:root:2024-04-12 17:26:05, Train, Epoch : 5, Step : 2640, Loss : 0.41248, Acc : 0.819, Sensitive_Loss : 0.14915, Sensitive_Acc : 23.800, Run Time : 7.14 sec
INFO:root:2024-04-12 17:26:13, Train, Epoch : 5, Step : 2650, Loss : 0.38198, Acc : 0.812, Sensitive_Loss : 0.17630, Sensitive_Acc : 23.600, Run Time : 7.30 sec
INFO:root:2024-04-12 17:26:20, Train, Epoch : 5, Step : 2660, Loss : 0.37883, Acc : 0.844, Sensitive_Loss : 0.14384, Sensitive_Acc : 18.700, Run Time : 7.23 sec
INFO:root:2024-04-12 17:26:27, Train, Epoch : 5, Step : 2670, Loss : 0.39700, Acc : 0.847, Sensitive_Loss : 0.09492, Sensitive_Acc : 20.800, Run Time : 6.92 sec
INFO:root:2024-04-12 17:26:34, Train, Epoch : 5, Step : 2680, Loss : 0.43485, Acc : 0.800, Sensitive_Loss : 0.12246, Sensitive_Acc : 21.800, Run Time : 6.65 sec
INFO:root:2024-04-12 17:26:41, Train, Epoch : 5, Step : 2690, Loss : 0.35167, Acc : 0.828, Sensitive_Loss : 0.09079, Sensitive_Acc : 20.200, Run Time : 6.99 sec
INFO:root:2024-04-12 17:26:48, Train, Epoch : 5, Step : 2700, Loss : 0.40158, Acc : 0.806, Sensitive_Loss : 0.17640, Sensitive_Acc : 22.000, Run Time : 7.09 sec
INFO:root:2024-04-12 17:28:16, Dev, Step : 2700, Loss : 0.51052, Acc : 0.782, Auc : 0.860, Sensitive_Loss : 0.19525, Sensitive_Acc : 21.511, Sensitive_Auc : 0.998, Mean auc: 0.860, Run Time : 88.49 sec
INFO:root:2024-04-12 17:28:22, Train, Epoch : 5, Step : 2710, Loss : 0.44588, Acc : 0.809, Sensitive_Loss : 0.13922, Sensitive_Acc : 18.400, Run Time : 94.01 sec
INFO:root:2024-04-12 17:28:29, Train, Epoch : 5, Step : 2720, Loss : 0.39881, Acc : 0.850, Sensitive_Loss : 0.10532, Sensitive_Acc : 16.200, Run Time : 6.94 sec
INFO:root:2024-04-12 17:28:36, Train, Epoch : 5, Step : 2730, Loss : 0.40053, Acc : 0.809, Sensitive_Loss : 0.18167, Sensitive_Acc : 21.800, Run Time : 7.20 sec
INFO:root:2024-04-12 17:28:43, Train, Epoch : 5, Step : 2740, Loss : 0.35421, Acc : 0.825, Sensitive_Loss : 0.11884, Sensitive_Acc : 22.100, Run Time : 7.59 sec
INFO:root:2024-04-12 17:28:51, Train, Epoch : 5, Step : 2750, Loss : 0.42402, Acc : 0.794, Sensitive_Loss : 0.11227, Sensitive_Acc : 22.900, Run Time : 7.19 sec
INFO:root:2024-04-12 17:28:58, Train, Epoch : 5, Step : 2760, Loss : 0.32486, Acc : 0.853, Sensitive_Loss : 0.13044, Sensitive_Acc : 23.800, Run Time : 7.11 sec
INFO:root:2024-04-12 17:29:05, Train, Epoch : 5, Step : 2770, Loss : 0.37672, Acc : 0.831, Sensitive_Loss : 0.10840, Sensitive_Acc : 25.000, Run Time : 7.45 sec
INFO:root:2024-04-12 17:29:12, Train, Epoch : 5, Step : 2780, Loss : 0.43096, Acc : 0.816, Sensitive_Loss : 0.13962, Sensitive_Acc : 22.100, Run Time : 6.90 sec
INFO:root:2024-04-12 17:29:19, Train, Epoch : 5, Step : 2790, Loss : 0.41358, Acc : 0.831, Sensitive_Loss : 0.10917, Sensitive_Acc : 26.500, Run Time : 6.62 sec
INFO:root:2024-04-12 17:29:26, Train, Epoch : 5, Step : 2800, Loss : 0.42281, Acc : 0.831, Sensitive_Loss : 0.14380, Sensitive_Acc : 20.800, Run Time : 7.62 sec
INFO:root:2024-04-12 17:30:54, Dev, Step : 2800, Loss : 0.50080, Acc : 0.787, Auc : 0.860, Sensitive_Loss : 0.19170, Sensitive_Acc : 21.421, Sensitive_Auc : 0.997, Mean auc: 0.860, Run Time : 87.72 sec
INFO:root:2024-04-12 17:30:59, Train, Epoch : 5, Step : 2810, Loss : 0.45151, Acc : 0.784, Sensitive_Loss : 0.15070, Sensitive_Acc : 21.500, Run Time : 93.03 sec
INFO:root:2024-04-12 17:31:07, Train, Epoch : 5, Step : 2820, Loss : 0.41048, Acc : 0.834, Sensitive_Loss : 0.13179, Sensitive_Acc : 19.500, Run Time : 7.82 sec
INFO:root:2024-04-12 17:31:14, Train, Epoch : 5, Step : 2830, Loss : 0.40107, Acc : 0.831, Sensitive_Loss : 0.07173, Sensitive_Acc : 20.500, Run Time : 6.91 sec
INFO:root:2024-04-12 17:31:22, Train, Epoch : 5, Step : 2840, Loss : 0.43510, Acc : 0.797, Sensitive_Loss : 0.13862, Sensitive_Acc : 22.100, Run Time : 7.48 sec
INFO:root:2024-04-12 17:31:28, Train, Epoch : 5, Step : 2850, Loss : 0.52565, Acc : 0.781, Sensitive_Loss : 0.18856, Sensitive_Acc : 20.700, Run Time : 6.95 sec
INFO:root:2024-04-12 17:31:35, Train, Epoch : 5, Step : 2860, Loss : 0.39947, Acc : 0.819, Sensitive_Loss : 0.16660, Sensitive_Acc : 19.400, Run Time : 6.94 sec
INFO:root:2024-04-12 17:31:43, Train, Epoch : 5, Step : 2870, Loss : 0.40858, Acc : 0.819, Sensitive_Loss : 0.09263, Sensitive_Acc : 24.500, Run Time : 7.19 sec
INFO:root:2024-04-12 17:31:50, Train, Epoch : 5, Step : 2880, Loss : 0.36385, Acc : 0.838, Sensitive_Loss : 0.09297, Sensitive_Acc : 15.800, Run Time : 7.31 sec
INFO:root:2024-04-12 17:31:57, Train, Epoch : 5, Step : 2890, Loss : 0.46995, Acc : 0.787, Sensitive_Loss : 0.10946, Sensitive_Acc : 18.500, Run Time : 6.89 sec
INFO:root:2024-04-12 17:32:04, Train, Epoch : 5, Step : 2900, Loss : 0.36143, Acc : 0.819, Sensitive_Loss : 0.11637, Sensitive_Acc : 23.800, Run Time : 7.43 sec
INFO:root:2024-04-12 17:33:32, Dev, Step : 2900, Loss : 0.50241, Acc : 0.788, Auc : 0.860, Sensitive_Loss : 0.18396, Sensitive_Acc : 21.511, Sensitive_Auc : 0.998, Mean auc: 0.860, Run Time : 87.62 sec
INFO:root:2024-04-12 17:33:37, Train, Epoch : 5, Step : 2910, Loss : 0.37258, Acc : 0.828, Sensitive_Loss : 0.12485, Sensitive_Acc : 18.400, Run Time : 93.09 sec
INFO:root:2024-04-12 17:33:45, Train, Epoch : 5, Step : 2920, Loss : 0.44853, Acc : 0.794, Sensitive_Loss : 0.11752, Sensitive_Acc : 23.200, Run Time : 7.38 sec
INFO:root:2024-04-12 17:33:51, Train, Epoch : 5, Step : 2930, Loss : 0.37266, Acc : 0.828, Sensitive_Loss : 0.16211, Sensitive_Acc : 22.700, Run Time : 6.72 sec
INFO:root:2024-04-12 17:33:59, Train, Epoch : 5, Step : 2940, Loss : 0.33103, Acc : 0.872, Sensitive_Loss : 0.17139, Sensitive_Acc : 23.300, Run Time : 7.60 sec
INFO:root:2024-04-12 17:34:06, Train, Epoch : 5, Step : 2950, Loss : 0.38402, Acc : 0.850, Sensitive_Loss : 0.14259, Sensitive_Acc : 19.100, Run Time : 7.19 sec
INFO:root:2024-04-12 17:34:13, Train, Epoch : 5, Step : 2960, Loss : 0.38881, Acc : 0.828, Sensitive_Loss : 0.08186, Sensitive_Acc : 18.800, Run Time : 6.89 sec
INFO:root:2024-04-12 17:34:21, Train, Epoch : 5, Step : 2970, Loss : 0.47555, Acc : 0.800, Sensitive_Loss : 0.12532, Sensitive_Acc : 26.200, Run Time : 7.47 sec
INFO:root:2024-04-12 17:34:27, Train, Epoch : 5, Step : 2980, Loss : 0.31054, Acc : 0.872, Sensitive_Loss : 0.10871, Sensitive_Acc : 19.000, Run Time : 6.74 sec
INFO:root:2024-04-12 17:34:34, Train, Epoch : 5, Step : 2990, Loss : 0.43469, Acc : 0.819, Sensitive_Loss : 0.08047, Sensitive_Acc : 16.200, Run Time : 7.12 sec
INFO:root:2024-04-12 17:34:42, Train, Epoch : 5, Step : 3000, Loss : 0.44676, Acc : 0.812, Sensitive_Loss : 0.07375, Sensitive_Acc : 24.000, Run Time : 7.42 sec
INFO:root:2024-04-12 17:36:10, Dev, Step : 3000, Loss : 0.52085, Acc : 0.777, Auc : 0.854, Sensitive_Loss : 0.21456, Sensitive_Acc : 21.316, Sensitive_Auc : 0.997, Mean auc: 0.854, Run Time : 88.52 sec
INFO:root:2024-04-12 17:36:16, Train, Epoch : 5, Step : 3010, Loss : 0.39648, Acc : 0.834, Sensitive_Loss : 0.18049, Sensitive_Acc : 22.900, Run Time : 94.11 sec
INFO:root:2024-04-12 17:36:23, Train, Epoch : 5, Step : 3020, Loss : 0.41131, Acc : 0.828, Sensitive_Loss : 0.15869, Sensitive_Acc : 22.700, Run Time : 7.40 sec
INFO:root:2024-04-12 17:36:30, Train, Epoch : 5, Step : 3030, Loss : 0.44010, Acc : 0.772, Sensitive_Loss : 0.16567, Sensitive_Acc : 20.100, Run Time : 7.01 sec
INFO:root:2024-04-12 17:36:37, Train, Epoch : 5, Step : 3040, Loss : 0.36224, Acc : 0.863, Sensitive_Loss : 0.09574, Sensitive_Acc : 20.300, Run Time : 7.10 sec
INFO:root:2024-04-12 17:36:45, Train, Epoch : 5, Step : 3050, Loss : 0.35754, Acc : 0.838, Sensitive_Loss : 0.17854, Sensitive_Acc : 20.800, Run Time : 7.05 sec
INFO:root:2024-04-12 17:36:52, Train, Epoch : 5, Step : 3060, Loss : 0.36238, Acc : 0.850, Sensitive_Loss : 0.07026, Sensitive_Acc : 26.400, Run Time : 7.34 sec
INFO:root:2024-04-12 17:36:59, Train, Epoch : 5, Step : 3070, Loss : 0.40332, Acc : 0.828, Sensitive_Loss : 0.09276, Sensitive_Acc : 21.900, Run Time : 6.95 sec
INFO:root:2024-04-12 17:37:06, Train, Epoch : 5, Step : 3080, Loss : 0.39857, Acc : 0.847, Sensitive_Loss : 0.10258, Sensitive_Acc : 24.100, Run Time : 7.18 sec
INFO:root:2024-04-12 17:37:13, Train, Epoch : 5, Step : 3090, Loss : 0.39319, Acc : 0.806, Sensitive_Loss : 0.14525, Sensitive_Acc : 23.400, Run Time : 7.39 sec
INFO:root:2024-04-12 17:37:20, Train, Epoch : 5, Step : 3100, Loss : 0.39782, Acc : 0.812, Sensitive_Loss : 0.12529, Sensitive_Acc : 22.400, Run Time : 7.05 sec
INFO:root:2024-04-12 17:38:49, Dev, Step : 3100, Loss : 0.50410, Acc : 0.787, Auc : 0.859, Sensitive_Loss : 0.18068, Sensitive_Acc : 21.511, Sensitive_Auc : 0.997, Mean auc: 0.859, Run Time : 88.88 sec
INFO:root:2024-04-12 17:38:55, Train, Epoch : 5, Step : 3110, Loss : 0.41139, Acc : 0.834, Sensitive_Loss : 0.07179, Sensitive_Acc : 15.600, Run Time : 94.72 sec
INFO:root:2024-04-12 17:39:02, Train, Epoch : 5, Step : 3120, Loss : 0.36081, Acc : 0.866, Sensitive_Loss : 0.11023, Sensitive_Acc : 23.900, Run Time : 7.10 sec
INFO:root:2024-04-12 17:39:09, Train, Epoch : 5, Step : 3130, Loss : 0.35308, Acc : 0.841, Sensitive_Loss : 0.11553, Sensitive_Acc : 25.100, Run Time : 6.77 sec
INFO:root:2024-04-12 17:39:16, Train, Epoch : 5, Step : 3140, Loss : 0.43697, Acc : 0.791, Sensitive_Loss : 0.14470, Sensitive_Acc : 23.800, Run Time : 7.13 sec
INFO:root:2024-04-12 17:39:24, Train, Epoch : 5, Step : 3150, Loss : 0.36188, Acc : 0.834, Sensitive_Loss : 0.14194, Sensitive_Acc : 20.200, Run Time : 7.41 sec
INFO:root:2024-04-12 17:39:30, Train, Epoch : 5, Step : 3160, Loss : 0.38993, Acc : 0.863, Sensitive_Loss : 0.15194, Sensitive_Acc : 22.700, Run Time : 6.71 sec
INFO:root:2024-04-12 17:39:37, Train, Epoch : 5, Step : 3170, Loss : 0.37395, Acc : 0.850, Sensitive_Loss : 0.15922, Sensitive_Acc : 22.400, Run Time : 6.57 sec
INFO:root:2024-04-12 17:41:04
INFO:root:y_pred: [0.08684023 0.00412792 0.09862621 ... 0.20029715 0.04124613 0.16538669]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [3.59826512e-03 2.93150835e-04 3.11778635e-02 1.32135730e-02
 8.41561414e-05 2.82360869e-03 9.87772793e-02 5.92285441e-03
 3.45964819e-01 9.99970794e-01 4.77616526e-02 1.65187623e-04
 3.44217592e-03 2.09769074e-04 9.99430478e-01 1.30164716e-02
 9.04473197e-03 9.99511361e-01 9.98906136e-01 1.46904204e-03
 9.83350813e-01 9.17978818e-04 9.92485438e-04 6.91729784e-03
 2.63786852e-01 6.21926010e-01 4.19889557e-06 1.02032046e-03
 6.25078566e-04 1.49323987e-02 2.31219921e-02 9.40838575e-01
 1.60030629e-02 9.50529933e-01 2.12596939e-03 5.19822497e-05
 7.76690943e-03 3.75160538e-02 1.89012680e-02 5.05433157e-02
 1.92120615e-02 9.96847212e-01 8.95035570e-04 4.34447982e-04
 9.88363206e-01 2.20954731e-01 6.49970114e-01 3.93270522e-01
 4.49666157e-02 9.99540210e-01 9.94546056e-01 9.99536633e-01
 9.79969800e-01 5.52685047e-03 1.90688938e-01 8.87168348e-01
 2.97091808e-03 6.65573776e-03 9.99349773e-01 1.22393772e-03
 1.40371238e-04 6.95703551e-03 7.85387866e-03 2.53696140e-04
 9.99111474e-01 3.71438801e-01 2.25431620e-04 1.58395529e-01
 1.68166738e-02 9.94166732e-01 9.99773800e-01 9.99872804e-01
 8.07592878e-05 4.17357981e-01 4.56213159e-03 9.30053294e-01
 4.94265147e-02 3.88761291e-05 1.96365494e-04 1.67968813e-02
 1.11603402e-01 2.65947171e-03 9.99663591e-01 9.99754965e-01
 3.36274272e-03 3.05224489e-02 6.26123697e-02 3.17705089e-05
 2.14666389e-02 1.38212956e-04 1.55316433e-02 3.53696406e-01
 5.49652555e-04 1.49194995e-04 1.87256630e-03 1.19638536e-02
 2.08081023e-04 7.84767389e-01 4.75949310e-02 3.98804853e-03
 5.57854678e-03 7.24666752e-03 1.46730945e-01 2.08502356e-03
 7.06168544e-03 1.02360698e-03 5.06036319e-02 3.10102761e-01
 4.97099400e-01 7.51119992e-03 5.83530404e-04 9.99930263e-01
 9.99888539e-01 1.37882816e-04 1.23241544e-01 1.59249514e-01
 3.81892873e-03 5.83448447e-03 2.68967785e-02 4.59317211e-03
 5.63595677e-03 3.12079821e-04 1.04260586e-01 2.17836860e-04
 1.54414552e-03 9.55901563e-01 9.53291485e-04 9.99566972e-01
 1.39768943e-02 2.58136820e-02 1.48308827e-02 1.41615514e-02
 5.63810696e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 17:41:04, Dev, Step : 3170, Loss : 0.51895, Acc : 0.779, Auc : 0.859, Sensitive_Loss : 0.20347, Sensitive_Acc : 21.436, Sensitive_Auc : 0.998, Mean auc: 0.859, Run Time : 87.38 sec
INFO:root:2024-04-12 17:41:14, Train, Epoch : 6, Step : 3180, Loss : 0.38619, Acc : 0.844, Sensitive_Loss : 0.07840, Sensitive_Acc : 20.900, Run Time : 8.57 sec
INFO:root:2024-04-12 17:41:21, Train, Epoch : 6, Step : 3190, Loss : 0.34520, Acc : 0.844, Sensitive_Loss : 0.14632, Sensitive_Acc : 20.100, Run Time : 7.30 sec
INFO:root:2024-04-12 17:41:28, Train, Epoch : 6, Step : 3200, Loss : 0.38338, Acc : 0.844, Sensitive_Loss : 0.12401, Sensitive_Acc : 24.100, Run Time : 6.79 sec
INFO:root:2024-04-12 17:42:56, Dev, Step : 3200, Loss : 0.50789, Acc : 0.785, Auc : 0.860, Sensitive_Loss : 0.17780, Sensitive_Acc : 21.511, Sensitive_Auc : 0.998, Mean auc: 0.860, Run Time : 87.93 sec
INFO:root:2024-04-12 17:43:01, Train, Epoch : 6, Step : 3210, Loss : 0.38049, Acc : 0.841, Sensitive_Loss : 0.10069, Sensitive_Acc : 26.200, Run Time : 93.41 sec
INFO:root:2024-04-12 17:43:09, Train, Epoch : 6, Step : 3220, Loss : 0.41115, Acc : 0.828, Sensitive_Loss : 0.09401, Sensitive_Acc : 21.700, Run Time : 7.54 sec
INFO:root:2024-04-12 17:43:16, Train, Epoch : 6, Step : 3230, Loss : 0.34771, Acc : 0.831, Sensitive_Loss : 0.10010, Sensitive_Acc : 23.900, Run Time : 7.08 sec
INFO:root:2024-04-12 17:43:23, Train, Epoch : 6, Step : 3240, Loss : 0.35823, Acc : 0.872, Sensitive_Loss : 0.16550, Sensitive_Acc : 22.300, Run Time : 6.75 sec
INFO:root:2024-04-12 17:43:30, Train, Epoch : 6, Step : 3250, Loss : 0.37045, Acc : 0.856, Sensitive_Loss : 0.09422, Sensitive_Acc : 24.800, Run Time : 7.48 sec
INFO:root:2024-04-12 17:43:38, Train, Epoch : 6, Step : 3260, Loss : 0.33999, Acc : 0.834, Sensitive_Loss : 0.13202, Sensitive_Acc : 22.600, Run Time : 7.57 sec
INFO:root:2024-04-12 17:43:44, Train, Epoch : 6, Step : 3270, Loss : 0.36872, Acc : 0.850, Sensitive_Loss : 0.13017, Sensitive_Acc : 20.200, Run Time : 6.40 sec
INFO:root:2024-04-12 17:43:51, Train, Epoch : 6, Step : 3280, Loss : 0.36416, Acc : 0.825, Sensitive_Loss : 0.10010, Sensitive_Acc : 21.200, Run Time : 6.91 sec
INFO:root:2024-04-12 17:43:58, Train, Epoch : 6, Step : 3290, Loss : 0.30775, Acc : 0.863, Sensitive_Loss : 0.15302, Sensitive_Acc : 26.100, Run Time : 7.40 sec
INFO:root:2024-04-12 17:44:05, Train, Epoch : 6, Step : 3300, Loss : 0.36558, Acc : 0.822, Sensitive_Loss : 0.12746, Sensitive_Acc : 15.700, Run Time : 6.95 sec
INFO:root:2024-04-12 17:45:34, Dev, Step : 3300, Loss : 0.52398, Acc : 0.781, Auc : 0.856, Sensitive_Loss : 0.17880, Sensitive_Acc : 21.511, Sensitive_Auc : 0.998, Mean auc: 0.856, Run Time : 88.55 sec
INFO:root:2024-04-12 17:45:40, Train, Epoch : 6, Step : 3310, Loss : 0.43613, Acc : 0.812, Sensitive_Loss : 0.14769, Sensitive_Acc : 17.700, Run Time : 94.35 sec
INFO:root:2024-04-12 17:45:47, Train, Epoch : 6, Step : 3320, Loss : 0.32487, Acc : 0.838, Sensitive_Loss : 0.10492, Sensitive_Acc : 26.100, Run Time : 7.28 sec
INFO:root:2024-04-12 17:45:54, Train, Epoch : 6, Step : 3330, Loss : 0.36257, Acc : 0.853, Sensitive_Loss : 0.18067, Sensitive_Acc : 24.300, Run Time : 6.82 sec
INFO:root:2024-04-12 17:46:01, Train, Epoch : 6, Step : 3340, Loss : 0.34731, Acc : 0.838, Sensitive_Loss : 0.09135, Sensitive_Acc : 23.000, Run Time : 7.06 sec
INFO:root:2024-04-12 17:46:08, Train, Epoch : 6, Step : 3350, Loss : 0.39609, Acc : 0.819, Sensitive_Loss : 0.08208, Sensitive_Acc : 21.900, Run Time : 7.30 sec
INFO:root:2024-04-12 17:46:15, Train, Epoch : 6, Step : 3360, Loss : 0.32902, Acc : 0.869, Sensitive_Loss : 0.10561, Sensitive_Acc : 17.200, Run Time : 6.87 sec
INFO:root:2024-04-12 17:46:22, Train, Epoch : 6, Step : 3370, Loss : 0.38654, Acc : 0.838, Sensitive_Loss : 0.12250, Sensitive_Acc : 24.300, Run Time : 6.78 sec
INFO:root:2024-04-12 17:46:29, Train, Epoch : 6, Step : 3380, Loss : 0.34083, Acc : 0.859, Sensitive_Loss : 0.10427, Sensitive_Acc : 24.500, Run Time : 7.26 sec
INFO:root:2024-04-12 17:46:36, Train, Epoch : 6, Step : 3390, Loss : 0.39580, Acc : 0.816, Sensitive_Loss : 0.11534, Sensitive_Acc : 18.500, Run Time : 7.26 sec
INFO:root:2024-04-12 17:46:43, Train, Epoch : 6, Step : 3400, Loss : 0.38555, Acc : 0.853, Sensitive_Loss : 0.06069, Sensitive_Acc : 20.300, Run Time : 7.08 sec
INFO:root:2024-04-12 17:48:12, Dev, Step : 3400, Loss : 0.52556, Acc : 0.778, Auc : 0.856, Sensitive_Loss : 0.19150, Sensitive_Acc : 21.617, Sensitive_Auc : 0.999, Mean auc: 0.856, Run Time : 88.14 sec
INFO:root:2024-04-12 17:48:17, Train, Epoch : 6, Step : 3410, Loss : 0.36455, Acc : 0.800, Sensitive_Loss : 0.14371, Sensitive_Acc : 20.700, Run Time : 93.82 sec
INFO:root:2024-04-12 17:48:24, Train, Epoch : 6, Step : 3420, Loss : 0.33552, Acc : 0.847, Sensitive_Loss : 0.08460, Sensitive_Acc : 24.500, Run Time : 7.25 sec
INFO:root:2024-04-12 17:48:31, Train, Epoch : 6, Step : 3430, Loss : 0.41492, Acc : 0.831, Sensitive_Loss : 0.11476, Sensitive_Acc : 20.800, Run Time : 6.72 sec
INFO:root:2024-04-12 17:48:38, Train, Epoch : 6, Step : 3440, Loss : 0.37246, Acc : 0.872, Sensitive_Loss : 0.10182, Sensitive_Acc : 24.100, Run Time : 7.29 sec
INFO:root:2024-04-12 17:48:45, Train, Epoch : 6, Step : 3450, Loss : 0.48857, Acc : 0.803, Sensitive_Loss : 0.12400, Sensitive_Acc : 19.900, Run Time : 6.92 sec
INFO:root:2024-04-12 17:48:52, Train, Epoch : 6, Step : 3460, Loss : 0.33759, Acc : 0.853, Sensitive_Loss : 0.11136, Sensitive_Acc : 23.000, Run Time : 6.95 sec
INFO:root:2024-04-12 17:49:00, Train, Epoch : 6, Step : 3470, Loss : 0.36800, Acc : 0.850, Sensitive_Loss : 0.12230, Sensitive_Acc : 17.000, Run Time : 7.14 sec
INFO:root:2024-04-12 17:49:07, Train, Epoch : 6, Step : 3480, Loss : 0.40255, Acc : 0.794, Sensitive_Loss : 0.09060, Sensitive_Acc : 17.700, Run Time : 7.46 sec
INFO:root:2024-04-12 17:49:14, Train, Epoch : 6, Step : 3490, Loss : 0.40071, Acc : 0.853, Sensitive_Loss : 0.11851, Sensitive_Acc : 22.200, Run Time : 7.34 sec
INFO:root:2024-04-12 17:49:21, Train, Epoch : 6, Step : 3500, Loss : 0.36649, Acc : 0.859, Sensitive_Loss : 0.12790, Sensitive_Acc : 21.800, Run Time : 7.18 sec
INFO:root:2024-04-12 17:50:49, Dev, Step : 3500, Loss : 0.53507, Acc : 0.774, Auc : 0.854, Sensitive_Loss : 0.18468, Sensitive_Acc : 21.511, Sensitive_Auc : 0.997, Mean auc: 0.854, Run Time : 87.61 sec
INFO:root:2024-04-12 17:50:55, Train, Epoch : 6, Step : 3510, Loss : 0.47708, Acc : 0.791, Sensitive_Loss : 0.11418, Sensitive_Acc : 25.800, Run Time : 93.03 sec
INFO:root:2024-04-12 17:51:02, Train, Epoch : 6, Step : 3520, Loss : 0.33147, Acc : 0.863, Sensitive_Loss : 0.13523, Sensitive_Acc : 21.600, Run Time : 7.42 sec
INFO:root:2024-04-12 17:51:09, Train, Epoch : 6, Step : 3530, Loss : 0.42629, Acc : 0.806, Sensitive_Loss : 0.13850, Sensitive_Acc : 21.600, Run Time : 7.49 sec
INFO:root:2024-04-12 17:51:16, Train, Epoch : 6, Step : 3540, Loss : 0.28887, Acc : 0.878, Sensitive_Loss : 0.09823, Sensitive_Acc : 17.700, Run Time : 6.97 sec
INFO:root:2024-04-12 17:51:23, Train, Epoch : 6, Step : 3550, Loss : 0.38830, Acc : 0.809, Sensitive_Loss : 0.11461, Sensitive_Acc : 19.600, Run Time : 6.72 sec
INFO:root:2024-04-12 17:51:30, Train, Epoch : 6, Step : 3560, Loss : 0.40275, Acc : 0.838, Sensitive_Loss : 0.08596, Sensitive_Acc : 21.000, Run Time : 7.14 sec
INFO:root:2024-04-12 17:51:38, Train, Epoch : 6, Step : 3570, Loss : 0.35812, Acc : 0.806, Sensitive_Loss : 0.08371, Sensitive_Acc : 22.500, Run Time : 7.36 sec
INFO:root:2024-04-12 17:51:45, Train, Epoch : 6, Step : 3580, Loss : 0.37030, Acc : 0.841, Sensitive_Loss : 0.07837, Sensitive_Acc : 25.200, Run Time : 6.96 sec
INFO:root:2024-04-12 17:51:51, Train, Epoch : 6, Step : 3590, Loss : 0.40199, Acc : 0.819, Sensitive_Loss : 0.08407, Sensitive_Acc : 25.500, Run Time : 6.82 sec
INFO:root:2024-04-12 17:51:59, Train, Epoch : 6, Step : 3600, Loss : 0.37626, Acc : 0.816, Sensitive_Loss : 0.15417, Sensitive_Acc : 23.500, Run Time : 7.50 sec
INFO:root:2024-04-12 17:53:28, Dev, Step : 3600, Loss : 0.51285, Acc : 0.784, Auc : 0.859, Sensitive_Loss : 0.19251, Sensitive_Acc : 21.301, Sensitive_Auc : 0.999, Mean auc: 0.859, Run Time : 88.61 sec
INFO:root:2024-04-12 17:53:33, Train, Epoch : 6, Step : 3610, Loss : 0.32676, Acc : 0.869, Sensitive_Loss : 0.14046, Sensitive_Acc : 23.700, Run Time : 94.49 sec
INFO:root:2024-04-12 17:53:40, Train, Epoch : 6, Step : 3620, Loss : 0.33090, Acc : 0.850, Sensitive_Loss : 0.09942, Sensitive_Acc : 15.800, Run Time : 7.00 sec
INFO:root:2024-04-12 17:53:47, Train, Epoch : 6, Step : 3630, Loss : 0.35029, Acc : 0.834, Sensitive_Loss : 0.12886, Sensitive_Acc : 21.900, Run Time : 6.99 sec
INFO:root:2024-04-12 17:53:54, Train, Epoch : 6, Step : 3640, Loss : 0.36375, Acc : 0.853, Sensitive_Loss : 0.23503, Sensitive_Acc : 20.500, Run Time : 7.08 sec
INFO:root:2024-04-12 17:54:01, Train, Epoch : 6, Step : 3650, Loss : 0.32409, Acc : 0.841, Sensitive_Loss : 0.09190, Sensitive_Acc : 27.000, Run Time : 6.80 sec
INFO:root:2024-04-12 17:54:09, Train, Epoch : 6, Step : 3660, Loss : 0.30406, Acc : 0.878, Sensitive_Loss : 0.12807, Sensitive_Acc : 23.600, Run Time : 7.72 sec
INFO:root:2024-04-12 17:54:16, Train, Epoch : 6, Step : 3670, Loss : 0.37105, Acc : 0.853, Sensitive_Loss : 0.07373, Sensitive_Acc : 22.000, Run Time : 7.14 sec
INFO:root:2024-04-12 17:54:23, Train, Epoch : 6, Step : 3680, Loss : 0.41055, Acc : 0.838, Sensitive_Loss : 0.14850, Sensitive_Acc : 21.500, Run Time : 7.29 sec
INFO:root:2024-04-12 17:54:31, Train, Epoch : 6, Step : 3690, Loss : 0.40597, Acc : 0.809, Sensitive_Loss : 0.07625, Sensitive_Acc : 22.600, Run Time : 7.64 sec
INFO:root:2024-04-12 17:54:39, Train, Epoch : 6, Step : 3700, Loss : 0.41812, Acc : 0.838, Sensitive_Loss : 0.14796, Sensitive_Acc : 25.400, Run Time : 7.47 sec
INFO:root:2024-04-12 17:56:08, Dev, Step : 3700, Loss : 0.52829, Acc : 0.774, Auc : 0.856, Sensitive_Loss : 0.17569, Sensitive_Acc : 21.586, Sensitive_Auc : 0.998, Mean auc: 0.856, Run Time : 89.88 sec
INFO:root:2024-04-12 17:56:14, Train, Epoch : 6, Step : 3710, Loss : 0.41798, Acc : 0.812, Sensitive_Loss : 0.14631, Sensitive_Acc : 23.200, Run Time : 95.61 sec
INFO:root:2024-04-12 17:56:21, Train, Epoch : 6, Step : 3720, Loss : 0.38110, Acc : 0.844, Sensitive_Loss : 0.14534, Sensitive_Acc : 23.300, Run Time : 7.18 sec
INFO:root:2024-04-12 17:56:28, Train, Epoch : 6, Step : 3730, Loss : 0.38771, Acc : 0.850, Sensitive_Loss : 0.11954, Sensitive_Acc : 22.000, Run Time : 6.87 sec
INFO:root:2024-04-12 17:56:35, Train, Epoch : 6, Step : 3740, Loss : 0.37791, Acc : 0.841, Sensitive_Loss : 0.09106, Sensitive_Acc : 23.900, Run Time : 7.27 sec
INFO:root:2024-04-12 17:56:43, Train, Epoch : 6, Step : 3750, Loss : 0.36593, Acc : 0.838, Sensitive_Loss : 0.11523, Sensitive_Acc : 24.400, Run Time : 7.34 sec
INFO:root:2024-04-12 17:56:49, Train, Epoch : 6, Step : 3760, Loss : 0.40259, Acc : 0.787, Sensitive_Loss : 0.13179, Sensitive_Acc : 20.000, Run Time : 6.63 sec
INFO:root:2024-04-12 17:56:57, Train, Epoch : 6, Step : 3770, Loss : 0.40542, Acc : 0.844, Sensitive_Loss : 0.08761, Sensitive_Acc : 21.500, Run Time : 7.33 sec
INFO:root:2024-04-12 17:57:04, Train, Epoch : 6, Step : 3780, Loss : 0.40155, Acc : 0.819, Sensitive_Loss : 0.11001, Sensitive_Acc : 21.500, Run Time : 7.50 sec
INFO:root:2024-04-12 17:57:12, Train, Epoch : 6, Step : 3790, Loss : 0.30831, Acc : 0.844, Sensitive_Loss : 0.13016, Sensitive_Acc : 23.300, Run Time : 7.68 sec
INFO:root:2024-04-12 17:57:19, Train, Epoch : 6, Step : 3800, Loss : 0.41004, Acc : 0.828, Sensitive_Loss : 0.16426, Sensitive_Acc : 21.000, Run Time : 6.69 sec
INFO:root:2024-04-12 17:58:47, Dev, Step : 3800, Loss : 0.51825, Acc : 0.782, Auc : 0.858, Sensitive_Loss : 0.19853, Sensitive_Acc : 21.406, Sensitive_Auc : 0.998, Mean auc: 0.858, Run Time : 87.91 sec
INFO:root:2024-04-12 18:00:16
INFO:root:y_pred: [0.09295083 0.00367601 0.08042275 ... 0.190689   0.05074608 0.2325589 ]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [2.30660057e-03 1.04823281e-04 2.94838287e-02 6.97570946e-03
 1.00273668e-04 8.12905608e-04 4.64902222e-02 4.56251530e-03
 1.59885064e-01 9.99938250e-01 7.83154964e-02 3.06264410e-04
 8.02826893e-04 1.50951222e-04 9.99395370e-01 1.07108885e-02
 9.49498732e-03 9.99038815e-01 9.97745693e-01 1.83546403e-03
 9.66410458e-01 1.04917015e-03 1.03414255e-04 3.99543624e-03
 1.20613843e-01 4.22173053e-01 1.76118272e-06 7.18856521e-04
 7.94456340e-04 2.37188525e-02 9.33615956e-03 9.34184551e-01
 7.04447087e-03 9.52928007e-01 1.72114186e-03 7.98944093e-05
 3.55422776e-03 1.53647652e-02 4.71536703e-02 3.63879651e-02
 9.63196438e-03 9.96996164e-01 2.51108722e-04 1.64922589e-04
 9.85699236e-01 7.02322572e-02 4.36093956e-01 4.07213837e-01
 6.42746836e-02 9.99470174e-01 9.88840401e-01 9.99463856e-01
 9.82641637e-01 5.16148144e-03 6.05430715e-02 7.62222588e-01
 1.97328115e-03 2.13429285e-03 9.99476492e-01 1.62095786e-03
 1.07074753e-04 1.55515084e-02 1.83960535e-02 4.30994027e-04
 9.97824669e-01 3.50079864e-01 2.11662409e-04 1.50603458e-01
 6.81781396e-03 9.90073562e-01 9.99792874e-01 9.99703944e-01
 2.44309449e-05 3.22048813e-01 1.42164994e-03 9.42272604e-01
 4.10206094e-02 3.52697716e-05 1.08508197e-04 9.37885046e-03
 3.90990674e-02 2.01713433e-03 9.99578178e-01 9.99555051e-01
 2.80508143e-03 1.30380830e-02 3.68418247e-02 3.03002726e-05
 9.25100502e-03 1.30135013e-04 1.18692890e-02 1.60787329e-01
 1.55658898e-04 1.67168750e-04 2.04571709e-03 4.98722214e-03
 2.28596691e-04 7.47088373e-01 1.00808777e-01 6.21600868e-03
 1.75106770e-03 3.96049628e-03 8.62873793e-02 7.73515378e-04
 2.58297939e-03 5.31167840e-04 2.31868848e-02 1.74127713e-01
 4.58094478e-01 9.81788430e-03 8.92352778e-04 9.99940753e-01
 9.99821723e-01 1.09559340e-04 4.83471192e-02 1.08214714e-01
 1.25580980e-03 3.86808417e-03 2.42235083e-02 2.93747149e-03
 6.02721982e-03 2.76551524e-04 5.46244644e-02 4.30641026e-04
 8.02938594e-04 9.42531109e-01 6.08159637e-04 9.99507189e-01
 4.55624145e-03 2.26380471e-02 1.47390040e-02 1.24086533e-02
 6.89983310e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 18:00:16, Dev, Step : 3804, Loss : 0.52167, Acc : 0.784, Auc : 0.858, Sensitive_Loss : 0.18650, Sensitive_Acc : 21.692, Sensitive_Auc : 0.998, Mean auc: 0.858, Run Time : 88.15 sec
INFO:root:2024-04-12 18:00:22, Train, Epoch : 7, Step : 3810, Loss : 0.16023, Acc : 0.497, Sensitive_Loss : 0.04923, Sensitive_Acc : 11.000, Run Time : 5.38 sec
INFO:root:2024-04-12 18:00:30, Train, Epoch : 7, Step : 3820, Loss : 0.34473, Acc : 0.841, Sensitive_Loss : 0.16769, Sensitive_Acc : 24.300, Run Time : 7.21 sec
INFO:root:2024-04-12 18:00:37, Train, Epoch : 7, Step : 3830, Loss : 0.35332, Acc : 0.850, Sensitive_Loss : 0.09485, Sensitive_Acc : 21.000, Run Time : 7.46 sec
INFO:root:2024-04-12 18:00:44, Train, Epoch : 7, Step : 3840, Loss : 0.33231, Acc : 0.850, Sensitive_Loss : 0.06977, Sensitive_Acc : 24.700, Run Time : 7.13 sec
INFO:root:2024-04-12 18:00:51, Train, Epoch : 7, Step : 3850, Loss : 0.35628, Acc : 0.831, Sensitive_Loss : 0.14298, Sensitive_Acc : 24.400, Run Time : 7.35 sec
INFO:root:2024-04-12 18:00:59, Train, Epoch : 7, Step : 3860, Loss : 0.31860, Acc : 0.866, Sensitive_Loss : 0.15374, Sensitive_Acc : 20.800, Run Time : 7.08 sec
INFO:root:2024-04-12 18:01:05, Train, Epoch : 7, Step : 3870, Loss : 0.35157, Acc : 0.847, Sensitive_Loss : 0.08776, Sensitive_Acc : 23.000, Run Time : 6.93 sec
INFO:root:2024-04-12 18:01:13, Train, Epoch : 7, Step : 3880, Loss : 0.34610, Acc : 0.856, Sensitive_Loss : 0.09663, Sensitive_Acc : 19.400, Run Time : 7.19 sec
INFO:root:2024-04-12 18:01:20, Train, Epoch : 7, Step : 3890, Loss : 0.40998, Acc : 0.803, Sensitive_Loss : 0.11704, Sensitive_Acc : 17.900, Run Time : 7.22 sec
INFO:root:2024-04-12 18:01:27, Train, Epoch : 7, Step : 3900, Loss : 0.29962, Acc : 0.866, Sensitive_Loss : 0.10174, Sensitive_Acc : 14.800, Run Time : 6.88 sec
INFO:root:2024-04-12 18:02:55, Dev, Step : 3900, Loss : 0.53444, Acc : 0.777, Auc : 0.854, Sensitive_Loss : 0.18054, Sensitive_Acc : 21.692, Sensitive_Auc : 0.998, Mean auc: 0.854, Run Time : 88.53 sec
INFO:root:2024-04-12 18:03:01, Train, Epoch : 7, Step : 3910, Loss : 0.33585, Acc : 0.863, Sensitive_Loss : 0.11813, Sensitive_Acc : 18.000, Run Time : 94.72 sec
INFO:root:2024-04-12 18:03:08, Train, Epoch : 7, Step : 3920, Loss : 0.30820, Acc : 0.847, Sensitive_Loss : 0.10058, Sensitive_Acc : 23.100, Run Time : 6.66 sec
INFO:root:2024-04-12 18:03:15, Train, Epoch : 7, Step : 3930, Loss : 0.32711, Acc : 0.853, Sensitive_Loss : 0.09062, Sensitive_Acc : 17.900, Run Time : 7.23 sec
INFO:root:2024-04-12 18:03:23, Train, Epoch : 7, Step : 3940, Loss : 0.35509, Acc : 0.822, Sensitive_Loss : 0.14628, Sensitive_Acc : 20.000, Run Time : 7.33 sec
INFO:root:2024-04-12 18:03:30, Train, Epoch : 7, Step : 3950, Loss : 0.39886, Acc : 0.819, Sensitive_Loss : 0.12202, Sensitive_Acc : 19.600, Run Time : 6.98 sec
INFO:root:2024-04-12 18:03:37, Train, Epoch : 7, Step : 3960, Loss : 0.33073, Acc : 0.859, Sensitive_Loss : 0.12125, Sensitive_Acc : 24.300, Run Time : 7.33 sec
INFO:root:2024-04-12 18:03:44, Train, Epoch : 7, Step : 3970, Loss : 0.34131, Acc : 0.850, Sensitive_Loss : 0.08467, Sensitive_Acc : 20.400, Run Time : 6.87 sec
INFO:root:2024-04-12 18:03:52, Train, Epoch : 7, Step : 3980, Loss : 0.34565, Acc : 0.856, Sensitive_Loss : 0.11066, Sensitive_Acc : 22.700, Run Time : 7.66 sec
INFO:root:2024-04-12 18:03:58, Train, Epoch : 7, Step : 3990, Loss : 0.32263, Acc : 0.875, Sensitive_Loss : 0.14710, Sensitive_Acc : 26.500, Run Time : 6.93 sec
INFO:root:2024-04-12 18:04:05, Train, Epoch : 7, Step : 4000, Loss : 0.43064, Acc : 0.825, Sensitive_Loss : 0.10084, Sensitive_Acc : 24.100, Run Time : 6.71 sec
INFO:root:2024-04-12 18:05:34, Dev, Step : 4000, Loss : 0.52847, Acc : 0.778, Auc : 0.852, Sensitive_Loss : 0.19261, Sensitive_Acc : 21.511, Sensitive_Auc : 0.998, Mean auc: 0.852, Run Time : 89.20 sec
INFO:root:2024-04-12 18:05:40, Train, Epoch : 7, Step : 4010, Loss : 0.37238, Acc : 0.844, Sensitive_Loss : 0.12266, Sensitive_Acc : 21.300, Run Time : 94.77 sec
INFO:root:2024-04-12 18:05:47, Train, Epoch : 7, Step : 4020, Loss : 0.36669, Acc : 0.847, Sensitive_Loss : 0.11224, Sensitive_Acc : 21.500, Run Time : 7.32 sec
INFO:root:2024-04-12 18:05:55, Train, Epoch : 7, Step : 4030, Loss : 0.37457, Acc : 0.853, Sensitive_Loss : 0.12599, Sensitive_Acc : 22.000, Run Time : 7.55 sec
INFO:root:2024-04-12 18:06:02, Train, Epoch : 7, Step : 4040, Loss : 0.33467, Acc : 0.834, Sensitive_Loss : 0.07981, Sensitive_Acc : 22.200, Run Time : 7.22 sec
INFO:root:2024-04-12 18:06:09, Train, Epoch : 7, Step : 4050, Loss : 0.39443, Acc : 0.819, Sensitive_Loss : 0.12733, Sensitive_Acc : 20.500, Run Time : 7.32 sec
INFO:root:2024-04-12 18:06:17, Train, Epoch : 7, Step : 4060, Loss : 0.35204, Acc : 0.812, Sensitive_Loss : 0.08505, Sensitive_Acc : 21.100, Run Time : 7.54 sec
INFO:root:2024-04-12 18:06:23, Train, Epoch : 7, Step : 4070, Loss : 0.34147, Acc : 0.856, Sensitive_Loss : 0.06680, Sensitive_Acc : 23.200, Run Time : 6.31 sec
INFO:root:2024-04-12 18:06:31, Train, Epoch : 7, Step : 4080, Loss : 0.35720, Acc : 0.841, Sensitive_Loss : 0.12979, Sensitive_Acc : 23.000, Run Time : 7.42 sec
INFO:root:2024-04-12 18:06:38, Train, Epoch : 7, Step : 4090, Loss : 0.31295, Acc : 0.866, Sensitive_Loss : 0.10483, Sensitive_Acc : 23.700, Run Time : 7.67 sec
INFO:root:2024-04-12 18:06:45, Train, Epoch : 7, Step : 4100, Loss : 0.32714, Acc : 0.844, Sensitive_Loss : 0.13115, Sensitive_Acc : 17.600, Run Time : 6.73 sec
INFO:root:2024-04-12 18:08:14, Dev, Step : 4100, Loss : 0.52313, Acc : 0.780, Auc : 0.853, Sensitive_Loss : 0.17700, Sensitive_Acc : 21.692, Sensitive_Auc : 0.998, Mean auc: 0.853, Run Time : 89.02 sec
INFO:root:2024-04-12 18:08:19, Train, Epoch : 7, Step : 4110, Loss : 0.37774, Acc : 0.806, Sensitive_Loss : 0.11600, Sensitive_Acc : 20.700, Run Time : 94.25 sec
INFO:root:2024-04-12 18:08:27, Train, Epoch : 7, Step : 4120, Loss : 0.41093, Acc : 0.812, Sensitive_Loss : 0.07747, Sensitive_Acc : 24.100, Run Time : 7.48 sec
INFO:root:2024-04-12 18:08:34, Train, Epoch : 7, Step : 4130, Loss : 0.34190, Acc : 0.856, Sensitive_Loss : 0.09258, Sensitive_Acc : 21.700, Run Time : 7.13 sec
INFO:root:2024-04-12 18:08:41, Train, Epoch : 7, Step : 4140, Loss : 0.35862, Acc : 0.856, Sensitive_Loss : 0.09863, Sensitive_Acc : 25.000, Run Time : 7.02 sec
INFO:root:2024-04-12 18:08:48, Train, Epoch : 7, Step : 4150, Loss : 0.32884, Acc : 0.850, Sensitive_Loss : 0.09651, Sensitive_Acc : 25.100, Run Time : 7.35 sec
INFO:root:2024-04-12 18:08:56, Train, Epoch : 7, Step : 4160, Loss : 0.35129, Acc : 0.856, Sensitive_Loss : 0.11746, Sensitive_Acc : 19.800, Run Time : 7.29 sec
INFO:root:2024-04-12 18:09:03, Train, Epoch : 7, Step : 4170, Loss : 0.39985, Acc : 0.825, Sensitive_Loss : 0.13702, Sensitive_Acc : 25.700, Run Time : 7.29 sec
INFO:root:2024-04-12 18:09:10, Train, Epoch : 7, Step : 4180, Loss : 0.31537, Acc : 0.869, Sensitive_Loss : 0.11771, Sensitive_Acc : 22.600, Run Time : 7.34 sec
INFO:root:2024-04-12 18:09:17, Train, Epoch : 7, Step : 4190, Loss : 0.34264, Acc : 0.822, Sensitive_Loss : 0.08702, Sensitive_Acc : 20.200, Run Time : 7.01 sec
INFO:root:2024-04-12 18:09:24, Train, Epoch : 7, Step : 4200, Loss : 0.33890, Acc : 0.800, Sensitive_Loss : 0.12882, Sensitive_Acc : 23.300, Run Time : 6.93 sec
INFO:root:2024-04-12 18:10:52, Dev, Step : 4200, Loss : 0.55234, Acc : 0.772, Auc : 0.852, Sensitive_Loss : 0.19632, Sensitive_Acc : 21.511, Sensitive_Auc : 0.998, Mean auc: 0.852, Run Time : 87.55 sec
INFO:root:2024-04-12 18:10:57, Train, Epoch : 7, Step : 4210, Loss : 0.35321, Acc : 0.853, Sensitive_Loss : 0.08643, Sensitive_Acc : 21.000, Run Time : 93.11 sec
INFO:root:2024-04-12 18:11:05, Train, Epoch : 7, Step : 4220, Loss : 0.36679, Acc : 0.847, Sensitive_Loss : 0.12570, Sensitive_Acc : 23.100, Run Time : 7.91 sec
INFO:root:2024-04-12 18:11:12, Train, Epoch : 7, Step : 4230, Loss : 0.38407, Acc : 0.844, Sensitive_Loss : 0.09139, Sensitive_Acc : 21.200, Run Time : 6.61 sec
INFO:root:2024-04-12 18:11:19, Train, Epoch : 7, Step : 4240, Loss : 0.31910, Acc : 0.841, Sensitive_Loss : 0.12187, Sensitive_Acc : 23.900, Run Time : 7.42 sec
INFO:root:2024-04-12 18:11:26, Train, Epoch : 7, Step : 4250, Loss : 0.34915, Acc : 0.853, Sensitive_Loss : 0.12715, Sensitive_Acc : 22.000, Run Time : 6.50 sec
INFO:root:2024-04-12 18:11:33, Train, Epoch : 7, Step : 4260, Loss : 0.42651, Acc : 0.825, Sensitive_Loss : 0.05312, Sensitive_Acc : 23.600, Run Time : 7.39 sec
INFO:root:2024-04-12 18:11:40, Train, Epoch : 7, Step : 4270, Loss : 0.32779, Acc : 0.884, Sensitive_Loss : 0.07714, Sensitive_Acc : 21.400, Run Time : 7.03 sec
INFO:root:2024-04-12 18:11:48, Train, Epoch : 7, Step : 4280, Loss : 0.37763, Acc : 0.841, Sensitive_Loss : 0.14094, Sensitive_Acc : 20.900, Run Time : 7.39 sec
INFO:root:2024-04-12 18:11:54, Train, Epoch : 7, Step : 4290, Loss : 0.31612, Acc : 0.859, Sensitive_Loss : 0.10402, Sensitive_Acc : 18.500, Run Time : 6.93 sec
INFO:root:2024-04-12 18:12:02, Train, Epoch : 7, Step : 4300, Loss : 0.40068, Acc : 0.838, Sensitive_Loss : 0.09335, Sensitive_Acc : 23.700, Run Time : 7.31 sec
INFO:root:2024-04-12 18:13:30, Dev, Step : 4300, Loss : 0.53564, Acc : 0.781, Auc : 0.856, Sensitive_Loss : 0.18875, Sensitive_Acc : 21.511, Sensitive_Auc : 0.996, Mean auc: 0.856, Run Time : 88.02 sec
INFO:root:2024-04-12 18:13:36, Train, Epoch : 7, Step : 4310, Loss : 0.33697, Acc : 0.875, Sensitive_Loss : 0.07392, Sensitive_Acc : 17.400, Run Time : 93.92 sec
INFO:root:2024-04-12 18:13:42, Train, Epoch : 7, Step : 4320, Loss : 0.38075, Acc : 0.816, Sensitive_Loss : 0.09156, Sensitive_Acc : 21.700, Run Time : 6.72 sec
INFO:root:2024-04-12 18:13:50, Train, Epoch : 7, Step : 4330, Loss : 0.35366, Acc : 0.847, Sensitive_Loss : 0.08216, Sensitive_Acc : 23.100, Run Time : 7.63 sec
INFO:root:2024-04-12 18:13:57, Train, Epoch : 7, Step : 4340, Loss : 0.32925, Acc : 0.847, Sensitive_Loss : 0.10029, Sensitive_Acc : 22.600, Run Time : 7.24 sec
INFO:root:2024-04-12 18:14:04, Train, Epoch : 7, Step : 4350, Loss : 0.35255, Acc : 0.816, Sensitive_Loss : 0.09662, Sensitive_Acc : 17.500, Run Time : 7.19 sec
INFO:root:2024-04-12 18:14:11, Train, Epoch : 7, Step : 4360, Loss : 0.32186, Acc : 0.872, Sensitive_Loss : 0.13283, Sensitive_Acc : 17.700, Run Time : 6.92 sec
INFO:root:2024-04-12 18:14:19, Train, Epoch : 7, Step : 4370, Loss : 0.35722, Acc : 0.803, Sensitive_Loss : 0.13150, Sensitive_Acc : 23.300, Run Time : 7.19 sec
INFO:root:2024-04-12 18:14:25, Train, Epoch : 7, Step : 4380, Loss : 0.34349, Acc : 0.863, Sensitive_Loss : 0.14169, Sensitive_Acc : 22.600, Run Time : 6.62 sec
INFO:root:2024-04-12 18:14:32, Train, Epoch : 7, Step : 4390, Loss : 0.33511, Acc : 0.834, Sensitive_Loss : 0.10407, Sensitive_Acc : 22.000, Run Time : 7.11 sec
INFO:root:2024-04-12 18:14:39, Train, Epoch : 7, Step : 4400, Loss : 0.30565, Acc : 0.881, Sensitive_Loss : 0.11145, Sensitive_Acc : 21.400, Run Time : 7.02 sec
INFO:root:2024-04-12 18:16:08, Dev, Step : 4400, Loss : 0.54685, Acc : 0.779, Auc : 0.854, Sensitive_Loss : 0.17254, Sensitive_Acc : 21.797, Sensitive_Auc : 0.997, Mean auc: 0.854, Run Time : 88.64 sec
INFO:root:2024-04-12 18:16:14, Train, Epoch : 7, Step : 4410, Loss : 0.39598, Acc : 0.819, Sensitive_Loss : 0.07214, Sensitive_Acc : 21.200, Run Time : 94.27 sec
INFO:root:2024-04-12 18:16:21, Train, Epoch : 7, Step : 4420, Loss : 0.39928, Acc : 0.822, Sensitive_Loss : 0.08823, Sensitive_Acc : 17.200, Run Time : 7.44 sec
INFO:root:2024-04-12 18:16:28, Train, Epoch : 7, Step : 4430, Loss : 0.30525, Acc : 0.863, Sensitive_Loss : 0.08686, Sensitive_Acc : 23.900, Run Time : 7.04 sec
INFO:root:2024-04-12 18:18:00
INFO:root:y_pred: [0.04586869 0.00334307 0.15171412 ... 0.2927148  0.01781138 0.39565337]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [2.78590689e-03 2.14653250e-04 3.39347236e-02 2.34805327e-02
 1.13874645e-04 9.28697642e-04 3.35197598e-02 5.76174865e-03
 2.18421146e-01 9.99943137e-01 4.66912314e-02 1.52022432e-04
 9.10793489e-04 1.86718447e-04 9.99643445e-01 1.01855630e-02
 3.04753501e-02 9.99513149e-01 9.99386787e-01 3.91741842e-03
 9.83946323e-01 7.28490762e-04 5.64645961e-05 4.39926656e-03
 1.75334483e-01 4.66093749e-01 5.18782736e-06 2.27895752e-03
 1.92840805e-03 6.89026387e-03 1.04152560e-02 9.55284595e-01
 5.04031125e-03 9.78439629e-01 1.18780253e-03 1.22185174e-05
 6.40234072e-03 3.43316123e-02 3.62675004e-02 3.80079038e-02
 1.17330886e-02 9.98954773e-01 2.94102618e-04 1.63445671e-04
 9.93897498e-01 9.13376287e-02 6.75385356e-01 4.06226605e-01
 1.07087016e-01 9.99813735e-01 9.96289372e-01 9.99212980e-01
 9.66152310e-01 2.23512785e-03 9.98456851e-02 9.23502088e-01
 4.32580756e-03 3.52955121e-03 9.99700427e-01 1.10841077e-03
 6.02922591e-05 2.00223792e-02 6.18592976e-03 2.86144728e-04
 9.99174297e-01 4.48819458e-01 1.24146725e-04 3.30469847e-01
 8.16634949e-03 9.96187747e-01 9.99769151e-01 9.99899387e-01
 2.40296722e-05 3.99888039e-01 2.40592286e-03 9.75562215e-01
 4.04283851e-02 9.40663995e-06 3.97217991e-05 1.99905746e-02
 4.18676510e-02 1.12153054e-03 9.99764144e-01 9.99899626e-01
 2.37217033e-03 1.03085386e-02 5.31159341e-02 1.95821649e-05
 1.55870942e-02 3.92571746e-05 1.50050130e-02 2.57583290e-01
 2.64827278e-04 6.51144219e-05 1.61391008e-03 2.60852790e-03
 1.02077589e-04 6.69999301e-01 5.73032387e-02 5.27378125e-03
 2.70049018e-03 9.71961860e-03 1.20890006e-01 3.86413769e-04
 3.41052539e-03 1.28278916e-03 5.09991646e-02 2.65067995e-01
 5.14112473e-01 7.49116996e-03 4.62492026e-04 9.99979734e-01
 9.99948740e-01 7.77676687e-05 1.18775375e-01 1.48054674e-01
 2.17919145e-03 4.20008972e-03 3.88232023e-02 3.00877122e-03
 2.97410204e-03 2.04074109e-04 3.89846005e-02 3.86307511e-04
 1.24408293e-03 9.64748859e-01 5.73649129e-04 9.99840736e-01
 6.71663089e-04 2.90745925e-02 1.57750081e-02 3.84146944e-02
 6.00997300e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 18:18:00, Dev, Step : 4438, Loss : 0.52254, Acc : 0.789, Auc : 0.857, Sensitive_Loss : 0.19698, Sensitive_Acc : 21.406, Sensitive_Auc : 0.997, Mean auc: 0.857, Run Time : 87.19 sec
INFO:root:2024-04-12 18:18:04, Train, Epoch : 8, Step : 4440, Loss : 0.06243, Acc : 0.181, Sensitive_Loss : 0.00621, Sensitive_Acc : 3.300, Run Time : 2.73 sec
INFO:root:2024-04-12 18:18:11, Train, Epoch : 8, Step : 4450, Loss : 0.33966, Acc : 0.866, Sensitive_Loss : 0.09631, Sensitive_Acc : 21.100, Run Time : 6.82 sec
INFO:root:2024-04-12 18:18:18, Train, Epoch : 8, Step : 4460, Loss : 0.30652, Acc : 0.891, Sensitive_Loss : 0.06000, Sensitive_Acc : 17.100, Run Time : 7.16 sec
INFO:root:2024-04-12 18:18:25, Train, Epoch : 8, Step : 4470, Loss : 0.33525, Acc : 0.856, Sensitive_Loss : 0.07415, Sensitive_Acc : 17.400, Run Time : 7.44 sec
INFO:root:2024-04-12 18:18:33, Train, Epoch : 8, Step : 4480, Loss : 0.28548, Acc : 0.881, Sensitive_Loss : 0.10129, Sensitive_Acc : 19.900, Run Time : 7.44 sec
INFO:root:2024-04-12 18:18:40, Train, Epoch : 8, Step : 4490, Loss : 0.29262, Acc : 0.859, Sensitive_Loss : 0.05789, Sensitive_Acc : 17.000, Run Time : 7.22 sec
INFO:root:2024-04-12 18:18:47, Train, Epoch : 8, Step : 4500, Loss : 0.32767, Acc : 0.866, Sensitive_Loss : 0.09853, Sensitive_Acc : 24.200, Run Time : 7.10 sec
INFO:root:2024-04-12 18:20:15, Dev, Step : 4500, Loss : 0.54406, Acc : 0.778, Auc : 0.856, Sensitive_Loss : 0.18713, Sensitive_Acc : 21.511, Sensitive_Auc : 0.997, Mean auc: 0.856, Run Time : 87.49 sec
INFO:root:2024-04-12 18:20:20, Train, Epoch : 8, Step : 4510, Loss : 0.28776, Acc : 0.863, Sensitive_Loss : 0.09819, Sensitive_Acc : 20.300, Run Time : 92.98 sec
INFO:root:2024-04-12 18:20:28, Train, Epoch : 8, Step : 4520, Loss : 0.31529, Acc : 0.859, Sensitive_Loss : 0.10824, Sensitive_Acc : 24.200, Run Time : 7.55 sec
INFO:root:2024-04-12 18:20:35, Train, Epoch : 8, Step : 4530, Loss : 0.39024, Acc : 0.850, Sensitive_Loss : 0.10433, Sensitive_Acc : 20.000, Run Time : 7.19 sec
INFO:root:2024-04-12 18:20:42, Train, Epoch : 8, Step : 4540, Loss : 0.32892, Acc : 0.866, Sensitive_Loss : 0.07356, Sensitive_Acc : 21.100, Run Time : 6.79 sec
INFO:root:2024-04-12 18:20:49, Train, Epoch : 8, Step : 4550, Loss : 0.26742, Acc : 0.897, Sensitive_Loss : 0.11872, Sensitive_Acc : 18.300, Run Time : 7.58 sec
INFO:root:2024-04-12 18:20:56, Train, Epoch : 8, Step : 4560, Loss : 0.32841, Acc : 0.859, Sensitive_Loss : 0.09588, Sensitive_Acc : 19.800, Run Time : 6.71 sec
INFO:root:2024-04-12 18:21:03, Train, Epoch : 8, Step : 4570, Loss : 0.38632, Acc : 0.841, Sensitive_Loss : 0.09153, Sensitive_Acc : 23.100, Run Time : 7.05 sec
INFO:root:2024-04-12 18:21:10, Train, Epoch : 8, Step : 4580, Loss : 0.30811, Acc : 0.866, Sensitive_Loss : 0.12028, Sensitive_Acc : 20.000, Run Time : 7.25 sec
INFO:root:2024-04-12 18:21:18, Train, Epoch : 8, Step : 4590, Loss : 0.32935, Acc : 0.853, Sensitive_Loss : 0.10351, Sensitive_Acc : 18.000, Run Time : 7.26 sec
INFO:root:2024-04-12 18:21:25, Train, Epoch : 8, Step : 4600, Loss : 0.33378, Acc : 0.859, Sensitive_Loss : 0.09703, Sensitive_Acc : 22.700, Run Time : 7.48 sec
INFO:root:2024-04-12 18:22:53, Dev, Step : 4600, Loss : 0.54089, Acc : 0.779, Auc : 0.853, Sensitive_Loss : 0.19585, Sensitive_Acc : 21.617, Sensitive_Auc : 0.998, Mean auc: 0.853, Run Time : 88.12 sec
INFO:root:2024-04-12 18:22:59, Train, Epoch : 8, Step : 4610, Loss : 0.33852, Acc : 0.847, Sensitive_Loss : 0.08065, Sensitive_Acc : 21.800, Run Time : 93.81 sec
INFO:root:2024-04-12 18:23:06, Train, Epoch : 8, Step : 4620, Loss : 0.34595, Acc : 0.853, Sensitive_Loss : 0.07473, Sensitive_Acc : 15.700, Run Time : 7.17 sec
INFO:root:2024-04-12 18:23:13, Train, Epoch : 8, Step : 4630, Loss : 0.30410, Acc : 0.881, Sensitive_Loss : 0.10598, Sensitive_Acc : 23.900, Run Time : 7.24 sec
INFO:root:2024-04-12 18:23:20, Train, Epoch : 8, Step : 4640, Loss : 0.34680, Acc : 0.850, Sensitive_Loss : 0.09027, Sensitive_Acc : 24.700, Run Time : 7.06 sec
INFO:root:2024-04-12 18:23:27, Train, Epoch : 8, Step : 4650, Loss : 0.31887, Acc : 0.850, Sensitive_Loss : 0.19253, Sensitive_Acc : 26.200, Run Time : 6.99 sec
INFO:root:2024-04-12 18:23:35, Train, Epoch : 8, Step : 4660, Loss : 0.29449, Acc : 0.891, Sensitive_Loss : 0.07751, Sensitive_Acc : 21.100, Run Time : 7.36 sec
INFO:root:2024-04-12 18:23:41, Train, Epoch : 8, Step : 4670, Loss : 0.29245, Acc : 0.884, Sensitive_Loss : 0.10305, Sensitive_Acc : 24.100, Run Time : 6.88 sec
INFO:root:2024-04-12 18:23:48, Train, Epoch : 8, Step : 4680, Loss : 0.37391, Acc : 0.847, Sensitive_Loss : 0.07474, Sensitive_Acc : 18.100, Run Time : 6.84 sec
INFO:root:2024-04-12 18:23:56, Train, Epoch : 8, Step : 4690, Loss : 0.33663, Acc : 0.844, Sensitive_Loss : 0.08656, Sensitive_Acc : 23.000, Run Time : 7.50 sec
INFO:root:2024-04-12 18:24:03, Train, Epoch : 8, Step : 4700, Loss : 0.35041, Acc : 0.844, Sensitive_Loss : 0.13961, Sensitive_Acc : 23.900, Run Time : 6.92 sec
INFO:root:2024-04-12 18:25:31, Dev, Step : 4700, Loss : 0.54161, Acc : 0.777, Auc : 0.851, Sensitive_Loss : 0.18786, Sensitive_Acc : 21.692, Sensitive_Auc : 0.998, Mean auc: 0.851, Run Time : 88.34 sec
INFO:root:2024-04-12 18:25:37, Train, Epoch : 8, Step : 4710, Loss : 0.34267, Acc : 0.863, Sensitive_Loss : 0.13997, Sensitive_Acc : 20.900, Run Time : 94.03 sec
INFO:root:2024-04-12 18:25:44, Train, Epoch : 8, Step : 4720, Loss : 0.30119, Acc : 0.878, Sensitive_Loss : 0.16210, Sensitive_Acc : 25.100, Run Time : 7.61 sec
INFO:root:2024-04-12 18:25:51, Train, Epoch : 8, Step : 4730, Loss : 0.31538, Acc : 0.878, Sensitive_Loss : 0.12534, Sensitive_Acc : 24.000, Run Time : 6.47 sec
INFO:root:2024-04-12 18:25:58, Train, Epoch : 8, Step : 4740, Loss : 0.29297, Acc : 0.891, Sensitive_Loss : 0.08698, Sensitive_Acc : 23.000, Run Time : 7.37 sec
INFO:root:2024-04-12 18:26:05, Train, Epoch : 8, Step : 4750, Loss : 0.33998, Acc : 0.847, Sensitive_Loss : 0.12895, Sensitive_Acc : 25.300, Run Time : 7.25 sec
INFO:root:2024-04-12 18:26:12, Train, Epoch : 8, Step : 4760, Loss : 0.38482, Acc : 0.859, Sensitive_Loss : 0.13433, Sensitive_Acc : 18.900, Run Time : 6.85 sec
INFO:root:2024-04-12 18:26:20, Train, Epoch : 8, Step : 4770, Loss : 0.30818, Acc : 0.881, Sensitive_Loss : 0.12680, Sensitive_Acc : 24.000, Run Time : 7.21 sec
INFO:root:2024-04-12 18:26:27, Train, Epoch : 8, Step : 4780, Loss : 0.32954, Acc : 0.878, Sensitive_Loss : 0.06685, Sensitive_Acc : 18.100, Run Time : 7.40 sec
INFO:root:2024-04-12 18:26:33, Train, Epoch : 8, Step : 4790, Loss : 0.31512, Acc : 0.847, Sensitive_Loss : 0.13237, Sensitive_Acc : 24.100, Run Time : 6.47 sec
INFO:root:2024-04-12 18:26:41, Train, Epoch : 8, Step : 4800, Loss : 0.33417, Acc : 0.866, Sensitive_Loss : 0.09654, Sensitive_Acc : 20.000, Run Time : 7.42 sec
INFO:root:2024-04-12 18:28:09, Dev, Step : 4800, Loss : 0.56429, Acc : 0.774, Auc : 0.853, Sensitive_Loss : 0.17389, Sensitive_Acc : 21.586, Sensitive_Auc : 0.997, Mean auc: 0.853, Run Time : 88.20 sec
INFO:root:2024-04-12 18:28:15, Train, Epoch : 8, Step : 4810, Loss : 0.40480, Acc : 0.831, Sensitive_Loss : 0.11501, Sensitive_Acc : 16.200, Run Time : 93.94 sec
INFO:root:2024-04-12 18:28:22, Train, Epoch : 8, Step : 4820, Loss : 0.34147, Acc : 0.847, Sensitive_Loss : 0.07408, Sensitive_Acc : 24.400, Run Time : 7.47 sec
INFO:root:2024-04-12 18:28:29, Train, Epoch : 8, Step : 4830, Loss : 0.32695, Acc : 0.875, Sensitive_Loss : 0.07140, Sensitive_Acc : 23.100, Run Time : 6.58 sec
INFO:root:2024-04-12 18:28:36, Train, Epoch : 8, Step : 4840, Loss : 0.30758, Acc : 0.878, Sensitive_Loss : 0.08710, Sensitive_Acc : 20.800, Run Time : 6.99 sec
INFO:root:2024-04-12 18:28:43, Train, Epoch : 8, Step : 4850, Loss : 0.29100, Acc : 0.887, Sensitive_Loss : 0.13379, Sensitive_Acc : 22.200, Run Time : 7.47 sec
INFO:root:2024-04-12 18:28:50, Train, Epoch : 8, Step : 4860, Loss : 0.31534, Acc : 0.881, Sensitive_Loss : 0.09353, Sensitive_Acc : 20.400, Run Time : 6.87 sec
INFO:root:2024-04-12 18:28:58, Train, Epoch : 8, Step : 4870, Loss : 0.36737, Acc : 0.834, Sensitive_Loss : 0.09670, Sensitive_Acc : 21.200, Run Time : 7.84 sec
INFO:root:2024-04-12 18:29:05, Train, Epoch : 8, Step : 4880, Loss : 0.36432, Acc : 0.831, Sensitive_Loss : 0.07205, Sensitive_Acc : 24.200, Run Time : 6.84 sec
INFO:root:2024-04-12 18:29:12, Train, Epoch : 8, Step : 4890, Loss : 0.32850, Acc : 0.825, Sensitive_Loss : 0.07926, Sensitive_Acc : 18.200, Run Time : 6.78 sec
INFO:root:2024-04-12 18:29:19, Train, Epoch : 8, Step : 4900, Loss : 0.31435, Acc : 0.875, Sensitive_Loss : 0.07598, Sensitive_Acc : 19.600, Run Time : 7.08 sec
INFO:root:2024-04-12 18:30:48, Dev, Step : 4900, Loss : 0.56167, Acc : 0.770, Auc : 0.852, Sensitive_Loss : 0.21656, Sensitive_Acc : 21.165, Sensitive_Auc : 0.997, Mean auc: 0.852, Run Time : 88.83 sec
INFO:root:2024-04-12 18:30:53, Train, Epoch : 8, Step : 4910, Loss : 0.24886, Acc : 0.897, Sensitive_Loss : 0.14651, Sensitive_Acc : 19.300, Run Time : 94.68 sec
INFO:root:2024-04-12 18:31:01, Train, Epoch : 8, Step : 4920, Loss : 0.42256, Acc : 0.797, Sensitive_Loss : 0.10803, Sensitive_Acc : 19.700, Run Time : 7.15 sec
INFO:root:2024-04-12 18:31:08, Train, Epoch : 8, Step : 4930, Loss : 0.33421, Acc : 0.847, Sensitive_Loss : 0.11799, Sensitive_Acc : 21.600, Run Time : 7.25 sec
INFO:root:2024-04-12 18:31:14, Train, Epoch : 8, Step : 4940, Loss : 0.36138, Acc : 0.872, Sensitive_Loss : 0.11291, Sensitive_Acc : 20.800, Run Time : 6.51 sec
INFO:root:2024-04-12 18:31:22, Train, Epoch : 8, Step : 4950, Loss : 0.24081, Acc : 0.878, Sensitive_Loss : 0.09360, Sensitive_Acc : 18.900, Run Time : 7.32 sec
INFO:root:2024-04-12 18:31:29, Train, Epoch : 8, Step : 4960, Loss : 0.28593, Acc : 0.906, Sensitive_Loss : 0.06318, Sensitive_Acc : 22.300, Run Time : 7.13 sec
INFO:root:2024-04-12 18:31:36, Train, Epoch : 8, Step : 4970, Loss : 0.30831, Acc : 0.847, Sensitive_Loss : 0.10015, Sensitive_Acc : 19.200, Run Time : 7.15 sec
INFO:root:2024-04-12 18:31:43, Train, Epoch : 8, Step : 4980, Loss : 0.39601, Acc : 0.809, Sensitive_Loss : 0.06058, Sensitive_Acc : 16.200, Run Time : 7.09 sec
INFO:root:2024-04-12 18:31:50, Train, Epoch : 8, Step : 4990, Loss : 0.38926, Acc : 0.841, Sensitive_Loss : 0.08191, Sensitive_Acc : 20.800, Run Time : 6.86 sec
INFO:root:2024-04-12 18:31:57, Train, Epoch : 8, Step : 5000, Loss : 0.37533, Acc : 0.847, Sensitive_Loss : 0.10941, Sensitive_Acc : 22.200, Run Time : 7.49 sec
INFO:root:2024-04-12 18:33:25, Dev, Step : 5000, Loss : 0.54960, Acc : 0.770, Auc : 0.849, Sensitive_Loss : 0.17780, Sensitive_Acc : 21.692, Sensitive_Auc : 0.996, Mean auc: 0.849, Run Time : 87.46 sec
INFO:root:2024-04-12 18:33:30, Train, Epoch : 8, Step : 5010, Loss : 0.36225, Acc : 0.866, Sensitive_Loss : 0.07634, Sensitive_Acc : 21.900, Run Time : 93.14 sec
INFO:root:2024-04-12 18:33:38, Train, Epoch : 8, Step : 5020, Loss : 0.29940, Acc : 0.884, Sensitive_Loss : 0.06973, Sensitive_Acc : 22.600, Run Time : 7.25 sec
INFO:root:2024-04-12 18:33:45, Train, Epoch : 8, Step : 5030, Loss : 0.40885, Acc : 0.806, Sensitive_Loss : 0.10704, Sensitive_Acc : 21.000, Run Time : 6.95 sec
INFO:root:2024-04-12 18:33:52, Train, Epoch : 8, Step : 5040, Loss : 0.29923, Acc : 0.875, Sensitive_Loss : 0.11504, Sensitive_Acc : 23.000, Run Time : 6.95 sec
INFO:root:2024-04-12 18:33:59, Train, Epoch : 8, Step : 5050, Loss : 0.31671, Acc : 0.825, Sensitive_Loss : 0.10695, Sensitive_Acc : 17.900, Run Time : 7.08 sec
INFO:root:2024-04-12 18:34:06, Train, Epoch : 8, Step : 5060, Loss : 0.29736, Acc : 0.897, Sensitive_Loss : 0.13612, Sensitive_Acc : 19.500, Run Time : 7.28 sec
INFO:root:2024-04-12 18:34:13, Train, Epoch : 8, Step : 5070, Loss : 0.29654, Acc : 0.863, Sensitive_Loss : 0.12584, Sensitive_Acc : 19.800, Run Time : 6.99 sec
INFO:root:2024-04-12 18:35:42
INFO:root:y_pred: [0.03478943 0.00194045 0.19297278 ... 0.34393337 0.01904944 0.31159645]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [4.02648048e-03 1.31659894e-04 4.72468808e-02 1.91416554e-02
 4.87753859e-05 2.11008155e-04 3.90240029e-02 2.33614375e-03
 2.31758773e-01 9.99931693e-01 3.46168838e-02 1.17690935e-04
 1.71068253e-03 1.79695635e-04 9.99676943e-01 7.56189227e-03
 1.60169378e-02 9.99396920e-01 9.99297023e-01 2.68656434e-03
 9.87477541e-01 8.90360738e-04 1.33476628e-04 3.23152589e-03
 2.31326178e-01 3.17485124e-01 3.93075061e-06 2.72685825e-03
 1.35587598e-03 1.15675991e-02 3.87056195e-03 9.67564285e-01
 5.51486248e-03 9.73949552e-01 1.11435645e-03 1.76279923e-06
 3.24157742e-03 3.60538699e-02 8.89005698e-03 3.49104740e-02
 6.36603124e-03 9.97024953e-01 1.75228866e-04 9.07590220e-05
 9.89648402e-01 9.53980088e-02 2.31824666e-01 2.30144680e-01
 9.03951228e-02 9.99867797e-01 9.96186197e-01 9.99510169e-01
 8.33333790e-01 7.96190754e-04 7.74831027e-02 9.22675431e-01
 9.12789721e-03 4.36801044e-03 9.99550998e-01 1.59887958e-03
 3.44643449e-05 1.90223008e-02 8.02278705e-03 1.43920086e-04
 9.98914599e-01 4.17109758e-01 9.13925833e-05 1.07995853e-01
 6.31866883e-03 9.94331956e-01 9.99555409e-01 9.99878645e-01
 3.44848922e-05 4.02447134e-01 1.22999854e-03 9.80872631e-01
 4.54549678e-02 1.30942244e-05 2.93102530e-05 6.18554652e-03
 2.14591790e-02 3.65653890e-03 9.99567211e-01 9.99895453e-01
 1.95557578e-03 4.63916361e-03 5.60046211e-02 1.20426666e-05
 1.72900055e-02 2.91806755e-05 1.56032192e-02 3.87621999e-01
 1.66509490e-04 5.93536606e-05 1.67858772e-04 1.23383268e-03
 7.10507011e-05 7.01866686e-01 3.02643273e-02 2.99516460e-03
 8.75136990e-04 4.72734170e-03 1.27128348e-01 1.15639355e-04
 1.14719232e-03 6.95587369e-04 2.11946443e-02 1.88276559e-01
 5.05074501e-01 4.93111322e-03 6.20304665e-04 9.99975681e-01
 9.99928355e-01 6.39801510e-05 5.81587516e-02 9.66213569e-02
 9.17951693e-04 2.55857711e-03 2.78927572e-02 2.17137183e-03
 3.41475662e-03 2.41309637e-04 3.36965360e-02 1.19818200e-04
 8.47314252e-04 9.62529004e-01 1.80566436e-04 9.99799192e-01
 5.96536323e-04 2.60083340e-02 9.47879814e-03 2.01248676e-02
 5.08141762e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 18:35:42, Dev, Step : 5072, Loss : 0.54705, Acc : 0.773, Auc : 0.851, Sensitive_Loss : 0.18782, Sensitive_Acc : 21.586, Sensitive_Auc : 0.997, Mean auc: 0.851, Run Time : 87.49 sec
INFO:root:2024-04-12 18:35:49, Train, Epoch : 9, Step : 5080, Loss : 0.21823, Acc : 0.703, Sensitive_Loss : 0.13079, Sensitive_Acc : 16.300, Run Time : 6.75 sec
INFO:root:2024-04-12 18:35:56, Train, Epoch : 9, Step : 5090, Loss : 0.29778, Acc : 0.856, Sensitive_Loss : 0.06079, Sensitive_Acc : 21.200, Run Time : 6.87 sec
INFO:root:2024-04-12 18:36:04, Train, Epoch : 9, Step : 5100, Loss : 0.27457, Acc : 0.897, Sensitive_Loss : 0.08120, Sensitive_Acc : 22.800, Run Time : 7.77 sec
INFO:root:2024-04-12 18:37:32, Dev, Step : 5100, Loss : 0.55373, Acc : 0.778, Auc : 0.852, Sensitive_Loss : 0.18505, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.852, Run Time : 88.21 sec
INFO:root:2024-04-12 18:37:37, Train, Epoch : 9, Step : 5110, Loss : 0.33239, Acc : 0.872, Sensitive_Loss : 0.13616, Sensitive_Acc : 24.900, Run Time : 93.53 sec
INFO:root:2024-04-12 18:37:45, Train, Epoch : 9, Step : 5120, Loss : 0.30500, Acc : 0.853, Sensitive_Loss : 0.12979, Sensitive_Acc : 25.300, Run Time : 7.44 sec
INFO:root:2024-04-12 18:37:52, Train, Epoch : 9, Step : 5130, Loss : 0.32880, Acc : 0.853, Sensitive_Loss : 0.11188, Sensitive_Acc : 17.900, Run Time : 7.12 sec
INFO:root:2024-04-12 18:37:59, Train, Epoch : 9, Step : 5140, Loss : 0.31952, Acc : 0.856, Sensitive_Loss : 0.15319, Sensitive_Acc : 20.200, Run Time : 6.90 sec
INFO:root:2024-04-12 18:38:06, Train, Epoch : 9, Step : 5150, Loss : 0.28340, Acc : 0.891, Sensitive_Loss : 0.08176, Sensitive_Acc : 24.800, Run Time : 7.28 sec
INFO:root:2024-04-12 18:38:13, Train, Epoch : 9, Step : 5160, Loss : 0.32301, Acc : 0.878, Sensitive_Loss : 0.08139, Sensitive_Acc : 22.600, Run Time : 7.18 sec
INFO:root:2024-04-12 18:38:21, Train, Epoch : 9, Step : 5170, Loss : 0.34702, Acc : 0.844, Sensitive_Loss : 0.10914, Sensitive_Acc : 20.800, Run Time : 7.54 sec
INFO:root:2024-04-12 18:38:28, Train, Epoch : 9, Step : 5180, Loss : 0.35972, Acc : 0.844, Sensitive_Loss : 0.07245, Sensitive_Acc : 15.800, Run Time : 7.13 sec
INFO:root:2024-04-12 18:38:35, Train, Epoch : 9, Step : 5190, Loss : 0.31334, Acc : 0.878, Sensitive_Loss : 0.06827, Sensitive_Acc : 20.600, Run Time : 6.76 sec
INFO:root:2024-04-12 18:38:42, Train, Epoch : 9, Step : 5200, Loss : 0.25998, Acc : 0.884, Sensitive_Loss : 0.11243, Sensitive_Acc : 21.700, Run Time : 7.13 sec
INFO:root:2024-04-12 18:40:10, Dev, Step : 5200, Loss : 0.56930, Acc : 0.773, Auc : 0.849, Sensitive_Loss : 0.19043, Sensitive_Acc : 21.496, Sensitive_Auc : 0.998, Mean auc: 0.849, Run Time : 88.42 sec
INFO:root:2024-04-12 18:40:16, Train, Epoch : 9, Step : 5210, Loss : 0.27804, Acc : 0.884, Sensitive_Loss : 0.11950, Sensitive_Acc : 24.200, Run Time : 93.89 sec
INFO:root:2024-04-12 18:40:23, Train, Epoch : 9, Step : 5220, Loss : 0.24595, Acc : 0.891, Sensitive_Loss : 0.07021, Sensitive_Acc : 21.800, Run Time : 7.17 sec
INFO:root:2024-04-12 18:40:30, Train, Epoch : 9, Step : 5230, Loss : 0.27235, Acc : 0.869, Sensitive_Loss : 0.06981, Sensitive_Acc : 26.000, Run Time : 6.96 sec
INFO:root:2024-04-12 18:40:38, Train, Epoch : 9, Step : 5240, Loss : 0.25736, Acc : 0.891, Sensitive_Loss : 0.09899, Sensitive_Acc : 21.200, Run Time : 7.84 sec
INFO:root:2024-04-12 18:40:45, Train, Epoch : 9, Step : 5250, Loss : 0.31333, Acc : 0.863, Sensitive_Loss : 0.09232, Sensitive_Acc : 20.400, Run Time : 7.12 sec
INFO:root:2024-04-12 18:40:52, Train, Epoch : 9, Step : 5260, Loss : 0.28611, Acc : 0.894, Sensitive_Loss : 0.08771, Sensitive_Acc : 22.500, Run Time : 7.05 sec
INFO:root:2024-04-12 18:40:59, Train, Epoch : 9, Step : 5270, Loss : 0.34708, Acc : 0.869, Sensitive_Loss : 0.11653, Sensitive_Acc : 14.900, Run Time : 6.70 sec
INFO:root:2024-04-12 18:41:05, Train, Epoch : 9, Step : 5280, Loss : 0.31109, Acc : 0.859, Sensitive_Loss : 0.09080, Sensitive_Acc : 21.200, Run Time : 6.87 sec
INFO:root:2024-04-12 18:41:13, Train, Epoch : 9, Step : 5290, Loss : 0.27937, Acc : 0.884, Sensitive_Loss : 0.13370, Sensitive_Acc : 25.300, Run Time : 7.21 sec
INFO:root:2024-04-12 18:41:20, Train, Epoch : 9, Step : 5300, Loss : 0.30878, Acc : 0.866, Sensitive_Loss : 0.13273, Sensitive_Acc : 23.100, Run Time : 7.40 sec
INFO:root:2024-04-12 18:42:48, Dev, Step : 5300, Loss : 0.57437, Acc : 0.773, Auc : 0.846, Sensitive_Loss : 0.21297, Sensitive_Acc : 21.211, Sensitive_Auc : 0.998, Mean auc: 0.846, Run Time : 87.79 sec
INFO:root:2024-04-12 18:42:54, Train, Epoch : 9, Step : 5310, Loss : 0.29338, Acc : 0.863, Sensitive_Loss : 0.09891, Sensitive_Acc : 23.400, Run Time : 93.46 sec
INFO:root:2024-04-12 18:43:00, Train, Epoch : 9, Step : 5320, Loss : 0.31126, Acc : 0.872, Sensitive_Loss : 0.11624, Sensitive_Acc : 23.000, Run Time : 6.74 sec
INFO:root:2024-04-12 18:43:08, Train, Epoch : 9, Step : 5330, Loss : 0.27781, Acc : 0.878, Sensitive_Loss : 0.07341, Sensitive_Acc : 24.100, Run Time : 7.52 sec
INFO:root:2024-04-12 18:43:15, Train, Epoch : 9, Step : 5340, Loss : 0.26173, Acc : 0.884, Sensitive_Loss : 0.12032, Sensitive_Acc : 20.000, Run Time : 6.90 sec
INFO:root:2024-04-12 18:43:22, Train, Epoch : 9, Step : 5350, Loss : 0.30561, Acc : 0.866, Sensitive_Loss : 0.09273, Sensitive_Acc : 18.000, Run Time : 7.54 sec
INFO:root:2024-04-12 18:43:29, Train, Epoch : 9, Step : 5360, Loss : 0.31944, Acc : 0.891, Sensitive_Loss : 0.12105, Sensitive_Acc : 23.700, Run Time : 6.96 sec
INFO:root:2024-04-12 18:43:37, Train, Epoch : 9, Step : 5370, Loss : 0.35292, Acc : 0.863, Sensitive_Loss : 0.13627, Sensitive_Acc : 23.300, Run Time : 7.61 sec
INFO:root:2024-04-12 18:43:44, Train, Epoch : 9, Step : 5380, Loss : 0.28884, Acc : 0.881, Sensitive_Loss : 0.12900, Sensitive_Acc : 21.700, Run Time : 7.22 sec
INFO:root:2024-04-12 18:43:51, Train, Epoch : 9, Step : 5390, Loss : 0.34539, Acc : 0.850, Sensitive_Loss : 0.08349, Sensitive_Acc : 21.000, Run Time : 6.63 sec
INFO:root:2024-04-12 18:43:58, Train, Epoch : 9, Step : 5400, Loss : 0.29314, Acc : 0.878, Sensitive_Loss : 0.07889, Sensitive_Acc : 24.400, Run Time : 7.44 sec
INFO:root:2024-04-12 18:45:26, Dev, Step : 5400, Loss : 0.61095, Acc : 0.761, Auc : 0.843, Sensitive_Loss : 0.17460, Sensitive_Acc : 21.586, Sensitive_Auc : 0.997, Mean auc: 0.843, Run Time : 87.73 sec
INFO:root:2024-04-12 18:45:31, Train, Epoch : 9, Step : 5410, Loss : 0.31574, Acc : 0.850, Sensitive_Loss : 0.08625, Sensitive_Acc : 18.800, Run Time : 93.28 sec
INFO:root:2024-04-12 18:45:39, Train, Epoch : 9, Step : 5420, Loss : 0.32709, Acc : 0.887, Sensitive_Loss : 0.10486, Sensitive_Acc : 24.100, Run Time : 7.50 sec
INFO:root:2024-04-12 18:45:46, Train, Epoch : 9, Step : 5430, Loss : 0.31536, Acc : 0.875, Sensitive_Loss : 0.09336, Sensitive_Acc : 24.100, Run Time : 6.94 sec
INFO:root:2024-04-12 18:45:53, Train, Epoch : 9, Step : 5440, Loss : 0.24613, Acc : 0.884, Sensitive_Loss : 0.16822, Sensitive_Acc : 17.800, Run Time : 7.06 sec
INFO:root:2024-04-12 18:46:00, Train, Epoch : 9, Step : 5450, Loss : 0.36506, Acc : 0.850, Sensitive_Loss : 0.10358, Sensitive_Acc : 22.200, Run Time : 7.31 sec
INFO:root:2024-04-12 18:46:08, Train, Epoch : 9, Step : 5460, Loss : 0.35169, Acc : 0.866, Sensitive_Loss : 0.09477, Sensitive_Acc : 22.300, Run Time : 7.35 sec
INFO:root:2024-04-12 18:46:14, Train, Epoch : 9, Step : 5470, Loss : 0.34910, Acc : 0.856, Sensitive_Loss : 0.09888, Sensitive_Acc : 21.600, Run Time : 6.90 sec
INFO:root:2024-04-12 18:46:21, Train, Epoch : 9, Step : 5480, Loss : 0.35551, Acc : 0.869, Sensitive_Loss : 0.08870, Sensitive_Acc : 20.000, Run Time : 6.81 sec
INFO:root:2024-04-12 18:46:29, Train, Epoch : 9, Step : 5490, Loss : 0.27548, Acc : 0.884, Sensitive_Loss : 0.10992, Sensitive_Acc : 22.500, Run Time : 7.66 sec
INFO:root:2024-04-12 18:46:36, Train, Epoch : 9, Step : 5500, Loss : 0.33433, Acc : 0.878, Sensitive_Loss : 0.11424, Sensitive_Acc : 21.400, Run Time : 7.58 sec
INFO:root:2024-04-12 18:48:04, Dev, Step : 5500, Loss : 0.56087, Acc : 0.770, Auc : 0.847, Sensitive_Loss : 0.19087, Sensitive_Acc : 21.692, Sensitive_Auc : 0.996, Mean auc: 0.847, Run Time : 87.89 sec
INFO:root:2024-04-12 18:48:10, Train, Epoch : 9, Step : 5510, Loss : 0.26108, Acc : 0.891, Sensitive_Loss : 0.07234, Sensitive_Acc : 24.000, Run Time : 93.51 sec
INFO:root:2024-04-12 18:48:17, Train, Epoch : 9, Step : 5520, Loss : 0.36128, Acc : 0.831, Sensitive_Loss : 0.07830, Sensitive_Acc : 20.800, Run Time : 6.96 sec
INFO:root:2024-04-12 18:48:24, Train, Epoch : 9, Step : 5530, Loss : 0.28210, Acc : 0.878, Sensitive_Loss : 0.09971, Sensitive_Acc : 18.400, Run Time : 7.19 sec
INFO:root:2024-04-12 18:48:31, Train, Epoch : 9, Step : 5540, Loss : 0.37296, Acc : 0.819, Sensitive_Loss : 0.05456, Sensitive_Acc : 20.800, Run Time : 6.86 sec
INFO:root:2024-04-12 18:48:38, Train, Epoch : 9, Step : 5550, Loss : 0.33410, Acc : 0.838, Sensitive_Loss : 0.11249, Sensitive_Acc : 19.300, Run Time : 7.30 sec
INFO:root:2024-04-12 18:48:45, Train, Epoch : 9, Step : 5560, Loss : 0.33038, Acc : 0.850, Sensitive_Loss : 0.06319, Sensitive_Acc : 18.400, Run Time : 7.18 sec
INFO:root:2024-04-12 18:48:52, Train, Epoch : 9, Step : 5570, Loss : 0.32367, Acc : 0.853, Sensitive_Loss : 0.08409, Sensitive_Acc : 19.100, Run Time : 6.91 sec
INFO:root:2024-04-12 18:49:00, Train, Epoch : 9, Step : 5580, Loss : 0.30963, Acc : 0.869, Sensitive_Loss : 0.11240, Sensitive_Acc : 24.600, Run Time : 7.13 sec
INFO:root:2024-04-12 18:49:07, Train, Epoch : 9, Step : 5590, Loss : 0.31067, Acc : 0.881, Sensitive_Loss : 0.09380, Sensitive_Acc : 18.800, Run Time : 7.23 sec
INFO:root:2024-04-12 18:49:14, Train, Epoch : 9, Step : 5600, Loss : 0.38074, Acc : 0.816, Sensitive_Loss : 0.10447, Sensitive_Acc : 20.500, Run Time : 7.09 sec
INFO:root:2024-04-12 18:50:42, Dev, Step : 5600, Loss : 0.58303, Acc : 0.770, Auc : 0.848, Sensitive_Loss : 0.19071, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.848, Run Time : 88.45 sec
INFO:root:2024-04-12 18:50:48, Train, Epoch : 9, Step : 5610, Loss : 0.27770, Acc : 0.875, Sensitive_Loss : 0.07704, Sensitive_Acc : 24.900, Run Time : 93.96 sec
INFO:root:2024-04-12 18:50:55, Train, Epoch : 9, Step : 5620, Loss : 0.24568, Acc : 0.875, Sensitive_Loss : 0.14169, Sensitive_Acc : 21.400, Run Time : 7.22 sec
INFO:root:2024-04-12 18:51:02, Train, Epoch : 9, Step : 5630, Loss : 0.38993, Acc : 0.828, Sensitive_Loss : 0.09081, Sensitive_Acc : 23.700, Run Time : 7.00 sec
INFO:root:2024-04-12 18:51:10, Train, Epoch : 9, Step : 5640, Loss : 0.29194, Acc : 0.844, Sensitive_Loss : 0.10491, Sensitive_Acc : 25.000, Run Time : 7.66 sec
INFO:root:2024-04-12 18:51:17, Train, Epoch : 9, Step : 5650, Loss : 0.34618, Acc : 0.838, Sensitive_Loss : 0.10009, Sensitive_Acc : 24.000, Run Time : 6.96 sec
INFO:root:2024-04-12 18:51:23, Train, Epoch : 9, Step : 5660, Loss : 0.24227, Acc : 0.897, Sensitive_Loss : 0.09984, Sensitive_Acc : 24.000, Run Time : 6.77 sec
INFO:root:2024-04-12 18:51:31, Train, Epoch : 9, Step : 5670, Loss : 0.29561, Acc : 0.853, Sensitive_Loss : 0.06688, Sensitive_Acc : 23.400, Run Time : 7.19 sec
INFO:root:2024-04-12 18:51:37, Train, Epoch : 9, Step : 5680, Loss : 0.27629, Acc : 0.900, Sensitive_Loss : 0.11590, Sensitive_Acc : 22.300, Run Time : 6.88 sec
INFO:root:2024-04-12 18:51:45, Train, Epoch : 9, Step : 5690, Loss : 0.29788, Acc : 0.869, Sensitive_Loss : 0.10032, Sensitive_Acc : 22.000, Run Time : 7.15 sec
INFO:root:2024-04-12 18:51:52, Train, Epoch : 9, Step : 5700, Loss : 0.28162, Acc : 0.884, Sensitive_Loss : 0.11278, Sensitive_Acc : 22.300, Run Time : 6.91 sec
INFO:root:2024-04-12 18:53:20, Dev, Step : 5700, Loss : 0.55950, Acc : 0.773, Auc : 0.850, Sensitive_Loss : 0.22206, Sensitive_Acc : 21.466, Sensitive_Auc : 0.996, Mean auc: 0.850, Run Time : 88.63 sec
INFO:root:2024-04-12 18:54:50
INFO:root:y_pred: [0.03198851 0.00093659 0.08094253 ... 0.15843895 0.00342035 0.29623973]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [2.7861518e-03 3.4927056e-04 2.0363772e-02 1.6331110e-02 2.2163704e-05
 9.6809631e-04 3.2312345e-02 2.2702960e-03 2.8132606e-01 9.9993896e-01
 2.4839407e-02 2.0313913e-04 4.7825812e-03 2.1316989e-04 9.9970043e-01
 1.5650112e-02 3.3933122e-02 9.9958009e-01 9.9923658e-01 1.9106369e-03
 9.8805201e-01 6.3123053e-04 2.3404855e-04 5.0066630e-03 4.3188325e-01
 3.0325574e-01 3.1301304e-06 4.9421876e-03 1.6662546e-03 1.5682187e-02
 1.1909976e-02 9.6737391e-01 7.1483608e-03 9.8850608e-01 1.4085270e-03
 1.9343636e-06 4.0543899e-03 4.3591730e-02 1.8962538e-02 4.3162998e-02
 1.3891772e-02 9.9871516e-01 1.3205368e-04 4.3340031e-05 9.9343866e-01
 7.4257441e-02 2.7599922e-01 3.5753343e-01 5.5342961e-02 9.9992108e-01
 9.9866951e-01 9.9967384e-01 9.2896390e-01 1.1820648e-03 1.1582820e-01
 9.5112473e-01 5.1044682e-03 2.9313525e-03 9.9985409e-01 1.2881352e-03
 9.3900409e-05 2.7350454e-02 6.2967222e-03 3.0678458e-04 9.9943930e-01
 4.0423235e-01 2.8103870e-05 1.7141989e-01 7.9952562e-03 9.9557674e-01
 9.9973434e-01 9.9991691e-01 1.7447430e-05 6.4199549e-01 1.8645339e-03
 9.8909003e-01 4.7709078e-02 4.1846324e-05 1.4454033e-05 7.9545751e-03
 7.1776584e-02 3.9506061e-03 9.9960309e-01 9.9997330e-01 2.2670159e-03
 2.9802944e-03 3.9747313e-02 1.1487748e-05 1.4653607e-02 5.3922930e-05
 2.4158688e-02 4.0813783e-01 4.2825416e-04 1.1248899e-04 1.1515103e-05
 4.5101070e-03 2.4192862e-04 7.2532076e-01 7.1097225e-02 3.5049089e-03
 1.8733488e-03 1.1116770e-02 1.9575997e-01 2.3832188e-04 6.0109753e-04
 6.1151659e-04 3.5855390e-02 3.0753422e-01 3.5996750e-01 3.3957667e-03
 5.7352398e-04 9.9998903e-01 9.9996448e-01 2.9580298e-05 9.8076396e-02
 9.7281113e-02 8.2240824e-04 1.8979044e-03 3.6763698e-02 5.2846335e-03
 8.3581852e-03 3.8864606e-04 2.9264171e-02 1.0473357e-04 9.8475721e-04
 9.8630875e-01 9.7792312e-05 9.9986053e-01 3.3258524e-04 4.2960327e-02
 1.1251571e-02 8.4593616e-02 8.3824852e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 18:54:50, Dev, Step : 5706, Loss : 0.56888, Acc : 0.773, Auc : 0.850, Sensitive_Loss : 0.20437, Sensitive_Acc : 21.602, Sensitive_Auc : 0.996, Mean auc: 0.850, Run Time : 87.19 sec
INFO:root:2024-04-12 18:54:55, Train, Epoch : 10, Step : 5710, Loss : 0.11593, Acc : 0.353, Sensitive_Loss : 0.03752, Sensitive_Acc : 9.800, Run Time : 4.13 sec
INFO:root:2024-04-12 18:55:02, Train, Epoch : 10, Step : 5720, Loss : 0.24854, Acc : 0.887, Sensitive_Loss : 0.09850, Sensitive_Acc : 22.800, Run Time : 7.16 sec
INFO:root:2024-04-12 18:55:09, Train, Epoch : 10, Step : 5730, Loss : 0.29917, Acc : 0.894, Sensitive_Loss : 0.08476, Sensitive_Acc : 21.500, Run Time : 7.15 sec
INFO:root:2024-04-12 18:55:17, Train, Epoch : 10, Step : 5740, Loss : 0.28549, Acc : 0.859, Sensitive_Loss : 0.08859, Sensitive_Acc : 18.000, Run Time : 7.47 sec
INFO:root:2024-04-12 18:55:23, Train, Epoch : 10, Step : 5750, Loss : 0.27429, Acc : 0.884, Sensitive_Loss : 0.07919, Sensitive_Acc : 23.500, Run Time : 6.77 sec
INFO:root:2024-04-12 18:55:31, Train, Epoch : 10, Step : 5760, Loss : 0.26542, Acc : 0.881, Sensitive_Loss : 0.08725, Sensitive_Acc : 18.900, Run Time : 7.33 sec
INFO:root:2024-04-12 18:55:38, Train, Epoch : 10, Step : 5770, Loss : 0.28910, Acc : 0.894, Sensitive_Loss : 0.11845, Sensitive_Acc : 21.600, Run Time : 7.09 sec
INFO:root:2024-04-12 18:55:45, Train, Epoch : 10, Step : 5780, Loss : 0.27428, Acc : 0.847, Sensitive_Loss : 0.11071, Sensitive_Acc : 19.500, Run Time : 7.11 sec
INFO:root:2024-04-12 18:55:52, Train, Epoch : 10, Step : 5790, Loss : 0.24338, Acc : 0.903, Sensitive_Loss : 0.09150, Sensitive_Acc : 25.100, Run Time : 7.54 sec
INFO:root:2024-04-12 18:55:59, Train, Epoch : 10, Step : 5800, Loss : 0.26367, Acc : 0.872, Sensitive_Loss : 0.05247, Sensitive_Acc : 19.600, Run Time : 6.69 sec
INFO:root:2024-04-12 18:57:27, Dev, Step : 5800, Loss : 0.57677, Acc : 0.776, Auc : 0.849, Sensitive_Loss : 0.18858, Sensitive_Acc : 21.586, Sensitive_Auc : 0.998, Mean auc: 0.849, Run Time : 88.13 sec
INFO:root:2024-04-12 18:57:33, Train, Epoch : 10, Step : 5810, Loss : 0.26326, Acc : 0.884, Sensitive_Loss : 0.06496, Sensitive_Acc : 18.000, Run Time : 93.87 sec
INFO:root:2024-04-12 18:57:41, Train, Epoch : 10, Step : 5820, Loss : 0.29851, Acc : 0.887, Sensitive_Loss : 0.14169, Sensitive_Acc : 22.800, Run Time : 7.59 sec
INFO:root:2024-04-12 18:57:47, Train, Epoch : 10, Step : 5830, Loss : 0.26949, Acc : 0.884, Sensitive_Loss : 0.08799, Sensitive_Acc : 22.400, Run Time : 6.78 sec
INFO:root:2024-04-12 18:57:55, Train, Epoch : 10, Step : 5840, Loss : 0.28812, Acc : 0.841, Sensitive_Loss : 0.09991, Sensitive_Acc : 21.300, Run Time : 7.16 sec
INFO:root:2024-04-12 18:58:01, Train, Epoch : 10, Step : 5850, Loss : 0.32934, Acc : 0.866, Sensitive_Loss : 0.07695, Sensitive_Acc : 19.900, Run Time : 6.82 sec
INFO:root:2024-04-12 18:58:09, Train, Epoch : 10, Step : 5860, Loss : 0.26061, Acc : 0.891, Sensitive_Loss : 0.10784, Sensitive_Acc : 26.800, Run Time : 7.48 sec
INFO:root:2024-04-12 18:58:15, Train, Epoch : 10, Step : 5870, Loss : 0.33846, Acc : 0.853, Sensitive_Loss : 0.09127, Sensitive_Acc : 21.000, Run Time : 6.59 sec
INFO:root:2024-04-12 18:58:23, Train, Epoch : 10, Step : 5880, Loss : 0.26212, Acc : 0.894, Sensitive_Loss : 0.07887, Sensitive_Acc : 17.600, Run Time : 7.88 sec
INFO:root:2024-04-12 18:58:30, Train, Epoch : 10, Step : 5890, Loss : 0.25467, Acc : 0.884, Sensitive_Loss : 0.06086, Sensitive_Acc : 14.800, Run Time : 6.95 sec
INFO:root:2024-04-12 18:58:37, Train, Epoch : 10, Step : 5900, Loss : 0.31241, Acc : 0.891, Sensitive_Loss : 0.09424, Sensitive_Acc : 19.900, Run Time : 7.04 sec
INFO:root:2024-04-12 19:00:05, Dev, Step : 5900, Loss : 0.58935, Acc : 0.772, Auc : 0.844, Sensitive_Loss : 0.19523, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.844, Run Time : 87.87 sec
INFO:root:2024-04-12 19:00:10, Train, Epoch : 10, Step : 5910, Loss : 0.28946, Acc : 0.863, Sensitive_Loss : 0.09221, Sensitive_Acc : 18.700, Run Time : 93.19 sec
INFO:root:2024-04-12 19:00:18, Train, Epoch : 10, Step : 5920, Loss : 0.29451, Acc : 0.869, Sensitive_Loss : 0.07449, Sensitive_Acc : 24.800, Run Time : 7.23 sec
INFO:root:2024-04-12 19:00:25, Train, Epoch : 10, Step : 5930, Loss : 0.27985, Acc : 0.900, Sensitive_Loss : 0.07926, Sensitive_Acc : 19.300, Run Time : 7.42 sec
INFO:root:2024-04-12 19:00:32, Train, Epoch : 10, Step : 5940, Loss : 0.24991, Acc : 0.906, Sensitive_Loss : 0.08578, Sensitive_Acc : 22.000, Run Time : 7.19 sec
INFO:root:2024-04-12 19:00:39, Train, Epoch : 10, Step : 5950, Loss : 0.30595, Acc : 0.875, Sensitive_Loss : 0.08908, Sensitive_Acc : 18.400, Run Time : 6.85 sec
INFO:root:2024-04-12 19:00:46, Train, Epoch : 10, Step : 5960, Loss : 0.23273, Acc : 0.884, Sensitive_Loss : 0.07926, Sensitive_Acc : 18.000, Run Time : 7.15 sec
INFO:root:2024-04-12 19:00:54, Train, Epoch : 10, Step : 5970, Loss : 0.30099, Acc : 0.884, Sensitive_Loss : 0.10714, Sensitive_Acc : 24.200, Run Time : 7.23 sec
INFO:root:2024-04-12 19:01:01, Train, Epoch : 10, Step : 5980, Loss : 0.33082, Acc : 0.866, Sensitive_Loss : 0.11277, Sensitive_Acc : 25.200, Run Time : 7.12 sec
INFO:root:2024-04-12 19:01:07, Train, Epoch : 10, Step : 5990, Loss : 0.25996, Acc : 0.891, Sensitive_Loss : 0.14701, Sensitive_Acc : 22.100, Run Time : 6.82 sec
INFO:root:2024-04-12 19:01:15, Train, Epoch : 10, Step : 6000, Loss : 0.28470, Acc : 0.869, Sensitive_Loss : 0.08852, Sensitive_Acc : 25.200, Run Time : 7.28 sec
INFO:root:2024-04-12 19:02:43, Dev, Step : 6000, Loss : 0.60129, Acc : 0.772, Auc : 0.845, Sensitive_Loss : 0.17641, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.845, Run Time : 88.53 sec
INFO:root:2024-04-12 19:02:49, Train, Epoch : 10, Step : 6010, Loss : 0.24061, Acc : 0.894, Sensitive_Loss : 0.10112, Sensitive_Acc : 14.200, Run Time : 93.98 sec
INFO:root:2024-04-12 19:02:55, Train, Epoch : 10, Step : 6020, Loss : 0.26149, Acc : 0.891, Sensitive_Loss : 0.07572, Sensitive_Acc : 23.400, Run Time : 6.73 sec
INFO:root:2024-04-12 19:03:03, Train, Epoch : 10, Step : 6030, Loss : 0.28737, Acc : 0.878, Sensitive_Loss : 0.09705, Sensitive_Acc : 19.700, Run Time : 7.31 sec
INFO:root:2024-04-12 19:03:10, Train, Epoch : 10, Step : 6040, Loss : 0.22123, Acc : 0.906, Sensitive_Loss : 0.13801, Sensitive_Acc : 25.900, Run Time : 7.26 sec
INFO:root:2024-04-12 19:03:17, Train, Epoch : 10, Step : 6050, Loss : 0.27618, Acc : 0.897, Sensitive_Loss : 0.10320, Sensitive_Acc : 23.800, Run Time : 6.91 sec
INFO:root:2024-04-12 19:03:24, Train, Epoch : 10, Step : 6060, Loss : 0.28215, Acc : 0.878, Sensitive_Loss : 0.07584, Sensitive_Acc : 21.200, Run Time : 7.23 sec
INFO:root:2024-04-12 19:03:32, Train, Epoch : 10, Step : 6070, Loss : 0.28491, Acc : 0.891, Sensitive_Loss : 0.09048, Sensitive_Acc : 20.600, Run Time : 7.53 sec
INFO:root:2024-04-12 19:03:38, Train, Epoch : 10, Step : 6080, Loss : 0.25647, Acc : 0.897, Sensitive_Loss : 0.10892, Sensitive_Acc : 21.900, Run Time : 6.72 sec
INFO:root:2024-04-12 19:03:46, Train, Epoch : 10, Step : 6090, Loss : 0.31644, Acc : 0.869, Sensitive_Loss : 0.05842, Sensitive_Acc : 18.800, Run Time : 7.26 sec
INFO:root:2024-04-12 19:03:53, Train, Epoch : 10, Step : 6100, Loss : 0.25871, Acc : 0.881, Sensitive_Loss : 0.05466, Sensitive_Acc : 22.400, Run Time : 7.14 sec
INFO:root:2024-04-12 19:05:21, Dev, Step : 6100, Loss : 0.63629, Acc : 0.765, Auc : 0.844, Sensitive_Loss : 0.18067, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.844, Run Time : 88.22 sec
INFO:root:2024-04-12 19:05:27, Train, Epoch : 10, Step : 6110, Loss : 0.31022, Acc : 0.894, Sensitive_Loss : 0.10811, Sensitive_Acc : 23.800, Run Time : 93.72 sec
INFO:root:2024-04-12 19:05:34, Train, Epoch : 10, Step : 6120, Loss : 0.29233, Acc : 0.869, Sensitive_Loss : 0.11452, Sensitive_Acc : 18.400, Run Time : 7.12 sec
INFO:root:2024-04-12 19:05:41, Train, Epoch : 10, Step : 6130, Loss : 0.30422, Acc : 0.872, Sensitive_Loss : 0.08191, Sensitive_Acc : 19.900, Run Time : 7.53 sec
INFO:root:2024-04-12 19:05:48, Train, Epoch : 10, Step : 6140, Loss : 0.28683, Acc : 0.853, Sensitive_Loss : 0.08044, Sensitive_Acc : 22.800, Run Time : 7.08 sec
INFO:root:2024-04-12 19:05:56, Train, Epoch : 10, Step : 6150, Loss : 0.30044, Acc : 0.891, Sensitive_Loss : 0.07192, Sensitive_Acc : 22.500, Run Time : 7.42 sec
INFO:root:2024-04-12 19:06:02, Train, Epoch : 10, Step : 6160, Loss : 0.27303, Acc : 0.881, Sensitive_Loss : 0.07115, Sensitive_Acc : 21.200, Run Time : 6.47 sec
INFO:root:2024-04-12 19:06:10, Train, Epoch : 10, Step : 6170, Loss : 0.28334, Acc : 0.878, Sensitive_Loss : 0.07219, Sensitive_Acc : 23.600, Run Time : 7.54 sec
INFO:root:2024-04-12 19:06:17, Train, Epoch : 10, Step : 6180, Loss : 0.28257, Acc : 0.894, Sensitive_Loss : 0.05414, Sensitive_Acc : 19.500, Run Time : 7.21 sec
INFO:root:2024-04-12 19:06:24, Train, Epoch : 10, Step : 6190, Loss : 0.29131, Acc : 0.884, Sensitive_Loss : 0.10842, Sensitive_Acc : 22.700, Run Time : 6.79 sec
INFO:root:2024-04-12 19:06:31, Train, Epoch : 10, Step : 6200, Loss : 0.25604, Acc : 0.897, Sensitive_Loss : 0.10703, Sensitive_Acc : 19.200, Run Time : 7.09 sec
INFO:root:2024-04-12 19:07:59, Dev, Step : 6200, Loss : 0.58016, Acc : 0.777, Auc : 0.846, Sensitive_Loss : 0.18412, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.846, Run Time : 87.97 sec
INFO:root:2024-04-12 19:08:04, Train, Epoch : 10, Step : 6210, Loss : 0.27173, Acc : 0.900, Sensitive_Loss : 0.10266, Sensitive_Acc : 17.600, Run Time : 93.56 sec
INFO:root:2024-04-12 19:08:12, Train, Epoch : 10, Step : 6220, Loss : 0.26633, Acc : 0.878, Sensitive_Loss : 0.06669, Sensitive_Acc : 21.900, Run Time : 7.51 sec
INFO:root:2024-04-12 19:08:18, Train, Epoch : 10, Step : 6230, Loss : 0.32125, Acc : 0.875, Sensitive_Loss : 0.12405, Sensitive_Acc : 19.400, Run Time : 6.64 sec
INFO:root:2024-04-12 19:08:26, Train, Epoch : 10, Step : 6240, Loss : 0.25224, Acc : 0.891, Sensitive_Loss : 0.14715, Sensitive_Acc : 26.200, Run Time : 7.28 sec
INFO:root:2024-04-12 19:08:33, Train, Epoch : 10, Step : 6250, Loss : 0.28506, Acc : 0.856, Sensitive_Loss : 0.11060, Sensitive_Acc : 25.100, Run Time : 7.49 sec
INFO:root:2024-04-12 19:08:40, Train, Epoch : 10, Step : 6260, Loss : 0.27444, Acc : 0.887, Sensitive_Loss : 0.11241, Sensitive_Acc : 19.100, Run Time : 6.88 sec
INFO:root:2024-04-12 19:08:47, Train, Epoch : 10, Step : 6270, Loss : 0.26253, Acc : 0.891, Sensitive_Loss : 0.09205, Sensitive_Acc : 23.200, Run Time : 7.35 sec
INFO:root:2024-04-12 19:08:55, Train, Epoch : 10, Step : 6280, Loss : 0.26903, Acc : 0.887, Sensitive_Loss : 0.14906, Sensitive_Acc : 22.700, Run Time : 7.21 sec
INFO:root:2024-04-12 19:09:02, Train, Epoch : 10, Step : 6290, Loss : 0.26265, Acc : 0.881, Sensitive_Loss : 0.12321, Sensitive_Acc : 21.400, Run Time : 7.22 sec
INFO:root:2024-04-12 19:09:09, Train, Epoch : 10, Step : 6300, Loss : 0.34534, Acc : 0.863, Sensitive_Loss : 0.07810, Sensitive_Acc : 23.800, Run Time : 7.02 sec
INFO:root:2024-04-12 19:10:37, Dev, Step : 6300, Loss : 0.62601, Acc : 0.766, Auc : 0.847, Sensitive_Loss : 0.19209, Sensitive_Acc : 21.586, Sensitive_Auc : 0.997, Mean auc: 0.847, Run Time : 87.88 sec
INFO:root:2024-04-12 19:10:42, Train, Epoch : 10, Step : 6310, Loss : 0.28883, Acc : 0.875, Sensitive_Loss : 0.08896, Sensitive_Acc : 26.400, Run Time : 93.39 sec
INFO:root:2024-04-12 19:10:50, Train, Epoch : 10, Step : 6320, Loss : 0.24761, Acc : 0.906, Sensitive_Loss : 0.06164, Sensitive_Acc : 20.800, Run Time : 7.51 sec
INFO:root:2024-04-12 19:10:57, Train, Epoch : 10, Step : 6330, Loss : 0.24583, Acc : 0.894, Sensitive_Loss : 0.06538, Sensitive_Acc : 24.600, Run Time : 7.03 sec
INFO:root:2024-04-12 19:11:03, Train, Epoch : 10, Step : 6340, Loss : 0.31736, Acc : 0.847, Sensitive_Loss : 0.08514, Sensitive_Acc : 19.700, Run Time : 6.38 sec
INFO:root:2024-04-12 19:12:30
INFO:root:y_pred: [0.07464706 0.00174371 0.04667036 ... 0.15489705 0.00371742 0.44305956]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.45470072e-03 1.51618544e-04 1.85224265e-02 1.58835445e-02
 9.25826225e-06 2.17897745e-04 1.69168711e-02 2.24281568e-03
 9.20897797e-02 9.99849677e-01 2.30196863e-02 9.35076969e-05
 9.53529961e-04 2.35241445e-04 9.97145355e-01 6.60142535e-03
 1.01661012e-02 9.97774541e-01 9.97925878e-01 6.29583956e-04
 9.45906937e-01 1.22871925e-03 2.17883626e-05 2.23558536e-03
 1.60642713e-01 8.49135220e-02 2.55272039e-06 3.79489944e-03
 3.88200046e-04 6.54982729e-03 4.08015260e-03 9.10291553e-01
 1.52824889e-03 9.69213426e-01 5.07787277e-04 1.04576225e-06
 1.29064790e-03 4.69616707e-03 4.62795142e-03 7.32790306e-03
 1.20606124e-02 9.97535348e-01 4.78470392e-05 7.89964452e-06
 9.87439454e-01 7.13824630e-02 8.39628279e-02 1.04355879e-01
 6.50791824e-02 9.99843597e-01 9.96985614e-01 9.99097586e-01
 8.85211885e-01 7.59097107e-04 4.37334329e-02 8.44450653e-01
 3.93512147e-03 1.66498206e-03 9.98519838e-01 3.95889481e-04
 4.92080071e-05 1.80834141e-02 2.19408236e-03 1.36863324e-04
 9.96552944e-01 2.61928648e-01 6.23410597e-05 1.07627459e-01
 6.24365872e-03 9.91548777e-01 9.99683261e-01 9.99855042e-01
 5.27529073e-06 1.64516836e-01 7.04769220e-04 9.42204177e-01
 2.61231642e-02 1.94630820e-06 9.96720792e-06 5.02903434e-03
 2.41311081e-02 1.80778652e-03 9.99348342e-01 9.99904633e-01
 7.11956993e-04 2.50445050e-03 2.03139354e-02 6.55648137e-06
 1.16003677e-02 2.67057403e-05 2.13591177e-02 1.15019329e-01
 2.03663556e-04 1.66771661e-05 1.52403200e-05 5.37262065e-04
 4.41474076e-05 7.35157967e-01 3.21302228e-02 2.96429126e-03
 2.30730118e-04 4.22485359e-03 6.02046326e-02 7.28510058e-05
 7.96428503e-05 2.03616000e-04 1.55623676e-02 2.82584190e-01
 1.56159833e-01 2.51565990e-03 8.44373542e-04 9.99975085e-01
 9.99818623e-01 1.43703710e-05 2.69990712e-02 2.30436288e-02
 4.99723974e-05 1.51206623e-03 2.51511186e-02 1.01229770e-03
 5.95521135e-03 7.85216325e-05 3.74541208e-02 3.98566190e-05
 1.03879930e-03 9.52027977e-01 7.28656596e-05 9.99729931e-01
 1.03072925e-04 1.50278565e-02 4.49416414e-03 2.82367617e-02
 2.99147097e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 19:12:30, Dev, Step : 6340, Loss : 0.59761, Acc : 0.773, Auc : 0.845, Sensitive_Loss : 0.16710, Sensitive_Acc : 21.692, Sensitive_Auc : 0.997, Mean auc: 0.845, Run Time : 86.89 sec
