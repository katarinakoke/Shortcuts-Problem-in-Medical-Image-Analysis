Running on desktop22:
stdin: is not a tty
/home/pmen/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_pmen.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/balanced_dataset_train.csv",
    "dev_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/balanced_dataset_val.csv",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.1,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-10 16:51:25, Train, Epoch : 1, Step : 10, Loss : 0.72216, Acc : 0.566, Sensitive_Loss : 0.69592, Sensitive_Acc : 15.300, Run Time : 11.11 sec
INFO:root:2024-04-10 16:51:33, Train, Epoch : 1, Step : 20, Loss : 0.65957, Acc : 0.581, Sensitive_Loss : 0.73170, Sensitive_Acc : 16.200, Run Time : 7.93 sec
INFO:root:2024-04-10 16:51:41, Train, Epoch : 1, Step : 30, Loss : 0.72234, Acc : 0.603, Sensitive_Loss : 0.64381, Sensitive_Acc : 16.000, Run Time : 7.75 sec
INFO:root:2024-04-10 16:51:49, Train, Epoch : 1, Step : 40, Loss : 0.61664, Acc : 0.669, Sensitive_Loss : 0.61803, Sensitive_Acc : 15.300, Run Time : 8.18 sec
INFO:root:2024-04-10 16:51:58, Train, Epoch : 1, Step : 50, Loss : 0.67436, Acc : 0.650, Sensitive_Loss : 0.61713, Sensitive_Acc : 15.900, Run Time : 9.32 sec
INFO:root:2024-04-10 16:52:07, Train, Epoch : 1, Step : 60, Loss : 0.57432, Acc : 0.669, Sensitive_Loss : 0.49977, Sensitive_Acc : 15.800, Run Time : 8.62 sec
INFO:root:2024-04-10 16:52:15, Train, Epoch : 1, Step : 70, Loss : 0.61078, Acc : 0.659, Sensitive_Loss : 0.56381, Sensitive_Acc : 16.300, Run Time : 8.04 sec
INFO:root:2024-04-10 16:52:24, Train, Epoch : 1, Step : 80, Loss : 0.59955, Acc : 0.659, Sensitive_Loss : 0.47535, Sensitive_Acc : 16.000, Run Time : 9.08 sec
INFO:root:2024-04-10 16:52:37, Train, Epoch : 1, Step : 90, Loss : 0.59617, Acc : 0.694, Sensitive_Loss : 0.47230, Sensitive_Acc : 17.400, Run Time : 12.52 sec
INFO:root:2024-04-10 16:52:47, Train, Epoch : 1, Step : 100, Loss : 0.56544, Acc : 0.694, Sensitive_Loss : 0.49610, Sensitive_Acc : 15.900, Run Time : 10.45 sec
INFO:root:2024-04-10 16:54:21, Dev, Step : 100, Loss : 0.60233, Acc : 0.687, Auc : 0.763, Sensitive_Loss : 0.62031, Sensitive_Acc : 15.964, Sensitive_Auc : 0.882, Mean auc: 0.763, Run Time : 93.90 sec
INFO:root:2024-04-10 16:54:22, Best, Step : 100, Loss : 0.60233, Acc : 0.687, Auc : 0.763, Sensitive_Loss : 0.62031, Sensitive_Acc : 15.964, Sensitive_Auc : 0.882, Best Auc : 0.763
INFO:root:2024-04-10 16:54:30, Train, Epoch : 1, Step : 110, Loss : 0.63673, Acc : 0.688, Sensitive_Loss : 0.45878, Sensitive_Acc : 17.200, Run Time : 102.57 sec
INFO:root:2024-04-10 16:54:40, Train, Epoch : 1, Step : 120, Loss : 0.62678, Acc : 0.662, Sensitive_Loss : 0.43100, Sensitive_Acc : 16.500, Run Time : 10.36 sec
INFO:root:2024-04-10 16:54:55, Train, Epoch : 1, Step : 130, Loss : 0.59504, Acc : 0.681, Sensitive_Loss : 0.38632, Sensitive_Acc : 16.100, Run Time : 14.62 sec
INFO:root:2024-04-10 16:55:05, Train, Epoch : 1, Step : 140, Loss : 0.69086, Acc : 0.688, Sensitive_Loss : 0.45399, Sensitive_Acc : 17.300, Run Time : 10.48 sec
INFO:root:2024-04-10 16:55:15, Train, Epoch : 1, Step : 150, Loss : 0.59242, Acc : 0.684, Sensitive_Loss : 0.39195, Sensitive_Acc : 15.900, Run Time : 9.71 sec
INFO:root:2024-04-10 16:55:29, Train, Epoch : 1, Step : 160, Loss : 0.63704, Acc : 0.647, Sensitive_Loss : 0.34284, Sensitive_Acc : 15.700, Run Time : 14.46 sec
INFO:root:2024-04-10 16:55:39, Train, Epoch : 1, Step : 170, Loss : 0.60013, Acc : 0.669, Sensitive_Loss : 0.37353, Sensitive_Acc : 16.900, Run Time : 10.21 sec
INFO:root:2024-04-10 16:55:49, Train, Epoch : 1, Step : 180, Loss : 0.58228, Acc : 0.675, Sensitive_Loss : 0.33795, Sensitive_Acc : 16.300, Run Time : 9.71 sec
INFO:root:2024-04-10 16:55:59, Train, Epoch : 1, Step : 190, Loss : 0.53950, Acc : 0.734, Sensitive_Loss : 0.30215, Sensitive_Acc : 15.400, Run Time : 10.08 sec
INFO:root:2024-04-10 16:56:09, Train, Epoch : 1, Step : 200, Loss : 0.62214, Acc : 0.697, Sensitive_Loss : 0.35685, Sensitive_Acc : 16.600, Run Time : 10.11 sec
INFO:root:2024-04-10 16:57:42, Dev, Step : 200, Loss : 0.57869, Acc : 0.717, Auc : 0.786, Sensitive_Loss : 0.31628, Sensitive_Acc : 16.093, Sensitive_Auc : 0.952, Mean auc: 0.786, Run Time : 92.84 sec
INFO:root:2024-04-10 16:57:45, Best, Step : 200, Loss : 0.57869, Acc : 0.717, Auc : 0.786, Sensitive_Loss : 0.31628, Sensitive_Acc : 16.093, Sensitive_Auc : 0.952, Best Auc : 0.786
INFO:root:2024-04-10 16:57:52, Train, Epoch : 1, Step : 210, Loss : 0.66632, Acc : 0.694, Sensitive_Loss : 0.30319, Sensitive_Acc : 17.000, Run Time : 102.55 sec
INFO:root:2024-04-10 16:58:02, Train, Epoch : 1, Step : 220, Loss : 0.53583, Acc : 0.706, Sensitive_Loss : 0.32145, Sensitive_Acc : 16.000, Run Time : 9.93 sec
INFO:root:2024-04-10 16:58:11, Train, Epoch : 1, Step : 230, Loss : 0.48770, Acc : 0.728, Sensitive_Loss : 0.30778, Sensitive_Acc : 17.600, Run Time : 9.00 sec
INFO:root:2024-04-10 16:58:20, Train, Epoch : 1, Step : 240, Loss : 0.60524, Acc : 0.688, Sensitive_Loss : 0.38264, Sensitive_Acc : 16.800, Run Time : 9.58 sec
INFO:root:2024-04-10 16:58:31, Train, Epoch : 1, Step : 250, Loss : 0.60711, Acc : 0.700, Sensitive_Loss : 0.31338, Sensitive_Acc : 15.400, Run Time : 10.27 sec
INFO:root:2024-04-10 16:58:40, Train, Epoch : 1, Step : 260, Loss : 0.57897, Acc : 0.678, Sensitive_Loss : 0.29685, Sensitive_Acc : 17.100, Run Time : 9.77 sec
INFO:root:2024-04-10 16:58:52, Train, Epoch : 1, Step : 270, Loss : 0.54460, Acc : 0.700, Sensitive_Loss : 0.29803, Sensitive_Acc : 16.400, Run Time : 12.00 sec
INFO:root:2024-04-10 16:59:02, Train, Epoch : 1, Step : 280, Loss : 0.56256, Acc : 0.706, Sensitive_Loss : 0.29499, Sensitive_Acc : 15.700, Run Time : 9.98 sec
INFO:root:2024-04-10 16:59:12, Train, Epoch : 1, Step : 290, Loss : 0.59304, Acc : 0.684, Sensitive_Loss : 0.26413, Sensitive_Acc : 16.200, Run Time : 9.59 sec
INFO:root:2024-04-10 16:59:24, Train, Epoch : 1, Step : 300, Loss : 0.60992, Acc : 0.713, Sensitive_Loss : 0.32690, Sensitive_Acc : 16.600, Run Time : 12.41 sec
INFO:root:2024-04-10 17:00:56, Dev, Step : 300, Loss : 0.58717, Acc : 0.706, Auc : 0.793, Sensitive_Loss : 0.31174, Sensitive_Acc : 16.350, Sensitive_Auc : 0.976, Mean auc: 0.793, Run Time : 92.02 sec
INFO:root:2024-04-10 17:00:57, Best, Step : 300, Loss : 0.58717, Acc : 0.706, Auc : 0.793, Sensitive_Loss : 0.31174, Sensitive_Acc : 16.350, Sensitive_Auc : 0.976, Best Auc : 0.793
INFO:root:2024-04-10 17:01:04, Train, Epoch : 1, Step : 310, Loss : 0.57025, Acc : 0.747, Sensitive_Loss : 0.26928, Sensitive_Acc : 17.400, Run Time : 99.18 sec
INFO:root:2024-04-10 17:01:13, Train, Epoch : 1, Step : 320, Loss : 0.59628, Acc : 0.706, Sensitive_Loss : 0.27640, Sensitive_Acc : 15.700, Run Time : 9.63 sec
INFO:root:2024-04-10 17:01:23, Train, Epoch : 1, Step : 330, Loss : 0.45621, Acc : 0.769, Sensitive_Loss : 0.26766, Sensitive_Acc : 15.600, Run Time : 9.60 sec
INFO:root:2024-04-10 17:01:35, Train, Epoch : 1, Step : 340, Loss : 0.66070, Acc : 0.678, Sensitive_Loss : 0.20672, Sensitive_Acc : 15.900, Run Time : 11.81 sec
INFO:root:2024-04-10 17:01:47, Train, Epoch : 1, Step : 350, Loss : 0.56436, Acc : 0.691, Sensitive_Loss : 0.22354, Sensitive_Acc : 15.400, Run Time : 12.42 sec
INFO:root:2024-04-10 17:01:57, Train, Epoch : 1, Step : 360, Loss : 0.52141, Acc : 0.716, Sensitive_Loss : 0.29279, Sensitive_Acc : 14.800, Run Time : 9.63 sec
INFO:root:2024-04-10 17:02:09, Train, Epoch : 1, Step : 370, Loss : 0.59908, Acc : 0.722, Sensitive_Loss : 0.22962, Sensitive_Acc : 16.600, Run Time : 12.28 sec
INFO:root:2024-04-10 17:02:22, Train, Epoch : 1, Step : 380, Loss : 0.50532, Acc : 0.728, Sensitive_Loss : 0.28949, Sensitive_Acc : 18.100, Run Time : 12.99 sec
INFO:root:2024-04-10 17:02:32, Train, Epoch : 1, Step : 390, Loss : 0.48216, Acc : 0.728, Sensitive_Loss : 0.23836, Sensitive_Acc : 16.500, Run Time : 9.89 sec
INFO:root:2024-04-10 17:02:41, Train, Epoch : 1, Step : 400, Loss : 0.49965, Acc : 0.738, Sensitive_Loss : 0.23867, Sensitive_Acc : 15.200, Run Time : 9.18 sec
INFO:root:2024-04-10 17:04:13, Dev, Step : 400, Loss : 0.56516, Acc : 0.720, Auc : 0.798, Sensitive_Loss : 0.24998, Sensitive_Acc : 16.307, Sensitive_Auc : 0.983, Mean auc: 0.798, Run Time : 92.35 sec
INFO:root:2024-04-10 17:04:14, Best, Step : 400, Loss : 0.56516, Acc : 0.720, Auc : 0.798, Sensitive_Loss : 0.24998, Sensitive_Acc : 16.307, Sensitive_Auc : 0.983, Best Auc : 0.798
INFO:root:2024-04-10 17:04:21, Train, Epoch : 1, Step : 410, Loss : 0.53086, Acc : 0.709, Sensitive_Loss : 0.25669, Sensitive_Acc : 17.500, Run Time : 100.11 sec
INFO:root:2024-04-10 17:04:34, Train, Epoch : 1, Step : 420, Loss : 0.52011, Acc : 0.731, Sensitive_Loss : 0.20461, Sensitive_Acc : 18.000, Run Time : 13.24 sec
INFO:root:2024-04-10 17:04:48, Train, Epoch : 1, Step : 430, Loss : 0.54026, Acc : 0.697, Sensitive_Loss : 0.23703, Sensitive_Acc : 16.800, Run Time : 13.29 sec
INFO:root:2024-04-10 17:04:57, Train, Epoch : 1, Step : 440, Loss : 0.60915, Acc : 0.669, Sensitive_Loss : 0.20242, Sensitive_Acc : 16.300, Run Time : 9.77 sec
INFO:root:2024-04-10 17:05:08, Train, Epoch : 1, Step : 450, Loss : 0.55160, Acc : 0.722, Sensitive_Loss : 0.20459, Sensitive_Acc : 16.700, Run Time : 10.33 sec
INFO:root:2024-04-10 17:05:21, Train, Epoch : 1, Step : 460, Loss : 0.56325, Acc : 0.713, Sensitive_Loss : 0.25327, Sensitive_Acc : 14.800, Run Time : 12.92 sec
INFO:root:2024-04-10 17:05:30, Train, Epoch : 1, Step : 470, Loss : 0.64279, Acc : 0.725, Sensitive_Loss : 0.24376, Sensitive_Acc : 16.100, Run Time : 9.30 sec
INFO:root:2024-04-10 17:05:39, Train, Epoch : 1, Step : 480, Loss : 0.53634, Acc : 0.709, Sensitive_Loss : 0.21877, Sensitive_Acc : 16.900, Run Time : 9.52 sec
INFO:root:2024-04-10 17:05:51, Train, Epoch : 1, Step : 490, Loss : 0.53344, Acc : 0.700, Sensitive_Loss : 0.20760, Sensitive_Acc : 17.900, Run Time : 11.76 sec
INFO:root:2024-04-10 17:06:02, Train, Epoch : 1, Step : 500, Loss : 0.65124, Acc : 0.697, Sensitive_Loss : 0.19071, Sensitive_Acc : 15.300, Run Time : 10.58 sec
INFO:root:2024-04-10 17:07:40, Dev, Step : 500, Loss : 0.55465, Acc : 0.735, Auc : 0.811, Sensitive_Loss : 0.20835, Sensitive_Acc : 16.107, Sensitive_Auc : 0.984, Mean auc: 0.811, Run Time : 98.28 sec
INFO:root:2024-04-10 17:07:41, Best, Step : 500, Loss : 0.55465, Acc : 0.735, Auc : 0.811, Sensitive_Loss : 0.20835, Sensitive_Acc : 16.107, Sensitive_Auc : 0.984, Best Auc : 0.811
INFO:root:2024-04-10 17:07:49, Train, Epoch : 1, Step : 510, Loss : 0.63895, Acc : 0.684, Sensitive_Loss : 0.24352, Sensitive_Acc : 16.400, Run Time : 107.20 sec
INFO:root:2024-04-10 17:08:02, Train, Epoch : 1, Step : 520, Loss : 0.53041, Acc : 0.703, Sensitive_Loss : 0.25054, Sensitive_Acc : 14.800, Run Time : 13.08 sec
INFO:root:2024-04-10 17:08:15, Train, Epoch : 1, Step : 530, Loss : 0.61668, Acc : 0.700, Sensitive_Loss : 0.28147, Sensitive_Acc : 16.000, Run Time : 12.48 sec
INFO:root:2024-04-10 17:08:26, Train, Epoch : 1, Step : 540, Loss : 0.57690, Acc : 0.694, Sensitive_Loss : 0.22572, Sensitive_Acc : 14.400, Run Time : 11.93 sec
INFO:root:2024-04-10 17:08:38, Train, Epoch : 1, Step : 550, Loss : 0.56830, Acc : 0.725, Sensitive_Loss : 0.18465, Sensitive_Acc : 17.400, Run Time : 11.92 sec
INFO:root:2024-04-10 17:08:49, Train, Epoch : 1, Step : 560, Loss : 0.62100, Acc : 0.684, Sensitive_Loss : 0.19898, Sensitive_Acc : 14.400, Run Time : 10.66 sec
INFO:root:2024-04-10 17:09:01, Train, Epoch : 1, Step : 570, Loss : 0.53000, Acc : 0.725, Sensitive_Loss : 0.21339, Sensitive_Acc : 15.400, Run Time : 12.45 sec
INFO:root:2024-04-10 17:09:14, Train, Epoch : 1, Step : 580, Loss : 0.55159, Acc : 0.706, Sensitive_Loss : 0.18581, Sensitive_Acc : 17.600, Run Time : 12.20 sec
INFO:root:2024-04-10 17:09:23, Train, Epoch : 1, Step : 590, Loss : 0.52674, Acc : 0.725, Sensitive_Loss : 0.17950, Sensitive_Acc : 15.500, Run Time : 9.68 sec
INFO:root:2024-04-10 17:09:34, Train, Epoch : 1, Step : 600, Loss : 0.51130, Acc : 0.713, Sensitive_Loss : 0.25014, Sensitive_Acc : 18.600, Run Time : 11.00 sec
INFO:root:2024-04-10 17:11:52, Dev, Step : 600, Loss : 0.61020, Acc : 0.697, Auc : 0.806, Sensitive_Loss : 0.22316, Sensitive_Acc : 16.279, Sensitive_Auc : 0.985, Mean auc: 0.806, Run Time : 137.78 sec
INFO:root:2024-04-10 17:12:02, Train, Epoch : 1, Step : 610, Loss : 0.55451, Acc : 0.697, Sensitive_Loss : 0.21358, Sensitive_Acc : 17.100, Run Time : 147.19 sec
INFO:root:2024-04-10 17:12:13, Train, Epoch : 1, Step : 620, Loss : 0.49520, Acc : 0.731, Sensitive_Loss : 0.21104, Sensitive_Acc : 16.000, Run Time : 11.45 sec
INFO:root:2024-04-10 17:12:25, Train, Epoch : 1, Step : 630, Loss : 0.54118, Acc : 0.734, Sensitive_Loss : 0.20876, Sensitive_Acc : 16.700, Run Time : 11.51 sec
INFO:root:2024-04-10 17:12:35, Train, Epoch : 1, Step : 640, Loss : 0.54613, Acc : 0.713, Sensitive_Loss : 0.20165, Sensitive_Acc : 15.500, Run Time : 10.03 sec
INFO:root:2024-04-10 17:14:18
INFO:root:y_pred: [0.30880046 0.1956515  0.7724736  ... 0.42512202 0.6010467  0.5329181 ]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9985433e-01 1.7976442e-02 7.4032462e-01 9.9996471e-01 9.9511695e-01
 9.9635160e-01 9.9940419e-01 3.7639542e-04 9.7488755e-01 9.9752134e-01
 1.3888672e-01 7.4159801e-01 6.8584848e-03 9.9487114e-01 9.9996746e-01
 9.9970299e-01 9.9995542e-01 8.9060152e-01 9.9955755e-01 9.9934489e-01
 9.9971074e-01 3.1713265e-01 9.9995136e-01 9.8832601e-01 9.9420679e-01
 3.5312176e-01 8.6356843e-01 8.4468395e-01 9.9998260e-01 2.3715623e-02
 4.4676963e-02 7.2048545e-01 2.1099998e-02 9.9984920e-01 2.3029339e-03
 9.9975961e-01 2.9121991e-03 9.9999094e-01 1.1143484e-01 9.9785030e-01
 9.9988604e-01 5.6061437e-03 5.1813167e-01 1.0202595e-02 1.4572969e-01
 8.5855186e-01 9.9844307e-01 9.8863518e-01 9.9293208e-01 9.9664199e-01
 2.5628656e-02 4.9390808e-01 1.3645838e-01 4.2228606e-01 9.9993551e-01
 3.0374297e-01 9.9899572e-01 9.9970335e-01 6.8966681e-01 1.5063877e-01
 1.6825801e-01 9.9892586e-01 2.4323077e-01 9.9996388e-01 9.9774987e-01
 7.4640387e-01 9.6990675e-01 9.6454531e-01 9.9996924e-01 9.9888033e-01
 7.9303227e-02 9.7595519e-01 9.8013204e-01 9.9834251e-01 9.9812084e-01
 2.0547755e-02 6.9002664e-01 1.6847494e-01 1.5079375e-02 9.9602902e-01
 2.6741034e-01 9.9969554e-01 9.9991298e-01 9.9895072e-01 2.0513268e-01
 9.9999940e-01 2.4847770e-02 5.8980815e-02 9.9960166e-01 9.9232650e-01
 6.8552953e-01 9.8779011e-01 2.8691205e-01 8.9830458e-01 6.5057796e-01
 9.9993682e-01 3.9857361e-01 9.9942648e-01 9.9727160e-01 6.9475123e-03
 4.9279514e-03 8.1724489e-01 9.9979502e-01 9.9608934e-01 9.9519831e-01
 9.7874212e-01 9.9984813e-01 3.6053026e-01 9.1234839e-01 9.9851114e-01
 1.2058164e-03 5.3570980e-01 5.0431782e-01 9.9974817e-01 9.9856901e-01
 1.1906811e-02 9.6357685e-01 5.8279857e-03 9.9999809e-01 9.4304860e-01
 9.9931240e-01 9.9968719e-01 4.2297587e-01 4.3146256e-01 2.2545154e-01
 4.1735433e-02 9.3145356e-02 1.1873284e-01 9.9962699e-01 9.9991167e-01
 8.2520163e-03 2.9613337e-02 2.0382978e-02 9.6319944e-01 9.9988878e-01
 9.9996924e-01 9.9960154e-01 7.1404588e-01 5.1270860e-01 9.8320109e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 17:14:18, Dev, Step : 644, Loss : 0.55651, Acc : 0.729, Auc : 0.808, Sensitive_Loss : 0.35526, Sensitive_Acc : 16.336, Sensitive_Auc : 0.993, Mean auc: 0.808, Run Time : 99.17 sec
INFO:root:2024-04-10 17:14:25, Train, Epoch : 2, Step : 650, Loss : 0.36435, Acc : 0.431, Sensitive_Loss : 0.12319, Sensitive_Acc : 9.300, Run Time : 6.07 sec
INFO:root:2024-04-10 17:14:35, Train, Epoch : 2, Step : 660, Loss : 0.54556, Acc : 0.744, Sensitive_Loss : 0.21205, Sensitive_Acc : 17.400, Run Time : 9.82 sec
INFO:root:2024-04-10 17:14:46, Train, Epoch : 2, Step : 670, Loss : 0.56409, Acc : 0.747, Sensitive_Loss : 0.21871, Sensitive_Acc : 16.900, Run Time : 10.86 sec
INFO:root:2024-04-10 17:14:59, Train, Epoch : 2, Step : 680, Loss : 0.49974, Acc : 0.759, Sensitive_Loss : 0.18180, Sensitive_Acc : 16.500, Run Time : 13.42 sec
INFO:root:2024-04-10 17:15:09, Train, Epoch : 2, Step : 690, Loss : 0.47248, Acc : 0.744, Sensitive_Loss : 0.16130, Sensitive_Acc : 16.600, Run Time : 9.50 sec
INFO:root:2024-04-10 17:15:19, Train, Epoch : 2, Step : 700, Loss : 0.47382, Acc : 0.766, Sensitive_Loss : 0.16537, Sensitive_Acc : 16.800, Run Time : 10.29 sec
INFO:root:2024-04-10 17:16:54, Dev, Step : 700, Loss : 0.56276, Acc : 0.738, Auc : 0.825, Sensitive_Loss : 0.24686, Sensitive_Acc : 16.250, Sensitive_Auc : 0.987, Mean auc: 0.825, Run Time : 95.39 sec
INFO:root:2024-04-10 17:16:55, Best, Step : 700, Loss : 0.56276, Acc : 0.738, Auc : 0.825, Sensitive_Loss : 0.24686, Sensitive_Acc : 16.250, Sensitive_Auc : 0.987, Best Auc : 0.825
INFO:root:2024-04-10 17:17:02, Train, Epoch : 2, Step : 710, Loss : 0.49611, Acc : 0.753, Sensitive_Loss : 0.16825, Sensitive_Acc : 14.500, Run Time : 103.06 sec
INFO:root:2024-04-10 17:17:15, Train, Epoch : 2, Step : 720, Loss : 0.53722, Acc : 0.738, Sensitive_Loss : 0.18684, Sensitive_Acc : 15.500, Run Time : 12.42 sec
INFO:root:2024-04-10 17:17:24, Train, Epoch : 2, Step : 730, Loss : 0.51067, Acc : 0.734, Sensitive_Loss : 0.16982, Sensitive_Acc : 16.100, Run Time : 9.87 sec
INFO:root:2024-04-10 17:17:35, Train, Epoch : 2, Step : 740, Loss : 0.52885, Acc : 0.731, Sensitive_Loss : 0.20255, Sensitive_Acc : 16.600, Run Time : 10.83 sec
INFO:root:2024-04-10 17:17:48, Train, Epoch : 2, Step : 750, Loss : 0.53250, Acc : 0.741, Sensitive_Loss : 0.15554, Sensitive_Acc : 16.700, Run Time : 13.04 sec
INFO:root:2024-04-10 17:17:58, Train, Epoch : 2, Step : 760, Loss : 0.44969, Acc : 0.781, Sensitive_Loss : 0.20386, Sensitive_Acc : 15.600, Run Time : 9.91 sec
INFO:root:2024-04-10 17:18:08, Train, Epoch : 2, Step : 770, Loss : 0.52439, Acc : 0.753, Sensitive_Loss : 0.14579, Sensitive_Acc : 17.000, Run Time : 10.18 sec
INFO:root:2024-04-10 17:18:19, Train, Epoch : 2, Step : 780, Loss : 0.50620, Acc : 0.772, Sensitive_Loss : 0.15760, Sensitive_Acc : 15.700, Run Time : 11.14 sec
INFO:root:2024-04-10 17:18:30, Train, Epoch : 2, Step : 790, Loss : 0.51068, Acc : 0.725, Sensitive_Loss : 0.15176, Sensitive_Acc : 14.900, Run Time : 10.40 sec
INFO:root:2024-04-10 17:18:40, Train, Epoch : 2, Step : 800, Loss : 0.54598, Acc : 0.731, Sensitive_Loss : 0.18115, Sensitive_Acc : 17.600, Run Time : 9.84 sec
INFO:root:2024-04-10 17:20:12, Dev, Step : 800, Loss : 0.55256, Acc : 0.732, Auc : 0.818, Sensitive_Loss : 0.18893, Sensitive_Acc : 16.321, Sensitive_Auc : 0.987, Mean auc: 0.818, Run Time : 92.38 sec
INFO:root:2024-04-10 17:20:19, Train, Epoch : 2, Step : 810, Loss : 0.53093, Acc : 0.728, Sensitive_Loss : 0.17976, Sensitive_Acc : 14.100, Run Time : 99.45 sec
INFO:root:2024-04-10 17:20:30, Train, Epoch : 2, Step : 820, Loss : 0.50336, Acc : 0.778, Sensitive_Loss : 0.16266, Sensitive_Acc : 17.100, Run Time : 10.69 sec
INFO:root:2024-04-10 17:20:42, Train, Epoch : 2, Step : 830, Loss : 0.54275, Acc : 0.741, Sensitive_Loss : 0.22308, Sensitive_Acc : 17.600, Run Time : 12.41 sec
INFO:root:2024-04-10 17:20:55, Train, Epoch : 2, Step : 840, Loss : 0.50637, Acc : 0.747, Sensitive_Loss : 0.15631, Sensitive_Acc : 17.300, Run Time : 12.22 sec
INFO:root:2024-04-10 17:21:05, Train, Epoch : 2, Step : 850, Loss : 0.49487, Acc : 0.744, Sensitive_Loss : 0.14505, Sensitive_Acc : 15.900, Run Time : 10.04 sec
INFO:root:2024-04-10 17:21:15, Train, Epoch : 2, Step : 860, Loss : 0.51035, Acc : 0.759, Sensitive_Loss : 0.13478, Sensitive_Acc : 16.900, Run Time : 10.36 sec
INFO:root:2024-04-10 17:21:28, Train, Epoch : 2, Step : 870, Loss : 0.47143, Acc : 0.794, Sensitive_Loss : 0.19584, Sensitive_Acc : 16.200, Run Time : 13.28 sec
INFO:root:2024-04-10 17:21:38, Train, Epoch : 2, Step : 880, Loss : 0.49609, Acc : 0.784, Sensitive_Loss : 0.16789, Sensitive_Acc : 16.100, Run Time : 9.57 sec
INFO:root:2024-04-10 17:21:48, Train, Epoch : 2, Step : 890, Loss : 0.49905, Acc : 0.722, Sensitive_Loss : 0.20160, Sensitive_Acc : 16.500, Run Time : 9.83 sec
INFO:root:2024-04-10 17:21:56, Train, Epoch : 2, Step : 900, Loss : 0.51123, Acc : 0.738, Sensitive_Loss : 0.17294, Sensitive_Acc : 14.600, Run Time : 8.59 sec
INFO:root:2024-04-10 17:23:38, Dev, Step : 900, Loss : 0.69679, Acc : 0.663, Auc : 0.822, Sensitive_Loss : 0.30536, Sensitive_Acc : 16.164, Sensitive_Auc : 0.991, Mean auc: 0.822, Run Time : 102.30 sec
INFO:root:2024-04-10 17:23:47, Train, Epoch : 2, Step : 910, Loss : 0.57796, Acc : 0.697, Sensitive_Loss : 0.21520, Sensitive_Acc : 15.300, Run Time : 111.22 sec
INFO:root:2024-04-10 17:24:02, Train, Epoch : 2, Step : 920, Loss : 0.48463, Acc : 0.756, Sensitive_Loss : 0.17684, Sensitive_Acc : 17.700, Run Time : 14.60 sec
INFO:root:2024-04-10 17:24:12, Train, Epoch : 2, Step : 930, Loss : 0.55432, Acc : 0.750, Sensitive_Loss : 0.17550, Sensitive_Acc : 15.900, Run Time : 9.50 sec
INFO:root:2024-04-10 17:24:24, Train, Epoch : 2, Step : 940, Loss : 0.50991, Acc : 0.759, Sensitive_Loss : 0.17496, Sensitive_Acc : 17.000, Run Time : 12.66 sec
INFO:root:2024-04-10 17:24:37, Train, Epoch : 2, Step : 950, Loss : 0.48008, Acc : 0.784, Sensitive_Loss : 0.11912, Sensitive_Acc : 15.900, Run Time : 13.06 sec
INFO:root:2024-04-10 17:24:47, Train, Epoch : 2, Step : 960, Loss : 0.45858, Acc : 0.762, Sensitive_Loss : 0.21553, Sensitive_Acc : 16.200, Run Time : 9.50 sec
INFO:root:2024-04-10 17:24:57, Train, Epoch : 2, Step : 970, Loss : 0.49927, Acc : 0.781, Sensitive_Loss : 0.14302, Sensitive_Acc : 16.100, Run Time : 9.97 sec
INFO:root:2024-04-10 17:25:09, Train, Epoch : 2, Step : 980, Loss : 0.51569, Acc : 0.753, Sensitive_Loss : 0.14698, Sensitive_Acc : 16.200, Run Time : 12.20 sec
INFO:root:2024-04-10 17:25:19, Train, Epoch : 2, Step : 990, Loss : 0.49232, Acc : 0.750, Sensitive_Loss : 0.19814, Sensitive_Acc : 16.800, Run Time : 10.00 sec
INFO:root:2024-04-10 17:25:29, Train, Epoch : 2, Step : 1000, Loss : 0.57859, Acc : 0.728, Sensitive_Loss : 0.17747, Sensitive_Acc : 15.600, Run Time : 9.68 sec
INFO:root:2024-04-10 17:27:22, Dev, Step : 1000, Loss : 0.54974, Acc : 0.745, Auc : 0.817, Sensitive_Loss : 0.26283, Sensitive_Acc : 16.464, Sensitive_Auc : 0.985, Mean auc: 0.817, Run Time : 113.87 sec
INFO:root:2024-04-10 17:27:30, Train, Epoch : 2, Step : 1010, Loss : 0.50926, Acc : 0.722, Sensitive_Loss : 0.17248, Sensitive_Acc : 16.100, Run Time : 120.99 sec
INFO:root:2024-04-10 17:27:39, Train, Epoch : 2, Step : 1020, Loss : 0.60505, Acc : 0.725, Sensitive_Loss : 0.15185, Sensitive_Acc : 14.700, Run Time : 9.00 sec
INFO:root:2024-04-10 17:27:49, Train, Epoch : 2, Step : 1030, Loss : 0.47586, Acc : 0.738, Sensitive_Loss : 0.14725, Sensitive_Acc : 16.900, Run Time : 10.45 sec
INFO:root:2024-04-10 17:28:01, Train, Epoch : 2, Step : 1040, Loss : 0.53922, Acc : 0.738, Sensitive_Loss : 0.10825, Sensitive_Acc : 15.800, Run Time : 11.85 sec
INFO:root:2024-04-10 17:28:10, Train, Epoch : 2, Step : 1050, Loss : 0.48586, Acc : 0.753, Sensitive_Loss : 0.14619, Sensitive_Acc : 16.800, Run Time : 9.03 sec
INFO:root:2024-04-10 17:28:19, Train, Epoch : 2, Step : 1060, Loss : 0.45437, Acc : 0.772, Sensitive_Loss : 0.18783, Sensitive_Acc : 17.100, Run Time : 9.45 sec
INFO:root:2024-04-10 17:28:29, Train, Epoch : 2, Step : 1070, Loss : 0.46557, Acc : 0.781, Sensitive_Loss : 0.18216, Sensitive_Acc : 15.800, Run Time : 10.06 sec
INFO:root:2024-04-10 17:28:38, Train, Epoch : 2, Step : 1080, Loss : 0.57840, Acc : 0.722, Sensitive_Loss : 0.14445, Sensitive_Acc : 14.900, Run Time : 8.96 sec
INFO:root:2024-04-10 17:28:47, Train, Epoch : 2, Step : 1090, Loss : 0.54360, Acc : 0.731, Sensitive_Loss : 0.17802, Sensitive_Acc : 17.100, Run Time : 8.61 sec
INFO:root:2024-04-10 17:28:56, Train, Epoch : 2, Step : 1100, Loss : 0.50146, Acc : 0.759, Sensitive_Loss : 0.14982, Sensitive_Acc : 16.000, Run Time : 9.45 sec
INFO:root:2024-04-10 17:30:29, Dev, Step : 1100, Loss : 0.58408, Acc : 0.706, Auc : 0.826, Sensitive_Loss : 0.17938, Sensitive_Acc : 16.236, Sensitive_Auc : 0.993, Mean auc: 0.826, Run Time : 92.98 sec
INFO:root:2024-04-10 17:30:30, Best, Step : 1100, Loss : 0.58408, Acc : 0.706, Auc : 0.826, Sensitive_Loss : 0.17938, Sensitive_Acc : 16.236, Sensitive_Auc : 0.993, Best Auc : 0.826
INFO:root:2024-04-10 17:30:39, Train, Epoch : 2, Step : 1110, Loss : 0.47516, Acc : 0.759, Sensitive_Loss : 0.15445, Sensitive_Acc : 16.000, Run Time : 102.92 sec
INFO:root:2024-04-10 17:30:52, Train, Epoch : 2, Step : 1120, Loss : 0.51181, Acc : 0.738, Sensitive_Loss : 0.15594, Sensitive_Acc : 14.900, Run Time : 13.09 sec
INFO:root:2024-04-10 17:31:03, Train, Epoch : 2, Step : 1130, Loss : 0.43383, Acc : 0.744, Sensitive_Loss : 0.11832, Sensitive_Acc : 16.500, Run Time : 10.32 sec
INFO:root:2024-04-10 17:31:13, Train, Epoch : 2, Step : 1140, Loss : 0.54974, Acc : 0.744, Sensitive_Loss : 0.15845, Sensitive_Acc : 18.000, Run Time : 10.66 sec
INFO:root:2024-04-10 17:31:27, Train, Epoch : 2, Step : 1150, Loss : 0.42515, Acc : 0.784, Sensitive_Loss : 0.14520, Sensitive_Acc : 16.000, Run Time : 13.90 sec
INFO:root:2024-04-10 17:31:37, Train, Epoch : 2, Step : 1160, Loss : 0.54660, Acc : 0.766, Sensitive_Loss : 0.14183, Sensitive_Acc : 14.400, Run Time : 9.82 sec
INFO:root:2024-04-10 17:31:48, Train, Epoch : 2, Step : 1170, Loss : 0.51145, Acc : 0.738, Sensitive_Loss : 0.11541, Sensitive_Acc : 15.400, Run Time : 10.64 sec
INFO:root:2024-04-10 17:32:02, Train, Epoch : 2, Step : 1180, Loss : 0.50834, Acc : 0.775, Sensitive_Loss : 0.13672, Sensitive_Acc : 17.000, Run Time : 14.20 sec
INFO:root:2024-04-10 17:32:12, Train, Epoch : 2, Step : 1190, Loss : 0.58058, Acc : 0.709, Sensitive_Loss : 0.12255, Sensitive_Acc : 15.700, Run Time : 10.16 sec
INFO:root:2024-04-10 17:32:23, Train, Epoch : 2, Step : 1200, Loss : 0.52992, Acc : 0.747, Sensitive_Loss : 0.13453, Sensitive_Acc : 15.700, Run Time : 11.27 sec
INFO:root:2024-04-10 17:34:24, Dev, Step : 1200, Loss : 0.57074, Acc : 0.725, Auc : 0.816, Sensitive_Loss : 0.24204, Sensitive_Acc : 16.136, Sensitive_Auc : 0.992, Mean auc: 0.816, Run Time : 120.18 sec
INFO:root:2024-04-10 17:34:30, Train, Epoch : 2, Step : 1210, Loss : 0.52217, Acc : 0.781, Sensitive_Loss : 0.17384, Sensitive_Acc : 18.600, Run Time : 126.96 sec
INFO:root:2024-04-10 17:34:40, Train, Epoch : 2, Step : 1220, Loss : 0.50411, Acc : 0.756, Sensitive_Loss : 0.20595, Sensitive_Acc : 15.400, Run Time : 9.97 sec
INFO:root:2024-04-10 17:34:53, Train, Epoch : 2, Step : 1230, Loss : 0.51576, Acc : 0.762, Sensitive_Loss : 0.19905, Sensitive_Acc : 18.700, Run Time : 12.53 sec
INFO:root:2024-04-10 17:35:04, Train, Epoch : 2, Step : 1240, Loss : 0.47485, Acc : 0.769, Sensitive_Loss : 0.15390, Sensitive_Acc : 17.200, Run Time : 10.85 sec
INFO:root:2024-04-10 17:35:13, Train, Epoch : 2, Step : 1250, Loss : 0.46305, Acc : 0.791, Sensitive_Loss : 0.18321, Sensitive_Acc : 18.800, Run Time : 8.96 sec
INFO:root:2024-04-10 17:35:27, Train, Epoch : 2, Step : 1260, Loss : 0.39202, Acc : 0.775, Sensitive_Loss : 0.14592, Sensitive_Acc : 17.200, Run Time : 14.11 sec
INFO:root:2024-04-10 17:35:37, Train, Epoch : 2, Step : 1270, Loss : 0.47694, Acc : 0.759, Sensitive_Loss : 0.10912, Sensitive_Acc : 15.200, Run Time : 10.57 sec
INFO:root:2024-04-10 17:35:46, Train, Epoch : 2, Step : 1280, Loss : 0.55248, Acc : 0.744, Sensitive_Loss : 0.16666, Sensitive_Acc : 16.300, Run Time : 9.09 sec
INFO:root:2024-04-10 17:37:28
INFO:root:y_pred: [0.35725674 0.11842783 0.79941505 ... 0.21016265 0.2944551  0.5885767 ]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9990761e-01 3.0535774e-02 7.1858090e-01 9.9998367e-01 9.9828112e-01
 9.9363858e-01 9.9984360e-01 3.1754338e-05 9.9112248e-01 9.9331313e-01
 4.2712754e-01 4.3908498e-01 1.2724732e-04 9.0805376e-01 9.9937254e-01
 9.9979597e-01 9.9882132e-01 9.9502242e-01 9.9592650e-01 9.9932742e-01
 9.8855060e-01 1.7691478e-01 9.9990475e-01 9.0994555e-01 8.2102126e-01
 6.2092347e-03 9.8766553e-01 5.7159901e-02 9.9992204e-01 1.1190131e-03
 7.0593998e-02 3.4064797e-01 1.6177330e-02 9.9275321e-01 2.7123035e-04
 9.9968255e-01 2.1022157e-04 9.9991488e-01 8.6576842e-02 9.8588353e-01
 9.9980587e-01 5.2013230e-03 1.4058150e-01 1.2896814e-03 6.2336898e-01
 5.1029527e-01 9.9984372e-01 8.0639690e-01 9.9844837e-01 9.9665904e-01
 1.0144355e-02 8.1915504e-01 8.5201561e-03 6.7126209e-01 9.9984097e-01
 2.7690414e-01 9.8111570e-01 9.9973601e-01 9.9084622e-01 2.0895669e-01
 1.3711094e-02 9.9833220e-01 4.1276288e-01 9.9984980e-01 9.9783236e-01
 3.1743661e-01 9.9430048e-01 8.5946113e-01 9.9995172e-01 9.9992454e-01
 1.6385880e-01 4.6528441e-01 9.7850204e-01 9.9972934e-01 9.3818569e-01
 1.0506556e-02 7.9867971e-01 1.8180566e-02 7.0461509e-04 9.8715425e-01
 9.4874129e-02 9.9924242e-01 9.9962234e-01 9.9873048e-01 5.0295502e-01
 9.9999928e-01 1.2280588e-03 6.1015831e-03 9.9746835e-01 9.9280691e-01
 1.7945835e-02 8.3354318e-01 4.4683430e-02 7.6192194e-01 1.7900003e-01
 9.9990213e-01 5.2093877e-03 9.9944490e-01 9.6927613e-01 4.7551360e-04
 3.7261567e-04 6.4487882e-02 9.8564583e-01 9.9983752e-01 9.9450761e-01
 9.4012505e-01 9.9973363e-01 9.7535379e-02 6.1912048e-01 9.9992228e-01
 6.0073999e-05 1.9912999e-02 4.0858978e-01 9.9956530e-01 9.8860651e-01
 4.3347402e-04 9.9228245e-01 2.0391615e-03 9.9999356e-01 8.4834450e-01
 9.9965131e-01 9.9993074e-01 5.9480226e-01 1.2384709e-01 2.3863351e-02
 2.7529462e-03 1.2427569e-02 3.1184042e-03 9.9753213e-01 9.9998951e-01
 1.2242688e-02 1.8808714e-03 1.7607799e-03 5.0713372e-01 9.9980813e-01
 9.9765801e-01 9.9890780e-01 7.9257473e-02 2.0540763e-01 9.9303943e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 17:37:28, Dev, Step : 1288, Loss : 0.56881, Acc : 0.729, Auc : 0.837, Sensitive_Loss : 0.19419, Sensitive_Acc : 16.107, Sensitive_Auc : 0.993, Mean auc: 0.837, Run Time : 94.98 sec
INFO:root:2024-04-10 17:37:29, Best, Step : 1288, Loss : 0.56881, Acc : 0.729,Auc : 0.837, Best Auc : 0.837, Sensitive_Loss : 0.19419, Sensitive_Acc : 16.107, Sensitive_Auc : 0.993
INFO:root:2024-04-10 17:37:33, Train, Epoch : 3, Step : 1290, Loss : 0.08734, Acc : 0.153, Sensitive_Loss : 0.04063, Sensitive_Acc : 3.000, Run Time : 2.77 sec
INFO:root:2024-04-10 17:37:42, Train, Epoch : 3, Step : 1300, Loss : 0.48431, Acc : 0.738, Sensitive_Loss : 0.13310, Sensitive_Acc : 17.100, Run Time : 9.23 sec
INFO:root:2024-04-10 17:39:17, Dev, Step : 1300, Loss : 0.53521, Acc : 0.749, Auc : 0.838, Sensitive_Loss : 0.20448, Sensitive_Acc : 16.193, Sensitive_Auc : 0.995, Mean auc: 0.838, Run Time : 94.23 sec
INFO:root:2024-04-10 17:39:17, Best, Step : 1300, Loss : 0.53521, Acc : 0.749, Auc : 0.838, Sensitive_Loss : 0.20448, Sensitive_Acc : 16.193, Sensitive_Auc : 0.995, Best Auc : 0.838
INFO:root:2024-04-10 17:39:25, Train, Epoch : 3, Step : 1310, Loss : 0.47050, Acc : 0.775, Sensitive_Loss : 0.11722, Sensitive_Acc : 15.400, Run Time : 102.23 sec
INFO:root:2024-04-10 17:39:38, Train, Epoch : 3, Step : 1320, Loss : 0.40048, Acc : 0.791, Sensitive_Loss : 0.13708, Sensitive_Acc : 16.900, Run Time : 13.47 sec
INFO:root:2024-04-10 17:39:48, Train, Epoch : 3, Step : 1330, Loss : 0.50703, Acc : 0.734, Sensitive_Loss : 0.12876, Sensitive_Acc : 16.600, Run Time : 10.31 sec
INFO:root:2024-04-10 17:39:59, Train, Epoch : 3, Step : 1340, Loss : 0.53234, Acc : 0.787, Sensitive_Loss : 0.13284, Sensitive_Acc : 16.300, Run Time : 10.40 sec
INFO:root:2024-04-10 17:40:12, Train, Epoch : 3, Step : 1350, Loss : 0.40052, Acc : 0.781, Sensitive_Loss : 0.12340, Sensitive_Acc : 15.200, Run Time : 12.74 sec
INFO:root:2024-04-10 17:40:22, Train, Epoch : 3, Step : 1360, Loss : 0.40687, Acc : 0.841, Sensitive_Loss : 0.17044, Sensitive_Acc : 15.500, Run Time : 10.92 sec
INFO:root:2024-04-10 17:40:32, Train, Epoch : 3, Step : 1370, Loss : 0.46980, Acc : 0.766, Sensitive_Loss : 0.14187, Sensitive_Acc : 17.500, Run Time : 9.99 sec
INFO:root:2024-04-10 17:40:43, Train, Epoch : 3, Step : 1380, Loss : 0.41671, Acc : 0.778, Sensitive_Loss : 0.11876, Sensitive_Acc : 15.300, Run Time : 10.60 sec
INFO:root:2024-04-10 17:40:53, Train, Epoch : 3, Step : 1390, Loss : 0.41889, Acc : 0.819, Sensitive_Loss : 0.13091, Sensitive_Acc : 13.100, Run Time : 10.31 sec
INFO:root:2024-04-10 17:41:03, Train, Epoch : 3, Step : 1400, Loss : 0.47248, Acc : 0.791, Sensitive_Loss : 0.15236, Sensitive_Acc : 16.000, Run Time : 9.31 sec
INFO:root:2024-04-10 17:42:38, Dev, Step : 1400, Loss : 0.52649, Acc : 0.759, Auc : 0.845, Sensitive_Loss : 0.15371, Sensitive_Acc : 16.350, Sensitive_Auc : 0.996, Mean auc: 0.845, Run Time : 95.70 sec
INFO:root:2024-04-10 17:42:39, Best, Step : 1400, Loss : 0.52649, Acc : 0.759, Auc : 0.845, Sensitive_Loss : 0.15371, Sensitive_Acc : 16.350, Sensitive_Auc : 0.996, Best Auc : 0.845
INFO:root:2024-04-10 17:42:47, Train, Epoch : 3, Step : 1410, Loss : 0.41584, Acc : 0.772, Sensitive_Loss : 0.14274, Sensitive_Acc : 14.800, Run Time : 103.91 sec
INFO:root:2024-04-10 17:43:00, Train, Epoch : 3, Step : 1420, Loss : 0.44168, Acc : 0.766, Sensitive_Loss : 0.12823, Sensitive_Acc : 18.100, Run Time : 13.35 sec
INFO:root:2024-04-10 17:43:14, Train, Epoch : 3, Step : 1430, Loss : 0.44659, Acc : 0.819, Sensitive_Loss : 0.13042, Sensitive_Acc : 15.600, Run Time : 13.66 sec
INFO:root:2024-04-10 17:43:24, Train, Epoch : 3, Step : 1440, Loss : 0.43095, Acc : 0.772, Sensitive_Loss : 0.11921, Sensitive_Acc : 17.100, Run Time : 10.06 sec
INFO:root:2024-04-10 17:43:37, Train, Epoch : 3, Step : 1450, Loss : 0.45821, Acc : 0.756, Sensitive_Loss : 0.12117, Sensitive_Acc : 17.400, Run Time : 13.47 sec
INFO:root:2024-04-10 17:43:48, Train, Epoch : 3, Step : 1460, Loss : 0.40521, Acc : 0.781, Sensitive_Loss : 0.08348, Sensitive_Acc : 17.000, Run Time : 10.97 sec
INFO:root:2024-04-10 17:43:58, Train, Epoch : 3, Step : 1470, Loss : 0.46885, Acc : 0.781, Sensitive_Loss : 0.11911, Sensitive_Acc : 15.900, Run Time : 9.74 sec
INFO:root:2024-04-10 17:44:09, Train, Epoch : 3, Step : 1480, Loss : 0.46404, Acc : 0.794, Sensitive_Loss : 0.14410, Sensitive_Acc : 15.400, Run Time : 10.80 sec
INFO:root:2024-04-10 17:44:21, Train, Epoch : 3, Step : 1490, Loss : 0.45662, Acc : 0.756, Sensitive_Loss : 0.17165, Sensitive_Acc : 15.100, Run Time : 12.79 sec
INFO:root:2024-04-10 17:44:31, Train, Epoch : 3, Step : 1500, Loss : 0.46598, Acc : 0.787, Sensitive_Loss : 0.12242, Sensitive_Acc : 16.500, Run Time : 9.36 sec
INFO:root:2024-04-10 17:46:03, Dev, Step : 1500, Loss : 0.51576, Acc : 0.767, Auc : 0.847, Sensitive_Loss : 0.16417, Sensitive_Acc : 16.393, Sensitive_Auc : 0.996, Mean auc: 0.847, Run Time : 92.36 sec
INFO:root:2024-04-10 17:46:04, Best, Step : 1500, Loss : 0.51576, Acc : 0.767, Auc : 0.847, Sensitive_Loss : 0.16417, Sensitive_Acc : 16.393, Sensitive_Auc : 0.996, Best Auc : 0.847
INFO:root:2024-04-10 17:46:11, Train, Epoch : 3, Step : 1510, Loss : 0.47119, Acc : 0.775, Sensitive_Loss : 0.09650, Sensitive_Acc : 18.400, Run Time : 99.81 sec
INFO:root:2024-04-10 17:46:21, Train, Epoch : 3, Step : 1520, Loss : 0.42234, Acc : 0.803, Sensitive_Loss : 0.12653, Sensitive_Acc : 15.100, Run Time : 10.26 sec
INFO:root:2024-04-10 17:46:31, Train, Epoch : 3, Step : 1530, Loss : 0.42858, Acc : 0.800, Sensitive_Loss : 0.13511, Sensitive_Acc : 15.900, Run Time : 10.11 sec
INFO:root:2024-04-10 17:46:44, Train, Epoch : 3, Step : 1540, Loss : 0.48828, Acc : 0.762, Sensitive_Loss : 0.09306, Sensitive_Acc : 14.900, Run Time : 13.03 sec
INFO:root:2024-04-10 17:46:53, Train, Epoch : 3, Step : 1550, Loss : 0.45414, Acc : 0.759, Sensitive_Loss : 0.11987, Sensitive_Acc : 16.700, Run Time : 9.23 sec
INFO:root:2024-04-10 17:47:03, Train, Epoch : 3, Step : 1560, Loss : 0.43148, Acc : 0.794, Sensitive_Loss : 0.10522, Sensitive_Acc : 15.900, Run Time : 9.39 sec
INFO:root:2024-04-10 17:47:12, Train, Epoch : 3, Step : 1570, Loss : 0.43823, Acc : 0.772, Sensitive_Loss : 0.13627, Sensitive_Acc : 16.900, Run Time : 9.07 sec
INFO:root:2024-04-10 17:47:23, Train, Epoch : 3, Step : 1580, Loss : 0.46026, Acc : 0.769, Sensitive_Loss : 0.11590, Sensitive_Acc : 16.200, Run Time : 11.23 sec
INFO:root:2024-04-10 17:47:33, Train, Epoch : 3, Step : 1590, Loss : 0.46036, Acc : 0.812, Sensitive_Loss : 0.12435, Sensitive_Acc : 15.800, Run Time : 10.31 sec
INFO:root:2024-04-10 17:47:42, Train, Epoch : 3, Step : 1600, Loss : 0.41362, Acc : 0.781, Sensitive_Loss : 0.13468, Sensitive_Acc : 16.000, Run Time : 9.02 sec
INFO:root:2024-04-10 17:49:15, Dev, Step : 1600, Loss : 0.52029, Acc : 0.767, Auc : 0.849, Sensitive_Loss : 0.13002, Sensitive_Acc : 16.379, Sensitive_Auc : 0.996, Mean auc: 0.849, Run Time : 92.74 sec
INFO:root:2024-04-10 17:49:16, Best, Step : 1600, Loss : 0.52029, Acc : 0.767, Auc : 0.849, Sensitive_Loss : 0.13002, Sensitive_Acc : 16.379, Sensitive_Auc : 0.996, Best Auc : 0.849
INFO:root:2024-04-10 17:49:23, Train, Epoch : 3, Step : 1610, Loss : 0.43374, Acc : 0.762, Sensitive_Loss : 0.09371, Sensitive_Acc : 15.900, Run Time : 100.36 sec
INFO:root:2024-04-10 17:49:33, Train, Epoch : 3, Step : 1620, Loss : 0.45778, Acc : 0.769, Sensitive_Loss : 0.11193, Sensitive_Acc : 15.600, Run Time : 10.10 sec
INFO:root:2024-04-10 17:49:44, Train, Epoch : 3, Step : 1630, Loss : 0.39279, Acc : 0.831, Sensitive_Loss : 0.14326, Sensitive_Acc : 15.400, Run Time : 10.86 sec
INFO:root:2024-04-10 17:49:53, Train, Epoch : 3, Step : 1640, Loss : 0.41938, Acc : 0.803, Sensitive_Loss : 0.14281, Sensitive_Acc : 17.200, Run Time : 9.72 sec
INFO:root:2024-04-10 17:50:03, Train, Epoch : 3, Step : 1650, Loss : 0.44146, Acc : 0.819, Sensitive_Loss : 0.09970, Sensitive_Acc : 15.300, Run Time : 9.54 sec
INFO:root:2024-04-10 17:50:15, Train, Epoch : 3, Step : 1660, Loss : 0.44430, Acc : 0.794, Sensitive_Loss : 0.14551, Sensitive_Acc : 16.900, Run Time : 12.29 sec
INFO:root:2024-04-10 17:50:26, Train, Epoch : 3, Step : 1670, Loss : 0.43559, Acc : 0.834, Sensitive_Loss : 0.10774, Sensitive_Acc : 16.500, Run Time : 10.80 sec
INFO:root:2024-04-10 17:50:35, Train, Epoch : 3, Step : 1680, Loss : 0.40236, Acc : 0.819, Sensitive_Loss : 0.12060, Sensitive_Acc : 16.800, Run Time : 9.35 sec
INFO:root:2024-04-10 17:50:46, Train, Epoch : 3, Step : 1690, Loss : 0.40200, Acc : 0.803, Sensitive_Loss : 0.13435, Sensitive_Acc : 17.800, Run Time : 11.11 sec
INFO:root:2024-04-10 17:50:59, Train, Epoch : 3, Step : 1700, Loss : 0.46583, Acc : 0.787, Sensitive_Loss : 0.08702, Sensitive_Acc : 16.600, Run Time : 12.65 sec
INFO:root:2024-04-10 17:52:33, Dev, Step : 1700, Loss : 0.50860, Acc : 0.771, Auc : 0.850, Sensitive_Loss : 0.13586, Sensitive_Acc : 16.321, Sensitive_Auc : 0.996, Mean auc: 0.850, Run Time : 94.09 sec
INFO:root:2024-04-10 17:52:34, Best, Step : 1700, Loss : 0.50860, Acc : 0.771, Auc : 0.850, Sensitive_Loss : 0.13586, Sensitive_Acc : 16.321, Sensitive_Auc : 0.996, Best Auc : 0.850
INFO:root:2024-04-10 17:52:41, Train, Epoch : 3, Step : 1710, Loss : 0.42413, Acc : 0.812, Sensitive_Loss : 0.13211, Sensitive_Acc : 16.000, Run Time : 101.94 sec
INFO:root:2024-04-10 17:52:50, Train, Epoch : 3, Step : 1720, Loss : 0.43478, Acc : 0.781, Sensitive_Loss : 0.13313, Sensitive_Acc : 16.100, Run Time : 9.38 sec
INFO:root:2024-04-10 17:53:01, Train, Epoch : 3, Step : 1730, Loss : 0.38426, Acc : 0.803, Sensitive_Loss : 0.08041, Sensitive_Acc : 17.200, Run Time : 10.80 sec
INFO:root:2024-04-10 17:53:13, Train, Epoch : 3, Step : 1740, Loss : 0.42262, Acc : 0.834, Sensitive_Loss : 0.10172, Sensitive_Acc : 16.100, Run Time : 11.81 sec
INFO:root:2024-04-10 17:53:24, Train, Epoch : 3, Step : 1750, Loss : 0.37833, Acc : 0.841, Sensitive_Loss : 0.15001, Sensitive_Acc : 16.600, Run Time : 10.59 sec
INFO:root:2024-04-10 17:53:33, Train, Epoch : 3, Step : 1760, Loss : 0.38530, Acc : 0.784, Sensitive_Loss : 0.13988, Sensitive_Acc : 16.600, Run Time : 9.28 sec
INFO:root:2024-04-10 17:53:42, Train, Epoch : 3, Step : 1770, Loss : 0.51620, Acc : 0.772, Sensitive_Loss : 0.11524, Sensitive_Acc : 15.200, Run Time : 8.99 sec
INFO:root:2024-04-10 17:53:53, Train, Epoch : 3, Step : 1780, Loss : 0.44382, Acc : 0.787, Sensitive_Loss : 0.15018, Sensitive_Acc : 17.100, Run Time : 11.31 sec
INFO:root:2024-04-10 17:54:03, Train, Epoch : 3, Step : 1790, Loss : 0.47309, Acc : 0.772, Sensitive_Loss : 0.11055, Sensitive_Acc : 15.500, Run Time : 9.57 sec
INFO:root:2024-04-10 17:54:12, Train, Epoch : 3, Step : 1800, Loss : 0.46394, Acc : 0.812, Sensitive_Loss : 0.11251, Sensitive_Acc : 15.300, Run Time : 9.09 sec
INFO:root:2024-04-10 17:55:45, Dev, Step : 1800, Loss : 0.50988, Acc : 0.772, Auc : 0.851, Sensitive_Loss : 0.13611, Sensitive_Acc : 16.364, Sensitive_Auc : 0.997, Mean auc: 0.851, Run Time : 93.14 sec
INFO:root:2024-04-10 17:55:46, Best, Step : 1800, Loss : 0.50988, Acc : 0.772, Auc : 0.851, Sensitive_Loss : 0.13611, Sensitive_Acc : 16.364, Sensitive_Auc : 0.997, Best Auc : 0.851
INFO:root:2024-04-10 17:55:52, Train, Epoch : 3, Step : 1810, Loss : 0.41738, Acc : 0.791, Sensitive_Loss : 0.10746, Sensitive_Acc : 17.600, Run Time : 100.06 sec
INFO:root:2024-04-10 17:56:02, Train, Epoch : 3, Step : 1820, Loss : 0.43151, Acc : 0.812, Sensitive_Loss : 0.13408, Sensitive_Acc : 15.100, Run Time : 10.08 sec
INFO:root:2024-04-10 17:56:11, Train, Epoch : 3, Step : 1830, Loss : 0.40267, Acc : 0.847, Sensitive_Loss : 0.13152, Sensitive_Acc : 15.900, Run Time : 9.52 sec
INFO:root:2024-04-10 17:56:21, Train, Epoch : 3, Step : 1840, Loss : 0.43336, Acc : 0.787, Sensitive_Loss : 0.12530, Sensitive_Acc : 15.300, Run Time : 9.45 sec
INFO:root:2024-04-10 17:56:30, Train, Epoch : 3, Step : 1850, Loss : 0.44762, Acc : 0.803, Sensitive_Loss : 0.09877, Sensitive_Acc : 14.500, Run Time : 9.40 sec
INFO:root:2024-04-10 17:56:40, Train, Epoch : 3, Step : 1860, Loss : 0.43720, Acc : 0.791, Sensitive_Loss : 0.12253, Sensitive_Acc : 16.800, Run Time : 9.84 sec
INFO:root:2024-04-10 17:56:52, Train, Epoch : 3, Step : 1870, Loss : 0.40050, Acc : 0.812, Sensitive_Loss : 0.12396, Sensitive_Acc : 16.000, Run Time : 12.05 sec
INFO:root:2024-04-10 17:57:02, Train, Epoch : 3, Step : 1880, Loss : 0.32866, Acc : 0.838, Sensitive_Loss : 0.10808, Sensitive_Acc : 16.900, Run Time : 9.90 sec
INFO:root:2024-04-10 17:57:12, Train, Epoch : 3, Step : 1890, Loss : 0.42054, Acc : 0.812, Sensitive_Loss : 0.11897, Sensitive_Acc : 15.400, Run Time : 9.82 sec
INFO:root:2024-04-10 17:57:21, Train, Epoch : 3, Step : 1900, Loss : 0.45650, Acc : 0.797, Sensitive_Loss : 0.09817, Sensitive_Acc : 15.900, Run Time : 8.94 sec
INFO:root:2024-04-10 17:59:07, Dev, Step : 1900, Loss : 0.52302, Acc : 0.764, Auc : 0.855, Sensitive_Loss : 0.14986, Sensitive_Acc : 16.307, Sensitive_Auc : 0.995, Mean auc: 0.855, Run Time : 106.55 sec
INFO:root:2024-04-10 17:59:08, Best, Step : 1900, Loss : 0.52302, Acc : 0.764, Auc : 0.855, Sensitive_Loss : 0.14986, Sensitive_Acc : 16.307, Sensitive_Auc : 0.995, Best Auc : 0.855
INFO:root:2024-04-10 17:59:14, Train, Epoch : 3, Step : 1910, Loss : 0.39355, Acc : 0.803, Sensitive_Loss : 0.10358, Sensitive_Acc : 16.300, Run Time : 113.52 sec
INFO:root:2024-04-10 17:59:24, Train, Epoch : 3, Step : 1920, Loss : 0.48153, Acc : 0.791, Sensitive_Loss : 0.13618, Sensitive_Acc : 15.700, Run Time : 9.47 sec
INFO:root:2024-04-10 17:59:34, Train, Epoch : 3, Step : 1930, Loss : 0.43767, Acc : 0.772, Sensitive_Loss : 0.12436, Sensitive_Acc : 18.100, Run Time : 9.78 sec
INFO:root:2024-04-10 18:01:10
INFO:root:y_pred: [0.5101644  0.12130383 0.85367346 ... 0.4676138  0.5405244  0.53974485]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9991441e-01 1.8075970e-03 5.7838714e-01 9.9997115e-01 9.9855536e-01
 9.9672973e-01 9.9960846e-01 2.5654756e-05 9.8915088e-01 9.9632865e-01
 1.9745106e-01 6.0570592e-01 1.2386413e-04 8.8410652e-01 9.9993193e-01
 9.9981707e-01 9.9422258e-01 9.8828727e-01 9.9471492e-01 9.9899381e-01
 9.9409544e-01 1.9619600e-01 9.9994814e-01 8.7421942e-01 9.2907417e-01
 4.0229079e-03 9.8664969e-01 4.4256523e-03 9.9993396e-01 2.4429963e-03
 5.7692900e-03 5.3187662e-01 6.7728842e-03 9.9419338e-01 3.5947443e-05
 9.9981183e-01 1.2756647e-04 9.9994826e-01 5.4187611e-02 9.8872745e-01
 9.9978381e-01 1.0426342e-03 1.3412119e-01 4.8931089e-04 3.2068139e-01
 7.3875803e-01 9.9994504e-01 9.4799227e-01 9.9304801e-01 9.9841285e-01
 1.6493936e-03 8.8170385e-01 6.6169160e-03 1.7772909e-01 9.9979776e-01
 3.8818559e-01 9.8941338e-01 9.9961746e-01 9.8713416e-01 8.4659318e-03
 5.7275114e-03 9.9902129e-01 4.1960621e-01 9.9982649e-01 9.9114823e-01
 6.2966809e-02 9.9748826e-01 8.6804372e-01 9.9998605e-01 9.9994767e-01
 1.7899876e-02 3.0277207e-01 8.9110166e-01 9.9985230e-01 9.8898590e-01
 3.9809691e-03 7.0590514e-01 7.0960927e-03 4.0254433e-04 9.9313760e-01
 8.7164737e-02 9.9980336e-01 9.9987996e-01 9.9969375e-01 5.0608528e-01
 9.9999905e-01 2.3772843e-04 9.6080871e-04 9.9487215e-01 9.9401349e-01
 2.5557660e-02 8.1158358e-01 1.7771749e-02 7.7363634e-01 4.1377109e-02
 9.9991369e-01 8.0008442e-03 9.9914896e-01 9.3926901e-01 2.0465204e-04
 3.6247913e-04 1.9219437e-01 9.9404103e-01 9.9984944e-01 9.9471045e-01
 8.2217437e-01 9.9930692e-01 1.6431561e-01 2.5734067e-01 9.9986291e-01
 3.5289282e-05 2.0141965e-03 4.5256853e-02 9.9925405e-01 9.9863428e-01
 2.6799613e-04 9.8743933e-01 4.3351288e-04 9.9998569e-01 4.9016371e-01
 9.9948514e-01 9.9994123e-01 5.4576272e-01 1.6974513e-01 1.1601665e-02
 5.7527847e-03 1.0253624e-02 3.1015003e-04 9.9456453e-01 9.9990416e-01
 3.8565393e-03 4.4301149e-04 9.4707031e-04 1.0847066e-01 9.9856442e-01
 9.9867237e-01 9.9951243e-01 1.7822741e-01 2.8331730e-01 9.8811215e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 18:01:10, Dev, Step : 1932, Loss : 0.51755, Acc : 0.768, Auc : 0.854, Sensitive_Loss : 0.14452, Sensitive_Acc : 16.307, Sensitive_Auc : 0.997, Mean auc: 0.854, Run Time : 94.95 sec
INFO:root:2024-04-10 18:01:19, Train, Epoch : 4, Step : 1940, Loss : 0.31743, Acc : 0.634, Sensitive_Loss : 0.07219, Sensitive_Acc : 12.200, Run Time : 8.03 sec
INFO:root:2024-04-10 18:01:28, Train, Epoch : 4, Step : 1950, Loss : 0.41775, Acc : 0.825, Sensitive_Loss : 0.10799, Sensitive_Acc : 16.300, Run Time : 8.77 sec
INFO:root:2024-04-10 18:01:37, Train, Epoch : 4, Step : 1960, Loss : 0.43365, Acc : 0.806, Sensitive_Loss : 0.13327, Sensitive_Acc : 16.200, Run Time : 9.08 sec
INFO:root:2024-04-10 18:01:48, Train, Epoch : 4, Step : 1970, Loss : 0.36682, Acc : 0.828, Sensitive_Loss : 0.14001, Sensitive_Acc : 15.500, Run Time : 10.52 sec
INFO:root:2024-04-10 18:01:58, Train, Epoch : 4, Step : 1980, Loss : 0.35045, Acc : 0.819, Sensitive_Loss : 0.11374, Sensitive_Acc : 17.200, Run Time : 10.46 sec
INFO:root:2024-04-10 18:02:07, Train, Epoch : 4, Step : 1990, Loss : 0.36846, Acc : 0.841, Sensitive_Loss : 0.12734, Sensitive_Acc : 15.700, Run Time : 9.52 sec
INFO:root:2024-04-10 18:02:16, Train, Epoch : 4, Step : 2000, Loss : 0.37928, Acc : 0.853, Sensitive_Loss : 0.10912, Sensitive_Acc : 14.900, Run Time : 8.85 sec
INFO:root:2024-04-10 18:03:52, Dev, Step : 2000, Loss : 0.52096, Acc : 0.766, Auc : 0.855, Sensitive_Loss : 0.13759, Sensitive_Acc : 16.321, Sensitive_Auc : 0.994, Mean auc: 0.855, Run Time : 95.31 sec
INFO:root:2024-04-10 18:03:53, Best, Step : 2000, Loss : 0.52096, Acc : 0.766, Auc : 0.855, Sensitive_Loss : 0.13759, Sensitive_Acc : 16.321, Sensitive_Auc : 0.994, Best Auc : 0.855
INFO:root:2024-04-10 18:03:59, Train, Epoch : 4, Step : 2010, Loss : 0.34856, Acc : 0.828, Sensitive_Loss : 0.11330, Sensitive_Acc : 16.500, Run Time : 102.86 sec
INFO:root:2024-04-10 18:04:10, Train, Epoch : 4, Step : 2020, Loss : 0.44482, Acc : 0.787, Sensitive_Loss : 0.10614, Sensitive_Acc : 15.300, Run Time : 11.27 sec
INFO:root:2024-04-10 18:04:21, Train, Epoch : 4, Step : 2030, Loss : 0.37422, Acc : 0.816, Sensitive_Loss : 0.12214, Sensitive_Acc : 15.200, Run Time : 10.34 sec
INFO:root:2024-04-10 18:04:30, Train, Epoch : 4, Step : 2040, Loss : 0.41261, Acc : 0.825, Sensitive_Loss : 0.12522, Sensitive_Acc : 17.800, Run Time : 9.17 sec
INFO:root:2024-04-10 18:04:39, Train, Epoch : 4, Step : 2050, Loss : 0.38016, Acc : 0.816, Sensitive_Loss : 0.12142, Sensitive_Acc : 16.000, Run Time : 9.29 sec
INFO:root:2024-04-10 18:04:49, Train, Epoch : 4, Step : 2060, Loss : 0.44076, Acc : 0.797, Sensitive_Loss : 0.09102, Sensitive_Acc : 16.100, Run Time : 10.12 sec
INFO:root:2024-04-10 18:04:59, Train, Epoch : 4, Step : 2070, Loss : 0.45379, Acc : 0.812, Sensitive_Loss : 0.10383, Sensitive_Acc : 16.600, Run Time : 9.53 sec
INFO:root:2024-04-10 18:05:08, Train, Epoch : 4, Step : 2080, Loss : 0.37282, Acc : 0.834, Sensitive_Loss : 0.08610, Sensitive_Acc : 18.100, Run Time : 9.40 sec
INFO:root:2024-04-10 18:05:19, Train, Epoch : 4, Step : 2090, Loss : 0.35021, Acc : 0.819, Sensitive_Loss : 0.12102, Sensitive_Acc : 17.400, Run Time : 11.01 sec
INFO:root:2024-04-10 18:05:29, Train, Epoch : 4, Step : 2100, Loss : 0.43970, Acc : 0.816, Sensitive_Loss : 0.13308, Sensitive_Acc : 17.200, Run Time : 9.52 sec
INFO:root:2024-04-10 18:07:01, Dev, Step : 2100, Loss : 0.51286, Acc : 0.775, Auc : 0.856, Sensitive_Loss : 0.14399, Sensitive_Acc : 16.364, Sensitive_Auc : 0.997, Mean auc: 0.856, Run Time : 92.45 sec
INFO:root:2024-04-10 18:07:02, Best, Step : 2100, Loss : 0.51286, Acc : 0.775, Auc : 0.856, Sensitive_Loss : 0.14399, Sensitive_Acc : 16.364, Sensitive_Auc : 0.997, Best Auc : 0.856
INFO:root:2024-04-10 18:07:09, Train, Epoch : 4, Step : 2110, Loss : 0.43404, Acc : 0.825, Sensitive_Loss : 0.09561, Sensitive_Acc : 15.300, Run Time : 99.89 sec
INFO:root:2024-04-10 18:07:19, Train, Epoch : 4, Step : 2120, Loss : 0.49768, Acc : 0.762, Sensitive_Loss : 0.11845, Sensitive_Acc : 15.700, Run Time : 10.48 sec
INFO:root:2024-04-10 18:07:30, Train, Epoch : 4, Step : 2130, Loss : 0.46083, Acc : 0.787, Sensitive_Loss : 0.10422, Sensitive_Acc : 17.100, Run Time : 10.27 sec
INFO:root:2024-04-10 18:07:42, Train, Epoch : 4, Step : 2140, Loss : 0.41927, Acc : 0.797, Sensitive_Loss : 0.13225, Sensitive_Acc : 17.300, Run Time : 12.41 sec
INFO:root:2024-04-10 18:07:52, Train, Epoch : 4, Step : 2150, Loss : 0.42782, Acc : 0.809, Sensitive_Loss : 0.10647, Sensitive_Acc : 17.200, Run Time : 9.60 sec
INFO:root:2024-04-10 18:08:01, Train, Epoch : 4, Step : 2160, Loss : 0.38546, Acc : 0.791, Sensitive_Loss : 0.10719, Sensitive_Acc : 16.300, Run Time : 9.54 sec
INFO:root:2024-04-10 18:08:10, Train, Epoch : 4, Step : 2170, Loss : 0.37680, Acc : 0.850, Sensitive_Loss : 0.11617, Sensitive_Acc : 15.900, Run Time : 9.15 sec
INFO:root:2024-04-10 18:08:20, Train, Epoch : 4, Step : 2180, Loss : 0.36861, Acc : 0.816, Sensitive_Loss : 0.10043, Sensitive_Acc : 15.900, Run Time : 9.60 sec
INFO:root:2024-04-10 18:08:29, Train, Epoch : 4, Step : 2190, Loss : 0.35141, Acc : 0.856, Sensitive_Loss : 0.13155, Sensitive_Acc : 15.400, Run Time : 9.09 sec
INFO:root:2024-04-10 18:08:38, Train, Epoch : 4, Step : 2200, Loss : 0.46195, Acc : 0.800, Sensitive_Loss : 0.10410, Sensitive_Acc : 16.000, Run Time : 8.82 sec
INFO:root:2024-04-10 18:10:11, Dev, Step : 2200, Loss : 0.49777, Acc : 0.782, Auc : 0.856, Sensitive_Loss : 0.15668, Sensitive_Acc : 16.350, Sensitive_Auc : 0.994, Mean auc: 0.856, Run Time : 93.23 sec
INFO:root:2024-04-10 18:10:19, Train, Epoch : 4, Step : 2210, Loss : 0.34752, Acc : 0.822, Sensitive_Loss : 0.09652, Sensitive_Acc : 15.700, Run Time : 100.83 sec
INFO:root:2024-04-10 18:10:30, Train, Epoch : 4, Step : 2220, Loss : 0.42113, Acc : 0.809, Sensitive_Loss : 0.14893, Sensitive_Acc : 16.200, Run Time : 11.74 sec
INFO:root:2024-04-10 18:10:40, Train, Epoch : 4, Step : 2230, Loss : 0.43647, Acc : 0.825, Sensitive_Loss : 0.12240, Sensitive_Acc : 16.800, Run Time : 9.24 sec
INFO:root:2024-04-10 18:10:49, Train, Epoch : 4, Step : 2240, Loss : 0.41870, Acc : 0.791, Sensitive_Loss : 0.11652, Sensitive_Acc : 15.100, Run Time : 9.18 sec
INFO:root:2024-04-10 18:11:02, Train, Epoch : 4, Step : 2250, Loss : 0.36906, Acc : 0.831, Sensitive_Loss : 0.12081, Sensitive_Acc : 15.700, Run Time : 12.92 sec
INFO:root:2024-04-10 18:11:11, Train, Epoch : 4, Step : 2260, Loss : 0.33386, Acc : 0.834, Sensitive_Loss : 0.11369, Sensitive_Acc : 15.600, Run Time : 9.06 sec
INFO:root:2024-04-10 18:11:19, Train, Epoch : 4, Step : 2270, Loss : 0.39090, Acc : 0.819, Sensitive_Loss : 0.12796, Sensitive_Acc : 16.700, Run Time : 8.60 sec
INFO:root:2024-04-10 18:11:29, Train, Epoch : 4, Step : 2280, Loss : 0.43483, Acc : 0.794, Sensitive_Loss : 0.11727, Sensitive_Acc : 16.800, Run Time : 10.00 sec
INFO:root:2024-04-10 18:11:40, Train, Epoch : 4, Step : 2290, Loss : 0.48633, Acc : 0.797, Sensitive_Loss : 0.18017, Sensitive_Acc : 17.600, Run Time : 10.49 sec
INFO:root:2024-04-10 18:11:49, Train, Epoch : 4, Step : 2300, Loss : 0.39589, Acc : 0.778, Sensitive_Loss : 0.10463, Sensitive_Acc : 17.400, Run Time : 9.48 sec
INFO:root:2024-04-10 18:13:22, Dev, Step : 2300, Loss : 0.54505, Acc : 0.759, Auc : 0.856, Sensitive_Loss : 0.15143, Sensitive_Acc : 16.307, Sensitive_Auc : 0.993, Mean auc: 0.856, Run Time : 92.91 sec
INFO:root:2024-04-10 18:13:30, Train, Epoch : 4, Step : 2310, Loss : 0.44210, Acc : 0.753, Sensitive_Loss : 0.08908, Sensitive_Acc : 17.700, Run Time : 100.51 sec
INFO:root:2024-04-10 18:13:40, Train, Epoch : 4, Step : 2320, Loss : 0.44754, Acc : 0.800, Sensitive_Loss : 0.08464, Sensitive_Acc : 17.000, Run Time : 9.91 sec
INFO:root:2024-04-10 18:13:50, Train, Epoch : 4, Step : 2330, Loss : 0.43457, Acc : 0.819, Sensitive_Loss : 0.11694, Sensitive_Acc : 17.300, Run Time : 10.09 sec
INFO:root:2024-04-10 18:13:59, Train, Epoch : 4, Step : 2340, Loss : 0.39179, Acc : 0.828, Sensitive_Loss : 0.11341, Sensitive_Acc : 16.500, Run Time : 9.68 sec
INFO:root:2024-04-10 18:14:09, Train, Epoch : 4, Step : 2350, Loss : 0.39977, Acc : 0.831, Sensitive_Loss : 0.10092, Sensitive_Acc : 16.900, Run Time : 9.32 sec
INFO:root:2024-04-10 18:14:18, Train, Epoch : 4, Step : 2360, Loss : 0.37712, Acc : 0.834, Sensitive_Loss : 0.13560, Sensitive_Acc : 17.900, Run Time : 9.61 sec
INFO:root:2024-04-10 18:14:28, Train, Epoch : 4, Step : 2370, Loss : 0.41901, Acc : 0.809, Sensitive_Loss : 0.09239, Sensitive_Acc : 16.400, Run Time : 9.19 sec
INFO:root:2024-04-10 18:14:37, Train, Epoch : 4, Step : 2380, Loss : 0.41748, Acc : 0.825, Sensitive_Loss : 0.08465, Sensitive_Acc : 15.600, Run Time : 9.36 sec
INFO:root:2024-04-10 18:14:47, Train, Epoch : 4, Step : 2390, Loss : 0.39346, Acc : 0.797, Sensitive_Loss : 0.09414, Sensitive_Acc : 17.000, Run Time : 9.70 sec
INFO:root:2024-04-10 18:14:58, Train, Epoch : 4, Step : 2400, Loss : 0.36801, Acc : 0.809, Sensitive_Loss : 0.11656, Sensitive_Acc : 17.000, Run Time : 11.08 sec
INFO:root:2024-04-10 18:16:37, Dev, Step : 2400, Loss : 0.51261, Acc : 0.774, Auc : 0.857, Sensitive_Loss : 0.13250, Sensitive_Acc : 16.350, Sensitive_Auc : 0.994, Mean auc: 0.857, Run Time : 99.22 sec
INFO:root:2024-04-10 18:16:39, Best, Step : 2400, Loss : 0.51261, Acc : 0.774, Auc : 0.857, Sensitive_Loss : 0.13250, Sensitive_Acc : 16.350, Sensitive_Auc : 0.994, Best Auc : 0.857
INFO:root:2024-04-10 18:16:47, Train, Epoch : 4, Step : 2410, Loss : 0.38677, Acc : 0.803, Sensitive_Loss : 0.11453, Sensitive_Acc : 15.200, Run Time : 109.03 sec
INFO:root:2024-04-10 18:16:57, Train, Epoch : 4, Step : 2420, Loss : 0.43491, Acc : 0.803, Sensitive_Loss : 0.10268, Sensitive_Acc : 16.400, Run Time : 9.82 sec
INFO:root:2024-04-10 18:17:05, Train, Epoch : 4, Step : 2430, Loss : 0.35794, Acc : 0.841, Sensitive_Loss : 0.15517, Sensitive_Acc : 16.800, Run Time : 8.82 sec
INFO:root:2024-04-10 18:17:15, Train, Epoch : 4, Step : 2440, Loss : 0.45126, Acc : 0.787, Sensitive_Loss : 0.07473, Sensitive_Acc : 15.800, Run Time : 9.89 sec
INFO:root:2024-04-10 18:17:25, Train, Epoch : 4, Step : 2450, Loss : 0.44190, Acc : 0.809, Sensitive_Loss : 0.08810, Sensitive_Acc : 16.700, Run Time : 9.54 sec
INFO:root:2024-04-10 18:17:34, Train, Epoch : 4, Step : 2460, Loss : 0.41009, Acc : 0.853, Sensitive_Loss : 0.07703, Sensitive_Acc : 16.700, Run Time : 9.15 sec
INFO:root:2024-04-10 18:17:43, Train, Epoch : 4, Step : 2470, Loss : 0.45622, Acc : 0.787, Sensitive_Loss : 0.10478, Sensitive_Acc : 15.100, Run Time : 9.02 sec
INFO:root:2024-04-10 18:17:53, Train, Epoch : 4, Step : 2480, Loss : 0.44731, Acc : 0.787, Sensitive_Loss : 0.12024, Sensitive_Acc : 15.200, Run Time : 9.71 sec
INFO:root:2024-04-10 18:18:03, Train, Epoch : 4, Step : 2490, Loss : 0.34825, Acc : 0.803, Sensitive_Loss : 0.07615, Sensitive_Acc : 15.500, Run Time : 10.05 sec
INFO:root:2024-04-10 18:18:11, Train, Epoch : 4, Step : 2500, Loss : 0.41401, Acc : 0.794, Sensitive_Loss : 0.12284, Sensitive_Acc : 16.500, Run Time : 8.53 sec
INFO:root:2024-04-10 18:19:58, Dev, Step : 2500, Loss : 0.51692, Acc : 0.772, Auc : 0.856, Sensitive_Loss : 0.14515, Sensitive_Acc : 16.336, Sensitive_Auc : 0.993, Mean auc: 0.856, Run Time : 106.37 sec
INFO:root:2024-04-10 18:20:04, Train, Epoch : 4, Step : 2510, Loss : 0.42630, Acc : 0.806, Sensitive_Loss : 0.07508, Sensitive_Acc : 16.800, Run Time : 112.95 sec
INFO:root:2024-04-10 18:20:15, Train, Epoch : 4, Step : 2520, Loss : 0.43879, Acc : 0.841, Sensitive_Loss : 0.12429, Sensitive_Acc : 16.900, Run Time : 10.31 sec
INFO:root:2024-04-10 18:20:25, Train, Epoch : 4, Step : 2530, Loss : 0.36906, Acc : 0.841, Sensitive_Loss : 0.09089, Sensitive_Acc : 15.100, Run Time : 10.77 sec
INFO:root:2024-04-10 18:20:34, Train, Epoch : 4, Step : 2540, Loss : 0.37243, Acc : 0.828, Sensitive_Loss : 0.07778, Sensitive_Acc : 15.900, Run Time : 9.04 sec
INFO:root:2024-04-10 18:20:43, Train, Epoch : 4, Step : 2550, Loss : 0.38553, Acc : 0.841, Sensitive_Loss : 0.09703, Sensitive_Acc : 14.800, Run Time : 8.71 sec
INFO:root:2024-04-10 18:20:53, Train, Epoch : 4, Step : 2560, Loss : 0.39120, Acc : 0.812, Sensitive_Loss : 0.06472, Sensitive_Acc : 16.100, Run Time : 9.47 sec
INFO:root:2024-04-10 18:21:01, Train, Epoch : 4, Step : 2570, Loss : 0.42912, Acc : 0.816, Sensitive_Loss : 0.10799, Sensitive_Acc : 15.500, Run Time : 8.83 sec
INFO:root:2024-04-10 18:22:42
INFO:root:y_pred: [0.45650202 0.06393061 0.9084815  ... 0.58203447 0.7121028  0.42918316]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9990225e-01 1.7115549e-03 5.2522540e-01 9.9997151e-01 9.9833530e-01
 9.9443793e-01 9.9963391e-01 1.0896394e-05 9.8752940e-01 9.9722016e-01
 1.2052434e-01 5.7271045e-01 2.5334219e-05 8.6576211e-01 9.9997604e-01
 9.9974436e-01 9.9061757e-01 9.8784745e-01 9.9497437e-01 9.9893814e-01
 9.9592042e-01 1.5121509e-01 9.9992335e-01 8.9664090e-01 9.2961979e-01
 4.2720032e-03 9.9185926e-01 2.5715418e-03 9.9994624e-01 1.7268748e-03
 1.9886401e-03 5.3978497e-01 1.0408530e-02 9.9558371e-01 3.0165485e-05
 9.9985397e-01 6.1248968e-05 9.9994755e-01 5.0169021e-02 9.8379183e-01
 9.9987483e-01 5.6644471e-04 5.6223780e-02 2.0269433e-04 1.9119541e-01
 6.3263077e-01 9.9998581e-01 9.5251966e-01 9.9257362e-01 9.9852693e-01
 9.0484414e-04 9.1469663e-01 4.3716971e-03 1.5171793e-01 9.9996197e-01
 2.7154768e-01 9.9073470e-01 9.9930751e-01 9.9568045e-01 7.6956325e-03
 5.4196231e-03 9.9965596e-01 4.0374079e-01 9.9992502e-01 9.9434859e-01
 3.6850285e-02 9.9734819e-01 9.0691215e-01 9.9999380e-01 9.9992478e-01
 1.4268923e-02 2.0743245e-01 8.7204760e-01 9.9986625e-01 9.9120355e-01
 1.4740678e-03 6.4601511e-01 3.8754372e-03 2.1271507e-04 9.9360400e-01
 5.5675045e-02 9.9992037e-01 9.9990559e-01 9.9982351e-01 4.0107143e-01
 9.9999940e-01 8.0489343e-05 1.0241562e-03 9.9280691e-01 9.9503219e-01
 1.7054211e-02 8.2813889e-01 1.5538138e-02 7.5785702e-01 1.9615296e-02
 9.9993551e-01 2.9326726e-03 9.9855143e-01 9.7087628e-01 6.5068052e-05
 1.7373155e-04 2.2760080e-01 9.9360931e-01 9.9987650e-01 9.9395919e-01
 7.4284852e-01 9.9884772e-01 7.0855305e-02 1.9186945e-01 9.9995804e-01
 2.3246839e-05 1.0205805e-03 5.1750157e-02 9.9959248e-01 9.9892908e-01
 6.6325323e-05 9.9245584e-01 4.6881827e-04 9.9997461e-01 4.3431315e-01
 9.9974293e-01 9.9996817e-01 6.3144445e-01 6.6029660e-02 6.8487767e-03
 4.4205203e-03 6.9685983e-03 9.7235214e-05 9.8799849e-01 9.9981898e-01
 1.1245018e-03 2.3978973e-04 6.9380243e-04 6.3049681e-02 9.9813902e-01
 9.9924541e-01 9.9941492e-01 1.8294010e-01 2.1503545e-01 9.8665011e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 18:22:42, Dev, Step : 2576, Loss : 0.50528, Acc : 0.780, Auc : 0.858, Sensitive_Loss : 0.13251, Sensitive_Acc : 16.379, Sensitive_Auc : 0.992, Mean auc: 0.858, Run Time : 96.30 sec
INFO:root:2024-04-10 18:22:43, Best, Step : 2576, Loss : 0.50528, Acc : 0.780,Auc : 0.858, Best Auc : 0.858, Sensitive_Loss : 0.13251, Sensitive_Acc : 16.379, Sensitive_Auc : 0.992
INFO:root:2024-04-10 18:22:48, Train, Epoch : 5, Step : 2580, Loss : 0.13476, Acc : 0.341, Sensitive_Loss : 0.05956, Sensitive_Acc : 7.200, Run Time : 4.38 sec
INFO:root:2024-04-10 18:22:57, Train, Epoch : 5, Step : 2590, Loss : 0.40133, Acc : 0.809, Sensitive_Loss : 0.11973, Sensitive_Acc : 16.800, Run Time : 8.70 sec
INFO:root:2024-04-10 18:23:06, Train, Epoch : 5, Step : 2600, Loss : 0.41372, Acc : 0.828, Sensitive_Loss : 0.07390, Sensitive_Acc : 15.500, Run Time : 9.01 sec
INFO:root:2024-04-10 18:24:42, Dev, Step : 2600, Loss : 0.50658, Acc : 0.777, Auc : 0.859, Sensitive_Loss : 0.13039, Sensitive_Acc : 16.379, Sensitive_Auc : 0.992, Mean auc: 0.859, Run Time : 95.83 sec
INFO:root:2024-04-10 18:24:43, Best, Step : 2600, Loss : 0.50658, Acc : 0.777, Auc : 0.859, Sensitive_Loss : 0.13039, Sensitive_Acc : 16.379, Sensitive_Auc : 0.992, Best Auc : 0.859
INFO:root:2024-04-10 18:24:49, Train, Epoch : 5, Step : 2610, Loss : 0.36760, Acc : 0.856, Sensitive_Loss : 0.11365, Sensitive_Acc : 17.800, Run Time : 102.71 sec
INFO:root:2024-04-10 18:25:01, Train, Epoch : 5, Step : 2620, Loss : 0.44709, Acc : 0.838, Sensitive_Loss : 0.12599, Sensitive_Acc : 17.800, Run Time : 11.96 sec
INFO:root:2024-04-10 18:25:10, Train, Epoch : 5, Step : 2630, Loss : 0.38887, Acc : 0.834, Sensitive_Loss : 0.08912, Sensitive_Acc : 17.500, Run Time : 9.49 sec
INFO:root:2024-04-10 18:25:20, Train, Epoch : 5, Step : 2640, Loss : 0.36400, Acc : 0.809, Sensitive_Loss : 0.08004, Sensitive_Acc : 16.600, Run Time : 9.40 sec
INFO:root:2024-04-10 18:25:29, Train, Epoch : 5, Step : 2650, Loss : 0.36737, Acc : 0.834, Sensitive_Loss : 0.10950, Sensitive_Acc : 16.300, Run Time : 9.54 sec
INFO:root:2024-04-10 18:25:39, Train, Epoch : 5, Step : 2660, Loss : 0.36626, Acc : 0.825, Sensitive_Loss : 0.08137, Sensitive_Acc : 17.000, Run Time : 9.98 sec
INFO:root:2024-04-10 18:25:49, Train, Epoch : 5, Step : 2670, Loss : 0.40368, Acc : 0.822, Sensitive_Loss : 0.10134, Sensitive_Acc : 18.400, Run Time : 9.37 sec
INFO:root:2024-04-10 18:25:58, Train, Epoch : 5, Step : 2680, Loss : 0.34115, Acc : 0.844, Sensitive_Loss : 0.14060, Sensitive_Acc : 16.200, Run Time : 9.83 sec
INFO:root:2024-04-10 18:26:10, Train, Epoch : 5, Step : 2690, Loss : 0.38434, Acc : 0.806, Sensitive_Loss : 0.07616, Sensitive_Acc : 17.600, Run Time : 11.31 sec
INFO:root:2024-04-10 18:26:19, Train, Epoch : 5, Step : 2700, Loss : 0.50400, Acc : 0.819, Sensitive_Loss : 0.08720, Sensitive_Acc : 15.800, Run Time : 8.94 sec
INFO:root:2024-04-10 18:27:52, Dev, Step : 2700, Loss : 0.52324, Acc : 0.772, Auc : 0.857, Sensitive_Loss : 0.13362, Sensitive_Acc : 16.336, Sensitive_Auc : 0.993, Mean auc: 0.857, Run Time : 93.91 sec
INFO:root:2024-04-10 18:28:00, Train, Epoch : 5, Step : 2710, Loss : 0.44369, Acc : 0.806, Sensitive_Loss : 0.08257, Sensitive_Acc : 17.200, Run Time : 101.25 sec
INFO:root:2024-04-10 18:28:10, Train, Epoch : 5, Step : 2720, Loss : 0.37449, Acc : 0.834, Sensitive_Loss : 0.10647, Sensitive_Acc : 16.500, Run Time : 10.18 sec
INFO:root:2024-04-10 18:28:21, Train, Epoch : 5, Step : 2730, Loss : 0.31671, Acc : 0.825, Sensitive_Loss : 0.13637, Sensitive_Acc : 15.000, Run Time : 11.06 sec
INFO:root:2024-04-10 18:28:34, Train, Epoch : 5, Step : 2740, Loss : 0.33288, Acc : 0.850, Sensitive_Loss : 0.09735, Sensitive_Acc : 16.300, Run Time : 12.51 sec
INFO:root:2024-04-10 18:28:44, Train, Epoch : 5, Step : 2750, Loss : 0.35814, Acc : 0.822, Sensitive_Loss : 0.07236, Sensitive_Acc : 16.400, Run Time : 10.01 sec
INFO:root:2024-04-10 18:28:54, Train, Epoch : 5, Step : 2760, Loss : 0.36106, Acc : 0.847, Sensitive_Loss : 0.09288, Sensitive_Acc : 15.200, Run Time : 9.92 sec
INFO:root:2024-04-10 18:29:06, Train, Epoch : 5, Step : 2770, Loss : 0.38364, Acc : 0.816, Sensitive_Loss : 0.09350, Sensitive_Acc : 15.600, Run Time : 12.39 sec
INFO:root:2024-04-10 18:29:16, Train, Epoch : 5, Step : 2780, Loss : 0.30337, Acc : 0.825, Sensitive_Loss : 0.08403, Sensitive_Acc : 15.400, Run Time : 9.73 sec
INFO:root:2024-04-10 18:29:25, Train, Epoch : 5, Step : 2790, Loss : 0.35695, Acc : 0.828, Sensitive_Loss : 0.07046, Sensitive_Acc : 17.100, Run Time : 9.26 sec
INFO:root:2024-04-10 18:29:35, Train, Epoch : 5, Step : 2800, Loss : 0.34656, Acc : 0.831, Sensitive_Loss : 0.13416, Sensitive_Acc : 16.200, Run Time : 10.42 sec
INFO:root:2024-04-10 18:31:08, Dev, Step : 2800, Loss : 0.52207, Acc : 0.772, Auc : 0.860, Sensitive_Loss : 0.14547, Sensitive_Acc : 16.364, Sensitive_Auc : 0.993, Mean auc: 0.860, Run Time : 93.05 sec
INFO:root:2024-04-10 18:31:09, Best, Step : 2800, Loss : 0.52207, Acc : 0.772, Auc : 0.860, Sensitive_Loss : 0.14547, Sensitive_Acc : 16.364, Sensitive_Auc : 0.993, Best Auc : 0.860
INFO:root:2024-04-10 18:31:16, Train, Epoch : 5, Step : 2810, Loss : 0.45763, Acc : 0.794, Sensitive_Loss : 0.14608, Sensitive_Acc : 16.900, Run Time : 100.26 sec
INFO:root:2024-04-10 18:31:25, Train, Epoch : 5, Step : 2820, Loss : 0.35904, Acc : 0.853, Sensitive_Loss : 0.09863, Sensitive_Acc : 15.700, Run Time : 9.47 sec
INFO:root:2024-04-10 18:31:34, Train, Epoch : 5, Step : 2830, Loss : 0.41341, Acc : 0.841, Sensitive_Loss : 0.13372, Sensitive_Acc : 15.400, Run Time : 9.30 sec
INFO:root:2024-04-10 18:31:44, Train, Epoch : 5, Step : 2840, Loss : 0.36302, Acc : 0.809, Sensitive_Loss : 0.08327, Sensitive_Acc : 16.200, Run Time : 9.48 sec
INFO:root:2024-04-10 18:31:53, Train, Epoch : 5, Step : 2850, Loss : 0.36868, Acc : 0.844, Sensitive_Loss : 0.12507, Sensitive_Acc : 15.800, Run Time : 9.55 sec
INFO:root:2024-04-10 18:32:03, Train, Epoch : 5, Step : 2860, Loss : 0.42665, Acc : 0.844, Sensitive_Loss : 0.07661, Sensitive_Acc : 17.400, Run Time : 9.15 sec
INFO:root:2024-04-10 18:32:13, Train, Epoch : 5, Step : 2870, Loss : 0.36658, Acc : 0.841, Sensitive_Loss : 0.13266, Sensitive_Acc : 16.600, Run Time : 10.00 sec
INFO:root:2024-04-10 18:32:21, Train, Epoch : 5, Step : 2880, Loss : 0.37254, Acc : 0.809, Sensitive_Loss : 0.12579, Sensitive_Acc : 17.400, Run Time : 8.94 sec
INFO:root:2024-04-10 18:32:31, Train, Epoch : 5, Step : 2890, Loss : 0.45889, Acc : 0.778, Sensitive_Loss : 0.11197, Sensitive_Acc : 14.500, Run Time : 9.19 sec
INFO:root:2024-04-10 18:32:40, Train, Epoch : 5, Step : 2900, Loss : 0.37063, Acc : 0.856, Sensitive_Loss : 0.09563, Sensitive_Acc : 15.800, Run Time : 8.85 sec
INFO:root:2024-04-10 18:34:16, Dev, Step : 2900, Loss : 0.50459, Acc : 0.780, Auc : 0.858, Sensitive_Loss : 0.13572, Sensitive_Acc : 16.364, Sensitive_Auc : 0.992, Mean auc: 0.858, Run Time : 96.74 sec
INFO:root:2024-04-10 18:34:24, Train, Epoch : 5, Step : 2910, Loss : 0.42565, Acc : 0.787, Sensitive_Loss : 0.08760, Sensitive_Acc : 15.700, Run Time : 104.21 sec
INFO:root:2024-04-10 18:34:34, Train, Epoch : 5, Step : 2920, Loss : 0.39794, Acc : 0.825, Sensitive_Loss : 0.07889, Sensitive_Acc : 15.700, Run Time : 9.91 sec
INFO:root:2024-04-10 18:34:45, Train, Epoch : 5, Step : 2930, Loss : 0.37882, Acc : 0.838, Sensitive_Loss : 0.08805, Sensitive_Acc : 16.200, Run Time : 11.45 sec
INFO:root:2024-04-10 18:34:58, Train, Epoch : 5, Step : 2940, Loss : 0.35732, Acc : 0.847, Sensitive_Loss : 0.08075, Sensitive_Acc : 16.100, Run Time : 12.42 sec
INFO:root:2024-04-10 18:35:09, Train, Epoch : 5, Step : 2950, Loss : 0.37427, Acc : 0.809, Sensitive_Loss : 0.08580, Sensitive_Acc : 17.300, Run Time : 11.01 sec
INFO:root:2024-04-10 18:35:18, Train, Epoch : 5, Step : 2960, Loss : 0.39519, Acc : 0.812, Sensitive_Loss : 0.11278, Sensitive_Acc : 16.600, Run Time : 9.86 sec
INFO:root:2024-04-10 18:35:29, Train, Epoch : 5, Step : 2970, Loss : 0.41822, Acc : 0.772, Sensitive_Loss : 0.10943, Sensitive_Acc : 15.600, Run Time : 10.38 sec
INFO:root:2024-04-10 18:35:38, Train, Epoch : 5, Step : 2980, Loss : 0.40060, Acc : 0.825, Sensitive_Loss : 0.10195, Sensitive_Acc : 17.000, Run Time : 9.51 sec
INFO:root:2024-04-10 18:35:47, Train, Epoch : 5, Step : 2990, Loss : 0.35941, Acc : 0.831, Sensitive_Loss : 0.08670, Sensitive_Acc : 15.100, Run Time : 9.04 sec
INFO:root:2024-04-10 18:35:59, Train, Epoch : 5, Step : 3000, Loss : 0.36958, Acc : 0.853, Sensitive_Loss : 0.13002, Sensitive_Acc : 16.200, Run Time : 12.06 sec
INFO:root:2024-04-10 18:37:59, Dev, Step : 3000, Loss : 0.53163, Acc : 0.770, Auc : 0.856, Sensitive_Loss : 0.12928, Sensitive_Acc : 16.350, Sensitive_Auc : 0.992, Mean auc: 0.856, Run Time : 119.53 sec
INFO:root:2024-04-10 18:38:05, Train, Epoch : 5, Step : 3010, Loss : 0.37194, Acc : 0.822, Sensitive_Loss : 0.11823, Sensitive_Acc : 17.200, Run Time : 125.91 sec
INFO:root:2024-04-10 18:38:15, Train, Epoch : 5, Step : 3020, Loss : 0.36965, Acc : 0.841, Sensitive_Loss : 0.09087, Sensitive_Acc : 15.000, Run Time : 9.44 sec
INFO:root:2024-04-10 18:38:27, Train, Epoch : 5, Step : 3030, Loss : 0.44639, Acc : 0.797, Sensitive_Loss : 0.08330, Sensitive_Acc : 16.500, Run Time : 11.89 sec
INFO:root:2024-04-10 18:38:36, Train, Epoch : 5, Step : 3040, Loss : 0.34052, Acc : 0.822, Sensitive_Loss : 0.09365, Sensitive_Acc : 16.400, Run Time : 9.05 sec
INFO:root:2024-04-10 18:38:45, Train, Epoch : 5, Step : 3050, Loss : 0.42035, Acc : 0.809, Sensitive_Loss : 0.12104, Sensitive_Acc : 17.500, Run Time : 9.00 sec
INFO:root:2024-04-10 18:38:53, Train, Epoch : 5, Step : 3060, Loss : 0.41783, Acc : 0.791, Sensitive_Loss : 0.12162, Sensitive_Acc : 15.800, Run Time : 8.56 sec
INFO:root:2024-04-10 18:39:02, Train, Epoch : 5, Step : 3070, Loss : 0.44149, Acc : 0.816, Sensitive_Loss : 0.11709, Sensitive_Acc : 17.700, Run Time : 9.03 sec
INFO:root:2024-04-10 18:39:13, Train, Epoch : 5, Step : 3080, Loss : 0.36878, Acc : 0.828, Sensitive_Loss : 0.08122, Sensitive_Acc : 15.700, Run Time : 10.27 sec
INFO:root:2024-04-10 18:39:22, Train, Epoch : 5, Step : 3090, Loss : 0.37172, Acc : 0.828, Sensitive_Loss : 0.10919, Sensitive_Acc : 16.800, Run Time : 9.24 sec
INFO:root:2024-04-10 18:39:30, Train, Epoch : 5, Step : 3100, Loss : 0.32257, Acc : 0.853, Sensitive_Loss : 0.10156, Sensitive_Acc : 16.900, Run Time : 8.41 sec
INFO:root:2024-04-10 18:41:15, Dev, Step : 3100, Loss : 0.51078, Acc : 0.779, Auc : 0.860, Sensitive_Loss : 0.16211, Sensitive_Acc : 16.336, Sensitive_Auc : 0.993, Mean auc: 0.860, Run Time : 104.96 sec
INFO:root:2024-04-10 18:41:22, Train, Epoch : 5, Step : 3110, Loss : 0.42680, Acc : 0.803, Sensitive_Loss : 0.12662, Sensitive_Acc : 16.100, Run Time : 111.44 sec
INFO:root:2024-04-10 18:41:31, Train, Epoch : 5, Step : 3120, Loss : 0.33161, Acc : 0.866, Sensitive_Loss : 0.08104, Sensitive_Acc : 15.400, Run Time : 9.31 sec
INFO:root:2024-04-10 18:41:42, Train, Epoch : 5, Step : 3130, Loss : 0.42566, Acc : 0.809, Sensitive_Loss : 0.11742, Sensitive_Acc : 16.900, Run Time : 10.87 sec
INFO:root:2024-04-10 18:41:51, Train, Epoch : 5, Step : 3140, Loss : 0.41871, Acc : 0.816, Sensitive_Loss : 0.09757, Sensitive_Acc : 16.700, Run Time : 9.21 sec
INFO:root:2024-04-10 18:42:00, Train, Epoch : 5, Step : 3150, Loss : 0.35480, Acc : 0.859, Sensitive_Loss : 0.09421, Sensitive_Acc : 16.200, Run Time : 9.27 sec
INFO:root:2024-04-10 18:42:09, Train, Epoch : 5, Step : 3160, Loss : 0.53319, Acc : 0.781, Sensitive_Loss : 0.05906, Sensitive_Acc : 16.600, Run Time : 8.61 sec
INFO:root:2024-04-10 18:42:18, Train, Epoch : 5, Step : 3170, Loss : 0.32946, Acc : 0.859, Sensitive_Loss : 0.10343, Sensitive_Acc : 16.000, Run Time : 9.47 sec
INFO:root:2024-04-10 18:42:27, Train, Epoch : 5, Step : 3180, Loss : 0.38631, Acc : 0.847, Sensitive_Loss : 0.10353, Sensitive_Acc : 16.900, Run Time : 8.98 sec
INFO:root:2024-04-10 18:42:36, Train, Epoch : 5, Step : 3190, Loss : 0.33930, Acc : 0.831, Sensitive_Loss : 0.06199, Sensitive_Acc : 15.400, Run Time : 8.84 sec
INFO:root:2024-04-10 18:42:45, Train, Epoch : 5, Step : 3200, Loss : 0.34347, Acc : 0.838, Sensitive_Loss : 0.11028, Sensitive_Acc : 15.900, Run Time : 8.39 sec
INFO:root:2024-04-10 18:44:18, Dev, Step : 3200, Loss : 0.50185, Acc : 0.781, Auc : 0.862, Sensitive_Loss : 0.14709, Sensitive_Acc : 16.321, Sensitive_Auc : 0.994, Mean auc: 0.862, Run Time : 93.69 sec
INFO:root:2024-04-10 18:44:19, Best, Step : 3200, Loss : 0.50185, Acc : 0.781, Auc : 0.862, Sensitive_Loss : 0.14709, Sensitive_Acc : 16.321, Sensitive_Auc : 0.994, Best Auc : 0.862
INFO:root:2024-04-10 18:44:25, Train, Epoch : 5, Step : 3210, Loss : 0.40051, Acc : 0.825, Sensitive_Loss : 0.08883, Sensitive_Acc : 17.000, Run Time : 100.67 sec
INFO:root:2024-04-10 18:44:34, Train, Epoch : 5, Step : 3220, Loss : 0.35079, Acc : 0.822, Sensitive_Loss : 0.09700, Sensitive_Acc : 16.500, Run Time : 8.89 sec
INFO:root:2024-04-10 18:46:09
INFO:root:y_pred: [0.44477698 0.03666753 0.895296   ... 0.5916213  0.731201   0.34482038]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9994254e-01 3.4443634e-03 5.0348765e-01 9.9998057e-01 9.9929261e-01
 9.9712062e-01 9.9988616e-01 5.3796689e-06 9.9552357e-01 9.9961638e-01
 1.8196110e-01 5.4512745e-01 1.6258531e-05 9.1189039e-01 9.9999154e-01
 9.9987352e-01 9.8896307e-01 9.9542540e-01 9.9714124e-01 9.9927491e-01
 9.9742740e-01 2.6903343e-01 9.9988139e-01 9.0878451e-01 9.6956605e-01
 1.2032203e-02 9.9760169e-01 5.7979976e-03 9.9996305e-01 3.5378861e-03
 5.6910231e-03 6.0139704e-01 1.9861011e-02 9.9694258e-01 2.6456710e-05
 9.9991143e-01 1.1242025e-04 9.9997127e-01 6.9404662e-02 9.8076457e-01
 9.9985123e-01 2.8444498e-04 7.7440120e-02 3.2775829e-04 3.1470558e-01
 6.9539785e-01 9.9998832e-01 9.8516995e-01 9.9441516e-01 9.9940062e-01
 1.1904989e-03 9.6733141e-01 4.8893667e-03 2.2888990e-01 9.9999762e-01
 3.7712002e-01 9.9418938e-01 9.9964726e-01 9.9758697e-01 1.0172251e-02
 8.1048012e-03 9.9957794e-01 3.1742588e-01 9.9996841e-01 9.9730861e-01
 4.2494345e-02 9.9860197e-01 9.4601882e-01 9.9999762e-01 9.9997640e-01
 1.9874824e-02 3.7662628e-01 9.1533631e-01 9.9994314e-01 9.9380714e-01
 1.4623856e-03 7.9564548e-01 4.9936976e-03 1.5625641e-04 9.9830049e-01
 4.1511990e-02 9.9996817e-01 9.9995589e-01 9.9987423e-01 4.5964745e-01
 9.9999976e-01 6.4420950e-05 5.4938754e-04 9.9201608e-01 9.9868089e-01
 2.8812652e-02 8.2147783e-01 1.9653231e-02 8.4498423e-01 2.5291590e-02
 9.9995935e-01 2.1316609e-03 9.9801242e-01 9.7989208e-01 2.4594285e-04
 8.0216577e-04 1.8109027e-01 9.9852645e-01 9.9994504e-01 9.9583769e-01
 6.9235635e-01 9.9803656e-01 1.1285371e-01 5.3209901e-01 9.9998438e-01
 2.5682637e-05 1.0625150e-03 3.0828176e-02 9.9969578e-01 9.9901521e-01
 7.7165678e-05 9.9480903e-01 8.2014618e-04 9.9996758e-01 6.1246181e-01
 9.9982268e-01 9.9997079e-01 6.6022700e-01 1.3637207e-01 5.7383971e-03
 1.8214652e-02 1.3836653e-02 1.5452494e-04 9.9111652e-01 9.9988472e-01
 9.2740002e-04 1.3355823e-04 6.7878113e-04 8.5691631e-02 9.9774444e-01
 9.9936694e-01 9.9954635e-01 3.7620953e-01 3.0617547e-01 9.9186581e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 18:46:09, Dev, Step : 3220, Loss : 0.51557, Acc : 0.774, Auc : 0.860, Sensitive_Loss : 0.14808, Sensitive_Acc : 16.336, Sensitive_Auc : 0.993, Mean auc: 0.860, Run Time : 94.25 sec
INFO:root:2024-04-10 18:46:20, Train, Epoch : 6, Step : 3230, Loss : 0.39309, Acc : 0.816, Sensitive_Loss : 0.12289, Sensitive_Acc : 14.600, Run Time : 10.24 sec
INFO:root:2024-04-10 18:46:29, Train, Epoch : 6, Step : 3240, Loss : 0.33025, Acc : 0.841, Sensitive_Loss : 0.12133, Sensitive_Acc : 17.100, Run Time : 9.61 sec
INFO:root:2024-04-10 18:46:39, Train, Epoch : 6, Step : 3250, Loss : 0.32253, Acc : 0.850, Sensitive_Loss : 0.07119, Sensitive_Acc : 16.100, Run Time : 9.30 sec
INFO:root:2024-04-10 18:46:48, Train, Epoch : 6, Step : 3260, Loss : 0.30284, Acc : 0.847, Sensitive_Loss : 0.10918, Sensitive_Acc : 17.000, Run Time : 9.46 sec
INFO:root:2024-04-10 18:46:57, Train, Epoch : 6, Step : 3270, Loss : 0.29068, Acc : 0.850, Sensitive_Loss : 0.08964, Sensitive_Acc : 17.000, Run Time : 9.14 sec
INFO:root:2024-04-10 18:47:07, Train, Epoch : 6, Step : 3280, Loss : 0.35446, Acc : 0.828, Sensitive_Loss : 0.09382, Sensitive_Acc : 15.600, Run Time : 9.68 sec
INFO:root:2024-04-10 18:47:18, Train, Epoch : 6, Step : 3290, Loss : 0.40618, Acc : 0.816, Sensitive_Loss : 0.06895, Sensitive_Acc : 17.200, Run Time : 10.55 sec
INFO:root:2024-04-10 18:47:27, Train, Epoch : 6, Step : 3300, Loss : 0.40939, Acc : 0.831, Sensitive_Loss : 0.08830, Sensitive_Acc : 17.600, Run Time : 9.32 sec
INFO:root:2024-04-10 18:49:03, Dev, Step : 3300, Loss : 0.51367, Acc : 0.777, Auc : 0.859, Sensitive_Loss : 0.14424, Sensitive_Acc : 16.321, Sensitive_Auc : 0.993, Mean auc: 0.859, Run Time : 95.94 sec
INFO:root:2024-04-10 18:49:09, Train, Epoch : 6, Step : 3310, Loss : 0.40843, Acc : 0.816, Sensitive_Loss : 0.09230, Sensitive_Acc : 15.600, Run Time : 102.40 sec
INFO:root:2024-04-10 18:49:20, Train, Epoch : 6, Step : 3320, Loss : 0.38046, Acc : 0.812, Sensitive_Loss : 0.08350, Sensitive_Acc : 16.400, Run Time : 10.99 sec
INFO:root:2024-04-10 18:49:30, Train, Epoch : 6, Step : 3330, Loss : 0.40410, Acc : 0.800, Sensitive_Loss : 0.10944, Sensitive_Acc : 17.500, Run Time : 10.16 sec
INFO:root:2024-04-10 18:49:40, Train, Epoch : 6, Step : 3340, Loss : 0.37610, Acc : 0.828, Sensitive_Loss : 0.09055, Sensitive_Acc : 16.100, Run Time : 9.70 sec
INFO:root:2024-04-10 18:49:50, Train, Epoch : 6, Step : 3350, Loss : 0.34820, Acc : 0.831, Sensitive_Loss : 0.09311, Sensitive_Acc : 17.000, Run Time : 9.42 sec
INFO:root:2024-04-10 18:49:59, Train, Epoch : 6, Step : 3360, Loss : 0.31472, Acc : 0.853, Sensitive_Loss : 0.12328, Sensitive_Acc : 14.200, Run Time : 9.43 sec
INFO:root:2024-04-10 18:50:09, Train, Epoch : 6, Step : 3370, Loss : 0.40547, Acc : 0.816, Sensitive_Loss : 0.10199, Sensitive_Acc : 15.800, Run Time : 10.30 sec
INFO:root:2024-04-10 18:50:19, Train, Epoch : 6, Step : 3380, Loss : 0.38937, Acc : 0.834, Sensitive_Loss : 0.08204, Sensitive_Acc : 17.700, Run Time : 9.30 sec
INFO:root:2024-04-10 18:50:31, Train, Epoch : 6, Step : 3390, Loss : 0.42769, Acc : 0.828, Sensitive_Loss : 0.07786, Sensitive_Acc : 17.200, Run Time : 12.48 sec
INFO:root:2024-04-10 18:50:40, Train, Epoch : 6, Step : 3400, Loss : 0.37584, Acc : 0.781, Sensitive_Loss : 0.11818, Sensitive_Acc : 16.200, Run Time : 8.68 sec
INFO:root:2024-04-10 18:52:16, Dev, Step : 3400, Loss : 0.53209, Acc : 0.766, Auc : 0.857, Sensitive_Loss : 0.14848, Sensitive_Acc : 16.321, Sensitive_Auc : 0.993, Mean auc: 0.857, Run Time : 96.33 sec
INFO:root:2024-04-10 18:52:23, Train, Epoch : 6, Step : 3410, Loss : 0.27416, Acc : 0.872, Sensitive_Loss : 0.07713, Sensitive_Acc : 18.300, Run Time : 103.26 sec
INFO:root:2024-04-10 18:52:32, Train, Epoch : 6, Step : 3420, Loss : 0.35893, Acc : 0.819, Sensitive_Loss : 0.08493, Sensitive_Acc : 15.900, Run Time : 9.35 sec
INFO:root:2024-04-10 18:52:44, Train, Epoch : 6, Step : 3430, Loss : 0.34256, Acc : 0.831, Sensitive_Loss : 0.12024, Sensitive_Acc : 16.400, Run Time : 11.70 sec
INFO:root:2024-04-10 18:52:56, Train, Epoch : 6, Step : 3440, Loss : 0.35766, Acc : 0.838, Sensitive_Loss : 0.09253, Sensitive_Acc : 17.100, Run Time : 11.72 sec
INFO:root:2024-04-10 18:53:05, Train, Epoch : 6, Step : 3450, Loss : 0.38955, Acc : 0.831, Sensitive_Loss : 0.09960, Sensitive_Acc : 17.800, Run Time : 9.14 sec
INFO:root:2024-04-10 18:53:14, Train, Epoch : 6, Step : 3460, Loss : 0.38888, Acc : 0.834, Sensitive_Loss : 0.11840, Sensitive_Acc : 16.700, Run Time : 9.02 sec
INFO:root:2024-04-10 18:53:23, Train, Epoch : 6, Step : 3470, Loss : 0.34734, Acc : 0.816, Sensitive_Loss : 0.08604, Sensitive_Acc : 16.900, Run Time : 9.01 sec
INFO:root:2024-04-10 18:53:32, Train, Epoch : 6, Step : 3480, Loss : 0.44186, Acc : 0.825, Sensitive_Loss : 0.14376, Sensitive_Acc : 18.100, Run Time : 8.86 sec
INFO:root:2024-04-10 18:53:41, Train, Epoch : 6, Step : 3490, Loss : 0.38143, Acc : 0.838, Sensitive_Loss : 0.09118, Sensitive_Acc : 15.100, Run Time : 8.97 sec
INFO:root:2024-04-10 18:53:50, Train, Epoch : 6, Step : 3500, Loss : 0.37670, Acc : 0.859, Sensitive_Loss : 0.07797, Sensitive_Acc : 16.900, Run Time : 9.32 sec
INFO:root:2024-04-10 18:55:24, Dev, Step : 3500, Loss : 0.49968, Acc : 0.781, Auc : 0.860, Sensitive_Loss : 0.13624, Sensitive_Acc : 16.293, Sensitive_Auc : 0.993, Mean auc: 0.860, Run Time : 94.14 sec
INFO:root:2024-04-10 18:55:34, Train, Epoch : 6, Step : 3510, Loss : 0.34482, Acc : 0.819, Sensitive_Loss : 0.08233, Sensitive_Acc : 15.400, Run Time : 104.36 sec
INFO:root:2024-04-10 18:55:45, Train, Epoch : 6, Step : 3520, Loss : 0.37281, Acc : 0.856, Sensitive_Loss : 0.09800, Sensitive_Acc : 17.200, Run Time : 10.76 sec
INFO:root:2024-04-10 18:55:54, Train, Epoch : 6, Step : 3530, Loss : 0.37937, Acc : 0.847, Sensitive_Loss : 0.05875, Sensitive_Acc : 15.600, Run Time : 9.19 sec
INFO:root:2024-04-10 18:56:04, Train, Epoch : 6, Step : 3540, Loss : 0.29451, Acc : 0.834, Sensitive_Loss : 0.10906, Sensitive_Acc : 16.400, Run Time : 10.03 sec
INFO:root:2024-04-10 18:56:15, Train, Epoch : 6, Step : 3550, Loss : 0.38689, Acc : 0.816, Sensitive_Loss : 0.11498, Sensitive_Acc : 16.900, Run Time : 10.15 sec
INFO:root:2024-04-10 18:56:23, Train, Epoch : 6, Step : 3560, Loss : 0.43455, Acc : 0.819, Sensitive_Loss : 0.09683, Sensitive_Acc : 16.900, Run Time : 8.69 sec
INFO:root:2024-04-10 18:56:34, Train, Epoch : 6, Step : 3570, Loss : 0.35306, Acc : 0.856, Sensitive_Loss : 0.14274, Sensitive_Acc : 14.800, Run Time : 10.66 sec
INFO:root:2024-04-10 18:56:42, Train, Epoch : 6, Step : 3580, Loss : 0.34708, Acc : 0.819, Sensitive_Loss : 0.09334, Sensitive_Acc : 16.400, Run Time : 8.35 sec
INFO:root:2024-04-10 18:56:51, Train, Epoch : 6, Step : 3590, Loss : 0.33816, Acc : 0.806, Sensitive_Loss : 0.09813, Sensitive_Acc : 17.200, Run Time : 8.38 sec
INFO:root:2024-04-10 18:57:00, Train, Epoch : 6, Step : 3600, Loss : 0.43831, Acc : 0.834, Sensitive_Loss : 0.12322, Sensitive_Acc : 17.400, Run Time : 9.70 sec
INFO:root:2024-04-10 18:58:44, Dev, Step : 3600, Loss : 0.50871, Acc : 0.775, Auc : 0.854, Sensitive_Loss : 0.13223, Sensitive_Acc : 16.264, Sensitive_Auc : 0.996, Mean auc: 0.854, Run Time : 103.97 sec
INFO:root:2024-04-10 18:58:51, Train, Epoch : 6, Step : 3610, Loss : 0.36091, Acc : 0.863, Sensitive_Loss : 0.08743, Sensitive_Acc : 16.900, Run Time : 110.29 sec
INFO:root:2024-04-10 18:59:02, Train, Epoch : 6, Step : 3620, Loss : 0.43226, Acc : 0.797, Sensitive_Loss : 0.08327, Sensitive_Acc : 17.700, Run Time : 11.69 sec
INFO:root:2024-04-10 18:59:13, Train, Epoch : 6, Step : 3630, Loss : 0.35142, Acc : 0.863, Sensitive_Loss : 0.09818, Sensitive_Acc : 15.700, Run Time : 10.84 sec
INFO:root:2024-04-10 18:59:22, Train, Epoch : 6, Step : 3640, Loss : 0.30052, Acc : 0.856, Sensitive_Loss : 0.08650, Sensitive_Acc : 15.800, Run Time : 8.91 sec
INFO:root:2024-04-10 18:59:32, Train, Epoch : 6, Step : 3650, Loss : 0.33013, Acc : 0.856, Sensitive_Loss : 0.09557, Sensitive_Acc : 17.000, Run Time : 9.46 sec
INFO:root:2024-04-10 18:59:41, Train, Epoch : 6, Step : 3660, Loss : 0.40013, Acc : 0.828, Sensitive_Loss : 0.07233, Sensitive_Acc : 16.300, Run Time : 9.58 sec
INFO:root:2024-04-10 18:59:49, Train, Epoch : 6, Step : 3670, Loss : 0.37411, Acc : 0.816, Sensitive_Loss : 0.11106, Sensitive_Acc : 16.000, Run Time : 8.26 sec
INFO:root:2024-04-10 18:59:58, Train, Epoch : 6, Step : 3680, Loss : 0.37335, Acc : 0.812, Sensitive_Loss : 0.09786, Sensitive_Acc : 15.500, Run Time : 8.72 sec
INFO:root:2024-04-10 19:00:08, Train, Epoch : 6, Step : 3690, Loss : 0.31749, Acc : 0.834, Sensitive_Loss : 0.07318, Sensitive_Acc : 15.500, Run Time : 9.63 sec
INFO:root:2024-04-10 19:00:17, Train, Epoch : 6, Step : 3700, Loss : 0.33485, Acc : 0.838, Sensitive_Loss : 0.08855, Sensitive_Acc : 16.500, Run Time : 9.30 sec
INFO:root:2024-04-10 19:02:03, Dev, Step : 3700, Loss : 0.51888, Acc : 0.774, Auc : 0.857, Sensitive_Loss : 0.13455, Sensitive_Acc : 16.307, Sensitive_Auc : 0.994, Mean auc: 0.857, Run Time : 106.09 sec
INFO:root:2024-04-10 19:02:10, Train, Epoch : 6, Step : 3710, Loss : 0.34652, Acc : 0.844, Sensitive_Loss : 0.08958, Sensitive_Acc : 16.800, Run Time : 112.66 sec
INFO:root:2024-04-10 19:02:22, Train, Epoch : 6, Step : 3720, Loss : 0.37610, Acc : 0.828, Sensitive_Loss : 0.13229, Sensitive_Acc : 15.300, Run Time : 12.05 sec
INFO:root:2024-04-10 19:02:30, Train, Epoch : 6, Step : 3730, Loss : 0.36627, Acc : 0.828, Sensitive_Loss : 0.08851, Sensitive_Acc : 16.400, Run Time : 8.69 sec
INFO:root:2024-04-10 19:02:39, Train, Epoch : 6, Step : 3740, Loss : 0.37447, Acc : 0.844, Sensitive_Loss : 0.09933, Sensitive_Acc : 18.600, Run Time : 8.74 sec
INFO:root:2024-04-10 19:02:52, Train, Epoch : 6, Step : 3750, Loss : 0.30322, Acc : 0.881, Sensitive_Loss : 0.10169, Sensitive_Acc : 18.900, Run Time : 12.80 sec
INFO:root:2024-04-10 19:03:03, Train, Epoch : 6, Step : 3760, Loss : 0.36153, Acc : 0.816, Sensitive_Loss : 0.10826, Sensitive_Acc : 16.500, Run Time : 10.80 sec
INFO:root:2024-04-10 19:03:12, Train, Epoch : 6, Step : 3770, Loss : 0.36435, Acc : 0.853, Sensitive_Loss : 0.10725, Sensitive_Acc : 14.800, Run Time : 9.32 sec
INFO:root:2024-04-10 19:03:26, Train, Epoch : 6, Step : 3780, Loss : 0.38052, Acc : 0.806, Sensitive_Loss : 0.09327, Sensitive_Acc : 15.700, Run Time : 13.90 sec
INFO:root:2024-04-10 19:03:35, Train, Epoch : 6, Step : 3790, Loss : 0.33773, Acc : 0.834, Sensitive_Loss : 0.06082, Sensitive_Acc : 17.000, Run Time : 9.14 sec
INFO:root:2024-04-10 19:03:44, Train, Epoch : 6, Step : 3800, Loss : 0.34336, Acc : 0.847, Sensitive_Loss : 0.10243, Sensitive_Acc : 14.700, Run Time : 8.68 sec
INFO:root:2024-04-10 19:05:27, Dev, Step : 3800, Loss : 0.50286, Acc : 0.779, Auc : 0.860, Sensitive_Loss : 0.12429, Sensitive_Acc : 16.321, Sensitive_Auc : 0.993, Mean auc: 0.860, Run Time : 103.04 sec
INFO:root:2024-04-10 19:05:33, Train, Epoch : 6, Step : 3810, Loss : 0.39273, Acc : 0.803, Sensitive_Loss : 0.07152, Sensitive_Acc : 16.000, Run Time : 109.45 sec
INFO:root:2024-04-10 19:05:47, Train, Epoch : 6, Step : 3820, Loss : 0.36461, Acc : 0.872, Sensitive_Loss : 0.07022, Sensitive_Acc : 16.900, Run Time : 13.91 sec
INFO:root:2024-04-10 19:05:57, Train, Epoch : 6, Step : 3830, Loss : 0.36859, Acc : 0.834, Sensitive_Loss : 0.10041, Sensitive_Acc : 17.900, Run Time : 9.67 sec
INFO:root:2024-04-10 19:06:05, Train, Epoch : 6, Step : 3840, Loss : 0.36916, Acc : 0.831, Sensitive_Loss : 0.13659, Sensitive_Acc : 16.800, Run Time : 8.63 sec
INFO:root:2024-04-10 19:06:17, Train, Epoch : 6, Step : 3850, Loss : 0.37649, Acc : 0.856, Sensitive_Loss : 0.10009, Sensitive_Acc : 15.900, Run Time : 11.50 sec
INFO:root:2024-04-10 19:06:26, Train, Epoch : 6, Step : 3860, Loss : 0.39407, Acc : 0.825, Sensitive_Loss : 0.09684, Sensitive_Acc : 17.300, Run Time : 9.47 sec
INFO:root:2024-04-10 19:08:11
INFO:root:y_pred: [0.38181767 0.03629665 0.88055277 ... 0.47219646 0.5969444  0.39167243]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9981540e-01 3.2581743e-03 3.8830596e-01 9.9997807e-01 9.9757785e-01
 9.9662209e-01 9.9975377e-01 5.2496803e-06 9.9592555e-01 9.9942529e-01
 9.7367555e-02 5.4101264e-01 1.0966470e-05 9.3359095e-01 9.9998403e-01
 9.9982858e-01 9.9038410e-01 9.9029583e-01 9.9863523e-01 9.9903488e-01
 9.9457318e-01 5.0433904e-01 9.9992192e-01 8.5067469e-01 9.7111982e-01
 9.1254525e-03 9.9794954e-01 2.2099686e-03 9.9997890e-01 1.7111737e-03
 3.3799580e-03 4.6087545e-01 1.2212699e-02 9.9621564e-01 2.8427006e-05
 9.9993789e-01 4.1483920e-05 9.9998116e-01 4.8475653e-02 9.6377224e-01
 9.9993408e-01 1.9331546e-04 9.9785358e-02 2.7628432e-04 2.9573965e-01
 7.5339192e-01 9.9998605e-01 9.8499596e-01 9.9629909e-01 9.9915099e-01
 1.0713786e-03 9.5964444e-01 1.5054790e-03 2.6294163e-01 9.9998546e-01
 2.1571712e-01 9.9103469e-01 9.9937397e-01 9.9852425e-01 1.0986523e-02
 7.9965005e-03 9.9956459e-01 1.2602770e-01 9.9995005e-01 9.9106491e-01
 1.9897396e-02 9.9612588e-01 9.4387704e-01 9.9999690e-01 9.9998391e-01
 1.8532366e-02 2.7991560e-01 8.6210489e-01 9.9993348e-01 9.8691177e-01
 9.0701203e-04 5.9570599e-01 4.1274051e-03 1.6211520e-04 9.9800986e-01
 4.2807590e-02 9.9997723e-01 9.9987435e-01 9.9987411e-01 3.9176387e-01
 9.9999928e-01 7.5855991e-05 6.6326675e-04 9.9231577e-01 9.9808997e-01
 2.1190373e-02 8.4441495e-01 1.4157683e-02 7.8351533e-01 2.2826876e-02
 9.9996698e-01 2.2146462e-03 9.9760520e-01 9.9019414e-01 5.3297535e-05
 1.3717206e-04 2.1254449e-01 9.9823713e-01 9.9997175e-01 9.9751914e-01
 7.2699559e-01 9.9804688e-01 8.5019410e-02 4.8829505e-01 9.9998939e-01
 5.4103643e-06 1.3265414e-03 2.0303495e-02 9.9953306e-01 9.9890161e-01
 5.5051201e-05 9.9008709e-01 5.0314202e-04 9.9997139e-01 2.3610108e-01
 9.9983716e-01 9.9999213e-01 6.5730387e-01 1.4421353e-01 2.6394264e-03
 6.0209674e-03 7.4964939e-03 4.2710933e-04 9.9581546e-01 9.9980098e-01
 4.3440703e-04 6.6683860e-04 5.1355007e-04 4.8656292e-02 9.9950302e-01
 9.9942541e-01 9.9964666e-01 4.8033637e-01 2.5893208e-01 9.9135661e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 19:08:11, Dev, Step : 3864, Loss : 0.52203, Acc : 0.782, Auc : 0.861, Sensitive_Loss : 0.13465, Sensitive_Acc : 16.321, Sensitive_Auc : 0.992, Mean auc: 0.861, Run Time : 101.24 sec
INFO:root:2024-04-10 19:08:20, Train, Epoch : 7, Step : 3870, Loss : 0.18840, Acc : 0.509, Sensitive_Loss : 0.04309, Sensitive_Acc : 11.100, Run Time : 7.20 sec
INFO:root:2024-04-10 19:08:28, Train, Epoch : 7, Step : 3880, Loss : 0.36630, Acc : 0.838, Sensitive_Loss : 0.10854, Sensitive_Acc : 14.200, Run Time : 8.53 sec
INFO:root:2024-04-10 19:08:38, Train, Epoch : 7, Step : 3890, Loss : 0.36968, Acc : 0.834, Sensitive_Loss : 0.07553, Sensitive_Acc : 15.600, Run Time : 9.33 sec
INFO:root:2024-04-10 19:08:49, Train, Epoch : 7, Step : 3900, Loss : 0.32039, Acc : 0.869, Sensitive_Loss : 0.10995, Sensitive_Acc : 16.100, Run Time : 11.58 sec
INFO:root:2024-04-10 19:10:30, Dev, Step : 3900, Loss : 0.51961, Acc : 0.775, Auc : 0.858, Sensitive_Loss : 0.12951, Sensitive_Acc : 16.350, Sensitive_Auc : 0.993, Mean auc: 0.858, Run Time : 100.35 sec
INFO:root:2024-04-10 19:10:36, Train, Epoch : 7, Step : 3910, Loss : 0.33869, Acc : 0.866, Sensitive_Loss : 0.11198, Sensitive_Acc : 17.400, Run Time : 107.05 sec
INFO:root:2024-04-10 19:10:46, Train, Epoch : 7, Step : 3920, Loss : 0.37713, Acc : 0.791, Sensitive_Loss : 0.09228, Sensitive_Acc : 15.500, Run Time : 9.39 sec
INFO:root:2024-04-10 19:10:58, Train, Epoch : 7, Step : 3930, Loss : 0.34367, Acc : 0.863, Sensitive_Loss : 0.08982, Sensitive_Acc : 16.600, Run Time : 12.10 sec
INFO:root:2024-04-10 19:11:07, Train, Epoch : 7, Step : 3940, Loss : 0.33060, Acc : 0.856, Sensitive_Loss : 0.07755, Sensitive_Acc : 17.000, Run Time : 9.08 sec
INFO:root:2024-04-10 19:11:16, Train, Epoch : 7, Step : 3950, Loss : 0.37774, Acc : 0.828, Sensitive_Loss : 0.10163, Sensitive_Acc : 16.400, Run Time : 9.13 sec
INFO:root:2024-04-10 19:11:29, Train, Epoch : 7, Step : 3960, Loss : 0.33508, Acc : 0.863, Sensitive_Loss : 0.11651, Sensitive_Acc : 15.200, Run Time : 12.41 sec
INFO:root:2024-04-10 19:11:38, Train, Epoch : 7, Step : 3970, Loss : 0.34370, Acc : 0.841, Sensitive_Loss : 0.12276, Sensitive_Acc : 16.600, Run Time : 9.56 sec
INFO:root:2024-04-10 19:11:48, Train, Epoch : 7, Step : 3980, Loss : 0.29148, Acc : 0.894, Sensitive_Loss : 0.09666, Sensitive_Acc : 16.600, Run Time : 9.55 sec
INFO:root:2024-04-10 19:12:01, Train, Epoch : 7, Step : 3990, Loss : 0.33045, Acc : 0.887, Sensitive_Loss : 0.05487, Sensitive_Acc : 16.900, Run Time : 13.84 sec
INFO:root:2024-04-10 19:12:11, Train, Epoch : 7, Step : 4000, Loss : 0.27624, Acc : 0.863, Sensitive_Loss : 0.07642, Sensitive_Acc : 15.200, Run Time : 9.45 sec
INFO:root:2024-04-10 19:13:51, Dev, Step : 4000, Loss : 0.52517, Acc : 0.781, Auc : 0.860, Sensitive_Loss : 0.13008, Sensitive_Acc : 16.264, Sensitive_Auc : 0.992, Mean auc: 0.860, Run Time : 99.78 sec
INFO:root:2024-04-10 19:14:01, Train, Epoch : 7, Step : 4010, Loss : 0.32300, Acc : 0.825, Sensitive_Loss : 0.09351, Sensitive_Acc : 17.900, Run Time : 109.94 sec
INFO:root:2024-04-10 19:14:11, Train, Epoch : 7, Step : 4020, Loss : 0.34747, Acc : 0.856, Sensitive_Loss : 0.09241, Sensitive_Acc : 16.500, Run Time : 9.90 sec
INFO:root:2024-04-10 19:14:24, Train, Epoch : 7, Step : 4030, Loss : 0.38827, Acc : 0.812, Sensitive_Loss : 0.07644, Sensitive_Acc : 15.000, Run Time : 13.50 sec
INFO:root:2024-04-10 19:14:33, Train, Epoch : 7, Step : 4040, Loss : 0.35945, Acc : 0.850, Sensitive_Loss : 0.09553, Sensitive_Acc : 14.500, Run Time : 9.08 sec
INFO:root:2024-04-10 19:14:43, Train, Epoch : 7, Step : 4050, Loss : 0.31444, Acc : 0.844, Sensitive_Loss : 0.09508, Sensitive_Acc : 16.600, Run Time : 9.20 sec
INFO:root:2024-04-10 19:14:53, Train, Epoch : 7, Step : 4060, Loss : 0.38149, Acc : 0.812, Sensitive_Loss : 0.09947, Sensitive_Acc : 17.100, Run Time : 10.83 sec
INFO:root:2024-04-10 19:15:02, Train, Epoch : 7, Step : 4070, Loss : 0.35153, Acc : 0.856, Sensitive_Loss : 0.11632, Sensitive_Acc : 15.700, Run Time : 8.82 sec
INFO:root:2024-04-10 19:15:12, Train, Epoch : 7, Step : 4080, Loss : 0.32524, Acc : 0.872, Sensitive_Loss : 0.06731, Sensitive_Acc : 15.400, Run Time : 9.47 sec
INFO:root:2024-04-10 19:15:21, Train, Epoch : 7, Step : 4090, Loss : 0.42657, Acc : 0.803, Sensitive_Loss : 0.09016, Sensitive_Acc : 18.500, Run Time : 9.09 sec
INFO:root:2024-04-10 19:15:30, Train, Epoch : 7, Step : 4100, Loss : 0.39641, Acc : 0.856, Sensitive_Loss : 0.11524, Sensitive_Acc : 17.400, Run Time : 9.17 sec
INFO:root:2024-04-10 19:17:09, Dev, Step : 4100, Loss : 0.52508, Acc : 0.774, Auc : 0.854, Sensitive_Loss : 0.14608, Sensitive_Acc : 16.307, Sensitive_Auc : 0.993, Mean auc: 0.854, Run Time : 99.13 sec
INFO:root:2024-04-10 19:17:17, Train, Epoch : 7, Step : 4110, Loss : 0.36188, Acc : 0.859, Sensitive_Loss : 0.08765, Sensitive_Acc : 14.800, Run Time : 107.34 sec
INFO:root:2024-04-10 19:17:29, Train, Epoch : 7, Step : 4120, Loss : 0.32650, Acc : 0.859, Sensitive_Loss : 0.13383, Sensitive_Acc : 15.300, Run Time : 11.25 sec
INFO:root:2024-04-10 19:17:37, Train, Epoch : 7, Step : 4130, Loss : 0.39576, Acc : 0.828, Sensitive_Loss : 0.08374, Sensitive_Acc : 16.800, Run Time : 8.68 sec
INFO:root:2024-04-10 19:17:48, Train, Epoch : 7, Step : 4140, Loss : 0.31868, Acc : 0.884, Sensitive_Loss : 0.10061, Sensitive_Acc : 15.400, Run Time : 10.44 sec
INFO:root:2024-04-10 19:17:57, Train, Epoch : 7, Step : 4150, Loss : 0.34097, Acc : 0.853, Sensitive_Loss : 0.10658, Sensitive_Acc : 16.300, Run Time : 9.43 sec
INFO:root:2024-04-10 19:18:05, Train, Epoch : 7, Step : 4160, Loss : 0.35193, Acc : 0.856, Sensitive_Loss : 0.14364, Sensitive_Acc : 16.700, Run Time : 8.18 sec
INFO:root:2024-04-10 19:18:13, Train, Epoch : 7, Step : 4170, Loss : 0.36631, Acc : 0.838, Sensitive_Loss : 0.10494, Sensitive_Acc : 16.300, Run Time : 8.13 sec
INFO:root:2024-04-10 19:18:25, Train, Epoch : 7, Step : 4180, Loss : 0.37876, Acc : 0.822, Sensitive_Loss : 0.07406, Sensitive_Acc : 15.800, Run Time : 11.85 sec
INFO:root:2024-04-10 19:18:34, Train, Epoch : 7, Step : 4190, Loss : 0.32880, Acc : 0.850, Sensitive_Loss : 0.10165, Sensitive_Acc : 15.500, Run Time : 9.22 sec
INFO:root:2024-04-10 19:18:45, Train, Epoch : 7, Step : 4200, Loss : 0.32030, Acc : 0.847, Sensitive_Loss : 0.08095, Sensitive_Acc : 16.100, Run Time : 10.39 sec
INFO:root:2024-04-10 19:20:30, Dev, Step : 4200, Loss : 0.51332, Acc : 0.776, Auc : 0.857, Sensitive_Loss : 0.12712, Sensitive_Acc : 16.379, Sensitive_Auc : 0.993, Mean auc: 0.857, Run Time : 105.07 sec
INFO:root:2024-04-10 19:20:37, Train, Epoch : 7, Step : 4210, Loss : 0.31389, Acc : 0.881, Sensitive_Loss : 0.07220, Sensitive_Acc : 15.100, Run Time : 111.73 sec
INFO:root:2024-04-10 19:20:49, Train, Epoch : 7, Step : 4220, Loss : 0.39631, Acc : 0.841, Sensitive_Loss : 0.08065, Sensitive_Acc : 16.900, Run Time : 11.89 sec
INFO:root:2024-04-10 19:21:01, Train, Epoch : 7, Step : 4230, Loss : 0.33404, Acc : 0.863, Sensitive_Loss : 0.07346, Sensitive_Acc : 15.800, Run Time : 12.09 sec
INFO:root:2024-04-10 19:21:10, Train, Epoch : 7, Step : 4240, Loss : 0.35362, Acc : 0.850, Sensitive_Loss : 0.09107, Sensitive_Acc : 16.600, Run Time : 9.07 sec
INFO:root:2024-04-10 19:21:22, Train, Epoch : 7, Step : 4250, Loss : 0.34794, Acc : 0.841, Sensitive_Loss : 0.13983, Sensitive_Acc : 15.100, Run Time : 12.14 sec
INFO:root:2024-04-10 19:21:30, Train, Epoch : 7, Step : 4260, Loss : 0.31887, Acc : 0.869, Sensitive_Loss : 0.08941, Sensitive_Acc : 16.500, Run Time : 8.38 sec
INFO:root:2024-04-10 19:21:39, Train, Epoch : 7, Step : 4270, Loss : 0.28323, Acc : 0.853, Sensitive_Loss : 0.09639, Sensitive_Acc : 16.200, Run Time : 8.72 sec
INFO:root:2024-04-10 19:21:50, Train, Epoch : 7, Step : 4280, Loss : 0.32245, Acc : 0.828, Sensitive_Loss : 0.07842, Sensitive_Acc : 17.600, Run Time : 10.64 sec
INFO:root:2024-04-10 19:21:58, Train, Epoch : 7, Step : 4290, Loss : 0.29639, Acc : 0.844, Sensitive_Loss : 0.10256, Sensitive_Acc : 16.900, Run Time : 8.70 sec
INFO:root:2024-04-10 19:22:07, Train, Epoch : 7, Step : 4300, Loss : 0.35298, Acc : 0.844, Sensitive_Loss : 0.09200, Sensitive_Acc : 15.700, Run Time : 8.31 sec
INFO:root:2024-04-10 19:23:55, Dev, Step : 4300, Loss : 0.56812, Acc : 0.767, Auc : 0.858, Sensitive_Loss : 0.13596, Sensitive_Acc : 16.436, Sensitive_Auc : 0.991, Mean auc: 0.858, Run Time : 108.18 sec
INFO:root:2024-04-10 19:24:03, Train, Epoch : 7, Step : 4310, Loss : 0.27735, Acc : 0.859, Sensitive_Loss : 0.07584, Sensitive_Acc : 16.700, Run Time : 116.04 sec
INFO:root:2024-04-10 19:24:12, Train, Epoch : 7, Step : 4320, Loss : 0.38167, Acc : 0.825, Sensitive_Loss : 0.05877, Sensitive_Acc : 16.200, Run Time : 9.70 sec
INFO:root:2024-04-10 19:24:24, Train, Epoch : 7, Step : 4330, Loss : 0.28544, Acc : 0.844, Sensitive_Loss : 0.10622, Sensitive_Acc : 16.600, Run Time : 11.56 sec
INFO:root:2024-04-10 19:24:33, Train, Epoch : 7, Step : 4340, Loss : 0.30834, Acc : 0.872, Sensitive_Loss : 0.07281, Sensitive_Acc : 15.700, Run Time : 8.72 sec
INFO:root:2024-04-10 19:24:42, Train, Epoch : 7, Step : 4350, Loss : 0.32976, Acc : 0.863, Sensitive_Loss : 0.08268, Sensitive_Acc : 16.100, Run Time : 9.30 sec
INFO:root:2024-04-10 19:24:54, Train, Epoch : 7, Step : 4360, Loss : 0.39428, Acc : 0.800, Sensitive_Loss : 0.06698, Sensitive_Acc : 16.500, Run Time : 12.19 sec
INFO:root:2024-04-10 19:25:04, Train, Epoch : 7, Step : 4370, Loss : 0.36199, Acc : 0.847, Sensitive_Loss : 0.12369, Sensitive_Acc : 16.700, Run Time : 9.78 sec
INFO:root:2024-04-10 19:25:13, Train, Epoch : 7, Step : 4380, Loss : 0.27545, Acc : 0.884, Sensitive_Loss : 0.07058, Sensitive_Acc : 14.800, Run Time : 8.72 sec
INFO:root:2024-04-10 19:25:23, Train, Epoch : 7, Step : 4390, Loss : 0.33200, Acc : 0.841, Sensitive_Loss : 0.09816, Sensitive_Acc : 16.500, Run Time : 10.50 sec
INFO:root:2024-04-10 19:25:31, Train, Epoch : 7, Step : 4400, Loss : 0.30525, Acc : 0.856, Sensitive_Loss : 0.11089, Sensitive_Acc : 17.100, Run Time : 7.75 sec
INFO:root:2024-04-10 19:27:15, Dev, Step : 4400, Loss : 0.53981, Acc : 0.774, Auc : 0.858, Sensitive_Loss : 0.15352, Sensitive_Acc : 16.393, Sensitive_Auc : 0.991, Mean auc: 0.858, Run Time : 104.20 sec
INFO:root:2024-04-10 19:27:24, Train, Epoch : 7, Step : 4410, Loss : 0.39300, Acc : 0.853, Sensitive_Loss : 0.10590, Sensitive_Acc : 16.800, Run Time : 113.61 sec
INFO:root:2024-04-10 19:27:34, Train, Epoch : 7, Step : 4420, Loss : 0.31178, Acc : 0.853, Sensitive_Loss : 0.09211, Sensitive_Acc : 16.300, Run Time : 9.18 sec
INFO:root:2024-04-10 19:27:43, Train, Epoch : 7, Step : 4430, Loss : 0.40396, Acc : 0.844, Sensitive_Loss : 0.07275, Sensitive_Acc : 16.000, Run Time : 9.55 sec
INFO:root:2024-04-10 19:27:55, Train, Epoch : 7, Step : 4440, Loss : 0.35256, Acc : 0.872, Sensitive_Loss : 0.12574, Sensitive_Acc : 14.900, Run Time : 11.67 sec
INFO:root:2024-04-10 19:28:03, Train, Epoch : 7, Step : 4450, Loss : 0.31718, Acc : 0.834, Sensitive_Loss : 0.09231, Sensitive_Acc : 17.100, Run Time : 8.54 sec
INFO:root:2024-04-10 19:28:12, Train, Epoch : 7, Step : 4460, Loss : 0.33125, Acc : 0.859, Sensitive_Loss : 0.08507, Sensitive_Acc : 16.100, Run Time : 8.19 sec
INFO:root:2024-04-10 19:28:23, Train, Epoch : 7, Step : 4470, Loss : 0.32846, Acc : 0.806, Sensitive_Loss : 0.10032, Sensitive_Acc : 15.800, Run Time : 10.94 sec
INFO:root:2024-04-10 19:28:32, Train, Epoch : 7, Step : 4480, Loss : 0.36140, Acc : 0.831, Sensitive_Loss : 0.07667, Sensitive_Acc : 16.300, Run Time : 9.09 sec
INFO:root:2024-04-10 19:28:43, Train, Epoch : 7, Step : 4490, Loss : 0.34388, Acc : 0.872, Sensitive_Loss : 0.10150, Sensitive_Acc : 16.200, Run Time : 11.36 sec
INFO:root:2024-04-10 19:28:54, Train, Epoch : 7, Step : 4500, Loss : 0.35860, Acc : 0.812, Sensitive_Loss : 0.07546, Sensitive_Acc : 17.900, Run Time : 10.76 sec
INFO:root:2024-04-10 19:30:35, Dev, Step : 4500, Loss : 0.53559, Acc : 0.779, Auc : 0.859, Sensitive_Loss : 0.15321, Sensitive_Acc : 16.307, Sensitive_Auc : 0.993, Mean auc: 0.859, Run Time : 101.42 sec
INFO:root:2024-04-10 19:32:26
INFO:root:y_pred: [0.43677676 0.03277799 0.89872825 ... 0.5953321  0.6417659  0.41078478]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.99842286e-01 1.29995923e-02 5.36997795e-01 9.99990225e-01
 9.95820522e-01 9.97352004e-01 9.99907970e-01 4.42602686e-06
 9.94163334e-01 9.99621272e-01 6.99144974e-02 6.19277120e-01
 5.91314711e-05 9.46821690e-01 9.99998093e-01 9.99943256e-01
 9.87903416e-01 9.94727194e-01 9.98486221e-01 9.99277532e-01
 9.95438755e-01 5.07875383e-01 9.99958277e-01 8.25234950e-01
 9.58600581e-01 1.29764853e-02 9.98642147e-01 4.29365551e-03
 9.99982595e-01 3.53266043e-03 5.61969262e-03 5.38948059e-01
 1.20084025e-02 9.97218490e-01 4.21675577e-05 9.99968886e-01
 4.86659665e-05 9.99972820e-01 3.66250649e-02 9.80134070e-01
 9.99918818e-01 5.93944744e-04 7.27183148e-02 4.07929561e-04
 3.73717576e-01 5.34490585e-01 9.99992013e-01 9.95790660e-01
 9.97207463e-01 9.99265134e-01 1.27709541e-03 9.55009878e-01
 2.93557695e-03 2.83015251e-01 9.99984384e-01 3.21602523e-01
 9.96491730e-01 9.99716222e-01 9.98737276e-01 1.32823205e-02
 9.19654407e-03 9.99520302e-01 2.70263106e-01 9.99968767e-01
 9.96798456e-01 2.31356379e-02 9.98120844e-01 9.41509664e-01
 9.99998093e-01 9.99976277e-01 1.63701773e-02 2.92518169e-01
 9.19278741e-01 9.99894977e-01 9.93716061e-01 1.07935595e-03
 7.94210315e-01 3.58712347e-03 8.47372285e-05 9.97938693e-01
 1.04937591e-02 9.99989152e-01 9.99868155e-01 9.99963999e-01
 3.33552867e-01 9.99999762e-01 7.92960709e-05 1.63025234e-03
 9.93848145e-01 9.98875201e-01 2.54533812e-02 9.05237615e-01
 2.69055441e-02 8.24410677e-01 3.37575674e-02 9.99981046e-01
 1.49358728e-03 9.97244835e-01 9.91857708e-01 3.05454596e-04
 1.88314030e-03 3.08900177e-01 9.99545991e-01 9.99924660e-01
 9.95627999e-01 7.50466287e-01 9.97573912e-01 4.99580503e-02
 6.48628116e-01 9.99994636e-01 2.34064482e-05 1.51774706e-03
 2.07667891e-02 9.99209225e-01 9.99043643e-01 7.86051678e-05
 9.95331109e-01 1.24604499e-03 9.99948502e-01 4.35695469e-01
 9.99910712e-01 9.99996781e-01 6.59326553e-01 2.78160930e-01
 5.09255053e-03 3.77029297e-03 1.42465457e-02 1.77110964e-03
 9.97471333e-01 9.99893785e-01 6.14213757e-04 9.12327261e-04
 1.04634778e-03 5.38433306e-02 9.98557866e-01 9.99761283e-01
 9.99749124e-01 4.55366403e-01 2.34924868e-01 9.94951248e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 19:32:26, Dev, Step : 4508, Loss : 0.52190, Acc : 0.782, Auc : 0.860, Sensitive_Loss : 0.14445, Sensitive_Acc : 16.307, Sensitive_Auc : 0.992, Mean auc: 0.860, Run Time : 105.97 sec
INFO:root:2024-04-10 19:32:29, Train, Epoch : 8, Step : 4510, Loss : 0.05509, Acc : 0.172, Sensitive_Loss : 0.00672, Sensitive_Acc : 3.100, Run Time : 2.69 sec
INFO:root:2024-04-10 19:32:37, Train, Epoch : 8, Step : 4520, Loss : 0.32591, Acc : 0.875, Sensitive_Loss : 0.06721, Sensitive_Acc : 15.700, Run Time : 7.76 sec
INFO:root:2024-04-10 19:32:46, Train, Epoch : 8, Step : 4530, Loss : 0.32220, Acc : 0.875, Sensitive_Loss : 0.08297, Sensitive_Acc : 16.500, Run Time : 8.54 sec
INFO:root:2024-04-10 19:33:00, Train, Epoch : 8, Step : 4540, Loss : 0.36368, Acc : 0.831, Sensitive_Loss : 0.16821, Sensitive_Acc : 15.700, Run Time : 14.52 sec
INFO:root:2024-04-10 19:33:08, Train, Epoch : 8, Step : 4550, Loss : 0.30591, Acc : 0.866, Sensitive_Loss : 0.08131, Sensitive_Acc : 15.900, Run Time : 8.17 sec
INFO:root:2024-04-10 19:33:17, Train, Epoch : 8, Step : 4560, Loss : 0.30965, Acc : 0.850, Sensitive_Loss : 0.08328, Sensitive_Acc : 16.800, Run Time : 8.24 sec
INFO:root:2024-04-10 19:33:28, Train, Epoch : 8, Step : 4570, Loss : 0.29998, Acc : 0.866, Sensitive_Loss : 0.09915, Sensitive_Acc : 16.000, Run Time : 11.36 sec
INFO:root:2024-04-10 19:33:37, Train, Epoch : 8, Step : 4580, Loss : 0.31888, Acc : 0.856, Sensitive_Loss : 0.09067, Sensitive_Acc : 16.000, Run Time : 9.29 sec
INFO:root:2024-04-10 19:33:46, Train, Epoch : 8, Step : 4590, Loss : 0.29691, Acc : 0.897, Sensitive_Loss : 0.07223, Sensitive_Acc : 18.200, Run Time : 9.03 sec
INFO:root:2024-04-10 19:33:58, Train, Epoch : 8, Step : 4600, Loss : 0.31451, Acc : 0.856, Sensitive_Loss : 0.07240, Sensitive_Acc : 16.400, Run Time : 11.22 sec
INFO:root:2024-04-10 19:35:40, Dev, Step : 4600, Loss : 0.54013, Acc : 0.778, Auc : 0.858, Sensitive_Loss : 0.12719, Sensitive_Acc : 16.364, Sensitive_Auc : 0.992, Mean auc: 0.858, Run Time : 102.83 sec
INFO:root:2024-04-10 19:35:48, Train, Epoch : 8, Step : 4610, Loss : 0.32858, Acc : 0.881, Sensitive_Loss : 0.08248, Sensitive_Acc : 16.000, Run Time : 110.21 sec
INFO:root:2024-04-10 19:36:02, Train, Epoch : 8, Step : 4620, Loss : 0.30150, Acc : 0.831, Sensitive_Loss : 0.07564, Sensitive_Acc : 15.500, Run Time : 14.04 sec
INFO:root:2024-04-10 19:36:11, Train, Epoch : 8, Step : 4630, Loss : 0.30690, Acc : 0.853, Sensitive_Loss : 0.09019, Sensitive_Acc : 15.500, Run Time : 9.24 sec
INFO:root:2024-04-10 19:36:27, Train, Epoch : 8, Step : 4640, Loss : 0.38575, Acc : 0.838, Sensitive_Loss : 0.08475, Sensitive_Acc : 17.000, Run Time : 16.22 sec
INFO:root:2024-04-10 19:36:38, Train, Epoch : 8, Step : 4650, Loss : 0.33925, Acc : 0.844, Sensitive_Loss : 0.07066, Sensitive_Acc : 18.100, Run Time : 10.44 sec
INFO:root:2024-04-10 19:36:49, Train, Epoch : 8, Step : 4660, Loss : 0.33247, Acc : 0.847, Sensitive_Loss : 0.05242, Sensitive_Acc : 15.900, Run Time : 11.36 sec
INFO:root:2024-04-10 19:37:03, Train, Epoch : 8, Step : 4670, Loss : 0.30218, Acc : 0.894, Sensitive_Loss : 0.09084, Sensitive_Acc : 16.300, Run Time : 13.97 sec
INFO:root:2024-04-10 19:37:21, Train, Epoch : 8, Step : 4680, Loss : 0.25761, Acc : 0.884, Sensitive_Loss : 0.10745, Sensitive_Acc : 16.500, Run Time : 18.44 sec
INFO:root:2024-04-10 19:37:48, Train, Epoch : 8, Step : 4690, Loss : 0.36076, Acc : 0.822, Sensitive_Loss : 0.07128, Sensitive_Acc : 15.700, Run Time : 26.43 sec
INFO:root:2024-04-10 19:38:08, Train, Epoch : 8, Step : 4700, Loss : 0.41147, Acc : 0.825, Sensitive_Loss : 0.07700, Sensitive_Acc : 15.800, Run Time : 20.03 sec
INFO:root:2024-04-10 19:40:24, Dev, Step : 4700, Loss : 0.54293, Acc : 0.779, Auc : 0.858, Sensitive_Loss : 0.12874, Sensitive_Acc : 16.250, Sensitive_Auc : 0.992, Mean auc: 0.858, Run Time : 136.47 sec
INFO:root:2024-04-10 19:40:31, Train, Epoch : 8, Step : 4710, Loss : 0.33414, Acc : 0.859, Sensitive_Loss : 0.07613, Sensitive_Acc : 15.500, Run Time : 143.55 sec
INFO:root:2024-04-10 19:40:42, Train, Epoch : 8, Step : 4720, Loss : 0.29464, Acc : 0.887, Sensitive_Loss : 0.08987, Sensitive_Acc : 17.700, Run Time : 10.13 sec
INFO:root:2024-04-10 19:40:58, Train, Epoch : 8, Step : 4730, Loss : 0.32110, Acc : 0.841, Sensitive_Loss : 0.08537, Sensitive_Acc : 15.700, Run Time : 16.72 sec
INFO:root:2024-04-10 19:41:08, Train, Epoch : 8, Step : 4740, Loss : 0.32290, Acc : 0.881, Sensitive_Loss : 0.10373, Sensitive_Acc : 15.600, Run Time : 9.57 sec
INFO:root:2024-04-10 19:41:22, Train, Epoch : 8, Step : 4750, Loss : 0.28484, Acc : 0.878, Sensitive_Loss : 0.06970, Sensitive_Acc : 15.900, Run Time : 13.75 sec
INFO:root:2024-04-10 19:41:33, Train, Epoch : 8, Step : 4760, Loss : 0.31870, Acc : 0.866, Sensitive_Loss : 0.08330, Sensitive_Acc : 16.200, Run Time : 11.47 sec
INFO:root:2024-04-10 19:41:43, Train, Epoch : 8, Step : 4770, Loss : 0.27755, Acc : 0.881, Sensitive_Loss : 0.07644, Sensitive_Acc : 15.700, Run Time : 9.50 sec
INFO:root:2024-04-10 19:41:57, Train, Epoch : 8, Step : 4780, Loss : 0.30504, Acc : 0.891, Sensitive_Loss : 0.09053, Sensitive_Acc : 16.400, Run Time : 13.99 sec
INFO:root:2024-04-10 19:42:07, Train, Epoch : 8, Step : 4790, Loss : 0.30681, Acc : 0.844, Sensitive_Loss : 0.06885, Sensitive_Acc : 17.000, Run Time : 10.36 sec
INFO:root:2024-04-10 19:42:21, Train, Epoch : 8, Step : 4800, Loss : 0.32470, Acc : 0.859, Sensitive_Loss : 0.08148, Sensitive_Acc : 16.900, Run Time : 13.85 sec
INFO:root:2024-04-10 19:44:09, Dev, Step : 4800, Loss : 0.51790, Acc : 0.786, Auc : 0.861, Sensitive_Loss : 0.13409, Sensitive_Acc : 16.250, Sensitive_Auc : 0.992, Mean auc: 0.861, Run Time : 108.58 sec
INFO:root:2024-04-10 19:44:17, Train, Epoch : 8, Step : 4810, Loss : 0.36000, Acc : 0.809, Sensitive_Loss : 0.06506, Sensitive_Acc : 17.000, Run Time : 116.00 sec
INFO:root:2024-04-10 19:44:31, Train, Epoch : 8, Step : 4820, Loss : 0.32156, Acc : 0.847, Sensitive_Loss : 0.10608, Sensitive_Acc : 16.100, Run Time : 14.37 sec
INFO:root:2024-04-10 19:44:40, Train, Epoch : 8, Step : 4830, Loss : 0.33237, Acc : 0.859, Sensitive_Loss : 0.08475, Sensitive_Acc : 16.000, Run Time : 9.06 sec
INFO:root:2024-04-10 19:44:54, Train, Epoch : 8, Step : 4840, Loss : 0.28718, Acc : 0.859, Sensitive_Loss : 0.09106, Sensitive_Acc : 17.400, Run Time : 13.39 sec
INFO:root:2024-04-10 19:45:06, Train, Epoch : 8, Step : 4850, Loss : 0.35349, Acc : 0.841, Sensitive_Loss : 0.13284, Sensitive_Acc : 16.000, Run Time : 12.49 sec
INFO:root:2024-04-10 19:45:15, Train, Epoch : 8, Step : 4860, Loss : 0.23184, Acc : 0.887, Sensitive_Loss : 0.08553, Sensitive_Acc : 16.700, Run Time : 8.76 sec
INFO:root:2024-04-10 19:45:30, Train, Epoch : 8, Step : 4870, Loss : 0.32503, Acc : 0.863, Sensitive_Loss : 0.04567, Sensitive_Acc : 14.400, Run Time : 14.70 sec
INFO:root:2024-04-10 19:45:39, Train, Epoch : 8, Step : 4880, Loss : 0.29695, Acc : 0.866, Sensitive_Loss : 0.07068, Sensitive_Acc : 15.200, Run Time : 9.18 sec
INFO:root:2024-04-10 19:45:51, Train, Epoch : 8, Step : 4890, Loss : 0.28604, Acc : 0.863, Sensitive_Loss : 0.13738, Sensitive_Acc : 16.500, Run Time : 12.44 sec
INFO:root:2024-04-10 19:46:01, Train, Epoch : 8, Step : 4900, Loss : 0.32482, Acc : 0.872, Sensitive_Loss : 0.11075, Sensitive_Acc : 17.700, Run Time : 9.54 sec
INFO:root:2024-04-10 19:47:50, Dev, Step : 4900, Loss : 0.53946, Acc : 0.775, Auc : 0.856, Sensitive_Loss : 0.16212, Sensitive_Acc : 16.279, Sensitive_Auc : 0.993, Mean auc: 0.856, Run Time : 109.70 sec
INFO:root:2024-04-10 19:47:58, Train, Epoch : 8, Step : 4910, Loss : 0.26921, Acc : 0.866, Sensitive_Loss : 0.08857, Sensitive_Acc : 16.100, Run Time : 117.04 sec
INFO:root:2024-04-10 19:48:08, Train, Epoch : 8, Step : 4920, Loss : 0.35843, Acc : 0.863, Sensitive_Loss : 0.11970, Sensitive_Acc : 14.200, Run Time : 10.39 sec
INFO:root:2024-04-10 19:48:21, Train, Epoch : 8, Step : 4930, Loss : 0.25358, Acc : 0.875, Sensitive_Loss : 0.08295, Sensitive_Acc : 16.100, Run Time : 12.66 sec
INFO:root:2024-04-10 19:48:31, Train, Epoch : 8, Step : 4940, Loss : 0.26816, Acc : 0.891, Sensitive_Loss : 0.12780, Sensitive_Acc : 14.600, Run Time : 10.26 sec
INFO:root:2024-04-10 19:48:41, Train, Epoch : 8, Step : 4950, Loss : 0.32895, Acc : 0.822, Sensitive_Loss : 0.10206, Sensitive_Acc : 14.100, Run Time : 10.10 sec
INFO:root:2024-04-10 19:48:55, Train, Epoch : 8, Step : 4960, Loss : 0.34065, Acc : 0.869, Sensitive_Loss : 0.09649, Sensitive_Acc : 16.700, Run Time : 13.84 sec
INFO:root:2024-04-10 19:49:06, Train, Epoch : 8, Step : 4970, Loss : 0.30605, Acc : 0.866, Sensitive_Loss : 0.08962, Sensitive_Acc : 15.400, Run Time : 10.55 sec
INFO:root:2024-04-10 19:49:19, Train, Epoch : 8, Step : 4980, Loss : 0.36928, Acc : 0.859, Sensitive_Loss : 0.09542, Sensitive_Acc : 19.600, Run Time : 12.96 sec
INFO:root:2024-04-10 19:49:29, Train, Epoch : 8, Step : 4990, Loss : 0.28730, Acc : 0.872, Sensitive_Loss : 0.06699, Sensitive_Acc : 16.600, Run Time : 10.47 sec
INFO:root:2024-04-10 19:49:41, Train, Epoch : 8, Step : 5000, Loss : 0.35428, Acc : 0.856, Sensitive_Loss : 0.09855, Sensitive_Acc : 14.800, Run Time : 12.39 sec
INFO:root:2024-04-10 19:51:32, Dev, Step : 5000, Loss : 0.53643, Acc : 0.784, Auc : 0.858, Sensitive_Loss : 0.11460, Sensitive_Acc : 16.264, Sensitive_Auc : 0.991, Mean auc: 0.858, Run Time : 110.92 sec
INFO:root:2024-04-10 19:51:39, Train, Epoch : 8, Step : 5010, Loss : 0.30612, Acc : 0.853, Sensitive_Loss : 0.09903, Sensitive_Acc : 17.500, Run Time : 117.92 sec
INFO:root:2024-04-10 19:51:56, Train, Epoch : 8, Step : 5020, Loss : 0.36355, Acc : 0.838, Sensitive_Loss : 0.08193, Sensitive_Acc : 17.100, Run Time : 16.99 sec
INFO:root:2024-04-10 19:52:06, Train, Epoch : 8, Step : 5030, Loss : 0.34714, Acc : 0.856, Sensitive_Loss : 0.09053, Sensitive_Acc : 17.400, Run Time : 9.59 sec
INFO:root:2024-04-10 19:52:18, Train, Epoch : 8, Step : 5040, Loss : 0.40682, Acc : 0.816, Sensitive_Loss : 0.11950, Sensitive_Acc : 16.800, Run Time : 12.48 sec
INFO:root:2024-04-10 19:52:31, Train, Epoch : 8, Step : 5050, Loss : 0.33477, Acc : 0.853, Sensitive_Loss : 0.11352, Sensitive_Acc : 17.500, Run Time : 12.30 sec
INFO:root:2024-04-10 19:52:41, Train, Epoch : 8, Step : 5060, Loss : 0.34499, Acc : 0.850, Sensitive_Loss : 0.06987, Sensitive_Acc : 15.900, Run Time : 10.57 sec
INFO:root:2024-04-10 19:52:55, Train, Epoch : 8, Step : 5070, Loss : 0.34420, Acc : 0.847, Sensitive_Loss : 0.08788, Sensitive_Acc : 16.000, Run Time : 13.93 sec
INFO:root:2024-04-10 19:53:06, Train, Epoch : 8, Step : 5080, Loss : 0.28258, Acc : 0.875, Sensitive_Loss : 0.08991, Sensitive_Acc : 18.000, Run Time : 10.85 sec
INFO:root:2024-04-10 19:53:18, Train, Epoch : 8, Step : 5090, Loss : 0.33378, Acc : 0.866, Sensitive_Loss : 0.12482, Sensitive_Acc : 16.500, Run Time : 12.28 sec
INFO:root:2024-04-10 19:53:28, Train, Epoch : 8, Step : 5100, Loss : 0.33507, Acc : 0.853, Sensitive_Loss : 0.08734, Sensitive_Acc : 16.400, Run Time : 9.31 sec
INFO:root:2024-04-10 19:55:23, Dev, Step : 5100, Loss : 0.52836, Acc : 0.785, Auc : 0.860, Sensitive_Loss : 0.12310, Sensitive_Acc : 16.364, Sensitive_Auc : 0.991, Mean auc: 0.860, Run Time : 115.36 sec
INFO:root:2024-04-10 19:55:30, Train, Epoch : 8, Step : 5110, Loss : 0.39826, Acc : 0.822, Sensitive_Loss : 0.09995, Sensitive_Acc : 17.900, Run Time : 122.22 sec
INFO:root:2024-04-10 19:55:39, Train, Epoch : 8, Step : 5120, Loss : 0.33424, Acc : 0.872, Sensitive_Loss : 0.08679, Sensitive_Acc : 18.900, Run Time : 9.03 sec
INFO:root:2024-04-10 19:55:55, Train, Epoch : 8, Step : 5130, Loss : 0.35345, Acc : 0.847, Sensitive_Loss : 0.14326, Sensitive_Acc : 15.400, Run Time : 16.55 sec
INFO:root:2024-04-10 19:56:05, Train, Epoch : 8, Step : 5140, Loss : 0.24503, Acc : 0.887, Sensitive_Loss : 0.13442, Sensitive_Acc : 16.000, Run Time : 9.79 sec
INFO:root:2024-04-10 19:56:16, Train, Epoch : 8, Step : 5150, Loss : 0.28184, Acc : 0.863, Sensitive_Loss : 0.07738, Sensitive_Acc : 14.800, Run Time : 10.62 sec
INFO:root:2024-04-10 19:58:03
INFO:root:y_pred: [0.463144   0.01336585 0.9439106  ... 0.54672235 0.47429678 0.14726302]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.99854684e-01 1.59681626e-02 6.76630497e-01 9.99996781e-01
 9.98303056e-01 9.98899937e-01 9.99915242e-01 4.97761812e-06
 9.90252376e-01 9.98916030e-01 1.17559873e-01 5.62020183e-01
 2.43058239e-05 9.56369519e-01 9.99997139e-01 9.99964952e-01
 9.94050086e-01 9.96305943e-01 9.98368442e-01 9.99338329e-01
 9.97355819e-01 5.00586450e-01 9.99940276e-01 7.47124910e-01
 9.79890943e-01 3.96885164e-02 9.99686122e-01 1.26101095e-02
 9.99981403e-01 6.39993558e-03 3.34885227e-03 6.41782761e-01
 1.69822201e-02 9.96474087e-01 1.66300288e-05 9.99963641e-01
 5.82581961e-05 9.99991775e-01 4.40199152e-02 9.82092857e-01
 9.99962687e-01 3.10971402e-04 7.67696649e-02 3.15776619e-04
 2.84529239e-01 7.74052739e-01 9.99984622e-01 9.98216450e-01
 9.98780549e-01 9.99571621e-01 2.87154224e-03 9.79084611e-01
 1.34367740e-03 3.93351495e-01 9.99948025e-01 4.13033456e-01
 9.95215654e-01 9.99894023e-01 9.98939931e-01 1.33427316e-02
 5.93593437e-03 9.99797761e-01 1.64365754e-01 9.99929547e-01
 9.90123451e-01 3.52921486e-02 9.98791039e-01 9.64807749e-01
 9.99997973e-01 9.99903202e-01 2.16925647e-02 3.77929300e-01
 8.94923449e-01 9.99902487e-01 9.94150937e-01 1.44360401e-03
 8.00768971e-01 3.72622651e-03 9.38350713e-05 9.95513022e-01
 3.13953422e-02 9.99992490e-01 9.99867201e-01 9.99958992e-01
 3.85477215e-01 9.99999523e-01 2.63116563e-05 1.97340990e-03
 9.90262866e-01 9.99221206e-01 7.56916357e-03 7.94071734e-01
 1.54792862e-02 9.57305729e-01 1.79454498e-02 9.99971867e-01
 1.43742270e-03 9.97711897e-01 9.96434450e-01 9.20038074e-05
 1.63220777e-03 5.02852738e-01 9.99613941e-01 9.99917865e-01
 9.97615457e-01 7.86009669e-01 9.97685790e-01 7.68925250e-02
 5.48435211e-01 9.99994159e-01 1.08944823e-05 1.17526099e-03
 3.30002867e-02 9.99669671e-01 9.99493718e-01 1.69481995e-04
 9.93514657e-01 2.20831623e-03 9.99945641e-01 5.56453824e-01
 9.99960184e-01 9.99988198e-01 8.32955420e-01 2.54386842e-01
 1.03066908e-02 1.59271446e-03 2.06925794e-02 7.09002488e-04
 9.97746408e-01 9.99838948e-01 5.37012937e-04 1.25097542e-03
 1.16553879e-03 6.01459593e-02 9.99069631e-01 9.99847651e-01
 9.99681473e-01 6.38802826e-01 2.83535391e-01 9.97294009e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 19:58:03, Dev, Step : 5152, Loss : 0.58305, Acc : 0.766, Auc : 0.857, Sensitive_Loss : 0.15385, Sensitive_Acc : 16.407, Sensitive_Auc : 0.992, Mean auc: 0.857, Run Time : 104.17 sec
INFO:root:2024-04-10 19:58:11, Train, Epoch : 9, Step : 5160, Loss : 0.22834, Acc : 0.706, Sensitive_Loss : 0.09718, Sensitive_Acc : 12.700, Run Time : 7.10 sec
INFO:root:2024-04-10 19:58:22, Train, Epoch : 9, Step : 5170, Loss : 0.31399, Acc : 0.866, Sensitive_Loss : 0.10861, Sensitive_Acc : 16.700, Run Time : 11.22 sec
INFO:root:2024-04-10 19:58:30, Train, Epoch : 9, Step : 5180, Loss : 0.30038, Acc : 0.875, Sensitive_Loss : 0.07982, Sensitive_Acc : 16.100, Run Time : 7.74 sec
INFO:root:2024-04-10 19:58:37, Train, Epoch : 9, Step : 5190, Loss : 0.31275, Acc : 0.847, Sensitive_Loss : 0.10467, Sensitive_Acc : 16.400, Run Time : 7.23 sec
INFO:root:2024-04-10 19:58:47, Train, Epoch : 9, Step : 5200, Loss : 0.27943, Acc : 0.878, Sensitive_Loss : 0.09193, Sensitive_Acc : 16.900, Run Time : 9.94 sec
INFO:root:2024-04-10 20:00:37, Dev, Step : 5200, Loss : 0.58044, Acc : 0.769, Auc : 0.858, Sensitive_Loss : 0.13923, Sensitive_Acc : 16.421, Sensitive_Auc : 0.992, Mean auc: 0.858, Run Time : 110.49 sec
INFO:root:2024-04-10 20:00:43, Train, Epoch : 9, Step : 5210, Loss : 0.30481, Acc : 0.875, Sensitive_Loss : 0.06178, Sensitive_Acc : 16.900, Run Time : 116.19 sec
INFO:root:2024-04-10 20:00:57, Train, Epoch : 9, Step : 5220, Loss : 0.32434, Acc : 0.869, Sensitive_Loss : 0.08919, Sensitive_Acc : 17.000, Run Time : 13.62 sec
INFO:root:2024-04-10 20:01:04, Train, Epoch : 9, Step : 5230, Loss : 0.30300, Acc : 0.850, Sensitive_Loss : 0.07536, Sensitive_Acc : 15.400, Run Time : 7.30 sec
INFO:root:2024-04-10 20:01:12, Train, Epoch : 9, Step : 5240, Loss : 0.32965, Acc : 0.834, Sensitive_Loss : 0.07740, Sensitive_Acc : 15.500, Run Time : 7.78 sec
INFO:root:2024-04-10 20:01:22, Train, Epoch : 9, Step : 5250, Loss : 0.28331, Acc : 0.875, Sensitive_Loss : 0.06234, Sensitive_Acc : 16.800, Run Time : 10.37 sec
INFO:root:2024-04-10 20:01:29, Train, Epoch : 9, Step : 5260, Loss : 0.28492, Acc : 0.875, Sensitive_Loss : 0.08066, Sensitive_Acc : 16.200, Run Time : 7.48 sec
INFO:root:2024-04-10 20:01:37, Train, Epoch : 9, Step : 5270, Loss : 0.33977, Acc : 0.881, Sensitive_Loss : 0.06180, Sensitive_Acc : 14.800, Run Time : 7.42 sec
INFO:root:2024-04-10 20:01:45, Train, Epoch : 9, Step : 5280, Loss : 0.26374, Acc : 0.869, Sensitive_Loss : 0.06471, Sensitive_Acc : 17.900, Run Time : 8.42 sec
INFO:root:2024-04-10 20:01:55, Train, Epoch : 9, Step : 5290, Loss : 0.27856, Acc : 0.863, Sensitive_Loss : 0.08463, Sensitive_Acc : 17.000, Run Time : 9.64 sec
INFO:root:2024-04-10 20:02:03, Train, Epoch : 9, Step : 5300, Loss : 0.29513, Acc : 0.878, Sensitive_Loss : 0.06738, Sensitive_Acc : 17.200, Run Time : 7.67 sec
INFO:root:2024-04-10 20:03:50, Dev, Step : 5300, Loss : 0.54002, Acc : 0.782, Auc : 0.857, Sensitive_Loss : 0.12627, Sensitive_Acc : 16.421, Sensitive_Auc : 0.993, Mean auc: 0.857, Run Time : 107.47 sec
INFO:root:2024-04-10 20:03:56, Train, Epoch : 9, Step : 5310, Loss : 0.27732, Acc : 0.853, Sensitive_Loss : 0.05947, Sensitive_Acc : 16.500, Run Time : 113.59 sec
INFO:root:2024-04-10 20:04:05, Train, Epoch : 9, Step : 5320, Loss : 0.27721, Acc : 0.856, Sensitive_Loss : 0.10713, Sensitive_Acc : 17.800, Run Time : 8.77 sec
INFO:root:2024-04-10 20:04:14, Train, Epoch : 9, Step : 5330, Loss : 0.27223, Acc : 0.894, Sensitive_Loss : 0.12210, Sensitive_Acc : 17.300, Run Time : 8.91 sec
INFO:root:2024-04-10 20:04:28, Train, Epoch : 9, Step : 5340, Loss : 0.30685, Acc : 0.866, Sensitive_Loss : 0.11592, Sensitive_Acc : 15.800, Run Time : 14.08 sec
INFO:root:2024-04-10 20:04:35, Train, Epoch : 9, Step : 5350, Loss : 0.23580, Acc : 0.872, Sensitive_Loss : 0.09911, Sensitive_Acc : 16.300, Run Time : 7.34 sec
INFO:root:2024-04-10 20:04:44, Train, Epoch : 9, Step : 5360, Loss : 0.27605, Acc : 0.900, Sensitive_Loss : 0.06879, Sensitive_Acc : 18.100, Run Time : 8.51 sec
INFO:root:2024-04-10 20:04:54, Train, Epoch : 9, Step : 5370, Loss : 0.32061, Acc : 0.853, Sensitive_Loss : 0.12661, Sensitive_Acc : 16.600, Run Time : 10.50 sec
INFO:root:2024-04-10 20:05:03, Train, Epoch : 9, Step : 5380, Loss : 0.28378, Acc : 0.872, Sensitive_Loss : 0.09969, Sensitive_Acc : 15.300, Run Time : 8.32 sec
INFO:root:2024-04-10 20:05:11, Train, Epoch : 9, Step : 5390, Loss : 0.30806, Acc : 0.859, Sensitive_Loss : 0.09539, Sensitive_Acc : 15.300, Run Time : 7.96 sec
INFO:root:2024-04-10 20:05:21, Train, Epoch : 9, Step : 5400, Loss : 0.27532, Acc : 0.878, Sensitive_Loss : 0.10641, Sensitive_Acc : 17.300, Run Time : 10.81 sec
INFO:root:2024-04-10 20:07:09, Dev, Step : 5400, Loss : 0.53828, Acc : 0.781, Auc : 0.861, Sensitive_Loss : 0.13195, Sensitive_Acc : 16.321, Sensitive_Auc : 0.992, Mean auc: 0.861, Run Time : 107.70 sec
INFO:root:2024-04-10 20:07:18, Train, Epoch : 9, Step : 5410, Loss : 0.31066, Acc : 0.881, Sensitive_Loss : 0.08807, Sensitive_Acc : 15.800, Run Time : 116.93 sec
INFO:root:2024-04-10 20:07:30, Train, Epoch : 9, Step : 5420, Loss : 0.31031, Acc : 0.863, Sensitive_Loss : 0.08141, Sensitive_Acc : 16.600, Run Time : 11.72 sec
INFO:root:2024-04-10 20:07:41, Train, Epoch : 9, Step : 5430, Loss : 0.25075, Acc : 0.856, Sensitive_Loss : 0.07932, Sensitive_Acc : 16.500, Run Time : 10.72 sec
INFO:root:2024-04-10 20:07:53, Train, Epoch : 9, Step : 5440, Loss : 0.28416, Acc : 0.869, Sensitive_Loss : 0.08381, Sensitive_Acc : 14.600, Run Time : 12.55 sec
INFO:root:2024-04-10 20:08:02, Train, Epoch : 9, Step : 5450, Loss : 0.27086, Acc : 0.872, Sensitive_Loss : 0.07771, Sensitive_Acc : 15.500, Run Time : 8.51 sec
INFO:root:2024-04-10 20:08:10, Train, Epoch : 9, Step : 5460, Loss : 0.34404, Acc : 0.841, Sensitive_Loss : 0.09034, Sensitive_Acc : 16.600, Run Time : 8.00 sec
INFO:root:2024-04-10 20:08:20, Train, Epoch : 9, Step : 5470, Loss : 0.26363, Acc : 0.875, Sensitive_Loss : 0.08793, Sensitive_Acc : 15.100, Run Time : 10.45 sec
INFO:root:2024-04-10 20:08:32, Train, Epoch : 9, Step : 5480, Loss : 0.35961, Acc : 0.856, Sensitive_Loss : 0.08978, Sensitive_Acc : 16.000, Run Time : 11.55 sec
INFO:root:2024-04-10 20:08:40, Train, Epoch : 9, Step : 5490, Loss : 0.28660, Acc : 0.853, Sensitive_Loss : 0.08689, Sensitive_Acc : 15.400, Run Time : 8.31 sec
INFO:root:2024-04-10 20:08:50, Train, Epoch : 9, Step : 5500, Loss : 0.32112, Acc : 0.881, Sensitive_Loss : 0.12228, Sensitive_Acc : 16.200, Run Time : 10.06 sec
INFO:root:2024-04-10 20:10:28, Dev, Step : 5500, Loss : 0.53923, Acc : 0.778, Auc : 0.856, Sensitive_Loss : 0.11489, Sensitive_Acc : 16.379, Sensitive_Auc : 0.993, Mean auc: 0.856, Run Time : 97.11 sec
INFO:root:2024-04-10 20:10:34, Train, Epoch : 9, Step : 5510, Loss : 0.30136, Acc : 0.853, Sensitive_Loss : 0.07828, Sensitive_Acc : 15.400, Run Time : 103.84 sec
INFO:root:2024-04-10 20:10:43, Train, Epoch : 9, Step : 5520, Loss : 0.25057, Acc : 0.859, Sensitive_Loss : 0.08650, Sensitive_Acc : 15.400, Run Time : 8.55 sec
INFO:root:2024-04-10 20:10:55, Train, Epoch : 9, Step : 5530, Loss : 0.30449, Acc : 0.859, Sensitive_Loss : 0.07686, Sensitive_Acc : 15.700, Run Time : 12.04 sec
INFO:root:2024-04-10 20:11:03, Train, Epoch : 9, Step : 5540, Loss : 0.31704, Acc : 0.866, Sensitive_Loss : 0.08862, Sensitive_Acc : 15.100, Run Time : 8.27 sec
INFO:root:2024-04-10 20:11:12, Train, Epoch : 9, Step : 5550, Loss : 0.26530, Acc : 0.887, Sensitive_Loss : 0.10096, Sensitive_Acc : 15.400, Run Time : 8.60 sec
INFO:root:2024-04-10 20:11:24, Train, Epoch : 9, Step : 5560, Loss : 0.32239, Acc : 0.863, Sensitive_Loss : 0.10038, Sensitive_Acc : 15.800, Run Time : 11.79 sec
INFO:root:2024-04-10 20:11:31, Train, Epoch : 9, Step : 5570, Loss : 0.33122, Acc : 0.866, Sensitive_Loss : 0.07033, Sensitive_Acc : 18.100, Run Time : 7.91 sec
INFO:root:2024-04-10 20:11:39, Train, Epoch : 9, Step : 5580, Loss : 0.31011, Acc : 0.856, Sensitive_Loss : 0.06911, Sensitive_Acc : 16.000, Run Time : 8.03 sec
INFO:root:2024-04-10 20:11:49, Train, Epoch : 9, Step : 5590, Loss : 0.26021, Acc : 0.884, Sensitive_Loss : 0.08977, Sensitive_Acc : 16.800, Run Time : 9.88 sec
INFO:root:2024-04-10 20:12:00, Train, Epoch : 9, Step : 5600, Loss : 0.28877, Acc : 0.887, Sensitive_Loss : 0.07999, Sensitive_Acc : 15.700, Run Time : 10.55 sec
INFO:root:2024-04-10 20:13:36, Dev, Step : 5600, Loss : 0.54336, Acc : 0.776, Auc : 0.856, Sensitive_Loss : 0.13053, Sensitive_Acc : 16.393, Sensitive_Auc : 0.991, Mean auc: 0.856, Run Time : 96.39 sec
INFO:root:2024-04-10 20:13:43, Train, Epoch : 9, Step : 5610, Loss : 0.34016, Acc : 0.831, Sensitive_Loss : 0.08403, Sensitive_Acc : 15.100, Run Time : 103.44 sec
INFO:root:2024-04-10 20:13:57, Train, Epoch : 9, Step : 5620, Loss : 0.35374, Acc : 0.831, Sensitive_Loss : 0.05398, Sensitive_Acc : 16.300, Run Time : 13.33 sec
INFO:root:2024-04-10 20:14:06, Train, Epoch : 9, Step : 5630, Loss : 0.33609, Acc : 0.866, Sensitive_Loss : 0.08509, Sensitive_Acc : 17.200, Run Time : 9.31 sec
INFO:root:2024-04-10 20:14:15, Train, Epoch : 9, Step : 5640, Loss : 0.30613, Acc : 0.872, Sensitive_Loss : 0.09124, Sensitive_Acc : 15.000, Run Time : 8.54 sec
INFO:root:2024-04-10 20:14:30, Train, Epoch : 9, Step : 5650, Loss : 0.30098, Acc : 0.878, Sensitive_Loss : 0.07205, Sensitive_Acc : 15.300, Run Time : 15.25 sec
INFO:root:2024-04-10 20:14:40, Train, Epoch : 9, Step : 5660, Loss : 0.37619, Acc : 0.850, Sensitive_Loss : 0.12165, Sensitive_Acc : 17.200, Run Time : 9.79 sec
INFO:root:2024-04-10 20:14:48, Train, Epoch : 9, Step : 5670, Loss : 0.25966, Acc : 0.900, Sensitive_Loss : 0.10831, Sensitive_Acc : 16.600, Run Time : 8.39 sec
INFO:root:2024-04-10 20:14:56, Train, Epoch : 9, Step : 5680, Loss : 0.31936, Acc : 0.859, Sensitive_Loss : 0.08943, Sensitive_Acc : 15.900, Run Time : 8.30 sec
INFO:root:2024-04-10 20:15:08, Train, Epoch : 9, Step : 5690, Loss : 0.28013, Acc : 0.881, Sensitive_Loss : 0.11327, Sensitive_Acc : 16.200, Run Time : 11.87 sec
INFO:root:2024-04-10 20:15:17, Train, Epoch : 9, Step : 5700, Loss : 0.33899, Acc : 0.866, Sensitive_Loss : 0.08526, Sensitive_Acc : 16.700, Run Time : 8.79 sec
INFO:root:2024-04-10 20:16:56, Dev, Step : 5700, Loss : 0.54265, Acc : 0.779, Auc : 0.858, Sensitive_Loss : 0.12944, Sensitive_Acc : 16.321, Sensitive_Auc : 0.990, Mean auc: 0.858, Run Time : 99.04 sec
INFO:root:2024-04-10 20:17:03, Train, Epoch : 9, Step : 5710, Loss : 0.33676, Acc : 0.881, Sensitive_Loss : 0.06424, Sensitive_Acc : 16.800, Run Time : 105.93 sec
INFO:root:2024-04-10 20:17:13, Train, Epoch : 9, Step : 5720, Loss : 0.32398, Acc : 0.831, Sensitive_Loss : 0.07897, Sensitive_Acc : 16.500, Run Time : 10.48 sec
INFO:root:2024-04-10 20:17:23, Train, Epoch : 9, Step : 5730, Loss : 0.29781, Acc : 0.875, Sensitive_Loss : 0.11074, Sensitive_Acc : 17.000, Run Time : 9.49 sec
INFO:root:2024-04-10 20:17:32, Train, Epoch : 9, Step : 5740, Loss : 0.28436, Acc : 0.894, Sensitive_Loss : 0.09582, Sensitive_Acc : 17.800, Run Time : 9.04 sec
INFO:root:2024-04-10 20:17:42, Train, Epoch : 9, Step : 5750, Loss : 0.34739, Acc : 0.850, Sensitive_Loss : 0.06005, Sensitive_Acc : 17.900, Run Time : 10.05 sec
INFO:root:2024-04-10 20:17:54, Train, Epoch : 9, Step : 5760, Loss : 0.27283, Acc : 0.878, Sensitive_Loss : 0.07609, Sensitive_Acc : 14.600, Run Time : 11.83 sec
INFO:root:2024-04-10 20:18:02, Train, Epoch : 9, Step : 5770, Loss : 0.27166, Acc : 0.897, Sensitive_Loss : 0.07509, Sensitive_Acc : 17.400, Run Time : 8.68 sec
INFO:root:2024-04-10 20:18:13, Train, Epoch : 9, Step : 5780, Loss : 0.23761, Acc : 0.916, Sensitive_Loss : 0.06691, Sensitive_Acc : 16.000, Run Time : 10.27 sec
INFO:root:2024-04-10 20:18:22, Train, Epoch : 9, Step : 5790, Loss : 0.29609, Acc : 0.863, Sensitive_Loss : 0.08878, Sensitive_Acc : 16.300, Run Time : 8.97 sec
INFO:root:2024-04-10 20:20:05
INFO:root:y_pred: [0.551912   0.01893029 0.9614772  ... 0.6672458  0.7135059  0.07627267]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9977356e-01 1.0949152e-02 7.1779013e-01 9.9999022e-01 9.9518025e-01
 9.9797863e-01 9.9985635e-01 8.2735369e-06 9.9360037e-01 9.9821019e-01
 7.2222918e-02 3.8410488e-01 5.3315462e-06 9.0125990e-01 9.9999928e-01
 9.9994051e-01 9.8512298e-01 9.9235690e-01 9.9695349e-01 9.9876308e-01
 9.9613839e-01 3.5577536e-01 9.9986279e-01 6.5348107e-01 9.5097136e-01
 1.0376585e-02 9.9955398e-01 3.0833946e-03 9.9996662e-01 3.8839509e-03
 2.3248761e-03 4.4947916e-01 1.0323602e-02 9.9747235e-01 2.2799257e-05
 9.9983191e-01 3.9733924e-05 9.9998319e-01 1.9053912e-02 9.4551551e-01
 9.9980682e-01 2.3543014e-04 2.6588602e-02 2.6491442e-04 2.0800383e-01
 5.3503120e-01 9.9998033e-01 9.9747521e-01 9.9680459e-01 9.9947435e-01
 1.2389802e-03 9.6660334e-01 6.6510995e-04 2.5976059e-01 9.9998653e-01
 2.8695530e-01 9.9335361e-01 9.9971491e-01 9.9858057e-01 5.2292831e-03
 6.2796017e-03 9.9965048e-01 1.1791513e-01 9.9988651e-01 9.7805196e-01
 2.0381613e-02 9.9831897e-01 9.4581330e-01 9.9999654e-01 9.9996424e-01
 1.3947373e-02 1.5344164e-01 8.5676253e-01 9.9992287e-01 9.9175745e-01
 9.6291088e-04 8.1584263e-01 1.0858497e-03 2.6185746e-05 9.9916458e-01
 1.4292537e-02 9.9998534e-01 9.9962759e-01 9.9989343e-01 3.4870544e-01
 9.9999952e-01 1.6929032e-05 1.6861879e-03 9.8492241e-01 9.9826813e-01
 4.4632326e-03 4.6862125e-01 7.1254759e-03 7.6210642e-01 1.1214346e-02
 9.9997234e-01 7.1306241e-04 9.9612302e-01 9.9322647e-01 1.1263808e-04
 1.0775591e-03 2.3625287e-01 9.9971944e-01 9.9991500e-01 9.8983896e-01
 8.3584255e-01 9.8490310e-01 4.9715027e-02 5.1615632e-01 9.9998903e-01
 1.6720338e-05 8.0631295e-04 1.2093335e-02 9.9939322e-01 9.9917370e-01
 7.1982962e-05 9.9312466e-01 6.9773779e-04 9.9991214e-01 2.3933282e-01
 9.9995387e-01 9.9998915e-01 7.7907330e-01 3.0701685e-01 9.7844582e-03
 9.5260056e-04 7.5532934e-03 6.9158175e-04 9.9648052e-01 9.9983871e-01
 2.2500943e-04 6.5699150e-04 4.9915613e-04 3.8600516e-02 9.9786621e-01
 9.9923205e-01 9.9985802e-01 3.9589167e-01 1.4367278e-01 9.9532241e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 20:20:05, Dev, Step : 5796, Loss : 0.55146, Acc : 0.777, Auc : 0.855, Sensitive_Loss : 0.12685, Sensitive_Acc : 16.293, Sensitive_Auc : 0.990, Mean auc: 0.855, Run Time : 98.46 sec
INFO:root:2024-04-10 20:20:10, Train, Epoch : 10, Step : 5800, Loss : 0.09075, Acc : 0.341, Sensitive_Loss : 0.05446, Sensitive_Acc : 6.900, Run Time : 3.97 sec
INFO:root:2024-04-10 20:21:54, Dev, Step : 5800, Loss : 0.55780, Acc : 0.777, Auc : 0.856, Sensitive_Loss : 0.13003, Sensitive_Acc : 16.293, Sensitive_Auc : 0.990, Mean auc: 0.856, Run Time : 104.34 sec
INFO:root:2024-04-10 20:22:03, Train, Epoch : 10, Step : 5810, Loss : 0.27056, Acc : 0.878, Sensitive_Loss : 0.08784, Sensitive_Acc : 16.300, Run Time : 112.64 sec
INFO:root:2024-04-10 20:22:13, Train, Epoch : 10, Step : 5820, Loss : 0.27468, Acc : 0.891, Sensitive_Loss : 0.06561, Sensitive_Acc : 17.200, Run Time : 10.19 sec
INFO:root:2024-04-10 20:22:23, Train, Epoch : 10, Step : 5830, Loss : 0.24879, Acc : 0.878, Sensitive_Loss : 0.07833, Sensitive_Acc : 16.200, Run Time : 10.12 sec
INFO:root:2024-04-10 20:22:35, Train, Epoch : 10, Step : 5840, Loss : 0.24849, Acc : 0.900, Sensitive_Loss : 0.10382, Sensitive_Acc : 14.600, Run Time : 12.31 sec
INFO:root:2024-04-10 20:22:46, Train, Epoch : 10, Step : 5850, Loss : 0.24497, Acc : 0.887, Sensitive_Loss : 0.08516, Sensitive_Acc : 15.900, Run Time : 10.59 sec
INFO:root:2024-04-10 20:22:55, Train, Epoch : 10, Step : 5860, Loss : 0.19292, Acc : 0.884, Sensitive_Loss : 0.09003, Sensitive_Acc : 16.600, Run Time : 8.76 sec
INFO:root:2024-04-10 20:23:04, Train, Epoch : 10, Step : 5870, Loss : 0.28538, Acc : 0.853, Sensitive_Loss : 0.07232, Sensitive_Acc : 16.300, Run Time : 9.22 sec
INFO:root:2024-04-10 20:23:14, Train, Epoch : 10, Step : 5880, Loss : 0.24509, Acc : 0.903, Sensitive_Loss : 0.08153, Sensitive_Acc : 16.000, Run Time : 9.74 sec
INFO:root:2024-04-10 20:23:23, Train, Epoch : 10, Step : 5890, Loss : 0.27122, Acc : 0.900, Sensitive_Loss : 0.09454, Sensitive_Acc : 16.600, Run Time : 9.78 sec
INFO:root:2024-04-10 20:23:34, Train, Epoch : 10, Step : 5900, Loss : 0.33405, Acc : 0.844, Sensitive_Loss : 0.06780, Sensitive_Acc : 15.500, Run Time : 10.40 sec
INFO:root:2024-04-10 20:25:09, Dev, Step : 5900, Loss : 0.58569, Acc : 0.765, Auc : 0.856, Sensitive_Loss : 0.13345, Sensitive_Acc : 16.293, Sensitive_Auc : 0.990, Mean auc: 0.856, Run Time : 95.02 sec
INFO:root:2024-04-10 20:25:15, Train, Epoch : 10, Step : 5910, Loss : 0.23311, Acc : 0.894, Sensitive_Loss : 0.08402, Sensitive_Acc : 14.900, Run Time : 101.24 sec
INFO:root:2024-04-10 20:25:24, Train, Epoch : 10, Step : 5920, Loss : 0.26109, Acc : 0.900, Sensitive_Loss : 0.09955, Sensitive_Acc : 17.100, Run Time : 8.59 sec
INFO:root:2024-04-10 20:25:35, Train, Epoch : 10, Step : 5930, Loss : 0.24973, Acc : 0.881, Sensitive_Loss : 0.08133, Sensitive_Acc : 16.900, Run Time : 11.28 sec
INFO:root:2024-04-10 20:25:46, Train, Epoch : 10, Step : 5940, Loss : 0.28805, Acc : 0.869, Sensitive_Loss : 0.07001, Sensitive_Acc : 16.100, Run Time : 10.81 sec
INFO:root:2024-04-10 20:25:54, Train, Epoch : 10, Step : 5950, Loss : 0.26389, Acc : 0.897, Sensitive_Loss : 0.07504, Sensitive_Acc : 17.400, Run Time : 8.75 sec
INFO:root:2024-04-10 20:26:05, Train, Epoch : 10, Step : 5960, Loss : 0.30744, Acc : 0.875, Sensitive_Loss : 0.11961, Sensitive_Acc : 16.700, Run Time : 10.08 sec
INFO:root:2024-04-10 20:26:17, Train, Epoch : 10, Step : 5970, Loss : 0.26790, Acc : 0.866, Sensitive_Loss : 0.11174, Sensitive_Acc : 14.500, Run Time : 12.85 sec
INFO:root:2024-04-10 20:26:25, Train, Epoch : 10, Step : 5980, Loss : 0.25335, Acc : 0.894, Sensitive_Loss : 0.10758, Sensitive_Acc : 17.500, Run Time : 7.95 sec
INFO:root:2024-04-10 20:26:35, Train, Epoch : 10, Step : 5990, Loss : 0.24011, Acc : 0.884, Sensitive_Loss : 0.07188, Sensitive_Acc : 15.400, Run Time : 9.45 sec
INFO:root:2024-04-10 20:26:46, Train, Epoch : 10, Step : 6000, Loss : 0.34792, Acc : 0.866, Sensitive_Loss : 0.05370, Sensitive_Acc : 18.000, Run Time : 10.73 sec
INFO:root:2024-04-10 20:28:19, Dev, Step : 6000, Loss : 0.55073, Acc : 0.779, Auc : 0.857, Sensitive_Loss : 0.11455, Sensitive_Acc : 16.293, Sensitive_Auc : 0.990, Mean auc: 0.857, Run Time : 93.89 sec
INFO:root:2024-04-10 20:28:26, Train, Epoch : 10, Step : 6010, Loss : 0.30831, Acc : 0.869, Sensitive_Loss : 0.09028, Sensitive_Acc : 18.400, Run Time : 100.90 sec
INFO:root:2024-04-10 20:28:36, Train, Epoch : 10, Step : 6020, Loss : 0.24447, Acc : 0.894, Sensitive_Loss : 0.05966, Sensitive_Acc : 15.800, Run Time : 9.78 sec
INFO:root:2024-04-10 20:28:46, Train, Epoch : 10, Step : 6030, Loss : 0.27085, Acc : 0.878, Sensitive_Loss : 0.08928, Sensitive_Acc : 17.800, Run Time : 10.21 sec
INFO:root:2024-04-10 20:28:56, Train, Epoch : 10, Step : 6040, Loss : 0.29452, Acc : 0.891, Sensitive_Loss : 0.10104, Sensitive_Acc : 16.800, Run Time : 9.65 sec
INFO:root:2024-04-10 20:29:09, Train, Epoch : 10, Step : 6050, Loss : 0.24671, Acc : 0.909, Sensitive_Loss : 0.08082, Sensitive_Acc : 16.600, Run Time : 12.72 sec
INFO:root:2024-04-10 20:29:21, Train, Epoch : 10, Step : 6060, Loss : 0.28184, Acc : 0.853, Sensitive_Loss : 0.07143, Sensitive_Acc : 16.600, Run Time : 12.19 sec
INFO:root:2024-04-10 20:29:29, Train, Epoch : 10, Step : 6070, Loss : 0.29134, Acc : 0.847, Sensitive_Loss : 0.06573, Sensitive_Acc : 14.900, Run Time : 8.44 sec
INFO:root:2024-04-10 20:29:39, Train, Epoch : 10, Step : 6080, Loss : 0.31071, Acc : 0.872, Sensitive_Loss : 0.08703, Sensitive_Acc : 14.900, Run Time : 9.40 sec
INFO:root:2024-04-10 20:29:50, Train, Epoch : 10, Step : 6090, Loss : 0.21277, Acc : 0.906, Sensitive_Loss : 0.08586, Sensitive_Acc : 17.000, Run Time : 10.95 sec
INFO:root:2024-04-10 20:29:58, Train, Epoch : 10, Step : 6100, Loss : 0.31938, Acc : 0.884, Sensitive_Loss : 0.09238, Sensitive_Acc : 16.700, Run Time : 8.22 sec
INFO:root:2024-04-10 20:31:32, Dev, Step : 6100, Loss : 0.56604, Acc : 0.781, Auc : 0.853, Sensitive_Loss : 0.12495, Sensitive_Acc : 16.379, Sensitive_Auc : 0.991, Mean auc: 0.853, Run Time : 93.82 sec
INFO:root:2024-04-10 20:31:39, Train, Epoch : 10, Step : 6110, Loss : 0.26640, Acc : 0.884, Sensitive_Loss : 0.10534, Sensitive_Acc : 17.100, Run Time : 101.24 sec
INFO:root:2024-04-10 20:31:50, Train, Epoch : 10, Step : 6120, Loss : 0.30470, Acc : 0.859, Sensitive_Loss : 0.08409, Sensitive_Acc : 16.600, Run Time : 11.17 sec
INFO:root:2024-04-10 20:31:59, Train, Epoch : 10, Step : 6130, Loss : 0.27116, Acc : 0.891, Sensitive_Loss : 0.08532, Sensitive_Acc : 17.800, Run Time : 8.54 sec
INFO:root:2024-04-10 20:32:09, Train, Epoch : 10, Step : 6140, Loss : 0.29675, Acc : 0.875, Sensitive_Loss : 0.09794, Sensitive_Acc : 15.000, Run Time : 9.67 sec
INFO:root:2024-04-10 20:32:22, Train, Epoch : 10, Step : 6150, Loss : 0.23152, Acc : 0.894, Sensitive_Loss : 0.06020, Sensitive_Acc : 16.400, Run Time : 13.21 sec
INFO:root:2024-04-10 20:32:30, Train, Epoch : 10, Step : 6160, Loss : 0.26828, Acc : 0.859, Sensitive_Loss : 0.07446, Sensitive_Acc : 17.600, Run Time : 8.25 sec
INFO:root:2024-04-10 20:32:39, Train, Epoch : 10, Step : 6170, Loss : 0.33246, Acc : 0.844, Sensitive_Loss : 0.10814, Sensitive_Acc : 15.500, Run Time : 9.13 sec
INFO:root:2024-04-10 20:32:53, Train, Epoch : 10, Step : 6180, Loss : 0.26788, Acc : 0.909, Sensitive_Loss : 0.08990, Sensitive_Acc : 16.800, Run Time : 14.35 sec
INFO:root:2024-04-10 20:33:03, Train, Epoch : 10, Step : 6190, Loss : 0.27391, Acc : 0.875, Sensitive_Loss : 0.11426, Sensitive_Acc : 15.900, Run Time : 9.47 sec
INFO:root:2024-04-10 20:33:12, Train, Epoch : 10, Step : 6200, Loss : 0.27388, Acc : 0.878, Sensitive_Loss : 0.11558, Sensitive_Acc : 15.300, Run Time : 8.90 sec
INFO:root:2024-04-10 20:34:46, Dev, Step : 6200, Loss : 0.59768, Acc : 0.763, Auc : 0.843, Sensitive_Loss : 0.13191, Sensitive_Acc : 16.379, Sensitive_Auc : 0.991, Mean auc: 0.843, Run Time : 93.92 sec
INFO:root:2024-04-10 20:34:52, Train, Epoch : 10, Step : 6210, Loss : 0.29546, Acc : 0.872, Sensitive_Loss : 0.07995, Sensitive_Acc : 16.300, Run Time : 100.39 sec
INFO:root:2024-04-10 20:35:04, Train, Epoch : 10, Step : 6220, Loss : 0.28282, Acc : 0.866, Sensitive_Loss : 0.09150, Sensitive_Acc : 15.400, Run Time : 11.47 sec
INFO:root:2024-04-10 20:35:13, Train, Epoch : 10, Step : 6230, Loss : 0.29390, Acc : 0.881, Sensitive_Loss : 0.09091, Sensitive_Acc : 16.800, Run Time : 8.96 sec
INFO:root:2024-04-10 20:35:22, Train, Epoch : 10, Step : 6240, Loss : 0.27640, Acc : 0.891, Sensitive_Loss : 0.15194, Sensitive_Acc : 15.800, Run Time : 9.14 sec
INFO:root:2024-04-10 20:35:33, Train, Epoch : 10, Step : 6250, Loss : 0.25999, Acc : 0.884, Sensitive_Loss : 0.06212, Sensitive_Acc : 15.800, Run Time : 11.00 sec
INFO:root:2024-04-10 20:35:42, Train, Epoch : 10, Step : 6260, Loss : 0.32210, Acc : 0.838, Sensitive_Loss : 0.07497, Sensitive_Acc : 16.200, Run Time : 9.47 sec
INFO:root:2024-04-10 20:35:51, Train, Epoch : 10, Step : 6270, Loss : 0.29730, Acc : 0.850, Sensitive_Loss : 0.07307, Sensitive_Acc : 17.000, Run Time : 8.20 sec
INFO:root:2024-04-10 20:36:02, Train, Epoch : 10, Step : 6280, Loss : 0.28596, Acc : 0.869, Sensitive_Loss : 0.07225, Sensitive_Acc : 17.100, Run Time : 11.18 sec
INFO:root:2024-04-10 20:36:11, Train, Epoch : 10, Step : 6290, Loss : 0.31260, Acc : 0.887, Sensitive_Loss : 0.06197, Sensitive_Acc : 17.100, Run Time : 9.27 sec
INFO:root:2024-04-10 20:36:19, Train, Epoch : 10, Step : 6300, Loss : 0.30788, Acc : 0.866, Sensitive_Loss : 0.12665, Sensitive_Acc : 15.000, Run Time : 8.07 sec
INFO:root:2024-04-10 20:37:52, Dev, Step : 6300, Loss : 0.56669, Acc : 0.774, Auc : 0.852, Sensitive_Loss : 0.12123, Sensitive_Acc : 16.421, Sensitive_Auc : 0.991, Mean auc: 0.852, Run Time : 93.25 sec
INFO:root:2024-04-10 20:38:01, Train, Epoch : 10, Step : 6310, Loss : 0.25021, Acc : 0.912, Sensitive_Loss : 0.07520, Sensitive_Acc : 17.000, Run Time : 102.23 sec
INFO:root:2024-04-10 20:38:09, Train, Epoch : 10, Step : 6320, Loss : 0.29868, Acc : 0.903, Sensitive_Loss : 0.06567, Sensitive_Acc : 15.500, Run Time : 7.79 sec
INFO:root:2024-04-10 20:38:18, Train, Epoch : 10, Step : 6330, Loss : 0.30008, Acc : 0.863, Sensitive_Loss : 0.10890, Sensitive_Acc : 17.000, Run Time : 8.62 sec
INFO:root:2024-04-10 20:38:29, Train, Epoch : 10, Step : 6340, Loss : 0.31219, Acc : 0.897, Sensitive_Loss : 0.08759, Sensitive_Acc : 16.200, Run Time : 10.68 sec
INFO:root:2024-04-10 20:38:37, Train, Epoch : 10, Step : 6350, Loss : 0.26037, Acc : 0.897, Sensitive_Loss : 0.07175, Sensitive_Acc : 16.600, Run Time : 8.60 sec
INFO:root:2024-04-10 20:38:45, Train, Epoch : 10, Step : 6360, Loss : 0.21681, Acc : 0.884, Sensitive_Loss : 0.07272, Sensitive_Acc : 15.800, Run Time : 7.72 sec
INFO:root:2024-04-10 20:38:53, Train, Epoch : 10, Step : 6370, Loss : 0.28562, Acc : 0.866, Sensitive_Loss : 0.08039, Sensitive_Acc : 17.600, Run Time : 7.71 sec
INFO:root:2024-04-10 20:39:03, Train, Epoch : 10, Step : 6380, Loss : 0.26649, Acc : 0.887, Sensitive_Loss : 0.09983, Sensitive_Acc : 16.100, Run Time : 10.70 sec
INFO:root:2024-04-10 20:39:12, Train, Epoch : 10, Step : 6390, Loss : 0.32204, Acc : 0.850, Sensitive_Loss : 0.08206, Sensitive_Acc : 16.700, Run Time : 8.48 sec
INFO:root:2024-04-10 20:39:20, Train, Epoch : 10, Step : 6400, Loss : 0.32809, Acc : 0.844, Sensitive_Loss : 0.06875, Sensitive_Acc : 15.700, Run Time : 7.94 sec
INFO:root:2024-04-10 20:40:56, Dev, Step : 6400, Loss : 0.56051, Acc : 0.776, Auc : 0.856, Sensitive_Loss : 0.13834, Sensitive_Acc : 16.407, Sensitive_Auc : 0.991, Mean auc: 0.856, Run Time : 96.40 sec
INFO:root:2024-04-10 20:41:06, Train, Epoch : 10, Step : 6410, Loss : 0.28541, Acc : 0.872, Sensitive_Loss : 0.07116, Sensitive_Acc : 14.900, Run Time : 106.28 sec
INFO:root:2024-04-10 20:41:15, Train, Epoch : 10, Step : 6420, Loss : 0.30769, Acc : 0.863, Sensitive_Loss : 0.06628, Sensitive_Acc : 17.600, Run Time : 8.75 sec
INFO:root:2024-04-10 20:41:25, Train, Epoch : 10, Step : 6430, Loss : 0.25504, Acc : 0.878, Sensitive_Loss : 0.08485, Sensitive_Acc : 16.300, Run Time : 9.88 sec
INFO:root:2024-04-10 20:41:35, Train, Epoch : 10, Step : 6440, Loss : 0.23784, Acc : 0.894, Sensitive_Loss : 0.13319, Sensitive_Acc : 16.100, Run Time : 10.42 sec
INFO:root:2024-04-10 20:43:22
INFO:root:y_pred: [0.3122971  0.02101398 0.93935883 ... 0.7242841  0.8393554  0.02825963]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.99737799e-01 9.39054042e-03 4.63828832e-01 9.99990821e-01
 9.95201111e-01 9.98952270e-01 9.99869585e-01 4.07067591e-06
 9.91859972e-01 9.94892955e-01 8.27607363e-02 4.10381019e-01
 6.69527026e-06 9.23114479e-01 9.99990821e-01 9.99928594e-01
 9.92081285e-01 9.96893883e-01 9.96142685e-01 9.98599708e-01
 9.96423423e-01 2.73425400e-01 9.99831557e-01 5.68694174e-01
 9.55919027e-01 1.09266862e-02 9.99455392e-01 1.21741556e-03
 9.99971628e-01 2.62045232e-03 6.79538120e-04 5.33653080e-01
 2.43554432e-02 9.96912360e-01 6.19016146e-06 9.99842048e-01
 1.00157658e-05 9.99982834e-01 2.35504098e-02 9.63299870e-01
 9.99887228e-01 9.86029409e-05 2.41239890e-02 1.97429603e-04
 1.74265683e-01 5.55739820e-01 9.99961853e-01 9.98185933e-01
 9.97446299e-01 9.99522090e-01 1.16728817e-03 9.87981141e-01
 5.39537752e-04 1.42929077e-01 9.99986649e-01 2.29474112e-01
 9.92690623e-01 9.99823987e-01 9.98235106e-01 5.50852111e-03
 2.68645561e-03 9.99590456e-01 8.50602761e-02 9.99830127e-01
 9.63218927e-01 2.01851204e-02 9.95049894e-01 9.76359785e-01
 9.99997616e-01 9.99938726e-01 6.59293495e-03 1.73724785e-01
 8.37930501e-01 9.99922514e-01 9.90104854e-01 1.34421629e-03
 7.31353343e-01 1.46503886e-03 1.45738568e-05 9.98606861e-01
 1.07917879e-02 9.99989986e-01 9.99729931e-01 9.99932408e-01
 2.85996646e-01 9.99999285e-01 6.83534790e-06 6.28793903e-04
 9.89320636e-01 9.98673081e-01 1.24350959e-03 4.05612320e-01
 6.41689310e-03 8.77249479e-01 1.06070414e-02 9.99956489e-01
 5.82810317e-04 9.89303589e-01 9.92701173e-01 7.58449241e-05
 1.04770088e-03 2.99095988e-01 9.99850273e-01 9.99890566e-01
 9.88458455e-01 8.15316737e-01 9.93019640e-01 2.06835661e-02
 3.67077947e-01 9.99997497e-01 5.27697102e-06 8.26430449e-04
 1.36797065e-02 9.99114335e-01 9.99170303e-01 6.03604240e-05
 9.95707273e-01 1.31990993e-03 9.99883175e-01 4.40934181e-01
 9.99966621e-01 9.99995589e-01 7.13167727e-01 2.15067372e-01
 4.01057489e-03 1.75004621e-04 6.65626070e-03 1.10658130e-03
 9.96569514e-01 9.99863148e-01 2.76708379e-05 3.44887871e-04
 5.49218443e-04 8.61455277e-02 9.96136844e-01 9.99113619e-01
 9.99765217e-01 4.47809130e-01 6.22261725e-02 9.94663477e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 20:43:22, Dev, Step : 6440, Loss : 0.59642, Acc : 0.767, Auc : 0.849, Sensitive_Loss : 0.12284, Sensitive_Acc : 16.364, Sensitive_Auc : 0.990, Mean auc: 0.849, Run Time : 106.40 sec
