Running on desktop18:
stdin: is not a tty
Activating chexpert environment...
2
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_val.csv",
    "pred_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Biased_Sex_1_pos01.csv",
    "pred_model": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Biased_Sex_1_pos011.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.05,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-27 08:58:16, Train, Epoch : 1, Step : 10, Loss : 0.72462, Acc : 0.572, Sensitive_Loss : 0.64748, Sensitive_Acc : 16.000, Run Time : 22.43 sec
INFO:root:2024-04-27 08:58:35, Train, Epoch : 1, Step : 20, Loss : 0.66387, Acc : 0.613, Sensitive_Loss : 0.65495, Sensitive_Acc : 14.500, Run Time : 18.72 sec
INFO:root:2024-04-27 08:58:53, Train, Epoch : 1, Step : 30, Loss : 0.62284, Acc : 0.703, Sensitive_Loss : 0.58910, Sensitive_Acc : 15.800, Run Time : 17.98 sec
INFO:root:2024-04-27 08:59:10, Train, Epoch : 1, Step : 40, Loss : 0.65860, Acc : 0.684, Sensitive_Loss : 0.53485, Sensitive_Acc : 16.600, Run Time : 17.52 sec
INFO:root:2024-04-27 08:59:29, Train, Epoch : 1, Step : 50, Loss : 0.52245, Acc : 0.728, Sensitive_Loss : 0.51389, Sensitive_Acc : 16.300, Run Time : 18.61 sec
INFO:root:2024-04-27 08:59:46, Train, Epoch : 1, Step : 60, Loss : 0.59031, Acc : 0.716, Sensitive_Loss : 0.46029, Sensitive_Acc : 15.500, Run Time : 17.39 sec
INFO:root:2024-04-27 09:00:04, Train, Epoch : 1, Step : 70, Loss : 0.51489, Acc : 0.738, Sensitive_Loss : 0.44381, Sensitive_Acc : 14.900, Run Time : 17.63 sec
INFO:root:2024-04-27 09:00:21, Train, Epoch : 1, Step : 80, Loss : 0.61731, Acc : 0.756, Sensitive_Loss : 0.37291, Sensitive_Acc : 15.800, Run Time : 16.90 sec
INFO:root:2024-04-27 09:00:38, Train, Epoch : 1, Step : 90, Loss : 0.49711, Acc : 0.741, Sensitive_Loss : 0.34531, Sensitive_Acc : 17.000, Run Time : 17.54 sec
INFO:root:2024-04-27 09:00:57, Train, Epoch : 1, Step : 100, Loss : 0.45874, Acc : 0.791, Sensitive_Loss : 0.35717, Sensitive_Acc : 16.700, Run Time : 18.26 sec
INFO:root:2024-04-27 09:05:00, Dev, Step : 100, Loss : 0.49107, Acc : 0.780, Auc : 0.858, Sensitive_Loss : 0.31312, Sensitive_Acc : 16.936, Sensitive_Auc : 0.975, Mean auc: 0.858, Run Time : 243.76 sec
INFO:root:2024-04-27 09:05:01, Best, Step : 100, Loss : 0.49107, Acc : 0.780, Auc : 0.858, Sensitive_Loss : 0.31312, Sensitive_Acc : 16.936, Sensitive_Auc : 0.975, Best Auc : 0.858
INFO:root:2024-04-27 09:05:15, Train, Epoch : 1, Step : 110, Loss : 0.54164, Acc : 0.753, Sensitive_Loss : 0.35163, Sensitive_Acc : 16.500, Run Time : 258.06 sec
INFO:root:2024-04-27 09:05:34, Train, Epoch : 1, Step : 120, Loss : 0.51670, Acc : 0.784, Sensitive_Loss : 0.34568, Sensitive_Acc : 17.000, Run Time : 19.41 sec
INFO:root:2024-04-27 09:05:52, Train, Epoch : 1, Step : 130, Loss : 0.60917, Acc : 0.725, Sensitive_Loss : 0.30852, Sensitive_Acc : 16.000, Run Time : 17.99 sec
INFO:root:2024-04-27 09:06:10, Train, Epoch : 1, Step : 140, Loss : 0.48679, Acc : 0.775, Sensitive_Loss : 0.30043, Sensitive_Acc : 16.400, Run Time : 18.07 sec
INFO:root:2024-04-27 09:06:28, Train, Epoch : 1, Step : 150, Loss : 0.51439, Acc : 0.756, Sensitive_Loss : 0.32296, Sensitive_Acc : 16.100, Run Time : 17.83 sec
INFO:root:2024-04-27 09:06:45, Train, Epoch : 1, Step : 160, Loss : 0.53836, Acc : 0.766, Sensitive_Loss : 0.32685, Sensitive_Acc : 15.400, Run Time : 17.48 sec
INFO:root:2024-04-27 09:07:03, Train, Epoch : 1, Step : 170, Loss : 0.56354, Acc : 0.766, Sensitive_Loss : 0.32628, Sensitive_Acc : 16.900, Run Time : 17.68 sec
INFO:root:2024-04-27 09:07:21, Train, Epoch : 1, Step : 180, Loss : 0.48703, Acc : 0.769, Sensitive_Loss : 0.28369, Sensitive_Acc : 16.400, Run Time : 17.91 sec
INFO:root:2024-04-27 09:07:39, Train, Epoch : 1, Step : 190, Loss : 0.49587, Acc : 0.800, Sensitive_Loss : 0.25063, Sensitive_Acc : 15.000, Run Time : 17.78 sec
INFO:root:2024-04-27 09:07:57, Train, Epoch : 1, Step : 200, Loss : 0.44747, Acc : 0.803, Sensitive_Loss : 0.30950, Sensitive_Acc : 15.100, Run Time : 17.97 sec
INFO:root:2024-04-27 09:12:00, Dev, Step : 200, Loss : 0.49708, Acc : 0.781, Auc : 0.864, Sensitive_Loss : 0.41031, Sensitive_Acc : 16.664, Sensitive_Auc : 0.977, Mean auc: 0.864, Run Time : 243.17 sec
INFO:root:2024-04-27 09:12:01, Best, Step : 200, Loss : 0.49708, Acc : 0.781, Auc : 0.864, Sensitive_Loss : 0.41031, Sensitive_Acc : 16.664, Sensitive_Auc : 0.977, Best Auc : 0.864
INFO:root:2024-04-27 09:12:14, Train, Epoch : 1, Step : 210, Loss : 0.46922, Acc : 0.819, Sensitive_Loss : 0.21961, Sensitive_Acc : 15.200, Run Time : 257.52 sec
INFO:root:2024-04-27 09:12:31, Train, Epoch : 1, Step : 220, Loss : 0.54315, Acc : 0.778, Sensitive_Loss : 0.27752, Sensitive_Acc : 16.400, Run Time : 16.76 sec
INFO:root:2024-04-27 09:12:49, Train, Epoch : 1, Step : 230, Loss : 0.39077, Acc : 0.803, Sensitive_Loss : 0.27349, Sensitive_Acc : 17.100, Run Time : 17.52 sec
INFO:root:2024-04-27 09:13:06, Train, Epoch : 1, Step : 240, Loss : 0.45841, Acc : 0.803, Sensitive_Loss : 0.27727, Sensitive_Acc : 15.800, Run Time : 17.36 sec
INFO:root:2024-04-27 09:13:25, Train, Epoch : 1, Step : 250, Loss : 0.39020, Acc : 0.816, Sensitive_Loss : 0.26287, Sensitive_Acc : 16.600, Run Time : 18.87 sec
INFO:root:2024-04-27 09:13:42, Train, Epoch : 1, Step : 260, Loss : 0.53023, Acc : 0.741, Sensitive_Loss : 0.27591, Sensitive_Acc : 17.000, Run Time : 16.80 sec
INFO:root:2024-04-27 09:14:00, Train, Epoch : 1, Step : 270, Loss : 0.47871, Acc : 0.803, Sensitive_Loss : 0.31008, Sensitive_Acc : 16.300, Run Time : 17.96 sec
INFO:root:2024-04-27 09:14:16, Train, Epoch : 1, Step : 280, Loss : 0.44428, Acc : 0.784, Sensitive_Loss : 0.35031, Sensitive_Acc : 16.400, Run Time : 16.84 sec
INFO:root:2024-04-27 09:14:34, Train, Epoch : 1, Step : 290, Loss : 0.48622, Acc : 0.806, Sensitive_Loss : 0.24385, Sensitive_Acc : 15.900, Run Time : 17.98 sec
INFO:root:2024-04-27 09:14:52, Train, Epoch : 1, Step : 300, Loss : 0.48457, Acc : 0.806, Sensitive_Loss : 0.28600, Sensitive_Acc : 15.600, Run Time : 18.08 sec
INFO:root:2024-04-27 09:18:56, Dev, Step : 300, Loss : 0.49852, Acc : 0.782, Auc : 0.874, Sensitive_Loss : 0.28943, Sensitive_Acc : 16.807, Sensitive_Auc : 0.980, Mean auc: 0.874, Run Time : 243.35 sec
INFO:root:2024-04-27 09:18:57, Best, Step : 300, Loss : 0.49852, Acc : 0.782, Auc : 0.874, Sensitive_Loss : 0.28943, Sensitive_Acc : 16.807, Sensitive_Auc : 0.980, Best Auc : 0.874
INFO:root:2024-04-27 09:19:09, Train, Epoch : 1, Step : 310, Loss : 0.37939, Acc : 0.809, Sensitive_Loss : 0.30377, Sensitive_Acc : 15.000, Run Time : 256.57 sec
INFO:root:2024-04-27 09:19:25, Train, Epoch : 1, Step : 320, Loss : 0.55024, Acc : 0.753, Sensitive_Loss : 0.24665, Sensitive_Acc : 17.400, Run Time : 15.77 sec
INFO:root:2024-04-27 09:19:42, Train, Epoch : 1, Step : 330, Loss : 0.44036, Acc : 0.791, Sensitive_Loss : 0.24828, Sensitive_Acc : 14.900, Run Time : 16.82 sec
INFO:root:2024-04-27 09:19:58, Train, Epoch : 1, Step : 340, Loss : 0.54721, Acc : 0.772, Sensitive_Loss : 0.24341, Sensitive_Acc : 18.200, Run Time : 15.92 sec
INFO:root:2024-04-27 09:20:15, Train, Epoch : 1, Step : 350, Loss : 0.48956, Acc : 0.762, Sensitive_Loss : 0.23325, Sensitive_Acc : 14.700, Run Time : 17.17 sec
INFO:root:2024-04-27 09:20:30, Train, Epoch : 1, Step : 360, Loss : 0.45007, Acc : 0.800, Sensitive_Loss : 0.24428, Sensitive_Acc : 16.000, Run Time : 15.62 sec
INFO:root:2024-04-27 09:20:48, Train, Epoch : 1, Step : 370, Loss : 0.46752, Acc : 0.772, Sensitive_Loss : 0.21414, Sensitive_Acc : 16.000, Run Time : 17.95 sec
INFO:root:2024-04-27 09:21:07, Train, Epoch : 1, Step : 380, Loss : 0.54634, Acc : 0.719, Sensitive_Loss : 0.27144, Sensitive_Acc : 16.300, Run Time : 18.83 sec
INFO:root:2024-04-27 09:21:24, Train, Epoch : 1, Step : 390, Loss : 0.46903, Acc : 0.784, Sensitive_Loss : 0.26130, Sensitive_Acc : 16.400, Run Time : 16.92 sec
INFO:root:2024-04-27 09:21:40, Train, Epoch : 1, Step : 400, Loss : 0.43074, Acc : 0.800, Sensitive_Loss : 0.25773, Sensitive_Acc : 15.900, Run Time : 15.94 sec
INFO:root:2024-04-27 09:25:46, Dev, Step : 400, Loss : 0.51983, Acc : 0.769, Auc : 0.857, Sensitive_Loss : 0.22792, Sensitive_Acc : 16.579, Sensitive_Auc : 0.970, Mean auc: 0.857, Run Time : 246.31 sec
INFO:root:2024-04-27 09:25:58, Train, Epoch : 1, Step : 410, Loss : 0.46884, Acc : 0.756, Sensitive_Loss : 0.26237, Sensitive_Acc : 16.500, Run Time : 257.87 sec
INFO:root:2024-04-27 09:26:15, Train, Epoch : 1, Step : 420, Loss : 0.51674, Acc : 0.784, Sensitive_Loss : 0.25162, Sensitive_Acc : 17.700, Run Time : 16.88 sec
INFO:root:2024-04-27 09:26:32, Train, Epoch : 1, Step : 430, Loss : 0.46962, Acc : 0.753, Sensitive_Loss : 0.25429, Sensitive_Acc : 15.500, Run Time : 17.44 sec
INFO:root:2024-04-27 09:26:49, Train, Epoch : 1, Step : 440, Loss : 0.50878, Acc : 0.766, Sensitive_Loss : 0.23180, Sensitive_Acc : 17.300, Run Time : 16.69 sec
INFO:root:2024-04-27 09:27:06, Train, Epoch : 1, Step : 450, Loss : 0.42961, Acc : 0.803, Sensitive_Loss : 0.24806, Sensitive_Acc : 16.400, Run Time : 16.68 sec
INFO:root:2024-04-27 09:27:24, Train, Epoch : 1, Step : 460, Loss : 0.40313, Acc : 0.784, Sensitive_Loss : 0.24851, Sensitive_Acc : 15.800, Run Time : 18.21 sec
INFO:root:2024-04-27 09:27:40, Train, Epoch : 1, Step : 470, Loss : 0.50228, Acc : 0.781, Sensitive_Loss : 0.25336, Sensitive_Acc : 14.800, Run Time : 16.60 sec
INFO:root:2024-04-27 09:27:57, Train, Epoch : 1, Step : 480, Loss : 0.39108, Acc : 0.816, Sensitive_Loss : 0.22103, Sensitive_Acc : 14.400, Run Time : 16.16 sec
INFO:root:2024-04-27 09:28:14, Train, Epoch : 1, Step : 490, Loss : 0.45867, Acc : 0.778, Sensitive_Loss : 0.24442, Sensitive_Acc : 16.000, Run Time : 17.62 sec
INFO:root:2024-04-27 09:28:32, Train, Epoch : 1, Step : 500, Loss : 0.47002, Acc : 0.775, Sensitive_Loss : 0.17743, Sensitive_Acc : 16.900, Run Time : 17.40 sec
INFO:root:2024-04-27 09:32:38, Dev, Step : 500, Loss : 0.45185, Acc : 0.802, Auc : 0.879, Sensitive_Loss : 0.19346, Sensitive_Acc : 16.607, Sensitive_Auc : 0.979, Mean auc: 0.879, Run Time : 246.11 sec
INFO:root:2024-04-27 09:32:38, Best, Step : 500, Loss : 0.45185, Acc : 0.802, Auc : 0.879, Sensitive_Loss : 0.19346, Sensitive_Acc : 16.607, Sensitive_Auc : 0.979, Best Auc : 0.879
INFO:root:2024-04-27 09:32:52, Train, Epoch : 1, Step : 510, Loss : 0.43370, Acc : 0.816, Sensitive_Loss : 0.25041, Sensitive_Acc : 16.800, Run Time : 260.17 sec
INFO:root:2024-04-27 09:33:07, Train, Epoch : 1, Step : 520, Loss : 0.46905, Acc : 0.791, Sensitive_Loss : 0.20818, Sensitive_Acc : 16.200, Run Time : 15.36 sec
INFO:root:2024-04-27 09:33:24, Train, Epoch : 1, Step : 530, Loss : 0.45480, Acc : 0.781, Sensitive_Loss : 0.17392, Sensitive_Acc : 17.300, Run Time : 16.91 sec
INFO:root:2024-04-27 09:33:41, Train, Epoch : 1, Step : 540, Loss : 0.46179, Acc : 0.800, Sensitive_Loss : 0.21478, Sensitive_Acc : 17.400, Run Time : 17.12 sec
INFO:root:2024-04-27 09:33:58, Train, Epoch : 1, Step : 550, Loss : 0.49171, Acc : 0.787, Sensitive_Loss : 0.22888, Sensitive_Acc : 16.500, Run Time : 17.34 sec
INFO:root:2024-04-27 09:34:16, Train, Epoch : 1, Step : 560, Loss : 0.45538, Acc : 0.769, Sensitive_Loss : 0.19968, Sensitive_Acc : 17.100, Run Time : 17.79 sec
INFO:root:2024-04-27 09:34:33, Train, Epoch : 1, Step : 570, Loss : 0.51291, Acc : 0.800, Sensitive_Loss : 0.18654, Sensitive_Acc : 16.100, Run Time : 16.84 sec
INFO:root:2024-04-27 09:34:51, Train, Epoch : 1, Step : 580, Loss : 0.53320, Acc : 0.759, Sensitive_Loss : 0.22844, Sensitive_Acc : 16.000, Run Time : 17.56 sec
INFO:root:2024-04-27 09:35:08, Train, Epoch : 1, Step : 590, Loss : 0.45496, Acc : 0.781, Sensitive_Loss : 0.19001, Sensitive_Acc : 17.000, Run Time : 17.06 sec
INFO:root:2024-04-27 09:35:24, Train, Epoch : 1, Step : 600, Loss : 0.44477, Acc : 0.787, Sensitive_Loss : 0.21965, Sensitive_Acc : 15.000, Run Time : 16.63 sec
INFO:root:2024-04-27 09:39:29, Dev, Step : 600, Loss : 0.53979, Acc : 0.754, Auc : 0.877, Sensitive_Loss : 0.25921, Sensitive_Acc : 16.864, Sensitive_Auc : 0.965, Mean auc: 0.877, Run Time : 245.01 sec
INFO:root:2024-04-27 09:39:41, Train, Epoch : 1, Step : 610, Loss : 0.40238, Acc : 0.800, Sensitive_Loss : 0.21111, Sensitive_Acc : 16.200, Run Time : 256.32 sec
INFO:root:2024-04-27 09:39:57, Train, Epoch : 1, Step : 620, Loss : 0.47715, Acc : 0.781, Sensitive_Loss : 0.23092, Sensitive_Acc : 16.200, Run Time : 16.29 sec
INFO:root:2024-04-27 09:44:06
INFO:root:y_pred: [0.39478067 0.9092903  0.19987988 ... 0.78876716 0.03139289 0.7119433 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [6.81724370e-01 2.43352167e-03 4.91121709e-02 1.38839765e-03
 9.70362246e-01 7.69481668e-03 9.99921441e-01 9.90052283e-01
 7.66986888e-03 4.91352528e-01 4.80652064e-01 9.97498691e-01
 9.87342834e-01 9.85007524e-01 1.68554097e-01 8.09219003e-01
 9.98076081e-01 8.51082057e-03 9.16400075e-01 4.78310198e-01
 9.85037148e-01 7.41473809e-02 9.95811582e-01 9.87327278e-01
 9.91894662e-01 6.83601439e-01 3.86390053e-02 9.93933380e-01
 9.93843913e-01 2.55553629e-02 5.88856861e-02 4.20363933e-01
 4.40869153e-01 9.26685855e-02 2.24048525e-01 3.30188088e-02
 4.53741908e-01 7.33812898e-02 9.65392947e-01 8.13824654e-01
 2.67912727e-03 1.00906016e-02 9.00763988e-01 5.22023952e-03
 9.89834607e-01 9.67207611e-01 9.90561306e-01 9.94885027e-01
 8.09058361e-03 9.87788737e-01 9.89278376e-01 1.12554036e-01
 1.29275262e-01 1.15582468e-02 1.88822963e-03 1.36916088e-02
 3.40167433e-02 6.52316352e-03 1.20518089e-03 3.15300882e-01
 1.06997574e-02 7.61172846e-02 1.69499174e-01 1.93830475e-01
 3.48776847e-01 9.94973779e-01 2.38474179e-02 9.91178691e-01
 9.69745755e-01 7.91467249e-01 6.54728487e-02 5.92549384e-01
 3.13400961e-02 1.09285861e-01 1.87552065e-01 1.10101316e-03
 9.92535427e-03 8.21712837e-02 5.45868091e-02 9.97348070e-01
 9.98901486e-01 6.01906446e-04 5.85754216e-01 8.00809916e-03
 9.52004671e-01 3.72398615e-01 4.68082987e-02 1.02187470e-01
 9.58944321e-01 9.96491969e-01 9.96105194e-01 9.43625346e-03
 7.23106414e-02 9.93982494e-01 1.98872715e-01 3.39920409e-02
 9.75159049e-01 9.97892797e-01 4.54099802e-03 1.25295132e-01
 9.64137554e-01 9.59864020e-01 9.23614740e-01 9.90205348e-01
 1.17732147e-02 8.79962921e-01 9.47992146e-01 9.89660501e-01
 9.79210556e-01 3.04139423e-04 9.03896809e-01 9.00876403e-01
 9.99175571e-03 9.91551518e-01 9.89868879e-01 9.82124925e-01
 6.97481811e-01 9.92098331e-01 1.21578295e-02 1.74250863e-02
 9.79155123e-01 9.90765929e-01 2.69132107e-03 9.37967181e-01
 9.99477804e-01 2.28520021e-01 8.47918868e-01 1.23697724e-02
 3.70806530e-02 9.83765185e-01 9.53845739e-01 4.22790200e-02
 1.27879307e-01 8.23845044e-02 9.91864324e-01 9.74468946e-01
 8.82116139e-01 5.52723557e-03 1.17080346e-01 5.54904938e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 09:44:06, Dev, Step : 626, Loss : 0.46298, Acc : 0.803, Auc : 0.874, Sensitive_Loss : 0.20113, Sensitive_Acc : 16.807, Sensitive_Auc : 0.982, Mean auc: 0.874, Run Time : 239.99 sec
INFO:root:2024-04-27 09:44:16, Train, Epoch : 2, Step : 630, Loss : 0.16783, Acc : 0.319, Sensitive_Loss : 0.06105, Sensitive_Acc : 6.500, Run Time : 8.75 sec
INFO:root:2024-04-27 09:44:32, Train, Epoch : 2, Step : 640, Loss : 0.41779, Acc : 0.825, Sensitive_Loss : 0.23671, Sensitive_Acc : 14.800, Run Time : 16.53 sec
INFO:root:2024-04-27 09:44:50, Train, Epoch : 2, Step : 650, Loss : 0.32962, Acc : 0.838, Sensitive_Loss : 0.27440, Sensitive_Acc : 18.000, Run Time : 17.47 sec
INFO:root:2024-04-27 09:45:06, Train, Epoch : 2, Step : 660, Loss : 0.38957, Acc : 0.834, Sensitive_Loss : 0.19891, Sensitive_Acc : 15.100, Run Time : 16.21 sec
INFO:root:2024-04-27 09:45:23, Train, Epoch : 2, Step : 670, Loss : 0.43216, Acc : 0.800, Sensitive_Loss : 0.19656, Sensitive_Acc : 16.700, Run Time : 17.59 sec
INFO:root:2024-04-27 09:45:42, Train, Epoch : 2, Step : 680, Loss : 0.43257, Acc : 0.800, Sensitive_Loss : 0.20983, Sensitive_Acc : 16.000, Run Time : 18.72 sec
INFO:root:2024-04-27 09:45:59, Train, Epoch : 2, Step : 690, Loss : 0.48927, Acc : 0.781, Sensitive_Loss : 0.31501, Sensitive_Acc : 14.800, Run Time : 17.12 sec
INFO:root:2024-04-27 09:46:16, Train, Epoch : 2, Step : 700, Loss : 0.40229, Acc : 0.803, Sensitive_Loss : 0.18747, Sensitive_Acc : 15.600, Run Time : 16.73 sec
INFO:root:2024-04-27 09:50:20, Dev, Step : 700, Loss : 0.48849, Acc : 0.786, Auc : 0.877, Sensitive_Loss : 0.22955, Sensitive_Acc : 16.671, Sensitive_Auc : 0.974, Mean auc: 0.877, Run Time : 244.49 sec
INFO:root:2024-04-27 09:50:33, Train, Epoch : 2, Step : 710, Loss : 0.46716, Acc : 0.787, Sensitive_Loss : 0.20834, Sensitive_Acc : 16.500, Run Time : 256.97 sec
INFO:root:2024-04-27 09:50:49, Train, Epoch : 2, Step : 720, Loss : 0.40502, Acc : 0.822, Sensitive_Loss : 0.18818, Sensitive_Acc : 16.100, Run Time : 16.55 sec
INFO:root:2024-04-27 09:51:07, Train, Epoch : 2, Step : 730, Loss : 0.43521, Acc : 0.753, Sensitive_Loss : 0.18960, Sensitive_Acc : 15.900, Run Time : 17.24 sec
INFO:root:2024-04-27 09:51:23, Train, Epoch : 2, Step : 740, Loss : 0.37987, Acc : 0.825, Sensitive_Loss : 0.19256, Sensitive_Acc : 16.800, Run Time : 16.48 sec
INFO:root:2024-04-27 09:51:40, Train, Epoch : 2, Step : 750, Loss : 0.45377, Acc : 0.787, Sensitive_Loss : 0.18081, Sensitive_Acc : 15.800, Run Time : 17.24 sec
INFO:root:2024-04-27 09:51:58, Train, Epoch : 2, Step : 760, Loss : 0.43768, Acc : 0.803, Sensitive_Loss : 0.15038, Sensitive_Acc : 14.200, Run Time : 17.29 sec
INFO:root:2024-04-27 09:52:15, Train, Epoch : 2, Step : 770, Loss : 0.42246, Acc : 0.812, Sensitive_Loss : 0.15684, Sensitive_Acc : 16.600, Run Time : 17.30 sec
INFO:root:2024-04-27 09:52:33, Train, Epoch : 2, Step : 780, Loss : 0.40357, Acc : 0.816, Sensitive_Loss : 0.15329, Sensitive_Acc : 16.100, Run Time : 18.25 sec
INFO:root:2024-04-27 09:52:50, Train, Epoch : 2, Step : 790, Loss : 0.38761, Acc : 0.828, Sensitive_Loss : 0.19134, Sensitive_Acc : 16.800, Run Time : 17.03 sec
INFO:root:2024-04-27 09:53:08, Train, Epoch : 2, Step : 800, Loss : 0.43460, Acc : 0.809, Sensitive_Loss : 0.14606, Sensitive_Acc : 16.900, Run Time : 17.69 sec
INFO:root:2024-04-27 09:57:15, Dev, Step : 800, Loss : 0.43231, Acc : 0.806, Auc : 0.888, Sensitive_Loss : 0.16392, Sensitive_Acc : 16.850, Sensitive_Auc : 0.986, Mean auc: 0.888, Run Time : 246.99 sec
INFO:root:2024-04-27 09:57:16, Best, Step : 800, Loss : 0.43231, Acc : 0.806, Auc : 0.888, Sensitive_Loss : 0.16392, Sensitive_Acc : 16.850, Sensitive_Auc : 0.986, Best Auc : 0.888
INFO:root:2024-04-27 09:57:29, Train, Epoch : 2, Step : 810, Loss : 0.34898, Acc : 0.847, Sensitive_Loss : 0.15933, Sensitive_Acc : 15.800, Run Time : 261.02 sec
INFO:root:2024-04-27 09:57:45, Train, Epoch : 2, Step : 820, Loss : 0.42504, Acc : 0.856, Sensitive_Loss : 0.16321, Sensitive_Acc : 17.400, Run Time : 15.88 sec
INFO:root:2024-04-27 09:58:02, Train, Epoch : 2, Step : 830, Loss : 0.38863, Acc : 0.831, Sensitive_Loss : 0.13413, Sensitive_Acc : 16.200, Run Time : 17.24 sec
INFO:root:2024-04-27 09:58:20, Train, Epoch : 2, Step : 840, Loss : 0.50337, Acc : 0.819, Sensitive_Loss : 0.17551, Sensitive_Acc : 17.800, Run Time : 17.49 sec
INFO:root:2024-04-27 09:58:37, Train, Epoch : 2, Step : 850, Loss : 0.46689, Acc : 0.825, Sensitive_Loss : 0.15686, Sensitive_Acc : 17.500, Run Time : 16.99 sec
INFO:root:2024-04-27 09:58:54, Train, Epoch : 2, Step : 860, Loss : 0.37306, Acc : 0.828, Sensitive_Loss : 0.18586, Sensitive_Acc : 15.700, Run Time : 17.61 sec
INFO:root:2024-04-27 09:59:11, Train, Epoch : 2, Step : 870, Loss : 0.38758, Acc : 0.816, Sensitive_Loss : 0.15345, Sensitive_Acc : 16.600, Run Time : 17.00 sec
INFO:root:2024-04-27 09:59:29, Train, Epoch : 2, Step : 880, Loss : 0.44519, Acc : 0.800, Sensitive_Loss : 0.18684, Sensitive_Acc : 16.200, Run Time : 17.43 sec
INFO:root:2024-04-27 09:59:46, Train, Epoch : 2, Step : 890, Loss : 0.38898, Acc : 0.831, Sensitive_Loss : 0.16907, Sensitive_Acc : 17.500, Run Time : 17.04 sec
INFO:root:2024-04-27 10:00:03, Train, Epoch : 2, Step : 900, Loss : 0.40849, Acc : 0.828, Sensitive_Loss : 0.15236, Sensitive_Acc : 16.200, Run Time : 17.28 sec
INFO:root:2024-04-27 10:04:08, Dev, Step : 900, Loss : 0.45781, Acc : 0.806, Auc : 0.882, Sensitive_Loss : 0.19283, Sensitive_Acc : 16.807, Sensitive_Auc : 0.983, Mean auc: 0.882, Run Time : 244.95 sec
INFO:root:2024-04-27 10:04:20, Train, Epoch : 2, Step : 910, Loss : 0.46937, Acc : 0.822, Sensitive_Loss : 0.23996, Sensitive_Acc : 16.900, Run Time : 257.49 sec
INFO:root:2024-04-27 10:04:37, Train, Epoch : 2, Step : 920, Loss : 0.46772, Acc : 0.772, Sensitive_Loss : 0.16247, Sensitive_Acc : 16.400, Run Time : 16.77 sec
INFO:root:2024-04-27 10:04:55, Train, Epoch : 2, Step : 930, Loss : 0.38817, Acc : 0.838, Sensitive_Loss : 0.17394, Sensitive_Acc : 17.000, Run Time : 17.61 sec
INFO:root:2024-04-27 10:05:12, Train, Epoch : 2, Step : 940, Loss : 0.43469, Acc : 0.775, Sensitive_Loss : 0.16416, Sensitive_Acc : 15.700, Run Time : 17.18 sec
INFO:root:2024-04-27 10:05:30, Train, Epoch : 2, Step : 950, Loss : 0.42802, Acc : 0.803, Sensitive_Loss : 0.15769, Sensitive_Acc : 15.500, Run Time : 17.98 sec
INFO:root:2024-04-27 10:05:47, Train, Epoch : 2, Step : 960, Loss : 0.46708, Acc : 0.794, Sensitive_Loss : 0.14820, Sensitive_Acc : 15.800, Run Time : 17.05 sec
INFO:root:2024-04-27 10:06:03, Train, Epoch : 2, Step : 970, Loss : 0.39883, Acc : 0.841, Sensitive_Loss : 0.14818, Sensitive_Acc : 15.100, Run Time : 15.99 sec
INFO:root:2024-04-27 10:06:19, Train, Epoch : 2, Step : 980, Loss : 0.35005, Acc : 0.831, Sensitive_Loss : 0.16561, Sensitive_Acc : 16.300, Run Time : 16.43 sec
INFO:root:2024-04-27 10:06:38, Train, Epoch : 2, Step : 990, Loss : 0.47468, Acc : 0.816, Sensitive_Loss : 0.17187, Sensitive_Acc : 15.300, Run Time : 18.03 sec
INFO:root:2024-04-27 10:06:55, Train, Epoch : 2, Step : 1000, Loss : 0.40387, Acc : 0.812, Sensitive_Loss : 0.21959, Sensitive_Acc : 16.600, Run Time : 17.79 sec
INFO:root:2024-04-27 10:10:59, Dev, Step : 1000, Loss : 0.44821, Acc : 0.813, Auc : 0.891, Sensitive_Loss : 0.17236, Sensitive_Acc : 16.807, Sensitive_Auc : 0.984, Mean auc: 0.891, Run Time : 243.79 sec
INFO:root:2024-04-27 10:11:00, Best, Step : 1000, Loss : 0.44821, Acc : 0.813, Auc : 0.891, Sensitive_Loss : 0.17236, Sensitive_Acc : 16.807, Sensitive_Auc : 0.984, Best Auc : 0.891
INFO:root:2024-04-27 10:11:12, Train, Epoch : 2, Step : 1010, Loss : 0.35130, Acc : 0.822, Sensitive_Loss : 0.15440, Sensitive_Acc : 17.900, Run Time : 256.86 sec
INFO:root:2024-04-27 10:11:30, Train, Epoch : 2, Step : 1020, Loss : 0.49501, Acc : 0.781, Sensitive_Loss : 0.14595, Sensitive_Acc : 17.000, Run Time : 17.36 sec
INFO:root:2024-04-27 10:11:47, Train, Epoch : 2, Step : 1030, Loss : 0.41470, Acc : 0.803, Sensitive_Loss : 0.16276, Sensitive_Acc : 16.400, Run Time : 17.27 sec
INFO:root:2024-04-27 10:12:03, Train, Epoch : 2, Step : 1040, Loss : 0.45891, Acc : 0.800, Sensitive_Loss : 0.12229, Sensitive_Acc : 15.000, Run Time : 16.34 sec
INFO:root:2024-04-27 10:12:22, Train, Epoch : 2, Step : 1050, Loss : 0.49771, Acc : 0.803, Sensitive_Loss : 0.10581, Sensitive_Acc : 18.400, Run Time : 18.53 sec
INFO:root:2024-04-27 10:12:40, Train, Epoch : 2, Step : 1060, Loss : 0.43050, Acc : 0.781, Sensitive_Loss : 0.21599, Sensitive_Acc : 15.900, Run Time : 18.00 sec
INFO:root:2024-04-27 10:12:57, Train, Epoch : 2, Step : 1070, Loss : 0.40943, Acc : 0.806, Sensitive_Loss : 0.13455, Sensitive_Acc : 16.700, Run Time : 17.31 sec
INFO:root:2024-04-27 10:13:15, Train, Epoch : 2, Step : 1080, Loss : 0.42941, Acc : 0.819, Sensitive_Loss : 0.17674, Sensitive_Acc : 16.200, Run Time : 17.88 sec
INFO:root:2024-04-27 10:13:32, Train, Epoch : 2, Step : 1090, Loss : 0.41151, Acc : 0.809, Sensitive_Loss : 0.20667, Sensitive_Acc : 16.500, Run Time : 17.57 sec
INFO:root:2024-04-27 10:13:50, Train, Epoch : 2, Step : 1100, Loss : 0.43177, Acc : 0.828, Sensitive_Loss : 0.16671, Sensitive_Acc : 17.500, Run Time : 17.10 sec
INFO:root:2024-04-27 10:17:53, Dev, Step : 1100, Loss : 0.45723, Acc : 0.799, Auc : 0.897, Sensitive_Loss : 0.33275, Sensitive_Acc : 16.779, Sensitive_Auc : 0.980, Mean auc: 0.897, Run Time : 243.63 sec
INFO:root:2024-04-27 10:17:54, Best, Step : 1100, Loss : 0.45723, Acc : 0.799, Auc : 0.897, Sensitive_Loss : 0.33275, Sensitive_Acc : 16.779, Sensitive_Auc : 0.980, Best Auc : 0.897
INFO:root:2024-04-27 10:18:07, Train, Epoch : 2, Step : 1110, Loss : 0.39745, Acc : 0.825, Sensitive_Loss : 0.19724, Sensitive_Acc : 16.200, Run Time : 257.87 sec
INFO:root:2024-04-27 10:18:24, Train, Epoch : 2, Step : 1120, Loss : 0.36322, Acc : 0.838, Sensitive_Loss : 0.15188, Sensitive_Acc : 16.600, Run Time : 16.38 sec
INFO:root:2024-04-27 10:18:42, Train, Epoch : 2, Step : 1130, Loss : 0.43206, Acc : 0.797, Sensitive_Loss : 0.15943, Sensitive_Acc : 16.300, Run Time : 17.99 sec
INFO:root:2024-04-27 10:19:01, Train, Epoch : 2, Step : 1140, Loss : 0.43053, Acc : 0.825, Sensitive_Loss : 0.15740, Sensitive_Acc : 16.500, Run Time : 19.11 sec
INFO:root:2024-04-27 10:19:16, Train, Epoch : 2, Step : 1150, Loss : 0.46591, Acc : 0.769, Sensitive_Loss : 0.19192, Sensitive_Acc : 16.600, Run Time : 15.13 sec
INFO:root:2024-04-27 10:19:33, Train, Epoch : 2, Step : 1160, Loss : 0.43519, Acc : 0.791, Sensitive_Loss : 0.21014, Sensitive_Acc : 14.200, Run Time : 17.33 sec
INFO:root:2024-04-27 10:19:51, Train, Epoch : 2, Step : 1170, Loss : 0.35256, Acc : 0.847, Sensitive_Loss : 0.19263, Sensitive_Acc : 17.100, Run Time : 17.97 sec
INFO:root:2024-04-27 10:20:09, Train, Epoch : 2, Step : 1180, Loss : 0.45930, Acc : 0.803, Sensitive_Loss : 0.13460, Sensitive_Acc : 16.100, Run Time : 17.32 sec
INFO:root:2024-04-27 10:20:27, Train, Epoch : 2, Step : 1190, Loss : 0.43173, Acc : 0.819, Sensitive_Loss : 0.11295, Sensitive_Acc : 15.200, Run Time : 17.92 sec
INFO:root:2024-04-27 10:20:43, Train, Epoch : 2, Step : 1200, Loss : 0.47154, Acc : 0.806, Sensitive_Loss : 0.13546, Sensitive_Acc : 15.900, Run Time : 16.31 sec
INFO:root:2024-04-27 10:24:48, Dev, Step : 1200, Loss : 0.45087, Acc : 0.803, Auc : 0.887, Sensitive_Loss : 0.20161, Sensitive_Acc : 16.879, Sensitive_Auc : 0.994, Mean auc: 0.887, Run Time : 245.11 sec
INFO:root:2024-04-27 10:25:01, Train, Epoch : 2, Step : 1210, Loss : 0.48301, Acc : 0.787, Sensitive_Loss : 0.13409, Sensitive_Acc : 16.200, Run Time : 257.93 sec
INFO:root:2024-04-27 10:25:18, Train, Epoch : 2, Step : 1220, Loss : 0.49526, Acc : 0.787, Sensitive_Loss : 0.17533, Sensitive_Acc : 15.800, Run Time : 17.20 sec
INFO:root:2024-04-27 10:25:35, Train, Epoch : 2, Step : 1230, Loss : 0.43039, Acc : 0.822, Sensitive_Loss : 0.18816, Sensitive_Acc : 15.000, Run Time : 16.52 sec
INFO:root:2024-04-27 10:25:54, Train, Epoch : 2, Step : 1240, Loss : 0.49397, Acc : 0.819, Sensitive_Loss : 0.10210, Sensitive_Acc : 16.000, Run Time : 19.35 sec
INFO:root:2024-04-27 10:26:09, Train, Epoch : 2, Step : 1250, Loss : 0.38240, Acc : 0.831, Sensitive_Loss : 0.13125, Sensitive_Acc : 16.100, Run Time : 15.35 sec
INFO:root:2024-04-27 10:30:26
INFO:root:y_pred: [0.04554806 0.70301    0.04806746 ... 0.450417   0.02466086 0.6707892 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.9413729e-01 4.0790360e-02 3.8574570e-01 5.5277050e-03 9.9994361e-01
 1.7660124e-02 9.9999726e-01 9.9991739e-01 9.2045022e-03 9.0425539e-01
 9.9918181e-01 9.9988174e-01 9.9828917e-01 9.9423850e-01 1.0378262e-01
 9.6635622e-01 9.9991584e-01 3.6128389e-04 9.5582414e-01 9.7863173e-01
 9.9905342e-01 2.7280992e-01 9.9984479e-01 9.9329489e-01 9.9889296e-01
 9.9864930e-01 1.1463780e-02 9.9993002e-01 9.9804711e-01 9.4158775e-01
 3.4863088e-01 8.5789537e-01 2.0294592e-01 1.8615151e-02 7.3650426e-01
 1.4964789e-01 3.8840163e-01 1.4294060e-01 9.9940860e-01 9.9820626e-01
 1.2712931e-03 7.7952491e-03 9.8774952e-01 3.8565355e-03 9.9996245e-01
 9.9724752e-01 9.9872226e-01 9.9922574e-01 1.0937203e-02 9.9602532e-01
 9.9969029e-01 7.4839056e-01 9.2294455e-01 1.0539334e-02 2.6014775e-02
 6.0502452e-01 6.2996984e-02 2.9487863e-01 7.5339801e-03 8.8111281e-01
 9.7378701e-02 5.3555214e-01 2.5496560e-01 9.8710352e-01 5.1144671e-01
 9.9998367e-01 2.7746499e-01 9.9988043e-01 9.9840051e-01 9.2668027e-01
 9.8305053e-01 7.3942876e-01 3.9928865e-01 5.2992105e-01 1.6771290e-01
 1.2739771e-02 9.2401996e-02 6.3070160e-01 4.9882135e-03 9.9990594e-01
 9.9969041e-01 1.7194926e-03 7.2985262e-01 6.3930415e-03 9.5969450e-01
 8.8101470e-01 9.9931462e-03 2.8242537e-01 9.9245912e-01 9.9954432e-01
 9.9998653e-01 3.0193502e-01 3.4597013e-02 9.9776578e-01 8.0104613e-01
 3.9722204e-02 9.9475479e-01 9.9999154e-01 1.5453341e-02 3.7286860e-01
 9.9680859e-01 9.9964643e-01 9.9831617e-01 9.9926311e-01 7.6427706e-02
 6.5768844e-01 9.9878818e-01 9.9984550e-01 9.8529774e-01 7.1761552e-03
 9.8263586e-01 9.9984884e-01 3.1712192e-01 9.9898416e-01 9.9990010e-01
 9.9466705e-01 9.1371578e-01 9.9985600e-01 2.4650309e-02 1.5916589e-01
 9.9963439e-01 9.9969840e-01 3.0113429e-02 9.9596655e-01 9.9999785e-01
 2.1067150e-01 9.9838519e-01 2.1892482e-01 6.8214637e-01 9.9914658e-01
 9.9892247e-01 3.2987711e-01 4.3477216e-01 1.3847162e-01 9.9971348e-01
 9.9557436e-01 9.9370462e-01 1.4350244e-01 2.3513557e-01 9.9903929e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 10:30:26, Dev, Step : 1252, Loss : 0.48972, Acc : 0.789, Auc : 0.896, Sensitive_Loss : 0.28056, Sensitive_Acc : 16.764, Sensitive_Auc : 0.993, Mean auc: 0.896, Run Time : 255.10 sec
INFO:root:2024-04-27 10:30:46, Train, Epoch : 3, Step : 1260, Loss : 0.33094, Acc : 0.644, Sensitive_Loss : 0.17306, Sensitive_Acc : 13.300, Run Time : 16.89 sec
INFO:root:2024-04-27 10:31:04, Train, Epoch : 3, Step : 1270, Loss : 0.40433, Acc : 0.841, Sensitive_Loss : 0.12431, Sensitive_Acc : 16.100, Run Time : 18.58 sec
INFO:root:2024-04-27 10:31:22, Train, Epoch : 3, Step : 1280, Loss : 0.42426, Acc : 0.812, Sensitive_Loss : 0.15124, Sensitive_Acc : 15.200, Run Time : 17.72 sec
INFO:root:2024-04-27 10:31:39, Train, Epoch : 3, Step : 1290, Loss : 0.39981, Acc : 0.844, Sensitive_Loss : 0.18553, Sensitive_Acc : 15.500, Run Time : 17.33 sec
INFO:root:2024-04-27 10:31:57, Train, Epoch : 3, Step : 1300, Loss : 0.41297, Acc : 0.800, Sensitive_Loss : 0.09346, Sensitive_Acc : 15.100, Run Time : 17.28 sec
INFO:root:2024-04-27 10:36:02, Dev, Step : 1300, Loss : 0.43282, Acc : 0.811, Auc : 0.901, Sensitive_Loss : 0.15771, Sensitive_Acc : 16.807, Sensitive_Auc : 0.994, Mean auc: 0.901, Run Time : 245.15 sec
INFO:root:2024-04-27 10:36:02, Best, Step : 1300, Loss : 0.43282, Acc : 0.811, Auc : 0.901, Sensitive_Loss : 0.15771, Sensitive_Acc : 16.807, Sensitive_Auc : 0.994, Best Auc : 0.901
INFO:root:2024-04-27 10:36:14, Train, Epoch : 3, Step : 1310, Loss : 0.36101, Acc : 0.850, Sensitive_Loss : 0.18253, Sensitive_Acc : 15.900, Run Time : 257.78 sec
INFO:root:2024-04-27 10:36:32, Train, Epoch : 3, Step : 1320, Loss : 0.41378, Acc : 0.816, Sensitive_Loss : 0.13734, Sensitive_Acc : 16.100, Run Time : 17.27 sec
INFO:root:2024-04-27 10:36:50, Train, Epoch : 3, Step : 1330, Loss : 0.41725, Acc : 0.791, Sensitive_Loss : 0.12594, Sensitive_Acc : 15.300, Run Time : 18.04 sec
INFO:root:2024-04-27 10:37:07, Train, Epoch : 3, Step : 1340, Loss : 0.45813, Acc : 0.812, Sensitive_Loss : 0.12654, Sensitive_Acc : 17.600, Run Time : 17.20 sec
INFO:root:2024-04-27 10:37:24, Train, Epoch : 3, Step : 1350, Loss : 0.33289, Acc : 0.841, Sensitive_Loss : 0.11376, Sensitive_Acc : 17.100, Run Time : 17.44 sec
INFO:root:2024-04-27 10:37:41, Train, Epoch : 3, Step : 1360, Loss : 0.40189, Acc : 0.831, Sensitive_Loss : 0.11237, Sensitive_Acc : 16.200, Run Time : 17.03 sec
INFO:root:2024-04-27 10:38:00, Train, Epoch : 3, Step : 1370, Loss : 0.37695, Acc : 0.844, Sensitive_Loss : 0.18482, Sensitive_Acc : 16.900, Run Time : 18.13 sec
INFO:root:2024-04-27 10:38:17, Train, Epoch : 3, Step : 1380, Loss : 0.34831, Acc : 0.856, Sensitive_Loss : 0.13157, Sensitive_Acc : 15.100, Run Time : 17.13 sec
INFO:root:2024-04-27 10:38:34, Train, Epoch : 3, Step : 1390, Loss : 0.39968, Acc : 0.816, Sensitive_Loss : 0.10961, Sensitive_Acc : 16.600, Run Time : 17.51 sec
INFO:root:2024-04-27 10:38:51, Train, Epoch : 3, Step : 1400, Loss : 0.34908, Acc : 0.847, Sensitive_Loss : 0.10307, Sensitive_Acc : 15.400, Run Time : 17.09 sec
INFO:root:2024-04-27 10:42:56, Dev, Step : 1400, Loss : 0.42363, Acc : 0.817, Auc : 0.906, Sensitive_Loss : 0.15320, Sensitive_Acc : 16.750, Sensitive_Auc : 0.995, Mean auc: 0.906, Run Time : 244.38 sec
INFO:root:2024-04-27 10:42:56, Best, Step : 1400, Loss : 0.42363, Acc : 0.817, Auc : 0.906, Sensitive_Loss : 0.15320, Sensitive_Acc : 16.750, Sensitive_Auc : 0.995, Best Auc : 0.906
INFO:root:2024-04-27 10:43:10, Train, Epoch : 3, Step : 1410, Loss : 0.29508, Acc : 0.878, Sensitive_Loss : 0.11453, Sensitive_Acc : 14.900, Run Time : 258.49 sec
INFO:root:2024-04-27 10:43:27, Train, Epoch : 3, Step : 1420, Loss : 0.37796, Acc : 0.850, Sensitive_Loss : 0.10153, Sensitive_Acc : 16.600, Run Time : 17.47 sec
INFO:root:2024-04-27 10:43:45, Train, Epoch : 3, Step : 1430, Loss : 0.37513, Acc : 0.856, Sensitive_Loss : 0.11121, Sensitive_Acc : 16.000, Run Time : 17.45 sec
INFO:root:2024-04-27 10:44:02, Train, Epoch : 3, Step : 1440, Loss : 0.30818, Acc : 0.847, Sensitive_Loss : 0.13729, Sensitive_Acc : 16.000, Run Time : 16.98 sec
INFO:root:2024-04-27 10:44:20, Train, Epoch : 3, Step : 1450, Loss : 0.36410, Acc : 0.812, Sensitive_Loss : 0.18846, Sensitive_Acc : 16.600, Run Time : 18.46 sec
INFO:root:2024-04-27 10:44:37, Train, Epoch : 3, Step : 1460, Loss : 0.35525, Acc : 0.831, Sensitive_Loss : 0.10929, Sensitive_Acc : 15.300, Run Time : 17.12 sec
INFO:root:2024-04-27 10:44:56, Train, Epoch : 3, Step : 1470, Loss : 0.33174, Acc : 0.847, Sensitive_Loss : 0.13400, Sensitive_Acc : 16.000, Run Time : 18.48 sec
INFO:root:2024-04-27 10:45:12, Train, Epoch : 3, Step : 1480, Loss : 0.37531, Acc : 0.844, Sensitive_Loss : 0.11460, Sensitive_Acc : 17.100, Run Time : 16.59 sec
INFO:root:2024-04-27 10:45:29, Train, Epoch : 3, Step : 1490, Loss : 0.31150, Acc : 0.863, Sensitive_Loss : 0.10914, Sensitive_Acc : 17.100, Run Time : 16.96 sec
INFO:root:2024-04-27 10:45:47, Train, Epoch : 3, Step : 1500, Loss : 0.39799, Acc : 0.809, Sensitive_Loss : 0.13729, Sensitive_Acc : 16.800, Run Time : 17.42 sec
INFO:root:2024-04-27 10:49:53, Dev, Step : 1500, Loss : 0.42611, Acc : 0.817, Auc : 0.908, Sensitive_Loss : 0.15695, Sensitive_Acc : 16.836, Sensitive_Auc : 0.994, Mean auc: 0.908, Run Time : 245.79 sec
INFO:root:2024-04-27 10:49:53, Best, Step : 1500, Loss : 0.42611, Acc : 0.817, Auc : 0.908, Sensitive_Loss : 0.15695, Sensitive_Acc : 16.836, Sensitive_Auc : 0.994, Best Auc : 0.908
INFO:root:2024-04-27 10:50:06, Train, Epoch : 3, Step : 1510, Loss : 0.35345, Acc : 0.844, Sensitive_Loss : 0.14095, Sensitive_Acc : 15.900, Run Time : 259.63 sec
INFO:root:2024-04-27 10:50:22, Train, Epoch : 3, Step : 1520, Loss : 0.39653, Acc : 0.838, Sensitive_Loss : 0.09361, Sensitive_Acc : 14.800, Run Time : 15.61 sec
INFO:root:2024-04-27 10:50:40, Train, Epoch : 3, Step : 1530, Loss : 0.33528, Acc : 0.838, Sensitive_Loss : 0.12475, Sensitive_Acc : 17.400, Run Time : 17.58 sec
INFO:root:2024-04-27 10:50:57, Train, Epoch : 3, Step : 1540, Loss : 0.33721, Acc : 0.850, Sensitive_Loss : 0.13249, Sensitive_Acc : 16.600, Run Time : 17.52 sec
INFO:root:2024-04-27 10:51:15, Train, Epoch : 3, Step : 1550, Loss : 0.37820, Acc : 0.816, Sensitive_Loss : 0.12772, Sensitive_Acc : 17.400, Run Time : 17.84 sec
INFO:root:2024-04-27 10:51:32, Train, Epoch : 3, Step : 1560, Loss : 0.40892, Acc : 0.816, Sensitive_Loss : 0.10442, Sensitive_Acc : 16.100, Run Time : 16.70 sec
INFO:root:2024-04-27 10:51:50, Train, Epoch : 3, Step : 1570, Loss : 0.35009, Acc : 0.847, Sensitive_Loss : 0.09662, Sensitive_Acc : 16.100, Run Time : 18.57 sec
INFO:root:2024-04-27 10:52:07, Train, Epoch : 3, Step : 1580, Loss : 0.36795, Acc : 0.822, Sensitive_Loss : 0.13742, Sensitive_Acc : 15.700, Run Time : 16.43 sec
INFO:root:2024-04-27 10:52:22, Train, Epoch : 3, Step : 1590, Loss : 0.41336, Acc : 0.806, Sensitive_Loss : 0.13526, Sensitive_Acc : 16.600, Run Time : 15.69 sec
INFO:root:2024-04-27 10:52:42, Train, Epoch : 3, Step : 1600, Loss : 0.31113, Acc : 0.856, Sensitive_Loss : 0.16594, Sensitive_Acc : 18.300, Run Time : 19.72 sec
INFO:root:2024-04-27 10:56:47, Dev, Step : 1600, Loss : 0.42321, Acc : 0.819, Auc : 0.907, Sensitive_Loss : 0.14979, Sensitive_Acc : 16.850, Sensitive_Auc : 0.994, Mean auc: 0.907, Run Time : 244.82 sec
INFO:root:2024-04-27 10:56:59, Train, Epoch : 3, Step : 1610, Loss : 0.31230, Acc : 0.853, Sensitive_Loss : 0.14696, Sensitive_Acc : 19.000, Run Time : 257.10 sec
INFO:root:2024-04-27 10:57:16, Train, Epoch : 3, Step : 1620, Loss : 0.31585, Acc : 0.856, Sensitive_Loss : 0.11306, Sensitive_Acc : 14.800, Run Time : 16.85 sec
INFO:root:2024-04-27 10:57:33, Train, Epoch : 3, Step : 1630, Loss : 0.34188, Acc : 0.844, Sensitive_Loss : 0.12950, Sensitive_Acc : 16.300, Run Time : 17.50 sec
INFO:root:2024-04-27 10:57:51, Train, Epoch : 3, Step : 1640, Loss : 0.43266, Acc : 0.806, Sensitive_Loss : 0.11173, Sensitive_Acc : 15.900, Run Time : 17.27 sec
INFO:root:2024-04-27 10:58:09, Train, Epoch : 3, Step : 1650, Loss : 0.32985, Acc : 0.866, Sensitive_Loss : 0.09322, Sensitive_Acc : 16.400, Run Time : 18.64 sec
INFO:root:2024-04-27 10:58:26, Train, Epoch : 3, Step : 1660, Loss : 0.35067, Acc : 0.841, Sensitive_Loss : 0.16158, Sensitive_Acc : 16.600, Run Time : 16.26 sec
INFO:root:2024-04-27 10:58:43, Train, Epoch : 3, Step : 1670, Loss : 0.35219, Acc : 0.859, Sensitive_Loss : 0.14949, Sensitive_Acc : 16.100, Run Time : 17.07 sec
INFO:root:2024-04-27 10:59:01, Train, Epoch : 3, Step : 1680, Loss : 0.35301, Acc : 0.825, Sensitive_Loss : 0.12972, Sensitive_Acc : 16.100, Run Time : 18.66 sec
INFO:root:2024-04-27 10:59:18, Train, Epoch : 3, Step : 1690, Loss : 0.38333, Acc : 0.844, Sensitive_Loss : 0.15715, Sensitive_Acc : 17.000, Run Time : 16.42 sec
INFO:root:2024-04-27 10:59:36, Train, Epoch : 3, Step : 1700, Loss : 0.34009, Acc : 0.869, Sensitive_Loss : 0.11237, Sensitive_Acc : 16.800, Run Time : 18.11 sec
INFO:root:2024-04-27 11:03:39, Dev, Step : 1700, Loss : 0.40836, Acc : 0.823, Auc : 0.910, Sensitive_Loss : 0.14399, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 243.53 sec
INFO:root:2024-04-27 11:03:40, Best, Step : 1700, Loss : 0.40836, Acc : 0.823, Auc : 0.910, Sensitive_Loss : 0.14399, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Best Auc : 0.910
INFO:root:2024-04-27 11:03:54, Train, Epoch : 3, Step : 1710, Loss : 0.39360, Acc : 0.859, Sensitive_Loss : 0.11891, Sensitive_Acc : 15.900, Run Time : 257.94 sec
INFO:root:2024-04-27 11:04:12, Train, Epoch : 3, Step : 1720, Loss : 0.31901, Acc : 0.859, Sensitive_Loss : 0.11792, Sensitive_Acc : 16.000, Run Time : 18.04 sec
INFO:root:2024-04-27 11:04:28, Train, Epoch : 3, Step : 1730, Loss : 0.32250, Acc : 0.831, Sensitive_Loss : 0.11460, Sensitive_Acc : 17.800, Run Time : 15.62 sec
INFO:root:2024-04-27 11:04:45, Train, Epoch : 3, Step : 1740, Loss : 0.35050, Acc : 0.856, Sensitive_Loss : 0.12076, Sensitive_Acc : 16.600, Run Time : 17.46 sec
INFO:root:2024-04-27 11:05:02, Train, Epoch : 3, Step : 1750, Loss : 0.33659, Acc : 0.850, Sensitive_Loss : 0.12562, Sensitive_Acc : 17.800, Run Time : 17.38 sec
INFO:root:2024-04-27 11:05:21, Train, Epoch : 3, Step : 1760, Loss : 0.33661, Acc : 0.872, Sensitive_Loss : 0.12178, Sensitive_Acc : 15.800, Run Time : 18.26 sec
INFO:root:2024-04-27 11:05:37, Train, Epoch : 3, Step : 1770, Loss : 0.33269, Acc : 0.856, Sensitive_Loss : 0.14115, Sensitive_Acc : 15.600, Run Time : 16.47 sec
INFO:root:2024-04-27 11:05:55, Train, Epoch : 3, Step : 1780, Loss : 0.39781, Acc : 0.831, Sensitive_Loss : 0.10994, Sensitive_Acc : 15.600, Run Time : 18.17 sec
INFO:root:2024-04-27 11:06:12, Train, Epoch : 3, Step : 1790, Loss : 0.33122, Acc : 0.850, Sensitive_Loss : 0.14620, Sensitive_Acc : 17.500, Run Time : 16.31 sec
INFO:root:2024-04-27 11:06:28, Train, Epoch : 3, Step : 1800, Loss : 0.33021, Acc : 0.853, Sensitive_Loss : 0.09538, Sensitive_Acc : 15.800, Run Time : 16.47 sec
INFO:root:2024-04-27 11:10:36, Dev, Step : 1800, Loss : 0.43178, Acc : 0.814, Auc : 0.911, Sensitive_Loss : 0.15426, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992, Mean auc: 0.911, Run Time : 247.68 sec
INFO:root:2024-04-27 11:10:37, Best, Step : 1800, Loss : 0.43178, Acc : 0.814, Auc : 0.911, Sensitive_Loss : 0.15426, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992, Best Auc : 0.911
INFO:root:2024-04-27 11:10:51, Train, Epoch : 3, Step : 1810, Loss : 0.43260, Acc : 0.800, Sensitive_Loss : 0.09042, Sensitive_Acc : 15.500, Run Time : 262.50 sec
INFO:root:2024-04-27 11:11:08, Train, Epoch : 3, Step : 1820, Loss : 0.26104, Acc : 0.903, Sensitive_Loss : 0.11274, Sensitive_Acc : 16.500, Run Time : 17.78 sec
INFO:root:2024-04-27 11:11:25, Train, Epoch : 3, Step : 1830, Loss : 0.42843, Acc : 0.831, Sensitive_Loss : 0.14465, Sensitive_Acc : 16.100, Run Time : 16.26 sec
INFO:root:2024-04-27 11:11:42, Train, Epoch : 3, Step : 1840, Loss : 0.40211, Acc : 0.831, Sensitive_Loss : 0.09817, Sensitive_Acc : 15.300, Run Time : 16.92 sec
INFO:root:2024-04-27 11:11:59, Train, Epoch : 3, Step : 1850, Loss : 0.34136, Acc : 0.863, Sensitive_Loss : 0.09535, Sensitive_Acc : 16.400, Run Time : 17.39 sec
INFO:root:2024-04-27 11:12:16, Train, Epoch : 3, Step : 1860, Loss : 0.27735, Acc : 0.894, Sensitive_Loss : 0.11078, Sensitive_Acc : 15.600, Run Time : 17.52 sec
INFO:root:2024-04-27 11:12:33, Train, Epoch : 3, Step : 1870, Loss : 0.36041, Acc : 0.856, Sensitive_Loss : 0.12871, Sensitive_Acc : 16.100, Run Time : 16.98 sec
INFO:root:2024-04-27 11:16:50
INFO:root:y_pred: [0.11302599 0.81724054 0.12426805 ... 0.6547901  0.01468777 0.91628426]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.60787296e-01 3.31528514e-04 5.51232770e-02 4.33349458e-04
 9.99512076e-01 4.62126359e-03 9.99992967e-01 9.99740303e-01
 1.43814017e-03 6.99305952e-01 9.97041404e-01 9.99902844e-01
 9.92336929e-01 9.70124006e-01 4.20978107e-03 8.39145541e-01
 9.99879241e-01 1.66583899e-03 8.45076919e-01 9.51955795e-01
 9.97544587e-01 1.37201631e-02 9.99780476e-01 9.55608845e-01
 9.98412728e-01 9.97900724e-01 3.60488833e-04 9.99392152e-01
 9.85855043e-01 1.93323001e-01 3.10582649e-02 3.23212832e-01
 6.73015416e-02 1.99576300e-02 6.08657487e-02 4.55992147e-02
 1.66620284e-01 2.34439727e-02 9.98427987e-01 9.95204687e-01
 3.62492749e-04 1.86249017e-04 9.75202143e-01 1.35812548e-03
 9.99962449e-01 9.97244954e-01 9.98641789e-01 9.98880565e-01
 8.11428670e-03 9.97777641e-01 9.99407530e-01 1.14987567e-01
 2.57122964e-01 1.75026921e-03 2.32987851e-03 7.80151859e-02
 1.17204050e-02 5.22008305e-03 1.37602532e-04 5.90244412e-01
 7.64213782e-03 1.74649894e-01 6.77800030e-02 9.75852787e-01
 3.28019172e-01 9.99897242e-01 2.05146428e-02 9.99714673e-01
 9.97878671e-01 6.28980577e-01 8.18994641e-01 6.37768269e-01
 8.15211795e-03 1.33868754e-01 5.47888223e-03 6.77590608e-04
 1.49324606e-03 1.08601011e-01 3.12563451e-03 9.99436080e-01
 9.99541759e-01 1.00737519e-03 5.57169557e-01 1.76269922e-03
 9.23062980e-01 3.10446471e-01 1.54998936e-02 3.46251428e-02
 9.64358151e-01 9.99547422e-01 9.99987602e-01 3.17577496e-02
 5.36832493e-03 9.98002589e-01 3.18369925e-01 2.34512519e-03
 9.85282123e-01 9.99964476e-01 7.00514880e-04 8.23769569e-02
 9.91851032e-01 9.99545515e-01 9.96137202e-01 9.98422146e-01
 1.58471391e-02 2.89356798e-01 9.97160912e-01 9.98318791e-01
 9.79290664e-01 2.24636009e-04 9.73206282e-01 9.99713480e-01
 7.84371346e-02 9.98939931e-01 9.98888075e-01 9.82051492e-01
 5.65104365e-01 9.98333752e-01 4.90776170e-03 8.67576525e-02
 9.98206377e-01 9.99137640e-01 9.33804724e-04 9.75487471e-01
 9.99990821e-01 1.24820195e-01 9.89349246e-01 7.96708558e-03
 4.16578650e-02 9.96846139e-01 9.95347679e-01 9.33467876e-03
 5.66118322e-02 9.90197621e-03 9.99075055e-01 9.90477681e-01
 9.86387134e-01 7.43421447e-03 3.54999118e-02 9.94841397e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 11:16:50, Dev, Step : 1878, Loss : 0.40159, Acc : 0.827, Auc : 0.910, Sensitive_Loss : 0.13065, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 243.69 sec
INFO:root:2024-04-27 11:16:57, Train, Epoch : 4, Step : 1880, Loss : 0.10466, Acc : 0.156, Sensitive_Loss : 0.01379, Sensitive_Acc : 2.700, Run Time : 5.99 sec
INFO:root:2024-04-27 11:17:15, Train, Epoch : 4, Step : 1890, Loss : 0.32747, Acc : 0.866, Sensitive_Loss : 0.14333, Sensitive_Acc : 14.900, Run Time : 18.17 sec
INFO:root:2024-04-27 11:17:31, Train, Epoch : 4, Step : 1900, Loss : 0.34087, Acc : 0.847, Sensitive_Loss : 0.11534, Sensitive_Acc : 15.300, Run Time : 15.86 sec
INFO:root:2024-04-27 11:21:36, Dev, Step : 1900, Loss : 0.39788, Acc : 0.829, Auc : 0.909, Sensitive_Loss : 0.12881, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.909, Run Time : 245.38 sec
INFO:root:2024-04-27 11:21:49, Train, Epoch : 4, Step : 1910, Loss : 0.32617, Acc : 0.872, Sensitive_Loss : 0.14108, Sensitive_Acc : 15.900, Run Time : 257.91 sec
INFO:root:2024-04-27 11:22:07, Train, Epoch : 4, Step : 1920, Loss : 0.32388, Acc : 0.856, Sensitive_Loss : 0.12909, Sensitive_Acc : 15.300, Run Time : 18.31 sec
INFO:root:2024-04-27 11:22:24, Train, Epoch : 4, Step : 1930, Loss : 0.30421, Acc : 0.859, Sensitive_Loss : 0.09927, Sensitive_Acc : 17.000, Run Time : 16.50 sec
INFO:root:2024-04-27 11:22:40, Train, Epoch : 4, Step : 1940, Loss : 0.33583, Acc : 0.831, Sensitive_Loss : 0.10795, Sensitive_Acc : 16.700, Run Time : 16.73 sec
INFO:root:2024-04-27 11:22:59, Train, Epoch : 4, Step : 1950, Loss : 0.29446, Acc : 0.884, Sensitive_Loss : 0.11882, Sensitive_Acc : 17.500, Run Time : 18.16 sec
INFO:root:2024-04-27 11:23:16, Train, Epoch : 4, Step : 1960, Loss : 0.35871, Acc : 0.869, Sensitive_Loss : 0.11965, Sensitive_Acc : 14.800, Run Time : 17.30 sec
INFO:root:2024-04-27 11:23:33, Train, Epoch : 4, Step : 1970, Loss : 0.31538, Acc : 0.881, Sensitive_Loss : 0.13765, Sensitive_Acc : 16.500, Run Time : 16.85 sec
INFO:root:2024-04-27 11:23:49, Train, Epoch : 4, Step : 1980, Loss : 0.32744, Acc : 0.853, Sensitive_Loss : 0.09348, Sensitive_Acc : 15.600, Run Time : 16.79 sec
INFO:root:2024-04-27 11:24:09, Train, Epoch : 4, Step : 1990, Loss : 0.31209, Acc : 0.869, Sensitive_Loss : 0.11225, Sensitive_Acc : 16.400, Run Time : 19.45 sec
INFO:root:2024-04-27 11:24:29, Train, Epoch : 4, Step : 2000, Loss : 0.36771, Acc : 0.859, Sensitive_Loss : 0.12685, Sensitive_Acc : 14.400, Run Time : 20.38 sec
INFO:root:2024-04-27 11:28:33, Dev, Step : 2000, Loss : 0.40433, Acc : 0.826, Auc : 0.913, Sensitive_Loss : 0.13606, Sensitive_Acc : 16.879, Sensitive_Auc : 0.993, Mean auc: 0.913, Run Time : 243.41 sec
INFO:root:2024-04-27 11:28:34, Best, Step : 2000, Loss : 0.40433, Acc : 0.826, Auc : 0.913, Sensitive_Loss : 0.13606, Sensitive_Acc : 16.879, Sensitive_Auc : 0.993, Best Auc : 0.913
INFO:root:2024-04-27 11:28:46, Train, Epoch : 4, Step : 2010, Loss : 0.34260, Acc : 0.878, Sensitive_Loss : 0.13649, Sensitive_Acc : 17.700, Run Time : 256.98 sec
INFO:root:2024-04-27 11:29:03, Train, Epoch : 4, Step : 2020, Loss : 0.32730, Acc : 0.875, Sensitive_Loss : 0.11829, Sensitive_Acc : 15.900, Run Time : 16.40 sec
INFO:root:2024-04-27 11:29:21, Train, Epoch : 4, Step : 2030, Loss : 0.31310, Acc : 0.866, Sensitive_Loss : 0.13626, Sensitive_Acc : 17.500, Run Time : 18.26 sec
INFO:root:2024-04-27 11:29:39, Train, Epoch : 4, Step : 2040, Loss : 0.33221, Acc : 0.866, Sensitive_Loss : 0.11400, Sensitive_Acc : 15.300, Run Time : 17.60 sec
INFO:root:2024-04-27 11:29:56, Train, Epoch : 4, Step : 2050, Loss : 0.32293, Acc : 0.847, Sensitive_Loss : 0.13724, Sensitive_Acc : 15.900, Run Time : 17.19 sec
INFO:root:2024-04-27 11:30:13, Train, Epoch : 4, Step : 2060, Loss : 0.32557, Acc : 0.863, Sensitive_Loss : 0.13121, Sensitive_Acc : 17.400, Run Time : 17.13 sec
INFO:root:2024-04-27 11:30:31, Train, Epoch : 4, Step : 2070, Loss : 0.28001, Acc : 0.891, Sensitive_Loss : 0.11284, Sensitive_Acc : 16.500, Run Time : 17.64 sec
INFO:root:2024-04-27 11:30:48, Train, Epoch : 4, Step : 2080, Loss : 0.37117, Acc : 0.841, Sensitive_Loss : 0.12180, Sensitive_Acc : 16.400, Run Time : 17.98 sec
INFO:root:2024-04-27 11:31:06, Train, Epoch : 4, Step : 2090, Loss : 0.35629, Acc : 0.834, Sensitive_Loss : 0.10079, Sensitive_Acc : 16.300, Run Time : 17.88 sec
INFO:root:2024-04-27 11:31:24, Train, Epoch : 4, Step : 2100, Loss : 0.37245, Acc : 0.834, Sensitive_Loss : 0.11914, Sensitive_Acc : 17.200, Run Time : 17.72 sec
INFO:root:2024-04-27 11:35:30, Dev, Step : 2100, Loss : 0.40330, Acc : 0.827, Auc : 0.911, Sensitive_Loss : 0.13015, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 245.64 sec
INFO:root:2024-04-27 11:35:43, Train, Epoch : 4, Step : 2110, Loss : 0.33850, Acc : 0.859, Sensitive_Loss : 0.14243, Sensitive_Acc : 16.000, Run Time : 258.41 sec
INFO:root:2024-04-27 11:36:00, Train, Epoch : 4, Step : 2120, Loss : 0.31493, Acc : 0.900, Sensitive_Loss : 0.11859, Sensitive_Acc : 16.100, Run Time : 17.91 sec
INFO:root:2024-04-27 11:36:18, Train, Epoch : 4, Step : 2130, Loss : 0.32307, Acc : 0.853, Sensitive_Loss : 0.10756, Sensitive_Acc : 16.400, Run Time : 17.33 sec
INFO:root:2024-04-27 11:36:35, Train, Epoch : 4, Step : 2140, Loss : 0.33192, Acc : 0.850, Sensitive_Loss : 0.14124, Sensitive_Acc : 16.000, Run Time : 17.35 sec
INFO:root:2024-04-27 11:36:52, Train, Epoch : 4, Step : 2150, Loss : 0.35333, Acc : 0.828, Sensitive_Loss : 0.12893, Sensitive_Acc : 16.800, Run Time : 16.93 sec
INFO:root:2024-04-27 11:37:09, Train, Epoch : 4, Step : 2160, Loss : 0.30814, Acc : 0.863, Sensitive_Loss : 0.11869, Sensitive_Acc : 14.800, Run Time : 17.43 sec
INFO:root:2024-04-27 11:37:26, Train, Epoch : 4, Step : 2170, Loss : 0.33177, Acc : 0.863, Sensitive_Loss : 0.11480, Sensitive_Acc : 17.900, Run Time : 16.93 sec
INFO:root:2024-04-27 11:37:44, Train, Epoch : 4, Step : 2180, Loss : 0.33811, Acc : 0.828, Sensitive_Loss : 0.10239, Sensitive_Acc : 15.300, Run Time : 17.72 sec
INFO:root:2024-04-27 11:38:02, Train, Epoch : 4, Step : 2190, Loss : 0.34669, Acc : 0.872, Sensitive_Loss : 0.09539, Sensitive_Acc : 16.800, Run Time : 17.64 sec
INFO:root:2024-04-27 11:38:20, Train, Epoch : 4, Step : 2200, Loss : 0.30758, Acc : 0.853, Sensitive_Loss : 0.08482, Sensitive_Acc : 15.400, Run Time : 18.44 sec
INFO:root:2024-04-27 11:42:24, Dev, Step : 2200, Loss : 0.40629, Acc : 0.826, Auc : 0.912, Sensitive_Loss : 0.12612, Sensitive_Acc : 16.907, Sensitive_Auc : 0.992, Mean auc: 0.912, Run Time : 243.63 sec
INFO:root:2024-04-27 11:42:35, Train, Epoch : 4, Step : 2210, Loss : 0.30921, Acc : 0.859, Sensitive_Loss : 0.10020, Sensitive_Acc : 16.500, Run Time : 255.15 sec
INFO:root:2024-04-27 11:42:53, Train, Epoch : 4, Step : 2220, Loss : 0.36103, Acc : 0.841, Sensitive_Loss : 0.10061, Sensitive_Acc : 17.600, Run Time : 17.74 sec
INFO:root:2024-04-27 11:43:10, Train, Epoch : 4, Step : 2230, Loss : 0.41497, Acc : 0.841, Sensitive_Loss : 0.11394, Sensitive_Acc : 16.300, Run Time : 17.01 sec
INFO:root:2024-04-27 11:43:28, Train, Epoch : 4, Step : 2240, Loss : 0.30261, Acc : 0.872, Sensitive_Loss : 0.11957, Sensitive_Acc : 16.800, Run Time : 17.93 sec
INFO:root:2024-04-27 11:43:45, Train, Epoch : 4, Step : 2250, Loss : 0.32479, Acc : 0.875, Sensitive_Loss : 0.10726, Sensitive_Acc : 15.800, Run Time : 17.29 sec
INFO:root:2024-04-27 11:44:03, Train, Epoch : 4, Step : 2260, Loss : 0.34961, Acc : 0.853, Sensitive_Loss : 0.12970, Sensitive_Acc : 15.300, Run Time : 17.94 sec
INFO:root:2024-04-27 11:44:21, Train, Epoch : 4, Step : 2270, Loss : 0.32361, Acc : 0.847, Sensitive_Loss : 0.13086, Sensitive_Acc : 15.600, Run Time : 17.53 sec
INFO:root:2024-04-27 11:44:38, Train, Epoch : 4, Step : 2280, Loss : 0.36733, Acc : 0.828, Sensitive_Loss : 0.13823, Sensitive_Acc : 16.200, Run Time : 17.61 sec
INFO:root:2024-04-27 11:44:55, Train, Epoch : 4, Step : 2290, Loss : 0.32209, Acc : 0.866, Sensitive_Loss : 0.12457, Sensitive_Acc : 15.300, Run Time : 16.63 sec
INFO:root:2024-04-27 11:45:13, Train, Epoch : 4, Step : 2300, Loss : 0.35923, Acc : 0.838, Sensitive_Loss : 0.12023, Sensitive_Acc : 14.900, Run Time : 17.74 sec
INFO:root:2024-04-27 11:49:19, Dev, Step : 2300, Loss : 0.41228, Acc : 0.826, Auc : 0.912, Sensitive_Loss : 0.12969, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.912, Run Time : 246.44 sec
INFO:root:2024-04-27 11:49:33, Train, Epoch : 4, Step : 2310, Loss : 0.30754, Acc : 0.878, Sensitive_Loss : 0.09424, Sensitive_Acc : 14.600, Run Time : 260.05 sec
INFO:root:2024-04-27 11:49:50, Train, Epoch : 4, Step : 2320, Loss : 0.34104, Acc : 0.853, Sensitive_Loss : 0.15264, Sensitive_Acc : 16.700, Run Time : 16.94 sec
INFO:root:2024-04-27 11:50:08, Train, Epoch : 4, Step : 2330, Loss : 0.36604, Acc : 0.856, Sensitive_Loss : 0.12367, Sensitive_Acc : 16.800, Run Time : 17.75 sec
INFO:root:2024-04-27 11:50:24, Train, Epoch : 4, Step : 2340, Loss : 0.34053, Acc : 0.863, Sensitive_Loss : 0.10210, Sensitive_Acc : 15.600, Run Time : 16.77 sec
INFO:root:2024-04-27 11:50:41, Train, Epoch : 4, Step : 2350, Loss : 0.35687, Acc : 0.850, Sensitive_Loss : 0.12149, Sensitive_Acc : 17.500, Run Time : 16.76 sec
INFO:root:2024-04-27 11:50:59, Train, Epoch : 4, Step : 2360, Loss : 0.30617, Acc : 0.847, Sensitive_Loss : 0.12759, Sensitive_Acc : 16.300, Run Time : 18.15 sec
INFO:root:2024-04-27 11:51:16, Train, Epoch : 4, Step : 2370, Loss : 0.34044, Acc : 0.838, Sensitive_Loss : 0.12459, Sensitive_Acc : 15.300, Run Time : 17.04 sec
INFO:root:2024-04-27 11:51:33, Train, Epoch : 4, Step : 2380, Loss : 0.33061, Acc : 0.884, Sensitive_Loss : 0.10331, Sensitive_Acc : 16.100, Run Time : 16.73 sec
INFO:root:2024-04-27 11:51:50, Train, Epoch : 4, Step : 2390, Loss : 0.34467, Acc : 0.847, Sensitive_Loss : 0.14276, Sensitive_Acc : 17.200, Run Time : 17.11 sec
INFO:root:2024-04-27 11:52:07, Train, Epoch : 4, Step : 2400, Loss : 0.28565, Acc : 0.878, Sensitive_Loss : 0.11841, Sensitive_Acc : 16.600, Run Time : 16.67 sec
INFO:root:2024-04-27 11:56:13, Dev, Step : 2400, Loss : 0.41689, Acc : 0.821, Auc : 0.912, Sensitive_Loss : 0.14306, Sensitive_Acc : 16.807, Sensitive_Auc : 0.993, Mean auc: 0.912, Run Time : 246.04 sec
INFO:root:2024-04-27 11:56:26, Train, Epoch : 4, Step : 2410, Loss : 0.39734, Acc : 0.831, Sensitive_Loss : 0.13765, Sensitive_Acc : 17.100, Run Time : 259.28 sec
INFO:root:2024-04-27 11:56:42, Train, Epoch : 4, Step : 2420, Loss : 0.30513, Acc : 0.881, Sensitive_Loss : 0.09245, Sensitive_Acc : 14.300, Run Time : 16.08 sec
INFO:root:2024-04-27 11:57:01, Train, Epoch : 4, Step : 2430, Loss : 0.33949, Acc : 0.875, Sensitive_Loss : 0.10439, Sensitive_Acc : 17.100, Run Time : 18.94 sec
INFO:root:2024-04-27 11:57:17, Train, Epoch : 4, Step : 2440, Loss : 0.32362, Acc : 0.847, Sensitive_Loss : 0.11013, Sensitive_Acc : 17.000, Run Time : 16.08 sec
INFO:root:2024-04-27 11:57:36, Train, Epoch : 4, Step : 2450, Loss : 0.28508, Acc : 0.859, Sensitive_Loss : 0.11336, Sensitive_Acc : 16.700, Run Time : 19.19 sec
INFO:root:2024-04-27 11:57:55, Train, Epoch : 4, Step : 2460, Loss : 0.35192, Acc : 0.844, Sensitive_Loss : 0.10364, Sensitive_Acc : 16.000, Run Time : 18.30 sec
INFO:root:2024-04-27 11:58:11, Train, Epoch : 4, Step : 2470, Loss : 0.26789, Acc : 0.866, Sensitive_Loss : 0.09723, Sensitive_Acc : 15.300, Run Time : 16.05 sec
INFO:root:2024-04-27 11:58:28, Train, Epoch : 4, Step : 2480, Loss : 0.36000, Acc : 0.819, Sensitive_Loss : 0.13947, Sensitive_Acc : 16.100, Run Time : 17.54 sec
INFO:root:2024-04-27 11:58:45, Train, Epoch : 4, Step : 2490, Loss : 0.34760, Acc : 0.859, Sensitive_Loss : 0.10130, Sensitive_Acc : 17.200, Run Time : 16.96 sec
INFO:root:2024-04-27 11:59:04, Train, Epoch : 4, Step : 2500, Loss : 0.38770, Acc : 0.838, Sensitive_Loss : 0.15944, Sensitive_Acc : 17.600, Run Time : 18.78 sec
INFO:root:2024-04-27 12:03:08, Dev, Step : 2500, Loss : 0.40807, Acc : 0.825, Auc : 0.912, Sensitive_Loss : 0.13256, Sensitive_Acc : 16.807, Sensitive_Auc : 0.992, Mean auc: 0.912, Run Time : 244.20 sec
INFO:root:2024-04-27 12:07:12
INFO:root:y_pred: [0.16690473 0.8802559  0.05667037 ... 0.64295334 0.01043462 0.91893226]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [8.6754107e-01 3.1452641e-04 2.9947175e-02 5.3430570e-04 9.9961472e-01
 1.5407145e-03 9.9998474e-01 9.9984908e-01 7.6729164e-04 7.7472091e-01
 9.9852496e-01 9.9994099e-01 9.9406189e-01 9.6953446e-01 3.0459764e-03
 8.4411341e-01 9.9987710e-01 9.1693201e-04 7.3313755e-01 9.3292338e-01
 9.9860018e-01 1.8916652e-02 9.9977940e-01 9.5942712e-01 9.9779487e-01
 9.9808550e-01 1.0752567e-04 9.9952555e-01 9.7860932e-01 1.0758488e-01
 1.8736377e-02 1.6583252e-01 9.2193045e-02 1.4795302e-02 7.0372395e-02
 1.7809318e-02 1.0857469e-01 1.7456539e-02 9.9829191e-01 9.9134177e-01
 1.8647061e-04 1.2590639e-04 9.8308623e-01 1.2422855e-03 9.9991000e-01
 9.9771535e-01 9.9753004e-01 9.9827778e-01 7.9656569e-03 9.9890089e-01
 9.9926704e-01 5.5656593e-02 1.2280004e-01 1.3899403e-03 1.0086135e-03
 1.7556483e-02 5.6702038e-03 3.1702241e-03 6.2487481e-05 5.0076085e-01
 2.3878762e-03 1.4937116e-01 8.3814614e-02 9.8194075e-01 4.3192923e-01
 9.9990845e-01 8.7421276e-03 9.9975902e-01 9.9828279e-01 8.0377364e-01
 8.2693249e-01 5.8593869e-01 3.4184481e-03 6.6590808e-02 3.3140806e-03
 8.5272221e-04 9.0991688e-04 6.8239361e-02 1.3779111e-03 9.9964476e-01
 9.9893349e-01 3.9992959e-04 4.5033365e-01 8.4111048e-04 9.1315311e-01
 2.6968098e-01 1.0888454e-02 1.1452568e-02 9.6208656e-01 9.9973077e-01
 9.9998534e-01 4.3040834e-02 4.7057383e-03 9.9756348e-01 2.9947874e-01
 1.3061564e-03 9.9235862e-01 9.9990284e-01 4.8605821e-04 4.4418689e-02
 9.9143970e-01 9.9925441e-01 9.9757522e-01 9.9609959e-01 1.0649963e-02
 1.7477781e-01 9.9621081e-01 9.9930382e-01 9.7090214e-01 1.8098269e-04
 9.8580837e-01 9.9950731e-01 7.0959382e-02 9.9821174e-01 9.9956125e-01
 9.8805529e-01 6.1060470e-01 9.9861979e-01 4.6882727e-03 3.7771337e-02
 9.9827230e-01 9.9913472e-01 5.0667697e-04 9.8348558e-01 9.9998164e-01
 1.2632251e-01 9.8568475e-01 3.8341882e-03 4.6883203e-02 9.9803454e-01
 9.9688703e-01 3.3952694e-03 3.5491336e-02 7.7625448e-03 9.9869293e-01
 9.8927432e-01 9.8789471e-01 2.8895603e-03 1.6928725e-02 9.9622899e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 12:07:12, Dev, Step : 2504, Loss : 0.39899, Acc : 0.830, Auc : 0.911, Sensitive_Loss : 0.11828, Sensitive_Acc : 16.907, Sensitive_Auc : 0.992, Mean auc: 0.911, Run Time : 242.22 sec
INFO:root:2024-04-27 12:07:26, Train, Epoch : 5, Step : 2510, Loss : 0.20965, Acc : 0.509, Sensitive_Loss : 0.06561, Sensitive_Acc : 9.400, Run Time : 12.31 sec
INFO:root:2024-04-27 12:07:45, Train, Epoch : 5, Step : 2520, Loss : 0.30435, Acc : 0.887, Sensitive_Loss : 0.10763, Sensitive_Acc : 16.400, Run Time : 19.34 sec
INFO:root:2024-04-27 12:08:03, Train, Epoch : 5, Step : 2530, Loss : 0.31268, Acc : 0.856, Sensitive_Loss : 0.09553, Sensitive_Acc : 18.100, Run Time : 17.68 sec
INFO:root:2024-04-27 12:08:23, Train, Epoch : 5, Step : 2540, Loss : 0.32984, Acc : 0.831, Sensitive_Loss : 0.14680, Sensitive_Acc : 16.300, Run Time : 19.81 sec
INFO:root:2024-04-27 12:08:40, Train, Epoch : 5, Step : 2550, Loss : 0.32900, Acc : 0.859, Sensitive_Loss : 0.11956, Sensitive_Acc : 18.200, Run Time : 17.86 sec
INFO:root:2024-04-27 12:08:58, Train, Epoch : 5, Step : 2560, Loss : 0.28504, Acc : 0.878, Sensitive_Loss : 0.14100, Sensitive_Acc : 15.800, Run Time : 17.89 sec
INFO:root:2024-04-27 12:09:16, Train, Epoch : 5, Step : 2570, Loss : 0.34301, Acc : 0.844, Sensitive_Loss : 0.10322, Sensitive_Acc : 15.400, Run Time : 17.23 sec
INFO:root:2024-04-27 12:09:33, Train, Epoch : 5, Step : 2580, Loss : 0.35391, Acc : 0.847, Sensitive_Loss : 0.14052, Sensitive_Acc : 14.500, Run Time : 17.52 sec
INFO:root:2024-04-27 12:09:51, Train, Epoch : 5, Step : 2590, Loss : 0.29957, Acc : 0.875, Sensitive_Loss : 0.09959, Sensitive_Acc : 16.300, Run Time : 17.73 sec
INFO:root:2024-04-27 12:10:08, Train, Epoch : 5, Step : 2600, Loss : 0.30936, Acc : 0.863, Sensitive_Loss : 0.10678, Sensitive_Acc : 16.800, Run Time : 17.18 sec
INFO:root:2024-04-27 12:14:12, Dev, Step : 2600, Loss : 0.43604, Acc : 0.813, Auc : 0.911, Sensitive_Loss : 0.14232, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 243.85 sec
INFO:root:2024-04-27 12:14:26, Train, Epoch : 5, Step : 2610, Loss : 0.29603, Acc : 0.866, Sensitive_Loss : 0.12891, Sensitive_Acc : 15.700, Run Time : 258.07 sec
INFO:root:2024-04-27 12:14:43, Train, Epoch : 5, Step : 2620, Loss : 0.30588, Acc : 0.887, Sensitive_Loss : 0.16670, Sensitive_Acc : 15.600, Run Time : 16.89 sec
INFO:root:2024-04-27 12:15:00, Train, Epoch : 5, Step : 2630, Loss : 0.31352, Acc : 0.847, Sensitive_Loss : 0.16414, Sensitive_Acc : 16.600, Run Time : 17.31 sec
INFO:root:2024-04-27 12:15:18, Train, Epoch : 5, Step : 2640, Loss : 0.32132, Acc : 0.875, Sensitive_Loss : 0.08167, Sensitive_Acc : 15.600, Run Time : 17.64 sec
INFO:root:2024-04-27 12:15:36, Train, Epoch : 5, Step : 2650, Loss : 0.32896, Acc : 0.850, Sensitive_Loss : 0.12706, Sensitive_Acc : 15.300, Run Time : 18.26 sec
INFO:root:2024-04-27 12:15:54, Train, Epoch : 5, Step : 2660, Loss : 0.32308, Acc : 0.866, Sensitive_Loss : 0.10191, Sensitive_Acc : 16.200, Run Time : 18.15 sec
INFO:root:2024-04-27 12:16:12, Train, Epoch : 5, Step : 2670, Loss : 0.27094, Acc : 0.881, Sensitive_Loss : 0.10919, Sensitive_Acc : 17.300, Run Time : 17.61 sec
INFO:root:2024-04-27 12:16:29, Train, Epoch : 5, Step : 2680, Loss : 0.30979, Acc : 0.869, Sensitive_Loss : 0.11621, Sensitive_Acc : 15.700, Run Time : 17.22 sec
INFO:root:2024-04-27 12:16:46, Train, Epoch : 5, Step : 2690, Loss : 0.29289, Acc : 0.894, Sensitive_Loss : 0.13538, Sensitive_Acc : 16.700, Run Time : 16.97 sec
INFO:root:2024-04-27 12:17:05, Train, Epoch : 5, Step : 2700, Loss : 0.28278, Acc : 0.891, Sensitive_Loss : 0.11222, Sensitive_Acc : 16.400, Run Time : 18.98 sec
INFO:root:2024-04-27 12:21:08, Dev, Step : 2700, Loss : 0.40275, Acc : 0.826, Auc : 0.911, Sensitive_Loss : 0.14097, Sensitive_Acc : 16.879, Sensitive_Auc : 0.992, Mean auc: 0.911, Run Time : 242.76 sec
INFO:root:2024-04-27 12:21:22, Train, Epoch : 5, Step : 2710, Loss : 0.35563, Acc : 0.844, Sensitive_Loss : 0.10820, Sensitive_Acc : 17.400, Run Time : 257.12 sec
INFO:root:2024-04-27 12:21:39, Train, Epoch : 5, Step : 2720, Loss : 0.38402, Acc : 0.797, Sensitive_Loss : 0.14768, Sensitive_Acc : 16.700, Run Time : 17.03 sec
INFO:root:2024-04-27 12:21:58, Train, Epoch : 5, Step : 2730, Loss : 0.29328, Acc : 0.884, Sensitive_Loss : 0.10720, Sensitive_Acc : 16.300, Run Time : 18.38 sec
INFO:root:2024-04-27 12:22:16, Train, Epoch : 5, Step : 2740, Loss : 0.27497, Acc : 0.878, Sensitive_Loss : 0.10820, Sensitive_Acc : 15.300, Run Time : 18.31 sec
INFO:root:2024-04-27 12:22:33, Train, Epoch : 5, Step : 2750, Loss : 0.26068, Acc : 0.881, Sensitive_Loss : 0.10369, Sensitive_Acc : 16.800, Run Time : 17.22 sec
INFO:root:2024-04-27 12:22:50, Train, Epoch : 5, Step : 2760, Loss : 0.27980, Acc : 0.881, Sensitive_Loss : 0.06668, Sensitive_Acc : 15.400, Run Time : 17.21 sec
INFO:root:2024-04-27 12:23:09, Train, Epoch : 5, Step : 2770, Loss : 0.31634, Acc : 0.875, Sensitive_Loss : 0.10508, Sensitive_Acc : 16.600, Run Time : 18.74 sec
INFO:root:2024-04-27 12:23:25, Train, Epoch : 5, Step : 2780, Loss : 0.33307, Acc : 0.853, Sensitive_Loss : 0.11232, Sensitive_Acc : 16.700, Run Time : 15.71 sec
INFO:root:2024-04-27 12:23:44, Train, Epoch : 5, Step : 2790, Loss : 0.28111, Acc : 0.891, Sensitive_Loss : 0.10990, Sensitive_Acc : 16.200, Run Time : 19.21 sec
INFO:root:2024-04-27 12:24:02, Train, Epoch : 5, Step : 2800, Loss : 0.35503, Acc : 0.847, Sensitive_Loss : 0.13039, Sensitive_Acc : 15.500, Run Time : 17.52 sec
INFO:root:2024-04-27 12:28:18, Dev, Step : 2800, Loss : 0.39706, Acc : 0.833, Auc : 0.911, Sensitive_Loss : 0.11626, Sensitive_Acc : 16.907, Sensitive_Auc : 0.994, Mean auc: 0.911, Run Time : 256.45 sec
INFO:root:2024-04-27 12:28:37, Train, Epoch : 5, Step : 2810, Loss : 0.33714, Acc : 0.853, Sensitive_Loss : 0.10969, Sensitive_Acc : 17.500, Run Time : 275.05 sec
INFO:root:2024-04-27 12:28:57, Train, Epoch : 5, Step : 2820, Loss : 0.32510, Acc : 0.859, Sensitive_Loss : 0.09199, Sensitive_Acc : 17.600, Run Time : 20.85 sec
INFO:root:2024-04-27 12:29:14, Train, Epoch : 5, Step : 2830, Loss : 0.31007, Acc : 0.894, Sensitive_Loss : 0.12459, Sensitive_Acc : 17.400, Run Time : 17.00 sec
INFO:root:2024-04-27 12:29:36, Train, Epoch : 5, Step : 2840, Loss : 0.36055, Acc : 0.875, Sensitive_Loss : 0.10582, Sensitive_Acc : 15.400, Run Time : 21.42 sec
INFO:root:2024-04-27 12:29:56, Train, Epoch : 5, Step : 2850, Loss : 0.30236, Acc : 0.856, Sensitive_Loss : 0.08870, Sensitive_Acc : 17.900, Run Time : 20.59 sec
INFO:root:2024-04-27 12:30:13, Train, Epoch : 5, Step : 2860, Loss : 0.30529, Acc : 0.872, Sensitive_Loss : 0.12912, Sensitive_Acc : 15.500, Run Time : 16.43 sec
INFO:root:2024-04-27 12:30:30, Train, Epoch : 5, Step : 2870, Loss : 0.35347, Acc : 0.872, Sensitive_Loss : 0.10006, Sensitive_Acc : 15.800, Run Time : 17.23 sec
INFO:root:2024-04-27 12:30:46, Train, Epoch : 5, Step : 2880, Loss : 0.34523, Acc : 0.878, Sensitive_Loss : 0.08824, Sensitive_Acc : 15.000, Run Time : 16.01 sec
INFO:root:2024-04-27 12:31:05, Train, Epoch : 5, Step : 2890, Loss : 0.29248, Acc : 0.887, Sensitive_Loss : 0.13539, Sensitive_Acc : 17.200, Run Time : 18.46 sec
INFO:root:2024-04-27 12:31:23, Train, Epoch : 5, Step : 2900, Loss : 0.24837, Acc : 0.903, Sensitive_Loss : 0.10559, Sensitive_Acc : 15.400, Run Time : 18.04 sec
INFO:root:2024-04-27 12:35:26, Dev, Step : 2900, Loss : 0.41482, Acc : 0.824, Auc : 0.909, Sensitive_Loss : 0.14257, Sensitive_Acc : 16.850, Sensitive_Auc : 0.994, Mean auc: 0.909, Run Time : 243.41 sec
INFO:root:2024-04-27 12:35:38, Train, Epoch : 5, Step : 2910, Loss : 0.26161, Acc : 0.891, Sensitive_Loss : 0.09801, Sensitive_Acc : 14.600, Run Time : 255.84 sec
INFO:root:2024-04-27 12:35:56, Train, Epoch : 5, Step : 2920, Loss : 0.22419, Acc : 0.878, Sensitive_Loss : 0.14454, Sensitive_Acc : 16.400, Run Time : 17.72 sec
INFO:root:2024-04-27 12:36:11, Train, Epoch : 5, Step : 2930, Loss : 0.34653, Acc : 0.825, Sensitive_Loss : 0.11937, Sensitive_Acc : 14.600, Run Time : 15.03 sec
INFO:root:2024-04-27 12:36:28, Train, Epoch : 5, Step : 2940, Loss : 0.35919, Acc : 0.831, Sensitive_Loss : 0.08715, Sensitive_Acc : 17.000, Run Time : 17.14 sec
INFO:root:2024-04-27 12:36:45, Train, Epoch : 5, Step : 2950, Loss : 0.33767, Acc : 0.863, Sensitive_Loss : 0.09817, Sensitive_Acc : 15.100, Run Time : 16.44 sec
INFO:root:2024-04-27 12:37:01, Train, Epoch : 5, Step : 2960, Loss : 0.36455, Acc : 0.831, Sensitive_Loss : 0.09596, Sensitive_Acc : 15.700, Run Time : 15.88 sec
INFO:root:2024-04-27 12:37:17, Train, Epoch : 5, Step : 2970, Loss : 0.32309, Acc : 0.859, Sensitive_Loss : 0.08516, Sensitive_Acc : 15.900, Run Time : 16.32 sec
INFO:root:2024-04-27 12:37:33, Train, Epoch : 5, Step : 2980, Loss : 0.31958, Acc : 0.875, Sensitive_Loss : 0.11202, Sensitive_Acc : 15.800, Run Time : 15.99 sec
INFO:root:2024-04-27 12:37:50, Train, Epoch : 5, Step : 2990, Loss : 0.32311, Acc : 0.872, Sensitive_Loss : 0.09207, Sensitive_Acc : 16.900, Run Time : 16.87 sec
INFO:root:2024-04-27 12:38:06, Train, Epoch : 5, Step : 3000, Loss : 0.31340, Acc : 0.875, Sensitive_Loss : 0.11373, Sensitive_Acc : 14.700, Run Time : 15.95 sec
INFO:root:2024-04-27 12:42:11, Dev, Step : 3000, Loss : 0.41200, Acc : 0.821, Auc : 0.911, Sensitive_Loss : 0.13689, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 244.73 sec
INFO:root:2024-04-27 12:42:23, Train, Epoch : 5, Step : 3010, Loss : 0.35304, Acc : 0.853, Sensitive_Loss : 0.11476, Sensitive_Acc : 17.200, Run Time : 257.63 sec
INFO:root:2024-04-27 12:42:41, Train, Epoch : 5, Step : 3020, Loss : 0.32475, Acc : 0.875, Sensitive_Loss : 0.10409, Sensitive_Acc : 16.000, Run Time : 17.93 sec
INFO:root:2024-04-27 12:43:00, Train, Epoch : 5, Step : 3030, Loss : 0.26363, Acc : 0.875, Sensitive_Loss : 0.10403, Sensitive_Acc : 16.400, Run Time : 18.58 sec
INFO:root:2024-04-27 12:43:17, Train, Epoch : 5, Step : 3040, Loss : 0.31713, Acc : 0.834, Sensitive_Loss : 0.09889, Sensitive_Acc : 14.700, Run Time : 17.50 sec
INFO:root:2024-04-27 12:43:35, Train, Epoch : 5, Step : 3050, Loss : 0.33164, Acc : 0.853, Sensitive_Loss : 0.12779, Sensitive_Acc : 15.300, Run Time : 17.58 sec
INFO:root:2024-04-27 12:43:53, Train, Epoch : 5, Step : 3060, Loss : 0.36036, Acc : 0.825, Sensitive_Loss : 0.07575, Sensitive_Acc : 16.300, Run Time : 18.01 sec
INFO:root:2024-04-27 12:44:11, Train, Epoch : 5, Step : 3070, Loss : 0.32629, Acc : 0.863, Sensitive_Loss : 0.14757, Sensitive_Acc : 16.200, Run Time : 18.23 sec
INFO:root:2024-04-27 12:44:29, Train, Epoch : 5, Step : 3080, Loss : 0.33395, Acc : 0.866, Sensitive_Loss : 0.13281, Sensitive_Acc : 15.900, Run Time : 18.03 sec
INFO:root:2024-04-27 12:44:47, Train, Epoch : 5, Step : 3090, Loss : 0.30821, Acc : 0.881, Sensitive_Loss : 0.11601, Sensitive_Acc : 15.700, Run Time : 17.53 sec
INFO:root:2024-04-27 12:45:04, Train, Epoch : 5, Step : 3100, Loss : 0.32038, Acc : 0.884, Sensitive_Loss : 0.10974, Sensitive_Acc : 15.600, Run Time : 17.26 sec
INFO:root:2024-04-27 12:49:08, Dev, Step : 3100, Loss : 0.41306, Acc : 0.826, Auc : 0.911, Sensitive_Loss : 0.13259, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 243.82 sec
INFO:root:2024-04-27 12:49:22, Train, Epoch : 5, Step : 3110, Loss : 0.31639, Acc : 0.881, Sensitive_Loss : 0.12749, Sensitive_Acc : 16.600, Run Time : 258.15 sec
INFO:root:2024-04-27 12:49:40, Train, Epoch : 5, Step : 3120, Loss : 0.31693, Acc : 0.887, Sensitive_Loss : 0.10709, Sensitive_Acc : 16.300, Run Time : 17.28 sec
INFO:root:2024-04-27 12:49:56, Train, Epoch : 5, Step : 3130, Loss : 0.32149, Acc : 0.866, Sensitive_Loss : 0.07434, Sensitive_Acc : 15.000, Run Time : 15.98 sec
INFO:root:2024-04-27 12:53:58
INFO:root:y_pred: [0.15361899 0.88097495 0.05489312 ... 0.80068004 0.01365366 0.94454545]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.02682543e-01 3.77197866e-04 2.14692634e-02 3.26586131e-04
 9.98825252e-01 1.19976141e-03 9.99943495e-01 9.99834299e-01
 9.49365844e-04 7.69831657e-01 9.95145023e-01 9.99938369e-01
 9.92038846e-01 9.72908199e-01 1.93595560e-03 8.31675351e-01
 9.99813974e-01 1.36157626e-03 7.12162673e-01 8.99369955e-01
 9.97564316e-01 1.49301132e-02 9.99693632e-01 9.63298321e-01
 9.98192966e-01 9.98550475e-01 1.34677000e-04 9.99192536e-01
 9.78675544e-01 2.04490751e-01 2.07665563e-02 3.96792367e-02
 7.34415874e-02 1.34801157e-02 8.53428245e-02 1.50643615e-02
 5.23662344e-02 1.11936787e-02 9.97784078e-01 9.71798003e-01
 1.04678154e-04 1.57832925e-04 9.75073397e-01 1.47012668e-03
 9.99767005e-01 9.97862160e-01 9.97389972e-01 9.96564209e-01
 1.09346397e-02 9.99270499e-01 9.98645008e-01 4.83881459e-02
 2.94710070e-01 1.93548191e-03 7.57588481e-04 2.67735925e-02
 5.47794951e-03 3.68338940e-03 7.76225206e-05 4.76330459e-01
 2.37136590e-03 2.32140020e-01 5.16234599e-02 9.81221497e-01
 2.59134322e-01 9.99864936e-01 8.51600431e-03 9.99763548e-01
 9.98832643e-01 6.11754715e-01 8.73648822e-01 5.32806575e-01
 3.14046885e-03 1.08827382e-01 3.70308850e-03 1.39317650e-03
 1.16088951e-03 1.45401567e-01 1.02783774e-03 9.99588192e-01
 9.98860240e-01 3.67496192e-04 3.66872728e-01 6.34511467e-04
 7.99540222e-01 9.55239609e-02 5.65730873e-03 8.16253573e-03
 9.71851051e-01 9.99611676e-01 9.99988198e-01 5.32531627e-02
 4.07356024e-03 9.96863723e-01 3.02663296e-01 1.14705565e-03
 9.86759126e-01 9.99849200e-01 6.09632581e-04 2.32374240e-02
 9.93119836e-01 9.99491215e-01 9.94978726e-01 9.93836939e-01
 7.17397360e-03 2.41518226e-02 9.93414640e-01 9.98996079e-01
 9.81713593e-01 2.01576884e-04 9.71881211e-01 9.99435127e-01
 6.74376115e-02 9.95269239e-01 9.99783337e-01 9.90947127e-01
 6.22777700e-01 9.99197423e-01 5.21789491e-03 3.43364067e-02
 9.96858716e-01 9.98715162e-01 2.37359694e-04 9.85264838e-01
 9.99944210e-01 1.37646049e-01 9.71086204e-01 1.56666955e-03
 1.64826009e-02 9.92494822e-01 9.94783700e-01 5.49259176e-03
 5.65657653e-02 8.29914119e-03 9.97705936e-01 9.84774888e-01
 9.81643140e-01 2.82547018e-03 2.10667681e-02 9.94402230e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 12:53:58, Dev, Step : 3130, Loss : 0.40426, Acc : 0.827, Auc : 0.911, Sensitive_Loss : 0.11460, Sensitive_Acc : 16.907, Sensitive_Auc : 0.994, Mean auc: 0.911, Run Time : 241.90 sec
INFO:root:2024-04-27 12:54:22, Train, Epoch : 6, Step : 3140, Loss : 0.28846, Acc : 0.894, Sensitive_Loss : 0.09890, Sensitive_Acc : 16.100, Run Time : 23.00 sec
INFO:root:2024-04-27 12:54:39, Train, Epoch : 6, Step : 3150, Loss : 0.24327, Acc : 0.881, Sensitive_Loss : 0.09476, Sensitive_Acc : 16.300, Run Time : 17.12 sec
INFO:root:2024-04-27 12:54:57, Train, Epoch : 6, Step : 3160, Loss : 0.32792, Acc : 0.847, Sensitive_Loss : 0.11390, Sensitive_Acc : 16.200, Run Time : 18.15 sec
INFO:root:2024-04-27 12:55:15, Train, Epoch : 6, Step : 3170, Loss : 0.31853, Acc : 0.866, Sensitive_Loss : 0.09587, Sensitive_Acc : 17.700, Run Time : 17.85 sec
INFO:root:2024-04-27 12:55:33, Train, Epoch : 6, Step : 3180, Loss : 0.38763, Acc : 0.834, Sensitive_Loss : 0.14040, Sensitive_Acc : 15.200, Run Time : 18.03 sec
INFO:root:2024-04-27 12:55:51, Train, Epoch : 6, Step : 3190, Loss : 0.34077, Acc : 0.872, Sensitive_Loss : 0.10359, Sensitive_Acc : 15.800, Run Time : 18.42 sec
INFO:root:2024-04-27 12:56:09, Train, Epoch : 6, Step : 3200, Loss : 0.29750, Acc : 0.850, Sensitive_Loss : 0.10904, Sensitive_Acc : 16.600, Run Time : 17.80 sec
INFO:root:2024-04-27 13:00:12, Dev, Step : 3200, Loss : 0.40761, Acc : 0.827, Auc : 0.910, Sensitive_Loss : 0.12064, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 243.44 sec
INFO:root:2024-04-27 13:00:27, Train, Epoch : 6, Step : 3210, Loss : 0.27168, Acc : 0.900, Sensitive_Loss : 0.08574, Sensitive_Acc : 17.100, Run Time : 257.92 sec
INFO:root:2024-04-27 13:00:43, Train, Epoch : 6, Step : 3220, Loss : 0.32277, Acc : 0.828, Sensitive_Loss : 0.13822, Sensitive_Acc : 16.100, Run Time : 16.06 sec
INFO:root:2024-04-27 13:01:01, Train, Epoch : 6, Step : 3230, Loss : 0.32617, Acc : 0.847, Sensitive_Loss : 0.11316, Sensitive_Acc : 17.500, Run Time : 17.75 sec
INFO:root:2024-04-27 13:01:19, Train, Epoch : 6, Step : 3240, Loss : 0.31660, Acc : 0.878, Sensitive_Loss : 0.09405, Sensitive_Acc : 15.600, Run Time : 18.46 sec
INFO:root:2024-04-27 13:01:37, Train, Epoch : 6, Step : 3250, Loss : 0.28785, Acc : 0.872, Sensitive_Loss : 0.11188, Sensitive_Acc : 15.500, Run Time : 17.95 sec
INFO:root:2024-04-27 13:01:56, Train, Epoch : 6, Step : 3260, Loss : 0.28422, Acc : 0.884, Sensitive_Loss : 0.08937, Sensitive_Acc : 15.900, Run Time : 18.47 sec
INFO:root:2024-04-27 13:02:13, Train, Epoch : 6, Step : 3270, Loss : 0.26918, Acc : 0.894, Sensitive_Loss : 0.12127, Sensitive_Acc : 16.400, Run Time : 17.24 sec
INFO:root:2024-04-27 13:02:31, Train, Epoch : 6, Step : 3280, Loss : 0.34274, Acc : 0.856, Sensitive_Loss : 0.09510, Sensitive_Acc : 15.500, Run Time : 18.51 sec
INFO:root:2024-04-27 13:02:49, Train, Epoch : 6, Step : 3290, Loss : 0.27152, Acc : 0.884, Sensitive_Loss : 0.09672, Sensitive_Acc : 17.000, Run Time : 17.44 sec
INFO:root:2024-04-27 13:03:06, Train, Epoch : 6, Step : 3300, Loss : 0.27596, Acc : 0.881, Sensitive_Loss : 0.09234, Sensitive_Acc : 16.300, Run Time : 17.71 sec
INFO:root:2024-04-27 13:07:10, Dev, Step : 3300, Loss : 0.41945, Acc : 0.818, Auc : 0.910, Sensitive_Loss : 0.13218, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 243.69 sec
INFO:root:2024-04-27 13:07:23, Train, Epoch : 6, Step : 3310, Loss : 0.26076, Acc : 0.909, Sensitive_Loss : 0.11376, Sensitive_Acc : 15.800, Run Time : 256.52 sec
INFO:root:2024-04-27 13:07:40, Train, Epoch : 6, Step : 3320, Loss : 0.34008, Acc : 0.859, Sensitive_Loss : 0.10074, Sensitive_Acc : 15.300, Run Time : 17.37 sec
INFO:root:2024-04-27 13:07:59, Train, Epoch : 6, Step : 3330, Loss : 0.24362, Acc : 0.922, Sensitive_Loss : 0.12437, Sensitive_Acc : 18.000, Run Time : 18.63 sec
INFO:root:2024-04-27 13:08:18, Train, Epoch : 6, Step : 3340, Loss : 0.26723, Acc : 0.919, Sensitive_Loss : 0.07711, Sensitive_Acc : 15.700, Run Time : 18.84 sec
INFO:root:2024-04-27 13:08:35, Train, Epoch : 6, Step : 3350, Loss : 0.30470, Acc : 0.859, Sensitive_Loss : 0.11018, Sensitive_Acc : 15.900, Run Time : 16.81 sec
INFO:root:2024-04-27 13:08:53, Train, Epoch : 6, Step : 3360, Loss : 0.32617, Acc : 0.869, Sensitive_Loss : 0.10745, Sensitive_Acc : 15.900, Run Time : 17.99 sec
INFO:root:2024-04-27 13:09:11, Train, Epoch : 6, Step : 3370, Loss : 0.32983, Acc : 0.859, Sensitive_Loss : 0.12989, Sensitive_Acc : 16.000, Run Time : 18.16 sec
INFO:root:2024-04-27 13:09:28, Train, Epoch : 6, Step : 3380, Loss : 0.27144, Acc : 0.878, Sensitive_Loss : 0.11261, Sensitive_Acc : 18.000, Run Time : 16.92 sec
INFO:root:2024-04-27 13:09:46, Train, Epoch : 6, Step : 3390, Loss : 0.40526, Acc : 0.825, Sensitive_Loss : 0.09303, Sensitive_Acc : 16.700, Run Time : 18.53 sec
INFO:root:2024-04-27 13:10:04, Train, Epoch : 6, Step : 3400, Loss : 0.30968, Acc : 0.881, Sensitive_Loss : 0.15695, Sensitive_Acc : 16.100, Run Time : 17.90 sec
INFO:root:2024-04-27 13:14:07, Dev, Step : 3400, Loss : 0.43216, Acc : 0.815, Auc : 0.910, Sensitive_Loss : 0.13240, Sensitive_Acc : 16.893, Sensitive_Auc : 0.994, Mean auc: 0.910, Run Time : 242.55 sec
INFO:root:2024-04-27 13:14:20, Train, Epoch : 6, Step : 3410, Loss : 0.27172, Acc : 0.878, Sensitive_Loss : 0.08638, Sensitive_Acc : 17.300, Run Time : 255.77 sec
INFO:root:2024-04-27 13:14:38, Train, Epoch : 6, Step : 3420, Loss : 0.29818, Acc : 0.869, Sensitive_Loss : 0.12800, Sensitive_Acc : 15.900, Run Time : 18.36 sec
INFO:root:2024-04-27 13:14:57, Train, Epoch : 6, Step : 3430, Loss : 0.26720, Acc : 0.891, Sensitive_Loss : 0.10881, Sensitive_Acc : 17.000, Run Time : 18.67 sec
INFO:root:2024-04-27 13:15:15, Train, Epoch : 6, Step : 3440, Loss : 0.25483, Acc : 0.856, Sensitive_Loss : 0.12766, Sensitive_Acc : 15.800, Run Time : 18.24 sec
INFO:root:2024-04-27 13:15:33, Train, Epoch : 6, Step : 3450, Loss : 0.24568, Acc : 0.906, Sensitive_Loss : 0.10815, Sensitive_Acc : 16.000, Run Time : 17.34 sec
INFO:root:2024-04-27 13:15:50, Train, Epoch : 6, Step : 3460, Loss : 0.24418, Acc : 0.894, Sensitive_Loss : 0.13666, Sensitive_Acc : 17.800, Run Time : 17.43 sec
INFO:root:2024-04-27 13:16:08, Train, Epoch : 6, Step : 3470, Loss : 0.27354, Acc : 0.872, Sensitive_Loss : 0.07817, Sensitive_Acc : 15.300, Run Time : 17.55 sec
INFO:root:2024-04-27 13:16:25, Train, Epoch : 6, Step : 3480, Loss : 0.30318, Acc : 0.866, Sensitive_Loss : 0.11765, Sensitive_Acc : 16.000, Run Time : 17.72 sec
INFO:root:2024-04-27 13:16:44, Train, Epoch : 6, Step : 3490, Loss : 0.30214, Acc : 0.878, Sensitive_Loss : 0.12890, Sensitive_Acc : 15.800, Run Time : 18.37 sec
INFO:root:2024-04-27 13:17:01, Train, Epoch : 6, Step : 3500, Loss : 0.27853, Acc : 0.894, Sensitive_Loss : 0.10344, Sensitive_Acc : 15.900, Run Time : 17.22 sec
INFO:root:2024-04-27 13:21:05, Dev, Step : 3500, Loss : 0.43001, Acc : 0.822, Auc : 0.911, Sensitive_Loss : 0.15293, Sensitive_Acc : 16.893, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 243.69 sec
INFO:root:2024-04-27 13:21:19, Train, Epoch : 6, Step : 3510, Loss : 0.33352, Acc : 0.853, Sensitive_Loss : 0.10186, Sensitive_Acc : 16.500, Run Time : 258.00 sec
INFO:root:2024-04-27 13:21:36, Train, Epoch : 6, Step : 3520, Loss : 0.28542, Acc : 0.875, Sensitive_Loss : 0.11419, Sensitive_Acc : 17.300, Run Time : 17.08 sec
INFO:root:2024-04-27 13:21:52, Train, Epoch : 6, Step : 3530, Loss : 0.25693, Acc : 0.894, Sensitive_Loss : 0.07504, Sensitive_Acc : 17.300, Run Time : 16.01 sec
INFO:root:2024-04-27 13:22:11, Train, Epoch : 6, Step : 3540, Loss : 0.28356, Acc : 0.919, Sensitive_Loss : 0.10856, Sensitive_Acc : 15.700, Run Time : 18.74 sec
INFO:root:2024-04-27 13:22:28, Train, Epoch : 6, Step : 3550, Loss : 0.27846, Acc : 0.878, Sensitive_Loss : 0.13298, Sensitive_Acc : 15.600, Run Time : 17.09 sec
INFO:root:2024-04-27 13:22:45, Train, Epoch : 6, Step : 3560, Loss : 0.24833, Acc : 0.909, Sensitive_Loss : 0.10294, Sensitive_Acc : 16.500, Run Time : 17.64 sec
INFO:root:2024-04-27 13:23:04, Train, Epoch : 6, Step : 3570, Loss : 0.32589, Acc : 0.850, Sensitive_Loss : 0.10665, Sensitive_Acc : 16.100, Run Time : 18.53 sec
INFO:root:2024-04-27 13:23:22, Train, Epoch : 6, Step : 3580, Loss : 0.27293, Acc : 0.906, Sensitive_Loss : 0.14380, Sensitive_Acc : 15.800, Run Time : 18.01 sec
INFO:root:2024-04-27 13:23:40, Train, Epoch : 6, Step : 3590, Loss : 0.31178, Acc : 0.850, Sensitive_Loss : 0.12639, Sensitive_Acc : 16.700, Run Time : 18.36 sec
INFO:root:2024-04-27 13:23:57, Train, Epoch : 6, Step : 3600, Loss : 0.27360, Acc : 0.863, Sensitive_Loss : 0.11906, Sensitive_Acc : 16.300, Run Time : 16.75 sec
INFO:root:2024-04-27 13:28:01, Dev, Step : 3600, Loss : 0.43127, Acc : 0.818, Auc : 0.909, Sensitive_Loss : 0.13706, Sensitive_Acc : 16.893, Sensitive_Auc : 0.993, Mean auc: 0.909, Run Time : 244.39 sec
INFO:root:2024-04-27 13:28:15, Train, Epoch : 6, Step : 3610, Loss : 0.31584, Acc : 0.863, Sensitive_Loss : 0.11878, Sensitive_Acc : 16.400, Run Time : 258.19 sec
INFO:root:2024-04-27 13:28:32, Train, Epoch : 6, Step : 3620, Loss : 0.32803, Acc : 0.856, Sensitive_Loss : 0.10322, Sensitive_Acc : 17.000, Run Time : 16.56 sec
INFO:root:2024-04-27 13:28:49, Train, Epoch : 6, Step : 3630, Loss : 0.30061, Acc : 0.881, Sensitive_Loss : 0.11955, Sensitive_Acc : 15.500, Run Time : 17.18 sec
INFO:root:2024-04-27 13:29:08, Train, Epoch : 6, Step : 3640, Loss : 0.34125, Acc : 0.872, Sensitive_Loss : 0.11399, Sensitive_Acc : 17.000, Run Time : 19.13 sec
INFO:root:2024-04-27 13:29:26, Train, Epoch : 6, Step : 3650, Loss : 0.29963, Acc : 0.856, Sensitive_Loss : 0.11117, Sensitive_Acc : 18.100, Run Time : 17.55 sec
INFO:root:2024-04-27 13:29:43, Train, Epoch : 6, Step : 3660, Loss : 0.26562, Acc : 0.872, Sensitive_Loss : 0.15093, Sensitive_Acc : 15.900, Run Time : 17.55 sec
INFO:root:2024-04-27 13:30:00, Train, Epoch : 6, Step : 3670, Loss : 0.23301, Acc : 0.903, Sensitive_Loss : 0.09500, Sensitive_Acc : 16.200, Run Time : 17.07 sec
INFO:root:2024-04-27 13:30:18, Train, Epoch : 6, Step : 3680, Loss : 0.29782, Acc : 0.884, Sensitive_Loss : 0.08857, Sensitive_Acc : 15.600, Run Time : 17.99 sec
INFO:root:2024-04-27 13:30:36, Train, Epoch : 6, Step : 3690, Loss : 0.30783, Acc : 0.866, Sensitive_Loss : 0.11897, Sensitive_Acc : 16.900, Run Time : 17.37 sec
INFO:root:2024-04-27 13:30:55, Train, Epoch : 6, Step : 3700, Loss : 0.32252, Acc : 0.834, Sensitive_Loss : 0.10025, Sensitive_Acc : 17.600, Run Time : 19.04 sec
INFO:root:2024-04-27 13:34:59, Dev, Step : 3700, Loss : 0.45804, Acc : 0.813, Auc : 0.909, Sensitive_Loss : 0.14609, Sensitive_Acc : 16.936, Sensitive_Auc : 0.992, Mean auc: 0.909, Run Time : 244.15 sec
INFO:root:2024-04-27 13:35:13, Train, Epoch : 6, Step : 3710, Loss : 0.34212, Acc : 0.831, Sensitive_Loss : 0.08256, Sensitive_Acc : 16.300, Run Time : 258.25 sec
INFO:root:2024-04-27 13:35:32, Train, Epoch : 6, Step : 3720, Loss : 0.29950, Acc : 0.859, Sensitive_Loss : 0.08370, Sensitive_Acc : 16.300, Run Time : 18.61 sec
INFO:root:2024-04-27 13:35:49, Train, Epoch : 6, Step : 3730, Loss : 0.25759, Acc : 0.891, Sensitive_Loss : 0.09135, Sensitive_Acc : 16.300, Run Time : 17.67 sec
INFO:root:2024-04-27 13:36:08, Train, Epoch : 6, Step : 3740, Loss : 0.33524, Acc : 0.847, Sensitive_Loss : 0.11018, Sensitive_Acc : 15.100, Run Time : 18.53 sec
INFO:root:2024-04-27 13:36:25, Train, Epoch : 6, Step : 3750, Loss : 0.29477, Acc : 0.847, Sensitive_Loss : 0.07119, Sensitive_Acc : 17.400, Run Time : 17.18 sec
INFO:root:2024-04-27 13:40:35
INFO:root:y_pred: [0.13251741 0.8741176  0.0407441  ... 0.6506505  0.00931884 0.94367975]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.41922128e-01 1.34254980e-03 5.06271459e-02 1.16887316e-03
 9.99871731e-01 2.25727912e-03 9.99969482e-01 9.99947071e-01
 3.32861347e-03 8.95516574e-01 9.99414325e-01 9.99984860e-01
 9.98675406e-01 9.88870144e-01 9.21421032e-03 9.49995875e-01
 9.99947548e-01 2.48521729e-03 9.07136917e-01 9.75704730e-01
 9.99100089e-01 1.04121946e-01 9.99852419e-01 9.92406309e-01
 9.99394655e-01 9.99657989e-01 5.48423093e-04 9.99796212e-01
 9.89684224e-01 3.00376475e-01 3.87220979e-02 2.14376301e-01
 1.19464830e-01 3.67481932e-02 2.04614446e-01 1.88156832e-02
 6.34822473e-02 2.00278070e-02 9.99450266e-01 9.94156599e-01
 2.31958940e-04 4.98577952e-04 9.89517629e-01 3.95852514e-03
 9.99967694e-01 9.99268234e-01 9.98772562e-01 9.99706089e-01
 3.32177691e-02 9.99879599e-01 9.99693036e-01 7.73090124e-02
 5.56657076e-01 8.27189069e-03 2.02473090e-03 5.29321954e-02
 9.38341487e-03 2.67286506e-02 2.20751739e-04 8.05936456e-01
 3.32370540e-03 5.39338052e-01 1.96323246e-01 9.96018112e-01
 3.46623838e-01 9.99932408e-01 8.59018322e-03 9.99948859e-01
 9.99442995e-01 8.75170290e-01 9.50399935e-01 8.78895104e-01
 5.02668880e-03 1.51738897e-01 1.27719156e-02 4.71523963e-03
 1.66992180e-03 1.85237691e-01 1.46150228e-03 9.99898076e-01
 9.99459088e-01 1.43491372e-03 8.23477566e-01 1.78365596e-03
 9.19417679e-01 4.04021859e-01 2.51756776e-02 1.67300124e-02
 9.85394835e-01 9.99882817e-01 9.99997616e-01 1.14405222e-01
 1.01027926e-02 9.99450147e-01 6.80636406e-01 1.67196931e-03
 9.94807541e-01 9.99960899e-01 1.61086954e-03 4.28169966e-02
 9.97756422e-01 9.99870420e-01 9.99002516e-01 9.97176409e-01
 6.62291143e-03 1.82827875e-01 9.98243570e-01 9.99748766e-01
 9.91989315e-01 4.49159357e-04 9.89808679e-01 9.99680281e-01
 1.87320173e-01 9.99210954e-01 9.99948144e-01 9.97872353e-01
 8.08977783e-01 9.99640584e-01 1.76461823e-02 6.10150173e-02
 9.99155283e-01 9.99906778e-01 9.87628242e-04 9.94229555e-01
 9.99988794e-01 3.27483267e-01 9.69014883e-01 3.56168137e-03
 1.77146956e-01 9.98903632e-01 9.99105871e-01 1.26199042e-02
 6.92562982e-02 1.74270626e-02 9.99501944e-01 9.94883180e-01
 9.97603238e-01 6.11488242e-03 3.97031009e-02 9.98550236e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 13:40:35, Dev, Step : 3756, Loss : 0.44310, Acc : 0.819, Auc : 0.908, Sensitive_Loss : 0.15673, Sensitive_Acc : 16.950, Sensitive_Auc : 0.993, Mean auc: 0.908, Run Time : 242.03 sec
INFO:root:2024-04-27 13:40:46, Train, Epoch : 7, Step : 3760, Loss : 0.10766, Acc : 0.356, Sensitive_Loss : 0.02208, Sensitive_Acc : 5.700, Run Time : 9.93 sec
INFO:root:2024-04-27 13:41:05, Train, Epoch : 7, Step : 3770, Loss : 0.21960, Acc : 0.903, Sensitive_Loss : 0.15746, Sensitive_Acc : 18.300, Run Time : 19.58 sec
INFO:root:2024-04-27 13:41:24, Train, Epoch : 7, Step : 3780, Loss : 0.27524, Acc : 0.900, Sensitive_Loss : 0.09465, Sensitive_Acc : 16.500, Run Time : 18.63 sec
INFO:root:2024-04-27 13:41:41, Train, Epoch : 7, Step : 3790, Loss : 0.26257, Acc : 0.869, Sensitive_Loss : 0.10104, Sensitive_Acc : 15.100, Run Time : 17.39 sec
INFO:root:2024-04-27 13:42:00, Train, Epoch : 7, Step : 3800, Loss : 0.24509, Acc : 0.863, Sensitive_Loss : 0.13708, Sensitive_Acc : 17.000, Run Time : 18.57 sec
INFO:root:2024-04-27 13:46:04, Dev, Step : 3800, Loss : 0.46274, Acc : 0.815, Auc : 0.911, Sensitive_Loss : 0.14050, Sensitive_Acc : 16.893, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 243.69 sec
INFO:root:2024-04-27 13:46:17, Train, Epoch : 7, Step : 3810, Loss : 0.27799, Acc : 0.866, Sensitive_Loss : 0.14851, Sensitive_Acc : 16.800, Run Time : 257.24 sec
INFO:root:2024-04-27 13:46:36, Train, Epoch : 7, Step : 3820, Loss : 0.24805, Acc : 0.887, Sensitive_Loss : 0.09775, Sensitive_Acc : 16.700, Run Time : 19.24 sec
INFO:root:2024-04-27 13:46:54, Train, Epoch : 7, Step : 3830, Loss : 0.25392, Acc : 0.872, Sensitive_Loss : 0.10044, Sensitive_Acc : 16.500, Run Time : 17.59 sec
INFO:root:2024-04-27 13:47:12, Train, Epoch : 7, Step : 3840, Loss : 0.26746, Acc : 0.878, Sensitive_Loss : 0.09632, Sensitive_Acc : 16.000, Run Time : 17.94 sec
INFO:root:2024-04-27 13:47:29, Train, Epoch : 7, Step : 3850, Loss : 0.28037, Acc : 0.866, Sensitive_Loss : 0.12398, Sensitive_Acc : 15.900, Run Time : 16.75 sec
INFO:root:2024-04-27 13:47:47, Train, Epoch : 7, Step : 3860, Loss : 0.33092, Acc : 0.831, Sensitive_Loss : 0.12334, Sensitive_Acc : 15.900, Run Time : 18.14 sec
INFO:root:2024-04-27 13:48:05, Train, Epoch : 7, Step : 3870, Loss : 0.31448, Acc : 0.872, Sensitive_Loss : 0.10078, Sensitive_Acc : 18.800, Run Time : 17.70 sec
INFO:root:2024-04-27 13:48:23, Train, Epoch : 7, Step : 3880, Loss : 0.23322, Acc : 0.900, Sensitive_Loss : 0.06608, Sensitive_Acc : 15.400, Run Time : 18.10 sec
INFO:root:2024-04-27 13:48:41, Train, Epoch : 7, Step : 3890, Loss : 0.30524, Acc : 0.884, Sensitive_Loss : 0.10078, Sensitive_Acc : 15.700, Run Time : 18.00 sec
INFO:root:2024-04-27 13:48:58, Train, Epoch : 7, Step : 3900, Loss : 0.32303, Acc : 0.859, Sensitive_Loss : 0.09324, Sensitive_Acc : 15.600, Run Time : 16.99 sec
INFO:root:2024-04-27 13:53:01, Dev, Step : 3900, Loss : 0.42334, Acc : 0.823, Auc : 0.910, Sensitive_Loss : 0.13415, Sensitive_Acc : 16.893, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 243.82 sec
INFO:root:2024-04-27 13:53:15, Train, Epoch : 7, Step : 3910, Loss : 0.23947, Acc : 0.906, Sensitive_Loss : 0.09249, Sensitive_Acc : 16.900, Run Time : 256.93 sec
INFO:root:2024-04-27 13:53:33, Train, Epoch : 7, Step : 3920, Loss : 0.23669, Acc : 0.906, Sensitive_Loss : 0.09107, Sensitive_Acc : 15.800, Run Time : 18.05 sec
INFO:root:2024-04-27 13:53:51, Train, Epoch : 7, Step : 3930, Loss : 0.22475, Acc : 0.912, Sensitive_Loss : 0.10778, Sensitive_Acc : 16.800, Run Time : 18.48 sec
INFO:root:2024-04-27 13:54:09, Train, Epoch : 7, Step : 3940, Loss : 0.25348, Acc : 0.884, Sensitive_Loss : 0.07913, Sensitive_Acc : 17.600, Run Time : 18.08 sec
INFO:root:2024-04-27 13:54:27, Train, Epoch : 7, Step : 3950, Loss : 0.28475, Acc : 0.881, Sensitive_Loss : 0.08561, Sensitive_Acc : 16.100, Run Time : 17.46 sec
INFO:root:2024-04-27 13:54:43, Train, Epoch : 7, Step : 3960, Loss : 0.25295, Acc : 0.881, Sensitive_Loss : 0.14483, Sensitive_Acc : 15.200, Run Time : 16.60 sec
INFO:root:2024-04-27 13:55:02, Train, Epoch : 7, Step : 3970, Loss : 0.22884, Acc : 0.903, Sensitive_Loss : 0.09809, Sensitive_Acc : 16.400, Run Time : 19.06 sec
INFO:root:2024-04-27 13:55:20, Train, Epoch : 7, Step : 3980, Loss : 0.22750, Acc : 0.919, Sensitive_Loss : 0.11199, Sensitive_Acc : 17.800, Run Time : 18.00 sec
INFO:root:2024-04-27 13:55:38, Train, Epoch : 7, Step : 3990, Loss : 0.29486, Acc : 0.856, Sensitive_Loss : 0.12237, Sensitive_Acc : 16.700, Run Time : 17.63 sec
INFO:root:2024-04-27 13:55:56, Train, Epoch : 7, Step : 4000, Loss : 0.29645, Acc : 0.863, Sensitive_Loss : 0.11739, Sensitive_Acc : 16.100, Run Time : 17.68 sec
INFO:root:2024-04-27 13:59:59, Dev, Step : 4000, Loss : 0.44540, Acc : 0.817, Auc : 0.909, Sensitive_Loss : 0.15178, Sensitive_Acc : 16.879, Sensitive_Auc : 0.993, Mean auc: 0.909, Run Time : 243.84 sec
INFO:root:2024-04-27 14:00:14, Train, Epoch : 7, Step : 4010, Loss : 0.35193, Acc : 0.847, Sensitive_Loss : 0.10469, Sensitive_Acc : 17.100, Run Time : 258.31 sec
INFO:root:2024-04-27 14:00:31, Train, Epoch : 7, Step : 4020, Loss : 0.35141, Acc : 0.856, Sensitive_Loss : 0.06402, Sensitive_Acc : 16.500, Run Time : 17.48 sec
INFO:root:2024-04-27 14:00:49, Train, Epoch : 7, Step : 4030, Loss : 0.27674, Acc : 0.878, Sensitive_Loss : 0.10765, Sensitive_Acc : 16.800, Run Time : 17.89 sec
INFO:root:2024-04-27 14:01:06, Train, Epoch : 7, Step : 4040, Loss : 0.34201, Acc : 0.856, Sensitive_Loss : 0.11298, Sensitive_Acc : 16.500, Run Time : 16.82 sec
INFO:root:2024-04-27 14:01:25, Train, Epoch : 7, Step : 4050, Loss : 0.25847, Acc : 0.903, Sensitive_Loss : 0.08565, Sensitive_Acc : 16.100, Run Time : 18.71 sec
INFO:root:2024-04-27 14:01:42, Train, Epoch : 7, Step : 4060, Loss : 0.28712, Acc : 0.881, Sensitive_Loss : 0.09636, Sensitive_Acc : 17.300, Run Time : 17.09 sec
INFO:root:2024-04-27 14:02:01, Train, Epoch : 7, Step : 4070, Loss : 0.31186, Acc : 0.869, Sensitive_Loss : 0.09732, Sensitive_Acc : 15.900, Run Time : 19.13 sec
INFO:root:2024-04-27 14:02:18, Train, Epoch : 7, Step : 4080, Loss : 0.25037, Acc : 0.903, Sensitive_Loss : 0.11455, Sensitive_Acc : 16.900, Run Time : 17.51 sec
INFO:root:2024-04-27 14:02:37, Train, Epoch : 7, Step : 4090, Loss : 0.27828, Acc : 0.909, Sensitive_Loss : 0.06995, Sensitive_Acc : 17.000, Run Time : 18.07 sec
INFO:root:2024-04-27 14:02:54, Train, Epoch : 7, Step : 4100, Loss : 0.28137, Acc : 0.878, Sensitive_Loss : 0.08040, Sensitive_Acc : 17.200, Run Time : 17.30 sec
INFO:root:2024-04-27 14:06:57, Dev, Step : 4100, Loss : 0.42046, Acc : 0.827, Auc : 0.910, Sensitive_Loss : 0.12502, Sensitive_Acc : 16.907, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 243.15 sec
INFO:root:2024-04-27 14:07:09, Train, Epoch : 7, Step : 4110, Loss : 0.24681, Acc : 0.900, Sensitive_Loss : 0.09218, Sensitive_Acc : 17.000, Run Time : 255.38 sec
INFO:root:2024-04-27 14:07:29, Train, Epoch : 7, Step : 4120, Loss : 0.29535, Acc : 0.900, Sensitive_Loss : 0.11719, Sensitive_Acc : 16.000, Run Time : 19.56 sec
INFO:root:2024-04-27 14:07:47, Train, Epoch : 7, Step : 4130, Loss : 0.25834, Acc : 0.881, Sensitive_Loss : 0.09187, Sensitive_Acc : 15.400, Run Time : 17.83 sec
INFO:root:2024-04-27 14:08:04, Train, Epoch : 7, Step : 4140, Loss : 0.28591, Acc : 0.863, Sensitive_Loss : 0.10583, Sensitive_Acc : 16.900, Run Time : 17.55 sec
INFO:root:2024-04-27 14:08:22, Train, Epoch : 7, Step : 4150, Loss : 0.28159, Acc : 0.897, Sensitive_Loss : 0.11310, Sensitive_Acc : 17.200, Run Time : 17.96 sec
INFO:root:2024-04-27 14:08:40, Train, Epoch : 7, Step : 4160, Loss : 0.32902, Acc : 0.859, Sensitive_Loss : 0.07816, Sensitive_Acc : 17.000, Run Time : 18.08 sec
INFO:root:2024-04-27 14:08:58, Train, Epoch : 7, Step : 4170, Loss : 0.28056, Acc : 0.863, Sensitive_Loss : 0.08283, Sensitive_Acc : 17.200, Run Time : 17.69 sec
INFO:root:2024-04-27 14:09:15, Train, Epoch : 7, Step : 4180, Loss : 0.27343, Acc : 0.894, Sensitive_Loss : 0.06986, Sensitive_Acc : 15.600, Run Time : 17.46 sec
INFO:root:2024-04-27 14:09:31, Train, Epoch : 7, Step : 4190, Loss : 0.32763, Acc : 0.891, Sensitive_Loss : 0.18737, Sensitive_Acc : 15.500, Run Time : 15.73 sec
INFO:root:2024-04-27 14:09:50, Train, Epoch : 7, Step : 4200, Loss : 0.23682, Acc : 0.897, Sensitive_Loss : 0.12823, Sensitive_Acc : 17.400, Run Time : 19.09 sec
INFO:root:2024-04-27 14:13:54, Dev, Step : 4200, Loss : 0.41325, Acc : 0.830, Auc : 0.909, Sensitive_Loss : 0.12530, Sensitive_Acc : 16.879, Sensitive_Auc : 0.994, Mean auc: 0.909, Run Time : 244.03 sec
INFO:root:2024-04-27 14:14:08, Train, Epoch : 7, Step : 4210, Loss : 0.26341, Acc : 0.881, Sensitive_Loss : 0.11509, Sensitive_Acc : 15.700, Run Time : 257.47 sec
INFO:root:2024-04-27 14:14:26, Train, Epoch : 7, Step : 4220, Loss : 0.28676, Acc : 0.881, Sensitive_Loss : 0.09294, Sensitive_Acc : 14.800, Run Time : 18.20 sec
INFO:root:2024-04-27 14:14:44, Train, Epoch : 7, Step : 4230, Loss : 0.34491, Acc : 0.869, Sensitive_Loss : 0.11585, Sensitive_Acc : 15.600, Run Time : 17.74 sec
INFO:root:2024-04-27 14:15:02, Train, Epoch : 7, Step : 4240, Loss : 0.32329, Acc : 0.875, Sensitive_Loss : 0.11319, Sensitive_Acc : 16.300, Run Time : 17.83 sec
INFO:root:2024-04-27 14:15:19, Train, Epoch : 7, Step : 4250, Loss : 0.26436, Acc : 0.878, Sensitive_Loss : 0.09148, Sensitive_Acc : 17.500, Run Time : 17.66 sec
INFO:root:2024-04-27 14:15:37, Train, Epoch : 7, Step : 4260, Loss : 0.28790, Acc : 0.881, Sensitive_Loss : 0.12650, Sensitive_Acc : 18.100, Run Time : 17.55 sec
INFO:root:2024-04-27 14:15:55, Train, Epoch : 7, Step : 4270, Loss : 0.25135, Acc : 0.900, Sensitive_Loss : 0.11787, Sensitive_Acc : 14.600, Run Time : 18.29 sec
INFO:root:2024-04-27 14:16:12, Train, Epoch : 7, Step : 4280, Loss : 0.27866, Acc : 0.866, Sensitive_Loss : 0.07668, Sensitive_Acc : 17.100, Run Time : 17.37 sec
INFO:root:2024-04-27 14:16:30, Train, Epoch : 7, Step : 4290, Loss : 0.31122, Acc : 0.863, Sensitive_Loss : 0.11571, Sensitive_Acc : 16.900, Run Time : 18.02 sec
INFO:root:2024-04-27 14:16:48, Train, Epoch : 7, Step : 4300, Loss : 0.29186, Acc : 0.863, Sensitive_Loss : 0.12760, Sensitive_Acc : 18.600, Run Time : 17.48 sec
INFO:root:2024-04-27 14:20:52, Dev, Step : 4300, Loss : 0.44750, Acc : 0.820, Auc : 0.909, Sensitive_Loss : 0.13903, Sensitive_Acc : 16.893, Sensitive_Auc : 0.994, Mean auc: 0.909, Run Time : 244.00 sec
INFO:root:2024-04-27 14:21:06, Train, Epoch : 7, Step : 4310, Loss : 0.28205, Acc : 0.891, Sensitive_Loss : 0.11354, Sensitive_Acc : 17.600, Run Time : 257.99 sec
INFO:root:2024-04-27 14:21:24, Train, Epoch : 7, Step : 4320, Loss : 0.25464, Acc : 0.884, Sensitive_Loss : 0.09799, Sensitive_Acc : 17.200, Run Time : 18.48 sec
INFO:root:2024-04-27 14:21:42, Train, Epoch : 7, Step : 4330, Loss : 0.24915, Acc : 0.884, Sensitive_Loss : 0.08418, Sensitive_Acc : 16.400, Run Time : 17.18 sec
INFO:root:2024-04-27 14:21:59, Train, Epoch : 7, Step : 4340, Loss : 0.30079, Acc : 0.891, Sensitive_Loss : 0.11398, Sensitive_Acc : 18.400, Run Time : 17.58 sec
INFO:root:2024-04-27 14:22:18, Train, Epoch : 7, Step : 4350, Loss : 0.28234, Acc : 0.887, Sensitive_Loss : 0.11616, Sensitive_Acc : 14.700, Run Time : 18.49 sec
INFO:root:2024-04-27 14:22:36, Train, Epoch : 7, Step : 4360, Loss : 0.28972, Acc : 0.891, Sensitive_Loss : 0.13406, Sensitive_Acc : 17.600, Run Time : 18.23 sec
INFO:root:2024-04-27 14:22:54, Train, Epoch : 7, Step : 4370, Loss : 0.33551, Acc : 0.834, Sensitive_Loss : 0.12642, Sensitive_Acc : 16.600, Run Time : 17.88 sec
INFO:root:2024-04-27 14:23:10, Train, Epoch : 7, Step : 4380, Loss : 0.29242, Acc : 0.884, Sensitive_Loss : 0.10981, Sensitive_Acc : 18.100, Run Time : 15.98 sec
INFO:root:2024-04-27 14:27:21
INFO:root:y_pred: [0.25195768 0.9592019  0.03175538 ... 0.5323678  0.00809723 0.92339444]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [8.7612218e-01 4.6398761e-04 3.5742011e-02 6.1331608e-04 9.9963546e-01
 1.0855958e-03 9.9994934e-01 9.9987173e-01 1.0856102e-03 8.5040987e-01
 9.9901974e-01 9.9998176e-01 9.9547881e-01 9.5547932e-01 2.7793993e-03
 8.6645782e-01 9.9996328e-01 1.6694758e-03 8.0731606e-01 9.4325751e-01
 9.9837238e-01 5.1010404e-02 9.9986601e-01 9.6752965e-01 9.9773550e-01
 9.9952090e-01 1.7321168e-04 9.9965608e-01 9.3895775e-01 1.1690567e-01
 3.6945380e-02 9.2963845e-02 1.2255392e-01 7.1441129e-02 3.2966301e-02
 2.2324432e-02 4.5356594e-02 1.1490641e-02 9.9872941e-01 9.9113399e-01
 9.4748328e-05 5.4782700e-05 9.8383707e-01 2.5589776e-03 9.9993205e-01
 9.9623162e-01 9.9784744e-01 9.9934191e-01 1.5492254e-02 9.9961311e-01
 9.9943274e-01 3.0869219e-02 1.8376850e-01 3.1833851e-03 7.5619062e-04
 3.1670209e-02 1.1311983e-02 3.5829197e-03 1.0155379e-04 5.8338612e-01
 2.3138116e-03 4.4433248e-01 8.4042668e-02 9.8747814e-01 4.3894097e-01
 9.9987996e-01 1.9578664e-03 9.9990356e-01 9.9896502e-01 7.5851297e-01
 8.8399345e-01 8.4190446e-01 2.8554979e-03 8.6852215e-02 2.2386378e-03
 1.2402679e-03 1.2709905e-03 1.3876241e-01 5.7266804e-04 9.9977261e-01
 9.9876750e-01 1.1317601e-03 7.0130396e-01 2.6276256e-03 8.3410537e-01
 2.3894899e-01 1.4450799e-02 5.2508409e-03 9.7648197e-01 9.9983513e-01
 9.9999237e-01 4.0213909e-02 1.0278275e-02 9.9825627e-01 5.3095120e-01
 6.8448955e-04 9.9680471e-01 9.9984682e-01 8.3106244e-04 1.8000167e-02
 9.9242592e-01 9.9966073e-01 9.9765503e-01 9.9482012e-01 7.7228667e-03
 1.5740910e-01 9.9699414e-01 9.9909496e-01 9.7960025e-01 2.8959484e-04
 9.8359263e-01 9.9857700e-01 1.1927146e-01 9.9895060e-01 9.9994421e-01
 9.9324542e-01 6.0014623e-01 9.9850166e-01 6.8790168e-03 2.9465437e-02
 9.9905103e-01 9.9985087e-01 5.1558751e-04 9.9016541e-01 9.9994123e-01
 1.4619546e-01 9.6157187e-01 3.4712052e-03 7.8497462e-02 9.9727780e-01
 9.9806219e-01 3.2960828e-03 2.7647045e-02 2.4221033e-02 9.9750584e-01
 9.8918569e-01 9.9578696e-01 1.5592352e-03 2.5253186e-02 9.9628723e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 14:27:21, Dev, Step : 4382, Loss : 0.43012, Acc : 0.821, Auc : 0.906, Sensitive_Loss : 0.12468, Sensitive_Acc : 16.879, Sensitive_Auc : 0.993, Mean auc: 0.906, Run Time : 249.54 sec
INFO:root:2024-04-27 14:27:45, Train, Epoch : 8, Step : 4390, Loss : 0.21378, Acc : 0.706, Sensitive_Loss : 0.08388, Sensitive_Acc : 13.000, Run Time : 21.51 sec
INFO:root:2024-04-27 14:28:05, Train, Epoch : 8, Step : 4400, Loss : 0.25223, Acc : 0.916, Sensitive_Loss : 0.08683, Sensitive_Acc : 17.200, Run Time : 20.12 sec
INFO:root:2024-04-27 14:32:15, Dev, Step : 4400, Loss : 0.44287, Acc : 0.817, Auc : 0.907, Sensitive_Loss : 0.14575, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.907, Run Time : 249.67 sec
INFO:root:2024-04-27 14:32:29, Train, Epoch : 8, Step : 4410, Loss : 0.26964, Acc : 0.872, Sensitive_Loss : 0.06354, Sensitive_Acc : 15.200, Run Time : 264.09 sec
INFO:root:2024-04-27 14:32:48, Train, Epoch : 8, Step : 4420, Loss : 0.25564, Acc : 0.897, Sensitive_Loss : 0.11565, Sensitive_Acc : 18.000, Run Time : 18.86 sec
INFO:root:2024-04-27 14:33:05, Train, Epoch : 8, Step : 4430, Loss : 0.27804, Acc : 0.891, Sensitive_Loss : 0.06988, Sensitive_Acc : 15.800, Run Time : 17.18 sec
INFO:root:2024-04-27 14:33:21, Train, Epoch : 8, Step : 4440, Loss : 0.24392, Acc : 0.903, Sensitive_Loss : 0.13545, Sensitive_Acc : 16.200, Run Time : 16.15 sec
INFO:root:2024-04-27 14:33:40, Train, Epoch : 8, Step : 4450, Loss : 0.26023, Acc : 0.894, Sensitive_Loss : 0.08732, Sensitive_Acc : 15.800, Run Time : 18.99 sec
INFO:root:2024-04-27 14:33:58, Train, Epoch : 8, Step : 4460, Loss : 0.25402, Acc : 0.903, Sensitive_Loss : 0.09288, Sensitive_Acc : 15.400, Run Time : 17.50 sec
INFO:root:2024-04-27 14:34:15, Train, Epoch : 8, Step : 4470, Loss : 0.29466, Acc : 0.869, Sensitive_Loss : 0.09277, Sensitive_Acc : 15.200, Run Time : 17.35 sec
INFO:root:2024-04-27 14:34:34, Train, Epoch : 8, Step : 4480, Loss : 0.24855, Acc : 0.900, Sensitive_Loss : 0.09858, Sensitive_Acc : 17.300, Run Time : 18.51 sec
INFO:root:2024-04-27 14:34:52, Train, Epoch : 8, Step : 4490, Loss : 0.27664, Acc : 0.891, Sensitive_Loss : 0.09764, Sensitive_Acc : 17.400, Run Time : 18.10 sec
INFO:root:2024-04-27 14:35:10, Train, Epoch : 8, Step : 4500, Loss : 0.28998, Acc : 0.891, Sensitive_Loss : 0.11523, Sensitive_Acc : 16.800, Run Time : 18.05 sec
INFO:root:2024-04-27 14:39:14, Dev, Step : 4500, Loss : 0.45106, Acc : 0.813, Auc : 0.905, Sensitive_Loss : 0.14533, Sensitive_Acc : 16.950, Sensitive_Auc : 0.993, Mean auc: 0.905, Run Time : 244.68 sec
INFO:root:2024-04-27 14:39:29, Train, Epoch : 8, Step : 4510, Loss : 0.26642, Acc : 0.887, Sensitive_Loss : 0.11991, Sensitive_Acc : 16.000, Run Time : 258.95 sec
INFO:root:2024-04-27 14:39:46, Train, Epoch : 8, Step : 4520, Loss : 0.28407, Acc : 0.859, Sensitive_Loss : 0.10009, Sensitive_Acc : 15.900, Run Time : 17.65 sec
INFO:root:2024-04-27 14:40:04, Train, Epoch : 8, Step : 4530, Loss : 0.22943, Acc : 0.925, Sensitive_Loss : 0.11613, Sensitive_Acc : 16.700, Run Time : 17.47 sec
INFO:root:2024-04-27 14:40:21, Train, Epoch : 8, Step : 4540, Loss : 0.35294, Acc : 0.866, Sensitive_Loss : 0.13986, Sensitive_Acc : 16.800, Run Time : 17.28 sec
INFO:root:2024-04-27 14:40:38, Train, Epoch : 8, Step : 4550, Loss : 0.28806, Acc : 0.875, Sensitive_Loss : 0.10053, Sensitive_Acc : 15.500, Run Time : 16.67 sec
INFO:root:2024-04-27 14:40:56, Train, Epoch : 8, Step : 4560, Loss : 0.26206, Acc : 0.903, Sensitive_Loss : 0.08510, Sensitive_Acc : 17.100, Run Time : 18.54 sec
INFO:root:2024-04-27 14:41:14, Train, Epoch : 8, Step : 4570, Loss : 0.20489, Acc : 0.903, Sensitive_Loss : 0.09372, Sensitive_Acc : 17.500, Run Time : 17.86 sec
INFO:root:2024-04-27 14:41:32, Train, Epoch : 8, Step : 4580, Loss : 0.28538, Acc : 0.878, Sensitive_Loss : 0.09747, Sensitive_Acc : 14.800, Run Time : 17.97 sec
INFO:root:2024-04-27 14:41:50, Train, Epoch : 8, Step : 4590, Loss : 0.28219, Acc : 0.891, Sensitive_Loss : 0.10276, Sensitive_Acc : 14.500, Run Time : 17.94 sec
INFO:root:2024-04-27 14:42:07, Train, Epoch : 8, Step : 4600, Loss : 0.21935, Acc : 0.884, Sensitive_Loss : 0.13945, Sensitive_Acc : 18.100, Run Time : 16.67 sec
INFO:root:2024-04-27 14:46:13, Dev, Step : 4600, Loss : 0.49103, Acc : 0.811, Auc : 0.905, Sensitive_Loss : 0.15533, Sensitive_Acc : 16.907, Sensitive_Auc : 0.993, Mean auc: 0.905, Run Time : 245.73 sec
INFO:root:2024-04-27 14:46:26, Train, Epoch : 8, Step : 4610, Loss : 0.25214, Acc : 0.881, Sensitive_Loss : 0.12581, Sensitive_Acc : 16.600, Run Time : 258.72 sec
INFO:root:2024-04-27 14:46:44, Train, Epoch : 8, Step : 4620, Loss : 0.25247, Acc : 0.903, Sensitive_Loss : 0.09834, Sensitive_Acc : 17.100, Run Time : 18.48 sec
INFO:root:2024-04-27 14:47:02, Train, Epoch : 8, Step : 4630, Loss : 0.32383, Acc : 0.856, Sensitive_Loss : 0.08949, Sensitive_Acc : 15.000, Run Time : 17.70 sec
INFO:root:2024-04-27 14:47:20, Train, Epoch : 8, Step : 4640, Loss : 0.24148, Acc : 0.903, Sensitive_Loss : 0.10382, Sensitive_Acc : 16.000, Run Time : 17.90 sec
INFO:root:2024-04-27 14:47:37, Train, Epoch : 8, Step : 4650, Loss : 0.22613, Acc : 0.912, Sensitive_Loss : 0.09916, Sensitive_Acc : 15.800, Run Time : 17.70 sec
INFO:root:2024-04-27 14:47:53, Train, Epoch : 8, Step : 4660, Loss : 0.25142, Acc : 0.891, Sensitive_Loss : 0.08919, Sensitive_Acc : 14.500, Run Time : 16.12 sec
INFO:root:2024-04-27 14:48:11, Train, Epoch : 8, Step : 4670, Loss : 0.27590, Acc : 0.903, Sensitive_Loss : 0.08162, Sensitive_Acc : 17.000, Run Time : 18.05 sec
INFO:root:2024-04-27 14:48:29, Train, Epoch : 8, Step : 4680, Loss : 0.23861, Acc : 0.887, Sensitive_Loss : 0.10824, Sensitive_Acc : 16.800, Run Time : 17.05 sec
INFO:root:2024-04-27 14:48:48, Train, Epoch : 8, Step : 4690, Loss : 0.20687, Acc : 0.894, Sensitive_Loss : 0.11243, Sensitive_Acc : 15.400, Run Time : 19.34 sec
INFO:root:2024-04-27 14:49:05, Train, Epoch : 8, Step : 4700, Loss : 0.19631, Acc : 0.925, Sensitive_Loss : 0.07488, Sensitive_Acc : 15.100, Run Time : 17.19 sec
INFO:root:2024-04-27 14:53:09, Dev, Step : 4700, Loss : 0.47199, Acc : 0.812, Auc : 0.906, Sensitive_Loss : 0.13752, Sensitive_Acc : 16.850, Sensitive_Auc : 0.992, Mean auc: 0.906, Run Time : 243.91 sec
INFO:root:2024-04-27 14:53:21, Train, Epoch : 8, Step : 4710, Loss : 0.31480, Acc : 0.881, Sensitive_Loss : 0.08567, Sensitive_Acc : 16.200, Run Time : 255.88 sec
INFO:root:2024-04-27 14:53:39, Train, Epoch : 8, Step : 4720, Loss : 0.22329, Acc : 0.903, Sensitive_Loss : 0.11891, Sensitive_Acc : 17.300, Run Time : 18.49 sec
INFO:root:2024-04-27 14:53:58, Train, Epoch : 8, Step : 4730, Loss : 0.24517, Acc : 0.887, Sensitive_Loss : 0.10684, Sensitive_Acc : 16.300, Run Time : 18.95 sec
INFO:root:2024-04-27 14:54:15, Train, Epoch : 8, Step : 4740, Loss : 0.20158, Acc : 0.912, Sensitive_Loss : 0.09107, Sensitive_Acc : 15.400, Run Time : 16.85 sec
INFO:root:2024-04-27 14:54:35, Train, Epoch : 8, Step : 4750, Loss : 0.30295, Acc : 0.869, Sensitive_Loss : 0.08711, Sensitive_Acc : 16.300, Run Time : 19.29 sec
INFO:root:2024-04-27 14:54:52, Train, Epoch : 8, Step : 4760, Loss : 0.28058, Acc : 0.856, Sensitive_Loss : 0.09070, Sensitive_Acc : 15.900, Run Time : 17.83 sec
INFO:root:2024-04-27 14:55:09, Train, Epoch : 8, Step : 4770, Loss : 0.24901, Acc : 0.900, Sensitive_Loss : 0.10883, Sensitive_Acc : 15.800, Run Time : 16.70 sec
INFO:root:2024-04-27 14:55:28, Train, Epoch : 8, Step : 4780, Loss : 0.21505, Acc : 0.897, Sensitive_Loss : 0.11077, Sensitive_Acc : 16.800, Run Time : 19.24 sec
INFO:root:2024-04-27 14:55:46, Train, Epoch : 8, Step : 4790, Loss : 0.26689, Acc : 0.884, Sensitive_Loss : 0.12515, Sensitive_Acc : 15.900, Run Time : 17.64 sec
INFO:root:2024-04-27 14:56:03, Train, Epoch : 8, Step : 4800, Loss : 0.30047, Acc : 0.881, Sensitive_Loss : 0.10086, Sensitive_Acc : 16.000, Run Time : 17.02 sec
INFO:root:2024-04-27 15:00:09, Dev, Step : 4800, Loss : 0.46675, Acc : 0.817, Auc : 0.907, Sensitive_Loss : 0.13871, Sensitive_Acc : 16.879, Sensitive_Auc : 0.993, Mean auc: 0.907, Run Time : 246.02 sec
INFO:root:2024-04-27 15:00:22, Train, Epoch : 8, Step : 4810, Loss : 0.33275, Acc : 0.853, Sensitive_Loss : 0.11105, Sensitive_Acc : 17.400, Run Time : 259.26 sec
INFO:root:2024-04-27 15:00:40, Train, Epoch : 8, Step : 4820, Loss : 0.23804, Acc : 0.887, Sensitive_Loss : 0.10775, Sensitive_Acc : 16.400, Run Time : 17.94 sec
INFO:root:2024-04-27 15:00:57, Train, Epoch : 8, Step : 4830, Loss : 0.24762, Acc : 0.887, Sensitive_Loss : 0.11366, Sensitive_Acc : 17.000, Run Time : 16.87 sec
INFO:root:2024-04-27 15:01:14, Train, Epoch : 8, Step : 4840, Loss : 0.25039, Acc : 0.897, Sensitive_Loss : 0.08389, Sensitive_Acc : 16.300, Run Time : 17.38 sec
INFO:root:2024-04-27 15:01:32, Train, Epoch : 8, Step : 4850, Loss : 0.21629, Acc : 0.928, Sensitive_Loss : 0.09243, Sensitive_Acc : 17.600, Run Time : 17.78 sec
INFO:root:2024-04-27 15:01:51, Train, Epoch : 8, Step : 4860, Loss : 0.25682, Acc : 0.894, Sensitive_Loss : 0.09686, Sensitive_Acc : 16.600, Run Time : 18.34 sec
INFO:root:2024-04-27 15:02:07, Train, Epoch : 8, Step : 4870, Loss : 0.28280, Acc : 0.900, Sensitive_Loss : 0.04996, Sensitive_Acc : 16.200, Run Time : 16.76 sec
INFO:root:2024-04-27 15:02:26, Train, Epoch : 8, Step : 4880, Loss : 0.24617, Acc : 0.891, Sensitive_Loss : 0.09631, Sensitive_Acc : 15.600, Run Time : 18.87 sec
INFO:root:2024-04-27 15:02:45, Train, Epoch : 8, Step : 4890, Loss : 0.24672, Acc : 0.909, Sensitive_Loss : 0.12929, Sensitive_Acc : 15.700, Run Time : 18.55 sec
INFO:root:2024-04-27 15:03:02, Train, Epoch : 8, Step : 4900, Loss : 0.23566, Acc : 0.891, Sensitive_Loss : 0.10152, Sensitive_Acc : 17.600, Run Time : 17.00 sec
INFO:root:2024-04-27 15:07:08, Dev, Step : 4900, Loss : 0.43512, Acc : 0.828, Auc : 0.908, Sensitive_Loss : 0.13355, Sensitive_Acc : 16.879, Sensitive_Auc : 0.994, Mean auc: 0.908, Run Time : 246.53 sec
INFO:root:2024-04-27 15:07:21, Train, Epoch : 8, Step : 4910, Loss : 0.24215, Acc : 0.909, Sensitive_Loss : 0.09355, Sensitive_Acc : 15.600, Run Time : 259.01 sec
INFO:root:2024-04-27 15:07:40, Train, Epoch : 8, Step : 4920, Loss : 0.22176, Acc : 0.891, Sensitive_Loss : 0.06986, Sensitive_Acc : 15.900, Run Time : 19.17 sec
INFO:root:2024-04-27 15:07:57, Train, Epoch : 8, Step : 4930, Loss : 0.19901, Acc : 0.916, Sensitive_Loss : 0.10608, Sensitive_Acc : 15.100, Run Time : 17.34 sec
INFO:root:2024-04-27 15:08:16, Train, Epoch : 8, Step : 4940, Loss : 0.24412, Acc : 0.884, Sensitive_Loss : 0.11549, Sensitive_Acc : 16.000, Run Time : 18.46 sec
INFO:root:2024-04-27 15:08:34, Train, Epoch : 8, Step : 4950, Loss : 0.25595, Acc : 0.891, Sensitive_Loss : 0.09897, Sensitive_Acc : 16.100, Run Time : 18.06 sec
INFO:root:2024-04-27 15:08:52, Train, Epoch : 8, Step : 4960, Loss : 0.27874, Acc : 0.881, Sensitive_Loss : 0.10987, Sensitive_Acc : 16.300, Run Time : 17.76 sec
INFO:root:2024-04-27 15:09:09, Train, Epoch : 8, Step : 4970, Loss : 0.22276, Acc : 0.909, Sensitive_Loss : 0.12644, Sensitive_Acc : 16.800, Run Time : 17.33 sec
INFO:root:2024-04-27 15:09:27, Train, Epoch : 8, Step : 4980, Loss : 0.31469, Acc : 0.878, Sensitive_Loss : 0.08933, Sensitive_Acc : 17.400, Run Time : 18.08 sec
INFO:root:2024-04-27 15:09:45, Train, Epoch : 8, Step : 4990, Loss : 0.24021, Acc : 0.903, Sensitive_Loss : 0.10680, Sensitive_Acc : 15.400, Run Time : 17.92 sec
INFO:root:2024-04-27 15:10:02, Train, Epoch : 8, Step : 5000, Loss : 0.22847, Acc : 0.909, Sensitive_Loss : 0.10161, Sensitive_Acc : 16.800, Run Time : 16.82 sec
INFO:root:2024-04-27 15:14:07, Dev, Step : 5000, Loss : 0.48276, Acc : 0.811, Auc : 0.906, Sensitive_Loss : 0.15491, Sensitive_Acc : 16.950, Sensitive_Auc : 0.994, Mean auc: 0.906, Run Time : 244.95 sec
INFO:root:2024-04-27 15:18:17
INFO:root:y_pred: [0.08338328 0.8940206  0.05701334 ... 0.4842569  0.01026676 0.97021335]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.4839692e-01 1.1238422e-03 8.2606763e-02 5.9867196e-04 9.9996603e-01
 2.0363368e-03 9.9996746e-01 9.9990749e-01 1.4244512e-03 8.4434378e-01
 9.9935645e-01 9.9999523e-01 9.9789774e-01 9.8000103e-01 3.9432505e-03
 9.6356225e-01 9.9997306e-01 4.2344681e-03 9.3395936e-01 9.7095144e-01
 9.9948788e-01 1.0909597e-01 9.9994493e-01 9.8271346e-01 9.9913967e-01
 9.9986100e-01 2.9403003e-04 9.9991441e-01 9.7412068e-01 3.1027082e-01
 3.6957551e-02 7.2397932e-02 1.0803714e-01 8.5300460e-02 8.6662307e-02
 3.0271806e-02 6.9133952e-02 1.7130606e-02 9.9954885e-01 9.8572749e-01
 1.3440644e-04 3.0230024e-04 9.8934722e-01 4.4577629e-03 9.9999189e-01
 9.9935609e-01 9.9883789e-01 9.9974555e-01 2.7672004e-02 9.9986136e-01
 9.9982661e-01 8.6304016e-02 4.2136058e-01 4.7299312e-03 7.6961523e-04
 1.7808584e-02 2.7246477e-02 1.4057441e-02 2.4021674e-04 7.6760793e-01
 5.6248964e-03 6.0753024e-01 1.6628113e-01 9.9508142e-01 3.2346568e-01
 9.9989295e-01 1.7230667e-03 9.9995375e-01 9.9959427e-01 7.7571946e-01
 8.7389642e-01 8.8905573e-01 6.0908650e-03 1.5073931e-01 4.9655540e-03
 3.0131154e-03 1.0361592e-03 3.5889164e-01 1.6913167e-03 9.9991012e-01
 9.9935800e-01 1.1843358e-03 6.7762476e-01 6.9413502e-03 9.5747924e-01
 3.5679984e-01 1.3804482e-02 8.0368668e-03 9.8752069e-01 9.9995470e-01
 9.9999905e-01 4.5001984e-02 3.4057554e-02 9.9908626e-01 5.7712930e-01
 1.6048095e-03 9.9833775e-01 9.9994326e-01 2.4075704e-03 9.3598329e-03
 9.9686986e-01 9.9979061e-01 9.9945718e-01 9.9731064e-01 1.5454118e-02
 2.1196373e-01 9.9718362e-01 9.9969995e-01 9.9369818e-01 5.1585439e-04
 9.9316823e-01 9.9970102e-01 1.8876383e-01 9.9980181e-01 9.9996817e-01
 9.9867934e-01 5.5957097e-01 9.9926251e-01 5.7514291e-03 2.7465157e-02
 9.9965405e-01 9.9992406e-01 1.3769501e-03 9.9456114e-01 9.9998498e-01
 3.0597630e-01 9.7399783e-01 6.6236551e-03 7.6572716e-02 9.9944049e-01
 9.9913919e-01 9.0855304e-03 4.5822732e-02 3.9458983e-02 9.9898404e-01
 9.9607760e-01 9.9903297e-01 5.4439125e-03 3.6282770e-02 9.9891472e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 15:18:17, Dev, Step : 5008, Loss : 0.45571, Acc : 0.818, Auc : 0.906, Sensitive_Loss : 0.14847, Sensitive_Acc : 16.893, Sensitive_Auc : 0.994, Mean auc: 0.906, Run Time : 241.96 sec
INFO:root:2024-04-27 15:18:25, Train, Epoch : 9, Step : 5010, Loss : 0.04243, Acc : 0.178, Sensitive_Loss : 0.02460, Sensitive_Acc : 3.200, Run Time : 6.99 sec
INFO:root:2024-04-27 15:18:42, Train, Epoch : 9, Step : 5020, Loss : 0.27561, Acc : 0.875, Sensitive_Loss : 0.14745, Sensitive_Acc : 15.900, Run Time : 16.92 sec
INFO:root:2024-04-27 15:19:01, Train, Epoch : 9, Step : 5030, Loss : 0.18534, Acc : 0.912, Sensitive_Loss : 0.10307, Sensitive_Acc : 16.200, Run Time : 19.55 sec
INFO:root:2024-04-27 15:19:20, Train, Epoch : 9, Step : 5040, Loss : 0.20549, Acc : 0.938, Sensitive_Loss : 0.08712, Sensitive_Acc : 17.400, Run Time : 18.51 sec
INFO:root:2024-04-27 15:19:37, Train, Epoch : 9, Step : 5050, Loss : 0.21940, Acc : 0.916, Sensitive_Loss : 0.07005, Sensitive_Acc : 15.400, Run Time : 16.92 sec
INFO:root:2024-04-27 15:19:56, Train, Epoch : 9, Step : 5060, Loss : 0.24701, Acc : 0.897, Sensitive_Loss : 0.10370, Sensitive_Acc : 15.900, Run Time : 18.76 sec
INFO:root:2024-04-27 15:20:14, Train, Epoch : 9, Step : 5070, Loss : 0.19136, Acc : 0.938, Sensitive_Loss : 0.09185, Sensitive_Acc : 16.700, Run Time : 18.12 sec
INFO:root:2024-04-27 15:20:31, Train, Epoch : 9, Step : 5080, Loss : 0.19674, Acc : 0.919, Sensitive_Loss : 0.07125, Sensitive_Acc : 16.300, Run Time : 17.58 sec
INFO:root:2024-04-27 15:20:49, Train, Epoch : 9, Step : 5090, Loss : 0.33887, Acc : 0.856, Sensitive_Loss : 0.08609, Sensitive_Acc : 16.800, Run Time : 18.13 sec
INFO:root:2024-04-27 15:21:06, Train, Epoch : 9, Step : 5100, Loss : 0.21326, Acc : 0.928, Sensitive_Loss : 0.11547, Sensitive_Acc : 16.000, Run Time : 16.78 sec
INFO:root:2024-04-27 15:25:11, Dev, Step : 5100, Loss : 0.45448, Acc : 0.817, Auc : 0.904, Sensitive_Loss : 0.15751, Sensitive_Acc : 16.893, Sensitive_Auc : 0.993, Mean auc: 0.904, Run Time : 244.57 sec
INFO:root:2024-04-27 15:25:24, Train, Epoch : 9, Step : 5110, Loss : 0.25015, Acc : 0.909, Sensitive_Loss : 0.08910, Sensitive_Acc : 15.200, Run Time : 257.86 sec
INFO:root:2024-04-27 15:25:41, Train, Epoch : 9, Step : 5120, Loss : 0.26079, Acc : 0.875, Sensitive_Loss : 0.08624, Sensitive_Acc : 17.000, Run Time : 17.32 sec
INFO:root:2024-04-27 15:26:00, Train, Epoch : 9, Step : 5130, Loss : 0.24381, Acc : 0.891, Sensitive_Loss : 0.12321, Sensitive_Acc : 17.000, Run Time : 18.41 sec
INFO:root:2024-04-27 15:26:18, Train, Epoch : 9, Step : 5140, Loss : 0.20895, Acc : 0.903, Sensitive_Loss : 0.12235, Sensitive_Acc : 15.600, Run Time : 18.42 sec
INFO:root:2024-04-27 15:26:36, Train, Epoch : 9, Step : 5150, Loss : 0.19953, Acc : 0.919, Sensitive_Loss : 0.11256, Sensitive_Acc : 16.900, Run Time : 17.72 sec
INFO:root:2024-04-27 15:26:54, Train, Epoch : 9, Step : 5160, Loss : 0.24929, Acc : 0.872, Sensitive_Loss : 0.10692, Sensitive_Acc : 16.600, Run Time : 17.75 sec
INFO:root:2024-04-27 15:27:12, Train, Epoch : 9, Step : 5170, Loss : 0.20131, Acc : 0.903, Sensitive_Loss : 0.10471, Sensitive_Acc : 16.400, Run Time : 17.83 sec
INFO:root:2024-04-27 15:27:29, Train, Epoch : 9, Step : 5180, Loss : 0.25586, Acc : 0.891, Sensitive_Loss : 0.10829, Sensitive_Acc : 15.800, Run Time : 17.84 sec
INFO:root:2024-04-27 15:27:46, Train, Epoch : 9, Step : 5190, Loss : 0.19436, Acc : 0.934, Sensitive_Loss : 0.10145, Sensitive_Acc : 16.600, Run Time : 17.12 sec
INFO:root:2024-04-27 15:28:04, Train, Epoch : 9, Step : 5200, Loss : 0.21633, Acc : 0.919, Sensitive_Loss : 0.07939, Sensitive_Acc : 17.200, Run Time : 17.79 sec
INFO:root:2024-04-27 15:32:09, Dev, Step : 5200, Loss : 0.48306, Acc : 0.812, Auc : 0.903, Sensitive_Loss : 0.15387, Sensitive_Acc : 16.750, Sensitive_Auc : 0.993, Mean auc: 0.903, Run Time : 244.38 sec
INFO:root:2024-04-27 15:32:22, Train, Epoch : 9, Step : 5210, Loss : 0.24150, Acc : 0.884, Sensitive_Loss : 0.11585, Sensitive_Acc : 16.700, Run Time : 257.68 sec
INFO:root:2024-04-27 15:32:39, Train, Epoch : 9, Step : 5220, Loss : 0.20474, Acc : 0.894, Sensitive_Loss : 0.10539, Sensitive_Acc : 16.400, Run Time : 17.45 sec
INFO:root:2024-04-27 15:32:57, Train, Epoch : 9, Step : 5230, Loss : 0.24632, Acc : 0.881, Sensitive_Loss : 0.13043, Sensitive_Acc : 15.800, Run Time : 18.05 sec
INFO:root:2024-04-27 15:33:15, Train, Epoch : 9, Step : 5240, Loss : 0.21002, Acc : 0.928, Sensitive_Loss : 0.08130, Sensitive_Acc : 17.100, Run Time : 17.45 sec
INFO:root:2024-04-27 15:33:33, Train, Epoch : 9, Step : 5250, Loss : 0.25330, Acc : 0.897, Sensitive_Loss : 0.12485, Sensitive_Acc : 16.800, Run Time : 18.32 sec
INFO:root:2024-04-27 15:33:52, Train, Epoch : 9, Step : 5260, Loss : 0.24198, Acc : 0.875, Sensitive_Loss : 0.14866, Sensitive_Acc : 17.300, Run Time : 18.78 sec
INFO:root:2024-04-27 15:34:10, Train, Epoch : 9, Step : 5270, Loss : 0.22127, Acc : 0.916, Sensitive_Loss : 0.12359, Sensitive_Acc : 16.000, Run Time : 17.75 sec
INFO:root:2024-04-27 15:34:27, Train, Epoch : 9, Step : 5280, Loss : 0.29762, Acc : 0.894, Sensitive_Loss : 0.08553, Sensitive_Acc : 16.300, Run Time : 17.24 sec
INFO:root:2024-04-27 15:34:43, Train, Epoch : 9, Step : 5290, Loss : 0.29707, Acc : 0.869, Sensitive_Loss : 0.09195, Sensitive_Acc : 17.100, Run Time : 16.24 sec
INFO:root:2024-04-27 15:35:03, Train, Epoch : 9, Step : 5300, Loss : 0.28968, Acc : 0.903, Sensitive_Loss : 0.10738, Sensitive_Acc : 15.500, Run Time : 20.13 sec
INFO:root:2024-04-27 15:39:06, Dev, Step : 5300, Loss : 0.44652, Acc : 0.820, Auc : 0.902, Sensitive_Loss : 0.14305, Sensitive_Acc : 16.893, Sensitive_Auc : 0.994, Mean auc: 0.902, Run Time : 243.10 sec
INFO:root:2024-04-27 15:39:19, Train, Epoch : 9, Step : 5310, Loss : 0.25162, Acc : 0.875, Sensitive_Loss : 0.10082, Sensitive_Acc : 18.400, Run Time : 255.93 sec
INFO:root:2024-04-27 15:39:38, Train, Epoch : 9, Step : 5320, Loss : 0.28689, Acc : 0.881, Sensitive_Loss : 0.09781, Sensitive_Acc : 16.000, Run Time : 18.73 sec
INFO:root:2024-04-27 15:39:56, Train, Epoch : 9, Step : 5330, Loss : 0.19056, Acc : 0.919, Sensitive_Loss : 0.10100, Sensitive_Acc : 17.100, Run Time : 17.94 sec
INFO:root:2024-04-27 15:40:13, Train, Epoch : 9, Step : 5340, Loss : 0.16190, Acc : 0.953, Sensitive_Loss : 0.11534, Sensitive_Acc : 17.100, Run Time : 16.86 sec
INFO:root:2024-04-27 15:40:33, Train, Epoch : 9, Step : 5350, Loss : 0.25106, Acc : 0.897, Sensitive_Loss : 0.12470, Sensitive_Acc : 16.400, Run Time : 19.66 sec
INFO:root:2024-04-27 15:40:50, Train, Epoch : 9, Step : 5360, Loss : 0.20908, Acc : 0.900, Sensitive_Loss : 0.09013, Sensitive_Acc : 16.500, Run Time : 17.31 sec
INFO:root:2024-04-27 15:41:08, Train, Epoch : 9, Step : 5370, Loss : 0.20572, Acc : 0.900, Sensitive_Loss : 0.10137, Sensitive_Acc : 16.200, Run Time : 18.20 sec
INFO:root:2024-04-27 15:41:26, Train, Epoch : 9, Step : 5380, Loss : 0.26918, Acc : 0.906, Sensitive_Loss : 0.12542, Sensitive_Acc : 16.100, Run Time : 18.38 sec
INFO:root:2024-04-27 15:41:43, Train, Epoch : 9, Step : 5390, Loss : 0.23537, Acc : 0.897, Sensitive_Loss : 0.11211, Sensitive_Acc : 16.600, Run Time : 17.09 sec
INFO:root:2024-04-27 15:42:01, Train, Epoch : 9, Step : 5400, Loss : 0.21174, Acc : 0.903, Sensitive_Loss : 0.09474, Sensitive_Acc : 15.900, Run Time : 17.77 sec
INFO:root:2024-04-27 15:46:06, Dev, Step : 5400, Loss : 0.49508, Acc : 0.809, Auc : 0.902, Sensitive_Loss : 0.13389, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.902, Run Time : 245.07 sec
INFO:root:2024-04-27 15:46:20, Train, Epoch : 9, Step : 5410, Loss : 0.23593, Acc : 0.919, Sensitive_Loss : 0.08866, Sensitive_Acc : 17.400, Run Time : 259.05 sec
INFO:root:2024-04-27 15:46:38, Train, Epoch : 9, Step : 5420, Loss : 0.23678, Acc : 0.912, Sensitive_Loss : 0.07513, Sensitive_Acc : 17.500, Run Time : 17.87 sec
INFO:root:2024-04-27 15:46:55, Train, Epoch : 9, Step : 5430, Loss : 0.20748, Acc : 0.916, Sensitive_Loss : 0.07882, Sensitive_Acc : 17.900, Run Time : 16.40 sec
INFO:root:2024-04-27 15:47:12, Train, Epoch : 9, Step : 5440, Loss : 0.27179, Acc : 0.894, Sensitive_Loss : 0.10810, Sensitive_Acc : 17.200, Run Time : 17.11 sec
INFO:root:2024-04-27 15:47:30, Train, Epoch : 9, Step : 5450, Loss : 0.22250, Acc : 0.919, Sensitive_Loss : 0.11510, Sensitive_Acc : 18.800, Run Time : 18.05 sec
INFO:root:2024-04-27 15:47:47, Train, Epoch : 9, Step : 5460, Loss : 0.20908, Acc : 0.928, Sensitive_Loss : 0.09232, Sensitive_Acc : 15.900, Run Time : 17.06 sec
INFO:root:2024-04-27 15:48:04, Train, Epoch : 9, Step : 5470, Loss : 0.28534, Acc : 0.866, Sensitive_Loss : 0.09601, Sensitive_Acc : 16.400, Run Time : 17.47 sec
INFO:root:2024-04-27 15:48:23, Train, Epoch : 9, Step : 5480, Loss : 0.20165, Acc : 0.931, Sensitive_Loss : 0.11432, Sensitive_Acc : 15.000, Run Time : 18.82 sec
INFO:root:2024-04-27 15:48:41, Train, Epoch : 9, Step : 5490, Loss : 0.19248, Acc : 0.919, Sensitive_Loss : 0.08586, Sensitive_Acc : 15.200, Run Time : 18.11 sec
INFO:root:2024-04-27 15:48:59, Train, Epoch : 9, Step : 5500, Loss : 0.26048, Acc : 0.875, Sensitive_Loss : 0.09695, Sensitive_Acc : 16.500, Run Time : 17.56 sec
INFO:root:2024-04-27 15:53:04, Dev, Step : 5500, Loss : 0.54754, Acc : 0.793, Auc : 0.903, Sensitive_Loss : 0.16759, Sensitive_Acc : 16.907, Sensitive_Auc : 0.993, Mean auc: 0.903, Run Time : 245.61 sec
INFO:root:2024-04-27 15:53:16, Train, Epoch : 9, Step : 5510, Loss : 0.26345, Acc : 0.906, Sensitive_Loss : 0.12543, Sensitive_Acc : 17.600, Run Time : 257.64 sec
INFO:root:2024-04-27 15:53:33, Train, Epoch : 9, Step : 5520, Loss : 0.22811, Acc : 0.872, Sensitive_Loss : 0.13362, Sensitive_Acc : 15.300, Run Time : 17.01 sec
INFO:root:2024-04-27 15:53:52, Train, Epoch : 9, Step : 5530, Loss : 0.26470, Acc : 0.884, Sensitive_Loss : 0.11445, Sensitive_Acc : 17.000, Run Time : 18.95 sec
INFO:root:2024-04-27 15:54:10, Train, Epoch : 9, Step : 5540, Loss : 0.21304, Acc : 0.897, Sensitive_Loss : 0.10519, Sensitive_Acc : 17.900, Run Time : 17.17 sec
INFO:root:2024-04-27 15:54:27, Train, Epoch : 9, Step : 5550, Loss : 0.29175, Acc : 0.887, Sensitive_Loss : 0.11283, Sensitive_Acc : 14.200, Run Time : 17.96 sec
INFO:root:2024-04-27 15:54:45, Train, Epoch : 9, Step : 5560, Loss : 0.20571, Acc : 0.897, Sensitive_Loss : 0.09225, Sensitive_Acc : 15.300, Run Time : 17.30 sec
INFO:root:2024-04-27 15:55:02, Train, Epoch : 9, Step : 5570, Loss : 0.26317, Acc : 0.903, Sensitive_Loss : 0.10907, Sensitive_Acc : 15.600, Run Time : 17.15 sec
INFO:root:2024-04-27 15:55:21, Train, Epoch : 9, Step : 5580, Loss : 0.25365, Acc : 0.878, Sensitive_Loss : 0.15069, Sensitive_Acc : 18.600, Run Time : 19.56 sec
INFO:root:2024-04-27 15:55:38, Train, Epoch : 9, Step : 5590, Loss : 0.30430, Acc : 0.884, Sensitive_Loss : 0.07211, Sensitive_Acc : 14.100, Run Time : 16.86 sec
INFO:root:2024-04-27 15:55:56, Train, Epoch : 9, Step : 5600, Loss : 0.21455, Acc : 0.909, Sensitive_Loss : 0.11867, Sensitive_Acc : 16.800, Run Time : 17.86 sec
INFO:root:2024-04-27 16:00:21, Dev, Step : 5600, Loss : 0.49348, Acc : 0.813, Auc : 0.902, Sensitive_Loss : 0.15354, Sensitive_Acc : 16.893, Sensitive_Auc : 0.993, Mean auc: 0.902, Run Time : 264.94 sec
INFO:root:2024-04-27 16:00:34, Train, Epoch : 9, Step : 5610, Loss : 0.22123, Acc : 0.916, Sensitive_Loss : 0.10335, Sensitive_Acc : 14.400, Run Time : 277.51 sec
INFO:root:2024-04-27 16:00:49, Train, Epoch : 9, Step : 5620, Loss : 0.30130, Acc : 0.872, Sensitive_Loss : 0.07649, Sensitive_Acc : 15.700, Run Time : 15.58 sec
INFO:root:2024-04-27 16:01:06, Train, Epoch : 9, Step : 5630, Loss : 0.24618, Acc : 0.906, Sensitive_Loss : 0.08372, Sensitive_Acc : 16.600, Run Time : 17.03 sec
INFO:root:2024-04-27 16:05:13
INFO:root:y_pred: [0.05251928 0.9870083  0.02592847 ... 0.5526965  0.01324404 0.91551125]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.69574809e-01 1.31128612e-03 2.23770469e-01 4.28731873e-04
 9.99810517e-01 1.09053124e-03 9.99931931e-01 9.99967694e-01
 1.94873346e-03 8.49065900e-01 9.99392748e-01 9.99996781e-01
 9.98295009e-01 9.91567552e-01 2.17978302e-02 9.82314348e-01
 9.99965310e-01 2.37054820e-03 9.43817437e-01 9.49867845e-01
 9.99502063e-01 5.89873157e-02 9.99811351e-01 9.91041481e-01
 9.99147177e-01 9.99846339e-01 4.33539477e-04 9.99926925e-01
 9.83759761e-01 2.08153710e-01 1.06272757e-01 7.90255815e-02
 1.60902321e-01 1.18355557e-01 3.96758951e-02 5.59200160e-02
 1.16278365e-01 5.87697886e-02 9.99196351e-01 9.94701505e-01
 9.88376269e-05 8.94691257e-05 9.91264224e-01 3.78966215e-03
 9.99993324e-01 9.99135673e-01 9.97343004e-01 9.99918580e-01
 4.24252115e-02 9.99955177e-01 9.99778450e-01 9.01337862e-02
 3.43587726e-01 3.65263945e-03 3.87000124e-04 3.16790491e-02
 1.71282589e-02 8.73917714e-03 3.62321211e-04 6.73828959e-01
 7.75543693e-03 4.12924111e-01 4.60231185e-01 9.96287465e-01
 5.55932045e-01 9.99884605e-01 2.73586065e-03 9.99983072e-01
 9.99435484e-01 9.27019000e-01 9.19406593e-01 9.34027970e-01
 7.86490645e-03 1.62605584e-01 9.79588088e-03 3.39670922e-03
 8.75714235e-04 2.70343751e-01 1.86526868e-03 9.99905348e-01
 9.99451101e-01 1.64279295e-03 8.38774920e-01 2.65071611e-03
 9.50868249e-01 3.98371100e-01 3.14309672e-02 8.93239304e-03
 9.91405010e-01 9.99962449e-01 9.99998927e-01 2.68970747e-02
 4.34381180e-02 9.98886406e-01 7.61553824e-01 1.72686600e-03
 9.99489665e-01 9.99915957e-01 1.83529488e-03 1.41984643e-02
 9.97536778e-01 9.99648333e-01 9.99084830e-01 9.96152699e-01
 6.74512656e-03 2.94740379e-01 9.98864055e-01 9.99699235e-01
 9.97675717e-01 6.06083835e-04 9.96026754e-01 9.99400377e-01
 2.14858592e-01 9.99747813e-01 9.99994874e-01 9.99327421e-01
 7.90873230e-01 9.99412060e-01 1.05423536e-02 1.74224339e-02
 9.99403000e-01 9.99948263e-01 7.51056883e-04 9.95003402e-01
 9.99981284e-01 3.67829561e-01 9.84272718e-01 8.21982138e-03
 7.68167600e-02 9.99389768e-01 9.98096883e-01 1.10244155e-02
 5.66909947e-02 1.95690431e-02 9.98701215e-01 9.96153891e-01
 9.98989522e-01 1.69295759e-03 4.05765139e-02 9.97794867e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 16:05:13, Dev, Step : 5634, Loss : 0.48316, Acc : 0.819, Auc : 0.902, Sensitive_Loss : 0.15931, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.902, Run Time : 241.39 sec
INFO:root:2024-04-27 16:05:28, Train, Epoch : 10, Step : 5640, Loss : 0.15737, Acc : 0.512, Sensitive_Loss : 0.05321, Sensitive_Acc : 9.300, Run Time : 14.58 sec
INFO:root:2024-04-27 16:05:47, Train, Epoch : 10, Step : 5650, Loss : 0.21287, Acc : 0.906, Sensitive_Loss : 0.07714, Sensitive_Acc : 16.100, Run Time : 19.08 sec
INFO:root:2024-04-27 16:06:06, Train, Epoch : 10, Step : 5660, Loss : 0.17592, Acc : 0.925, Sensitive_Loss : 0.09000, Sensitive_Acc : 15.600, Run Time : 18.82 sec
INFO:root:2024-04-27 16:06:23, Train, Epoch : 10, Step : 5670, Loss : 0.19346, Acc : 0.900, Sensitive_Loss : 0.09474, Sensitive_Acc : 16.200, Run Time : 16.77 sec
INFO:root:2024-04-27 16:06:41, Train, Epoch : 10, Step : 5680, Loss : 0.21989, Acc : 0.900, Sensitive_Loss : 0.07981, Sensitive_Acc : 17.100, Run Time : 18.12 sec
INFO:root:2024-04-27 16:07:00, Train, Epoch : 10, Step : 5690, Loss : 0.25298, Acc : 0.887, Sensitive_Loss : 0.07911, Sensitive_Acc : 18.100, Run Time : 18.91 sec
INFO:root:2024-04-27 16:07:17, Train, Epoch : 10, Step : 5700, Loss : 0.23478, Acc : 0.906, Sensitive_Loss : 0.11406, Sensitive_Acc : 17.100, Run Time : 17.15 sec
INFO:root:2024-04-27 16:11:22, Dev, Step : 5700, Loss : 0.45817, Acc : 0.824, Auc : 0.904, Sensitive_Loss : 0.13739, Sensitive_Acc : 16.879, Sensitive_Auc : 0.993, Mean auc: 0.904, Run Time : 244.45 sec
INFO:root:2024-04-27 16:11:35, Train, Epoch : 10, Step : 5710, Loss : 0.18482, Acc : 0.944, Sensitive_Loss : 0.08316, Sensitive_Acc : 15.200, Run Time : 257.18 sec
INFO:root:2024-04-27 16:11:51, Train, Epoch : 10, Step : 5720, Loss : 0.22653, Acc : 0.909, Sensitive_Loss : 0.08106, Sensitive_Acc : 15.800, Run Time : 16.76 sec
INFO:root:2024-04-27 16:12:10, Train, Epoch : 10, Step : 5730, Loss : 0.28044, Acc : 0.884, Sensitive_Loss : 0.08000, Sensitive_Acc : 16.200, Run Time : 18.30 sec
INFO:root:2024-04-27 16:12:28, Train, Epoch : 10, Step : 5740, Loss : 0.24246, Acc : 0.900, Sensitive_Loss : 0.07743, Sensitive_Acc : 16.600, Run Time : 18.61 sec
INFO:root:2024-04-27 16:12:45, Train, Epoch : 10, Step : 5750, Loss : 0.23243, Acc : 0.909, Sensitive_Loss : 0.09004, Sensitive_Acc : 16.400, Run Time : 16.84 sec
INFO:root:2024-04-27 16:13:03, Train, Epoch : 10, Step : 5760, Loss : 0.23637, Acc : 0.884, Sensitive_Loss : 0.07715, Sensitive_Acc : 15.500, Run Time : 17.89 sec
INFO:root:2024-04-27 16:13:19, Train, Epoch : 10, Step : 5770, Loss : 0.22818, Acc : 0.900, Sensitive_Loss : 0.07769, Sensitive_Acc : 15.600, Run Time : 16.15 sec
INFO:root:2024-04-27 16:13:37, Train, Epoch : 10, Step : 5780, Loss : 0.17199, Acc : 0.928, Sensitive_Loss : 0.09045, Sensitive_Acc : 16.200, Run Time : 17.99 sec
INFO:root:2024-04-27 16:13:55, Train, Epoch : 10, Step : 5790, Loss : 0.23660, Acc : 0.912, Sensitive_Loss : 0.07095, Sensitive_Acc : 15.800, Run Time : 17.83 sec
INFO:root:2024-04-27 16:14:13, Train, Epoch : 10, Step : 5800, Loss : 0.16905, Acc : 0.922, Sensitive_Loss : 0.13153, Sensitive_Acc : 16.300, Run Time : 18.09 sec
INFO:root:2024-04-27 16:18:18, Dev, Step : 5800, Loss : 0.48967, Acc : 0.814, Auc : 0.902, Sensitive_Loss : 0.16326, Sensitive_Acc : 16.907, Sensitive_Auc : 0.992, Mean auc: 0.902, Run Time : 244.61 sec
INFO:root:2024-04-27 16:18:30, Train, Epoch : 10, Step : 5810, Loss : 0.24278, Acc : 0.900, Sensitive_Loss : 0.09369, Sensitive_Acc : 15.900, Run Time : 256.92 sec
INFO:root:2024-04-27 16:18:48, Train, Epoch : 10, Step : 5820, Loss : 0.19597, Acc : 0.925, Sensitive_Loss : 0.11599, Sensitive_Acc : 16.200, Run Time : 17.99 sec
INFO:root:2024-04-27 16:19:06, Train, Epoch : 10, Step : 5830, Loss : 0.19537, Acc : 0.925, Sensitive_Loss : 0.05783, Sensitive_Acc : 16.400, Run Time : 17.69 sec
INFO:root:2024-04-27 16:19:24, Train, Epoch : 10, Step : 5840, Loss : 0.22392, Acc : 0.897, Sensitive_Loss : 0.08909, Sensitive_Acc : 14.900, Run Time : 18.18 sec
INFO:root:2024-04-27 16:19:42, Train, Epoch : 10, Step : 5850, Loss : 0.16330, Acc : 0.934, Sensitive_Loss : 0.10830, Sensitive_Acc : 16.800, Run Time : 17.81 sec
INFO:root:2024-04-27 16:19:58, Train, Epoch : 10, Step : 5860, Loss : 0.18186, Acc : 0.922, Sensitive_Loss : 0.11751, Sensitive_Acc : 17.600, Run Time : 16.67 sec
INFO:root:2024-04-27 16:20:16, Train, Epoch : 10, Step : 5870, Loss : 0.22228, Acc : 0.897, Sensitive_Loss : 0.11164, Sensitive_Acc : 16.400, Run Time : 17.70 sec
INFO:root:2024-04-27 16:20:35, Train, Epoch : 10, Step : 5880, Loss : 0.21654, Acc : 0.903, Sensitive_Loss : 0.13477, Sensitive_Acc : 18.300, Run Time : 18.62 sec
INFO:root:2024-04-27 16:20:52, Train, Epoch : 10, Step : 5890, Loss : 0.19665, Acc : 0.903, Sensitive_Loss : 0.11901, Sensitive_Acc : 15.400, Run Time : 17.66 sec
INFO:root:2024-04-27 16:21:10, Train, Epoch : 10, Step : 5900, Loss : 0.21779, Acc : 0.900, Sensitive_Loss : 0.09784, Sensitive_Acc : 14.900, Run Time : 17.97 sec
INFO:root:2024-04-27 16:25:14, Dev, Step : 5900, Loss : 0.47767, Acc : 0.817, Auc : 0.902, Sensitive_Loss : 0.14952, Sensitive_Acc : 16.950, Sensitive_Auc : 0.993, Mean auc: 0.902, Run Time : 244.12 sec
INFO:root:2024-04-27 16:25:28, Train, Epoch : 10, Step : 5910, Loss : 0.25102, Acc : 0.894, Sensitive_Loss : 0.11477, Sensitive_Acc : 14.900, Run Time : 257.65 sec
INFO:root:2024-04-27 16:25:44, Train, Epoch : 10, Step : 5920, Loss : 0.22692, Acc : 0.887, Sensitive_Loss : 0.09352, Sensitive_Acc : 15.300, Run Time : 16.38 sec
INFO:root:2024-04-27 16:26:02, Train, Epoch : 10, Step : 5930, Loss : 0.21776, Acc : 0.903, Sensitive_Loss : 0.07892, Sensitive_Acc : 16.400, Run Time : 17.36 sec
INFO:root:2024-04-27 16:26:19, Train, Epoch : 10, Step : 5940, Loss : 0.22560, Acc : 0.919, Sensitive_Loss : 0.08579, Sensitive_Acc : 17.300, Run Time : 17.17 sec
INFO:root:2024-04-27 16:26:41, Train, Epoch : 10, Step : 5950, Loss : 0.21588, Acc : 0.938, Sensitive_Loss : 0.09651, Sensitive_Acc : 15.700, Run Time : 21.75 sec
INFO:root:2024-04-27 16:27:05, Train, Epoch : 10, Step : 5960, Loss : 0.20100, Acc : 0.922, Sensitive_Loss : 0.08585, Sensitive_Acc : 16.600, Run Time : 24.10 sec
INFO:root:2024-04-27 16:27:29, Train, Epoch : 10, Step : 5970, Loss : 0.18752, Acc : 0.906, Sensitive_Loss : 0.12212, Sensitive_Acc : 15.900, Run Time : 24.16 sec
INFO:root:2024-04-27 16:27:53, Train, Epoch : 10, Step : 5980, Loss : 0.19981, Acc : 0.881, Sensitive_Loss : 0.09203, Sensitive_Acc : 17.800, Run Time : 24.04 sec
INFO:root:2024-04-27 16:28:09, Train, Epoch : 10, Step : 5990, Loss : 0.24115, Acc : 0.897, Sensitive_Loss : 0.13287, Sensitive_Acc : 14.900, Run Time : 16.43 sec
INFO:root:2024-04-27 16:28:32, Train, Epoch : 10, Step : 6000, Loss : 0.23230, Acc : 0.897, Sensitive_Loss : 0.13213, Sensitive_Acc : 17.500, Run Time : 22.96 sec
INFO:root:2024-04-27 16:32:37, Dev, Step : 6000, Loss : 0.47809, Acc : 0.820, Auc : 0.904, Sensitive_Loss : 0.14546, Sensitive_Acc : 16.950, Sensitive_Auc : 0.993, Mean auc: 0.904, Run Time : 245.17 sec
INFO:root:2024-04-27 16:32:50, Train, Epoch : 10, Step : 6010, Loss : 0.24090, Acc : 0.894, Sensitive_Loss : 0.08625, Sensitive_Acc : 17.200, Run Time : 257.83 sec
INFO:root:2024-04-27 16:33:08, Train, Epoch : 10, Step : 6020, Loss : 0.21971, Acc : 0.897, Sensitive_Loss : 0.09933, Sensitive_Acc : 16.900, Run Time : 18.23 sec
INFO:root:2024-04-27 16:33:26, Train, Epoch : 10, Step : 6030, Loss : 0.22433, Acc : 0.897, Sensitive_Loss : 0.08775, Sensitive_Acc : 16.500, Run Time : 17.23 sec
INFO:root:2024-04-27 16:33:42, Train, Epoch : 10, Step : 6040, Loss : 0.25222, Acc : 0.906, Sensitive_Loss : 0.08967, Sensitive_Acc : 14.900, Run Time : 16.77 sec
INFO:root:2024-04-27 16:33:59, Train, Epoch : 10, Step : 6050, Loss : 0.21793, Acc : 0.912, Sensitive_Loss : 0.08961, Sensitive_Acc : 15.900, Run Time : 16.72 sec
INFO:root:2024-04-27 16:34:18, Train, Epoch : 10, Step : 6060, Loss : 0.25281, Acc : 0.884, Sensitive_Loss : 0.11249, Sensitive_Acc : 16.400, Run Time : 18.79 sec
INFO:root:2024-04-27 16:34:34, Train, Epoch : 10, Step : 6070, Loss : 0.20337, Acc : 0.912, Sensitive_Loss : 0.09189, Sensitive_Acc : 17.600, Run Time : 16.31 sec
INFO:root:2024-04-27 16:34:53, Train, Epoch : 10, Step : 6080, Loss : 0.20899, Acc : 0.900, Sensitive_Loss : 0.10748, Sensitive_Acc : 17.800, Run Time : 19.00 sec
INFO:root:2024-04-27 16:35:11, Train, Epoch : 10, Step : 6090, Loss : 0.22354, Acc : 0.925, Sensitive_Loss : 0.09910, Sensitive_Acc : 18.600, Run Time : 17.72 sec
INFO:root:2024-04-27 16:35:28, Train, Epoch : 10, Step : 6100, Loss : 0.21505, Acc : 0.919, Sensitive_Loss : 0.10048, Sensitive_Acc : 17.000, Run Time : 16.83 sec
INFO:root:2024-04-27 16:39:32, Dev, Step : 6100, Loss : 0.48510, Acc : 0.818, Auc : 0.901, Sensitive_Loss : 0.12277, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.901, Run Time : 244.22 sec
INFO:root:2024-04-27 16:39:45, Train, Epoch : 10, Step : 6110, Loss : 0.22101, Acc : 0.909, Sensitive_Loss : 0.06637, Sensitive_Acc : 16.400, Run Time : 257.61 sec
INFO:root:2024-04-27 16:40:04, Train, Epoch : 10, Step : 6120, Loss : 0.23013, Acc : 0.894, Sensitive_Loss : 0.10411, Sensitive_Acc : 16.200, Run Time : 18.44 sec
INFO:root:2024-04-27 16:40:21, Train, Epoch : 10, Step : 6130, Loss : 0.17980, Acc : 0.906, Sensitive_Loss : 0.14071, Sensitive_Acc : 17.100, Run Time : 17.56 sec
INFO:root:2024-04-27 16:40:40, Train, Epoch : 10, Step : 6140, Loss : 0.21335, Acc : 0.912, Sensitive_Loss : 0.09066, Sensitive_Acc : 16.700, Run Time : 18.47 sec
INFO:root:2024-04-27 16:40:58, Train, Epoch : 10, Step : 6150, Loss : 0.24160, Acc : 0.909, Sensitive_Loss : 0.08600, Sensitive_Acc : 17.700, Run Time : 18.01 sec
INFO:root:2024-04-27 16:41:15, Train, Epoch : 10, Step : 6160, Loss : 0.22994, Acc : 0.900, Sensitive_Loss : 0.11164, Sensitive_Acc : 17.100, Run Time : 17.42 sec
INFO:root:2024-04-27 16:41:33, Train, Epoch : 10, Step : 6170, Loss : 0.29450, Acc : 0.881, Sensitive_Loss : 0.07753, Sensitive_Acc : 17.500, Run Time : 17.63 sec
INFO:root:2024-04-27 16:41:51, Train, Epoch : 10, Step : 6180, Loss : 0.23036, Acc : 0.900, Sensitive_Loss : 0.09216, Sensitive_Acc : 15.900, Run Time : 17.71 sec
INFO:root:2024-04-27 16:42:08, Train, Epoch : 10, Step : 6190, Loss : 0.23415, Acc : 0.906, Sensitive_Loss : 0.08143, Sensitive_Acc : 16.000, Run Time : 17.84 sec
INFO:root:2024-04-27 16:42:26, Train, Epoch : 10, Step : 6200, Loss : 0.24968, Acc : 0.903, Sensitive_Loss : 0.11554, Sensitive_Acc : 17.600, Run Time : 17.96 sec
INFO:root:2024-04-27 16:46:30, Dev, Step : 6200, Loss : 0.51153, Acc : 0.805, Auc : 0.899, Sensitive_Loss : 0.15912, Sensitive_Acc : 16.907, Sensitive_Auc : 0.992, Mean auc: 0.899, Run Time : 243.07 sec
INFO:root:2024-04-27 16:46:43, Train, Epoch : 10, Step : 6210, Loss : 0.20750, Acc : 0.916, Sensitive_Loss : 0.09501, Sensitive_Acc : 16.100, Run Time : 256.15 sec
INFO:root:2024-04-27 16:47:01, Train, Epoch : 10, Step : 6220, Loss : 0.20605, Acc : 0.906, Sensitive_Loss : 0.08771, Sensitive_Acc : 17.200, Run Time : 18.80 sec
INFO:root:2024-04-27 16:47:19, Train, Epoch : 10, Step : 6230, Loss : 0.27920, Acc : 0.866, Sensitive_Loss : 0.11720, Sensitive_Acc : 17.400, Run Time : 17.92 sec
INFO:root:2024-04-27 16:47:37, Train, Epoch : 10, Step : 6240, Loss : 0.20435, Acc : 0.916, Sensitive_Loss : 0.11183, Sensitive_Acc : 16.300, Run Time : 17.76 sec
INFO:root:2024-04-27 16:47:54, Train, Epoch : 10, Step : 6250, Loss : 0.19743, Acc : 0.922, Sensitive_Loss : 0.11316, Sensitive_Acc : 17.500, Run Time : 17.23 sec
INFO:root:2024-04-27 16:48:09, Train, Epoch : 10, Step : 6260, Loss : 0.18135, Acc : 0.928, Sensitive_Loss : 0.11453, Sensitive_Acc : 16.700, Run Time : 14.40 sec
INFO:root:2024-04-27 16:52:10
INFO:root:y_pred: [0.12369659 0.9483741  0.01467716 ... 0.612614   0.02642151 0.9770511 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.1408497e-01 8.6468650e-04 1.7964475e-01 3.1698521e-04 9.9986565e-01
 1.7554564e-03 9.9987459e-01 9.9991679e-01 1.0466409e-03 7.1541560e-01
 9.9668509e-01 9.9999547e-01 9.9628937e-01 9.8263860e-01 1.2109298e-02
 9.7170448e-01 9.9994385e-01 3.4136125e-03 9.3463987e-01 8.1847680e-01
 9.9916387e-01 1.7835757e-01 9.9979717e-01 9.8685533e-01 9.9838030e-01
 9.9959487e-01 1.3865015e-04 9.9982733e-01 9.7999632e-01 1.4013022e-01
 8.5586235e-02 5.0868981e-02 1.1553892e-01 4.4547621e-02 4.4324577e-02
 3.2402519e-02 4.8935715e-02 3.7183829e-02 9.9871886e-01 9.9018598e-01
 1.2718666e-04 1.7051162e-04 9.8701906e-01 3.3809829e-03 9.9999130e-01
 9.9750119e-01 9.9884772e-01 9.9974567e-01 2.8893033e-02 9.9992585e-01
 9.9942386e-01 8.0693237e-02 4.0808451e-01 4.5316271e-03 4.7914832e-04
 1.5605300e-02 1.3611346e-02 1.2564055e-02 4.7472128e-04 6.3369393e-01
 6.1434763e-03 4.8265013e-01 8.2877897e-02 9.8159021e-01 4.5952678e-01
 9.9986541e-01 1.1759452e-03 9.9995780e-01 9.9944919e-01 7.6762837e-01
 8.4760296e-01 9.4159037e-01 9.1756983e-03 1.1354379e-01 3.6588588e-03
 2.2789121e-03 3.8696543e-04 1.8722373e-01 1.9533238e-03 9.9978179e-01
 9.9747181e-01 1.0015506e-03 7.8161150e-01 2.5052333e-03 8.6885023e-01
 2.9943910e-01 2.0070799e-02 1.2126857e-02 9.9334270e-01 9.9991655e-01
 9.9999678e-01 2.5681730e-02 4.7602311e-02 9.9643266e-01 6.0224986e-01
 4.1434509e-03 9.9897766e-01 9.9986160e-01 4.7538956e-03 7.9866797e-03
 9.9791557e-01 9.9947280e-01 9.9325776e-01 9.9084818e-01 6.0500195e-03
 1.4602037e-01 9.9576914e-01 9.9889743e-01 9.9443603e-01 4.6913791e-04
 9.9451458e-01 9.9889922e-01 1.0656763e-01 9.9818569e-01 9.9998069e-01
 9.9841177e-01 3.9333883e-01 9.9887913e-01 5.2341791e-03 2.3784926e-02
 9.9952781e-01 9.9983120e-01 1.3816461e-03 9.9178070e-01 9.9998164e-01
 1.7429481e-01 9.7766066e-01 2.1612464e-02 3.7038244e-02 9.9880826e-01
 9.9742609e-01 5.4693264e-03 5.7161443e-02 1.6924432e-02 9.9716944e-01
 9.8795301e-01 9.9827194e-01 2.9848036e-03 5.7392299e-02 9.9847668e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 16:52:10, Dev, Step : 6260, Loss : 0.48926, Acc : 0.813, Auc : 0.902, Sensitive_Loss : 0.13927, Sensitive_Acc : 16.779, Sensitive_Auc : 0.992, Mean auc: 0.902, Run Time : 241.68 sec
