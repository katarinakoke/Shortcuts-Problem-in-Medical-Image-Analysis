Running on desktop25:
stdin: is not a tty
Activating chexpert environment...
1
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_val.csv",
    "pred_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Biased_Sex_1_pos01.csv",
    "pred_model": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Biased_Sex_1_pos011.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.1,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-28 01:08:00, Train, Epoch : 1, Step : 10, Loss : 0.67052, Acc : 0.625, Sensitive_Loss : 0.66840, Sensitive_Acc : 14.400, Run Time : 13.54 sec
INFO:root:2024-04-28 01:08:12, Train, Epoch : 1, Step : 20, Loss : 0.60440, Acc : 0.666, Sensitive_Loss : 0.58447, Sensitive_Acc : 16.000, Run Time : 11.42 sec
INFO:root:2024-04-28 01:08:23, Train, Epoch : 1, Step : 30, Loss : 0.60534, Acc : 0.706, Sensitive_Loss : 0.49200, Sensitive_Acc : 15.500, Run Time : 11.52 sec
INFO:root:2024-04-28 01:08:35, Train, Epoch : 1, Step : 40, Loss : 0.50542, Acc : 0.741, Sensitive_Loss : 0.45438, Sensitive_Acc : 14.700, Run Time : 12.08 sec
INFO:root:2024-04-28 01:08:47, Train, Epoch : 1, Step : 50, Loss : 0.49574, Acc : 0.747, Sensitive_Loss : 0.44160, Sensitive_Acc : 16.500, Run Time : 11.95 sec
INFO:root:2024-04-28 01:09:00, Train, Epoch : 1, Step : 60, Loss : 0.53764, Acc : 0.769, Sensitive_Loss : 0.38204, Sensitive_Acc : 16.000, Run Time : 12.45 sec
INFO:root:2024-04-28 01:09:11, Train, Epoch : 1, Step : 70, Loss : 0.48948, Acc : 0.781, Sensitive_Loss : 0.35117, Sensitive_Acc : 15.400, Run Time : 11.05 sec
INFO:root:2024-04-28 01:09:22, Train, Epoch : 1, Step : 80, Loss : 0.48568, Acc : 0.719, Sensitive_Loss : 0.41059, Sensitive_Acc : 16.300, Run Time : 11.39 sec
INFO:root:2024-04-28 01:09:33, Train, Epoch : 1, Step : 90, Loss : 0.50978, Acc : 0.778, Sensitive_Loss : 0.28557, Sensitive_Acc : 17.900, Run Time : 11.47 sec
INFO:root:2024-04-28 01:09:46, Train, Epoch : 1, Step : 100, Loss : 0.47275, Acc : 0.800, Sensitive_Loss : 0.28794, Sensitive_Acc : 17.400, Run Time : 12.16 sec
INFO:root:2024-04-28 01:12:28, Dev, Step : 100, Loss : 0.58772, Acc : 0.737, Auc : 0.849, Sensitive_Loss : 0.30290, Sensitive_Acc : 16.850, Sensitive_Auc : 0.962, Mean auc: 0.849, Run Time : 162.52 sec
INFO:root:2024-04-28 01:12:29, Best, Step : 100, Loss : 0.58772, Acc : 0.737, Auc : 0.849, Sensitive_Loss : 0.30290, Sensitive_Acc : 16.850, Sensitive_Auc : 0.962, Best Auc : 0.849
INFO:root:2024-04-28 01:12:37, Train, Epoch : 1, Step : 110, Loss : 0.46612, Acc : 0.791, Sensitive_Loss : 0.36494, Sensitive_Acc : 16.700, Run Time : 171.72 sec
INFO:root:2024-04-28 01:12:49, Train, Epoch : 1, Step : 120, Loss : 0.57269, Acc : 0.759, Sensitive_Loss : 0.28985, Sensitive_Acc : 17.500, Run Time : 11.96 sec
INFO:root:2024-04-28 01:13:02, Train, Epoch : 1, Step : 130, Loss : 0.57959, Acc : 0.741, Sensitive_Loss : 0.37241, Sensitive_Acc : 17.000, Run Time : 12.19 sec
INFO:root:2024-04-28 01:13:13, Train, Epoch : 1, Step : 140, Loss : 0.51410, Acc : 0.744, Sensitive_Loss : 0.28951, Sensitive_Acc : 17.000, Run Time : 11.71 sec
INFO:root:2024-04-28 01:13:25, Train, Epoch : 1, Step : 150, Loss : 0.45447, Acc : 0.738, Sensitive_Loss : 0.29698, Sensitive_Acc : 16.100, Run Time : 11.41 sec
INFO:root:2024-04-28 01:13:37, Train, Epoch : 1, Step : 160, Loss : 0.49104, Acc : 0.759, Sensitive_Loss : 0.29290, Sensitive_Acc : 17.500, Run Time : 12.31 sec
INFO:root:2024-04-28 01:13:49, Train, Epoch : 1, Step : 170, Loss : 0.46114, Acc : 0.747, Sensitive_Loss : 0.32085, Sensitive_Acc : 14.800, Run Time : 11.93 sec
INFO:root:2024-04-28 01:14:00, Train, Epoch : 1, Step : 180, Loss : 0.49892, Acc : 0.772, Sensitive_Loss : 0.23215, Sensitive_Acc : 15.400, Run Time : 11.29 sec
INFO:root:2024-04-28 01:14:12, Train, Epoch : 1, Step : 190, Loss : 0.45615, Acc : 0.772, Sensitive_Loss : 0.25825, Sensitive_Acc : 17.200, Run Time : 12.27 sec
INFO:root:2024-04-28 01:14:24, Train, Epoch : 1, Step : 200, Loss : 0.61137, Acc : 0.747, Sensitive_Loss : 0.25666, Sensitive_Acc : 13.600, Run Time : 11.70 sec
INFO:root:2024-04-28 01:16:59, Dev, Step : 200, Loss : 0.47967, Acc : 0.790, Auc : 0.866, Sensitive_Loss : 0.29008, Sensitive_Acc : 16.764, Sensitive_Auc : 0.983, Mean auc: 0.866, Run Time : 154.74 sec
INFO:root:2024-04-28 01:17:00, Best, Step : 200, Loss : 0.47967, Acc : 0.790, Auc : 0.866, Sensitive_Loss : 0.29008, Sensitive_Acc : 16.764, Sensitive_Auc : 0.983, Best Auc : 0.866
INFO:root:2024-04-28 01:17:08, Train, Epoch : 1, Step : 210, Loss : 0.50277, Acc : 0.762, Sensitive_Loss : 0.26747, Sensitive_Acc : 16.600, Run Time : 163.77 sec
INFO:root:2024-04-28 01:17:19, Train, Epoch : 1, Step : 220, Loss : 0.46623, Acc : 0.797, Sensitive_Loss : 0.25461, Sensitive_Acc : 16.400, Run Time : 11.18 sec
INFO:root:2024-04-28 01:17:32, Train, Epoch : 1, Step : 230, Loss : 0.43042, Acc : 0.800, Sensitive_Loss : 0.25474, Sensitive_Acc : 18.200, Run Time : 12.65 sec
INFO:root:2024-04-28 01:17:43, Train, Epoch : 1, Step : 240, Loss : 0.54150, Acc : 0.775, Sensitive_Loss : 0.24680, Sensitive_Acc : 15.400, Run Time : 11.74 sec
INFO:root:2024-04-28 01:17:55, Train, Epoch : 1, Step : 250, Loss : 0.49711, Acc : 0.781, Sensitive_Loss : 0.21567, Sensitive_Acc : 17.500, Run Time : 11.71 sec
INFO:root:2024-04-28 01:18:07, Train, Epoch : 1, Step : 260, Loss : 0.48668, Acc : 0.800, Sensitive_Loss : 0.24589, Sensitive_Acc : 15.800, Run Time : 12.27 sec
INFO:root:2024-04-28 01:18:20, Train, Epoch : 1, Step : 270, Loss : 0.37896, Acc : 0.816, Sensitive_Loss : 0.20157, Sensitive_Acc : 15.500, Run Time : 12.53 sec
INFO:root:2024-04-28 01:18:31, Train, Epoch : 1, Step : 280, Loss : 0.54575, Acc : 0.766, Sensitive_Loss : 0.20418, Sensitive_Acc : 15.900, Run Time : 11.37 sec
INFO:root:2024-04-28 01:18:43, Train, Epoch : 1, Step : 290, Loss : 0.51927, Acc : 0.762, Sensitive_Loss : 0.28208, Sensitive_Acc : 16.100, Run Time : 11.71 sec
INFO:root:2024-04-28 01:18:55, Train, Epoch : 1, Step : 300, Loss : 0.45098, Acc : 0.806, Sensitive_Loss : 0.19479, Sensitive_Acc : 14.500, Run Time : 11.59 sec
INFO:root:2024-04-28 01:21:30, Dev, Step : 300, Loss : 0.56377, Acc : 0.748, Auc : 0.867, Sensitive_Loss : 0.28470, Sensitive_Acc : 16.636, Sensitive_Auc : 0.979, Mean auc: 0.867, Run Time : 155.00 sec
INFO:root:2024-04-28 01:21:30, Best, Step : 300, Loss : 0.56377, Acc : 0.748, Auc : 0.867, Sensitive_Loss : 0.28470, Sensitive_Acc : 16.636, Sensitive_Auc : 0.979, Best Auc : 0.867
INFO:root:2024-04-28 01:21:39, Train, Epoch : 1, Step : 310, Loss : 0.50589, Acc : 0.753, Sensitive_Loss : 0.20469, Sensitive_Acc : 15.700, Run Time : 164.22 sec
INFO:root:2024-04-28 01:21:51, Train, Epoch : 1, Step : 320, Loss : 0.50245, Acc : 0.769, Sensitive_Loss : 0.20626, Sensitive_Acc : 17.800, Run Time : 11.99 sec
INFO:root:2024-04-28 01:22:03, Train, Epoch : 1, Step : 330, Loss : 0.44710, Acc : 0.794, Sensitive_Loss : 0.20888, Sensitive_Acc : 14.400, Run Time : 12.21 sec
INFO:root:2024-04-28 01:22:15, Train, Epoch : 1, Step : 340, Loss : 0.53174, Acc : 0.759, Sensitive_Loss : 0.24815, Sensitive_Acc : 16.700, Run Time : 11.81 sec
INFO:root:2024-04-28 01:22:27, Train, Epoch : 1, Step : 350, Loss : 0.54465, Acc : 0.781, Sensitive_Loss : 0.21418, Sensitive_Acc : 15.100, Run Time : 11.63 sec
INFO:root:2024-04-28 01:22:39, Train, Epoch : 1, Step : 360, Loss : 0.52648, Acc : 0.769, Sensitive_Loss : 0.19061, Sensitive_Acc : 15.000, Run Time : 12.04 sec
INFO:root:2024-04-28 01:22:50, Train, Epoch : 1, Step : 370, Loss : 0.52324, Acc : 0.769, Sensitive_Loss : 0.23177, Sensitive_Acc : 15.400, Run Time : 11.66 sec
INFO:root:2024-04-28 01:23:02, Train, Epoch : 1, Step : 380, Loss : 0.48574, Acc : 0.794, Sensitive_Loss : 0.15911, Sensitive_Acc : 14.800, Run Time : 11.63 sec
INFO:root:2024-04-28 01:23:13, Train, Epoch : 1, Step : 390, Loss : 0.46875, Acc : 0.797, Sensitive_Loss : 0.19276, Sensitive_Acc : 15.900, Run Time : 11.07 sec
INFO:root:2024-04-28 01:23:25, Train, Epoch : 1, Step : 400, Loss : 0.47995, Acc : 0.784, Sensitive_Loss : 0.17824, Sensitive_Acc : 14.000, Run Time : 12.09 sec
INFO:root:2024-04-28 01:26:00, Dev, Step : 400, Loss : 0.46180, Acc : 0.797, Auc : 0.873, Sensitive_Loss : 0.19087, Sensitive_Acc : 16.764, Sensitive_Auc : 0.981, Mean auc: 0.873, Run Time : 154.83 sec
INFO:root:2024-04-28 01:26:01, Best, Step : 400, Loss : 0.46180, Acc : 0.797, Auc : 0.873, Sensitive_Loss : 0.19087, Sensitive_Acc : 16.764, Sensitive_Auc : 0.981, Best Auc : 0.873
INFO:root:2024-04-28 01:26:08, Train, Epoch : 1, Step : 410, Loss : 0.50649, Acc : 0.787, Sensitive_Loss : 0.19158, Sensitive_Acc : 17.000, Run Time : 163.41 sec
INFO:root:2024-04-28 01:26:20, Train, Epoch : 1, Step : 420, Loss : 0.49032, Acc : 0.772, Sensitive_Loss : 0.15928, Sensitive_Acc : 16.500, Run Time : 11.96 sec
INFO:root:2024-04-28 01:26:33, Train, Epoch : 1, Step : 430, Loss : 0.42189, Acc : 0.812, Sensitive_Loss : 0.22115, Sensitive_Acc : 16.300, Run Time : 12.34 sec
INFO:root:2024-04-28 01:26:44, Train, Epoch : 1, Step : 440, Loss : 0.52239, Acc : 0.784, Sensitive_Loss : 0.19822, Sensitive_Acc : 15.500, Run Time : 11.25 sec
INFO:root:2024-04-28 01:26:56, Train, Epoch : 1, Step : 450, Loss : 0.51876, Acc : 0.762, Sensitive_Loss : 0.20000, Sensitive_Acc : 16.500, Run Time : 12.30 sec
INFO:root:2024-04-28 01:27:08, Train, Epoch : 1, Step : 460, Loss : 0.49789, Acc : 0.766, Sensitive_Loss : 0.19021, Sensitive_Acc : 16.200, Run Time : 12.17 sec
INFO:root:2024-04-28 01:27:20, Train, Epoch : 1, Step : 470, Loss : 0.48213, Acc : 0.772, Sensitive_Loss : 0.21196, Sensitive_Acc : 15.800, Run Time : 12.01 sec
INFO:root:2024-04-28 01:27:33, Train, Epoch : 1, Step : 480, Loss : 0.46884, Acc : 0.794, Sensitive_Loss : 0.16426, Sensitive_Acc : 16.100, Run Time : 12.55 sec
INFO:root:2024-04-28 01:27:45, Train, Epoch : 1, Step : 490, Loss : 0.39794, Acc : 0.778, Sensitive_Loss : 0.18976, Sensitive_Acc : 15.500, Run Time : 12.04 sec
INFO:root:2024-04-28 01:27:56, Train, Epoch : 1, Step : 500, Loss : 0.55639, Acc : 0.775, Sensitive_Loss : 0.14869, Sensitive_Acc : 15.900, Run Time : 11.10 sec
INFO:root:2024-04-28 01:30:32, Dev, Step : 500, Loss : 0.48511, Acc : 0.775, Auc : 0.878, Sensitive_Loss : 0.20051, Sensitive_Acc : 16.693, Sensitive_Auc : 0.980, Mean auc: 0.878, Run Time : 156.18 sec
INFO:root:2024-04-28 01:30:33, Best, Step : 500, Loss : 0.48511, Acc : 0.775, Auc : 0.878, Sensitive_Loss : 0.20051, Sensitive_Acc : 16.693, Sensitive_Auc : 0.980, Best Auc : 0.878
INFO:root:2024-04-28 01:30:42, Train, Epoch : 1, Step : 510, Loss : 0.49368, Acc : 0.738, Sensitive_Loss : 0.17731, Sensitive_Acc : 15.400, Run Time : 165.35 sec
INFO:root:2024-04-28 01:30:54, Train, Epoch : 1, Step : 520, Loss : 0.46316, Acc : 0.794, Sensitive_Loss : 0.17935, Sensitive_Acc : 18.000, Run Time : 12.36 sec
INFO:root:2024-04-28 01:31:06, Train, Epoch : 1, Step : 530, Loss : 0.38424, Acc : 0.816, Sensitive_Loss : 0.13676, Sensitive_Acc : 16.200, Run Time : 11.71 sec
INFO:root:2024-04-28 01:31:17, Train, Epoch : 1, Step : 540, Loss : 0.56304, Acc : 0.738, Sensitive_Loss : 0.21204, Sensitive_Acc : 17.900, Run Time : 11.70 sec
INFO:root:2024-04-28 01:31:29, Train, Epoch : 1, Step : 550, Loss : 0.41581, Acc : 0.828, Sensitive_Loss : 0.15379, Sensitive_Acc : 17.100, Run Time : 12.12 sec
INFO:root:2024-04-28 01:31:42, Train, Epoch : 1, Step : 560, Loss : 0.48147, Acc : 0.772, Sensitive_Loss : 0.19508, Sensitive_Acc : 15.200, Run Time : 12.11 sec
INFO:root:2024-04-28 01:31:53, Train, Epoch : 1, Step : 570, Loss : 0.42453, Acc : 0.831, Sensitive_Loss : 0.14113, Sensitive_Acc : 14.700, Run Time : 11.33 sec
INFO:root:2024-04-28 01:32:04, Train, Epoch : 1, Step : 580, Loss : 0.46735, Acc : 0.750, Sensitive_Loss : 0.17005, Sensitive_Acc : 18.400, Run Time : 11.08 sec
INFO:root:2024-04-28 01:32:16, Train, Epoch : 1, Step : 590, Loss : 0.46838, Acc : 0.806, Sensitive_Loss : 0.14955, Sensitive_Acc : 16.900, Run Time : 12.28 sec
INFO:root:2024-04-28 01:32:28, Train, Epoch : 1, Step : 600, Loss : 0.38661, Acc : 0.794, Sensitive_Loss : 0.19023, Sensitive_Acc : 15.900, Run Time : 11.41 sec
INFO:root:2024-04-28 01:35:03, Dev, Step : 600, Loss : 0.48634, Acc : 0.784, Auc : 0.879, Sensitive_Loss : 0.20018, Sensitive_Acc : 16.921, Sensitive_Auc : 0.984, Mean auc: 0.879, Run Time : 155.18 sec
INFO:root:2024-04-28 01:35:03, Best, Step : 600, Loss : 0.48634, Acc : 0.784, Auc : 0.879, Sensitive_Loss : 0.20018, Sensitive_Acc : 16.921, Sensitive_Auc : 0.984, Best Auc : 0.879
INFO:root:2024-04-28 01:35:12, Train, Epoch : 1, Step : 610, Loss : 0.44150, Acc : 0.753, Sensitive_Loss : 0.18934, Sensitive_Acc : 16.900, Run Time : 163.93 sec
INFO:root:2024-04-28 01:35:23, Train, Epoch : 1, Step : 620, Loss : 0.44954, Acc : 0.787, Sensitive_Loss : 0.20210, Sensitive_Acc : 15.700, Run Time : 11.75 sec
INFO:root:2024-04-28 01:38:03
INFO:root:y_pred: [0.17512155 0.9141335  0.04912592 ... 0.83728755 0.00698196 0.659578  ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.61808920e-01 3.24189034e-03 1.67447701e-01 3.25854999e-05
 9.96602893e-01 3.17044114e-03 9.99712169e-01 9.98476923e-01
 9.46636871e-03 6.21408582e-01 9.90971684e-01 9.99827385e-01
 9.90081728e-01 9.37458456e-01 1.11506380e-01 9.53805089e-01
 9.99406457e-01 8.19056513e-05 7.39375770e-01 9.93664384e-01
 9.85946715e-01 2.81133145e-01 9.91867840e-01 6.38863087e-01
 9.99106824e-01 9.20868874e-01 4.94844373e-03 9.99734938e-01
 9.83126462e-01 3.33049804e-01 3.99882458e-02 7.44746506e-01
 1.00823194e-01 7.39015415e-02 1.49177955e-02 4.15969156e-02
 5.65265179e-01 1.02005340e-01 9.97964263e-01 9.85971749e-01
 3.29626550e-04 4.41934094e-02 9.94751990e-01 5.77346182e-05
 9.98225272e-01 9.97247398e-01 9.99019384e-01 9.94492650e-01
 4.21570465e-02 9.81891811e-01 9.96941507e-01 2.16337875e-03
 2.79873144e-02 9.24798567e-03 9.60179677e-05 3.82542871e-02
 3.35779101e-01 2.05430854e-03 1.50679401e-03 6.83873296e-02
 1.17977895e-02 2.49042381e-02 9.02091563e-02 9.51412857e-01
 4.56618756e-01 9.99620199e-01 8.94741912e-04 9.84151840e-01
 9.91166413e-01 9.42573786e-01 4.96207684e-01 8.33390236e-01
 3.02687325e-02 1.00422814e-01 3.55712138e-02 3.46624001e-04
 1.03926204e-01 4.26209830e-02 2.39076419e-03 9.95978117e-01
 9.94687080e-01 5.87502378e-04 1.81108519e-01 3.68170836e-03
 9.95332778e-01 9.09289181e-01 1.18130250e-02 2.37060234e-01
 9.95318353e-01 9.98544097e-01 9.99649405e-01 2.32640505e-02
 1.89572107e-02 9.98447239e-01 6.71404228e-02 4.10530437e-03
 9.94293988e-01 9.72824693e-01 4.41585726e-04 3.90709527e-02
 9.91391897e-01 7.90215969e-01 9.91701782e-01 9.80788231e-01
 2.31863081e-01 8.15969408e-01 9.86639142e-01 9.92112637e-01
 9.37281787e-01 6.42418722e-03 9.92945731e-01 9.90139127e-01
 3.41167338e-02 9.85745907e-01 9.89594102e-01 9.97396469e-01
 9.27810967e-01 9.85705256e-01 4.82594445e-02 3.33004177e-01
 9.81573403e-01 9.97763515e-01 2.84698435e-05 9.85479116e-01
 9.99834180e-01 4.82498825e-01 9.88651574e-01 3.65967862e-02
 7.28284478e-01 9.88240898e-01 9.31577325e-01 2.79173109e-04
 3.87349539e-02 6.27396489e-03 9.97188270e-01 9.95925784e-01
 9.77907598e-01 5.68864355e-03 5.21317311e-02 8.93997729e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 01:38:03, Dev, Step : 626, Loss : 0.45737, Acc : 0.798, Auc : 0.877, Sensitive_Loss : 0.20375, Sensitive_Acc : 16.864, Sensitive_Auc : 0.980, Mean auc: 0.877, Run Time : 152.79 sec
INFO:root:2024-04-28 01:38:09, Train, Epoch : 2, Step : 630, Loss : 0.16595, Acc : 0.341, Sensitive_Loss : 0.04113, Sensitive_Acc : 6.400, Run Time : 5.53 sec
INFO:root:2024-04-28 01:38:22, Train, Epoch : 2, Step : 640, Loss : 0.36314, Acc : 0.816, Sensitive_Loss : 0.16604, Sensitive_Acc : 14.800, Run Time : 12.62 sec
INFO:root:2024-04-28 01:38:33, Train, Epoch : 2, Step : 650, Loss : 0.37866, Acc : 0.841, Sensitive_Loss : 0.14315, Sensitive_Acc : 16.100, Run Time : 10.73 sec
INFO:root:2024-04-28 01:38:45, Train, Epoch : 2, Step : 660, Loss : 0.43127, Acc : 0.825, Sensitive_Loss : 0.14492, Sensitive_Acc : 14.900, Run Time : 12.01 sec
INFO:root:2024-04-28 01:38:55, Train, Epoch : 2, Step : 670, Loss : 0.39905, Acc : 0.825, Sensitive_Loss : 0.17479, Sensitive_Acc : 15.300, Run Time : 10.73 sec
INFO:root:2024-04-28 01:39:07, Train, Epoch : 2, Step : 680, Loss : 0.33832, Acc : 0.853, Sensitive_Loss : 0.13483, Sensitive_Acc : 15.400, Run Time : 11.19 sec
INFO:root:2024-04-28 01:39:18, Train, Epoch : 2, Step : 690, Loss : 0.46490, Acc : 0.791, Sensitive_Loss : 0.16238, Sensitive_Acc : 16.600, Run Time : 11.79 sec
INFO:root:2024-04-28 01:39:30, Train, Epoch : 2, Step : 700, Loss : 0.48337, Acc : 0.778, Sensitive_Loss : 0.18475, Sensitive_Acc : 16.200, Run Time : 11.12 sec
INFO:root:2024-04-28 01:42:05, Dev, Step : 700, Loss : 0.48480, Acc : 0.784, Auc : 0.880, Sensitive_Loss : 0.24677, Sensitive_Acc : 16.779, Sensitive_Auc : 0.980, Mean auc: 0.880, Run Time : 155.55 sec
INFO:root:2024-04-28 01:42:06, Best, Step : 700, Loss : 0.48480, Acc : 0.784, Auc : 0.880, Sensitive_Loss : 0.24677, Sensitive_Acc : 16.779, Sensitive_Auc : 0.980, Best Auc : 0.880
INFO:root:2024-04-28 01:42:15, Train, Epoch : 2, Step : 710, Loss : 0.50119, Acc : 0.772, Sensitive_Loss : 0.16746, Sensitive_Acc : 15.900, Run Time : 165.19 sec
INFO:root:2024-04-28 01:42:26, Train, Epoch : 2, Step : 720, Loss : 0.48828, Acc : 0.787, Sensitive_Loss : 0.13430, Sensitive_Acc : 16.000, Run Time : 11.14 sec
INFO:root:2024-04-28 01:42:38, Train, Epoch : 2, Step : 730, Loss : 0.42028, Acc : 0.806, Sensitive_Loss : 0.20787, Sensitive_Acc : 15.600, Run Time : 11.71 sec
INFO:root:2024-04-28 01:42:49, Train, Epoch : 2, Step : 740, Loss : 0.44243, Acc : 0.809, Sensitive_Loss : 0.16981, Sensitive_Acc : 17.500, Run Time : 11.48 sec
INFO:root:2024-04-28 01:43:00, Train, Epoch : 2, Step : 750, Loss : 0.41681, Acc : 0.794, Sensitive_Loss : 0.16345, Sensitive_Acc : 16.900, Run Time : 11.28 sec
INFO:root:2024-04-28 01:43:12, Train, Epoch : 2, Step : 760, Loss : 0.40543, Acc : 0.797, Sensitive_Loss : 0.15770, Sensitive_Acc : 15.400, Run Time : 11.71 sec
INFO:root:2024-04-28 01:43:24, Train, Epoch : 2, Step : 770, Loss : 0.48280, Acc : 0.772, Sensitive_Loss : 0.20440, Sensitive_Acc : 14.300, Run Time : 11.48 sec
INFO:root:2024-04-28 01:43:34, Train, Epoch : 2, Step : 780, Loss : 0.40909, Acc : 0.822, Sensitive_Loss : 0.14322, Sensitive_Acc : 15.900, Run Time : 10.67 sec
INFO:root:2024-04-28 01:43:46, Train, Epoch : 2, Step : 790, Loss : 0.41700, Acc : 0.822, Sensitive_Loss : 0.13432, Sensitive_Acc : 18.200, Run Time : 11.80 sec
INFO:root:2024-04-28 01:43:57, Train, Epoch : 2, Step : 800, Loss : 0.45862, Acc : 0.791, Sensitive_Loss : 0.17744, Sensitive_Acc : 16.500, Run Time : 11.44 sec
INFO:root:2024-04-28 01:46:32, Dev, Step : 800, Loss : 0.48641, Acc : 0.781, Auc : 0.874, Sensitive_Loss : 0.17203, Sensitive_Acc : 16.664, Sensitive_Auc : 0.980, Mean auc: 0.874, Run Time : 154.70 sec
INFO:root:2024-04-28 01:46:40, Train, Epoch : 2, Step : 810, Loss : 0.43750, Acc : 0.816, Sensitive_Loss : 0.14145, Sensitive_Acc : 17.800, Run Time : 162.75 sec
INFO:root:2024-04-28 01:46:52, Train, Epoch : 2, Step : 820, Loss : 0.43231, Acc : 0.819, Sensitive_Loss : 0.15622, Sensitive_Acc : 17.100, Run Time : 12.01 sec
INFO:root:2024-04-28 01:47:04, Train, Epoch : 2, Step : 830, Loss : 0.44839, Acc : 0.806, Sensitive_Loss : 0.18844, Sensitive_Acc : 15.200, Run Time : 11.31 sec
INFO:root:2024-04-28 01:47:16, Train, Epoch : 2, Step : 840, Loss : 0.46570, Acc : 0.803, Sensitive_Loss : 0.14776, Sensitive_Acc : 15.600, Run Time : 12.42 sec
INFO:root:2024-04-28 01:47:27, Train, Epoch : 2, Step : 850, Loss : 0.41196, Acc : 0.819, Sensitive_Loss : 0.16090, Sensitive_Acc : 15.400, Run Time : 11.44 sec
INFO:root:2024-04-28 01:47:39, Train, Epoch : 2, Step : 860, Loss : 0.42552, Acc : 0.778, Sensitive_Loss : 0.12032, Sensitive_Acc : 18.000, Run Time : 11.40 sec
INFO:root:2024-04-28 01:47:49, Train, Epoch : 2, Step : 870, Loss : 0.44554, Acc : 0.803, Sensitive_Loss : 0.19428, Sensitive_Acc : 15.800, Run Time : 10.49 sec
INFO:root:2024-04-28 01:48:00, Train, Epoch : 2, Step : 880, Loss : 0.42941, Acc : 0.812, Sensitive_Loss : 0.13691, Sensitive_Acc : 15.800, Run Time : 11.08 sec
INFO:root:2024-04-28 01:48:12, Train, Epoch : 2, Step : 890, Loss : 0.41320, Acc : 0.794, Sensitive_Loss : 0.18904, Sensitive_Acc : 15.700, Run Time : 11.19 sec
INFO:root:2024-04-28 01:48:24, Train, Epoch : 2, Step : 900, Loss : 0.40643, Acc : 0.819, Sensitive_Loss : 0.15233, Sensitive_Acc : 16.600, Run Time : 12.41 sec
INFO:root:2024-04-28 01:50:57, Dev, Step : 900, Loss : 0.47315, Acc : 0.789, Auc : 0.888, Sensitive_Loss : 0.14483, Sensitive_Acc : 16.764, Sensitive_Auc : 0.983, Mean auc: 0.888, Run Time : 153.23 sec
INFO:root:2024-04-28 01:50:58, Best, Step : 900, Loss : 0.47315, Acc : 0.789, Auc : 0.888, Sensitive_Loss : 0.14483, Sensitive_Acc : 16.764, Sensitive_Auc : 0.983, Best Auc : 0.888
INFO:root:2024-04-28 01:51:07, Train, Epoch : 2, Step : 910, Loss : 0.44910, Acc : 0.816, Sensitive_Loss : 0.11485, Sensitive_Acc : 15.900, Run Time : 162.73 sec
INFO:root:2024-04-28 01:51:17, Train, Epoch : 2, Step : 920, Loss : 0.44618, Acc : 0.781, Sensitive_Loss : 0.18341, Sensitive_Acc : 16.200, Run Time : 10.77 sec
INFO:root:2024-04-28 01:51:29, Train, Epoch : 2, Step : 930, Loss : 0.50450, Acc : 0.778, Sensitive_Loss : 0.16117, Sensitive_Acc : 14.600, Run Time : 11.65 sec
INFO:root:2024-04-28 01:51:41, Train, Epoch : 2, Step : 940, Loss : 0.44520, Acc : 0.797, Sensitive_Loss : 0.13008, Sensitive_Acc : 15.800, Run Time : 11.56 sec
INFO:root:2024-04-28 01:51:52, Train, Epoch : 2, Step : 950, Loss : 0.45690, Acc : 0.787, Sensitive_Loss : 0.16360, Sensitive_Acc : 15.700, Run Time : 11.44 sec
INFO:root:2024-04-28 01:52:05, Train, Epoch : 2, Step : 960, Loss : 0.40178, Acc : 0.819, Sensitive_Loss : 0.11823, Sensitive_Acc : 17.500, Run Time : 12.63 sec
INFO:root:2024-04-28 01:52:15, Train, Epoch : 2, Step : 970, Loss : 0.39831, Acc : 0.822, Sensitive_Loss : 0.13778, Sensitive_Acc : 16.400, Run Time : 10.24 sec
INFO:root:2024-04-28 01:52:26, Train, Epoch : 2, Step : 980, Loss : 0.42341, Acc : 0.806, Sensitive_Loss : 0.16194, Sensitive_Acc : 15.600, Run Time : 11.49 sec
INFO:root:2024-04-28 01:52:38, Train, Epoch : 2, Step : 990, Loss : 0.49753, Acc : 0.772, Sensitive_Loss : 0.16511, Sensitive_Acc : 16.200, Run Time : 11.32 sec
INFO:root:2024-04-28 01:52:49, Train, Epoch : 2, Step : 1000, Loss : 0.49580, Acc : 0.781, Sensitive_Loss : 0.15234, Sensitive_Acc : 15.800, Run Time : 11.63 sec
INFO:root:2024-04-28 01:55:24, Dev, Step : 1000, Loss : 0.48596, Acc : 0.779, Auc : 0.885, Sensitive_Loss : 0.28990, Sensitive_Acc : 16.536, Sensitive_Auc : 0.991, Mean auc: 0.885, Run Time : 154.48 sec
INFO:root:2024-04-28 01:55:33, Train, Epoch : 2, Step : 1010, Loss : 0.38237, Acc : 0.856, Sensitive_Loss : 0.15115, Sensitive_Acc : 16.000, Run Time : 163.37 sec
INFO:root:2024-04-28 01:55:44, Train, Epoch : 2, Step : 1020, Loss : 0.46951, Acc : 0.803, Sensitive_Loss : 0.13812, Sensitive_Acc : 14.900, Run Time : 10.81 sec
INFO:root:2024-04-28 01:55:55, Train, Epoch : 2, Step : 1030, Loss : 0.44331, Acc : 0.800, Sensitive_Loss : 0.15276, Sensitive_Acc : 18.100, Run Time : 11.29 sec
INFO:root:2024-04-28 01:56:07, Train, Epoch : 2, Step : 1040, Loss : 0.48376, Acc : 0.747, Sensitive_Loss : 0.18185, Sensitive_Acc : 15.600, Run Time : 11.61 sec
INFO:root:2024-04-28 01:56:18, Train, Epoch : 2, Step : 1050, Loss : 0.42614, Acc : 0.828, Sensitive_Loss : 0.12858, Sensitive_Acc : 16.500, Run Time : 11.50 sec
INFO:root:2024-04-28 01:56:30, Train, Epoch : 2, Step : 1060, Loss : 0.42957, Acc : 0.803, Sensitive_Loss : 0.12835, Sensitive_Acc : 16.200, Run Time : 11.72 sec
INFO:root:2024-04-28 01:56:42, Train, Epoch : 2, Step : 1070, Loss : 0.44791, Acc : 0.816, Sensitive_Loss : 0.13295, Sensitive_Acc : 16.800, Run Time : 11.79 sec
INFO:root:2024-04-28 01:56:53, Train, Epoch : 2, Step : 1080, Loss : 0.38777, Acc : 0.822, Sensitive_Loss : 0.11013, Sensitive_Acc : 17.000, Run Time : 11.73 sec
INFO:root:2024-04-28 01:57:05, Train, Epoch : 2, Step : 1090, Loss : 0.40287, Acc : 0.838, Sensitive_Loss : 0.14670, Sensitive_Acc : 16.200, Run Time : 11.68 sec
INFO:root:2024-04-28 01:57:16, Train, Epoch : 2, Step : 1100, Loss : 0.41654, Acc : 0.828, Sensitive_Loss : 0.13279, Sensitive_Acc : 16.700, Run Time : 11.23 sec
INFO:root:2024-04-28 01:59:50, Dev, Step : 1100, Loss : 0.47964, Acc : 0.797, Auc : 0.894, Sensitive_Loss : 0.23483, Sensitive_Acc : 16.636, Sensitive_Auc : 0.993, Mean auc: 0.894, Run Time : 153.61 sec
INFO:root:2024-04-28 01:59:50, Best, Step : 1100, Loss : 0.47964, Acc : 0.797, Auc : 0.894, Sensitive_Loss : 0.23483, Sensitive_Acc : 16.636, Sensitive_Auc : 0.993, Best Auc : 0.894
INFO:root:2024-04-28 01:59:59, Train, Epoch : 2, Step : 1110, Loss : 0.48996, Acc : 0.794, Sensitive_Loss : 0.13352, Sensitive_Acc : 16.200, Run Time : 162.69 sec
INFO:root:2024-04-28 02:00:10, Train, Epoch : 2, Step : 1120, Loss : 0.38940, Acc : 0.794, Sensitive_Loss : 0.13534, Sensitive_Acc : 15.000, Run Time : 11.38 sec
INFO:root:2024-04-28 02:00:22, Train, Epoch : 2, Step : 1130, Loss : 0.47394, Acc : 0.775, Sensitive_Loss : 0.15203, Sensitive_Acc : 16.800, Run Time : 12.00 sec
INFO:root:2024-04-28 02:00:33, Train, Epoch : 2, Step : 1140, Loss : 0.38834, Acc : 0.819, Sensitive_Loss : 0.15218, Sensitive_Acc : 14.400, Run Time : 11.18 sec
INFO:root:2024-04-28 02:00:44, Train, Epoch : 2, Step : 1150, Loss : 0.44590, Acc : 0.791, Sensitive_Loss : 0.17852, Sensitive_Acc : 16.800, Run Time : 10.80 sec
INFO:root:2024-04-28 02:00:56, Train, Epoch : 2, Step : 1160, Loss : 0.39059, Acc : 0.844, Sensitive_Loss : 0.13253, Sensitive_Acc : 15.700, Run Time : 11.92 sec
INFO:root:2024-04-28 02:01:08, Train, Epoch : 2, Step : 1170, Loss : 0.32907, Acc : 0.866, Sensitive_Loss : 0.12641, Sensitive_Acc : 17.200, Run Time : 11.60 sec
INFO:root:2024-04-28 02:01:20, Train, Epoch : 2, Step : 1180, Loss : 0.44746, Acc : 0.800, Sensitive_Loss : 0.18388, Sensitive_Acc : 15.600, Run Time : 11.78 sec
INFO:root:2024-04-28 02:01:30, Train, Epoch : 2, Step : 1190, Loss : 0.39856, Acc : 0.828, Sensitive_Loss : 0.16036, Sensitive_Acc : 16.800, Run Time : 10.28 sec
INFO:root:2024-04-28 02:01:42, Train, Epoch : 2, Step : 1200, Loss : 0.41607, Acc : 0.803, Sensitive_Loss : 0.12024, Sensitive_Acc : 16.900, Run Time : 11.91 sec
INFO:root:2024-04-28 02:04:15, Dev, Step : 1200, Loss : 0.43948, Acc : 0.800, Auc : 0.895, Sensitive_Loss : 0.12682, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992, Mean auc: 0.895, Run Time : 153.61 sec
INFO:root:2024-04-28 02:04:16, Best, Step : 1200, Loss : 0.43948, Acc : 0.800, Auc : 0.895, Sensitive_Loss : 0.12682, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992, Best Auc : 0.895
INFO:root:2024-04-28 02:04:25, Train, Epoch : 2, Step : 1210, Loss : 0.39313, Acc : 0.816, Sensitive_Loss : 0.12073, Sensitive_Acc : 16.900, Run Time : 162.83 sec
INFO:root:2024-04-28 02:04:36, Train, Epoch : 2, Step : 1220, Loss : 0.47973, Acc : 0.797, Sensitive_Loss : 0.16831, Sensitive_Acc : 16.100, Run Time : 11.65 sec
INFO:root:2024-04-28 02:04:48, Train, Epoch : 2, Step : 1230, Loss : 0.42937, Acc : 0.794, Sensitive_Loss : 0.18390, Sensitive_Acc : 14.800, Run Time : 11.51 sec
INFO:root:2024-04-28 02:05:00, Train, Epoch : 2, Step : 1240, Loss : 0.45976, Acc : 0.803, Sensitive_Loss : 0.14866, Sensitive_Acc : 16.700, Run Time : 12.08 sec
INFO:root:2024-04-28 02:05:11, Train, Epoch : 2, Step : 1250, Loss : 0.42454, Acc : 0.806, Sensitive_Loss : 0.12360, Sensitive_Acc : 16.400, Run Time : 11.09 sec
INFO:root:2024-04-28 02:07:45
INFO:root:y_pred: [0.08517992 0.6968709  0.0429837  ... 0.50885266 0.01371829 0.5983581 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.88754630e-01 8.89167935e-03 8.36290300e-01 1.99490995e-03
 9.99720514e-01 2.64683273e-03 9.99992609e-01 9.99899864e-01
 1.57156936e-03 9.80268896e-01 9.97606516e-01 9.99993801e-01
 9.94722962e-01 9.79553223e-01 8.68642256e-02 9.92131650e-01
 9.99970317e-01 2.05763206e-02 5.86001277e-01 9.94676113e-01
 9.99315023e-01 4.46642153e-02 9.99824464e-01 9.68037665e-01
 9.99895930e-01 9.99793828e-01 1.00506097e-03 9.99914169e-01
 9.96715546e-01 4.22721595e-01 1.39478207e-01 9.61799443e-01
 1.85482260e-02 3.00139010e-01 4.93602082e-02 3.79610732e-02
 1.99191481e-01 7.39118531e-02 9.99786556e-01 9.99619246e-01
 1.16215029e-03 1.29142962e-02 9.93071973e-01 5.52404963e-04
 9.99977708e-01 9.99820888e-01 9.99849796e-01 9.81422901e-01
 1.46966651e-01 9.89584029e-01 9.99209225e-01 5.07344082e-02
 7.17877507e-01 1.35192340e-02 5.88507578e-03 1.20053187e-01
 5.51469862e-01 5.36583783e-03 1.50222316e-01 3.52804482e-01
 2.03126386e-01 3.26536596e-01 5.52524686e-01 9.88286436e-01
 8.19648862e-01 9.99975324e-01 1.10386208e-01 9.99964833e-01
 9.99245405e-01 9.37218070e-01 9.95586872e-01 9.15939808e-01
 6.27596229e-02 1.02846324e-01 6.05009049e-02 3.32970172e-02
 2.17953622e-01 5.47040582e-01 7.29376124e-03 9.99854565e-01
 9.99823511e-01 9.69181582e-02 8.91726732e-01 7.67623633e-02
 9.95093465e-01 9.62029219e-01 3.86511125e-02 2.68709958e-01
 9.93777752e-01 9.99869108e-01 9.99993563e-01 4.51909155e-02
 6.37793839e-02 9.99749601e-01 7.10714579e-01 2.12034173e-02
 9.98982370e-01 9.99704063e-01 3.54246306e-03 4.02968675e-01
 9.98262942e-01 9.99116004e-01 9.98359859e-01 9.98613954e-01
 1.28370330e-01 7.72979796e-01 9.99436557e-01 9.99798357e-01
 9.92723644e-01 3.86750064e-04 9.98895526e-01 9.95145023e-01
 2.77794730e-02 9.99821365e-01 9.98621583e-01 9.99865890e-01
 8.71073008e-01 9.99674678e-01 8.51731896e-02 7.75288343e-01
 9.99619484e-01 9.99847293e-01 5.98963164e-03 9.99365032e-01
 9.99998331e-01 7.69895673e-01 9.98471916e-01 1.08533666e-01
 5.21331966e-01 9.85818207e-01 9.97361124e-01 3.18200588e-02
 5.30698663e-03 4.14503068e-02 9.99268711e-01 9.99022245e-01
 9.93938684e-01 6.86509768e-03 3.00959982e-02 9.99454200e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 02:07:45, Dev, Step : 1252, Loss : 0.45867, Acc : 0.797, Auc : 0.899, Sensitive_Loss : 0.22827, Sensitive_Acc : 16.793, Sensitive_Auc : 0.986, Mean auc: 0.899, Run Time : 153.06 sec
INFO:root:2024-04-28 02:07:46, Best, Step : 1252, Loss : 0.45867, Acc : 0.797,Auc : 0.899, Best Auc : 0.899, Sensitive_Loss : 0.22827, Sensitive_Acc : 16.793, Sensitive_Auc : 0.986
INFO:root:2024-04-28 02:07:59, Train, Epoch : 3, Step : 1260, Loss : 0.30630, Acc : 0.644, Sensitive_Loss : 0.11692, Sensitive_Acc : 12.900, Run Time : 11.86 sec
INFO:root:2024-04-28 02:08:10, Train, Epoch : 3, Step : 1270, Loss : 0.37247, Acc : 0.841, Sensitive_Loss : 0.11046, Sensitive_Acc : 17.100, Run Time : 11.22 sec
INFO:root:2024-04-28 02:08:21, Train, Epoch : 3, Step : 1280, Loss : 0.41311, Acc : 0.809, Sensitive_Loss : 0.10762, Sensitive_Acc : 16.800, Run Time : 10.83 sec
INFO:root:2024-04-28 02:08:32, Train, Epoch : 3, Step : 1290, Loss : 0.40583, Acc : 0.850, Sensitive_Loss : 0.12025, Sensitive_Acc : 14.300, Run Time : 11.48 sec
INFO:root:2024-04-28 02:08:44, Train, Epoch : 3, Step : 1300, Loss : 0.37472, Acc : 0.816, Sensitive_Loss : 0.09300, Sensitive_Acc : 15.500, Run Time : 11.99 sec
INFO:root:2024-04-28 02:11:18, Dev, Step : 1300, Loss : 0.40756, Acc : 0.827, Auc : 0.904, Sensitive_Loss : 0.12753, Sensitive_Acc : 16.779, Sensitive_Auc : 0.989, Mean auc: 0.904, Run Time : 154.12 sec
INFO:root:2024-04-28 02:11:19, Best, Step : 1300, Loss : 0.40756, Acc : 0.827, Auc : 0.904, Sensitive_Loss : 0.12753, Sensitive_Acc : 16.779, Sensitive_Auc : 0.989, Best Auc : 0.904
INFO:root:2024-04-28 02:11:28, Train, Epoch : 3, Step : 1310, Loss : 0.32361, Acc : 0.850, Sensitive_Loss : 0.12081, Sensitive_Acc : 16.700, Run Time : 163.60 sec
INFO:root:2024-04-28 02:11:39, Train, Epoch : 3, Step : 1320, Loss : 0.34727, Acc : 0.869, Sensitive_Loss : 0.09297, Sensitive_Acc : 13.600, Run Time : 11.04 sec
INFO:root:2024-04-28 02:11:51, Train, Epoch : 3, Step : 1330, Loss : 0.33891, Acc : 0.881, Sensitive_Loss : 0.12092, Sensitive_Acc : 18.000, Run Time : 12.37 sec
INFO:root:2024-04-28 02:12:02, Train, Epoch : 3, Step : 1340, Loss : 0.35850, Acc : 0.859, Sensitive_Loss : 0.09408, Sensitive_Acc : 15.600, Run Time : 10.62 sec
INFO:root:2024-04-28 02:12:14, Train, Epoch : 3, Step : 1350, Loss : 0.30990, Acc : 0.866, Sensitive_Loss : 0.08173, Sensitive_Acc : 16.100, Run Time : 12.15 sec
INFO:root:2024-04-28 02:12:25, Train, Epoch : 3, Step : 1360, Loss : 0.44916, Acc : 0.803, Sensitive_Loss : 0.10664, Sensitive_Acc : 14.600, Run Time : 11.53 sec
INFO:root:2024-04-28 02:12:36, Train, Epoch : 3, Step : 1370, Loss : 0.30105, Acc : 0.872, Sensitive_Loss : 0.12908, Sensitive_Acc : 16.700, Run Time : 10.54 sec
INFO:root:2024-04-28 02:12:48, Train, Epoch : 3, Step : 1380, Loss : 0.43043, Acc : 0.794, Sensitive_Loss : 0.12819, Sensitive_Acc : 16.400, Run Time : 11.61 sec
INFO:root:2024-04-28 02:12:59, Train, Epoch : 3, Step : 1390, Loss : 0.41918, Acc : 0.803, Sensitive_Loss : 0.12515, Sensitive_Acc : 16.200, Run Time : 11.58 sec
INFO:root:2024-04-28 02:13:11, Train, Epoch : 3, Step : 1400, Loss : 0.33707, Acc : 0.856, Sensitive_Loss : 0.14311, Sensitive_Acc : 16.200, Run Time : 11.60 sec
INFO:root:2024-04-28 02:15:45, Dev, Step : 1400, Loss : 0.40678, Acc : 0.822, Auc : 0.908, Sensitive_Loss : 0.13507, Sensitive_Acc : 16.793, Sensitive_Auc : 0.989, Mean auc: 0.908, Run Time : 154.67 sec
INFO:root:2024-04-28 02:15:46, Best, Step : 1400, Loss : 0.40678, Acc : 0.822, Auc : 0.908, Sensitive_Loss : 0.13507, Sensitive_Acc : 16.793, Sensitive_Auc : 0.989, Best Auc : 0.908
INFO:root:2024-04-28 02:15:55, Train, Epoch : 3, Step : 1410, Loss : 0.33479, Acc : 0.856, Sensitive_Loss : 0.10061, Sensitive_Acc : 16.400, Run Time : 164.02 sec
INFO:root:2024-04-28 02:16:06, Train, Epoch : 3, Step : 1420, Loss : 0.32342, Acc : 0.875, Sensitive_Loss : 0.11040, Sensitive_Acc : 15.800, Run Time : 11.53 sec
INFO:root:2024-04-28 02:16:17, Train, Epoch : 3, Step : 1430, Loss : 0.34642, Acc : 0.838, Sensitive_Loss : 0.09854, Sensitive_Acc : 16.300, Run Time : 10.83 sec
INFO:root:2024-04-28 02:16:29, Train, Epoch : 3, Step : 1440, Loss : 0.31664, Acc : 0.844, Sensitive_Loss : 0.12955, Sensitive_Acc : 16.800, Run Time : 11.53 sec
INFO:root:2024-04-28 02:16:41, Train, Epoch : 3, Step : 1450, Loss : 0.38573, Acc : 0.838, Sensitive_Loss : 0.09340, Sensitive_Acc : 15.400, Run Time : 12.52 sec
INFO:root:2024-04-28 02:16:51, Train, Epoch : 3, Step : 1460, Loss : 0.38208, Acc : 0.847, Sensitive_Loss : 0.10799, Sensitive_Acc : 17.000, Run Time : 10.30 sec
INFO:root:2024-04-28 02:17:04, Train, Epoch : 3, Step : 1470, Loss : 0.38133, Acc : 0.844, Sensitive_Loss : 0.08779, Sensitive_Acc : 16.700, Run Time : 12.19 sec
INFO:root:2024-04-28 02:17:14, Train, Epoch : 3, Step : 1480, Loss : 0.34932, Acc : 0.859, Sensitive_Loss : 0.09494, Sensitive_Acc : 16.000, Run Time : 10.76 sec
INFO:root:2024-04-28 02:17:26, Train, Epoch : 3, Step : 1490, Loss : 0.35897, Acc : 0.828, Sensitive_Loss : 0.10460, Sensitive_Acc : 16.800, Run Time : 11.80 sec
INFO:root:2024-04-28 02:17:37, Train, Epoch : 3, Step : 1500, Loss : 0.32557, Acc : 0.856, Sensitive_Loss : 0.07309, Sensitive_Acc : 16.300, Run Time : 10.99 sec
INFO:root:2024-04-28 02:20:13, Dev, Step : 1500, Loss : 0.40621, Acc : 0.827, Auc : 0.908, Sensitive_Loss : 0.13208, Sensitive_Acc : 16.750, Sensitive_Auc : 0.992, Mean auc: 0.908, Run Time : 155.90 sec
INFO:root:2024-04-28 02:20:14, Best, Step : 1500, Loss : 0.40621, Acc : 0.827, Auc : 0.908, Sensitive_Loss : 0.13208, Sensitive_Acc : 16.750, Sensitive_Auc : 0.992, Best Auc : 0.908
INFO:root:2024-04-28 02:20:22, Train, Epoch : 3, Step : 1510, Loss : 0.33947, Acc : 0.856, Sensitive_Loss : 0.11455, Sensitive_Acc : 16.000, Run Time : 164.68 sec
INFO:root:2024-04-28 02:20:34, Train, Epoch : 3, Step : 1520, Loss : 0.31636, Acc : 0.825, Sensitive_Loss : 0.09401, Sensitive_Acc : 15.200, Run Time : 11.93 sec
INFO:root:2024-04-28 02:20:45, Train, Epoch : 3, Step : 1530, Loss : 0.36614, Acc : 0.825, Sensitive_Loss : 0.09664, Sensitive_Acc : 15.400, Run Time : 11.37 sec
INFO:root:2024-04-28 02:20:57, Train, Epoch : 3, Step : 1540, Loss : 0.34764, Acc : 0.816, Sensitive_Loss : 0.10743, Sensitive_Acc : 16.300, Run Time : 11.85 sec
INFO:root:2024-04-28 02:21:08, Train, Epoch : 3, Step : 1550, Loss : 0.41466, Acc : 0.844, Sensitive_Loss : 0.09414, Sensitive_Acc : 16.900, Run Time : 11.38 sec
INFO:root:2024-04-28 02:21:20, Train, Epoch : 3, Step : 1560, Loss : 0.35257, Acc : 0.856, Sensitive_Loss : 0.13076, Sensitive_Acc : 15.700, Run Time : 12.02 sec
INFO:root:2024-04-28 02:21:32, Train, Epoch : 3, Step : 1570, Loss : 0.33159, Acc : 0.859, Sensitive_Loss : 0.11236, Sensitive_Acc : 17.300, Run Time : 11.35 sec
INFO:root:2024-04-28 02:21:43, Train, Epoch : 3, Step : 1580, Loss : 0.34429, Acc : 0.850, Sensitive_Loss : 0.10732, Sensitive_Acc : 15.500, Run Time : 11.29 sec
INFO:root:2024-04-28 02:22:05, Train, Epoch : 3, Step : 1590, Loss : 0.42164, Acc : 0.831, Sensitive_Loss : 0.10651, Sensitive_Acc : 18.900, Run Time : 21.68 sec
INFO:root:2024-04-28 02:22:23, Train, Epoch : 3, Step : 1600, Loss : 0.46923, Acc : 0.775, Sensitive_Loss : 0.11392, Sensitive_Acc : 16.200, Run Time : 18.58 sec
INFO:root:2024-04-28 02:25:38, Dev, Step : 1600, Loss : 0.40238, Acc : 0.826, Auc : 0.908, Sensitive_Loss : 0.12089, Sensitive_Acc : 16.850, Sensitive_Auc : 0.992, Mean auc: 0.908, Run Time : 194.69 sec
INFO:root:2024-04-28 02:25:39, Best, Step : 1600, Loss : 0.40238, Acc : 0.826, Auc : 0.908, Sensitive_Loss : 0.12089, Sensitive_Acc : 16.850, Sensitive_Auc : 0.992, Best Auc : 0.908
INFO:root:2024-04-28 02:25:47, Train, Epoch : 3, Step : 1610, Loss : 0.36768, Acc : 0.872, Sensitive_Loss : 0.08187, Sensitive_Acc : 17.600, Run Time : 203.33 sec
INFO:root:2024-04-28 02:25:59, Train, Epoch : 3, Step : 1620, Loss : 0.34294, Acc : 0.838, Sensitive_Loss : 0.09147, Sensitive_Acc : 16.000, Run Time : 12.06 sec
INFO:root:2024-04-28 02:26:10, Train, Epoch : 3, Step : 1630, Loss : 0.28371, Acc : 0.887, Sensitive_Loss : 0.10944, Sensitive_Acc : 15.700, Run Time : 10.96 sec
INFO:root:2024-04-28 02:26:21, Train, Epoch : 3, Step : 1640, Loss : 0.34605, Acc : 0.841, Sensitive_Loss : 0.10399, Sensitive_Acc : 17.300, Run Time : 11.18 sec
INFO:root:2024-04-28 02:26:32, Train, Epoch : 3, Step : 1650, Loss : 0.37466, Acc : 0.844, Sensitive_Loss : 0.06609, Sensitive_Acc : 16.600, Run Time : 11.35 sec
INFO:root:2024-04-28 02:26:44, Train, Epoch : 3, Step : 1660, Loss : 0.33488, Acc : 0.841, Sensitive_Loss : 0.08644, Sensitive_Acc : 16.000, Run Time : 11.69 sec
INFO:root:2024-04-28 02:26:55, Train, Epoch : 3, Step : 1670, Loss : 0.32551, Acc : 0.869, Sensitive_Loss : 0.09885, Sensitive_Acc : 17.300, Run Time : 11.21 sec
INFO:root:2024-04-28 02:27:07, Train, Epoch : 3, Step : 1680, Loss : 0.36199, Acc : 0.831, Sensitive_Loss : 0.09726, Sensitive_Acc : 16.300, Run Time : 11.90 sec
INFO:root:2024-04-28 02:27:19, Train, Epoch : 3, Step : 1690, Loss : 0.39717, Acc : 0.825, Sensitive_Loss : 0.14598, Sensitive_Acc : 16.400, Run Time : 11.89 sec
INFO:root:2024-04-28 02:27:30, Train, Epoch : 3, Step : 1700, Loss : 0.37749, Acc : 0.816, Sensitive_Loss : 0.12311, Sensitive_Acc : 16.100, Run Time : 11.15 sec
INFO:root:2024-04-28 02:30:06, Dev, Step : 1700, Loss : 0.40448, Acc : 0.826, Auc : 0.909, Sensitive_Loss : 0.13058, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.909, Run Time : 155.44 sec
INFO:root:2024-04-28 02:30:06, Best, Step : 1700, Loss : 0.40448, Acc : 0.826, Auc : 0.909, Sensitive_Loss : 0.13058, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Best Auc : 0.909
INFO:root:2024-04-28 02:30:15, Train, Epoch : 3, Step : 1710, Loss : 0.35318, Acc : 0.856, Sensitive_Loss : 0.09614, Sensitive_Acc : 17.100, Run Time : 164.68 sec
INFO:root:2024-04-28 02:30:26, Train, Epoch : 3, Step : 1720, Loss : 0.38985, Acc : 0.800, Sensitive_Loss : 0.12563, Sensitive_Acc : 15.500, Run Time : 11.55 sec
INFO:root:2024-04-28 02:30:38, Train, Epoch : 3, Step : 1730, Loss : 0.35369, Acc : 0.853, Sensitive_Loss : 0.07953, Sensitive_Acc : 16.300, Run Time : 11.93 sec
INFO:root:2024-04-28 02:30:51, Train, Epoch : 3, Step : 1740, Loss : 0.32040, Acc : 0.856, Sensitive_Loss : 0.10246, Sensitive_Acc : 16.600, Run Time : 12.30 sec
INFO:root:2024-04-28 02:31:02, Train, Epoch : 3, Step : 1750, Loss : 0.35548, Acc : 0.850, Sensitive_Loss : 0.09487, Sensitive_Acc : 15.600, Run Time : 11.68 sec
INFO:root:2024-04-28 02:31:14, Train, Epoch : 3, Step : 1760, Loss : 0.42579, Acc : 0.816, Sensitive_Loss : 0.13676, Sensitive_Acc : 15.000, Run Time : 11.47 sec
INFO:root:2024-04-28 02:31:25, Train, Epoch : 3, Step : 1770, Loss : 0.31426, Acc : 0.850, Sensitive_Loss : 0.10397, Sensitive_Acc : 15.700, Run Time : 11.36 sec
INFO:root:2024-04-28 02:31:36, Train, Epoch : 3, Step : 1780, Loss : 0.33919, Acc : 0.847, Sensitive_Loss : 0.09241, Sensitive_Acc : 16.000, Run Time : 10.98 sec
INFO:root:2024-04-28 02:31:48, Train, Epoch : 3, Step : 1790, Loss : 0.34059, Acc : 0.850, Sensitive_Loss : 0.09006, Sensitive_Acc : 16.600, Run Time : 11.51 sec
INFO:root:2024-04-28 02:31:59, Train, Epoch : 3, Step : 1800, Loss : 0.35533, Acc : 0.822, Sensitive_Loss : 0.09616, Sensitive_Acc : 16.900, Run Time : 11.70 sec
INFO:root:2024-04-28 02:34:34, Dev, Step : 1800, Loss : 0.40469, Acc : 0.827, Auc : 0.910, Sensitive_Loss : 0.11493, Sensitive_Acc : 16.907, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 155.14 sec
INFO:root:2024-04-28 02:34:35, Best, Step : 1800, Loss : 0.40469, Acc : 0.827, Auc : 0.910, Sensitive_Loss : 0.11493, Sensitive_Acc : 16.907, Sensitive_Auc : 0.993, Best Auc : 0.910
INFO:root:2024-04-28 02:34:44, Train, Epoch : 3, Step : 1810, Loss : 0.35866, Acc : 0.850, Sensitive_Loss : 0.09789, Sensitive_Acc : 17.100, Run Time : 164.31 sec
INFO:root:2024-04-28 02:34:56, Train, Epoch : 3, Step : 1820, Loss : 0.40019, Acc : 0.831, Sensitive_Loss : 0.15245, Sensitive_Acc : 15.200, Run Time : 12.30 sec
INFO:root:2024-04-28 02:35:08, Train, Epoch : 3, Step : 1830, Loss : 0.30042, Acc : 0.872, Sensitive_Loss : 0.08485, Sensitive_Acc : 16.800, Run Time : 12.07 sec
INFO:root:2024-04-28 02:35:19, Train, Epoch : 3, Step : 1840, Loss : 0.40314, Acc : 0.825, Sensitive_Loss : 0.07724, Sensitive_Acc : 16.200, Run Time : 11.20 sec
INFO:root:2024-04-28 02:35:31, Train, Epoch : 3, Step : 1850, Loss : 0.32034, Acc : 0.878, Sensitive_Loss : 0.06574, Sensitive_Acc : 15.200, Run Time : 11.67 sec
INFO:root:2024-04-28 02:35:42, Train, Epoch : 3, Step : 1860, Loss : 0.37849, Acc : 0.863, Sensitive_Loss : 0.07856, Sensitive_Acc : 16.400, Run Time : 10.74 sec
INFO:root:2024-04-28 02:35:54, Train, Epoch : 3, Step : 1870, Loss : 0.42028, Acc : 0.816, Sensitive_Loss : 0.08563, Sensitive_Acc : 14.900, Run Time : 11.92 sec
INFO:root:2024-04-28 02:38:37
INFO:root:y_pred: [0.08860491 0.88099    0.02183206 ... 0.60522926 0.00673396 0.82106656]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.8065591e-01 1.7464345e-03 3.5664013e-01 1.6392155e-04 9.9989045e-01
 6.6474278e-04 9.9999070e-01 9.9986374e-01 4.1146640e-04 8.0679065e-01
 9.9809188e-01 9.9999356e-01 9.9722350e-01 9.8890215e-01 2.8394960e-02
 9.9422354e-01 9.9998939e-01 1.4990023e-03 6.0645390e-01 9.8312253e-01
 9.9872106e-01 3.7804380e-02 9.9985337e-01 9.8888129e-01 9.9973828e-01
 9.9966371e-01 1.2228392e-04 9.9976903e-01 9.9540210e-01 3.5042316e-01
 2.0071985e-02 9.2144263e-01 1.4927258e-02 5.9281867e-02 1.5942020e-02
 1.0479121e-03 3.0379949e-02 1.4918468e-02 9.9975508e-01 9.9967158e-01
 4.4750286e-05 1.0460732e-02 9.9064273e-01 1.6737577e-04 9.9997604e-01
 9.9924755e-01 9.9957663e-01 9.7698557e-01 1.6735896e-02 9.9484825e-01
 9.9827874e-01 5.6565413e-03 6.3292688e-01 1.6945087e-03 1.0144501e-03
 1.9746624e-02 2.9045528e-02 4.0532309e-03 9.5811393e-03 1.6635130e-01
 2.5796933e-02 1.2241684e-01 1.2089346e-01 9.8738933e-01 5.9633851e-01
 9.9997914e-01 4.7990326e-03 9.9992287e-01 9.9468625e-01 6.7615169e-01
 9.8870289e-01 5.7855201e-01 7.2739250e-03 1.1294172e-01 2.6670913e-03
 4.3752082e-03 4.2010885e-02 1.5563691e-01 3.2561860e-04 9.9982029e-01
 9.9993467e-01 3.0846773e-03 6.7680967e-01 1.3268786e-02 9.7279525e-01
 9.6385640e-01 3.9993472e-02 1.1064232e-01 9.6295863e-01 9.9978250e-01
 9.9999475e-01 1.5357978e-02 3.8680180e-03 9.9967051e-01 3.1014413e-01
 3.1663775e-03 9.9642783e-01 9.9916005e-01 1.8408589e-04 6.9301769e-02
 9.9809057e-01 9.9856168e-01 9.9811018e-01 9.9935836e-01 3.1388454e-02
 3.3162987e-01 9.9715412e-01 9.9959773e-01 9.8002326e-01 8.3012463e-05
 9.9885273e-01 9.9492681e-01 1.6225113e-02 9.9983597e-01 9.9946374e-01
 9.9989200e-01 7.9268634e-01 9.9943441e-01 4.7314219e-02 6.4760941e-01
 9.9986172e-01 9.9981481e-01 1.3529422e-03 9.9727231e-01 9.9999869e-01
 4.7170261e-01 9.9820423e-01 1.1902247e-02 1.6799393e-01 9.5363575e-01
 9.9648428e-01 1.6426609e-03 3.5106926e-03 3.3221778e-02 9.9841642e-01
 9.9922013e-01 9.9357903e-01 1.1799864e-03 2.1027746e-02 9.9906629e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 02:38:37, Dev, Step : 1878, Loss : 0.41077, Acc : 0.822, Auc : 0.910, Sensitive_Loss : 0.13541, Sensitive_Acc : 16.779, Sensitive_Auc : 0.992, Mean auc: 0.910, Run Time : 155.41 sec
INFO:root:2024-04-28 02:38:43, Train, Epoch : 4, Step : 1880, Loss : 0.07808, Acc : 0.163, Sensitive_Loss : 0.01723, Sensitive_Acc : 3.300, Run Time : 4.62 sec
INFO:root:2024-04-28 02:38:54, Train, Epoch : 4, Step : 1890, Loss : 0.32947, Acc : 0.850, Sensitive_Loss : 0.09319, Sensitive_Acc : 16.500, Run Time : 11.59 sec
INFO:root:2024-04-28 02:39:06, Train, Epoch : 4, Step : 1900, Loss : 0.28391, Acc : 0.853, Sensitive_Loss : 0.08357, Sensitive_Acc : 16.300, Run Time : 11.19 sec
INFO:root:2024-04-28 02:41:41, Dev, Step : 1900, Loss : 0.42942, Acc : 0.812, Auc : 0.910, Sensitive_Loss : 0.15370, Sensitive_Acc : 16.721, Sensitive_Auc : 0.992, Mean auc: 0.910, Run Time : 155.46 sec
INFO:root:2024-04-28 02:41:49, Train, Epoch : 4, Step : 1910, Loss : 0.29607, Acc : 0.869, Sensitive_Loss : 0.09698, Sensitive_Acc : 16.300, Run Time : 163.61 sec
INFO:root:2024-04-28 02:42:01, Train, Epoch : 4, Step : 1920, Loss : 0.32468, Acc : 0.863, Sensitive_Loss : 0.10730, Sensitive_Acc : 15.200, Run Time : 11.51 sec
INFO:root:2024-04-28 02:42:13, Train, Epoch : 4, Step : 1930, Loss : 0.36969, Acc : 0.844, Sensitive_Loss : 0.10678, Sensitive_Acc : 14.200, Run Time : 12.37 sec
INFO:root:2024-04-28 02:42:25, Train, Epoch : 4, Step : 1940, Loss : 0.33931, Acc : 0.844, Sensitive_Loss : 0.11421, Sensitive_Acc : 15.100, Run Time : 11.61 sec
INFO:root:2024-04-28 02:42:37, Train, Epoch : 4, Step : 1950, Loss : 0.36685, Acc : 0.834, Sensitive_Loss : 0.06128, Sensitive_Acc : 16.200, Run Time : 12.40 sec
INFO:root:2024-04-28 02:42:49, Train, Epoch : 4, Step : 1960, Loss : 0.34735, Acc : 0.819, Sensitive_Loss : 0.10830, Sensitive_Acc : 15.300, Run Time : 11.89 sec
INFO:root:2024-04-28 02:43:00, Train, Epoch : 4, Step : 1970, Loss : 0.38972, Acc : 0.838, Sensitive_Loss : 0.08884, Sensitive_Acc : 15.600, Run Time : 10.93 sec
INFO:root:2024-04-28 02:43:11, Train, Epoch : 4, Step : 1980, Loss : 0.34585, Acc : 0.828, Sensitive_Loss : 0.10031, Sensitive_Acc : 14.900, Run Time : 10.64 sec
INFO:root:2024-04-28 02:43:23, Train, Epoch : 4, Step : 1990, Loss : 0.36873, Acc : 0.847, Sensitive_Loss : 0.07142, Sensitive_Acc : 17.000, Run Time : 11.92 sec
INFO:root:2024-04-28 02:43:34, Train, Epoch : 4, Step : 2000, Loss : 0.43137, Acc : 0.828, Sensitive_Loss : 0.10261, Sensitive_Acc : 15.300, Run Time : 11.34 sec
INFO:root:2024-04-28 02:46:10, Dev, Step : 2000, Loss : 0.40897, Acc : 0.821, Auc : 0.909, Sensitive_Loss : 0.11742, Sensitive_Acc : 16.793, Sensitive_Auc : 0.993, Mean auc: 0.909, Run Time : 156.34 sec
INFO:root:2024-04-28 02:46:19, Train, Epoch : 4, Step : 2010, Loss : 0.26862, Acc : 0.869, Sensitive_Loss : 0.10319, Sensitive_Acc : 17.900, Run Time : 165.13 sec
INFO:root:2024-04-28 02:46:30, Train, Epoch : 4, Step : 2020, Loss : 0.37879, Acc : 0.831, Sensitive_Loss : 0.08513, Sensitive_Acc : 16.900, Run Time : 11.29 sec
INFO:root:2024-04-28 02:46:41, Train, Epoch : 4, Step : 2030, Loss : 0.33926, Acc : 0.891, Sensitive_Loss : 0.13700, Sensitive_Acc : 15.700, Run Time : 11.17 sec
INFO:root:2024-04-28 02:46:53, Train, Epoch : 4, Step : 2040, Loss : 0.36369, Acc : 0.847, Sensitive_Loss : 0.10117, Sensitive_Acc : 17.100, Run Time : 11.75 sec
INFO:root:2024-04-28 02:47:05, Train, Epoch : 4, Step : 2050, Loss : 0.40204, Acc : 0.847, Sensitive_Loss : 0.07908, Sensitive_Acc : 16.800, Run Time : 12.20 sec
INFO:root:2024-04-28 02:47:17, Train, Epoch : 4, Step : 2060, Loss : 0.35024, Acc : 0.853, Sensitive_Loss : 0.08484, Sensitive_Acc : 17.100, Run Time : 11.27 sec
INFO:root:2024-04-28 02:47:28, Train, Epoch : 4, Step : 2070, Loss : 0.31897, Acc : 0.869, Sensitive_Loss : 0.08677, Sensitive_Acc : 16.700, Run Time : 11.79 sec
INFO:root:2024-04-28 02:47:40, Train, Epoch : 4, Step : 2080, Loss : 0.28765, Acc : 0.884, Sensitive_Loss : 0.11149, Sensitive_Acc : 16.200, Run Time : 11.86 sec
INFO:root:2024-04-28 02:47:52, Train, Epoch : 4, Step : 2090, Loss : 0.38624, Acc : 0.841, Sensitive_Loss : 0.07985, Sensitive_Acc : 17.000, Run Time : 11.36 sec
INFO:root:2024-04-28 02:48:02, Train, Epoch : 4, Step : 2100, Loss : 0.37859, Acc : 0.844, Sensitive_Loss : 0.11808, Sensitive_Acc : 15.700, Run Time : 10.65 sec
INFO:root:2024-04-28 02:50:37, Dev, Step : 2100, Loss : 0.40935, Acc : 0.821, Auc : 0.910, Sensitive_Loss : 0.11684, Sensitive_Acc : 16.879, Sensitive_Auc : 0.994, Mean auc: 0.910, Run Time : 155.10 sec
INFO:root:2024-04-28 02:50:46, Train, Epoch : 4, Step : 2110, Loss : 0.32368, Acc : 0.828, Sensitive_Loss : 0.10424, Sensitive_Acc : 15.000, Run Time : 163.61 sec
INFO:root:2024-04-28 02:50:57, Train, Epoch : 4, Step : 2120, Loss : 0.28950, Acc : 0.847, Sensitive_Loss : 0.09373, Sensitive_Acc : 15.800, Run Time : 11.16 sec
INFO:root:2024-04-28 02:51:09, Train, Epoch : 4, Step : 2130, Loss : 0.31668, Acc : 0.863, Sensitive_Loss : 0.10090, Sensitive_Acc : 15.400, Run Time : 11.76 sec
INFO:root:2024-04-28 02:51:20, Train, Epoch : 4, Step : 2140, Loss : 0.30306, Acc : 0.887, Sensitive_Loss : 0.10850, Sensitive_Acc : 14.700, Run Time : 11.53 sec
INFO:root:2024-04-28 02:51:32, Train, Epoch : 4, Step : 2150, Loss : 0.35394, Acc : 0.847, Sensitive_Loss : 0.08781, Sensitive_Acc : 16.800, Run Time : 11.53 sec
INFO:root:2024-04-28 02:51:43, Train, Epoch : 4, Step : 2160, Loss : 0.30991, Acc : 0.847, Sensitive_Loss : 0.11277, Sensitive_Acc : 15.800, Run Time : 10.80 sec
INFO:root:2024-04-28 02:51:55, Train, Epoch : 4, Step : 2170, Loss : 0.27561, Acc : 0.891, Sensitive_Loss : 0.15234, Sensitive_Acc : 17.300, Run Time : 12.26 sec
INFO:root:2024-04-28 02:52:06, Train, Epoch : 4, Step : 2180, Loss : 0.34912, Acc : 0.859, Sensitive_Loss : 0.12177, Sensitive_Acc : 16.000, Run Time : 11.19 sec
INFO:root:2024-04-28 02:52:17, Train, Epoch : 4, Step : 2190, Loss : 0.32450, Acc : 0.838, Sensitive_Loss : 0.08889, Sensitive_Acc : 15.200, Run Time : 11.17 sec
INFO:root:2024-04-28 02:52:29, Train, Epoch : 4, Step : 2200, Loss : 0.36042, Acc : 0.825, Sensitive_Loss : 0.09340, Sensitive_Acc : 15.000, Run Time : 11.68 sec
INFO:root:2024-04-28 02:55:05, Dev, Step : 2200, Loss : 0.41267, Acc : 0.819, Auc : 0.910, Sensitive_Loss : 0.12410, Sensitive_Acc : 16.750, Sensitive_Auc : 0.992, Mean auc: 0.910, Run Time : 156.11 sec
INFO:root:2024-04-28 02:55:13, Train, Epoch : 4, Step : 2210, Loss : 0.31615, Acc : 0.863, Sensitive_Loss : 0.09969, Sensitive_Acc : 15.500, Run Time : 164.38 sec
INFO:root:2024-04-28 02:55:25, Train, Epoch : 4, Step : 2220, Loss : 0.39490, Acc : 0.834, Sensitive_Loss : 0.09144, Sensitive_Acc : 17.000, Run Time : 11.97 sec
INFO:root:2024-04-28 02:55:37, Train, Epoch : 4, Step : 2230, Loss : 0.25849, Acc : 0.906, Sensitive_Loss : 0.11919, Sensitive_Acc : 15.700, Run Time : 11.53 sec
INFO:root:2024-04-28 02:55:48, Train, Epoch : 4, Step : 2240, Loss : 0.39569, Acc : 0.834, Sensitive_Loss : 0.10728, Sensitive_Acc : 16.300, Run Time : 11.11 sec
INFO:root:2024-04-28 02:55:59, Train, Epoch : 4, Step : 2250, Loss : 0.30475, Acc : 0.872, Sensitive_Loss : 0.09781, Sensitive_Acc : 18.200, Run Time : 11.34 sec
INFO:root:2024-04-28 02:56:11, Train, Epoch : 4, Step : 2260, Loss : 0.27529, Acc : 0.887, Sensitive_Loss : 0.10598, Sensitive_Acc : 16.200, Run Time : 11.24 sec
INFO:root:2024-04-28 02:56:22, Train, Epoch : 4, Step : 2270, Loss : 0.29387, Acc : 0.850, Sensitive_Loss : 0.11241, Sensitive_Acc : 15.300, Run Time : 11.54 sec
INFO:root:2024-04-28 02:56:33, Train, Epoch : 4, Step : 2280, Loss : 0.37625, Acc : 0.850, Sensitive_Loss : 0.07292, Sensitive_Acc : 14.600, Run Time : 10.70 sec
INFO:root:2024-04-28 02:56:45, Train, Epoch : 4, Step : 2290, Loss : 0.32468, Acc : 0.859, Sensitive_Loss : 0.09886, Sensitive_Acc : 16.600, Run Time : 11.73 sec
INFO:root:2024-04-28 02:56:56, Train, Epoch : 4, Step : 2300, Loss : 0.41782, Acc : 0.831, Sensitive_Loss : 0.14373, Sensitive_Acc : 17.700, Run Time : 11.67 sec
INFO:root:2024-04-28 02:59:31, Dev, Step : 2300, Loss : 0.42973, Acc : 0.813, Auc : 0.911, Sensitive_Loss : 0.13797, Sensitive_Acc : 16.807, Sensitive_Auc : 0.992, Mean auc: 0.911, Run Time : 154.74 sec
INFO:root:2024-04-28 02:59:32, Best, Step : 2300, Loss : 0.42973, Acc : 0.813, Auc : 0.911, Sensitive_Loss : 0.13797, Sensitive_Acc : 16.807, Sensitive_Auc : 0.992, Best Auc : 0.911
INFO:root:2024-04-28 02:59:40, Train, Epoch : 4, Step : 2310, Loss : 0.27228, Acc : 0.900, Sensitive_Loss : 0.08682, Sensitive_Acc : 16.400, Run Time : 164.23 sec
INFO:root:2024-04-28 02:59:52, Train, Epoch : 4, Step : 2320, Loss : 0.37347, Acc : 0.828, Sensitive_Loss : 0.09738, Sensitive_Acc : 16.300, Run Time : 11.62 sec
INFO:root:2024-04-28 03:00:05, Train, Epoch : 4, Step : 2330, Loss : 0.38202, Acc : 0.838, Sensitive_Loss : 0.09547, Sensitive_Acc : 16.900, Run Time : 12.42 sec
INFO:root:2024-04-28 03:00:15, Train, Epoch : 4, Step : 2340, Loss : 0.33250, Acc : 0.841, Sensitive_Loss : 0.08148, Sensitive_Acc : 17.100, Run Time : 10.89 sec
INFO:root:2024-04-28 03:00:27, Train, Epoch : 4, Step : 2350, Loss : 0.35486, Acc : 0.847, Sensitive_Loss : 0.10173, Sensitive_Acc : 17.200, Run Time : 11.35 sec
INFO:root:2024-04-28 03:00:38, Train, Epoch : 4, Step : 2360, Loss : 0.35222, Acc : 0.850, Sensitive_Loss : 0.09548, Sensitive_Acc : 16.600, Run Time : 10.81 sec
INFO:root:2024-04-28 03:00:49, Train, Epoch : 4, Step : 2370, Loss : 0.32045, Acc : 0.853, Sensitive_Loss : 0.10388, Sensitive_Acc : 15.300, Run Time : 11.10 sec
INFO:root:2024-04-28 03:01:00, Train, Epoch : 4, Step : 2380, Loss : 0.34844, Acc : 0.866, Sensitive_Loss : 0.09345, Sensitive_Acc : 16.500, Run Time : 11.61 sec
INFO:root:2024-04-28 03:01:12, Train, Epoch : 4, Step : 2390, Loss : 0.33185, Acc : 0.866, Sensitive_Loss : 0.06705, Sensitive_Acc : 17.400, Run Time : 11.41 sec
INFO:root:2024-04-28 03:01:24, Train, Epoch : 4, Step : 2400, Loss : 0.30954, Acc : 0.872, Sensitive_Loss : 0.08946, Sensitive_Acc : 17.300, Run Time : 12.11 sec
INFO:root:2024-04-28 03:03:58, Dev, Step : 2400, Loss : 0.40800, Acc : 0.824, Auc : 0.911, Sensitive_Loss : 0.13091, Sensitive_Acc : 16.736, Sensitive_Auc : 0.992, Mean auc: 0.911, Run Time : 153.98 sec
INFO:root:2024-04-28 03:03:58, Best, Step : 2400, Loss : 0.40800, Acc : 0.824, Auc : 0.911, Sensitive_Loss : 0.13091, Sensitive_Acc : 16.736, Sensitive_Auc : 0.992, Best Auc : 0.911
INFO:root:2024-04-28 03:04:07, Train, Epoch : 4, Step : 2410, Loss : 0.31487, Acc : 0.856, Sensitive_Loss : 0.11580, Sensitive_Acc : 16.900, Run Time : 163.42 sec
INFO:root:2024-04-28 03:04:19, Train, Epoch : 4, Step : 2420, Loss : 0.34675, Acc : 0.825, Sensitive_Loss : 0.09258, Sensitive_Acc : 16.400, Run Time : 11.92 sec
INFO:root:2024-04-28 03:04:31, Train, Epoch : 4, Step : 2430, Loss : 0.32563, Acc : 0.844, Sensitive_Loss : 0.06149, Sensitive_Acc : 15.700, Run Time : 11.43 sec
INFO:root:2024-04-28 03:04:42, Train, Epoch : 4, Step : 2440, Loss : 0.32027, Acc : 0.866, Sensitive_Loss : 0.13747, Sensitive_Acc : 16.700, Run Time : 11.01 sec
INFO:root:2024-04-28 03:04:53, Train, Epoch : 4, Step : 2450, Loss : 0.33886, Acc : 0.856, Sensitive_Loss : 0.09069, Sensitive_Acc : 16.100, Run Time : 11.29 sec
INFO:root:2024-04-28 03:05:05, Train, Epoch : 4, Step : 2460, Loss : 0.26685, Acc : 0.863, Sensitive_Loss : 0.10063, Sensitive_Acc : 16.800, Run Time : 11.74 sec
INFO:root:2024-04-28 03:05:16, Train, Epoch : 4, Step : 2470, Loss : 0.32808, Acc : 0.838, Sensitive_Loss : 0.11721, Sensitive_Acc : 16.000, Run Time : 11.27 sec
INFO:root:2024-04-28 03:05:27, Train, Epoch : 4, Step : 2480, Loss : 0.31102, Acc : 0.869, Sensitive_Loss : 0.11169, Sensitive_Acc : 15.800, Run Time : 10.85 sec
INFO:root:2024-04-28 03:05:39, Train, Epoch : 4, Step : 2490, Loss : 0.32184, Acc : 0.856, Sensitive_Loss : 0.11483, Sensitive_Acc : 15.400, Run Time : 12.71 sec
INFO:root:2024-04-28 03:05:50, Train, Epoch : 4, Step : 2500, Loss : 0.36682, Acc : 0.853, Sensitive_Loss : 0.10311, Sensitive_Acc : 15.900, Run Time : 10.51 sec
INFO:root:2024-04-28 03:08:25, Dev, Step : 2500, Loss : 0.41960, Acc : 0.818, Auc : 0.912, Sensitive_Loss : 0.13976, Sensitive_Acc : 16.764, Sensitive_Auc : 0.992, Mean auc: 0.912, Run Time : 154.82 sec
INFO:root:2024-04-28 03:08:25, Best, Step : 2500, Loss : 0.41960, Acc : 0.818, Auc : 0.912, Sensitive_Loss : 0.13976, Sensitive_Acc : 16.764, Sensitive_Auc : 0.992, Best Auc : 0.912
INFO:root:2024-04-28 03:11:01
INFO:root:y_pred: [0.13591652 0.8755152  0.01374928 ... 0.6449777  0.00397337 0.80544716]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.74156857e-01 1.38304895e-03 2.97638953e-01 8.17970067e-05
 9.99848843e-01 5.40487410e-04 9.99989271e-01 9.99796569e-01
 6.34889817e-04 7.80720711e-01 9.98343229e-01 9.99995470e-01
 9.97210443e-01 9.89430785e-01 2.54728645e-02 9.93007660e-01
 9.99990940e-01 1.39240059e-03 6.29213274e-01 9.82787013e-01
 9.98033822e-01 5.02700098e-02 9.99793828e-01 9.90308106e-01
 9.99890804e-01 9.99680638e-01 1.28671119e-04 9.99609292e-01
 9.94777322e-01 2.76722580e-01 2.76188720e-02 8.55261922e-01
 2.65864991e-02 4.40010279e-02 2.10024007e-02 9.89956548e-04
 2.39306111e-02 5.09285275e-03 9.99847770e-01 9.99632478e-01
 3.28820497e-05 1.52354809e-02 9.92212951e-01 3.21005966e-04
 9.99952197e-01 9.99444783e-01 9.99606192e-01 9.87723231e-01
 3.35095935e-02 9.98128355e-01 9.97972310e-01 4.95327078e-03
 7.24479735e-01 2.33905949e-03 7.53031636e-04 1.51075311e-02
 1.30530875e-02 6.37800526e-03 7.14999158e-03 2.13137373e-01
 5.34018949e-02 1.45138100e-01 6.26994744e-02 9.90696669e-01
 5.14501989e-01 9.99967456e-01 4.69944905e-03 9.99887109e-01
 9.94914293e-01 6.30871415e-01 9.78633761e-01 7.06287026e-01
 3.06005590e-03 1.52708367e-01 1.52546377e-03 3.87330027e-03
 3.69561799e-02 1.31347984e-01 4.83271957e-04 9.99755681e-01
 9.99852419e-01 2.22934713e-03 7.37841189e-01 8.95164628e-03
 9.83133137e-01 9.35381949e-01 5.23396432e-02 1.57804236e-01
 9.75287735e-01 9.99803960e-01 9.99993682e-01 1.67063251e-02
 3.67867341e-03 9.99822319e-01 2.23012120e-01 4.29568673e-03
 9.95997787e-01 9.98708725e-01 3.35840450e-04 3.32365185e-02
 9.98519957e-01 9.98501420e-01 9.96874213e-01 9.98894513e-01
 2.54098624e-02 2.08333552e-01 9.95992601e-01 9.99088526e-01
 9.82976019e-01 8.33178710e-05 9.98455524e-01 9.91927981e-01
 1.57062653e-02 9.99807537e-01 9.99459207e-01 9.99954224e-01
 8.38912129e-01 9.98673677e-01 6.29360229e-02 6.38374150e-01
 9.99814332e-01 9.99735892e-01 2.50404654e-03 9.98256147e-01
 9.99997616e-01 5.79054475e-01 9.95807409e-01 1.15656042e-02
 1.76961139e-01 9.47413445e-01 9.97836769e-01 1.32366398e-03
 3.07178427e-03 3.70882116e-02 9.97536421e-01 9.98845339e-01
 9.89019930e-01 9.56630625e-04 1.21939788e-02 9.98794079e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 03:11:01, Dev, Step : 2504, Loss : 0.41738, Acc : 0.819, Auc : 0.912, Sensitive_Loss : 0.13212, Sensitive_Acc : 16.764, Sensitive_Auc : 0.991, Mean auc: 0.912, Run Time : 153.23 sec
INFO:root:2024-04-28 03:11:01, Best, Step : 2504, Loss : 0.41738, Acc : 0.819,Auc : 0.912, Best Auc : 0.912, Sensitive_Loss : 0.13212, Sensitive_Acc : 16.764, Sensitive_Auc : 0.991
INFO:root:2024-04-28 03:11:11, Train, Epoch : 5, Step : 2510, Loss : 0.21436, Acc : 0.497, Sensitive_Loss : 0.07773, Sensitive_Acc : 9.900, Run Time : 8.81 sec
INFO:root:2024-04-28 03:11:22, Train, Epoch : 5, Step : 2520, Loss : 0.30358, Acc : 0.850, Sensitive_Loss : 0.07929, Sensitive_Acc : 16.100, Run Time : 10.99 sec
INFO:root:2024-04-28 03:11:34, Train, Epoch : 5, Step : 2530, Loss : 0.32294, Acc : 0.841, Sensitive_Loss : 0.12164, Sensitive_Acc : 16.500, Run Time : 11.45 sec
INFO:root:2024-04-28 03:11:46, Train, Epoch : 5, Step : 2540, Loss : 0.28142, Acc : 0.881, Sensitive_Loss : 0.10329, Sensitive_Acc : 16.500, Run Time : 11.93 sec
INFO:root:2024-04-28 03:11:57, Train, Epoch : 5, Step : 2550, Loss : 0.28697, Acc : 0.856, Sensitive_Loss : 0.09938, Sensitive_Acc : 17.100, Run Time : 11.87 sec
INFO:root:2024-04-28 03:12:08, Train, Epoch : 5, Step : 2560, Loss : 0.31625, Acc : 0.872, Sensitive_Loss : 0.09311, Sensitive_Acc : 16.400, Run Time : 10.99 sec
INFO:root:2024-04-28 03:12:19, Train, Epoch : 5, Step : 2570, Loss : 0.23931, Acc : 0.903, Sensitive_Loss : 0.08980, Sensitive_Acc : 16.700, Run Time : 10.97 sec
INFO:root:2024-04-28 03:12:30, Train, Epoch : 5, Step : 2580, Loss : 0.30672, Acc : 0.881, Sensitive_Loss : 0.11000, Sensitive_Acc : 16.900, Run Time : 11.03 sec
INFO:root:2024-04-28 03:12:42, Train, Epoch : 5, Step : 2590, Loss : 0.37156, Acc : 0.853, Sensitive_Loss : 0.08084, Sensitive_Acc : 15.700, Run Time : 11.75 sec
INFO:root:2024-04-28 03:12:54, Train, Epoch : 5, Step : 2600, Loss : 0.30586, Acc : 0.891, Sensitive_Loss : 0.09095, Sensitive_Acc : 17.300, Run Time : 11.67 sec
INFO:root:2024-04-28 03:15:28, Dev, Step : 2600, Loss : 0.39968, Acc : 0.831, Auc : 0.911, Sensitive_Loss : 0.11102, Sensitive_Acc : 16.779, Sensitive_Auc : 0.994, Mean auc: 0.911, Run Time : 154.10 sec
INFO:root:2024-04-28 03:15:37, Train, Epoch : 5, Step : 2610, Loss : 0.30637, Acc : 0.891, Sensitive_Loss : 0.11478, Sensitive_Acc : 15.900, Run Time : 162.72 sec
INFO:root:2024-04-28 03:15:48, Train, Epoch : 5, Step : 2620, Loss : 0.29992, Acc : 0.869, Sensitive_Loss : 0.10493, Sensitive_Acc : 16.600, Run Time : 11.74 sec
INFO:root:2024-04-28 03:15:59, Train, Epoch : 5, Step : 2630, Loss : 0.33263, Acc : 0.834, Sensitive_Loss : 0.11071, Sensitive_Acc : 15.800, Run Time : 10.88 sec
INFO:root:2024-04-28 03:16:12, Train, Epoch : 5, Step : 2640, Loss : 0.34757, Acc : 0.856, Sensitive_Loss : 0.10689, Sensitive_Acc : 15.700, Run Time : 12.60 sec
INFO:root:2024-04-28 03:16:22, Train, Epoch : 5, Step : 2650, Loss : 0.32788, Acc : 0.834, Sensitive_Loss : 0.11519, Sensitive_Acc : 15.800, Run Time : 10.47 sec
INFO:root:2024-04-28 03:16:34, Train, Epoch : 5, Step : 2660, Loss : 0.31809, Acc : 0.887, Sensitive_Loss : 0.09181, Sensitive_Acc : 17.100, Run Time : 11.81 sec
INFO:root:2024-04-28 03:16:45, Train, Epoch : 5, Step : 2670, Loss : 0.36645, Acc : 0.850, Sensitive_Loss : 0.08855, Sensitive_Acc : 15.900, Run Time : 11.29 sec
INFO:root:2024-04-28 03:16:57, Train, Epoch : 5, Step : 2680, Loss : 0.33126, Acc : 0.853, Sensitive_Loss : 0.07381, Sensitive_Acc : 16.800, Run Time : 11.40 sec
INFO:root:2024-04-28 03:17:09, Train, Epoch : 5, Step : 2690, Loss : 0.30295, Acc : 0.878, Sensitive_Loss : 0.11507, Sensitive_Acc : 16.200, Run Time : 11.84 sec
INFO:root:2024-04-28 03:17:19, Train, Epoch : 5, Step : 2700, Loss : 0.29781, Acc : 0.866, Sensitive_Loss : 0.07775, Sensitive_Acc : 16.500, Run Time : 10.72 sec
INFO:root:2024-04-28 03:19:55, Dev, Step : 2700, Loss : 0.43383, Acc : 0.813, Auc : 0.908, Sensitive_Loss : 0.12643, Sensitive_Acc : 16.764, Sensitive_Auc : 0.993, Mean auc: 0.908, Run Time : 155.26 sec
INFO:root:2024-04-28 03:20:04, Train, Epoch : 5, Step : 2710, Loss : 0.31758, Acc : 0.866, Sensitive_Loss : 0.10173, Sensitive_Acc : 17.100, Run Time : 164.93 sec
INFO:root:2024-04-28 03:20:15, Train, Epoch : 5, Step : 2720, Loss : 0.29887, Acc : 0.856, Sensitive_Loss : 0.07196, Sensitive_Acc : 15.700, Run Time : 10.90 sec
INFO:root:2024-04-28 03:20:27, Train, Epoch : 5, Step : 2730, Loss : 0.28461, Acc : 0.881, Sensitive_Loss : 0.09261, Sensitive_Acc : 14.400, Run Time : 11.76 sec
INFO:root:2024-04-28 03:20:38, Train, Epoch : 5, Step : 2740, Loss : 0.31624, Acc : 0.859, Sensitive_Loss : 0.08195, Sensitive_Acc : 16.300, Run Time : 10.98 sec
INFO:root:2024-04-28 03:20:50, Train, Epoch : 5, Step : 2750, Loss : 0.29235, Acc : 0.869, Sensitive_Loss : 0.09384, Sensitive_Acc : 14.900, Run Time : 11.71 sec
INFO:root:2024-04-28 03:21:01, Train, Epoch : 5, Step : 2760, Loss : 0.34350, Acc : 0.881, Sensitive_Loss : 0.09263, Sensitive_Acc : 17.000, Run Time : 11.88 sec
INFO:root:2024-04-28 03:21:13, Train, Epoch : 5, Step : 2770, Loss : 0.34183, Acc : 0.863, Sensitive_Loss : 0.12196, Sensitive_Acc : 15.700, Run Time : 11.74 sec
INFO:root:2024-04-28 03:21:25, Train, Epoch : 5, Step : 2780, Loss : 0.32903, Acc : 0.859, Sensitive_Loss : 0.08191, Sensitive_Acc : 16.800, Run Time : 11.43 sec
INFO:root:2024-04-28 03:21:35, Train, Epoch : 5, Step : 2790, Loss : 0.30569, Acc : 0.834, Sensitive_Loss : 0.07888, Sensitive_Acc : 17.700, Run Time : 10.83 sec
INFO:root:2024-04-28 03:21:47, Train, Epoch : 5, Step : 2800, Loss : 0.32665, Acc : 0.878, Sensitive_Loss : 0.08433, Sensitive_Acc : 15.400, Run Time : 11.16 sec
INFO:root:2024-04-28 03:24:21, Dev, Step : 2800, Loss : 0.41968, Acc : 0.819, Auc : 0.911, Sensitive_Loss : 0.12452, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 154.17 sec
INFO:root:2024-04-28 03:24:30, Train, Epoch : 5, Step : 2810, Loss : 0.32330, Acc : 0.853, Sensitive_Loss : 0.06786, Sensitive_Acc : 17.100, Run Time : 163.18 sec
INFO:root:2024-04-28 03:24:41, Train, Epoch : 5, Step : 2820, Loss : 0.29297, Acc : 0.881, Sensitive_Loss : 0.12098, Sensitive_Acc : 15.300, Run Time : 11.05 sec
INFO:root:2024-04-28 03:24:52, Train, Epoch : 5, Step : 2830, Loss : 0.32650, Acc : 0.869, Sensitive_Loss : 0.09186, Sensitive_Acc : 14.900, Run Time : 11.39 sec
INFO:root:2024-04-28 03:25:05, Train, Epoch : 5, Step : 2840, Loss : 0.26933, Acc : 0.894, Sensitive_Loss : 0.07938, Sensitive_Acc : 16.000, Run Time : 12.32 sec
INFO:root:2024-04-28 03:25:15, Train, Epoch : 5, Step : 2850, Loss : 0.27000, Acc : 0.887, Sensitive_Loss : 0.08204, Sensitive_Acc : 16.200, Run Time : 10.88 sec
INFO:root:2024-04-28 03:25:26, Train, Epoch : 5, Step : 2860, Loss : 0.30853, Acc : 0.863, Sensitive_Loss : 0.06349, Sensitive_Acc : 16.300, Run Time : 10.50 sec
INFO:root:2024-04-28 03:25:38, Train, Epoch : 5, Step : 2870, Loss : 0.29209, Acc : 0.869, Sensitive_Loss : 0.10218, Sensitive_Acc : 16.300, Run Time : 11.58 sec
INFO:root:2024-04-28 03:25:49, Train, Epoch : 5, Step : 2880, Loss : 0.31445, Acc : 0.859, Sensitive_Loss : 0.09981, Sensitive_Acc : 16.500, Run Time : 11.10 sec
INFO:root:2024-04-28 03:26:01, Train, Epoch : 5, Step : 2890, Loss : 0.33131, Acc : 0.844, Sensitive_Loss : 0.11438, Sensitive_Acc : 16.100, Run Time : 12.23 sec
INFO:root:2024-04-28 03:26:12, Train, Epoch : 5, Step : 2900, Loss : 0.37560, Acc : 0.844, Sensitive_Loss : 0.07208, Sensitive_Acc : 16.400, Run Time : 11.39 sec
INFO:root:2024-04-28 03:28:47, Dev, Step : 2900, Loss : 0.41526, Acc : 0.816, Auc : 0.909, Sensitive_Loss : 0.11266, Sensitive_Acc : 16.793, Sensitive_Auc : 0.993, Mean auc: 0.909, Run Time : 154.79 sec
INFO:root:2024-04-28 03:28:56, Train, Epoch : 5, Step : 2910, Loss : 0.31009, Acc : 0.916, Sensitive_Loss : 0.06263, Sensitive_Acc : 18.600, Run Time : 163.66 sec
INFO:root:2024-04-28 03:29:07, Train, Epoch : 5, Step : 2920, Loss : 0.32665, Acc : 0.869, Sensitive_Loss : 0.08083, Sensitive_Acc : 16.700, Run Time : 11.34 sec
INFO:root:2024-04-28 03:29:18, Train, Epoch : 5, Step : 2930, Loss : 0.35446, Acc : 0.872, Sensitive_Loss : 0.09509, Sensitive_Acc : 16.100, Run Time : 11.15 sec
INFO:root:2024-04-28 03:29:30, Train, Epoch : 5, Step : 2940, Loss : 0.30198, Acc : 0.863, Sensitive_Loss : 0.05136, Sensitive_Acc : 15.800, Run Time : 11.53 sec
INFO:root:2024-04-28 03:29:42, Train, Epoch : 5, Step : 2950, Loss : 0.27878, Acc : 0.859, Sensitive_Loss : 0.10957, Sensitive_Acc : 17.200, Run Time : 11.68 sec
INFO:root:2024-04-28 03:29:52, Train, Epoch : 5, Step : 2960, Loss : 0.31832, Acc : 0.838, Sensitive_Loss : 0.09105, Sensitive_Acc : 16.300, Run Time : 10.85 sec
INFO:root:2024-04-28 03:30:05, Train, Epoch : 5, Step : 2970, Loss : 0.25962, Acc : 0.894, Sensitive_Loss : 0.07678, Sensitive_Acc : 16.600, Run Time : 12.42 sec
INFO:root:2024-04-28 03:30:17, Train, Epoch : 5, Step : 2980, Loss : 0.28958, Acc : 0.869, Sensitive_Loss : 0.09459, Sensitive_Acc : 16.200, Run Time : 11.85 sec
INFO:root:2024-04-28 03:30:28, Train, Epoch : 5, Step : 2990, Loss : 0.32244, Acc : 0.859, Sensitive_Loss : 0.08132, Sensitive_Acc : 14.700, Run Time : 11.13 sec
INFO:root:2024-04-28 03:30:38, Train, Epoch : 5, Step : 3000, Loss : 0.30364, Acc : 0.859, Sensitive_Loss : 0.10161, Sensitive_Acc : 16.100, Run Time : 10.59 sec
INFO:root:2024-04-28 03:33:13, Dev, Step : 3000, Loss : 0.43248, Acc : 0.817, Auc : 0.910, Sensitive_Loss : 0.12525, Sensitive_Acc : 16.807, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 154.41 sec
INFO:root:2024-04-28 03:33:22, Train, Epoch : 5, Step : 3010, Loss : 0.31970, Acc : 0.863, Sensitive_Loss : 0.12385, Sensitive_Acc : 17.400, Run Time : 163.46 sec
INFO:root:2024-04-28 03:33:33, Train, Epoch : 5, Step : 3020, Loss : 0.28892, Acc : 0.875, Sensitive_Loss : 0.06911, Sensitive_Acc : 15.200, Run Time : 11.46 sec
INFO:root:2024-04-28 03:33:45, Train, Epoch : 5, Step : 3030, Loss : 0.29089, Acc : 0.894, Sensitive_Loss : 0.12085, Sensitive_Acc : 17.300, Run Time : 11.33 sec
INFO:root:2024-04-28 03:33:57, Train, Epoch : 5, Step : 3040, Loss : 0.40994, Acc : 0.794, Sensitive_Loss : 0.10564, Sensitive_Acc : 15.400, Run Time : 12.12 sec
INFO:root:2024-04-28 03:34:08, Train, Epoch : 5, Step : 3050, Loss : 0.32089, Acc : 0.863, Sensitive_Loss : 0.08992, Sensitive_Acc : 15.500, Run Time : 11.42 sec
INFO:root:2024-04-28 03:34:20, Train, Epoch : 5, Step : 3060, Loss : 0.33460, Acc : 0.866, Sensitive_Loss : 0.07460, Sensitive_Acc : 16.300, Run Time : 11.27 sec
INFO:root:2024-04-28 03:34:31, Train, Epoch : 5, Step : 3070, Loss : 0.36657, Acc : 0.828, Sensitive_Loss : 0.10720, Sensitive_Acc : 16.800, Run Time : 11.80 sec
INFO:root:2024-04-28 03:34:42, Train, Epoch : 5, Step : 3080, Loss : 0.40318, Acc : 0.831, Sensitive_Loss : 0.07275, Sensitive_Acc : 16.500, Run Time : 10.78 sec
INFO:root:2024-04-28 03:34:54, Train, Epoch : 5, Step : 3090, Loss : 0.32722, Acc : 0.847, Sensitive_Loss : 0.08351, Sensitive_Acc : 14.600, Run Time : 11.85 sec
INFO:root:2024-04-28 03:35:05, Train, Epoch : 5, Step : 3100, Loss : 0.34058, Acc : 0.838, Sensitive_Loss : 0.13063, Sensitive_Acc : 15.700, Run Time : 11.25 sec
INFO:root:2024-04-28 03:37:40, Dev, Step : 3100, Loss : 0.40887, Acc : 0.826, Auc : 0.911, Sensitive_Loss : 0.12197, Sensitive_Acc : 16.893, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 154.78 sec
INFO:root:2024-04-28 03:37:49, Train, Epoch : 5, Step : 3110, Loss : 0.35100, Acc : 0.841, Sensitive_Loss : 0.06326, Sensitive_Acc : 16.800, Run Time : 163.43 sec
INFO:root:2024-04-28 03:38:01, Train, Epoch : 5, Step : 3120, Loss : 0.30763, Acc : 0.881, Sensitive_Loss : 0.08582, Sensitive_Acc : 17.000, Run Time : 12.01 sec
INFO:root:2024-04-28 03:38:11, Train, Epoch : 5, Step : 3130, Loss : 0.29065, Acc : 0.881, Sensitive_Loss : 0.11685, Sensitive_Acc : 15.100, Run Time : 10.58 sec
INFO:root:2024-04-28 03:40:44
INFO:root:y_pred: [0.05780309 0.9410423  0.01193626 ... 0.80507094 0.00419723 0.87361723]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.80254114e-01 8.33237020e-04 2.16481999e-01 4.11054789e-05
 9.99862194e-01 3.79142672e-04 9.99983311e-01 9.99820173e-01
 1.94908818e-04 6.18416727e-01 9.98324811e-01 9.99992728e-01
 9.95804846e-01 9.91817296e-01 1.76511742e-02 9.89692390e-01
 9.99980807e-01 1.00265001e-03 5.55311382e-01 9.78425562e-01
 9.98415709e-01 8.49142298e-02 9.99584138e-01 9.93182719e-01
 9.99746978e-01 9.99842405e-01 1.04343897e-04 9.99528885e-01
 9.96236265e-01 3.18417042e-01 1.15295304e-02 7.58667946e-01
 1.44410776e-02 2.84893587e-02 2.97470093e-02 4.10590932e-04
 1.15894936e-02 2.41309218e-03 9.99788225e-01 9.99327421e-01
 9.70223846e-06 2.09192727e-02 9.86487448e-01 1.31322624e-04
 9.99898434e-01 9.99224424e-01 9.99509573e-01 9.75611925e-01
 4.92277704e-02 9.96878266e-01 9.97535348e-01 4.26067784e-03
 6.40146434e-01 1.43806217e-03 2.39624031e-04 4.60878899e-03
 4.23604622e-03 5.36282780e-03 4.35130531e-03 1.26239151e-01
 3.39148305e-02 9.78605971e-02 2.80228946e-02 9.90830541e-01
 3.04022372e-01 9.99938369e-01 3.27044330e-03 9.99676824e-01
 9.95910168e-01 4.36113477e-01 9.44035828e-01 6.13376796e-01
 1.56778505e-03 1.51212290e-01 5.34758321e-04 7.37376139e-03
 1.47406859e-02 8.89597237e-02 9.25906788e-05 9.99689698e-01
 9.99720514e-01 9.12562420e-04 3.72417867e-01 6.69765240e-03
 9.69504893e-01 9.27875340e-01 4.33013365e-02 5.00967354e-02
 9.78143573e-01 9.99789894e-01 9.99991536e-01 9.78924613e-03
 3.76535137e-03 9.99648213e-01 1.63128048e-01 2.88741780e-03
 9.91239488e-01 9.93666351e-01 1.58182476e-04 4.30362159e-03
 9.98312950e-01 9.98162568e-01 9.95871007e-01 9.97315109e-01
 1.37677044e-02 1.46847472e-01 9.94429588e-01 9.99527693e-01
 9.78579462e-01 5.27692428e-05 9.97253954e-01 9.87532794e-01
 1.09861605e-02 9.99821961e-01 9.99687433e-01 9.99964118e-01
 8.41897428e-01 9.99425292e-01 3.01678758e-02 4.53833610e-01
 9.99772489e-01 9.99619007e-01 7.52755848e-04 9.96446550e-01
 9.99996781e-01 3.73018682e-01 9.94446158e-01 4.83289687e-03
 9.16659757e-02 9.20084059e-01 9.97848630e-01 5.63539448e-04
 1.44967646e-03 2.81989891e-02 9.97608066e-01 9.96858597e-01
 9.83264863e-01 4.31487744e-04 6.82415813e-03 9.99033570e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 03:40:44, Dev, Step : 3130, Loss : 0.40584, Acc : 0.823, Auc : 0.911, Sensitive_Loss : 0.11336, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 153.07 sec
INFO:root:2024-04-28 03:40:58, Train, Epoch : 6, Step : 3140, Loss : 0.30996, Acc : 0.878, Sensitive_Loss : 0.08657, Sensitive_Acc : 16.700, Run Time : 12.97 sec
INFO:root:2024-04-28 03:41:10, Train, Epoch : 6, Step : 3150, Loss : 0.33202, Acc : 0.887, Sensitive_Loss : 0.10318, Sensitive_Acc : 15.700, Run Time : 12.16 sec
INFO:root:2024-04-28 03:41:22, Train, Epoch : 6, Step : 3160, Loss : 0.25862, Acc : 0.903, Sensitive_Loss : 0.07348, Sensitive_Acc : 17.000, Run Time : 11.72 sec
INFO:root:2024-04-28 03:41:34, Train, Epoch : 6, Step : 3170, Loss : 0.30947, Acc : 0.853, Sensitive_Loss : 0.06133, Sensitive_Acc : 16.800, Run Time : 11.40 sec
INFO:root:2024-04-28 03:41:44, Train, Epoch : 6, Step : 3180, Loss : 0.28315, Acc : 0.869, Sensitive_Loss : 0.08818, Sensitive_Acc : 17.300, Run Time : 10.39 sec
INFO:root:2024-04-28 03:41:56, Train, Epoch : 6, Step : 3190, Loss : 0.25447, Acc : 0.891, Sensitive_Loss : 0.08524, Sensitive_Acc : 17.100, Run Time : 11.73 sec
INFO:root:2024-04-28 03:42:07, Train, Epoch : 6, Step : 3200, Loss : 0.31829, Acc : 0.822, Sensitive_Loss : 0.11523, Sensitive_Acc : 15.500, Run Time : 11.68 sec
INFO:root:2024-04-28 03:44:42, Dev, Step : 3200, Loss : 0.45562, Acc : 0.807, Auc : 0.912, Sensitive_Loss : 0.13385, Sensitive_Acc : 16.764, Sensitive_Auc : 0.993, Mean auc: 0.912, Run Time : 154.94 sec
INFO:root:2024-04-28 03:44:52, Train, Epoch : 6, Step : 3210, Loss : 0.39536, Acc : 0.841, Sensitive_Loss : 0.11228, Sensitive_Acc : 17.300, Run Time : 164.31 sec
INFO:root:2024-04-28 03:45:03, Train, Epoch : 6, Step : 3220, Loss : 0.28463, Acc : 0.878, Sensitive_Loss : 0.08343, Sensitive_Acc : 15.900, Run Time : 10.98 sec
INFO:root:2024-04-28 03:45:14, Train, Epoch : 6, Step : 3230, Loss : 0.26410, Acc : 0.900, Sensitive_Loss : 0.09126, Sensitive_Acc : 16.800, Run Time : 10.89 sec
INFO:root:2024-04-28 03:45:25, Train, Epoch : 6, Step : 3240, Loss : 0.29887, Acc : 0.863, Sensitive_Loss : 0.07580, Sensitive_Acc : 16.200, Run Time : 11.64 sec
INFO:root:2024-04-28 03:45:37, Train, Epoch : 6, Step : 3250, Loss : 0.26065, Acc : 0.875, Sensitive_Loss : 0.09037, Sensitive_Acc : 16.000, Run Time : 11.68 sec
INFO:root:2024-04-28 03:45:48, Train, Epoch : 6, Step : 3260, Loss : 0.26586, Acc : 0.897, Sensitive_Loss : 0.12113, Sensitive_Acc : 15.700, Run Time : 10.86 sec
INFO:root:2024-04-28 03:46:00, Train, Epoch : 6, Step : 3270, Loss : 0.32322, Acc : 0.875, Sensitive_Loss : 0.09450, Sensitive_Acc : 16.300, Run Time : 12.02 sec
INFO:root:2024-04-28 03:46:11, Train, Epoch : 6, Step : 3280, Loss : 0.34223, Acc : 0.838, Sensitive_Loss : 0.09769, Sensitive_Acc : 15.900, Run Time : 11.63 sec
INFO:root:2024-04-28 03:46:23, Train, Epoch : 6, Step : 3290, Loss : 0.25855, Acc : 0.897, Sensitive_Loss : 0.10248, Sensitive_Acc : 16.600, Run Time : 11.84 sec
INFO:root:2024-04-28 03:46:35, Train, Epoch : 6, Step : 3300, Loss : 0.29351, Acc : 0.887, Sensitive_Loss : 0.08306, Sensitive_Acc : 16.400, Run Time : 11.32 sec
INFO:root:2024-04-28 03:49:09, Dev, Step : 3300, Loss : 0.41773, Acc : 0.820, Auc : 0.911, Sensitive_Loss : 0.12448, Sensitive_Acc : 16.793, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 154.05 sec
INFO:root:2024-04-28 03:49:18, Train, Epoch : 6, Step : 3310, Loss : 0.40076, Acc : 0.838, Sensitive_Loss : 0.10779, Sensitive_Acc : 15.700, Run Time : 163.10 sec
INFO:root:2024-04-28 03:49:30, Train, Epoch : 6, Step : 3320, Loss : 0.29242, Acc : 0.906, Sensitive_Loss : 0.08300, Sensitive_Acc : 17.000, Run Time : 12.01 sec
INFO:root:2024-04-28 03:49:41, Train, Epoch : 6, Step : 3330, Loss : 0.28416, Acc : 0.894, Sensitive_Loss : 0.06987, Sensitive_Acc : 15.400, Run Time : 11.02 sec
INFO:root:2024-04-28 03:49:51, Train, Epoch : 6, Step : 3340, Loss : 0.28140, Acc : 0.900, Sensitive_Loss : 0.07634, Sensitive_Acc : 16.900, Run Time : 10.63 sec
INFO:root:2024-04-28 03:50:04, Train, Epoch : 6, Step : 3350, Loss : 0.30686, Acc : 0.878, Sensitive_Loss : 0.08934, Sensitive_Acc : 15.900, Run Time : 12.64 sec
INFO:root:2024-04-28 03:50:16, Train, Epoch : 6, Step : 3360, Loss : 0.27003, Acc : 0.878, Sensitive_Loss : 0.08677, Sensitive_Acc : 16.600, Run Time : 11.74 sec
INFO:root:2024-04-28 03:50:27, Train, Epoch : 6, Step : 3370, Loss : 0.31235, Acc : 0.853, Sensitive_Loss : 0.07286, Sensitive_Acc : 17.100, Run Time : 10.83 sec
INFO:root:2024-04-28 03:50:37, Train, Epoch : 6, Step : 3380, Loss : 0.27608, Acc : 0.881, Sensitive_Loss : 0.10254, Sensitive_Acc : 15.100, Run Time : 10.86 sec
INFO:root:2024-04-28 03:50:49, Train, Epoch : 6, Step : 3390, Loss : 0.35220, Acc : 0.844, Sensitive_Loss : 0.14338, Sensitive_Acc : 14.500, Run Time : 11.73 sec
INFO:root:2024-04-28 03:51:00, Train, Epoch : 6, Step : 3400, Loss : 0.29465, Acc : 0.859, Sensitive_Loss : 0.07876, Sensitive_Acc : 15.900, Run Time : 10.99 sec
INFO:root:2024-04-28 03:53:35, Dev, Step : 3400, Loss : 0.42444, Acc : 0.819, Auc : 0.910, Sensitive_Loss : 0.11328, Sensitive_Acc : 16.793, Sensitive_Auc : 0.994, Mean auc: 0.910, Run Time : 154.44 sec
INFO:root:2024-04-28 03:53:44, Train, Epoch : 6, Step : 3410, Loss : 0.28961, Acc : 0.856, Sensitive_Loss : 0.10077, Sensitive_Acc : 16.700, Run Time : 163.73 sec
INFO:root:2024-04-28 03:53:55, Train, Epoch : 6, Step : 3420, Loss : 0.26916, Acc : 0.897, Sensitive_Loss : 0.07238, Sensitive_Acc : 16.300, Run Time : 11.11 sec
INFO:root:2024-04-28 03:54:06, Train, Epoch : 6, Step : 3430, Loss : 0.29271, Acc : 0.863, Sensitive_Loss : 0.14562, Sensitive_Acc : 16.500, Run Time : 11.43 sec
INFO:root:2024-04-28 03:54:17, Train, Epoch : 6, Step : 3440, Loss : 0.27617, Acc : 0.869, Sensitive_Loss : 0.09889, Sensitive_Acc : 15.300, Run Time : 10.81 sec
INFO:root:2024-04-28 03:54:29, Train, Epoch : 6, Step : 3450, Loss : 0.32502, Acc : 0.863, Sensitive_Loss : 0.08740, Sensitive_Acc : 17.700, Run Time : 11.70 sec
INFO:root:2024-04-28 03:54:41, Train, Epoch : 6, Step : 3460, Loss : 0.27977, Acc : 0.863, Sensitive_Loss : 0.10995, Sensitive_Acc : 15.400, Run Time : 11.67 sec
INFO:root:2024-04-28 03:54:52, Train, Epoch : 6, Step : 3470, Loss : 0.22969, Acc : 0.909, Sensitive_Loss : 0.09881, Sensitive_Acc : 16.900, Run Time : 11.61 sec
INFO:root:2024-04-28 03:55:03, Train, Epoch : 6, Step : 3480, Loss : 0.30648, Acc : 0.863, Sensitive_Loss : 0.09017, Sensitive_Acc : 16.000, Run Time : 10.99 sec
INFO:root:2024-04-28 03:55:15, Train, Epoch : 6, Step : 3490, Loss : 0.28914, Acc : 0.859, Sensitive_Loss : 0.10122, Sensitive_Acc : 16.100, Run Time : 11.80 sec
INFO:root:2024-04-28 03:55:27, Train, Epoch : 6, Step : 3500, Loss : 0.27540, Acc : 0.872, Sensitive_Loss : 0.09375, Sensitive_Acc : 17.000, Run Time : 11.86 sec
INFO:root:2024-04-28 03:58:02, Dev, Step : 3500, Loss : 0.43397, Acc : 0.820, Auc : 0.911, Sensitive_Loss : 0.12731, Sensitive_Acc : 16.779, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 155.18 sec
INFO:root:2024-04-28 03:58:11, Train, Epoch : 6, Step : 3510, Loss : 0.29874, Acc : 0.869, Sensitive_Loss : 0.06990, Sensitive_Acc : 16.400, Run Time : 163.68 sec
INFO:root:2024-04-28 03:58:22, Train, Epoch : 6, Step : 3520, Loss : 0.37471, Acc : 0.834, Sensitive_Loss : 0.06931, Sensitive_Acc : 15.600, Run Time : 11.77 sec
INFO:root:2024-04-28 03:58:34, Train, Epoch : 6, Step : 3530, Loss : 0.29442, Acc : 0.850, Sensitive_Loss : 0.11745, Sensitive_Acc : 17.400, Run Time : 11.57 sec
INFO:root:2024-04-28 03:58:45, Train, Epoch : 6, Step : 3540, Loss : 0.29174, Acc : 0.878, Sensitive_Loss : 0.06673, Sensitive_Acc : 15.700, Run Time : 10.78 sec
INFO:root:2024-04-28 03:58:57, Train, Epoch : 6, Step : 3550, Loss : 0.27945, Acc : 0.875, Sensitive_Loss : 0.11221, Sensitive_Acc : 16.400, Run Time : 11.85 sec
INFO:root:2024-04-28 03:59:08, Train, Epoch : 6, Step : 3560, Loss : 0.28569, Acc : 0.866, Sensitive_Loss : 0.09831, Sensitive_Acc : 14.500, Run Time : 11.40 sec
INFO:root:2024-04-28 03:59:20, Train, Epoch : 6, Step : 3570, Loss : 0.27607, Acc : 0.875, Sensitive_Loss : 0.09754, Sensitive_Acc : 15.500, Run Time : 11.60 sec
INFO:root:2024-04-28 03:59:31, Train, Epoch : 6, Step : 3580, Loss : 0.28839, Acc : 0.891, Sensitive_Loss : 0.12651, Sensitive_Acc : 14.900, Run Time : 11.68 sec
INFO:root:2024-04-28 03:59:42, Train, Epoch : 6, Step : 3590, Loss : 0.32274, Acc : 0.878, Sensitive_Loss : 0.06778, Sensitive_Acc : 15.900, Run Time : 11.24 sec
INFO:root:2024-04-28 03:59:53, Train, Epoch : 6, Step : 3600, Loss : 0.28288, Acc : 0.897, Sensitive_Loss : 0.08670, Sensitive_Acc : 16.100, Run Time : 10.24 sec
INFO:root:2024-04-28 04:02:28, Dev, Step : 3600, Loss : 0.40512, Acc : 0.825, Auc : 0.909, Sensitive_Loss : 0.10522, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.909, Run Time : 155.33 sec
INFO:root:2024-04-28 04:02:37, Train, Epoch : 6, Step : 3610, Loss : 0.22532, Acc : 0.909, Sensitive_Loss : 0.05391, Sensitive_Acc : 15.300, Run Time : 164.11 sec
INFO:root:2024-04-28 04:02:49, Train, Epoch : 6, Step : 3620, Loss : 0.35505, Acc : 0.850, Sensitive_Loss : 0.07002, Sensitive_Acc : 15.200, Run Time : 12.33 sec
INFO:root:2024-04-28 04:03:01, Train, Epoch : 6, Step : 3630, Loss : 0.34496, Acc : 0.841, Sensitive_Loss : 0.13940, Sensitive_Acc : 18.800, Run Time : 11.37 sec
INFO:root:2024-04-28 04:03:12, Train, Epoch : 6, Step : 3640, Loss : 0.22581, Acc : 0.909, Sensitive_Loss : 0.06862, Sensitive_Acc : 15.500, Run Time : 11.20 sec
INFO:root:2024-04-28 04:03:22, Train, Epoch : 6, Step : 3650, Loss : 0.34159, Acc : 0.841, Sensitive_Loss : 0.13156, Sensitive_Acc : 15.400, Run Time : 10.37 sec
INFO:root:2024-04-28 04:03:34, Train, Epoch : 6, Step : 3660, Loss : 0.30473, Acc : 0.891, Sensitive_Loss : 0.06197, Sensitive_Acc : 17.300, Run Time : 11.98 sec
INFO:root:2024-04-28 04:03:45, Train, Epoch : 6, Step : 3670, Loss : 0.23581, Acc : 0.900, Sensitive_Loss : 0.08307, Sensitive_Acc : 15.100, Run Time : 10.74 sec
INFO:root:2024-04-28 04:03:56, Train, Epoch : 6, Step : 3680, Loss : 0.33538, Acc : 0.853, Sensitive_Loss : 0.07149, Sensitive_Acc : 17.400, Run Time : 11.45 sec
INFO:root:2024-04-28 04:04:09, Train, Epoch : 6, Step : 3690, Loss : 0.28327, Acc : 0.856, Sensitive_Loss : 0.11536, Sensitive_Acc : 16.800, Run Time : 12.48 sec
INFO:root:2024-04-28 04:04:20, Train, Epoch : 6, Step : 3700, Loss : 0.32757, Acc : 0.891, Sensitive_Loss : 0.08120, Sensitive_Acc : 17.100, Run Time : 11.73 sec
INFO:root:2024-04-28 04:06:54, Dev, Step : 3700, Loss : 0.43268, Acc : 0.819, Auc : 0.910, Sensitive_Loss : 0.12426, Sensitive_Acc : 16.836, Sensitive_Auc : 0.994, Mean auc: 0.910, Run Time : 153.91 sec
INFO:root:2024-04-28 04:07:03, Train, Epoch : 6, Step : 3710, Loss : 0.27693, Acc : 0.897, Sensitive_Loss : 0.09520, Sensitive_Acc : 16.600, Run Time : 162.39 sec
INFO:root:2024-04-28 04:07:15, Train, Epoch : 6, Step : 3720, Loss : 0.32227, Acc : 0.853, Sensitive_Loss : 0.11623, Sensitive_Acc : 15.900, Run Time : 11.95 sec
INFO:root:2024-04-28 04:07:26, Train, Epoch : 6, Step : 3730, Loss : 0.33060, Acc : 0.869, Sensitive_Loss : 0.13348, Sensitive_Acc : 16.700, Run Time : 10.83 sec
INFO:root:2024-04-28 04:07:38, Train, Epoch : 6, Step : 3740, Loss : 0.29753, Acc : 0.869, Sensitive_Loss : 0.09895, Sensitive_Acc : 14.800, Run Time : 12.05 sec
INFO:root:2024-04-28 04:07:49, Train, Epoch : 6, Step : 3750, Loss : 0.37178, Acc : 0.863, Sensitive_Loss : 0.11498, Sensitive_Acc : 15.500, Run Time : 10.82 sec
INFO:root:2024-04-28 04:10:27
INFO:root:y_pred: [0.03872402 0.94835204 0.01148685 ... 0.7455209  0.00564301 0.85867286]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.91259277e-01 1.33132131e-03 2.54801005e-01 2.23074512e-05
 9.99926805e-01 4.71122301e-04 9.99987960e-01 9.99892116e-01
 1.97062458e-04 7.58362114e-01 9.98874605e-01 9.99984980e-01
 9.97452199e-01 9.86373901e-01 1.57785397e-02 9.88447011e-01
 9.99988914e-01 2.49322341e-03 6.33258760e-01 9.73931253e-01
 9.98977184e-01 6.55761212e-02 9.99607503e-01 9.95921016e-01
 9.99785960e-01 9.99869704e-01 6.10591378e-05 9.99086738e-01
 9.96291637e-01 3.04420412e-01 2.57032439e-02 8.01391244e-01
 6.43146317e-03 4.09674272e-02 3.62511277e-02 4.17898176e-04
 8.37972015e-03 2.41220742e-03 9.99884605e-01 9.99273717e-01
 8.60134151e-06 1.49335694e-02 9.92190897e-01 3.54440563e-04
 9.99880672e-01 9.99452174e-01 9.99687910e-01 9.73788321e-01
 4.07539085e-02 9.98044372e-01 9.98672128e-01 4.30204021e-03
 7.98896968e-01 8.25294876e-04 1.92623993e-04 2.20318940e-02
 5.24467696e-03 5.29435836e-03 7.06440769e-03 2.59132624e-01
 9.66897979e-02 1.45936340e-01 3.81313078e-02 9.94986653e-01
 2.96321899e-01 9.99947906e-01 1.78347190e-03 9.99839902e-01
 9.94238734e-01 3.68335962e-01 8.89352858e-01 6.81430995e-01
 2.32617720e-03 1.98154271e-01 3.69028246e-04 4.49323235e-03
 1.05214035e-02 1.14763200e-01 1.28816217e-04 9.99740541e-01
 9.99722421e-01 1.86407159e-03 4.51980114e-01 3.92442616e-03
 9.44922388e-01 9.53009844e-01 1.29327141e-02 8.95348042e-02
 9.77808774e-01 9.99835968e-01 9.99994159e-01 4.99958638e-03
 1.91077066e-03 9.99680638e-01 2.33300030e-01 3.53576499e-03
 9.97539282e-01 9.98002350e-01 1.44789883e-04 4.77705337e-03
 9.98686969e-01 9.98622894e-01 9.97674763e-01 9.97634292e-01
 2.00348869e-02 1.38179094e-01 9.95747149e-01 9.99456823e-01
 9.83107328e-01 3.72156537e-05 9.97294009e-01 9.91105378e-01
 1.88510697e-02 9.99610364e-01 9.99715626e-01 9.99969006e-01
 8.71996880e-01 9.99181211e-01 4.29795533e-02 3.12579006e-01
 9.99821126e-01 9.99521136e-01 9.24558612e-04 9.96226072e-01
 9.99994874e-01 4.16596889e-01 9.94349420e-01 5.57701010e-03
 9.06082243e-02 9.13718939e-01 9.97797132e-01 5.62167377e-04
 1.19340664e-03 3.31324451e-02 9.97295558e-01 9.97315466e-01
 9.86454308e-01 2.96779908e-04 5.98945841e-03 9.99568284e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 04:10:27, Dev, Step : 3756, Loss : 0.42270, Acc : 0.820, Auc : 0.911, Sensitive_Loss : 0.11294, Sensitive_Acc : 16.850, Sensitive_Auc : 0.994, Mean auc: 0.911, Run Time : 152.93 sec
INFO:root:2024-04-28 04:10:35, Train, Epoch : 7, Step : 3760, Loss : 0.13931, Acc : 0.344, Sensitive_Loss : 0.03227, Sensitive_Acc : 7.000, Run Time : 6.65 sec
INFO:root:2024-04-28 04:10:46, Train, Epoch : 7, Step : 3770, Loss : 0.29648, Acc : 0.838, Sensitive_Loss : 0.10020, Sensitive_Acc : 15.800, Run Time : 11.20 sec
INFO:root:2024-04-28 04:10:58, Train, Epoch : 7, Step : 3780, Loss : 0.22326, Acc : 0.912, Sensitive_Loss : 0.08958, Sensitive_Acc : 16.000, Run Time : 11.68 sec
INFO:root:2024-04-28 04:11:09, Train, Epoch : 7, Step : 3790, Loss : 0.27720, Acc : 0.884, Sensitive_Loss : 0.07896, Sensitive_Acc : 15.700, Run Time : 10.70 sec
INFO:root:2024-04-28 04:11:21, Train, Epoch : 7, Step : 3800, Loss : 0.27983, Acc : 0.875, Sensitive_Loss : 0.06270, Sensitive_Acc : 16.400, Run Time : 12.06 sec
INFO:root:2024-04-28 04:13:55, Dev, Step : 3800, Loss : 0.41452, Acc : 0.825, Auc : 0.912, Sensitive_Loss : 0.10728, Sensitive_Acc : 16.850, Sensitive_Auc : 0.995, Mean auc: 0.912, Run Time : 153.96 sec
INFO:root:2024-04-28 04:14:04, Train, Epoch : 7, Step : 3810, Loss : 0.26601, Acc : 0.897, Sensitive_Loss : 0.09454, Sensitive_Acc : 15.800, Run Time : 163.05 sec
INFO:root:2024-04-28 04:14:15, Train, Epoch : 7, Step : 3820, Loss : 0.33557, Acc : 0.841, Sensitive_Loss : 0.11272, Sensitive_Acc : 16.900, Run Time : 10.85 sec
INFO:root:2024-04-28 04:14:26, Train, Epoch : 7, Step : 3830, Loss : 0.26245, Acc : 0.897, Sensitive_Loss : 0.15964, Sensitive_Acc : 16.100, Run Time : 11.69 sec
INFO:root:2024-04-28 04:14:38, Train, Epoch : 7, Step : 3840, Loss : 0.24334, Acc : 0.884, Sensitive_Loss : 0.06844, Sensitive_Acc : 17.200, Run Time : 11.18 sec
INFO:root:2024-04-28 04:14:49, Train, Epoch : 7, Step : 3850, Loss : 0.30746, Acc : 0.884, Sensitive_Loss : 0.09543, Sensitive_Acc : 15.700, Run Time : 11.31 sec
INFO:root:2024-04-28 04:15:00, Train, Epoch : 7, Step : 3860, Loss : 0.23528, Acc : 0.912, Sensitive_Loss : 0.07195, Sensitive_Acc : 16.500, Run Time : 11.55 sec
INFO:root:2024-04-28 04:15:12, Train, Epoch : 7, Step : 3870, Loss : 0.25670, Acc : 0.875, Sensitive_Loss : 0.09632, Sensitive_Acc : 18.100, Run Time : 11.71 sec
INFO:root:2024-04-28 04:15:24, Train, Epoch : 7, Step : 3880, Loss : 0.24068, Acc : 0.881, Sensitive_Loss : 0.11245, Sensitive_Acc : 17.000, Run Time : 11.50 sec
INFO:root:2024-04-28 04:15:35, Train, Epoch : 7, Step : 3890, Loss : 0.34599, Acc : 0.850, Sensitive_Loss : 0.06347, Sensitive_Acc : 16.600, Run Time : 11.34 sec
INFO:root:2024-04-28 04:15:46, Train, Epoch : 7, Step : 3900, Loss : 0.31325, Acc : 0.869, Sensitive_Loss : 0.11377, Sensitive_Acc : 14.900, Run Time : 11.19 sec
INFO:root:2024-04-28 04:18:22, Dev, Step : 3900, Loss : 0.44734, Acc : 0.816, Auc : 0.909, Sensitive_Loss : 0.11897, Sensitive_Acc : 16.793, Sensitive_Auc : 0.994, Mean auc: 0.909, Run Time : 155.56 sec
INFO:root:2024-04-28 04:18:31, Train, Epoch : 7, Step : 3910, Loss : 0.23827, Acc : 0.912, Sensitive_Loss : 0.10527, Sensitive_Acc : 16.600, Run Time : 164.40 sec
INFO:root:2024-04-28 04:18:42, Train, Epoch : 7, Step : 3920, Loss : 0.29573, Acc : 0.900, Sensitive_Loss : 0.10056, Sensitive_Acc : 18.000, Run Time : 11.12 sec
INFO:root:2024-04-28 04:18:54, Train, Epoch : 7, Step : 3930, Loss : 0.25396, Acc : 0.887, Sensitive_Loss : 0.07340, Sensitive_Acc : 17.200, Run Time : 11.94 sec
INFO:root:2024-04-28 04:19:06, Train, Epoch : 7, Step : 3940, Loss : 0.32541, Acc : 0.856, Sensitive_Loss : 0.11317, Sensitive_Acc : 15.600, Run Time : 11.94 sec
INFO:root:2024-04-28 04:19:16, Train, Epoch : 7, Step : 3950, Loss : 0.24058, Acc : 0.900, Sensitive_Loss : 0.05922, Sensitive_Acc : 17.000, Run Time : 10.66 sec
INFO:root:2024-04-28 04:19:27, Train, Epoch : 7, Step : 3960, Loss : 0.25034, Acc : 0.909, Sensitive_Loss : 0.12145, Sensitive_Acc : 17.700, Run Time : 10.93 sec
INFO:root:2024-04-28 04:19:39, Train, Epoch : 7, Step : 3970, Loss : 0.31534, Acc : 0.878, Sensitive_Loss : 0.09946, Sensitive_Acc : 17.300, Run Time : 12.35 sec
INFO:root:2024-04-28 04:19:51, Train, Epoch : 7, Step : 3980, Loss : 0.22322, Acc : 0.903, Sensitive_Loss : 0.11060, Sensitive_Acc : 16.900, Run Time : 11.43 sec
INFO:root:2024-04-28 04:20:02, Train, Epoch : 7, Step : 3990, Loss : 0.26590, Acc : 0.881, Sensitive_Loss : 0.07945, Sensitive_Acc : 16.500, Run Time : 11.58 sec
INFO:root:2024-04-28 04:20:14, Train, Epoch : 7, Step : 4000, Loss : 0.25157, Acc : 0.912, Sensitive_Loss : 0.08151, Sensitive_Acc : 16.100, Run Time : 11.13 sec
INFO:root:2024-04-28 04:23:32, Dev, Step : 4000, Loss : 0.43557, Acc : 0.821, Auc : 0.909, Sensitive_Loss : 0.11613, Sensitive_Acc : 16.836, Sensitive_Auc : 0.995, Mean auc: 0.909, Run Time : 198.20 sec
INFO:root:2024-04-28 04:23:40, Train, Epoch : 7, Step : 4010, Loss : 0.27994, Acc : 0.872, Sensitive_Loss : 0.06527, Sensitive_Acc : 15.900, Run Time : 206.32 sec
INFO:root:2024-04-28 04:23:53, Train, Epoch : 7, Step : 4020, Loss : 0.23532, Acc : 0.891, Sensitive_Loss : 0.06783, Sensitive_Acc : 15.600, Run Time : 12.89 sec
INFO:root:2024-04-28 04:24:16, Train, Epoch : 7, Step : 4030, Loss : 0.24607, Acc : 0.884, Sensitive_Loss : 0.07992, Sensitive_Acc : 17.300, Run Time : 23.00 sec
INFO:root:2024-04-28 04:24:28, Train, Epoch : 7, Step : 4040, Loss : 0.26314, Acc : 0.900, Sensitive_Loss : 0.11345, Sensitive_Acc : 16.200, Run Time : 11.85 sec
INFO:root:2024-04-28 04:24:38, Train, Epoch : 7, Step : 4050, Loss : 0.32661, Acc : 0.887, Sensitive_Loss : 0.09217, Sensitive_Acc : 16.300, Run Time : 10.40 sec
INFO:root:2024-04-28 04:24:50, Train, Epoch : 7, Step : 4060, Loss : 0.32521, Acc : 0.847, Sensitive_Loss : 0.09184, Sensitive_Acc : 15.300, Run Time : 11.80 sec
INFO:root:2024-04-28 04:25:01, Train, Epoch : 7, Step : 4070, Loss : 0.32679, Acc : 0.869, Sensitive_Loss : 0.07843, Sensitive_Acc : 16.000, Run Time : 11.43 sec
INFO:root:2024-04-28 04:25:12, Train, Epoch : 7, Step : 4080, Loss : 0.27414, Acc : 0.891, Sensitive_Loss : 0.12906, Sensitive_Acc : 16.800, Run Time : 11.20 sec
INFO:root:2024-04-28 04:25:24, Train, Epoch : 7, Step : 4090, Loss : 0.29015, Acc : 0.859, Sensitive_Loss : 0.05906, Sensitive_Acc : 16.500, Run Time : 11.28 sec
INFO:root:2024-04-28 04:25:36, Train, Epoch : 7, Step : 4100, Loss : 0.29126, Acc : 0.872, Sensitive_Loss : 0.07925, Sensitive_Acc : 16.300, Run Time : 11.76 sec
INFO:root:2024-04-28 04:28:13, Dev, Step : 4100, Loss : 0.44298, Acc : 0.815, Auc : 0.907, Sensitive_Loss : 0.11988, Sensitive_Acc : 16.836, Sensitive_Auc : 0.994, Mean auc: 0.907, Run Time : 157.74 sec
INFO:root:2024-04-28 04:28:22, Train, Epoch : 7, Step : 4110, Loss : 0.27560, Acc : 0.881, Sensitive_Loss : 0.11587, Sensitive_Acc : 15.400, Run Time : 166.37 sec
INFO:root:2024-04-28 04:28:33, Train, Epoch : 7, Step : 4120, Loss : 0.23068, Acc : 0.878, Sensitive_Loss : 0.09761, Sensitive_Acc : 17.100, Run Time : 11.38 sec
INFO:root:2024-04-28 04:28:49, Train, Epoch : 7, Step : 4130, Loss : 0.30946, Acc : 0.847, Sensitive_Loss : 0.08541, Sensitive_Acc : 16.300, Run Time : 15.77 sec
INFO:root:2024-04-28 04:29:00, Train, Epoch : 7, Step : 4140, Loss : 0.22091, Acc : 0.884, Sensitive_Loss : 0.06497, Sensitive_Acc : 16.200, Run Time : 11.38 sec
INFO:root:2024-04-28 04:29:11, Train, Epoch : 7, Step : 4150, Loss : 0.26883, Acc : 0.872, Sensitive_Loss : 0.08747, Sensitive_Acc : 16.800, Run Time : 10.53 sec
INFO:root:2024-04-28 04:29:23, Train, Epoch : 7, Step : 4160, Loss : 0.24348, Acc : 0.878, Sensitive_Loss : 0.06792, Sensitive_Acc : 14.300, Run Time : 11.59 sec
INFO:root:2024-04-28 04:29:36, Train, Epoch : 7, Step : 4170, Loss : 0.30416, Acc : 0.872, Sensitive_Loss : 0.10700, Sensitive_Acc : 16.300, Run Time : 13.82 sec
INFO:root:2024-04-28 04:29:48, Train, Epoch : 7, Step : 4180, Loss : 0.25466, Acc : 0.881, Sensitive_Loss : 0.07764, Sensitive_Acc : 16.800, Run Time : 11.47 sec
INFO:root:2024-04-28 04:29:59, Train, Epoch : 7, Step : 4190, Loss : 0.26576, Acc : 0.894, Sensitive_Loss : 0.09357, Sensitive_Acc : 17.500, Run Time : 11.00 sec
INFO:root:2024-04-28 04:30:11, Train, Epoch : 7, Step : 4200, Loss : 0.29064, Acc : 0.881, Sensitive_Loss : 0.08333, Sensitive_Acc : 15.800, Run Time : 11.76 sec
INFO:root:2024-04-28 04:32:47, Dev, Step : 4200, Loss : 0.42190, Acc : 0.824, Auc : 0.910, Sensitive_Loss : 0.11279, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 155.90 sec
INFO:root:2024-04-28 04:32:55, Train, Epoch : 7, Step : 4210, Loss : 0.24123, Acc : 0.909, Sensitive_Loss : 0.11046, Sensitive_Acc : 17.500, Run Time : 164.38 sec
INFO:root:2024-04-28 04:33:10, Train, Epoch : 7, Step : 4220, Loss : 0.27576, Acc : 0.900, Sensitive_Loss : 0.06710, Sensitive_Acc : 17.000, Run Time : 14.73 sec
INFO:root:2024-04-28 04:33:22, Train, Epoch : 7, Step : 4230, Loss : 0.27360, Acc : 0.900, Sensitive_Loss : 0.09614, Sensitive_Acc : 17.900, Run Time : 11.93 sec
INFO:root:2024-04-28 04:33:32, Train, Epoch : 7, Step : 4240, Loss : 0.26032, Acc : 0.884, Sensitive_Loss : 0.12277, Sensitive_Acc : 17.100, Run Time : 10.81 sec
INFO:root:2024-04-28 04:33:45, Train, Epoch : 7, Step : 4250, Loss : 0.25875, Acc : 0.884, Sensitive_Loss : 0.06960, Sensitive_Acc : 15.500, Run Time : 12.05 sec
INFO:root:2024-04-28 04:33:59, Train, Epoch : 7, Step : 4260, Loss : 0.22722, Acc : 0.909, Sensitive_Loss : 0.07327, Sensitive_Acc : 16.800, Run Time : 14.37 sec
INFO:root:2024-04-28 04:34:12, Train, Epoch : 7, Step : 4270, Loss : 0.33852, Acc : 0.859, Sensitive_Loss : 0.08370, Sensitive_Acc : 14.900, Run Time : 12.83 sec
INFO:root:2024-04-28 04:34:21, Train, Epoch : 7, Step : 4280, Loss : 0.29159, Acc : 0.887, Sensitive_Loss : 0.08792, Sensitive_Acc : 16.600, Run Time : 9.55 sec
INFO:root:2024-04-28 04:34:33, Train, Epoch : 7, Step : 4290, Loss : 0.24416, Acc : 0.859, Sensitive_Loss : 0.07598, Sensitive_Acc : 16.800, Run Time : 11.77 sec
INFO:root:2024-04-28 04:34:44, Train, Epoch : 7, Step : 4300, Loss : 0.25261, Acc : 0.878, Sensitive_Loss : 0.08847, Sensitive_Acc : 14.900, Run Time : 11.19 sec
INFO:root:2024-04-28 04:37:20, Dev, Step : 4300, Loss : 0.43832, Acc : 0.821, Auc : 0.910, Sensitive_Loss : 0.11869, Sensitive_Acc : 16.793, Sensitive_Auc : 0.994, Mean auc: 0.910, Run Time : 155.75 sec
INFO:root:2024-04-28 04:37:29, Train, Epoch : 7, Step : 4310, Loss : 0.25107, Acc : 0.891, Sensitive_Loss : 0.09513, Sensitive_Acc : 16.700, Run Time : 164.75 sec
INFO:root:2024-04-28 04:37:40, Train, Epoch : 7, Step : 4320, Loss : 0.27720, Acc : 0.891, Sensitive_Loss : 0.07841, Sensitive_Acc : 16.500, Run Time : 11.47 sec
INFO:root:2024-04-28 04:37:51, Train, Epoch : 7, Step : 4330, Loss : 0.28835, Acc : 0.872, Sensitive_Loss : 0.10912, Sensitive_Acc : 15.400, Run Time : 10.90 sec
INFO:root:2024-04-28 04:38:03, Train, Epoch : 7, Step : 4340, Loss : 0.26704, Acc : 0.897, Sensitive_Loss : 0.05143, Sensitive_Acc : 18.800, Run Time : 12.03 sec
INFO:root:2024-04-28 04:38:14, Train, Epoch : 7, Step : 4350, Loss : 0.29356, Acc : 0.872, Sensitive_Loss : 0.07724, Sensitive_Acc : 15.400, Run Time : 10.93 sec
INFO:root:2024-04-28 04:38:26, Train, Epoch : 7, Step : 4360, Loss : 0.30411, Acc : 0.863, Sensitive_Loss : 0.09070, Sensitive_Acc : 15.900, Run Time : 11.75 sec
INFO:root:2024-04-28 04:38:37, Train, Epoch : 7, Step : 4370, Loss : 0.32163, Acc : 0.866, Sensitive_Loss : 0.09321, Sensitive_Acc : 16.400, Run Time : 11.07 sec
INFO:root:2024-04-28 04:38:49, Train, Epoch : 7, Step : 4380, Loss : 0.32171, Acc : 0.887, Sensitive_Loss : 0.08293, Sensitive_Acc : 16.400, Run Time : 11.67 sec
INFO:root:2024-04-28 04:41:23
INFO:root:y_pred: [0.04645458 0.9719406  0.01283661 ... 0.7716593  0.007791   0.8714089 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.96627867e-01 1.94854592e-03 2.95450240e-01 3.20568943e-05
 9.99851584e-01 7.50573876e-04 9.99979973e-01 9.99937177e-01
 1.07913642e-04 8.49169791e-01 9.99137759e-01 9.99986410e-01
 9.97167408e-01 9.88334537e-01 2.17346773e-02 9.90464568e-01
 9.99977112e-01 2.81712622e-03 6.83132887e-01 9.72002268e-01
 9.99403596e-01 4.68034558e-02 9.99703109e-01 9.94892240e-01
 9.99741256e-01 9.99825656e-01 9.66417720e-05 9.98970270e-01
 9.97846365e-01 2.68129766e-01 3.64642851e-02 8.99204612e-01
 1.54974274e-02 5.51595576e-02 2.51833312e-02 8.91999167e-04
 1.14646340e-02 3.76826059e-03 9.99853015e-01 9.99297142e-01
 1.23090986e-05 8.67731031e-03 9.91402447e-01 5.89950883e-04
 9.99907255e-01 9.99604523e-01 9.99505639e-01 9.61695552e-01
 4.62053381e-02 9.96969759e-01 9.98447359e-01 7.57840322e-03
 7.41817296e-01 1.96926971e-03 2.66150339e-04 1.50548490e-02
 5.11020049e-03 4.45007114e-03 9.62847192e-03 3.92887115e-01
 1.01655178e-01 1.35036916e-01 4.66223210e-02 9.90050316e-01
 3.06691915e-01 9.99956250e-01 2.45799194e-03 9.99923587e-01
 9.97922480e-01 5.33353448e-01 9.57452714e-01 6.81160986e-01
 2.57045985e-03 1.26678184e-01 8.33399477e-04 1.32128457e-02
 7.28478655e-03 1.52964249e-01 1.37155308e-04 9.99727309e-01
 9.99595106e-01 2.47274665e-03 5.33984542e-01 3.69271985e-03
 9.62053359e-01 9.64167714e-01 1.60532985e-02 1.16205186e-01
 9.86243308e-01 9.99903083e-01 9.99989867e-01 2.39575515e-03
 2.05357163e-03 9.99738753e-01 3.75457346e-01 4.80946060e-03
 9.98189509e-01 9.97012615e-01 4.20805591e-04 9.56599321e-03
 9.99074817e-01 9.99200046e-01 9.96546447e-01 9.97398019e-01
 2.77950987e-02 1.33104116e-01 9.93903935e-01 9.99669075e-01
 9.90536630e-01 6.80914964e-05 9.95290995e-01 9.86256182e-01
 1.95719730e-02 9.99779522e-01 9.99912262e-01 9.99963284e-01
 8.61855805e-01 9.99260843e-01 4.68365997e-02 1.48005128e-01
 9.99839187e-01 9.99722302e-01 2.37675197e-03 9.97395635e-01
 9.99995589e-01 5.10934830e-01 9.96361315e-01 7.25242635e-03
 3.20815034e-02 9.35454547e-01 9.98636305e-01 7.75018299e-04
 9.97561030e-04 4.04045507e-02 9.94965851e-01 9.97641325e-01
 9.92202044e-01 1.81661628e-04 3.51927569e-03 9.99024630e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 04:41:23, Dev, Step : 4382, Loss : 0.43163, Acc : 0.825, Auc : 0.908, Sensitive_Loss : 0.11989, Sensitive_Acc : 16.836, Sensitive_Auc : 0.995, Mean auc: 0.908, Run Time : 152.94 sec
INFO:root:2024-04-28 04:41:35, Train, Epoch : 8, Step : 4390, Loss : 0.22017, Acc : 0.709, Sensitive_Loss : 0.08707, Sensitive_Acc : 12.900, Run Time : 11.36 sec
INFO:root:2024-04-28 04:41:46, Train, Epoch : 8, Step : 4400, Loss : 0.23372, Acc : 0.922, Sensitive_Loss : 0.08121, Sensitive_Acc : 17.500, Run Time : 10.73 sec
INFO:root:2024-04-28 04:44:21, Dev, Step : 4400, Loss : 0.44363, Acc : 0.814, Auc : 0.906, Sensitive_Loss : 0.13070, Sensitive_Acc : 16.807, Sensitive_Auc : 0.994, Mean auc: 0.906, Run Time : 155.41 sec
INFO:root:2024-04-28 04:44:30, Train, Epoch : 8, Step : 4410, Loss : 0.31999, Acc : 0.856, Sensitive_Loss : 0.07989, Sensitive_Acc : 15.900, Run Time : 163.88 sec
INFO:root:2024-04-28 04:44:42, Train, Epoch : 8, Step : 4420, Loss : 0.27679, Acc : 0.900, Sensitive_Loss : 0.06999, Sensitive_Acc : 16.500, Run Time : 11.66 sec
INFO:root:2024-04-28 04:44:53, Train, Epoch : 8, Step : 4430, Loss : 0.26599, Acc : 0.891, Sensitive_Loss : 0.07737, Sensitive_Acc : 16.100, Run Time : 11.11 sec
INFO:root:2024-04-28 04:45:04, Train, Epoch : 8, Step : 4440, Loss : 0.23660, Acc : 0.912, Sensitive_Loss : 0.07010, Sensitive_Acc : 15.100, Run Time : 11.42 sec
INFO:root:2024-04-28 04:45:15, Train, Epoch : 8, Step : 4450, Loss : 0.26770, Acc : 0.900, Sensitive_Loss : 0.10501, Sensitive_Acc : 16.100, Run Time : 10.89 sec
INFO:root:2024-04-28 04:45:27, Train, Epoch : 8, Step : 4460, Loss : 0.30789, Acc : 0.863, Sensitive_Loss : 0.09090, Sensitive_Acc : 18.500, Run Time : 12.07 sec
INFO:root:2024-04-28 04:45:39, Train, Epoch : 8, Step : 4470, Loss : 0.22422, Acc : 0.887, Sensitive_Loss : 0.06859, Sensitive_Acc : 17.000, Run Time : 12.00 sec
INFO:root:2024-04-28 04:45:51, Train, Epoch : 8, Step : 4480, Loss : 0.24361, Acc : 0.887, Sensitive_Loss : 0.05201, Sensitive_Acc : 16.200, Run Time : 11.73 sec
INFO:root:2024-04-28 04:46:02, Train, Epoch : 8, Step : 4490, Loss : 0.25748, Acc : 0.887, Sensitive_Loss : 0.09848, Sensitive_Acc : 15.100, Run Time : 10.79 sec
INFO:root:2024-04-28 04:46:13, Train, Epoch : 8, Step : 4500, Loss : 0.22529, Acc : 0.919, Sensitive_Loss : 0.09835, Sensitive_Acc : 17.900, Run Time : 11.33 sec
INFO:root:2024-04-28 04:48:47, Dev, Step : 4500, Loss : 0.42723, Acc : 0.823, Auc : 0.908, Sensitive_Loss : 0.12479, Sensitive_Acc : 16.850, Sensitive_Auc : 0.995, Mean auc: 0.908, Run Time : 154.31 sec
INFO:root:2024-04-28 04:48:56, Train, Epoch : 8, Step : 4510, Loss : 0.23667, Acc : 0.900, Sensitive_Loss : 0.10719, Sensitive_Acc : 17.100, Run Time : 162.70 sec
INFO:root:2024-04-28 04:49:07, Train, Epoch : 8, Step : 4520, Loss : 0.20446, Acc : 0.938, Sensitive_Loss : 0.07233, Sensitive_Acc : 17.200, Run Time : 11.38 sec
INFO:root:2024-04-28 04:49:18, Train, Epoch : 8, Step : 4530, Loss : 0.34140, Acc : 0.869, Sensitive_Loss : 0.08321, Sensitive_Acc : 15.600, Run Time : 11.34 sec
INFO:root:2024-04-28 04:49:31, Train, Epoch : 8, Step : 4540, Loss : 0.32883, Acc : 0.875, Sensitive_Loss : 0.05721, Sensitive_Acc : 16.200, Run Time : 12.26 sec
INFO:root:2024-04-28 04:49:42, Train, Epoch : 8, Step : 4550, Loss : 0.33285, Acc : 0.884, Sensitive_Loss : 0.09250, Sensitive_Acc : 16.000, Run Time : 11.70 sec
INFO:root:2024-04-28 04:49:54, Train, Epoch : 8, Step : 4560, Loss : 0.29986, Acc : 0.853, Sensitive_Loss : 0.08878, Sensitive_Acc : 16.100, Run Time : 11.16 sec
INFO:root:2024-04-28 04:50:05, Train, Epoch : 8, Step : 4570, Loss : 0.27695, Acc : 0.887, Sensitive_Loss : 0.07726, Sensitive_Acc : 16.000, Run Time : 11.57 sec
INFO:root:2024-04-28 04:50:18, Train, Epoch : 8, Step : 4580, Loss : 0.22977, Acc : 0.909, Sensitive_Loss : 0.08216, Sensitive_Acc : 16.100, Run Time : 12.63 sec
INFO:root:2024-04-28 04:50:29, Train, Epoch : 8, Step : 4590, Loss : 0.24895, Acc : 0.884, Sensitive_Loss : 0.10048, Sensitive_Acc : 15.900, Run Time : 11.06 sec
INFO:root:2024-04-28 04:50:40, Train, Epoch : 8, Step : 4600, Loss : 0.22053, Acc : 0.909, Sensitive_Loss : 0.06992, Sensitive_Acc : 16.400, Run Time : 10.74 sec
INFO:root:2024-04-28 04:53:15, Dev, Step : 4600, Loss : 0.47270, Acc : 0.806, Auc : 0.905, Sensitive_Loss : 0.14601, Sensitive_Acc : 16.750, Sensitive_Auc : 0.994, Mean auc: 0.905, Run Time : 155.19 sec
INFO:root:2024-04-28 04:53:23, Train, Epoch : 8, Step : 4610, Loss : 0.29437, Acc : 0.881, Sensitive_Loss : 0.10032, Sensitive_Acc : 17.100, Run Time : 163.97 sec
INFO:root:2024-04-28 04:53:35, Train, Epoch : 8, Step : 4620, Loss : 0.23070, Acc : 0.891, Sensitive_Loss : 0.09313, Sensitive_Acc : 16.100, Run Time : 11.22 sec
INFO:root:2024-04-28 04:53:46, Train, Epoch : 8, Step : 4630, Loss : 0.23519, Acc : 0.909, Sensitive_Loss : 0.07368, Sensitive_Acc : 15.700, Run Time : 11.58 sec
INFO:root:2024-04-28 04:53:58, Train, Epoch : 8, Step : 4640, Loss : 0.28761, Acc : 0.869, Sensitive_Loss : 0.08534, Sensitive_Acc : 16.900, Run Time : 11.98 sec
INFO:root:2024-04-28 04:54:09, Train, Epoch : 8, Step : 4650, Loss : 0.18807, Acc : 0.919, Sensitive_Loss : 0.04925, Sensitive_Acc : 15.800, Run Time : 10.91 sec
INFO:root:2024-04-28 04:54:21, Train, Epoch : 8, Step : 4660, Loss : 0.32056, Acc : 0.859, Sensitive_Loss : 0.07906, Sensitive_Acc : 15.600, Run Time : 11.73 sec
INFO:root:2024-04-28 04:54:32, Train, Epoch : 8, Step : 4670, Loss : 0.29687, Acc : 0.884, Sensitive_Loss : 0.06060, Sensitive_Acc : 16.300, Run Time : 11.29 sec
INFO:root:2024-04-28 04:54:43, Train, Epoch : 8, Step : 4680, Loss : 0.19385, Acc : 0.903, Sensitive_Loss : 0.05020, Sensitive_Acc : 16.600, Run Time : 10.92 sec
INFO:root:2024-04-28 04:54:55, Train, Epoch : 8, Step : 4690, Loss : 0.27526, Acc : 0.866, Sensitive_Loss : 0.15277, Sensitive_Acc : 16.000, Run Time : 12.24 sec
INFO:root:2024-04-28 04:55:07, Train, Epoch : 8, Step : 4700, Loss : 0.27889, Acc : 0.887, Sensitive_Loss : 0.05104, Sensitive_Acc : 17.100, Run Time : 11.96 sec
INFO:root:2024-04-28 04:57:42, Dev, Step : 4700, Loss : 0.44954, Acc : 0.822, Auc : 0.908, Sensitive_Loss : 0.11528, Sensitive_Acc : 16.893, Sensitive_Auc : 0.996, Mean auc: 0.908, Run Time : 154.22 sec
INFO:root:2024-04-28 04:57:50, Train, Epoch : 8, Step : 4710, Loss : 0.24224, Acc : 0.894, Sensitive_Loss : 0.08289, Sensitive_Acc : 16.300, Run Time : 162.70 sec
INFO:root:2024-04-28 04:58:01, Train, Epoch : 8, Step : 4720, Loss : 0.28707, Acc : 0.884, Sensitive_Loss : 0.11545, Sensitive_Acc : 17.500, Run Time : 11.10 sec
INFO:root:2024-04-28 04:58:13, Train, Epoch : 8, Step : 4730, Loss : 0.25426, Acc : 0.872, Sensitive_Loss : 0.07628, Sensitive_Acc : 17.400, Run Time : 11.66 sec
INFO:root:2024-04-28 04:58:24, Train, Epoch : 8, Step : 4740, Loss : 0.22399, Acc : 0.912, Sensitive_Loss : 0.06792, Sensitive_Acc : 16.400, Run Time : 10.98 sec
INFO:root:2024-04-28 04:58:36, Train, Epoch : 8, Step : 4750, Loss : 0.27961, Acc : 0.891, Sensitive_Loss : 0.07430, Sensitive_Acc : 16.200, Run Time : 11.88 sec
INFO:root:2024-04-28 04:58:48, Train, Epoch : 8, Step : 4760, Loss : 0.24552, Acc : 0.897, Sensitive_Loss : 0.09681, Sensitive_Acc : 16.900, Run Time : 12.18 sec
INFO:root:2024-04-28 04:58:59, Train, Epoch : 8, Step : 4770, Loss : 0.24885, Acc : 0.887, Sensitive_Loss : 0.07991, Sensitive_Acc : 15.500, Run Time : 10.69 sec
INFO:root:2024-04-28 04:59:11, Train, Epoch : 8, Step : 4780, Loss : 0.25850, Acc : 0.884, Sensitive_Loss : 0.07082, Sensitive_Acc : 15.600, Run Time : 12.20 sec
INFO:root:2024-04-28 04:59:21, Train, Epoch : 8, Step : 4790, Loss : 0.29564, Acc : 0.863, Sensitive_Loss : 0.08865, Sensitive_Acc : 14.900, Run Time : 10.70 sec
INFO:root:2024-04-28 04:59:33, Train, Epoch : 8, Step : 4800, Loss : 0.26766, Acc : 0.891, Sensitive_Loss : 0.09323, Sensitive_Acc : 15.500, Run Time : 11.14 sec
INFO:root:2024-04-28 05:02:08, Dev, Step : 4800, Loss : 0.45320, Acc : 0.816, Auc : 0.904, Sensitive_Loss : 0.11729, Sensitive_Acc : 16.793, Sensitive_Auc : 0.996, Mean auc: 0.904, Run Time : 155.48 sec
INFO:root:2024-04-28 05:02:17, Train, Epoch : 8, Step : 4810, Loss : 0.29130, Acc : 0.887, Sensitive_Loss : 0.09397, Sensitive_Acc : 17.300, Run Time : 164.41 sec
INFO:root:2024-04-28 05:02:28, Train, Epoch : 8, Step : 4820, Loss : 0.20199, Acc : 0.919, Sensitive_Loss : 0.05045, Sensitive_Acc : 17.400, Run Time : 11.18 sec
INFO:root:2024-04-28 05:02:39, Train, Epoch : 8, Step : 4830, Loss : 0.26032, Acc : 0.894, Sensitive_Loss : 0.11177, Sensitive_Acc : 16.400, Run Time : 11.06 sec
INFO:root:2024-04-28 05:02:51, Train, Epoch : 8, Step : 4840, Loss : 0.21252, Acc : 0.903, Sensitive_Loss : 0.11967, Sensitive_Acc : 15.500, Run Time : 11.30 sec
INFO:root:2024-04-28 05:03:03, Train, Epoch : 8, Step : 4850, Loss : 0.32009, Acc : 0.856, Sensitive_Loss : 0.07360, Sensitive_Acc : 14.900, Run Time : 12.02 sec
INFO:root:2024-04-28 05:03:14, Train, Epoch : 8, Step : 4860, Loss : 0.22071, Acc : 0.906, Sensitive_Loss : 0.08151, Sensitive_Acc : 16.100, Run Time : 11.26 sec
INFO:root:2024-04-28 05:03:26, Train, Epoch : 8, Step : 4870, Loss : 0.22893, Acc : 0.903, Sensitive_Loss : 0.12135, Sensitive_Acc : 16.100, Run Time : 12.01 sec
INFO:root:2024-04-28 05:03:37, Train, Epoch : 8, Step : 4880, Loss : 0.17411, Acc : 0.919, Sensitive_Loss : 0.11061, Sensitive_Acc : 16.800, Run Time : 10.92 sec
INFO:root:2024-04-28 05:03:49, Train, Epoch : 8, Step : 4890, Loss : 0.28777, Acc : 0.878, Sensitive_Loss : 0.06240, Sensitive_Acc : 14.800, Run Time : 12.14 sec
INFO:root:2024-04-28 05:04:01, Train, Epoch : 8, Step : 4900, Loss : 0.26117, Acc : 0.881, Sensitive_Loss : 0.11146, Sensitive_Acc : 15.800, Run Time : 11.66 sec
INFO:root:2024-04-28 05:06:35, Dev, Step : 4900, Loss : 0.45071, Acc : 0.820, Auc : 0.905, Sensitive_Loss : 0.12297, Sensitive_Acc : 16.836, Sensitive_Auc : 0.995, Mean auc: 0.905, Run Time : 154.50 sec
INFO:root:2024-04-28 05:06:43, Train, Epoch : 8, Step : 4910, Loss : 0.22377, Acc : 0.897, Sensitive_Loss : 0.07988, Sensitive_Acc : 16.200, Run Time : 162.88 sec
INFO:root:2024-04-28 05:06:56, Train, Epoch : 8, Step : 4920, Loss : 0.26090, Acc : 0.900, Sensitive_Loss : 0.06829, Sensitive_Acc : 17.500, Run Time : 12.13 sec
INFO:root:2024-04-28 05:07:07, Train, Epoch : 8, Step : 4930, Loss : 0.27467, Acc : 0.891, Sensitive_Loss : 0.08038, Sensitive_Acc : 17.200, Run Time : 11.47 sec
INFO:root:2024-04-28 05:07:18, Train, Epoch : 8, Step : 4940, Loss : 0.22907, Acc : 0.887, Sensitive_Loss : 0.05827, Sensitive_Acc : 17.100, Run Time : 10.78 sec
INFO:root:2024-04-28 05:07:29, Train, Epoch : 8, Step : 4950, Loss : 0.28069, Acc : 0.881, Sensitive_Loss : 0.08735, Sensitive_Acc : 16.100, Run Time : 11.26 sec
INFO:root:2024-04-28 05:07:40, Train, Epoch : 8, Step : 4960, Loss : 0.20571, Acc : 0.909, Sensitive_Loss : 0.07160, Sensitive_Acc : 16.100, Run Time : 11.39 sec
INFO:root:2024-04-28 05:07:52, Train, Epoch : 8, Step : 4970, Loss : 0.22605, Acc : 0.903, Sensitive_Loss : 0.10713, Sensitive_Acc : 17.400, Run Time : 11.89 sec
INFO:root:2024-04-28 05:08:04, Train, Epoch : 8, Step : 4980, Loss : 0.25410, Acc : 0.875, Sensitive_Loss : 0.11060, Sensitive_Acc : 16.200, Run Time : 12.14 sec
INFO:root:2024-04-28 05:08:16, Train, Epoch : 8, Step : 4990, Loss : 0.33241, Acc : 0.866, Sensitive_Loss : 0.10467, Sensitive_Acc : 15.300, Run Time : 11.53 sec
INFO:root:2024-04-28 05:08:27, Train, Epoch : 8, Step : 5000, Loss : 0.19261, Acc : 0.906, Sensitive_Loss : 0.09875, Sensitive_Acc : 17.400, Run Time : 10.70 sec
INFO:root:2024-04-28 05:11:01, Dev, Step : 5000, Loss : 0.42948, Acc : 0.820, Auc : 0.905, Sensitive_Loss : 0.12500, Sensitive_Acc : 16.836, Sensitive_Auc : 0.996, Mean auc: 0.905, Run Time : 154.17 sec
INFO:root:2024-04-28 05:13:39
INFO:root:y_pred: [0.04703526 0.9819165  0.00313414 ... 0.79655105 0.00314066 0.8782762 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.9688333e-01 1.5006656e-03 2.5637731e-01 1.9390480e-05 9.9989903e-01
 3.1746062e-04 9.9996305e-01 9.9988663e-01 8.6840897e-05 8.0562413e-01
 9.9793434e-01 9.9998939e-01 9.8909456e-01 9.7801125e-01 6.4533222e-03
 9.9020165e-01 9.9997175e-01 3.6427598e-03 5.5790085e-01 9.6344626e-01
 9.9910212e-01 1.2463394e-01 9.9978083e-01 9.9671614e-01 9.9969745e-01
 9.9978656e-01 7.5731099e-05 9.9852020e-01 9.9732322e-01 3.7884039e-01
 1.5018232e-02 7.4258828e-01 1.2290340e-02 3.8387373e-02 2.4296179e-02
 1.6734527e-03 8.4933983e-03 2.2618468e-03 9.9988413e-01 9.9951005e-01
 1.3301670e-05 9.8366989e-03 9.8163283e-01 3.8603140e-04 9.9980062e-01
 9.9957269e-01 9.9949229e-01 9.6751952e-01 5.7607640e-02 9.9842656e-01
 9.9888271e-01 3.1879840e-03 6.6368824e-01 8.9528295e-04 1.4562313e-04
 8.4848301e-03 5.0442438e-03 1.1898586e-02 1.0875078e-02 2.7131215e-01
 6.7379855e-02 1.4928985e-01 6.9378786e-02 9.9358422e-01 1.8146722e-01
 9.9992561e-01 2.7102176e-03 9.9985254e-01 9.9760145e-01 5.5329072e-01
 8.7224454e-01 6.2730277e-01 2.0276795e-03 7.6508224e-02 3.8532930e-04
 7.3778322e-03 1.2960244e-02 1.6691281e-01 2.2177007e-04 9.9975687e-01
 9.9937266e-01 1.9737941e-03 5.1529866e-01 3.1120956e-03 9.5590037e-01
 9.7336495e-01 1.4278861e-02 1.1006897e-01 9.8680693e-01 9.9994242e-01
 9.9997818e-01 5.3183818e-03 2.5600535e-03 9.9978727e-01 3.2216305e-01
 5.1777773e-03 9.9932575e-01 9.9685127e-01 1.9657210e-04 9.8478757e-03
 9.9886215e-01 9.9807596e-01 9.9588060e-01 9.9443215e-01 3.8408000e-02
 5.9190098e-02 9.9212599e-01 9.9951625e-01 9.7643822e-01 4.9483289e-05
 9.9693632e-01 9.8743373e-01 8.3955079e-03 9.9985611e-01 9.9970418e-01
 9.9995577e-01 9.4822866e-01 9.9961841e-01 2.4077021e-02 1.6866240e-01
 9.9983990e-01 9.9941862e-01 9.8778529e-04 9.9886024e-01 9.9998951e-01
 5.4476660e-01 9.9411857e-01 5.2762083e-03 2.7996754e-02 9.4555902e-01
 9.9865216e-01 8.8850141e-04 7.3662121e-04 3.0687977e-02 9.8591655e-01
 9.9668473e-01 9.8286003e-01 1.5363196e-04 1.7506233e-03 9.9792361e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 05:13:39, Dev, Step : 5008, Loss : 0.43203, Acc : 0.825, Auc : 0.907, Sensitive_Loss : 0.11521, Sensitive_Acc : 16.836, Sensitive_Auc : 0.995, Mean auc: 0.907, Run Time : 152.79 sec
INFO:root:2024-04-28 05:13:45, Train, Epoch : 9, Step : 5010, Loss : 0.03109, Acc : 0.188, Sensitive_Loss : 0.01527, Sensitive_Acc : 2.900, Run Time : 4.27 sec
INFO:root:2024-04-28 05:13:56, Train, Epoch : 9, Step : 5020, Loss : 0.23329, Acc : 0.922, Sensitive_Loss : 0.08831, Sensitive_Acc : 16.400, Run Time : 10.96 sec
INFO:root:2024-04-28 05:14:07, Train, Epoch : 9, Step : 5030, Loss : 0.19809, Acc : 0.906, Sensitive_Loss : 0.08289, Sensitive_Acc : 16.100, Run Time : 11.35 sec
INFO:root:2024-04-28 05:14:18, Train, Epoch : 9, Step : 5040, Loss : 0.21113, Acc : 0.912, Sensitive_Loss : 0.07119, Sensitive_Acc : 15.700, Run Time : 11.38 sec
INFO:root:2024-04-28 05:14:30, Train, Epoch : 9, Step : 5050, Loss : 0.24007, Acc : 0.894, Sensitive_Loss : 0.07318, Sensitive_Acc : 17.300, Run Time : 11.95 sec
INFO:root:2024-04-28 05:14:42, Train, Epoch : 9, Step : 5060, Loss : 0.22738, Acc : 0.909, Sensitive_Loss : 0.07195, Sensitive_Acc : 16.500, Run Time : 12.11 sec
INFO:root:2024-04-28 05:14:53, Train, Epoch : 9, Step : 5070, Loss : 0.22449, Acc : 0.884, Sensitive_Loss : 0.10287, Sensitive_Acc : 17.100, Run Time : 10.89 sec
INFO:root:2024-04-28 05:15:04, Train, Epoch : 9, Step : 5080, Loss : 0.22710, Acc : 0.894, Sensitive_Loss : 0.06700, Sensitive_Acc : 14.900, Run Time : 11.08 sec
INFO:root:2024-04-28 05:15:16, Train, Epoch : 9, Step : 5090, Loss : 0.24151, Acc : 0.884, Sensitive_Loss : 0.10973, Sensitive_Acc : 15.100, Run Time : 12.04 sec
INFO:root:2024-04-28 05:15:27, Train, Epoch : 9, Step : 5100, Loss : 0.19711, Acc : 0.919, Sensitive_Loss : 0.06203, Sensitive_Acc : 15.800, Run Time : 10.85 sec
INFO:root:2024-04-28 05:18:02, Dev, Step : 5100, Loss : 0.45468, Acc : 0.820, Auc : 0.905, Sensitive_Loss : 0.13245, Sensitive_Acc : 16.793, Sensitive_Auc : 0.994, Mean auc: 0.905, Run Time : 154.90 sec
INFO:root:2024-04-28 05:18:10, Train, Epoch : 9, Step : 5110, Loss : 0.27141, Acc : 0.881, Sensitive_Loss : 0.08079, Sensitive_Acc : 15.600, Run Time : 163.19 sec
INFO:root:2024-04-28 05:18:22, Train, Epoch : 9, Step : 5120, Loss : 0.21113, Acc : 0.900, Sensitive_Loss : 0.07914, Sensitive_Acc : 15.300, Run Time : 11.51 sec
INFO:root:2024-04-28 05:18:34, Train, Epoch : 9, Step : 5130, Loss : 0.22598, Acc : 0.909, Sensitive_Loss : 0.08694, Sensitive_Acc : 15.000, Run Time : 11.59 sec
INFO:root:2024-04-28 05:18:45, Train, Epoch : 9, Step : 5140, Loss : 0.22253, Acc : 0.897, Sensitive_Loss : 0.06835, Sensitive_Acc : 15.800, Run Time : 11.75 sec
INFO:root:2024-04-28 05:18:57, Train, Epoch : 9, Step : 5150, Loss : 0.28705, Acc : 0.897, Sensitive_Loss : 0.07200, Sensitive_Acc : 18.000, Run Time : 11.42 sec
INFO:root:2024-04-28 05:19:08, Train, Epoch : 9, Step : 5160, Loss : 0.20923, Acc : 0.912, Sensitive_Loss : 0.06664, Sensitive_Acc : 16.800, Run Time : 11.48 sec
INFO:root:2024-04-28 05:19:19, Train, Epoch : 9, Step : 5170, Loss : 0.25913, Acc : 0.881, Sensitive_Loss : 0.06292, Sensitive_Acc : 16.400, Run Time : 11.25 sec
INFO:root:2024-04-28 05:19:31, Train, Epoch : 9, Step : 5180, Loss : 0.24716, Acc : 0.875, Sensitive_Loss : 0.09217, Sensitive_Acc : 16.500, Run Time : 11.38 sec
INFO:root:2024-04-28 05:19:42, Train, Epoch : 9, Step : 5190, Loss : 0.26632, Acc : 0.894, Sensitive_Loss : 0.09825, Sensitive_Acc : 16.700, Run Time : 11.09 sec
INFO:root:2024-04-28 05:19:54, Train, Epoch : 9, Step : 5200, Loss : 0.23864, Acc : 0.903, Sensitive_Loss : 0.13460, Sensitive_Acc : 18.000, Run Time : 11.62 sec
INFO:root:2024-04-28 05:22:28, Dev, Step : 5200, Loss : 0.46942, Acc : 0.814, Auc : 0.901, Sensitive_Loss : 0.12696, Sensitive_Acc : 16.793, Sensitive_Auc : 0.994, Mean auc: 0.901, Run Time : 154.31 sec
INFO:root:2024-04-28 05:22:37, Train, Epoch : 9, Step : 5210, Loss : 0.18458, Acc : 0.941, Sensitive_Loss : 0.07036, Sensitive_Acc : 16.100, Run Time : 163.36 sec
INFO:root:2024-04-28 05:22:49, Train, Epoch : 9, Step : 5220, Loss : 0.21217, Acc : 0.931, Sensitive_Loss : 0.06739, Sensitive_Acc : 16.000, Run Time : 11.93 sec
INFO:root:2024-04-28 05:23:00, Train, Epoch : 9, Step : 5230, Loss : 0.20345, Acc : 0.916, Sensitive_Loss : 0.09721, Sensitive_Acc : 15.800, Run Time : 10.96 sec
INFO:root:2024-04-28 05:23:12, Train, Epoch : 9, Step : 5240, Loss : 0.21390, Acc : 0.912, Sensitive_Loss : 0.08577, Sensitive_Acc : 17.000, Run Time : 11.69 sec
INFO:root:2024-04-28 05:23:22, Train, Epoch : 9, Step : 5250, Loss : 0.25047, Acc : 0.894, Sensitive_Loss : 0.07794, Sensitive_Acc : 16.100, Run Time : 10.50 sec
INFO:root:2024-04-28 05:23:34, Train, Epoch : 9, Step : 5260, Loss : 0.24850, Acc : 0.884, Sensitive_Loss : 0.10618, Sensitive_Acc : 16.100, Run Time : 11.60 sec
INFO:root:2024-04-28 05:23:46, Train, Epoch : 9, Step : 5270, Loss : 0.25645, Acc : 0.887, Sensitive_Loss : 0.08446, Sensitive_Acc : 15.400, Run Time : 12.16 sec
INFO:root:2024-04-28 05:23:57, Train, Epoch : 9, Step : 5280, Loss : 0.18117, Acc : 0.931, Sensitive_Loss : 0.06155, Sensitive_Acc : 17.100, Run Time : 11.14 sec
INFO:root:2024-04-28 05:24:08, Train, Epoch : 9, Step : 5290, Loss : 0.20095, Acc : 0.903, Sensitive_Loss : 0.08483, Sensitive_Acc : 16.000, Run Time : 10.76 sec
INFO:root:2024-04-28 05:24:20, Train, Epoch : 9, Step : 5300, Loss : 0.21236, Acc : 0.887, Sensitive_Loss : 0.06242, Sensitive_Acc : 16.200, Run Time : 12.27 sec
INFO:root:2024-04-28 05:26:54, Dev, Step : 5300, Loss : 0.47748, Acc : 0.813, Auc : 0.904, Sensitive_Loss : 0.12191, Sensitive_Acc : 16.907, Sensitive_Auc : 0.994, Mean auc: 0.904, Run Time : 153.99 sec
INFO:root:2024-04-28 05:27:03, Train, Epoch : 9, Step : 5310, Loss : 0.23917, Acc : 0.900, Sensitive_Loss : 0.08946, Sensitive_Acc : 18.200, Run Time : 163.02 sec
INFO:root:2024-04-28 05:27:14, Train, Epoch : 9, Step : 5320, Loss : 0.21609, Acc : 0.919, Sensitive_Loss : 0.06048, Sensitive_Acc : 16.200, Run Time : 11.25 sec
INFO:root:2024-04-28 05:27:26, Train, Epoch : 9, Step : 5330, Loss : 0.21974, Acc : 0.928, Sensitive_Loss : 0.06652, Sensitive_Acc : 16.400, Run Time : 11.63 sec
INFO:root:2024-04-28 05:27:37, Train, Epoch : 9, Step : 5340, Loss : 0.27493, Acc : 0.878, Sensitive_Loss : 0.07886, Sensitive_Acc : 17.300, Run Time : 10.90 sec
INFO:root:2024-04-28 05:27:48, Train, Epoch : 9, Step : 5350, Loss : 0.27797, Acc : 0.884, Sensitive_Loss : 0.05768, Sensitive_Acc : 16.500, Run Time : 11.07 sec
INFO:root:2024-04-28 05:28:00, Train, Epoch : 9, Step : 5360, Loss : 0.20235, Acc : 0.919, Sensitive_Loss : 0.06882, Sensitive_Acc : 16.500, Run Time : 12.58 sec
INFO:root:2024-04-28 05:28:12, Train, Epoch : 9, Step : 5370, Loss : 0.22685, Acc : 0.922, Sensitive_Loss : 0.09320, Sensitive_Acc : 16.400, Run Time : 11.26 sec
INFO:root:2024-04-28 05:28:22, Train, Epoch : 9, Step : 5380, Loss : 0.31606, Acc : 0.844, Sensitive_Loss : 0.07854, Sensitive_Acc : 14.400, Run Time : 10.77 sec
INFO:root:2024-04-28 05:28:34, Train, Epoch : 9, Step : 5390, Loss : 0.24248, Acc : 0.891, Sensitive_Loss : 0.06654, Sensitive_Acc : 16.100, Run Time : 11.85 sec
INFO:root:2024-04-28 05:28:45, Train, Epoch : 9, Step : 5400, Loss : 0.27329, Acc : 0.894, Sensitive_Loss : 0.08624, Sensitive_Acc : 15.400, Run Time : 10.76 sec
INFO:root:2024-04-28 05:31:21, Dev, Step : 5400, Loss : 0.45329, Acc : 0.818, Auc : 0.902, Sensitive_Loss : 0.12623, Sensitive_Acc : 16.907, Sensitive_Auc : 0.994, Mean auc: 0.902, Run Time : 155.77 sec
INFO:root:2024-04-28 05:31:29, Train, Epoch : 9, Step : 5410, Loss : 0.26620, Acc : 0.894, Sensitive_Loss : 0.09774, Sensitive_Acc : 16.200, Run Time : 164.17 sec
INFO:root:2024-04-28 05:31:42, Train, Epoch : 9, Step : 5420, Loss : 0.25264, Acc : 0.897, Sensitive_Loss : 0.09854, Sensitive_Acc : 15.400, Run Time : 12.88 sec
INFO:root:2024-04-28 05:31:53, Train, Epoch : 9, Step : 5430, Loss : 0.31709, Acc : 0.856, Sensitive_Loss : 0.08602, Sensitive_Acc : 16.700, Run Time : 11.26 sec
INFO:root:2024-04-28 05:32:03, Train, Epoch : 9, Step : 5440, Loss : 0.20034, Acc : 0.912, Sensitive_Loss : 0.05774, Sensitive_Acc : 16.300, Run Time : 9.97 sec
INFO:root:2024-04-28 05:32:16, Train, Epoch : 9, Step : 5450, Loss : 0.23014, Acc : 0.900, Sensitive_Loss : 0.07651, Sensitive_Acc : 16.800, Run Time : 12.68 sec
INFO:root:2024-04-28 05:32:26, Train, Epoch : 9, Step : 5460, Loss : 0.21601, Acc : 0.906, Sensitive_Loss : 0.09134, Sensitive_Acc : 17.600, Run Time : 10.28 sec
INFO:root:2024-04-28 05:32:38, Train, Epoch : 9, Step : 5470, Loss : 0.22107, Acc : 0.916, Sensitive_Loss : 0.11574, Sensitive_Acc : 16.500, Run Time : 11.60 sec
INFO:root:2024-04-28 05:32:50, Train, Epoch : 9, Step : 5480, Loss : 0.25087, Acc : 0.894, Sensitive_Loss : 0.07741, Sensitive_Acc : 15.600, Run Time : 12.04 sec
INFO:root:2024-04-28 05:33:01, Train, Epoch : 9, Step : 5490, Loss : 0.21934, Acc : 0.900, Sensitive_Loss : 0.08510, Sensitive_Acc : 16.100, Run Time : 11.18 sec
INFO:root:2024-04-28 05:33:13, Train, Epoch : 9, Step : 5500, Loss : 0.24465, Acc : 0.891, Sensitive_Loss : 0.08188, Sensitive_Acc : 16.600, Run Time : 11.59 sec
INFO:root:2024-04-28 05:35:47, Dev, Step : 5500, Loss : 0.47542, Acc : 0.812, Auc : 0.902, Sensitive_Loss : 0.14621, Sensitive_Acc : 16.821, Sensitive_Auc : 0.995, Mean auc: 0.902, Run Time : 154.10 sec
INFO:root:2024-04-28 05:35:56, Train, Epoch : 9, Step : 5510, Loss : 0.23905, Acc : 0.900, Sensitive_Loss : 0.09539, Sensitive_Acc : 15.500, Run Time : 163.61 sec
INFO:root:2024-04-28 05:36:08, Train, Epoch : 9, Step : 5520, Loss : 0.26236, Acc : 0.894, Sensitive_Loss : 0.05295, Sensitive_Acc : 16.200, Run Time : 11.22 sec
INFO:root:2024-04-28 05:36:18, Train, Epoch : 9, Step : 5530, Loss : 0.21470, Acc : 0.934, Sensitive_Loss : 0.06670, Sensitive_Acc : 16.900, Run Time : 10.20 sec
INFO:root:2024-04-28 05:36:30, Train, Epoch : 9, Step : 5540, Loss : 0.30505, Acc : 0.884, Sensitive_Loss : 0.07886, Sensitive_Acc : 15.000, Run Time : 12.09 sec
INFO:root:2024-04-28 05:36:41, Train, Epoch : 9, Step : 5550, Loss : 0.20469, Acc : 0.919, Sensitive_Loss : 0.06325, Sensitive_Acc : 15.700, Run Time : 11.38 sec
INFO:root:2024-04-28 05:36:53, Train, Epoch : 9, Step : 5560, Loss : 0.23294, Acc : 0.906, Sensitive_Loss : 0.05484, Sensitive_Acc : 18.400, Run Time : 11.59 sec
INFO:root:2024-04-28 05:37:04, Train, Epoch : 9, Step : 5570, Loss : 0.21868, Acc : 0.912, Sensitive_Loss : 0.11557, Sensitive_Acc : 15.800, Run Time : 10.83 sec
INFO:root:2024-04-28 05:37:16, Train, Epoch : 9, Step : 5580, Loss : 0.24087, Acc : 0.872, Sensitive_Loss : 0.07434, Sensitive_Acc : 15.400, Run Time : 12.26 sec
INFO:root:2024-04-28 05:37:28, Train, Epoch : 9, Step : 5590, Loss : 0.25085, Acc : 0.903, Sensitive_Loss : 0.13672, Sensitive_Acc : 16.100, Run Time : 11.63 sec
INFO:root:2024-04-28 05:37:39, Train, Epoch : 9, Step : 5600, Loss : 0.21943, Acc : 0.900, Sensitive_Loss : 0.09248, Sensitive_Acc : 17.100, Run Time : 11.35 sec
INFO:root:2024-04-28 05:40:13, Dev, Step : 5600, Loss : 0.54343, Acc : 0.803, Auc : 0.904, Sensitive_Loss : 0.14881, Sensitive_Acc : 16.821, Sensitive_Auc : 0.995, Mean auc: 0.904, Run Time : 154.42 sec
INFO:root:2024-04-28 05:40:23, Train, Epoch : 9, Step : 5610, Loss : 0.25773, Acc : 0.850, Sensitive_Loss : 0.08052, Sensitive_Acc : 16.300, Run Time : 163.73 sec
INFO:root:2024-04-28 05:40:34, Train, Epoch : 9, Step : 5620, Loss : 0.22701, Acc : 0.900, Sensitive_Loss : 0.05227, Sensitive_Acc : 16.000, Run Time : 11.41 sec
INFO:root:2024-04-28 05:40:46, Train, Epoch : 9, Step : 5630, Loss : 0.22163, Acc : 0.897, Sensitive_Loss : 0.07465, Sensitive_Acc : 15.900, Run Time : 11.79 sec
INFO:root:2024-04-28 05:43:22
INFO:root:y_pred: [6.9251568e-03 9.8363930e-01 2.1887815e-03 ... 6.2120628e-01 4.6664162e-04
 7.4297410e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.98849630e-01 1.61854050e-03 5.01914501e-01 5.19569257e-05
 9.99981761e-01 1.48394145e-03 9.99984860e-01 9.99944925e-01
 1.99989037e-04 9.01253998e-01 9.99246120e-01 9.99989748e-01
 9.96702254e-01 9.82159913e-01 1.31892916e-02 9.94090736e-01
 9.99990582e-01 1.15831420e-02 7.65187204e-01 9.90123808e-01
 9.99751508e-01 3.06562364e-01 9.99900937e-01 9.98302341e-01
 9.99950528e-01 9.99872804e-01 1.12234629e-04 9.99531746e-01
 9.97851849e-01 7.20812976e-01 5.74229099e-02 9.07834351e-01
 1.61736421e-02 4.98961657e-02 7.34014437e-02 3.90108232e-03
 1.44425239e-02 4.37297160e-03 9.99949098e-01 9.99657154e-01
 1.35408827e-05 4.34450544e-02 9.91195917e-01 2.58147251e-03
 9.99886394e-01 9.99664664e-01 9.99799907e-01 9.90106940e-01
 1.93610474e-01 9.99463022e-01 9.99702036e-01 7.05080386e-03
 8.80388856e-01 2.00851378e-03 3.03325418e-04 1.80097651e-02
 9.65473056e-03 1.89692248e-02 3.44187617e-02 6.12830758e-01
 1.59362018e-01 2.59723544e-01 1.28115505e-01 9.98005450e-01
 3.29557687e-01 9.99969363e-01 2.13228189e-03 9.99951720e-01
 9.99089718e-01 7.24849820e-01 9.44261312e-01 7.43081689e-01
 3.11199971e-03 2.82173932e-01 8.84269713e-04 3.58698219e-02
 1.61629319e-02 2.44645089e-01 3.37736070e-04 9.99893188e-01
 9.99804556e-01 6.94011757e-03 7.00717628e-01 1.51327820e-02
 9.85601962e-01 9.83536482e-01 4.50085253e-02 1.92499742e-01
 9.92557526e-01 9.99953985e-01 9.99994755e-01 3.75049678e-03
 6.31534914e-03 9.99932408e-01 6.22630358e-01 3.22914906e-02
 9.99472558e-01 9.97045338e-01 5.21991926e-04 7.46232038e-03
 9.99617219e-01 9.99179065e-01 9.98539448e-01 9.96797979e-01
 4.03124914e-02 1.73019767e-01 9.97145712e-01 9.99810159e-01
 9.93954718e-01 7.83617870e-05 9.98306394e-01 9.96705353e-01
 5.03680483e-02 9.99944806e-01 9.99873042e-01 9.99970913e-01
 9.66835022e-01 9.99792278e-01 4.91726734e-02 1.67684972e-01
 9.99954820e-01 9.99735534e-01 1.94196915e-03 9.99146819e-01
 9.99995589e-01 6.18870080e-01 9.98420835e-01 1.10241715e-02
 3.59930508e-02 9.93385732e-01 9.99426246e-01 4.74789558e-04
 1.06236967e-03 8.17777961e-02 9.90213692e-01 9.98601139e-01
 9.98227775e-01 2.15324384e-04 1.06161283e-02 9.99668241e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 05:43:22, Dev, Step : 5634, Loss : 0.53548, Acc : 0.806, Auc : 0.901, Sensitive_Loss : 0.15368, Sensitive_Acc : 16.836, Sensitive_Auc : 0.995, Mean auc: 0.901, Run Time : 152.63 sec
INFO:root:2024-04-28 05:43:32, Train, Epoch : 10, Step : 5640, Loss : 0.12641, Acc : 0.556, Sensitive_Loss : 0.04680, Sensitive_Acc : 8.800, Run Time : 9.04 sec
INFO:root:2024-04-28 05:43:43, Train, Epoch : 10, Step : 5650, Loss : 0.23934, Acc : 0.894, Sensitive_Loss : 0.08292, Sensitive_Acc : 15.500, Run Time : 11.22 sec
INFO:root:2024-04-28 05:43:55, Train, Epoch : 10, Step : 5660, Loss : 0.28198, Acc : 0.869, Sensitive_Loss : 0.07873, Sensitive_Acc : 17.400, Run Time : 12.07 sec
INFO:root:2024-04-28 05:44:06, Train, Epoch : 10, Step : 5670, Loss : 0.23983, Acc : 0.919, Sensitive_Loss : 0.11104, Sensitive_Acc : 16.200, Run Time : 11.29 sec
INFO:root:2024-04-28 05:44:18, Train, Epoch : 10, Step : 5680, Loss : 0.21393, Acc : 0.887, Sensitive_Loss : 0.09632, Sensitive_Acc : 16.000, Run Time : 11.98 sec
INFO:root:2024-04-28 05:44:30, Train, Epoch : 10, Step : 5690, Loss : 0.19194, Acc : 0.916, Sensitive_Loss : 0.07624, Sensitive_Acc : 15.400, Run Time : 11.33 sec
INFO:root:2024-04-28 05:44:40, Train, Epoch : 10, Step : 5700, Loss : 0.15607, Acc : 0.925, Sensitive_Loss : 0.08812, Sensitive_Acc : 15.600, Run Time : 10.48 sec
INFO:root:2024-04-28 05:47:15, Dev, Step : 5700, Loss : 0.49804, Acc : 0.809, Auc : 0.899, Sensitive_Loss : 0.13927, Sensitive_Acc : 16.793, Sensitive_Auc : 0.996, Mean auc: 0.899, Run Time : 154.57 sec
INFO:root:2024-04-28 05:47:23, Train, Epoch : 10, Step : 5710, Loss : 0.25683, Acc : 0.884, Sensitive_Loss : 0.10212, Sensitive_Acc : 18.000, Run Time : 162.93 sec
INFO:root:2024-04-28 05:47:35, Train, Epoch : 10, Step : 5720, Loss : 0.20413, Acc : 0.900, Sensitive_Loss : 0.09605, Sensitive_Acc : 16.000, Run Time : 11.94 sec
INFO:root:2024-04-28 05:47:46, Train, Epoch : 10, Step : 5730, Loss : 0.21032, Acc : 0.922, Sensitive_Loss : 0.05812, Sensitive_Acc : 16.100, Run Time : 11.20 sec
INFO:root:2024-04-28 05:47:58, Train, Epoch : 10, Step : 5740, Loss : 0.23851, Acc : 0.919, Sensitive_Loss : 0.09832, Sensitive_Acc : 15.500, Run Time : 11.98 sec
INFO:root:2024-04-28 05:48:09, Train, Epoch : 10, Step : 5750, Loss : 0.20670, Acc : 0.897, Sensitive_Loss : 0.08462, Sensitive_Acc : 16.700, Run Time : 10.50 sec
INFO:root:2024-04-28 05:48:20, Train, Epoch : 10, Step : 5760, Loss : 0.20161, Acc : 0.906, Sensitive_Loss : 0.06041, Sensitive_Acc : 16.700, Run Time : 11.02 sec
INFO:root:2024-04-28 05:48:31, Train, Epoch : 10, Step : 5770, Loss : 0.23410, Acc : 0.887, Sensitive_Loss : 0.11973, Sensitive_Acc : 15.000, Run Time : 11.33 sec
INFO:root:2024-04-28 05:48:43, Train, Epoch : 10, Step : 5780, Loss : 0.22755, Acc : 0.916, Sensitive_Loss : 0.08245, Sensitive_Acc : 15.400, Run Time : 12.25 sec
INFO:root:2024-04-28 05:48:54, Train, Epoch : 10, Step : 5790, Loss : 0.22372, Acc : 0.906, Sensitive_Loss : 0.08846, Sensitive_Acc : 16.900, Run Time : 10.77 sec
INFO:root:2024-04-28 05:49:06, Train, Epoch : 10, Step : 5800, Loss : 0.19983, Acc : 0.912, Sensitive_Loss : 0.08586, Sensitive_Acc : 15.800, Run Time : 11.65 sec
INFO:root:2024-04-28 05:51:41, Dev, Step : 5800, Loss : 0.48046, Acc : 0.812, Auc : 0.900, Sensitive_Loss : 0.12531, Sensitive_Acc : 16.836, Sensitive_Auc : 0.996, Mean auc: 0.900, Run Time : 155.09 sec
INFO:root:2024-04-28 05:51:49, Train, Epoch : 10, Step : 5810, Loss : 0.21629, Acc : 0.916, Sensitive_Loss : 0.06431, Sensitive_Acc : 16.900, Run Time : 163.47 sec
INFO:root:2024-04-28 05:52:01, Train, Epoch : 10, Step : 5820, Loss : 0.20232, Acc : 0.909, Sensitive_Loss : 0.04964, Sensitive_Acc : 16.800, Run Time : 11.77 sec
INFO:root:2024-04-28 05:52:12, Train, Epoch : 10, Step : 5830, Loss : 0.22762, Acc : 0.894, Sensitive_Loss : 0.09198, Sensitive_Acc : 16.800, Run Time : 11.43 sec
INFO:root:2024-04-28 05:52:24, Train, Epoch : 10, Step : 5840, Loss : 0.21276, Acc : 0.934, Sensitive_Loss : 0.09411, Sensitive_Acc : 17.500, Run Time : 11.78 sec
INFO:root:2024-04-28 05:52:36, Train, Epoch : 10, Step : 5850, Loss : 0.17740, Acc : 0.931, Sensitive_Loss : 0.08412, Sensitive_Acc : 17.100, Run Time : 11.49 sec
INFO:root:2024-04-28 05:52:47, Train, Epoch : 10, Step : 5860, Loss : 0.18554, Acc : 0.931, Sensitive_Loss : 0.10520, Sensitive_Acc : 16.300, Run Time : 10.98 sec
INFO:root:2024-04-28 05:52:57, Train, Epoch : 10, Step : 5870, Loss : 0.23156, Acc : 0.900, Sensitive_Loss : 0.05114, Sensitive_Acc : 17.200, Run Time : 10.62 sec
INFO:root:2024-04-28 05:53:09, Train, Epoch : 10, Step : 5880, Loss : 0.19540, Acc : 0.919, Sensitive_Loss : 0.06043, Sensitive_Acc : 16.500, Run Time : 12.03 sec
INFO:root:2024-04-28 05:53:21, Train, Epoch : 10, Step : 5890, Loss : 0.19565, Acc : 0.934, Sensitive_Loss : 0.10094, Sensitive_Acc : 16.500, Run Time : 11.71 sec
INFO:root:2024-04-28 05:53:31, Train, Epoch : 10, Step : 5900, Loss : 0.25135, Acc : 0.887, Sensitive_Loss : 0.09004, Sensitive_Acc : 16.000, Run Time : 10.38 sec
INFO:root:2024-04-28 05:56:07, Dev, Step : 5900, Loss : 0.45533, Acc : 0.823, Auc : 0.904, Sensitive_Loss : 0.11030, Sensitive_Acc : 16.936, Sensitive_Auc : 0.995, Mean auc: 0.904, Run Time : 155.35 sec
INFO:root:2024-04-28 05:56:15, Train, Epoch : 10, Step : 5910, Loss : 0.19461, Acc : 0.919, Sensitive_Loss : 0.07801, Sensitive_Acc : 14.700, Run Time : 163.74 sec
INFO:root:2024-04-28 05:56:27, Train, Epoch : 10, Step : 5920, Loss : 0.23216, Acc : 0.884, Sensitive_Loss : 0.12546, Sensitive_Acc : 16.100, Run Time : 12.18 sec
INFO:root:2024-04-28 05:56:38, Train, Epoch : 10, Step : 5930, Loss : 0.21971, Acc : 0.916, Sensitive_Loss : 0.06485, Sensitive_Acc : 17.500, Run Time : 11.00 sec
INFO:root:2024-04-28 05:56:49, Train, Epoch : 10, Step : 5940, Loss : 0.22828, Acc : 0.881, Sensitive_Loss : 0.06799, Sensitive_Acc : 16.200, Run Time : 10.62 sec
INFO:root:2024-04-28 05:57:00, Train, Epoch : 10, Step : 5950, Loss : 0.17675, Acc : 0.928, Sensitive_Loss : 0.07476, Sensitive_Acc : 15.200, Run Time : 11.43 sec
INFO:root:2024-04-28 05:57:12, Train, Epoch : 10, Step : 5960, Loss : 0.19084, Acc : 0.916, Sensitive_Loss : 0.08142, Sensitive_Acc : 16.900, Run Time : 12.22 sec
INFO:root:2024-04-28 05:57:23, Train, Epoch : 10, Step : 5970, Loss : 0.18625, Acc : 0.909, Sensitive_Loss : 0.10147, Sensitive_Acc : 17.300, Run Time : 10.68 sec
INFO:root:2024-04-28 05:57:36, Train, Epoch : 10, Step : 5980, Loss : 0.23841, Acc : 0.909, Sensitive_Loss : 0.07722, Sensitive_Acc : 15.300, Run Time : 12.47 sec
INFO:root:2024-04-28 05:57:47, Train, Epoch : 10, Step : 5990, Loss : 0.21531, Acc : 0.916, Sensitive_Loss : 0.09471, Sensitive_Acc : 17.000, Run Time : 11.77 sec
INFO:root:2024-04-28 05:57:58, Train, Epoch : 10, Step : 6000, Loss : 0.18455, Acc : 0.925, Sensitive_Loss : 0.11471, Sensitive_Acc : 17.200, Run Time : 10.84 sec
INFO:root:2024-04-28 06:00:32, Dev, Step : 6000, Loss : 0.48246, Acc : 0.811, Auc : 0.903, Sensitive_Loss : 0.12574, Sensitive_Acc : 16.850, Sensitive_Auc : 0.994, Mean auc: 0.903, Run Time : 154.16 sec
INFO:root:2024-04-28 06:00:41, Train, Epoch : 10, Step : 6010, Loss : 0.19193, Acc : 0.916, Sensitive_Loss : 0.06356, Sensitive_Acc : 15.600, Run Time : 162.69 sec
INFO:root:2024-04-28 06:00:53, Train, Epoch : 10, Step : 6020, Loss : 0.25088, Acc : 0.887, Sensitive_Loss : 0.09859, Sensitive_Acc : 18.800, Run Time : 12.23 sec
INFO:root:2024-04-28 06:01:05, Train, Epoch : 10, Step : 6030, Loss : 0.19012, Acc : 0.900, Sensitive_Loss : 0.05998, Sensitive_Acc : 15.800, Run Time : 11.96 sec
INFO:root:2024-04-28 06:01:16, Train, Epoch : 10, Step : 6040, Loss : 0.22587, Acc : 0.909, Sensitive_Loss : 0.12721, Sensitive_Acc : 15.000, Run Time : 10.75 sec
INFO:root:2024-04-28 06:01:28, Train, Epoch : 10, Step : 6050, Loss : 0.22528, Acc : 0.897, Sensitive_Loss : 0.07468, Sensitive_Acc : 15.000, Run Time : 11.89 sec
INFO:root:2024-04-28 06:01:39, Train, Epoch : 10, Step : 6060, Loss : 0.17912, Acc : 0.928, Sensitive_Loss : 0.06263, Sensitive_Acc : 16.000, Run Time : 11.70 sec
INFO:root:2024-04-28 06:01:51, Train, Epoch : 10, Step : 6070, Loss : 0.20104, Acc : 0.916, Sensitive_Loss : 0.09450, Sensitive_Acc : 16.200, Run Time : 11.09 sec
INFO:root:2024-04-28 06:02:02, Train, Epoch : 10, Step : 6080, Loss : 0.18078, Acc : 0.931, Sensitive_Loss : 0.06271, Sensitive_Acc : 17.300, Run Time : 11.41 sec
INFO:root:2024-04-28 06:02:14, Train, Epoch : 10, Step : 6090, Loss : 0.21864, Acc : 0.887, Sensitive_Loss : 0.07911, Sensitive_Acc : 16.800, Run Time : 11.68 sec
INFO:root:2024-04-28 06:02:25, Train, Epoch : 10, Step : 6100, Loss : 0.18594, Acc : 0.925, Sensitive_Loss : 0.08830, Sensitive_Acc : 17.100, Run Time : 11.47 sec
INFO:root:2024-04-28 06:05:00, Dev, Step : 6100, Loss : 0.51304, Acc : 0.809, Auc : 0.903, Sensitive_Loss : 0.12803, Sensitive_Acc : 16.850, Sensitive_Auc : 0.994, Mean auc: 0.903, Run Time : 154.60 sec
INFO:root:2024-04-28 06:05:08, Train, Epoch : 10, Step : 6110, Loss : 0.18999, Acc : 0.922, Sensitive_Loss : 0.04965, Sensitive_Acc : 15.900, Run Time : 163.11 sec
INFO:root:2024-04-28 06:05:20, Train, Epoch : 10, Step : 6120, Loss : 0.22353, Acc : 0.903, Sensitive_Loss : 0.07925, Sensitive_Acc : 16.000, Run Time : 11.45 sec
INFO:root:2024-04-28 06:05:31, Train, Epoch : 10, Step : 6130, Loss : 0.22751, Acc : 0.916, Sensitive_Loss : 0.11379, Sensitive_Acc : 16.100, Run Time : 11.71 sec
INFO:root:2024-04-28 06:05:43, Train, Epoch : 10, Step : 6140, Loss : 0.18559, Acc : 0.925, Sensitive_Loss : 0.09069, Sensitive_Acc : 15.300, Run Time : 11.13 sec
INFO:root:2024-04-28 06:05:54, Train, Epoch : 10, Step : 6150, Loss : 0.23782, Acc : 0.900, Sensitive_Loss : 0.06886, Sensitive_Acc : 15.900, Run Time : 11.32 sec
INFO:root:2024-04-28 06:06:05, Train, Epoch : 10, Step : 6160, Loss : 0.26227, Acc : 0.866, Sensitive_Loss : 0.09969, Sensitive_Acc : 16.300, Run Time : 11.45 sec
INFO:root:2024-04-28 06:06:17, Train, Epoch : 10, Step : 6170, Loss : 0.16749, Acc : 0.928, Sensitive_Loss : 0.05906, Sensitive_Acc : 16.500, Run Time : 11.24 sec
INFO:root:2024-04-28 06:06:28, Train, Epoch : 10, Step : 6180, Loss : 0.22801, Acc : 0.903, Sensitive_Loss : 0.06384, Sensitive_Acc : 17.400, Run Time : 11.46 sec
INFO:root:2024-04-28 06:06:39, Train, Epoch : 10, Step : 6190, Loss : 0.23750, Acc : 0.891, Sensitive_Loss : 0.08877, Sensitive_Acc : 17.400, Run Time : 11.38 sec
INFO:root:2024-04-28 06:06:51, Train, Epoch : 10, Step : 6200, Loss : 0.24938, Acc : 0.906, Sensitive_Loss : 0.06418, Sensitive_Acc : 18.000, Run Time : 11.76 sec
INFO:root:2024-04-28 06:09:26, Dev, Step : 6200, Loss : 0.47059, Acc : 0.823, Auc : 0.903, Sensitive_Loss : 0.11471, Sensitive_Acc : 16.836, Sensitive_Auc : 0.995, Mean auc: 0.903, Run Time : 154.91 sec
INFO:root:2024-04-28 06:09:35, Train, Epoch : 10, Step : 6210, Loss : 0.24046, Acc : 0.903, Sensitive_Loss : 0.10238, Sensitive_Acc : 16.500, Run Time : 163.78 sec
INFO:root:2024-04-28 06:09:46, Train, Epoch : 10, Step : 6220, Loss : 0.17685, Acc : 0.950, Sensitive_Loss : 0.10338, Sensitive_Acc : 16.500, Run Time : 11.21 sec
INFO:root:2024-04-28 06:09:58, Train, Epoch : 10, Step : 6230, Loss : 0.17699, Acc : 0.916, Sensitive_Loss : 0.09967, Sensitive_Acc : 16.000, Run Time : 11.48 sec
INFO:root:2024-04-28 06:10:09, Train, Epoch : 10, Step : 6240, Loss : 0.18107, Acc : 0.919, Sensitive_Loss : 0.07888, Sensitive_Acc : 14.400, Run Time : 11.23 sec
INFO:root:2024-04-28 06:10:21, Train, Epoch : 10, Step : 6250, Loss : 0.21810, Acc : 0.916, Sensitive_Loss : 0.06487, Sensitive_Acc : 17.200, Run Time : 11.65 sec
INFO:root:2024-04-28 06:10:31, Train, Epoch : 10, Step : 6260, Loss : 0.19456, Acc : 0.922, Sensitive_Loss : 0.10404, Sensitive_Acc : 16.300, Run Time : 10.33 sec
INFO:root:2024-04-28 06:13:04
INFO:root:y_pred: [1.4314515e-02 9.8685092e-01 9.6862216e-04 ... 5.3858751e-01 1.3163853e-03
 9.4009691e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.9684614e-01 7.6887815e-04 2.2689398e-01 3.2418680e-05 9.9993789e-01
 9.4885187e-04 9.9996257e-01 9.9992967e-01 1.0493310e-04 8.7515873e-01
 9.9919504e-01 9.9998522e-01 9.9776709e-01 9.8757148e-01 1.3853899e-02
 9.9021143e-01 9.9997711e-01 6.4631561e-03 6.4006895e-01 9.8163646e-01
 9.9963927e-01 6.3433096e-02 9.9997067e-01 9.9868482e-01 9.9970835e-01
 9.9952483e-01 1.0974023e-04 9.9940991e-01 9.9888510e-01 6.7217553e-01
 7.8813888e-02 7.4781996e-01 1.8924557e-02 4.6710063e-02 2.3241445e-02
 1.3979331e-03 2.2082716e-02 5.0324234e-03 9.9987614e-01 9.9943393e-01
 5.8151486e-06 2.1230178e-02 9.8469245e-01 1.4130311e-03 9.9947673e-01
 9.9929667e-01 9.9955994e-01 9.8557544e-01 1.3295601e-01 9.9898475e-01
 9.9969435e-01 2.8180506e-03 8.2961458e-01 9.8687154e-04 1.7160672e-04
 1.5488415e-02 6.5442850e-03 2.0034639e-02 1.0625798e-02 5.0756305e-01
 1.4127462e-01 2.7104917e-01 7.0266135e-02 9.9755150e-01 4.2218846e-01
 9.9995911e-01 1.9217774e-03 9.9993658e-01 9.9868757e-01 7.9339612e-01
 9.2600358e-01 4.9900147e-01 1.1109489e-03 1.1992722e-01 1.1669324e-03
 1.3523259e-02 1.3787374e-02 1.8420215e-01 2.5246650e-04 9.9986577e-01
 9.9950826e-01 4.8315600e-03 6.9668382e-01 2.5269049e-03 9.5178366e-01
 9.8690701e-01 3.0174071e-02 1.0486844e-01 9.8602724e-01 9.9994946e-01
 9.9998069e-01 3.8486375e-03 2.9304551e-03 9.9987185e-01 3.8165995e-01
 6.4129694e-03 9.9902546e-01 9.9893123e-01 2.1740196e-04 1.1442030e-02
 9.9945599e-01 9.9921751e-01 9.9870300e-01 9.9482179e-01 2.0433245e-02
 6.8529315e-02 9.9369037e-01 9.9972528e-01 9.8493850e-01 1.1544160e-04
 9.9728119e-01 9.9722195e-01 2.2959169e-02 9.9993360e-01 9.9987423e-01
 9.9995089e-01 9.5367855e-01 9.9932849e-01 5.0231319e-02 2.0506077e-01
 9.9992561e-01 9.9980110e-01 1.0338461e-03 9.9892056e-01 9.9998891e-01
 4.7084349e-01 9.9702400e-01 1.3421755e-02 2.6438212e-02 9.7963756e-01
 9.9767035e-01 9.0660114e-04 1.5120115e-03 4.9447015e-02 9.8702896e-01
 9.9816054e-01 9.9626750e-01 3.0973871e-04 6.6152140e-03 9.9937075e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 06:13:04, Dev, Step : 6260, Loss : 0.52597, Acc : 0.810, Auc : 0.900, Sensitive_Loss : 0.12952, Sensitive_Acc : 16.836, Sensitive_Auc : 0.995, Mean auc: 0.900, Run Time : 152.90 sec
