Running on desktop22:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/balanced_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/balanced_dataset_val.csv",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.05,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-10 08:14:29, Train, Epoch : 1, Step : 10, Loss : 0.74073, Acc : 0.506, Sensitive_Loss : 0.75727, Sensitive_Acc : 15.900, Run Time : 10.99 sec
INFO:root:2024-04-10 08:14:39, Train, Epoch : 1, Step : 20, Loss : 0.69464, Acc : 0.569, Sensitive_Loss : 0.69010, Sensitive_Acc : 15.800, Run Time : 9.25 sec
INFO:root:2024-04-10 08:14:48, Train, Epoch : 1, Step : 30, Loss : 0.69687, Acc : 0.662, Sensitive_Loss : 0.69405, Sensitive_Acc : 17.700, Run Time : 8.91 sec
INFO:root:2024-04-10 08:14:57, Train, Epoch : 1, Step : 40, Loss : 0.66037, Acc : 0.669, Sensitive_Loss : 0.71491, Sensitive_Acc : 14.800, Run Time : 9.45 sec
INFO:root:2024-04-10 08:15:06, Train, Epoch : 1, Step : 50, Loss : 0.68098, Acc : 0.694, Sensitive_Loss : 0.64625, Sensitive_Acc : 15.100, Run Time : 8.90 sec
INFO:root:2024-04-10 08:15:15, Train, Epoch : 1, Step : 60, Loss : 0.70195, Acc : 0.628, Sensitive_Loss : 0.61379, Sensitive_Acc : 15.100, Run Time : 9.48 sec
INFO:root:2024-04-10 08:15:24, Train, Epoch : 1, Step : 70, Loss : 0.55715, Acc : 0.669, Sensitive_Loss : 0.54800, Sensitive_Acc : 16.100, Run Time : 8.69 sec
INFO:root:2024-04-10 08:15:34, Train, Epoch : 1, Step : 80, Loss : 0.57193, Acc : 0.675, Sensitive_Loss : 0.58443, Sensitive_Acc : 15.000, Run Time : 9.58 sec
INFO:root:2024-04-10 08:15:43, Train, Epoch : 1, Step : 90, Loss : 0.56243, Acc : 0.700, Sensitive_Loss : 0.58447, Sensitive_Acc : 15.000, Run Time : 9.40 sec
INFO:root:2024-04-10 08:15:52, Train, Epoch : 1, Step : 100, Loss : 0.52718, Acc : 0.706, Sensitive_Loss : 0.46294, Sensitive_Acc : 14.900, Run Time : 9.24 sec
INFO:root:2024-04-10 08:17:47, Dev, Step : 100, Loss : 0.59037, Acc : 0.695, Auc : 0.768, Sensitive_Loss : 0.51092, Sensitive_Acc : 15.950, Sensitive_Auc : 0.900, Mean auc: 0.768, Run Time : 114.23 sec
INFO:root:2024-04-10 08:17:47, Best, Step : 100, Loss : 0.59037, Acc : 0.695, Auc : 0.768, Sensitive_Loss : 0.51092, Sensitive_Acc : 15.950, Sensitive_Auc : 0.900, Best Auc : 0.768
INFO:root:2024-04-10 08:17:54, Train, Epoch : 1, Step : 110, Loss : 0.59712, Acc : 0.694, Sensitive_Loss : 0.54408, Sensitive_Acc : 15.200, Run Time : 121.67 sec
INFO:root:2024-04-10 08:18:04, Train, Epoch : 1, Step : 120, Loss : 0.59476, Acc : 0.662, Sensitive_Loss : 0.52859, Sensitive_Acc : 17.100, Run Time : 10.10 sec
INFO:root:2024-04-10 08:18:14, Train, Epoch : 1, Step : 130, Loss : 0.58172, Acc : 0.694, Sensitive_Loss : 0.48856, Sensitive_Acc : 16.800, Run Time : 10.19 sec
INFO:root:2024-04-10 08:18:25, Train, Epoch : 1, Step : 140, Loss : 0.62057, Acc : 0.700, Sensitive_Loss : 0.48102, Sensitive_Acc : 16.000, Run Time : 10.62 sec
INFO:root:2024-04-10 08:18:35, Train, Epoch : 1, Step : 150, Loss : 0.52372, Acc : 0.750, Sensitive_Loss : 0.46762, Sensitive_Acc : 15.300, Run Time : 9.95 sec
INFO:root:2024-04-10 08:18:45, Train, Epoch : 1, Step : 160, Loss : 0.65909, Acc : 0.641, Sensitive_Loss : 0.45802, Sensitive_Acc : 15.400, Run Time : 10.23 sec
INFO:root:2024-04-10 08:18:56, Train, Epoch : 1, Step : 170, Loss : 0.60457, Acc : 0.672, Sensitive_Loss : 0.48594, Sensitive_Acc : 16.800, Run Time : 10.62 sec
INFO:root:2024-04-10 08:19:06, Train, Epoch : 1, Step : 180, Loss : 0.64130, Acc : 0.650, Sensitive_Loss : 0.47687, Sensitive_Acc : 16.900, Run Time : 10.26 sec
INFO:root:2024-04-10 08:19:16, Train, Epoch : 1, Step : 190, Loss : 0.64308, Acc : 0.681, Sensitive_Loss : 0.51273, Sensitive_Acc : 17.300, Run Time : 10.32 sec
INFO:root:2024-04-10 08:19:28, Train, Epoch : 1, Step : 200, Loss : 0.55443, Acc : 0.709, Sensitive_Loss : 0.37494, Sensitive_Acc : 15.700, Run Time : 11.79 sec
INFO:root:2024-04-10 08:21:02, Dev, Step : 200, Loss : 0.62974, Acc : 0.675, Auc : 0.745, Sensitive_Loss : 0.45281, Sensitive_Acc : 15.993, Sensitive_Auc : 0.915, Mean auc: 0.745, Run Time : 93.95 sec
INFO:root:2024-04-10 08:21:11, Train, Epoch : 1, Step : 210, Loss : 0.58508, Acc : 0.666, Sensitive_Loss : 0.40245, Sensitive_Acc : 16.400, Run Time : 102.98 sec
INFO:root:2024-04-10 08:21:23, Train, Epoch : 1, Step : 220, Loss : 0.59813, Acc : 0.706, Sensitive_Loss : 0.44148, Sensitive_Acc : 14.500, Run Time : 12.25 sec
INFO:root:2024-04-10 08:21:35, Train, Epoch : 1, Step : 230, Loss : 0.57109, Acc : 0.703, Sensitive_Loss : 0.41640, Sensitive_Acc : 15.800, Run Time : 11.33 sec
INFO:root:2024-04-10 08:21:48, Train, Epoch : 1, Step : 240, Loss : 0.60272, Acc : 0.700, Sensitive_Loss : 0.42424, Sensitive_Acc : 16.300, Run Time : 13.38 sec
INFO:root:2024-04-10 08:21:59, Train, Epoch : 1, Step : 250, Loss : 0.55256, Acc : 0.741, Sensitive_Loss : 0.39339, Sensitive_Acc : 15.300, Run Time : 11.30 sec
INFO:root:2024-04-10 08:22:11, Train, Epoch : 1, Step : 260, Loss : 0.54486, Acc : 0.750, Sensitive_Loss : 0.35297, Sensitive_Acc : 17.300, Run Time : 11.78 sec
INFO:root:2024-04-10 08:22:22, Train, Epoch : 1, Step : 270, Loss : 0.57334, Acc : 0.669, Sensitive_Loss : 0.37530, Sensitive_Acc : 16.100, Run Time : 11.12 sec
INFO:root:2024-04-10 08:22:34, Train, Epoch : 1, Step : 280, Loss : 0.63840, Acc : 0.719, Sensitive_Loss : 0.37090, Sensitive_Acc : 15.400, Run Time : 11.63 sec
INFO:root:2024-04-10 08:22:45, Train, Epoch : 1, Step : 290, Loss : 0.57276, Acc : 0.688, Sensitive_Loss : 0.31268, Sensitive_Acc : 16.400, Run Time : 11.25 sec
INFO:root:2024-04-10 08:22:58, Train, Epoch : 1, Step : 300, Loss : 0.55640, Acc : 0.722, Sensitive_Loss : 0.36006, Sensitive_Acc : 16.500, Run Time : 13.30 sec
INFO:root:2024-04-10 08:24:35, Dev, Step : 300, Loss : 0.56981, Acc : 0.719, Auc : 0.794, Sensitive_Loss : 0.48579, Sensitive_Acc : 16.136, Sensitive_Auc : 0.943, Mean auc: 0.794, Run Time : 96.95 sec
INFO:root:2024-04-10 08:24:36, Best, Step : 300, Loss : 0.56981, Acc : 0.719, Auc : 0.794, Sensitive_Loss : 0.48579, Sensitive_Acc : 16.136, Sensitive_Auc : 0.943, Best Auc : 0.794
INFO:root:2024-04-10 08:24:44, Train, Epoch : 1, Step : 310, Loss : 0.49273, Acc : 0.741, Sensitive_Loss : 0.37240, Sensitive_Acc : 17.600, Run Time : 105.83 sec
INFO:root:2024-04-10 08:24:57, Train, Epoch : 1, Step : 320, Loss : 0.66896, Acc : 0.697, Sensitive_Loss : 0.29159, Sensitive_Acc : 16.200, Run Time : 12.72 sec
INFO:root:2024-04-10 08:25:08, Train, Epoch : 1, Step : 330, Loss : 0.58510, Acc : 0.681, Sensitive_Loss : 0.32850, Sensitive_Acc : 16.800, Run Time : 11.57 sec
INFO:root:2024-04-10 08:25:21, Train, Epoch : 1, Step : 340, Loss : 0.47940, Acc : 0.747, Sensitive_Loss : 0.28147, Sensitive_Acc : 15.500, Run Time : 12.52 sec
INFO:root:2024-04-10 08:25:32, Train, Epoch : 1, Step : 350, Loss : 0.58159, Acc : 0.716, Sensitive_Loss : 0.35008, Sensitive_Acc : 15.700, Run Time : 10.99 sec
INFO:root:2024-04-10 08:25:43, Train, Epoch : 1, Step : 360, Loss : 0.55984, Acc : 0.684, Sensitive_Loss : 0.30401, Sensitive_Acc : 15.600, Run Time : 11.22 sec
INFO:root:2024-04-10 08:25:54, Train, Epoch : 1, Step : 370, Loss : 0.54637, Acc : 0.697, Sensitive_Loss : 0.31427, Sensitive_Acc : 16.000, Run Time : 10.75 sec
INFO:root:2024-04-10 08:26:05, Train, Epoch : 1, Step : 380, Loss : 0.52367, Acc : 0.706, Sensitive_Loss : 0.26445, Sensitive_Acc : 16.900, Run Time : 11.24 sec
INFO:root:2024-04-10 08:26:16, Train, Epoch : 1, Step : 390, Loss : 0.51963, Acc : 0.728, Sensitive_Loss : 0.27516, Sensitive_Acc : 17.500, Run Time : 10.60 sec
INFO:root:2024-04-10 08:26:28, Train, Epoch : 1, Step : 400, Loss : 0.66753, Acc : 0.669, Sensitive_Loss : 0.27795, Sensitive_Acc : 15.500, Run Time : 12.60 sec
INFO:root:2024-04-10 08:28:03, Dev, Step : 400, Loss : 0.64140, Acc : 0.671, Auc : 0.793, Sensitive_Loss : 0.29919, Sensitive_Acc : 16.379, Sensitive_Auc : 0.954, Mean auc: 0.793, Run Time : 94.26 sec
INFO:root:2024-04-10 08:28:10, Train, Epoch : 1, Step : 410, Loss : 0.58757, Acc : 0.731, Sensitive_Loss : 0.31245, Sensitive_Acc : 15.600, Run Time : 102.01 sec
INFO:root:2024-04-10 08:28:22, Train, Epoch : 1, Step : 420, Loss : 0.50308, Acc : 0.744, Sensitive_Loss : 0.31018, Sensitive_Acc : 17.000, Run Time : 11.11 sec
INFO:root:2024-04-10 08:28:32, Train, Epoch : 1, Step : 430, Loss : 0.50722, Acc : 0.762, Sensitive_Loss : 0.34489, Sensitive_Acc : 14.800, Run Time : 10.66 sec
INFO:root:2024-04-10 08:28:44, Train, Epoch : 1, Step : 440, Loss : 0.49688, Acc : 0.706, Sensitive_Loss : 0.24722, Sensitive_Acc : 14.000, Run Time : 12.07 sec
INFO:root:2024-04-10 08:28:55, Train, Epoch : 1, Step : 450, Loss : 0.53666, Acc : 0.728, Sensitive_Loss : 0.33764, Sensitive_Acc : 15.000, Run Time : 11.03 sec
INFO:root:2024-04-10 08:29:06, Train, Epoch : 1, Step : 460, Loss : 0.54716, Acc : 0.713, Sensitive_Loss : 0.27753, Sensitive_Acc : 15.800, Run Time : 11.08 sec
INFO:root:2024-04-10 08:29:19, Train, Epoch : 1, Step : 470, Loss : 0.51043, Acc : 0.722, Sensitive_Loss : 0.28543, Sensitive_Acc : 15.600, Run Time : 12.72 sec
INFO:root:2024-04-10 08:29:30, Train, Epoch : 1, Step : 480, Loss : 0.49833, Acc : 0.738, Sensitive_Loss : 0.36920, Sensitive_Acc : 16.900, Run Time : 11.08 sec
INFO:root:2024-04-10 08:29:41, Train, Epoch : 1, Step : 490, Loss : 0.57051, Acc : 0.747, Sensitive_Loss : 0.31453, Sensitive_Acc : 16.900, Run Time : 10.92 sec
INFO:root:2024-04-10 08:29:53, Train, Epoch : 1, Step : 500, Loss : 0.52604, Acc : 0.766, Sensitive_Loss : 0.25679, Sensitive_Acc : 17.600, Run Time : 11.47 sec
INFO:root:2024-04-10 08:31:27, Dev, Step : 500, Loss : 0.57518, Acc : 0.723, Auc : 0.796, Sensitive_Loss : 0.26927, Sensitive_Acc : 16.150, Sensitive_Auc : 0.972, Mean auc: 0.796, Run Time : 94.27 sec
INFO:root:2024-04-10 08:31:27, Best, Step : 500, Loss : 0.57518, Acc : 0.723, Auc : 0.796, Sensitive_Loss : 0.26927, Sensitive_Acc : 16.150, Sensitive_Auc : 0.972, Best Auc : 0.796
INFO:root:2024-04-10 08:31:35, Train, Epoch : 1, Step : 510, Loss : 0.53619, Acc : 0.700, Sensitive_Loss : 0.24457, Sensitive_Acc : 18.000, Run Time : 102.62 sec
INFO:root:2024-04-10 08:31:47, Train, Epoch : 1, Step : 520, Loss : 0.61334, Acc : 0.713, Sensitive_Loss : 0.30287, Sensitive_Acc : 16.700, Run Time : 11.72 sec
INFO:root:2024-04-10 08:31:58, Train, Epoch : 1, Step : 530, Loss : 0.59919, Acc : 0.709, Sensitive_Loss : 0.29648, Sensitive_Acc : 16.600, Run Time : 10.80 sec
INFO:root:2024-04-10 08:32:11, Train, Epoch : 1, Step : 540, Loss : 0.56920, Acc : 0.706, Sensitive_Loss : 0.28592, Sensitive_Acc : 16.100, Run Time : 13.16 sec
INFO:root:2024-04-10 08:32:22, Train, Epoch : 1, Step : 550, Loss : 0.46743, Acc : 0.728, Sensitive_Loss : 0.27502, Sensitive_Acc : 16.000, Run Time : 10.90 sec
INFO:root:2024-04-10 08:32:33, Train, Epoch : 1, Step : 560, Loss : 0.57934, Acc : 0.728, Sensitive_Loss : 0.29513, Sensitive_Acc : 17.000, Run Time : 11.23 sec
INFO:root:2024-04-10 08:32:44, Train, Epoch : 1, Step : 570, Loss : 0.52858, Acc : 0.675, Sensitive_Loss : 0.21154, Sensitive_Acc : 15.800, Run Time : 11.07 sec
INFO:root:2024-04-10 08:32:55, Train, Epoch : 1, Step : 580, Loss : 0.53856, Acc : 0.728, Sensitive_Loss : 0.29545, Sensitive_Acc : 15.200, Run Time : 11.42 sec
INFO:root:2024-04-10 08:33:06, Train, Epoch : 1, Step : 590, Loss : 0.55385, Acc : 0.750, Sensitive_Loss : 0.23224, Sensitive_Acc : 16.100, Run Time : 10.71 sec
INFO:root:2024-04-10 08:33:17, Train, Epoch : 1, Step : 600, Loss : 0.63936, Acc : 0.722, Sensitive_Loss : 0.22492, Sensitive_Acc : 16.800, Run Time : 11.08 sec
INFO:root:2024-04-10 08:34:52, Dev, Step : 600, Loss : 0.63833, Acc : 0.656, Auc : 0.806, Sensitive_Loss : 0.22948, Sensitive_Acc : 15.979, Sensitive_Auc : 0.979, Mean auc: 0.806, Run Time : 95.21 sec
INFO:root:2024-04-10 08:34:53, Best, Step : 600, Loss : 0.63833, Acc : 0.656, Auc : 0.806, Sensitive_Loss : 0.22948, Sensitive_Acc : 15.979, Sensitive_Auc : 0.979, Best Auc : 0.806
INFO:root:2024-04-10 08:35:03, Train, Epoch : 1, Step : 610, Loss : 0.54678, Acc : 0.728, Sensitive_Loss : 0.26319, Sensitive_Acc : 16.200, Run Time : 106.17 sec
INFO:root:2024-04-10 08:35:15, Train, Epoch : 1, Step : 620, Loss : 0.49015, Acc : 0.744, Sensitive_Loss : 0.24050, Sensitive_Acc : 16.300, Run Time : 11.83 sec
INFO:root:2024-04-10 08:35:26, Train, Epoch : 1, Step : 630, Loss : 0.46690, Acc : 0.756, Sensitive_Loss : 0.24671, Sensitive_Acc : 15.400, Run Time : 10.98 sec
INFO:root:2024-04-10 08:35:39, Train, Epoch : 1, Step : 640, Loss : 0.55278, Acc : 0.713, Sensitive_Loss : 0.21858, Sensitive_Acc : 15.200, Run Time : 12.95 sec
INFO:root:2024-04-10 08:37:16
INFO:root:y_pred: [0.46730256 0.1497833  0.3004751  ... 0.4070619  0.3622123  0.29884624]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.85502958e-01 2.24894173e-02 7.82466710e-01 9.98851657e-01
 9.94438767e-01 9.21771049e-01 9.90905643e-01 2.25216313e-03
 9.87474561e-01 9.91195083e-01 6.85799897e-01 4.58454400e-01
 1.85746385e-03 8.55558455e-01 9.95752692e-01 9.86974955e-01
 9.72706676e-01 9.48976398e-01 9.04657543e-01 9.73636746e-01
 9.93607998e-01 2.21384138e-01 9.87726986e-01 9.40216422e-01
 8.22299123e-01 2.07912028e-01 9.70749915e-01 3.74468148e-01
 9.97113347e-01 2.59921923e-02 1.56206908e-02 1.99714378e-01
 3.07181180e-01 9.84094799e-01 3.08653660e-04 9.97940481e-01
 4.77366801e-03 9.99197423e-01 1.38084486e-01 9.85842645e-01
 9.96316254e-01 3.32144350e-02 2.28682548e-01 9.65267792e-03
 6.80872977e-01 4.28312719e-01 9.36003745e-01 8.15750837e-01
 8.32416415e-01 9.82374072e-01 8.03260580e-02 7.99388945e-01
 1.67420313e-01 9.76987243e-01 9.46970642e-01 3.42818528e-01
 9.94962811e-01 9.57240641e-01 9.27078664e-01 3.55321579e-02
 1.11364396e-02 9.23001707e-01 6.96083128e-01 9.94273484e-01
 9.12781417e-01 4.11660582e-01 8.49003673e-01 6.89213276e-01
 9.97418761e-01 9.68159556e-01 7.04371370e-03 9.77727056e-01
 9.64852631e-01 9.24518645e-01 9.86888528e-01 1.09617854e-03
 5.62218986e-02 5.09400479e-02 3.95721523e-03 9.94940758e-01
 1.42181039e-01 9.22673285e-01 9.95982289e-01 9.95595038e-01
 1.33804530e-01 9.99620557e-01 1.80585049e-02 3.31272036e-01
 9.92380798e-01 9.61246610e-01 4.13572602e-02 8.45286667e-01
 7.34397173e-02 9.39063132e-01 1.11626804e-01 9.98597682e-01
 1.41476676e-01 9.78143215e-01 7.17779338e-01 1.47350609e-01
 9.76389367e-03 8.92758071e-02 9.64928210e-01 9.98046160e-01
 6.58113718e-01 8.91132474e-01 9.79608357e-01 4.10874784e-02
 3.71793151e-01 9.91118371e-01 1.24540804e-02 8.13265815e-02
 7.86754549e-01 9.91678357e-01 8.53150845e-01 5.89215830e-02
 8.58859360e-01 1.60642579e-01 9.97792006e-01 7.96425045e-01
 9.92516339e-01 9.99197900e-01 5.00024736e-01 6.01414382e-01
 5.02993822e-01 2.86895130e-02 6.51673004e-02 2.18216912e-03
 9.57132757e-01 9.97858703e-01 1.10138515e-02 2.76447888e-02
 1.74493692e-03 3.29480410e-01 9.96652424e-01 9.84225154e-01
 9.06087339e-01 1.88334033e-01 1.25242155e-02 4.88727450e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 08:37:16, Dev, Step : 644, Loss : 0.56341, Acc : 0.727, Auc : 0.817, Sensitive_Loss : 0.27986, Sensitive_Acc : 16.121, Sensitive_Auc : 0.971, Mean auc: 0.817, Run Time : 91.86 sec
INFO:root:2024-04-10 08:37:16, Best, Step : 644, Loss : 0.56341, Acc : 0.727,Auc : 0.817, Best Auc : 0.817, Sensitive_Loss : 0.27986, Sensitive_Acc : 16.121, Sensitive_Auc : 0.971
INFO:root:2024-04-10 08:37:23, Train, Epoch : 2, Step : 650, Loss : 0.23687, Acc : 0.463, Sensitive_Loss : 0.13230, Sensitive_Acc : 9.900, Run Time : 6.15 sec
INFO:root:2024-04-10 08:37:32, Train, Epoch : 2, Step : 660, Loss : 0.49028, Acc : 0.734, Sensitive_Loss : 0.21388, Sensitive_Acc : 17.400, Run Time : 9.13 sec
INFO:root:2024-04-10 08:37:41, Train, Epoch : 2, Step : 670, Loss : 0.49096, Acc : 0.766, Sensitive_Loss : 0.19145, Sensitive_Acc : 16.300, Run Time : 8.73 sec
INFO:root:2024-04-10 08:37:50, Train, Epoch : 2, Step : 680, Loss : 0.52451, Acc : 0.728, Sensitive_Loss : 0.24248, Sensitive_Acc : 16.800, Run Time : 8.75 sec
INFO:root:2024-04-10 08:37:58, Train, Epoch : 2, Step : 690, Loss : 0.53056, Acc : 0.747, Sensitive_Loss : 0.24621, Sensitive_Acc : 16.000, Run Time : 8.63 sec
INFO:root:2024-04-10 08:38:07, Train, Epoch : 2, Step : 700, Loss : 0.53583, Acc : 0.759, Sensitive_Loss : 0.20005, Sensitive_Acc : 15.000, Run Time : 8.77 sec
INFO:root:2024-04-10 08:39:40, Dev, Step : 700, Loss : 0.57560, Acc : 0.727, Auc : 0.818, Sensitive_Loss : 0.20582, Sensitive_Acc : 16.150, Sensitive_Auc : 0.976, Mean auc: 0.818, Run Time : 93.16 sec
INFO:root:2024-04-10 08:39:41, Best, Step : 700, Loss : 0.57560, Acc : 0.727, Auc : 0.818, Sensitive_Loss : 0.20582, Sensitive_Acc : 16.150, Sensitive_Auc : 0.976, Best Auc : 0.818
INFO:root:2024-04-10 08:39:47, Train, Epoch : 2, Step : 710, Loss : 0.49359, Acc : 0.825, Sensitive_Loss : 0.19478, Sensitive_Acc : 16.800, Run Time : 100.27 sec
INFO:root:2024-04-10 08:39:57, Train, Epoch : 2, Step : 720, Loss : 0.42715, Acc : 0.741, Sensitive_Loss : 0.26412, Sensitive_Acc : 18.400, Run Time : 9.61 sec
INFO:root:2024-04-10 08:40:07, Train, Epoch : 2, Step : 730, Loss : 0.54428, Acc : 0.738, Sensitive_Loss : 0.21135, Sensitive_Acc : 15.400, Run Time : 10.40 sec
INFO:root:2024-04-10 08:40:17, Train, Epoch : 2, Step : 740, Loss : 0.52409, Acc : 0.772, Sensitive_Loss : 0.19762, Sensitive_Acc : 15.000, Run Time : 9.23 sec
INFO:root:2024-04-10 08:40:26, Train, Epoch : 2, Step : 750, Loss : 0.44268, Acc : 0.772, Sensitive_Loss : 0.26009, Sensitive_Acc : 16.100, Run Time : 9.41 sec
INFO:root:2024-04-10 08:40:36, Train, Epoch : 2, Step : 760, Loss : 0.54036, Acc : 0.747, Sensitive_Loss : 0.21315, Sensitive_Acc : 15.800, Run Time : 9.52 sec
INFO:root:2024-04-10 08:40:45, Train, Epoch : 2, Step : 770, Loss : 0.49543, Acc : 0.791, Sensitive_Loss : 0.24473, Sensitive_Acc : 16.200, Run Time : 9.02 sec
INFO:root:2024-04-10 08:40:54, Train, Epoch : 2, Step : 780, Loss : 0.53849, Acc : 0.762, Sensitive_Loss : 0.20329, Sensitive_Acc : 15.300, Run Time : 9.55 sec
INFO:root:2024-04-10 08:41:03, Train, Epoch : 2, Step : 790, Loss : 0.49415, Acc : 0.759, Sensitive_Loss : 0.22151, Sensitive_Acc : 15.200, Run Time : 9.26 sec
INFO:root:2024-04-10 08:41:14, Train, Epoch : 2, Step : 800, Loss : 0.55658, Acc : 0.738, Sensitive_Loss : 0.25303, Sensitive_Acc : 15.500, Run Time : 10.40 sec
INFO:root:2024-04-10 08:42:48, Dev, Step : 800, Loss : 0.58120, Acc : 0.717, Auc : 0.810, Sensitive_Loss : 0.21912, Sensitive_Acc : 16.379, Sensitive_Auc : 0.992, Mean auc: 0.810, Run Time : 93.83 sec
INFO:root:2024-04-10 08:42:55, Train, Epoch : 2, Step : 810, Loss : 0.49284, Acc : 0.744, Sensitive_Loss : 0.21816, Sensitive_Acc : 16.800, Run Time : 101.56 sec
INFO:root:2024-04-10 08:43:05, Train, Epoch : 2, Step : 820, Loss : 0.45359, Acc : 0.747, Sensitive_Loss : 0.17771, Sensitive_Acc : 16.300, Run Time : 9.69 sec
INFO:root:2024-04-10 08:43:15, Train, Epoch : 2, Step : 830, Loss : 0.49669, Acc : 0.756, Sensitive_Loss : 0.21710, Sensitive_Acc : 16.100, Run Time : 9.44 sec
INFO:root:2024-04-10 08:43:26, Train, Epoch : 2, Step : 840, Loss : 0.51924, Acc : 0.734, Sensitive_Loss : 0.22256, Sensitive_Acc : 16.100, Run Time : 11.21 sec
INFO:root:2024-04-10 08:43:38, Train, Epoch : 2, Step : 850, Loss : 0.51971, Acc : 0.756, Sensitive_Loss : 0.17261, Sensitive_Acc : 15.300, Run Time : 11.94 sec
INFO:root:2024-04-10 08:43:48, Train, Epoch : 2, Step : 860, Loss : 0.45214, Acc : 0.791, Sensitive_Loss : 0.22812, Sensitive_Acc : 16.800, Run Time : 9.90 sec
INFO:root:2024-04-10 08:43:58, Train, Epoch : 2, Step : 870, Loss : 0.53506, Acc : 0.747, Sensitive_Loss : 0.20678, Sensitive_Acc : 16.300, Run Time : 10.11 sec
INFO:root:2024-04-10 08:44:09, Train, Epoch : 2, Step : 880, Loss : 0.50971, Acc : 0.759, Sensitive_Loss : 0.14366, Sensitive_Acc : 16.400, Run Time : 11.43 sec
INFO:root:2024-04-10 08:44:18, Train, Epoch : 2, Step : 890, Loss : 0.53585, Acc : 0.775, Sensitive_Loss : 0.16018, Sensitive_Acc : 15.500, Run Time : 8.81 sec
INFO:root:2024-04-10 08:44:27, Train, Epoch : 2, Step : 900, Loss : 0.54233, Acc : 0.747, Sensitive_Loss : 0.21956, Sensitive_Acc : 16.900, Run Time : 8.69 sec
INFO:root:2024-04-10 08:46:00, Dev, Step : 900, Loss : 0.53385, Acc : 0.756, Auc : 0.825, Sensitive_Loss : 0.26819, Sensitive_Acc : 16.150, Sensitive_Auc : 0.992, Mean auc: 0.825, Run Time : 93.64 sec
INFO:root:2024-04-10 08:46:01, Best, Step : 900, Loss : 0.53385, Acc : 0.756, Auc : 0.825, Sensitive_Loss : 0.26819, Sensitive_Acc : 16.150, Sensitive_Auc : 0.992, Best Auc : 0.825
INFO:root:2024-04-10 08:46:07, Train, Epoch : 2, Step : 910, Loss : 0.52980, Acc : 0.741, Sensitive_Loss : 0.22658, Sensitive_Acc : 17.500, Run Time : 100.82 sec
INFO:root:2024-04-10 08:46:17, Train, Epoch : 2, Step : 920, Loss : 0.56694, Acc : 0.759, Sensitive_Loss : 0.18395, Sensitive_Acc : 16.000, Run Time : 9.44 sec
INFO:root:2024-04-10 08:46:29, Train, Epoch : 2, Step : 930, Loss : 0.51820, Acc : 0.741, Sensitive_Loss : 0.23367, Sensitive_Acc : 15.600, Run Time : 12.21 sec
INFO:root:2024-04-10 08:46:38, Train, Epoch : 2, Step : 940, Loss : 0.41599, Acc : 0.800, Sensitive_Loss : 0.25686, Sensitive_Acc : 17.900, Run Time : 8.54 sec
INFO:root:2024-04-10 08:46:46, Train, Epoch : 2, Step : 950, Loss : 0.51680, Acc : 0.734, Sensitive_Loss : 0.30052, Sensitive_Acc : 16.200, Run Time : 8.69 sec
INFO:root:2024-04-10 08:46:56, Train, Epoch : 2, Step : 960, Loss : 0.52631, Acc : 0.725, Sensitive_Loss : 0.21531, Sensitive_Acc : 15.500, Run Time : 9.89 sec
INFO:root:2024-04-10 08:47:05, Train, Epoch : 2, Step : 970, Loss : 0.51129, Acc : 0.734, Sensitive_Loss : 0.21610, Sensitive_Acc : 14.900, Run Time : 8.90 sec
INFO:root:2024-04-10 08:47:15, Train, Epoch : 2, Step : 980, Loss : 0.55826, Acc : 0.678, Sensitive_Loss : 0.18492, Sensitive_Acc : 15.100, Run Time : 9.74 sec
INFO:root:2024-04-10 08:47:25, Train, Epoch : 2, Step : 990, Loss : 0.61357, Acc : 0.756, Sensitive_Loss : 0.22483, Sensitive_Acc : 16.000, Run Time : 10.06 sec
INFO:root:2024-04-10 08:47:35, Train, Epoch : 2, Step : 1000, Loss : 0.53405, Acc : 0.731, Sensitive_Loss : 0.16294, Sensitive_Acc : 16.900, Run Time : 10.56 sec
INFO:root:2024-04-10 08:49:09, Dev, Step : 1000, Loss : 0.53193, Acc : 0.752, Auc : 0.826, Sensitive_Loss : 0.26255, Sensitive_Acc : 16.307, Sensitive_Auc : 0.985, Mean auc: 0.826, Run Time : 93.49 sec
INFO:root:2024-04-10 08:49:10, Best, Step : 1000, Loss : 0.53193, Acc : 0.752, Auc : 0.826, Sensitive_Loss : 0.26255, Sensitive_Acc : 16.307, Sensitive_Auc : 0.985, Best Auc : 0.826
INFO:root:2024-04-10 08:49:17, Train, Epoch : 2, Step : 1010, Loss : 0.48305, Acc : 0.759, Sensitive_Loss : 0.21203, Sensitive_Acc : 15.200, Run Time : 101.13 sec
INFO:root:2024-04-10 08:49:26, Train, Epoch : 2, Step : 1020, Loss : 0.50861, Acc : 0.784, Sensitive_Loss : 0.21984, Sensitive_Acc : 15.800, Run Time : 9.23 sec
INFO:root:2024-04-10 08:49:35, Train, Epoch : 2, Step : 1030, Loss : 0.44489, Acc : 0.775, Sensitive_Loss : 0.18044, Sensitive_Acc : 17.400, Run Time : 9.18 sec
INFO:root:2024-04-10 08:49:44, Train, Epoch : 2, Step : 1040, Loss : 0.49896, Acc : 0.766, Sensitive_Loss : 0.23031, Sensitive_Acc : 15.800, Run Time : 8.87 sec
INFO:root:2024-04-10 08:49:53, Train, Epoch : 2, Step : 1050, Loss : 0.48811, Acc : 0.741, Sensitive_Loss : 0.18698, Sensitive_Acc : 16.400, Run Time : 9.32 sec
INFO:root:2024-04-10 08:50:02, Train, Epoch : 2, Step : 1060, Loss : 0.53543, Acc : 0.778, Sensitive_Loss : 0.20916, Sensitive_Acc : 15.500, Run Time : 8.62 sec
INFO:root:2024-04-10 08:50:11, Train, Epoch : 2, Step : 1070, Loss : 0.52610, Acc : 0.772, Sensitive_Loss : 0.22842, Sensitive_Acc : 16.200, Run Time : 9.01 sec
INFO:root:2024-04-10 08:50:20, Train, Epoch : 2, Step : 1080, Loss : 0.50135, Acc : 0.725, Sensitive_Loss : 0.16688, Sensitive_Acc : 15.700, Run Time : 9.40 sec
INFO:root:2024-04-10 08:50:29, Train, Epoch : 2, Step : 1090, Loss : 0.50204, Acc : 0.709, Sensitive_Loss : 0.26219, Sensitive_Acc : 18.600, Run Time : 9.04 sec
INFO:root:2024-04-10 08:50:38, Train, Epoch : 2, Step : 1100, Loss : 0.45537, Acc : 0.731, Sensitive_Loss : 0.21013, Sensitive_Acc : 16.200, Run Time : 9.05 sec
INFO:root:2024-04-10 08:52:12, Dev, Step : 1100, Loss : 0.52767, Acc : 0.753, Auc : 0.831, Sensitive_Loss : 0.35217, Sensitive_Acc : 16.193, Sensitive_Auc : 0.978, Mean auc: 0.831, Run Time : 93.80 sec
INFO:root:2024-04-10 08:52:13, Best, Step : 1100, Loss : 0.52767, Acc : 0.753, Auc : 0.831, Sensitive_Loss : 0.35217, Sensitive_Acc : 16.193, Sensitive_Auc : 0.978, Best Auc : 0.831
INFO:root:2024-04-10 08:52:19, Train, Epoch : 2, Step : 1110, Loss : 0.52498, Acc : 0.766, Sensitive_Loss : 0.19868, Sensitive_Acc : 15.700, Run Time : 100.84 sec
INFO:root:2024-04-10 08:52:28, Train, Epoch : 2, Step : 1120, Loss : 0.53696, Acc : 0.753, Sensitive_Loss : 0.20117, Sensitive_Acc : 15.200, Run Time : 8.87 sec
INFO:root:2024-04-10 08:52:38, Train, Epoch : 2, Step : 1130, Loss : 0.52369, Acc : 0.756, Sensitive_Loss : 0.17987, Sensitive_Acc : 18.600, Run Time : 9.59 sec
INFO:root:2024-04-10 08:52:46, Train, Epoch : 2, Step : 1140, Loss : 0.51529, Acc : 0.766, Sensitive_Loss : 0.25936, Sensitive_Acc : 14.200, Run Time : 8.48 sec
INFO:root:2024-04-10 08:52:55, Train, Epoch : 2, Step : 1150, Loss : 0.52302, Acc : 0.756, Sensitive_Loss : 0.15212, Sensitive_Acc : 16.100, Run Time : 8.69 sec
INFO:root:2024-04-10 08:53:04, Train, Epoch : 2, Step : 1160, Loss : 0.53104, Acc : 0.719, Sensitive_Loss : 0.17800, Sensitive_Acc : 16.900, Run Time : 8.86 sec
INFO:root:2024-04-10 08:53:13, Train, Epoch : 2, Step : 1170, Loss : 0.43882, Acc : 0.778, Sensitive_Loss : 0.17430, Sensitive_Acc : 17.300, Run Time : 9.28 sec
INFO:root:2024-04-10 08:53:22, Train, Epoch : 2, Step : 1180, Loss : 0.56317, Acc : 0.756, Sensitive_Loss : 0.19107, Sensitive_Acc : 17.200, Run Time : 9.15 sec
INFO:root:2024-04-10 08:53:31, Train, Epoch : 2, Step : 1190, Loss : 0.51113, Acc : 0.744, Sensitive_Loss : 0.21037, Sensitive_Acc : 15.700, Run Time : 8.76 sec
INFO:root:2024-04-10 08:53:40, Train, Epoch : 2, Step : 1200, Loss : 0.56368, Acc : 0.725, Sensitive_Loss : 0.15292, Sensitive_Acc : 16.800, Run Time : 8.75 sec
INFO:root:2024-04-10 08:55:13, Dev, Step : 1200, Loss : 0.53866, Acc : 0.748, Auc : 0.831, Sensitive_Loss : 0.17199, Sensitive_Acc : 16.164, Sensitive_Auc : 0.976, Mean auc: 0.831, Run Time : 93.75 sec
INFO:root:2024-04-10 08:55:14, Best, Step : 1200, Loss : 0.53866, Acc : 0.748, Auc : 0.831, Sensitive_Loss : 0.17199, Sensitive_Acc : 16.164, Sensitive_Auc : 0.976, Best Auc : 0.831
INFO:root:2024-04-10 08:55:20, Train, Epoch : 2, Step : 1210, Loss : 0.50626, Acc : 0.762, Sensitive_Loss : 0.16553, Sensitive_Acc : 16.600, Run Time : 100.64 sec
INFO:root:2024-04-10 08:55:30, Train, Epoch : 2, Step : 1220, Loss : 0.54120, Acc : 0.725, Sensitive_Loss : 0.20303, Sensitive_Acc : 16.500, Run Time : 9.55 sec
INFO:root:2024-04-10 08:55:39, Train, Epoch : 2, Step : 1230, Loss : 0.52182, Acc : 0.725, Sensitive_Loss : 0.21013, Sensitive_Acc : 16.900, Run Time : 9.29 sec
INFO:root:2024-04-10 08:55:48, Train, Epoch : 2, Step : 1240, Loss : 0.47214, Acc : 0.769, Sensitive_Loss : 0.19035, Sensitive_Acc : 17.000, Run Time : 9.08 sec
INFO:root:2024-04-10 08:55:57, Train, Epoch : 2, Step : 1250, Loss : 0.40457, Acc : 0.784, Sensitive_Loss : 0.20550, Sensitive_Acc : 15.200, Run Time : 8.94 sec
INFO:root:2024-04-10 08:56:06, Train, Epoch : 2, Step : 1260, Loss : 0.49252, Acc : 0.750, Sensitive_Loss : 0.20179, Sensitive_Acc : 17.700, Run Time : 9.03 sec
INFO:root:2024-04-10 08:56:15, Train, Epoch : 2, Step : 1270, Loss : 0.53190, Acc : 0.747, Sensitive_Loss : 0.17896, Sensitive_Acc : 17.900, Run Time : 8.59 sec
INFO:root:2024-04-10 08:56:23, Train, Epoch : 2, Step : 1280, Loss : 0.58911, Acc : 0.734, Sensitive_Loss : 0.14293, Sensitive_Acc : 16.000, Run Time : 8.47 sec
INFO:root:2024-04-10 08:58:03
INFO:root:y_pred: [0.45489463 0.34036657 0.42242524 ... 0.41761953 0.44518206 0.40940866]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.93824840e-01 6.13611948e-04 7.00836301e-01 9.98270273e-01
 9.98938620e-01 8.45557809e-01 9.86410558e-01 5.94471116e-04
 9.92315114e-01 9.82573450e-01 5.88163257e-01 1.54358715e-01
 2.45679199e-04 7.85376012e-01 9.98980105e-01 9.97000873e-01
 9.60919380e-01 7.70777404e-01 9.96270657e-01 9.57843423e-01
 9.96747732e-01 4.02357757e-01 9.92047846e-01 7.77223527e-01
 6.80951118e-01 1.46154715e-02 9.90394056e-01 6.15170449e-02
 9.86149788e-01 9.07018431e-04 1.80730631e-03 1.83052391e-01
 5.52715249e-02 9.95733917e-01 1.52123375e-05 9.69246566e-01
 9.68167791e-04 9.99595106e-01 9.87941306e-03 9.40113485e-01
 9.93216217e-01 6.95664203e-04 2.81253420e-02 3.38568119e-04
 1.78114802e-01 2.15136558e-01 9.18522537e-01 9.02689159e-01
 8.56340408e-01 9.12279904e-01 1.29166851e-03 6.80955052e-01
 8.85188486e-03 9.91839111e-01 9.57547784e-01 6.44595828e-03
 9.16569293e-01 9.86280560e-01 9.84675050e-01 3.92731366e-04
 1.55322468e-02 9.54071760e-01 3.10017169e-01 9.96341407e-01
 8.81448507e-01 1.70292351e-02 6.08873665e-01 1.27450585e-01
 9.94716942e-01 9.54212904e-01 3.59450234e-04 2.49327809e-01
 8.23942184e-01 9.50561345e-01 9.82532620e-01 1.07635558e-03
 5.35621606e-02 5.18359616e-03 5.39958361e-04 9.99933004e-01
 3.13106418e-01 9.87839639e-01 9.97587442e-01 9.92798805e-01
 3.86193059e-02 9.99861956e-01 1.40830828e-03 7.68922046e-02
 9.95918334e-01 9.85829890e-01 3.50632034e-02 3.60955983e-01
 3.90299372e-02 7.45793402e-01 1.53965592e-01 9.99633312e-01
 4.70043067e-03 9.78784263e-01 7.55078793e-01 4.17720107e-03
 1.32116384e-03 5.88015281e-02 9.79960084e-01 9.99611557e-01
 4.42352206e-01 8.78991783e-01 9.94552553e-01 8.10013991e-03
 5.47424704e-02 9.46979105e-01 8.57615087e-05 2.04541240e-04
 2.51477063e-01 9.97883260e-01 9.22857761e-01 1.22349123e-02
 8.84607732e-01 1.53141888e-02 9.98192251e-01 1.38544112e-01
 9.95886743e-01 9.99538541e-01 8.79742354e-02 6.68291032e-01
 4.30328041e-01 2.07886356e-03 1.68387797e-02 2.65178969e-04
 9.36403394e-01 9.94736731e-01 9.80572426e-04 1.20557705e-03
 7.08406151e-04 1.19697288e-01 9.98672128e-01 9.77425873e-01
 8.75695884e-01 9.19859782e-02 3.08209341e-02 8.50153744e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 08:58:03, Dev, Step : 1288, Loss : 0.56195, Acc : 0.744, Auc : 0.825, Sensitive_Loss : 0.17824, Sensitive_Acc : 16.279, Sensitive_Auc : 0.973, Mean auc: 0.825, Run Time : 92.91 sec
INFO:root:2024-04-10 08:58:07, Train, Epoch : 3, Step : 1290, Loss : 0.09372, Acc : 0.166, Sensitive_Loss : 0.02213, Sensitive_Acc : 2.700, Run Time : 2.62 sec
INFO:root:2024-04-10 08:58:15, Train, Epoch : 3, Step : 1300, Loss : 0.45478, Acc : 0.769, Sensitive_Loss : 0.14868, Sensitive_Acc : 15.900, Run Time : 8.34 sec
INFO:root:2024-04-10 08:59:47, Dev, Step : 1300, Loss : 0.55052, Acc : 0.748, Auc : 0.829, Sensitive_Loss : 0.19072, Sensitive_Acc : 16.264, Sensitive_Auc : 0.974, Mean auc: 0.829, Run Time : 92.26 sec
INFO:root:2024-04-10 08:59:53, Train, Epoch : 3, Step : 1310, Loss : 0.50533, Acc : 0.734, Sensitive_Loss : 0.17903, Sensitive_Acc : 16.800, Run Time : 98.35 sec
INFO:root:2024-04-10 09:00:02, Train, Epoch : 3, Step : 1320, Loss : 0.47964, Acc : 0.759, Sensitive_Loss : 0.18469, Sensitive_Acc : 15.000, Run Time : 8.42 sec
INFO:root:2024-04-10 09:00:10, Train, Epoch : 3, Step : 1330, Loss : 0.51525, Acc : 0.775, Sensitive_Loss : 0.14829, Sensitive_Acc : 16.500, Run Time : 8.57 sec
INFO:root:2024-04-10 09:00:19, Train, Epoch : 3, Step : 1340, Loss : 0.44681, Acc : 0.816, Sensitive_Loss : 0.18236, Sensitive_Acc : 16.300, Run Time : 8.60 sec
INFO:root:2024-04-10 09:00:28, Train, Epoch : 3, Step : 1350, Loss : 0.44026, Acc : 0.800, Sensitive_Loss : 0.18519, Sensitive_Acc : 17.000, Run Time : 8.77 sec
INFO:root:2024-04-10 09:00:37, Train, Epoch : 3, Step : 1360, Loss : 0.48555, Acc : 0.775, Sensitive_Loss : 0.16568, Sensitive_Acc : 16.800, Run Time : 8.68 sec
INFO:root:2024-04-10 09:00:45, Train, Epoch : 3, Step : 1370, Loss : 0.38820, Acc : 0.797, Sensitive_Loss : 0.15233, Sensitive_Acc : 17.500, Run Time : 8.28 sec
INFO:root:2024-04-10 09:00:53, Train, Epoch : 3, Step : 1380, Loss : 0.43844, Acc : 0.809, Sensitive_Loss : 0.10787, Sensitive_Acc : 16.400, Run Time : 8.68 sec
INFO:root:2024-04-10 09:01:03, Train, Epoch : 3, Step : 1390, Loss : 0.40084, Acc : 0.800, Sensitive_Loss : 0.16698, Sensitive_Acc : 17.000, Run Time : 9.28 sec
INFO:root:2024-04-10 09:01:11, Train, Epoch : 3, Step : 1400, Loss : 0.38447, Acc : 0.809, Sensitive_Loss : 0.13911, Sensitive_Acc : 14.800, Run Time : 8.63 sec
INFO:root:2024-04-10 09:02:46, Dev, Step : 1400, Loss : 0.52466, Acc : 0.755, Auc : 0.847, Sensitive_Loss : 0.21551, Sensitive_Acc : 16.207, Sensitive_Auc : 0.980, Mean auc: 0.847, Run Time : 94.24 sec
INFO:root:2024-04-10 09:02:46, Best, Step : 1400, Loss : 0.52466, Acc : 0.755, Auc : 0.847, Sensitive_Loss : 0.21551, Sensitive_Acc : 16.207, Sensitive_Auc : 0.980, Best Auc : 0.847
INFO:root:2024-04-10 09:02:52, Train, Epoch : 3, Step : 1410, Loss : 0.41927, Acc : 0.831, Sensitive_Loss : 0.14794, Sensitive_Acc : 15.800, Run Time : 101.04 sec
INFO:root:2024-04-10 09:03:02, Train, Epoch : 3, Step : 1420, Loss : 0.46560, Acc : 0.778, Sensitive_Loss : 0.20532, Sensitive_Acc : 18.400, Run Time : 9.10 sec
INFO:root:2024-04-10 09:03:11, Train, Epoch : 3, Step : 1430, Loss : 0.43243, Acc : 0.806, Sensitive_Loss : 0.15362, Sensitive_Acc : 15.800, Run Time : 9.62 sec
INFO:root:2024-04-10 09:03:21, Train, Epoch : 3, Step : 1440, Loss : 0.42667, Acc : 0.787, Sensitive_Loss : 0.15897, Sensitive_Acc : 17.700, Run Time : 9.94 sec
INFO:root:2024-04-10 09:03:30, Train, Epoch : 3, Step : 1450, Loss : 0.38626, Acc : 0.812, Sensitive_Loss : 0.18358, Sensitive_Acc : 14.800, Run Time : 8.58 sec
INFO:root:2024-04-10 09:03:39, Train, Epoch : 3, Step : 1460, Loss : 0.40688, Acc : 0.784, Sensitive_Loss : 0.14677, Sensitive_Acc : 15.900, Run Time : 8.91 sec
INFO:root:2024-04-10 09:03:48, Train, Epoch : 3, Step : 1470, Loss : 0.40681, Acc : 0.797, Sensitive_Loss : 0.17684, Sensitive_Acc : 15.600, Run Time : 9.20 sec
INFO:root:2024-04-10 09:03:58, Train, Epoch : 3, Step : 1480, Loss : 0.47762, Acc : 0.778, Sensitive_Loss : 0.14566, Sensitive_Acc : 15.600, Run Time : 9.98 sec
INFO:root:2024-04-10 09:04:07, Train, Epoch : 3, Step : 1490, Loss : 0.43881, Acc : 0.803, Sensitive_Loss : 0.20020, Sensitive_Acc : 16.500, Run Time : 9.32 sec
INFO:root:2024-04-10 09:04:16, Train, Epoch : 3, Step : 1500, Loss : 0.45912, Acc : 0.816, Sensitive_Loss : 0.13837, Sensitive_Acc : 16.200, Run Time : 8.65 sec
INFO:root:2024-04-10 09:05:49, Dev, Step : 1500, Loss : 0.51642, Acc : 0.771, Auc : 0.851, Sensitive_Loss : 0.20276, Sensitive_Acc : 16.193, Sensitive_Auc : 0.981, Mean auc: 0.851, Run Time : 93.65 sec
INFO:root:2024-04-10 09:05:50, Best, Step : 1500, Loss : 0.51642, Acc : 0.771, Auc : 0.851, Sensitive_Loss : 0.20276, Sensitive_Acc : 16.193, Sensitive_Auc : 0.981, Best Auc : 0.851
INFO:root:2024-04-10 09:05:57, Train, Epoch : 3, Step : 1510, Loss : 0.41029, Acc : 0.806, Sensitive_Loss : 0.18498, Sensitive_Acc : 16.000, Run Time : 100.78 sec
INFO:root:2024-04-10 09:06:06, Train, Epoch : 3, Step : 1520, Loss : 0.46759, Acc : 0.772, Sensitive_Loss : 0.15283, Sensitive_Acc : 15.700, Run Time : 9.53 sec
INFO:root:2024-04-10 09:06:15, Train, Epoch : 3, Step : 1530, Loss : 0.48966, Acc : 0.794, Sensitive_Loss : 0.16488, Sensitive_Acc : 16.800, Run Time : 9.39 sec
INFO:root:2024-04-10 09:06:25, Train, Epoch : 3, Step : 1540, Loss : 0.46072, Acc : 0.791, Sensitive_Loss : 0.14601, Sensitive_Acc : 15.600, Run Time : 9.62 sec
INFO:root:2024-04-10 09:06:34, Train, Epoch : 3, Step : 1550, Loss : 0.40846, Acc : 0.809, Sensitive_Loss : 0.15552, Sensitive_Acc : 14.900, Run Time : 8.77 sec
INFO:root:2024-04-10 09:06:43, Train, Epoch : 3, Step : 1560, Loss : 0.39171, Acc : 0.812, Sensitive_Loss : 0.16464, Sensitive_Acc : 16.600, Run Time : 9.49 sec
INFO:root:2024-04-10 09:06:52, Train, Epoch : 3, Step : 1570, Loss : 0.44193, Acc : 0.822, Sensitive_Loss : 0.18014, Sensitive_Acc : 17.600, Run Time : 8.90 sec
INFO:root:2024-04-10 09:07:01, Train, Epoch : 3, Step : 1580, Loss : 0.46610, Acc : 0.747, Sensitive_Loss : 0.15345, Sensitive_Acc : 15.400, Run Time : 8.69 sec
INFO:root:2024-04-10 09:07:10, Train, Epoch : 3, Step : 1590, Loss : 0.42263, Acc : 0.797, Sensitive_Loss : 0.18310, Sensitive_Acc : 15.800, Run Time : 9.33 sec
INFO:root:2024-04-10 09:07:19, Train, Epoch : 3, Step : 1600, Loss : 0.42049, Acc : 0.806, Sensitive_Loss : 0.18265, Sensitive_Acc : 16.900, Run Time : 9.19 sec
INFO:root:2024-04-10 09:08:53, Dev, Step : 1600, Loss : 0.51570, Acc : 0.770, Auc : 0.852, Sensitive_Loss : 0.19590, Sensitive_Acc : 16.179, Sensitive_Auc : 0.982, Mean auc: 0.852, Run Time : 93.73 sec
INFO:root:2024-04-10 09:08:54, Best, Step : 1600, Loss : 0.51570, Acc : 0.770, Auc : 0.852, Sensitive_Loss : 0.19590, Sensitive_Acc : 16.179, Sensitive_Auc : 0.982, Best Auc : 0.852
INFO:root:2024-04-10 09:09:00, Train, Epoch : 3, Step : 1610, Loss : 0.38394, Acc : 0.831, Sensitive_Loss : 0.12829, Sensitive_Acc : 15.800, Run Time : 100.58 sec
INFO:root:2024-04-10 09:09:10, Train, Epoch : 3, Step : 1620, Loss : 0.45184, Acc : 0.791, Sensitive_Loss : 0.11998, Sensitive_Acc : 17.200, Run Time : 9.54 sec
INFO:root:2024-04-10 09:09:18, Train, Epoch : 3, Step : 1630, Loss : 0.42515, Acc : 0.819, Sensitive_Loss : 0.13010, Sensitive_Acc : 17.300, Run Time : 8.70 sec
INFO:root:2024-04-10 09:09:28, Train, Epoch : 3, Step : 1640, Loss : 0.41314, Acc : 0.816, Sensitive_Loss : 0.15594, Sensitive_Acc : 16.300, Run Time : 10.22 sec
INFO:root:2024-04-10 09:09:38, Train, Epoch : 3, Step : 1650, Loss : 0.43009, Acc : 0.787, Sensitive_Loss : 0.21762, Sensitive_Acc : 14.900, Run Time : 9.14 sec
INFO:root:2024-04-10 09:09:47, Train, Epoch : 3, Step : 1660, Loss : 0.42908, Acc : 0.787, Sensitive_Loss : 0.20646, Sensitive_Acc : 16.300, Run Time : 9.15 sec
INFO:root:2024-04-10 09:09:55, Train, Epoch : 3, Step : 1670, Loss : 0.39742, Acc : 0.797, Sensitive_Loss : 0.22725, Sensitive_Acc : 16.000, Run Time : 8.58 sec
INFO:root:2024-04-10 09:10:04, Train, Epoch : 3, Step : 1680, Loss : 0.43976, Acc : 0.781, Sensitive_Loss : 0.13941, Sensitive_Acc : 16.600, Run Time : 8.91 sec
INFO:root:2024-04-10 09:10:13, Train, Epoch : 3, Step : 1690, Loss : 0.39474, Acc : 0.800, Sensitive_Loss : 0.16084, Sensitive_Acc : 17.100, Run Time : 8.64 sec
INFO:root:2024-04-10 09:10:22, Train, Epoch : 3, Step : 1700, Loss : 0.35479, Acc : 0.809, Sensitive_Loss : 0.20840, Sensitive_Acc : 15.000, Run Time : 8.98 sec
INFO:root:2024-04-10 09:11:56, Dev, Step : 1700, Loss : 0.51043, Acc : 0.772, Auc : 0.854, Sensitive_Loss : 0.19994, Sensitive_Acc : 16.164, Sensitive_Auc : 0.981, Mean auc: 0.854, Run Time : 93.89 sec
INFO:root:2024-04-10 09:11:56, Best, Step : 1700, Loss : 0.51043, Acc : 0.772, Auc : 0.854, Sensitive_Loss : 0.19994, Sensitive_Acc : 16.164, Sensitive_Auc : 0.981, Best Auc : 0.854
INFO:root:2024-04-10 09:12:03, Train, Epoch : 3, Step : 1710, Loss : 0.52467, Acc : 0.787, Sensitive_Loss : 0.20510, Sensitive_Acc : 16.600, Run Time : 100.84 sec
INFO:root:2024-04-10 09:12:12, Train, Epoch : 3, Step : 1720, Loss : 0.42740, Acc : 0.806, Sensitive_Loss : 0.14431, Sensitive_Acc : 16.000, Run Time : 8.89 sec
INFO:root:2024-04-10 09:12:22, Train, Epoch : 3, Step : 1730, Loss : 0.43137, Acc : 0.819, Sensitive_Loss : 0.15064, Sensitive_Acc : 17.900, Run Time : 10.04 sec
INFO:root:2024-04-10 09:12:31, Train, Epoch : 3, Step : 1740, Loss : 0.38584, Acc : 0.803, Sensitive_Loss : 0.09491, Sensitive_Acc : 16.300, Run Time : 8.94 sec
INFO:root:2024-04-10 09:12:39, Train, Epoch : 3, Step : 1750, Loss : 0.45649, Acc : 0.812, Sensitive_Loss : 0.11631, Sensitive_Acc : 17.300, Run Time : 8.86 sec
INFO:root:2024-04-10 09:12:48, Train, Epoch : 3, Step : 1760, Loss : 0.44942, Acc : 0.825, Sensitive_Loss : 0.14723, Sensitive_Acc : 15.000, Run Time : 8.25 sec
INFO:root:2024-04-10 09:12:57, Train, Epoch : 3, Step : 1770, Loss : 0.43820, Acc : 0.791, Sensitive_Loss : 0.15121, Sensitive_Acc : 16.200, Run Time : 9.31 sec
INFO:root:2024-04-10 09:13:06, Train, Epoch : 3, Step : 1780, Loss : 0.44010, Acc : 0.794, Sensitive_Loss : 0.17564, Sensitive_Acc : 15.400, Run Time : 9.03 sec
INFO:root:2024-04-10 09:13:15, Train, Epoch : 3, Step : 1790, Loss : 0.41703, Acc : 0.806, Sensitive_Loss : 0.14420, Sensitive_Acc : 17.500, Run Time : 8.84 sec
INFO:root:2024-04-10 09:13:23, Train, Epoch : 3, Step : 1800, Loss : 0.47295, Acc : 0.800, Sensitive_Loss : 0.19297, Sensitive_Acc : 16.800, Run Time : 8.30 sec
INFO:root:2024-04-10 09:14:57, Dev, Step : 1800, Loss : 0.53108, Acc : 0.762, Auc : 0.853, Sensitive_Loss : 0.19712, Sensitive_Acc : 16.164, Sensitive_Auc : 0.982, Mean auc: 0.853, Run Time : 94.02 sec
INFO:root:2024-04-10 09:15:04, Train, Epoch : 3, Step : 1810, Loss : 0.47603, Acc : 0.781, Sensitive_Loss : 0.13429, Sensitive_Acc : 17.100, Run Time : 100.80 sec
INFO:root:2024-04-10 09:15:13, Train, Epoch : 3, Step : 1820, Loss : 0.45893, Acc : 0.791, Sensitive_Loss : 0.18208, Sensitive_Acc : 16.400, Run Time : 9.25 sec
INFO:root:2024-04-10 09:15:22, Train, Epoch : 3, Step : 1830, Loss : 0.38221, Acc : 0.828, Sensitive_Loss : 0.13862, Sensitive_Acc : 16.300, Run Time : 8.94 sec
INFO:root:2024-04-10 09:15:32, Train, Epoch : 3, Step : 1840, Loss : 0.40217, Acc : 0.841, Sensitive_Loss : 0.12908, Sensitive_Acc : 16.600, Run Time : 9.32 sec
INFO:root:2024-04-10 09:15:41, Train, Epoch : 3, Step : 1850, Loss : 0.46732, Acc : 0.791, Sensitive_Loss : 0.13427, Sensitive_Acc : 16.000, Run Time : 9.26 sec
INFO:root:2024-04-10 09:15:50, Train, Epoch : 3, Step : 1860, Loss : 0.34353, Acc : 0.822, Sensitive_Loss : 0.10642, Sensitive_Acc : 16.000, Run Time : 8.76 sec
INFO:root:2024-04-10 09:15:59, Train, Epoch : 3, Step : 1870, Loss : 0.46620, Acc : 0.791, Sensitive_Loss : 0.19494, Sensitive_Acc : 15.900, Run Time : 9.27 sec
INFO:root:2024-04-10 09:16:08, Train, Epoch : 3, Step : 1880, Loss : 0.43376, Acc : 0.800, Sensitive_Loss : 0.14549, Sensitive_Acc : 16.200, Run Time : 8.81 sec
INFO:root:2024-04-10 09:16:19, Train, Epoch : 3, Step : 1890, Loss : 0.41984, Acc : 0.794, Sensitive_Loss : 0.12663, Sensitive_Acc : 16.400, Run Time : 11.09 sec
INFO:root:2024-04-10 09:16:28, Train, Epoch : 3, Step : 1900, Loss : 0.42876, Acc : 0.812, Sensitive_Loss : 0.13925, Sensitive_Acc : 15.600, Run Time : 9.43 sec
INFO:root:2024-04-10 09:18:03, Dev, Step : 1900, Loss : 0.51191, Acc : 0.776, Auc : 0.854, Sensitive_Loss : 0.17242, Sensitive_Acc : 16.121, Sensitive_Auc : 0.982, Mean auc: 0.854, Run Time : 94.83 sec
INFO:root:2024-04-10 09:18:10, Train, Epoch : 3, Step : 1910, Loss : 0.35392, Acc : 0.834, Sensitive_Loss : 0.12410, Sensitive_Acc : 17.500, Run Time : 101.39 sec
INFO:root:2024-04-10 09:18:18, Train, Epoch : 3, Step : 1920, Loss : 0.43734, Acc : 0.834, Sensitive_Loss : 0.17172, Sensitive_Acc : 15.500, Run Time : 8.96 sec
INFO:root:2024-04-10 09:18:28, Train, Epoch : 3, Step : 1930, Loss : 0.46161, Acc : 0.784, Sensitive_Loss : 0.12851, Sensitive_Acc : 16.000, Run Time : 9.50 sec
INFO:root:2024-04-10 09:20:02
INFO:root:y_pred: [0.22153409 0.18030159 0.74534625 ... 0.64751524 0.6837194  0.33880743]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.98138189e-01 5.12529723e-03 6.28133595e-01 9.98853683e-01
 9.99763787e-01 9.48953986e-01 9.96886671e-01 7.19944539e-04
 9.88320470e-01 9.96673226e-01 8.77163887e-01 5.42014122e-01
 7.53250497e-04 9.67890501e-01 9.99270380e-01 9.99661565e-01
 9.90923464e-01 9.25832510e-01 9.98075604e-01 9.95387971e-01
 9.97523844e-01 3.77613693e-01 9.93619382e-01 9.25862849e-01
 9.34492171e-01 6.25862032e-02 9.88763690e-01 8.81720483e-02
 9.97609615e-01 1.38992928e-02 4.90124058e-03 2.11101115e-01
 5.01028970e-02 9.98777926e-01 6.46157423e-05 9.93277729e-01
 1.12352113e-03 9.99890924e-01 9.25914049e-02 9.93068576e-01
 9.98143792e-01 2.51650484e-03 9.67686400e-02 8.67880939e-04
 5.42139530e-01 4.73226786e-01 9.82901692e-01 9.80392158e-01
 8.90547574e-01 9.77688789e-01 6.08624192e-03 6.73398972e-01
 1.19167510e-02 9.16301370e-01 9.70642507e-01 4.18458879e-02
 9.69938397e-01 9.91887391e-01 9.95404959e-01 8.95812921e-03
 1.81129836e-02 9.91928697e-01 6.60012782e-01 9.99041855e-01
 9.61067915e-01 1.52756304e-01 8.84566784e-01 3.52266073e-01
 9.98723447e-01 9.85529900e-01 5.84434997e-03 8.23662996e-01
 9.14456546e-01 9.89109278e-01 9.94092762e-01 2.84482725e-03
 4.71939087e-01 8.05494003e-03 2.02468294e-03 9.98643100e-01
 3.63004655e-01 9.90297139e-01 9.99542713e-01 9.98736918e-01
 1.71400651e-01 9.99936581e-01 4.15514689e-03 1.08618237e-01
 9.97168601e-01 9.91295815e-01 5.63927218e-02 6.09291732e-01
 1.67894587e-01 8.40029120e-01 4.21672553e-01 9.99931335e-01
 2.43514664e-02 9.87408042e-01 9.59075749e-01 5.66628296e-03
 2.55813543e-03 2.01550707e-01 9.94909942e-01 9.99854684e-01
 8.47642541e-01 8.69160354e-01 9.98598158e-01 8.43398795e-02
 2.77076393e-01 9.81087506e-01 3.55742872e-04 4.64128423e-03
 6.47343338e-01 9.98056412e-01 9.88815904e-01 8.54930002e-03
 9.63911235e-01 1.25960885e-02 9.99405861e-01 2.31669754e-01
 9.95820403e-01 9.99680519e-01 2.03787595e-01 8.07952166e-01
 6.24725759e-01 9.58939642e-03 1.05997704e-01 8.27771379e-04
 9.67770457e-01 9.96332645e-01 1.51419668e-02 1.37197529e-03
 3.96746956e-03 5.70801318e-01 9.98769224e-01 9.95034218e-01
 9.30688083e-01 5.73135138e-01 1.27013862e-01 9.75657463e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 09:20:02, Dev, Step : 1932, Loss : 0.53492, Acc : 0.761, Auc : 0.854, Sensitive_Loss : 0.18548, Sensitive_Acc : 16.193, Sensitive_Auc : 0.982, Mean auc: 0.854, Run Time : 92.21 sec
INFO:root:2024-04-10 09:20:10, Train, Epoch : 4, Step : 1940, Loss : 0.36270, Acc : 0.641, Sensitive_Loss : 0.07272, Sensitive_Acc : 12.700, Run Time : 7.39 sec
INFO:root:2024-04-10 09:20:19, Train, Epoch : 4, Step : 1950, Loss : 0.44865, Acc : 0.781, Sensitive_Loss : 0.15699, Sensitive_Acc : 17.200, Run Time : 8.86 sec
INFO:root:2024-04-10 09:20:28, Train, Epoch : 4, Step : 1960, Loss : 0.42330, Acc : 0.791, Sensitive_Loss : 0.12342, Sensitive_Acc : 16.100, Run Time : 8.53 sec
INFO:root:2024-04-10 09:20:36, Train, Epoch : 4, Step : 1970, Loss : 0.38654, Acc : 0.825, Sensitive_Loss : 0.17699, Sensitive_Acc : 15.200, Run Time : 8.32 sec
INFO:root:2024-04-10 09:20:46, Train, Epoch : 4, Step : 1980, Loss : 0.38891, Acc : 0.809, Sensitive_Loss : 0.13189, Sensitive_Acc : 15.300, Run Time : 9.71 sec
INFO:root:2024-04-10 09:20:54, Train, Epoch : 4, Step : 1990, Loss : 0.42905, Acc : 0.816, Sensitive_Loss : 0.16783, Sensitive_Acc : 16.400, Run Time : 8.32 sec
INFO:root:2024-04-10 09:21:03, Train, Epoch : 4, Step : 2000, Loss : 0.33417, Acc : 0.859, Sensitive_Loss : 0.16830, Sensitive_Acc : 16.900, Run Time : 8.51 sec
INFO:root:2024-04-10 09:22:36, Dev, Step : 2000, Loss : 0.55041, Acc : 0.755, Auc : 0.856, Sensitive_Loss : 0.21503, Sensitive_Acc : 16.107, Sensitive_Auc : 0.980, Mean auc: 0.856, Run Time : 93.89 sec
INFO:root:2024-04-10 09:22:37, Best, Step : 2000, Loss : 0.55041, Acc : 0.755, Auc : 0.856, Sensitive_Loss : 0.21503, Sensitive_Acc : 16.107, Sensitive_Auc : 0.980, Best Auc : 0.856
INFO:root:2024-04-10 09:22:44, Train, Epoch : 4, Step : 2010, Loss : 0.39518, Acc : 0.844, Sensitive_Loss : 0.13584, Sensitive_Acc : 16.100, Run Time : 101.37 sec
INFO:root:2024-04-10 09:22:53, Train, Epoch : 4, Step : 2020, Loss : 0.43536, Acc : 0.794, Sensitive_Loss : 0.14639, Sensitive_Acc : 17.000, Run Time : 9.01 sec
INFO:root:2024-04-10 09:23:02, Train, Epoch : 4, Step : 2030, Loss : 0.41082, Acc : 0.847, Sensitive_Loss : 0.17371, Sensitive_Acc : 15.300, Run Time : 9.14 sec
INFO:root:2024-04-10 09:23:11, Train, Epoch : 4, Step : 2040, Loss : 0.37107, Acc : 0.816, Sensitive_Loss : 0.18703, Sensitive_Acc : 15.900, Run Time : 8.96 sec
INFO:root:2024-04-10 09:23:20, Train, Epoch : 4, Step : 2050, Loss : 0.42567, Acc : 0.791, Sensitive_Loss : 0.21810, Sensitive_Acc : 16.200, Run Time : 8.93 sec
INFO:root:2024-04-10 09:23:29, Train, Epoch : 4, Step : 2060, Loss : 0.38603, Acc : 0.831, Sensitive_Loss : 0.22901, Sensitive_Acc : 16.100, Run Time : 9.36 sec
INFO:root:2024-04-10 09:23:39, Train, Epoch : 4, Step : 2070, Loss : 0.37601, Acc : 0.831, Sensitive_Loss : 0.12469, Sensitive_Acc : 15.300, Run Time : 9.20 sec
INFO:root:2024-04-10 09:23:47, Train, Epoch : 4, Step : 2080, Loss : 0.41621, Acc : 0.797, Sensitive_Loss : 0.12982, Sensitive_Acc : 15.900, Run Time : 8.62 sec
INFO:root:2024-04-10 09:23:57, Train, Epoch : 4, Step : 2090, Loss : 0.45835, Acc : 0.781, Sensitive_Loss : 0.14824, Sensitive_Acc : 16.400, Run Time : 9.92 sec
INFO:root:2024-04-10 09:24:06, Train, Epoch : 4, Step : 2100, Loss : 0.37455, Acc : 0.838, Sensitive_Loss : 0.17994, Sensitive_Acc : 15.000, Run Time : 9.25 sec
INFO:root:2024-04-10 09:25:43, Dev, Step : 2100, Loss : 0.53306, Acc : 0.765, Auc : 0.854, Sensitive_Loss : 0.19874, Sensitive_Acc : 16.336, Sensitive_Auc : 0.982, Mean auc: 0.854, Run Time : 96.33 sec
INFO:root:2024-04-10 09:25:50, Train, Epoch : 4, Step : 2110, Loss : 0.41930, Acc : 0.806, Sensitive_Loss : 0.15420, Sensitive_Acc : 16.400, Run Time : 103.86 sec
INFO:root:2024-04-10 09:26:00, Train, Epoch : 4, Step : 2120, Loss : 0.30860, Acc : 0.869, Sensitive_Loss : 0.11702, Sensitive_Acc : 17.200, Run Time : 10.01 sec
INFO:root:2024-04-10 09:26:10, Train, Epoch : 4, Step : 2130, Loss : 0.38799, Acc : 0.800, Sensitive_Loss : 0.10525, Sensitive_Acc : 17.700, Run Time : 9.45 sec
INFO:root:2024-04-10 09:26:20, Train, Epoch : 4, Step : 2140, Loss : 0.44929, Acc : 0.819, Sensitive_Loss : 0.11435, Sensitive_Acc : 16.200, Run Time : 10.24 sec
INFO:root:2024-04-10 09:26:30, Train, Epoch : 4, Step : 2150, Loss : 0.42686, Acc : 0.828, Sensitive_Loss : 0.09758, Sensitive_Acc : 15.200, Run Time : 10.00 sec
INFO:root:2024-04-10 09:26:40, Train, Epoch : 4, Step : 2160, Loss : 0.37165, Acc : 0.822, Sensitive_Loss : 0.12854, Sensitive_Acc : 17.900, Run Time : 9.77 sec
INFO:root:2024-04-10 09:26:49, Train, Epoch : 4, Step : 2170, Loss : 0.40038, Acc : 0.812, Sensitive_Loss : 0.12722, Sensitive_Acc : 17.000, Run Time : 9.76 sec
INFO:root:2024-04-10 09:27:01, Train, Epoch : 4, Step : 2180, Loss : 0.38736, Acc : 0.838, Sensitive_Loss : 0.11426, Sensitive_Acc : 17.500, Run Time : 12.00 sec
INFO:root:2024-04-10 09:27:11, Train, Epoch : 4, Step : 2190, Loss : 0.35207, Acc : 0.844, Sensitive_Loss : 0.15117, Sensitive_Acc : 16.200, Run Time : 9.87 sec
INFO:root:2024-04-10 09:27:21, Train, Epoch : 4, Step : 2200, Loss : 0.38635, Acc : 0.831, Sensitive_Loss : 0.12234, Sensitive_Acc : 16.200, Run Time : 9.53 sec
INFO:root:2024-04-10 09:28:59, Dev, Step : 2200, Loss : 0.50586, Acc : 0.777, Auc : 0.857, Sensitive_Loss : 0.16885, Sensitive_Acc : 16.150, Sensitive_Auc : 0.983, Mean auc: 0.857, Run Time : 98.22 sec
INFO:root:2024-04-10 09:29:00, Best, Step : 2200, Loss : 0.50586, Acc : 0.777, Auc : 0.857, Sensitive_Loss : 0.16885, Sensitive_Acc : 16.150, Sensitive_Auc : 0.983, Best Auc : 0.857
INFO:root:2024-04-10 09:29:07, Train, Epoch : 4, Step : 2210, Loss : 0.32703, Acc : 0.859, Sensitive_Loss : 0.14769, Sensitive_Acc : 16.600, Run Time : 105.91 sec
INFO:root:2024-04-10 09:29:16, Train, Epoch : 4, Step : 2220, Loss : 0.40787, Acc : 0.838, Sensitive_Loss : 0.14112, Sensitive_Acc : 15.100, Run Time : 9.63 sec
INFO:root:2024-04-10 09:29:26, Train, Epoch : 4, Step : 2230, Loss : 0.39261, Acc : 0.841, Sensitive_Loss : 0.18255, Sensitive_Acc : 17.100, Run Time : 9.50 sec
INFO:root:2024-04-10 09:29:35, Train, Epoch : 4, Step : 2240, Loss : 0.45281, Acc : 0.803, Sensitive_Loss : 0.18468, Sensitive_Acc : 15.500, Run Time : 9.17 sec
INFO:root:2024-04-10 09:29:45, Train, Epoch : 4, Step : 2250, Loss : 0.36873, Acc : 0.819, Sensitive_Loss : 0.15769, Sensitive_Acc : 14.500, Run Time : 9.76 sec
INFO:root:2024-04-10 09:29:54, Train, Epoch : 4, Step : 2260, Loss : 0.35445, Acc : 0.816, Sensitive_Loss : 0.15797, Sensitive_Acc : 15.800, Run Time : 9.04 sec
INFO:root:2024-04-10 09:30:03, Train, Epoch : 4, Step : 2270, Loss : 0.37701, Acc : 0.822, Sensitive_Loss : 0.12297, Sensitive_Acc : 17.200, Run Time : 9.21 sec
INFO:root:2024-04-10 09:30:12, Train, Epoch : 4, Step : 2280, Loss : 0.36249, Acc : 0.844, Sensitive_Loss : 0.13207, Sensitive_Acc : 15.200, Run Time : 9.42 sec
INFO:root:2024-04-10 09:30:22, Train, Epoch : 4, Step : 2290, Loss : 0.48183, Acc : 0.766, Sensitive_Loss : 0.14026, Sensitive_Acc : 15.300, Run Time : 9.24 sec
INFO:root:2024-04-10 09:30:31, Train, Epoch : 4, Step : 2300, Loss : 0.33799, Acc : 0.872, Sensitive_Loss : 0.19402, Sensitive_Acc : 17.000, Run Time : 9.19 sec
INFO:root:2024-04-10 09:32:05, Dev, Step : 2300, Loss : 0.51908, Acc : 0.774, Auc : 0.858, Sensitive_Loss : 0.17411, Sensitive_Acc : 16.079, Sensitive_Auc : 0.982, Mean auc: 0.858, Run Time : 93.94 sec
INFO:root:2024-04-10 09:32:05, Best, Step : 2300, Loss : 0.51908, Acc : 0.774, Auc : 0.858, Sensitive_Loss : 0.17411, Sensitive_Acc : 16.079, Sensitive_Auc : 0.982, Best Auc : 0.858
INFO:root:2024-04-10 09:32:12, Train, Epoch : 4, Step : 2310, Loss : 0.41659, Acc : 0.825, Sensitive_Loss : 0.11830, Sensitive_Acc : 16.600, Run Time : 101.05 sec
INFO:root:2024-04-10 09:32:21, Train, Epoch : 4, Step : 2320, Loss : 0.47507, Acc : 0.797, Sensitive_Loss : 0.12863, Sensitive_Acc : 15.700, Run Time : 9.09 sec
INFO:root:2024-04-10 09:32:32, Train, Epoch : 4, Step : 2330, Loss : 0.40051, Acc : 0.816, Sensitive_Loss : 0.14975, Sensitive_Acc : 16.900, Run Time : 10.77 sec
INFO:root:2024-04-10 09:32:40, Train, Epoch : 4, Step : 2340, Loss : 0.43191, Acc : 0.766, Sensitive_Loss : 0.15323, Sensitive_Acc : 14.800, Run Time : 8.67 sec
INFO:root:2024-04-10 09:32:49, Train, Epoch : 4, Step : 2350, Loss : 0.42283, Acc : 0.834, Sensitive_Loss : 0.11966, Sensitive_Acc : 17.100, Run Time : 8.65 sec
INFO:root:2024-04-10 09:32:58, Train, Epoch : 4, Step : 2360, Loss : 0.31019, Acc : 0.825, Sensitive_Loss : 0.13159, Sensitive_Acc : 16.800, Run Time : 9.22 sec
INFO:root:2024-04-10 09:33:07, Train, Epoch : 4, Step : 2370, Loss : 0.44185, Acc : 0.803, Sensitive_Loss : 0.14831, Sensitive_Acc : 15.600, Run Time : 9.05 sec
INFO:root:2024-04-10 09:33:16, Train, Epoch : 4, Step : 2380, Loss : 0.44574, Acc : 0.806, Sensitive_Loss : 0.13914, Sensitive_Acc : 14.700, Run Time : 8.84 sec
INFO:root:2024-04-10 09:33:25, Train, Epoch : 4, Step : 2390, Loss : 0.48636, Acc : 0.794, Sensitive_Loss : 0.10747, Sensitive_Acc : 16.000, Run Time : 9.28 sec
INFO:root:2024-04-10 09:33:34, Train, Epoch : 4, Step : 2400, Loss : 0.39867, Acc : 0.828, Sensitive_Loss : 0.16592, Sensitive_Acc : 16.100, Run Time : 9.02 sec
INFO:root:2024-04-10 09:35:08, Dev, Step : 2400, Loss : 0.52984, Acc : 0.769, Auc : 0.856, Sensitive_Loss : 0.17143, Sensitive_Acc : 16.236, Sensitive_Auc : 0.982, Mean auc: 0.856, Run Time : 93.71 sec
INFO:root:2024-04-10 09:35:15, Train, Epoch : 4, Step : 2410, Loss : 0.45495, Acc : 0.753, Sensitive_Loss : 0.14610, Sensitive_Acc : 17.900, Run Time : 100.28 sec
INFO:root:2024-04-10 09:35:25, Train, Epoch : 4, Step : 2420, Loss : 0.38175, Acc : 0.831, Sensitive_Loss : 0.12728, Sensitive_Acc : 17.900, Run Time : 9.91 sec
INFO:root:2024-04-10 09:35:33, Train, Epoch : 4, Step : 2430, Loss : 0.40006, Acc : 0.809, Sensitive_Loss : 0.12228, Sensitive_Acc : 16.600, Run Time : 8.72 sec
INFO:root:2024-04-10 09:35:43, Train, Epoch : 4, Step : 2440, Loss : 0.39311, Acc : 0.838, Sensitive_Loss : 0.12785, Sensitive_Acc : 16.300, Run Time : 9.33 sec
INFO:root:2024-04-10 09:35:52, Train, Epoch : 4, Step : 2450, Loss : 0.42029, Acc : 0.819, Sensitive_Loss : 0.17790, Sensitive_Acc : 16.300, Run Time : 9.50 sec
INFO:root:2024-04-10 09:36:01, Train, Epoch : 4, Step : 2460, Loss : 0.35085, Acc : 0.844, Sensitive_Loss : 0.13682, Sensitive_Acc : 17.600, Run Time : 8.97 sec
INFO:root:2024-04-10 09:36:10, Train, Epoch : 4, Step : 2470, Loss : 0.41985, Acc : 0.822, Sensitive_Loss : 0.14342, Sensitive_Acc : 16.900, Run Time : 8.69 sec
INFO:root:2024-04-10 09:36:19, Train, Epoch : 4, Step : 2480, Loss : 0.40525, Acc : 0.812, Sensitive_Loss : 0.18090, Sensitive_Acc : 17.700, Run Time : 9.25 sec
INFO:root:2024-04-10 09:36:29, Train, Epoch : 4, Step : 2490, Loss : 0.43122, Acc : 0.797, Sensitive_Loss : 0.14637, Sensitive_Acc : 16.800, Run Time : 10.20 sec
INFO:root:2024-04-10 09:36:38, Train, Epoch : 4, Step : 2500, Loss : 0.43766, Acc : 0.803, Sensitive_Loss : 0.12901, Sensitive_Acc : 16.000, Run Time : 8.85 sec
INFO:root:2024-04-10 09:38:13, Dev, Step : 2500, Loss : 0.51146, Acc : 0.775, Auc : 0.858, Sensitive_Loss : 0.20140, Sensitive_Acc : 16.164, Sensitive_Auc : 0.984, Mean auc: 0.858, Run Time : 94.49 sec
INFO:root:2024-04-10 09:38:19, Train, Epoch : 4, Step : 2510, Loss : 0.34861, Acc : 0.787, Sensitive_Loss : 0.13541, Sensitive_Acc : 17.200, Run Time : 100.93 sec
INFO:root:2024-04-10 09:38:28, Train, Epoch : 4, Step : 2520, Loss : 0.34182, Acc : 0.841, Sensitive_Loss : 0.12119, Sensitive_Acc : 17.100, Run Time : 9.09 sec
INFO:root:2024-04-10 09:38:37, Train, Epoch : 4, Step : 2530, Loss : 0.36436, Acc : 0.853, Sensitive_Loss : 0.16144, Sensitive_Acc : 16.600, Run Time : 9.03 sec
INFO:root:2024-04-10 09:38:49, Train, Epoch : 4, Step : 2540, Loss : 0.41765, Acc : 0.812, Sensitive_Loss : 0.13845, Sensitive_Acc : 17.300, Run Time : 12.20 sec
INFO:root:2024-04-10 09:38:59, Train, Epoch : 4, Step : 2550, Loss : 0.44719, Acc : 0.794, Sensitive_Loss : 0.15066, Sensitive_Acc : 15.400, Run Time : 9.22 sec
INFO:root:2024-04-10 09:39:08, Train, Epoch : 4, Step : 2560, Loss : 0.42102, Acc : 0.772, Sensitive_Loss : 0.15907, Sensitive_Acc : 17.900, Run Time : 8.97 sec
INFO:root:2024-04-10 09:39:18, Train, Epoch : 4, Step : 2570, Loss : 0.44505, Acc : 0.819, Sensitive_Loss : 0.19589, Sensitive_Acc : 16.200, Run Time : 10.30 sec
INFO:root:2024-04-10 09:40:56
INFO:root:y_pred: [0.2429255  0.14661126 0.89095604 ... 0.77815896 0.8572277  0.39723817]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9901640e-01 9.3638489e-04 5.9622103e-01 9.9946755e-01 9.9972397e-01
 9.6240699e-01 9.9817204e-01 6.0578558e-04 9.9481010e-01 9.9617553e-01
 7.8654760e-01 3.4753075e-01 1.8825817e-04 9.6805352e-01 9.9959248e-01
 9.9978548e-01 9.9276465e-01 9.3285024e-01 9.9569726e-01 9.9196351e-01
 9.9906474e-01 2.9926682e-01 9.9342960e-01 9.0772009e-01 9.2810488e-01
 3.1568717e-02 9.8467708e-01 3.6040835e-02 9.9886984e-01 6.6834618e-03
 2.4059468e-03 1.2166707e-01 5.7152219e-02 9.9783993e-01 1.8507482e-05
 9.9436945e-01 4.0869246e-04 9.9987328e-01 4.6398614e-02 9.9142796e-01
 9.9905735e-01 2.9332054e-04 2.7692080e-02 4.1418147e-04 3.1116208e-01
 2.7357954e-01 9.9011528e-01 9.7601545e-01 8.0367804e-01 9.7163421e-01
 1.4998135e-03 5.5979449e-01 1.3283783e-02 7.5426358e-01 9.8370206e-01
 3.8900573e-02 9.7340381e-01 9.9186969e-01 9.9610525e-01 1.0071880e-02
 5.4080975e-03 9.9257189e-01 4.3769571e-01 9.9941671e-01 9.6136099e-01
 4.3038938e-02 8.8085753e-01 4.5367491e-01 9.9931562e-01 9.8372054e-01
 3.9795944e-03 7.5884557e-01 9.1264492e-01 9.8907101e-01 9.8985755e-01
 1.3885273e-03 5.4911649e-01 2.2560658e-03 5.9916201e-04 9.9968088e-01
 2.3418082e-01 9.9339491e-01 9.9930155e-01 9.9877316e-01 7.3382452e-02
 9.9996352e-01 6.4386695e-04 4.1654222e-02 9.9715483e-01 9.8925811e-01
 4.5917064e-02 3.4864360e-01 1.1759439e-01 7.0571911e-01 3.0446059e-01
 9.9992430e-01 6.8913675e-03 9.9205983e-01 9.5332134e-01 4.9988679e-03
 7.0743397e-04 1.4582585e-01 9.9483901e-01 9.9980909e-01 8.6629802e-01
 8.5254425e-01 9.9829572e-01 3.9253574e-02 1.3677879e-01 9.7848791e-01
 6.2362524e-05 1.8297117e-03 3.9308918e-01 9.9928051e-01 9.9033326e-01
 1.9508796e-03 9.6744931e-01 3.7553317e-03 9.9932992e-01 1.3899344e-01
 9.9635327e-01 9.9963105e-01 2.4800059e-01 7.8181976e-01 3.7614608e-01
 7.8935977e-03 5.3350892e-02 2.4974038e-04 9.4158614e-01 9.9754888e-01
 1.8853305e-02 3.8401983e-04 1.7682962e-03 5.4206073e-01 9.9860221e-01
 9.9404222e-01 8.8382190e-01 5.1152122e-01 8.5790761e-02 9.8047161e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 09:40:56, Dev, Step : 2576, Loss : 0.50547, Acc : 0.780, Auc : 0.858, Sensitive_Loss : 0.15333, Sensitive_Acc : 16.279, Sensitive_Auc : 0.984, Mean auc: 0.858, Run Time : 92.76 sec
INFO:root:2024-04-10 09:41:02, Train, Epoch : 5, Step : 2580, Loss : 0.13389, Acc : 0.322, Sensitive_Loss : 0.03747, Sensitive_Acc : 6.900, Run Time : 4.25 sec
INFO:root:2024-04-10 09:41:11, Train, Epoch : 5, Step : 2590, Loss : 0.30739, Acc : 0.866, Sensitive_Loss : 0.15526, Sensitive_Acc : 15.800, Run Time : 8.95 sec
INFO:root:2024-04-10 09:41:20, Train, Epoch : 5, Step : 2600, Loss : 0.33720, Acc : 0.863, Sensitive_Loss : 0.12764, Sensitive_Acc : 17.700, Run Time : 8.20 sec
INFO:root:2024-04-10 09:42:53, Dev, Step : 2600, Loss : 0.51876, Acc : 0.777, Auc : 0.861, Sensitive_Loss : 0.17072, Sensitive_Acc : 16.336, Sensitive_Auc : 0.983, Mean auc: 0.861, Run Time : 93.11 sec
INFO:root:2024-04-10 09:42:53, Best, Step : 2600, Loss : 0.51876, Acc : 0.777, Auc : 0.861, Sensitive_Loss : 0.17072, Sensitive_Acc : 16.336, Sensitive_Auc : 0.983, Best Auc : 0.861
INFO:root:2024-04-10 09:43:00, Train, Epoch : 5, Step : 2610, Loss : 0.37985, Acc : 0.847, Sensitive_Loss : 0.12807, Sensitive_Acc : 15.500, Run Time : 100.23 sec
INFO:root:2024-04-10 09:43:09, Train, Epoch : 5, Step : 2620, Loss : 0.43678, Acc : 0.812, Sensitive_Loss : 0.13893, Sensitive_Acc : 15.000, Run Time : 9.69 sec
INFO:root:2024-04-10 09:43:21, Train, Epoch : 5, Step : 2630, Loss : 0.44655, Acc : 0.825, Sensitive_Loss : 0.12963, Sensitive_Acc : 15.900, Run Time : 11.32 sec
INFO:root:2024-04-10 09:43:30, Train, Epoch : 5, Step : 2640, Loss : 0.41604, Acc : 0.791, Sensitive_Loss : 0.15776, Sensitive_Acc : 15.200, Run Time : 9.00 sec
INFO:root:2024-04-10 09:43:39, Train, Epoch : 5, Step : 2650, Loss : 0.37199, Acc : 0.850, Sensitive_Loss : 0.16560, Sensitive_Acc : 16.100, Run Time : 9.38 sec
INFO:root:2024-04-10 09:43:49, Train, Epoch : 5, Step : 2660, Loss : 0.38925, Acc : 0.803, Sensitive_Loss : 0.14696, Sensitive_Acc : 17.000, Run Time : 9.56 sec
INFO:root:2024-04-10 09:43:58, Train, Epoch : 5, Step : 2670, Loss : 0.37043, Acc : 0.838, Sensitive_Loss : 0.12381, Sensitive_Acc : 16.800, Run Time : 9.07 sec
INFO:root:2024-04-10 09:44:07, Train, Epoch : 5, Step : 2680, Loss : 0.33645, Acc : 0.863, Sensitive_Loss : 0.12962, Sensitive_Acc : 15.600, Run Time : 8.83 sec
INFO:root:2024-04-10 09:44:16, Train, Epoch : 5, Step : 2690, Loss : 0.38298, Acc : 0.844, Sensitive_Loss : 0.17506, Sensitive_Acc : 14.700, Run Time : 8.93 sec
INFO:root:2024-04-10 09:44:26, Train, Epoch : 5, Step : 2700, Loss : 0.33089, Acc : 0.850, Sensitive_Loss : 0.14021, Sensitive_Acc : 16.600, Run Time : 10.36 sec
INFO:root:2024-04-10 09:46:00, Dev, Step : 2700, Loss : 0.51863, Acc : 0.776, Auc : 0.860, Sensitive_Loss : 0.18530, Sensitive_Acc : 16.207, Sensitive_Auc : 0.980, Mean auc: 0.860, Run Time : 94.46 sec
INFO:root:2024-04-10 09:46:07, Train, Epoch : 5, Step : 2710, Loss : 0.40643, Acc : 0.819, Sensitive_Loss : 0.16698, Sensitive_Acc : 17.000, Run Time : 101.13 sec
INFO:root:2024-04-10 09:46:17, Train, Epoch : 5, Step : 2720, Loss : 0.41265, Acc : 0.825, Sensitive_Loss : 0.13860, Sensitive_Acc : 16.600, Run Time : 9.70 sec
INFO:root:2024-04-10 09:46:26, Train, Epoch : 5, Step : 2730, Loss : 0.36613, Acc : 0.831, Sensitive_Loss : 0.13112, Sensitive_Acc : 17.100, Run Time : 9.17 sec
INFO:root:2024-04-10 09:46:35, Train, Epoch : 5, Step : 2740, Loss : 0.33977, Acc : 0.841, Sensitive_Loss : 0.14281, Sensitive_Acc : 15.800, Run Time : 9.38 sec
INFO:root:2024-04-10 09:46:47, Train, Epoch : 5, Step : 2750, Loss : 0.38241, Acc : 0.791, Sensitive_Loss : 0.12611, Sensitive_Acc : 15.600, Run Time : 11.25 sec
INFO:root:2024-04-10 09:46:56, Train, Epoch : 5, Step : 2760, Loss : 0.38672, Acc : 0.863, Sensitive_Loss : 0.14307, Sensitive_Acc : 15.900, Run Time : 9.10 sec
INFO:root:2024-04-10 09:47:05, Train, Epoch : 5, Step : 2770, Loss : 0.32809, Acc : 0.859, Sensitive_Loss : 0.14393, Sensitive_Acc : 14.500, Run Time : 9.18 sec
INFO:root:2024-04-10 09:47:14, Train, Epoch : 5, Step : 2780, Loss : 0.38754, Acc : 0.816, Sensitive_Loss : 0.17381, Sensitive_Acc : 17.700, Run Time : 9.58 sec
INFO:root:2024-04-10 09:47:23, Train, Epoch : 5, Step : 2790, Loss : 0.42095, Acc : 0.812, Sensitive_Loss : 0.12812, Sensitive_Acc : 15.900, Run Time : 8.69 sec
INFO:root:2024-04-10 09:47:32, Train, Epoch : 5, Step : 2800, Loss : 0.39896, Acc : 0.844, Sensitive_Loss : 0.13579, Sensitive_Acc : 15.900, Run Time : 9.03 sec
INFO:root:2024-04-10 09:49:05, Dev, Step : 2800, Loss : 0.50513, Acc : 0.784, Auc : 0.861, Sensitive_Loss : 0.16839, Sensitive_Acc : 16.279, Sensitive_Auc : 0.983, Mean auc: 0.861, Run Time : 92.97 sec
INFO:root:2024-04-10 09:49:06, Best, Step : 2800, Loss : 0.50513, Acc : 0.784, Auc : 0.861, Sensitive_Loss : 0.16839, Sensitive_Acc : 16.279, Sensitive_Auc : 0.983, Best Auc : 0.861
INFO:root:2024-04-10 09:49:12, Train, Epoch : 5, Step : 2810, Loss : 0.35569, Acc : 0.847, Sensitive_Loss : 0.11737, Sensitive_Acc : 17.600, Run Time : 99.75 sec
INFO:root:2024-04-10 09:49:21, Train, Epoch : 5, Step : 2820, Loss : 0.42648, Acc : 0.828, Sensitive_Loss : 0.15388, Sensitive_Acc : 16.900, Run Time : 8.89 sec
INFO:root:2024-04-10 09:49:30, Train, Epoch : 5, Step : 2830, Loss : 0.36775, Acc : 0.847, Sensitive_Loss : 0.13098, Sensitive_Acc : 15.600, Run Time : 9.31 sec
INFO:root:2024-04-10 09:49:40, Train, Epoch : 5, Step : 2840, Loss : 0.36961, Acc : 0.819, Sensitive_Loss : 0.14888, Sensitive_Acc : 16.300, Run Time : 9.46 sec
INFO:root:2024-04-10 09:49:49, Train, Epoch : 5, Step : 2850, Loss : 0.33130, Acc : 0.828, Sensitive_Loss : 0.16273, Sensitive_Acc : 17.100, Run Time : 9.25 sec
INFO:root:2024-04-10 09:49:58, Train, Epoch : 5, Step : 2860, Loss : 0.41004, Acc : 0.809, Sensitive_Loss : 0.12562, Sensitive_Acc : 16.500, Run Time : 8.81 sec
INFO:root:2024-04-10 09:50:07, Train, Epoch : 5, Step : 2870, Loss : 0.34351, Acc : 0.844, Sensitive_Loss : 0.09606, Sensitive_Acc : 16.600, Run Time : 9.38 sec
INFO:root:2024-04-10 09:50:16, Train, Epoch : 5, Step : 2880, Loss : 0.33267, Acc : 0.872, Sensitive_Loss : 0.17337, Sensitive_Acc : 16.400, Run Time : 8.92 sec
INFO:root:2024-04-10 09:50:24, Train, Epoch : 5, Step : 2890, Loss : 0.39423, Acc : 0.841, Sensitive_Loss : 0.11359, Sensitive_Acc : 15.900, Run Time : 8.51 sec
INFO:root:2024-04-10 09:50:33, Train, Epoch : 5, Step : 2900, Loss : 0.43844, Acc : 0.794, Sensitive_Loss : 0.14375, Sensitive_Acc : 16.500, Run Time : 8.94 sec
INFO:root:2024-04-10 09:52:07, Dev, Step : 2900, Loss : 0.51427, Acc : 0.784, Auc : 0.859, Sensitive_Loss : 0.15028, Sensitive_Acc : 16.279, Sensitive_Auc : 0.983, Mean auc: 0.859, Run Time : 93.83 sec
INFO:root:2024-04-10 09:52:14, Train, Epoch : 5, Step : 2910, Loss : 0.41402, Acc : 0.816, Sensitive_Loss : 0.14447, Sensitive_Acc : 15.800, Run Time : 100.74 sec
INFO:root:2024-04-10 09:52:24, Train, Epoch : 5, Step : 2920, Loss : 0.36730, Acc : 0.822, Sensitive_Loss : 0.11244, Sensitive_Acc : 15.500, Run Time : 9.87 sec
INFO:root:2024-04-10 09:52:33, Train, Epoch : 5, Step : 2930, Loss : 0.37557, Acc : 0.838, Sensitive_Loss : 0.17880, Sensitive_Acc : 15.400, Run Time : 9.49 sec
INFO:root:2024-04-10 09:52:42, Train, Epoch : 5, Step : 2940, Loss : 0.36928, Acc : 0.819, Sensitive_Loss : 0.15842, Sensitive_Acc : 15.600, Run Time : 8.91 sec
INFO:root:2024-04-10 09:52:51, Train, Epoch : 5, Step : 2950, Loss : 0.33046, Acc : 0.847, Sensitive_Loss : 0.14354, Sensitive_Acc : 17.000, Run Time : 9.06 sec
INFO:root:2024-04-10 09:53:00, Train, Epoch : 5, Step : 2960, Loss : 0.36714, Acc : 0.822, Sensitive_Loss : 0.15683, Sensitive_Acc : 17.500, Run Time : 8.77 sec
INFO:root:2024-04-10 09:53:09, Train, Epoch : 5, Step : 2970, Loss : 0.39915, Acc : 0.831, Sensitive_Loss : 0.17163, Sensitive_Acc : 17.900, Run Time : 8.45 sec
INFO:root:2024-04-10 09:53:18, Train, Epoch : 5, Step : 2980, Loss : 0.40957, Acc : 0.809, Sensitive_Loss : 0.15638, Sensitive_Acc : 15.600, Run Time : 8.98 sec
INFO:root:2024-04-10 09:53:27, Train, Epoch : 5, Step : 2990, Loss : 0.36428, Acc : 0.838, Sensitive_Loss : 0.13694, Sensitive_Acc : 15.800, Run Time : 8.96 sec
INFO:root:2024-04-10 09:53:35, Train, Epoch : 5, Step : 3000, Loss : 0.39102, Acc : 0.819, Sensitive_Loss : 0.12406, Sensitive_Acc : 17.500, Run Time : 8.80 sec
INFO:root:2024-04-10 09:55:09, Dev, Step : 3000, Loss : 0.53737, Acc : 0.774, Auc : 0.860, Sensitive_Loss : 0.19807, Sensitive_Acc : 16.264, Sensitive_Auc : 0.984, Mean auc: 0.860, Run Time : 93.97 sec
INFO:root:2024-04-10 09:55:16, Train, Epoch : 5, Step : 3010, Loss : 0.39146, Acc : 0.812, Sensitive_Loss : 0.12146, Sensitive_Acc : 15.600, Run Time : 100.38 sec
INFO:root:2024-04-10 09:55:25, Train, Epoch : 5, Step : 3020, Loss : 0.34682, Acc : 0.853, Sensitive_Loss : 0.14711, Sensitive_Acc : 15.700, Run Time : 9.11 sec
INFO:root:2024-04-10 09:55:34, Train, Epoch : 5, Step : 3030, Loss : 0.39894, Acc : 0.797, Sensitive_Loss : 0.12069, Sensitive_Acc : 14.300, Run Time : 8.92 sec
INFO:root:2024-04-10 09:55:44, Train, Epoch : 5, Step : 3040, Loss : 0.38412, Acc : 0.816, Sensitive_Loss : 0.10843, Sensitive_Acc : 17.400, Run Time : 10.03 sec
INFO:root:2024-04-10 09:55:53, Train, Epoch : 5, Step : 3050, Loss : 0.39195, Acc : 0.819, Sensitive_Loss : 0.16354, Sensitive_Acc : 15.800, Run Time : 9.03 sec
INFO:root:2024-04-10 09:56:02, Train, Epoch : 5, Step : 3060, Loss : 0.34374, Acc : 0.853, Sensitive_Loss : 0.14311, Sensitive_Acc : 17.000, Run Time : 8.90 sec
INFO:root:2024-04-10 09:56:10, Train, Epoch : 5, Step : 3070, Loss : 0.34907, Acc : 0.825, Sensitive_Loss : 0.16417, Sensitive_Acc : 17.200, Run Time : 8.66 sec
INFO:root:2024-04-10 09:56:20, Train, Epoch : 5, Step : 3080, Loss : 0.39505, Acc : 0.841, Sensitive_Loss : 0.12594, Sensitive_Acc : 15.100, Run Time : 9.77 sec
INFO:root:2024-04-10 09:56:29, Train, Epoch : 5, Step : 3090, Loss : 0.42055, Acc : 0.809, Sensitive_Loss : 0.14275, Sensitive_Acc : 17.100, Run Time : 8.92 sec
INFO:root:2024-04-10 09:56:38, Train, Epoch : 5, Step : 3100, Loss : 0.37021, Acc : 0.831, Sensitive_Loss : 0.10204, Sensitive_Acc : 16.400, Run Time : 8.83 sec
INFO:root:2024-04-10 09:58:12, Dev, Step : 3100, Loss : 0.52832, Acc : 0.772, Auc : 0.862, Sensitive_Loss : 0.17763, Sensitive_Acc : 16.250, Sensitive_Auc : 0.985, Mean auc: 0.862, Run Time : 93.65 sec
INFO:root:2024-04-10 09:58:12, Best, Step : 3100, Loss : 0.52832, Acc : 0.772, Auc : 0.862, Sensitive_Loss : 0.17763, Sensitive_Acc : 16.250, Sensitive_Auc : 0.985, Best Auc : 0.862
INFO:root:2024-04-10 09:58:19, Train, Epoch : 5, Step : 3110, Loss : 0.38937, Acc : 0.797, Sensitive_Loss : 0.17964, Sensitive_Acc : 17.800, Run Time : 100.58 sec
INFO:root:2024-04-10 09:58:28, Train, Epoch : 5, Step : 3120, Loss : 0.36414, Acc : 0.809, Sensitive_Loss : 0.12222, Sensitive_Acc : 16.600, Run Time : 9.22 sec
INFO:root:2024-04-10 09:58:38, Train, Epoch : 5, Step : 3130, Loss : 0.39981, Acc : 0.822, Sensitive_Loss : 0.13253, Sensitive_Acc : 14.900, Run Time : 9.92 sec
INFO:root:2024-04-10 09:58:47, Train, Epoch : 5, Step : 3140, Loss : 0.37334, Acc : 0.856, Sensitive_Loss : 0.11818, Sensitive_Acc : 15.300, Run Time : 9.12 sec
INFO:root:2024-04-10 09:58:56, Train, Epoch : 5, Step : 3150, Loss : 0.37957, Acc : 0.841, Sensitive_Loss : 0.12662, Sensitive_Acc : 16.700, Run Time : 9.14 sec
INFO:root:2024-04-10 09:59:06, Train, Epoch : 5, Step : 3160, Loss : 0.33988, Acc : 0.841, Sensitive_Loss : 0.13045, Sensitive_Acc : 16.500, Run Time : 10.26 sec
INFO:root:2024-04-10 09:59:15, Train, Epoch : 5, Step : 3170, Loss : 0.40564, Acc : 0.809, Sensitive_Loss : 0.13081, Sensitive_Acc : 17.100, Run Time : 9.12 sec
INFO:root:2024-04-10 09:59:24, Train, Epoch : 5, Step : 3180, Loss : 0.33250, Acc : 0.844, Sensitive_Loss : 0.11134, Sensitive_Acc : 15.400, Run Time : 8.84 sec
INFO:root:2024-04-10 09:59:33, Train, Epoch : 5, Step : 3190, Loss : 0.36144, Acc : 0.822, Sensitive_Loss : 0.09574, Sensitive_Acc : 16.800, Run Time : 8.77 sec
INFO:root:2024-04-10 09:59:42, Train, Epoch : 5, Step : 3200, Loss : 0.30557, Acc : 0.853, Sensitive_Loss : 0.14073, Sensitive_Acc : 15.600, Run Time : 9.17 sec
INFO:root:2024-04-10 10:01:17, Dev, Step : 3200, Loss : 0.52030, Acc : 0.778, Auc : 0.859, Sensitive_Loss : 0.17879, Sensitive_Acc : 16.279, Sensitive_Auc : 0.983, Mean auc: 0.859, Run Time : 94.71 sec
INFO:root:2024-04-10 10:01:23, Train, Epoch : 5, Step : 3210, Loss : 0.35573, Acc : 0.841, Sensitive_Loss : 0.11580, Sensitive_Acc : 15.000, Run Time : 101.26 sec
INFO:root:2024-04-10 10:01:32, Train, Epoch : 5, Step : 3220, Loss : 0.35758, Acc : 0.850, Sensitive_Loss : 0.11673, Sensitive_Acc : 15.500, Run Time : 8.81 sec
INFO:root:2024-04-10 10:03:04
INFO:root:y_pred: [0.4545388  0.06763385 0.92849565 ... 0.7106273  0.71197903 0.36609486]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.99604642e-01 6.58348785e-04 6.84797227e-01 9.99662519e-01
 9.99590695e-01 9.81113195e-01 9.98957396e-01 7.45822792e-04
 9.95268285e-01 9.98621702e-01 8.84886503e-01 5.64363956e-01
 4.81916766e-04 9.74024892e-01 9.99825060e-01 9.99879241e-01
 9.91161466e-01 9.56664562e-01 9.97106969e-01 9.96827304e-01
 9.99397516e-01 6.32065594e-01 9.96180058e-01 9.53462183e-01
 9.81125832e-01 5.50216325e-02 9.91191924e-01 7.41206780e-02
 9.99336183e-01 1.45488288e-02 3.94409150e-03 1.17405713e-01
 4.65854965e-02 9.99156594e-01 5.71057462e-05 9.97283220e-01
 6.52832445e-04 9.99915838e-01 9.41188559e-02 9.97021973e-01
 9.99315977e-01 6.77478616e-04 3.70692164e-02 7.74301530e-04
 4.44120944e-01 4.78099197e-01 9.93570328e-01 9.87365067e-01
 9.45601761e-01 9.89880979e-01 3.49010620e-03 7.37142265e-01
 5.25077730e-02 8.39585543e-01 9.91097867e-01 8.71602595e-02
 9.76398408e-01 9.97384965e-01 9.98612165e-01 8.63235164e-03
 7.51720928e-03 9.95958507e-01 7.42712021e-01 9.99596179e-01
 9.77742791e-01 9.42646489e-02 9.50949311e-01 5.76872587e-01
 9.99519587e-01 9.91786063e-01 5.54502662e-03 8.64509404e-01
 9.61584568e-01 9.97295320e-01 9.96084213e-01 5.27721457e-03
 7.64177859e-01 6.63671037e-03 2.29578977e-03 9.99729812e-01
 3.44064742e-01 9.96634781e-01 9.99669671e-01 9.99531031e-01
 2.65434414e-01 9.99982238e-01 1.63421524e-03 1.01375304e-01
 9.98486102e-01 9.96077120e-01 7.63491020e-02 5.98774910e-01
 1.47463694e-01 8.70184958e-01 3.82337660e-01 9.99958038e-01
 2.28016153e-02 9.94374752e-01 9.81150031e-01 5.03960298e-03
 1.01543684e-03 3.13103616e-01 9.97772038e-01 9.99925137e-01
 9.41178977e-01 9.45990205e-01 9.99407053e-01 3.81825306e-02
 3.34171325e-01 9.86063957e-01 5.72268145e-05 5.53110335e-03
 6.22301936e-01 9.99685884e-01 9.91479993e-01 2.97070481e-03
 9.87296700e-01 8.74471851e-03 9.99578893e-01 2.31476307e-01
 9.97188866e-01 9.99686599e-01 4.68823910e-01 8.97066295e-01
 7.07931161e-01 1.71427559e-02 1.64853871e-01 5.35375206e-04
 9.78051722e-01 9.98903990e-01 2.31125932e-02 1.57784205e-03
 6.04498899e-03 6.60203934e-01 9.98849630e-01 9.96559560e-01
 9.65920269e-01 5.82998276e-01 1.41029865e-01 9.93181527e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 10:03:04, Dev, Step : 3220, Loss : 0.50541, Acc : 0.779, Auc : 0.860, Sensitive_Loss : 0.19509, Sensitive_Acc : 16.279, Sensitive_Auc : 0.983, Mean auc: 0.860, Run Time : 91.60 sec
INFO:root:2024-04-10 10:03:15, Train, Epoch : 6, Step : 3230, Loss : 0.32165, Acc : 0.844, Sensitive_Loss : 0.11778, Sensitive_Acc : 15.600, Run Time : 10.04 sec
INFO:root:2024-04-10 10:03:23, Train, Epoch : 6, Step : 3240, Loss : 0.29858, Acc : 0.838, Sensitive_Loss : 0.14528, Sensitive_Acc : 16.900, Run Time : 8.37 sec
INFO:root:2024-04-10 10:03:32, Train, Epoch : 6, Step : 3250, Loss : 0.39224, Acc : 0.856, Sensitive_Loss : 0.12764, Sensitive_Acc : 16.100, Run Time : 8.68 sec
INFO:root:2024-04-10 10:03:41, Train, Epoch : 6, Step : 3260, Loss : 0.34362, Acc : 0.834, Sensitive_Loss : 0.12348, Sensitive_Acc : 16.200, Run Time : 8.66 sec
INFO:root:2024-04-10 10:03:50, Train, Epoch : 6, Step : 3270, Loss : 0.37904, Acc : 0.850, Sensitive_Loss : 0.12586, Sensitive_Acc : 16.000, Run Time : 9.10 sec
INFO:root:2024-04-10 10:03:59, Train, Epoch : 6, Step : 3280, Loss : 0.41562, Acc : 0.831, Sensitive_Loss : 0.15186, Sensitive_Acc : 17.200, Run Time : 9.17 sec
INFO:root:2024-04-10 10:04:08, Train, Epoch : 6, Step : 3290, Loss : 0.31818, Acc : 0.850, Sensitive_Loss : 0.13063, Sensitive_Acc : 17.400, Run Time : 9.12 sec
INFO:root:2024-04-10 10:04:17, Train, Epoch : 6, Step : 3300, Loss : 0.33961, Acc : 0.853, Sensitive_Loss : 0.10856, Sensitive_Acc : 15.000, Run Time : 8.72 sec
INFO:root:2024-04-10 10:05:51, Dev, Step : 3300, Loss : 0.53588, Acc : 0.771, Auc : 0.858, Sensitive_Loss : 0.16753, Sensitive_Acc : 16.236, Sensitive_Auc : 0.981, Mean auc: 0.858, Run Time : 94.02 sec
INFO:root:2024-04-10 10:05:57, Train, Epoch : 6, Step : 3310, Loss : 0.27485, Acc : 0.878, Sensitive_Loss : 0.17328, Sensitive_Acc : 17.000, Run Time : 100.26 sec
INFO:root:2024-04-10 10:06:06, Train, Epoch : 6, Step : 3320, Loss : 0.37567, Acc : 0.834, Sensitive_Loss : 0.13438, Sensitive_Acc : 15.700, Run Time : 9.18 sec
INFO:root:2024-04-10 10:06:16, Train, Epoch : 6, Step : 3330, Loss : 0.29720, Acc : 0.863, Sensitive_Loss : 0.14594, Sensitive_Acc : 15.800, Run Time : 9.42 sec
INFO:root:2024-04-10 10:06:25, Train, Epoch : 6, Step : 3340, Loss : 0.36521, Acc : 0.828, Sensitive_Loss : 0.12680, Sensitive_Acc : 18.000, Run Time : 8.99 sec
INFO:root:2024-04-10 10:06:34, Train, Epoch : 6, Step : 3350, Loss : 0.33247, Acc : 0.853, Sensitive_Loss : 0.12945, Sensitive_Acc : 17.100, Run Time : 9.10 sec
INFO:root:2024-04-10 10:06:43, Train, Epoch : 6, Step : 3360, Loss : 0.32917, Acc : 0.828, Sensitive_Loss : 0.12324, Sensitive_Acc : 15.600, Run Time : 9.12 sec
INFO:root:2024-04-10 10:06:52, Train, Epoch : 6, Step : 3370, Loss : 0.42139, Acc : 0.800, Sensitive_Loss : 0.14136, Sensitive_Acc : 15.700, Run Time : 9.32 sec
INFO:root:2024-04-10 10:07:02, Train, Epoch : 6, Step : 3380, Loss : 0.34529, Acc : 0.841, Sensitive_Loss : 0.15289, Sensitive_Acc : 15.800, Run Time : 9.66 sec
INFO:root:2024-04-10 10:07:11, Train, Epoch : 6, Step : 3390, Loss : 0.41432, Acc : 0.797, Sensitive_Loss : 0.18269, Sensitive_Acc : 16.100, Run Time : 9.39 sec
INFO:root:2024-04-10 10:07:20, Train, Epoch : 6, Step : 3400, Loss : 0.36522, Acc : 0.841, Sensitive_Loss : 0.17847, Sensitive_Acc : 15.900, Run Time : 9.16 sec
INFO:root:2024-04-10 10:08:55, Dev, Step : 3400, Loss : 0.53919, Acc : 0.772, Auc : 0.859, Sensitive_Loss : 0.15466, Sensitive_Acc : 16.264, Sensitive_Auc : 0.984, Mean auc: 0.859, Run Time : 94.52 sec
INFO:root:2024-04-10 10:09:01, Train, Epoch : 6, Step : 3410, Loss : 0.34998, Acc : 0.850, Sensitive_Loss : 0.20150, Sensitive_Acc : 15.700, Run Time : 101.14 sec
INFO:root:2024-04-10 10:09:11, Train, Epoch : 6, Step : 3420, Loss : 0.37948, Acc : 0.828, Sensitive_Loss : 0.14973, Sensitive_Acc : 16.900, Run Time : 9.20 sec
INFO:root:2024-04-10 10:09:20, Train, Epoch : 6, Step : 3430, Loss : 0.33048, Acc : 0.863, Sensitive_Loss : 0.12304, Sensitive_Acc : 18.500, Run Time : 9.44 sec
INFO:root:2024-04-10 10:09:30, Train, Epoch : 6, Step : 3440, Loss : 0.36417, Acc : 0.819, Sensitive_Loss : 0.13485, Sensitive_Acc : 16.900, Run Time : 9.69 sec
INFO:root:2024-04-10 10:09:39, Train, Epoch : 6, Step : 3450, Loss : 0.35426, Acc : 0.853, Sensitive_Loss : 0.11695, Sensitive_Acc : 16.200, Run Time : 9.25 sec
INFO:root:2024-04-10 10:09:49, Train, Epoch : 6, Step : 3460, Loss : 0.35570, Acc : 0.819, Sensitive_Loss : 0.14887, Sensitive_Acc : 17.600, Run Time : 9.52 sec
INFO:root:2024-04-10 10:09:57, Train, Epoch : 6, Step : 3470, Loss : 0.38415, Acc : 0.806, Sensitive_Loss : 0.18383, Sensitive_Acc : 16.800, Run Time : 8.85 sec
INFO:root:2024-04-10 10:10:06, Train, Epoch : 6, Step : 3480, Loss : 0.32228, Acc : 0.844, Sensitive_Loss : 0.13046, Sensitive_Acc : 16.500, Run Time : 8.84 sec
INFO:root:2024-04-10 10:10:15, Train, Epoch : 6, Step : 3490, Loss : 0.35168, Acc : 0.834, Sensitive_Loss : 0.14481, Sensitive_Acc : 16.800, Run Time : 9.13 sec
INFO:root:2024-04-10 10:10:24, Train, Epoch : 6, Step : 3500, Loss : 0.31231, Acc : 0.853, Sensitive_Loss : 0.09638, Sensitive_Acc : 15.400, Run Time : 9.06 sec
INFO:root:2024-04-10 10:11:58, Dev, Step : 3500, Loss : 0.56847, Acc : 0.764, Auc : 0.858, Sensitive_Loss : 0.15404, Sensitive_Acc : 16.279, Sensitive_Auc : 0.985, Mean auc: 0.858, Run Time : 94.00 sec
INFO:root:2024-04-10 10:12:06, Train, Epoch : 6, Step : 3510, Loss : 0.32806, Acc : 0.850, Sensitive_Loss : 0.14344, Sensitive_Acc : 15.800, Run Time : 101.13 sec
INFO:root:2024-04-10 10:12:16, Train, Epoch : 6, Step : 3520, Loss : 0.34523, Acc : 0.863, Sensitive_Loss : 0.13698, Sensitive_Acc : 15.900, Run Time : 10.28 sec
INFO:root:2024-04-10 10:12:25, Train, Epoch : 6, Step : 3530, Loss : 0.40309, Acc : 0.803, Sensitive_Loss : 0.13913, Sensitive_Acc : 16.300, Run Time : 9.33 sec
INFO:root:2024-04-10 10:12:35, Train, Epoch : 6, Step : 3540, Loss : 0.39706, Acc : 0.831, Sensitive_Loss : 0.12886, Sensitive_Acc : 15.000, Run Time : 9.35 sec
INFO:root:2024-04-10 10:12:46, Train, Epoch : 6, Step : 3550, Loss : 0.36493, Acc : 0.838, Sensitive_Loss : 0.09733, Sensitive_Acc : 14.900, Run Time : 11.35 sec
INFO:root:2024-04-10 10:12:55, Train, Epoch : 6, Step : 3560, Loss : 0.32825, Acc : 0.834, Sensitive_Loss : 0.10252, Sensitive_Acc : 16.300, Run Time : 9.23 sec
INFO:root:2024-04-10 10:13:04, Train, Epoch : 6, Step : 3570, Loss : 0.34335, Acc : 0.850, Sensitive_Loss : 0.09911, Sensitive_Acc : 17.000, Run Time : 9.21 sec
INFO:root:2024-04-10 10:13:14, Train, Epoch : 6, Step : 3580, Loss : 0.28710, Acc : 0.872, Sensitive_Loss : 0.14498, Sensitive_Acc : 16.600, Run Time : 9.74 sec
INFO:root:2024-04-10 10:13:23, Train, Epoch : 6, Step : 3590, Loss : 0.34705, Acc : 0.803, Sensitive_Loss : 0.08820, Sensitive_Acc : 16.000, Run Time : 8.53 sec
INFO:root:2024-04-10 10:13:32, Train, Epoch : 6, Step : 3600, Loss : 0.28512, Acc : 0.859, Sensitive_Loss : 0.12615, Sensitive_Acc : 17.300, Run Time : 9.10 sec
INFO:root:2024-04-10 10:15:06, Dev, Step : 3600, Loss : 0.50522, Acc : 0.783, Auc : 0.862, Sensitive_Loss : 0.19448, Sensitive_Acc : 16.279, Sensitive_Auc : 0.986, Mean auc: 0.862, Run Time : 93.97 sec
INFO:root:2024-04-10 10:15:12, Train, Epoch : 6, Step : 3610, Loss : 0.38904, Acc : 0.834, Sensitive_Loss : 0.10606, Sensitive_Acc : 17.500, Run Time : 100.65 sec
INFO:root:2024-04-10 10:15:21, Train, Epoch : 6, Step : 3620, Loss : 0.33755, Acc : 0.859, Sensitive_Loss : 0.13998, Sensitive_Acc : 16.800, Run Time : 8.54 sec
INFO:root:2024-04-10 10:15:30, Train, Epoch : 6, Step : 3630, Loss : 0.35161, Acc : 0.850, Sensitive_Loss : 0.12267, Sensitive_Acc : 16.800, Run Time : 9.36 sec
INFO:root:2024-04-10 10:15:40, Train, Epoch : 6, Step : 3640, Loss : 0.35535, Acc : 0.838, Sensitive_Loss : 0.11228, Sensitive_Acc : 17.100, Run Time : 9.32 sec
INFO:root:2024-04-10 10:15:50, Train, Epoch : 6, Step : 3650, Loss : 0.43782, Acc : 0.834, Sensitive_Loss : 0.11787, Sensitive_Acc : 17.400, Run Time : 10.21 sec
INFO:root:2024-04-10 10:15:59, Train, Epoch : 6, Step : 3660, Loss : 0.38984, Acc : 0.841, Sensitive_Loss : 0.11200, Sensitive_Acc : 16.400, Run Time : 8.83 sec
INFO:root:2024-04-10 10:16:08, Train, Epoch : 6, Step : 3670, Loss : 0.40060, Acc : 0.812, Sensitive_Loss : 0.10785, Sensitive_Acc : 18.300, Run Time : 8.94 sec
INFO:root:2024-04-10 10:16:17, Train, Epoch : 6, Step : 3680, Loss : 0.37295, Acc : 0.850, Sensitive_Loss : 0.13910, Sensitive_Acc : 18.000, Run Time : 9.43 sec
INFO:root:2024-04-10 10:16:27, Train, Epoch : 6, Step : 3690, Loss : 0.37805, Acc : 0.856, Sensitive_Loss : 0.13505, Sensitive_Acc : 17.600, Run Time : 9.66 sec
INFO:root:2024-04-10 10:16:37, Train, Epoch : 6, Step : 3700, Loss : 0.33910, Acc : 0.853, Sensitive_Loss : 0.14300, Sensitive_Acc : 15.800, Run Time : 9.93 sec
INFO:root:2024-04-10 10:18:11, Dev, Step : 3700, Loss : 0.54214, Acc : 0.776, Auc : 0.859, Sensitive_Loss : 0.19620, Sensitive_Acc : 16.236, Sensitive_Auc : 0.984, Mean auc: 0.859, Run Time : 94.17 sec
INFO:root:2024-04-10 10:18:17, Train, Epoch : 6, Step : 3710, Loss : 0.32413, Acc : 0.853, Sensitive_Loss : 0.12199, Sensitive_Acc : 16.300, Run Time : 100.81 sec
INFO:root:2024-04-10 10:18:28, Train, Epoch : 6, Step : 3720, Loss : 0.33682, Acc : 0.834, Sensitive_Loss : 0.08446, Sensitive_Acc : 15.300, Run Time : 10.69 sec
INFO:root:2024-04-10 10:18:37, Train, Epoch : 6, Step : 3730, Loss : 0.37268, Acc : 0.866, Sensitive_Loss : 0.12497, Sensitive_Acc : 17.100, Run Time : 9.31 sec
INFO:root:2024-04-10 10:18:46, Train, Epoch : 6, Step : 3740, Loss : 0.41420, Acc : 0.841, Sensitive_Loss : 0.14309, Sensitive_Acc : 16.300, Run Time : 8.73 sec
INFO:root:2024-04-10 10:18:57, Train, Epoch : 6, Step : 3750, Loss : 0.31869, Acc : 0.838, Sensitive_Loss : 0.14950, Sensitive_Acc : 17.100, Run Time : 10.56 sec
INFO:root:2024-04-10 10:19:06, Train, Epoch : 6, Step : 3760, Loss : 0.37842, Acc : 0.844, Sensitive_Loss : 0.13313, Sensitive_Acc : 14.300, Run Time : 9.46 sec
INFO:root:2024-04-10 10:19:15, Train, Epoch : 6, Step : 3770, Loss : 0.44012, Acc : 0.775, Sensitive_Loss : 0.12340, Sensitive_Acc : 16.600, Run Time : 8.49 sec
INFO:root:2024-04-10 10:19:23, Train, Epoch : 6, Step : 3780, Loss : 0.32435, Acc : 0.875, Sensitive_Loss : 0.11451, Sensitive_Acc : 17.400, Run Time : 8.64 sec
INFO:root:2024-04-10 10:19:32, Train, Epoch : 6, Step : 3790, Loss : 0.41307, Acc : 0.812, Sensitive_Loss : 0.14408, Sensitive_Acc : 18.600, Run Time : 9.13 sec
INFO:root:2024-04-10 10:19:41, Train, Epoch : 6, Step : 3800, Loss : 0.40522, Acc : 0.812, Sensitive_Loss : 0.15263, Sensitive_Acc : 18.000, Run Time : 8.68 sec
INFO:root:2024-04-10 10:21:14, Dev, Step : 3800, Loss : 0.52086, Acc : 0.780, Auc : 0.859, Sensitive_Loss : 0.20910, Sensitive_Acc : 16.250, Sensitive_Auc : 0.983, Mean auc: 0.859, Run Time : 92.87 sec
INFO:root:2024-04-10 10:21:20, Train, Epoch : 6, Step : 3810, Loss : 0.35176, Acc : 0.850, Sensitive_Loss : 0.10136, Sensitive_Acc : 14.700, Run Time : 99.26 sec
INFO:root:2024-04-10 10:21:29, Train, Epoch : 6, Step : 3820, Loss : 0.37186, Acc : 0.834, Sensitive_Loss : 0.12192, Sensitive_Acc : 15.600, Run Time : 8.46 sec
INFO:root:2024-04-10 10:21:38, Train, Epoch : 6, Step : 3830, Loss : 0.41360, Acc : 0.806, Sensitive_Loss : 0.09094, Sensitive_Acc : 16.300, Run Time : 8.97 sec
INFO:root:2024-04-10 10:21:46, Train, Epoch : 6, Step : 3840, Loss : 0.39629, Acc : 0.806, Sensitive_Loss : 0.15578, Sensitive_Acc : 16.100, Run Time : 8.71 sec
INFO:root:2024-04-10 10:21:55, Train, Epoch : 6, Step : 3850, Loss : 0.40541, Acc : 0.794, Sensitive_Loss : 0.12638, Sensitive_Acc : 16.700, Run Time : 8.66 sec
INFO:root:2024-04-10 10:22:04, Train, Epoch : 6, Step : 3860, Loss : 0.39530, Acc : 0.841, Sensitive_Loss : 0.13572, Sensitive_Acc : 16.400, Run Time : 9.23 sec
INFO:root:2024-04-10 10:23:41
INFO:root:y_pred: [0.47233206 0.12271798 0.8825704  ... 0.7883181  0.6129398  0.1593784 ]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.99574840e-01 3.21127474e-04 2.67479777e-01 9.99646425e-01
 9.99404073e-01 9.76040125e-01 9.98911381e-01 2.51105870e-04
 9.95369732e-01 9.99225378e-01 7.52397418e-01 5.10591686e-01
 9.22222534e-05 9.80579317e-01 9.99926448e-01 9.99916911e-01
 9.88414884e-01 9.36353147e-01 9.98028338e-01 9.94709253e-01
 9.99796450e-01 5.42213917e-01 9.95326161e-01 9.23470557e-01
 9.76708591e-01 1.65130924e-02 9.92964029e-01 6.89760000e-02
 9.99488235e-01 7.30277738e-03 2.85909674e-03 8.38315114e-02
 2.71149073e-02 9.99003828e-01 3.03407523e-05 9.94602144e-01
 4.21842793e-04 9.99846697e-01 5.04505634e-02 9.96592045e-01
 9.99564707e-01 2.29639991e-04 2.88675316e-02 5.85823786e-04
 2.82011777e-01 3.60137850e-01 9.94483531e-01 9.84584630e-01
 8.55180144e-01 9.66582179e-01 8.05405260e-04 6.26925290e-01
 3.34613137e-02 8.07099283e-01 9.90850747e-01 8.23602304e-02
 9.20238078e-01 9.96262014e-01 9.99079585e-01 4.75662202e-03
 3.82526312e-03 9.93620694e-01 6.17099702e-01 9.99559700e-01
 9.71626103e-01 5.89488894e-02 9.36606050e-01 5.12108326e-01
 9.99782860e-01 9.95126128e-01 2.31510424e-03 7.35145986e-01
 9.56531286e-01 9.97308135e-01 9.93898511e-01 2.03163666e-03
 6.82414293e-01 3.39406589e-03 1.59778993e-03 9.99894261e-01
 1.84196681e-01 9.97728407e-01 9.99475658e-01 9.99337375e-01
 1.17932677e-01 9.99985933e-01 4.15045855e-04 3.99990082e-02
 9.97437239e-01 9.91486251e-01 4.63152379e-02 5.04396141e-01
 8.23665187e-02 7.77273476e-01 2.33585104e-01 9.99935627e-01
 1.59652699e-02 9.93158638e-01 9.89112854e-01 2.84022256e-03
 3.88458837e-04 2.28626043e-01 9.98209000e-01 9.99920964e-01
 9.52594459e-01 9.44302678e-01 9.99117911e-01 2.32007336e-02
 3.02957118e-01 9.88509715e-01 1.39488375e-05 2.29071779e-03
 3.95224452e-01 9.99642253e-01 9.87979054e-01 1.95051567e-03
 9.87777412e-01 4.45896527e-03 9.99529004e-01 3.02195817e-01
 9.98592198e-01 9.99788940e-01 3.87657166e-01 8.13810825e-01
 7.12388158e-01 8.51774774e-03 1.04098774e-01 2.88513984e-04
 9.61153448e-01 9.97768998e-01 1.37426564e-02 7.88309437e-04
 4.73601790e-03 6.98039412e-01 9.97223854e-01 9.95226622e-01
 9.78215635e-01 4.56134826e-01 1.06340058e-01 9.66595531e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 10:23:41, Dev, Step : 3864, Loss : 0.51498, Acc : 0.783, Auc : 0.858, Sensitive_Loss : 0.16586, Sensitive_Acc : 16.207, Sensitive_Auc : 0.983, Mean auc: 0.858, Run Time : 92.77 sec
INFO:root:2024-04-10 10:23:48, Train, Epoch : 7, Step : 3870, Loss : 0.22478, Acc : 0.506, Sensitive_Loss : 0.08961, Sensitive_Acc : 9.600, Run Time : 6.02 sec
INFO:root:2024-04-10 10:23:56, Train, Epoch : 7, Step : 3880, Loss : 0.38788, Acc : 0.844, Sensitive_Loss : 0.12958, Sensitive_Acc : 16.300, Run Time : 8.35 sec
INFO:root:2024-04-10 10:24:05, Train, Epoch : 7, Step : 3890, Loss : 0.33220, Acc : 0.866, Sensitive_Loss : 0.09856, Sensitive_Acc : 16.400, Run Time : 8.36 sec
INFO:root:2024-04-10 10:24:13, Train, Epoch : 7, Step : 3900, Loss : 0.27759, Acc : 0.847, Sensitive_Loss : 0.17519, Sensitive_Acc : 15.100, Run Time : 8.38 sec
INFO:root:2024-04-10 10:25:49, Dev, Step : 3900, Loss : 0.51820, Acc : 0.780, Auc : 0.860, Sensitive_Loss : 0.15607, Sensitive_Acc : 16.207, Sensitive_Auc : 0.984, Mean auc: 0.860, Run Time : 96.40 sec
INFO:root:2024-04-10 10:25:56, Train, Epoch : 7, Step : 3910, Loss : 0.37763, Acc : 0.800, Sensitive_Loss : 0.10818, Sensitive_Acc : 16.700, Run Time : 103.15 sec
INFO:root:2024-04-10 10:26:05, Train, Epoch : 7, Step : 3920, Loss : 0.32118, Acc : 0.834, Sensitive_Loss : 0.12830, Sensitive_Acc : 16.400, Run Time : 9.35 sec
INFO:root:2024-04-10 10:26:18, Train, Epoch : 7, Step : 3930, Loss : 0.30074, Acc : 0.887, Sensitive_Loss : 0.13733, Sensitive_Acc : 15.800, Run Time : 12.56 sec
INFO:root:2024-04-10 10:26:27, Train, Epoch : 7, Step : 3940, Loss : 0.25118, Acc : 0.875, Sensitive_Loss : 0.09448, Sensitive_Acc : 16.700, Run Time : 9.38 sec
INFO:root:2024-04-10 10:26:37, Train, Epoch : 7, Step : 3950, Loss : 0.27943, Acc : 0.853, Sensitive_Loss : 0.09199, Sensitive_Acc : 16.500, Run Time : 9.70 sec
INFO:root:2024-04-10 10:26:46, Train, Epoch : 7, Step : 3960, Loss : 0.41956, Acc : 0.791, Sensitive_Loss : 0.10613, Sensitive_Acc : 17.400, Run Time : 8.70 sec
INFO:root:2024-04-10 10:26:55, Train, Epoch : 7, Step : 3970, Loss : 0.33280, Acc : 0.872, Sensitive_Loss : 0.13293, Sensitive_Acc : 16.400, Run Time : 8.98 sec
INFO:root:2024-04-10 10:27:04, Train, Epoch : 7, Step : 3980, Loss : 0.34738, Acc : 0.844, Sensitive_Loss : 0.08654, Sensitive_Acc : 16.200, Run Time : 9.60 sec
INFO:root:2024-04-10 10:27:14, Train, Epoch : 7, Step : 3990, Loss : 0.35369, Acc : 0.844, Sensitive_Loss : 0.15117, Sensitive_Acc : 14.700, Run Time : 9.30 sec
INFO:root:2024-04-10 10:27:23, Train, Epoch : 7, Step : 4000, Loss : 0.36175, Acc : 0.859, Sensitive_Loss : 0.10949, Sensitive_Acc : 16.300, Run Time : 9.05 sec
INFO:root:2024-04-10 10:29:02, Dev, Step : 4000, Loss : 0.54538, Acc : 0.777, Auc : 0.862, Sensitive_Loss : 0.15741, Sensitive_Acc : 16.293, Sensitive_Auc : 0.986, Mean auc: 0.862, Run Time : 99.03 sec
INFO:root:2024-04-10 10:29:08, Train, Epoch : 7, Step : 4010, Loss : 0.40359, Acc : 0.816, Sensitive_Loss : 0.13044, Sensitive_Acc : 15.800, Run Time : 105.72 sec
INFO:root:2024-04-10 10:29:18, Train, Epoch : 7, Step : 4020, Loss : 0.33795, Acc : 0.869, Sensitive_Loss : 0.16989, Sensitive_Acc : 14.700, Run Time : 9.50 sec
INFO:root:2024-04-10 10:29:27, Train, Epoch : 7, Step : 4030, Loss : 0.26688, Acc : 0.869, Sensitive_Loss : 0.10896, Sensitive_Acc : 15.800, Run Time : 9.40 sec
INFO:root:2024-04-10 10:29:38, Train, Epoch : 7, Step : 4040, Loss : 0.32835, Acc : 0.834, Sensitive_Loss : 0.13240, Sensitive_Acc : 15.200, Run Time : 10.34 sec
INFO:root:2024-04-10 10:29:47, Train, Epoch : 7, Step : 4050, Loss : 0.26373, Acc : 0.881, Sensitive_Loss : 0.14944, Sensitive_Acc : 16.000, Run Time : 9.47 sec
INFO:root:2024-04-10 10:29:57, Train, Epoch : 7, Step : 4060, Loss : 0.25488, Acc : 0.884, Sensitive_Loss : 0.15354, Sensitive_Acc : 14.900, Run Time : 9.39 sec
INFO:root:2024-04-10 10:30:05, Train, Epoch : 7, Step : 4070, Loss : 0.30837, Acc : 0.866, Sensitive_Loss : 0.08745, Sensitive_Acc : 17.800, Run Time : 8.77 sec
INFO:root:2024-04-10 10:30:14, Train, Epoch : 7, Step : 4080, Loss : 0.31678, Acc : 0.847, Sensitive_Loss : 0.16098, Sensitive_Acc : 15.900, Run Time : 8.97 sec
INFO:root:2024-04-10 10:30:24, Train, Epoch : 7, Step : 4090, Loss : 0.37902, Acc : 0.828, Sensitive_Loss : 0.09288, Sensitive_Acc : 17.000, Run Time : 9.51 sec
INFO:root:2024-04-10 10:30:33, Train, Epoch : 7, Step : 4100, Loss : 0.35982, Acc : 0.847, Sensitive_Loss : 0.18186, Sensitive_Acc : 15.900, Run Time : 8.89 sec
INFO:root:2024-04-10 10:32:07, Dev, Step : 4100, Loss : 0.52822, Acc : 0.781, Auc : 0.862, Sensitive_Loss : 0.17835, Sensitive_Acc : 16.364, Sensitive_Auc : 0.985, Mean auc: 0.862, Run Time : 94.15 sec
INFO:root:2024-04-10 10:32:14, Train, Epoch : 7, Step : 4110, Loss : 0.40215, Acc : 0.834, Sensitive_Loss : 0.13040, Sensitive_Acc : 16.700, Run Time : 101.09 sec
INFO:root:2024-04-10 10:32:23, Train, Epoch : 7, Step : 4120, Loss : 0.29163, Acc : 0.863, Sensitive_Loss : 0.10766, Sensitive_Acc : 17.500, Run Time : 8.78 sec
INFO:root:2024-04-10 10:32:31, Train, Epoch : 7, Step : 4130, Loss : 0.33404, Acc : 0.856, Sensitive_Loss : 0.12503, Sensitive_Acc : 15.900, Run Time : 8.83 sec
INFO:root:2024-04-10 10:32:41, Train, Epoch : 7, Step : 4140, Loss : 0.34582, Acc : 0.838, Sensitive_Loss : 0.12525, Sensitive_Acc : 15.500, Run Time : 9.27 sec
INFO:root:2024-04-10 10:32:49, Train, Epoch : 7, Step : 4150, Loss : 0.33156, Acc : 0.812, Sensitive_Loss : 0.13614, Sensitive_Acc : 15.700, Run Time : 8.82 sec
INFO:root:2024-04-10 10:32:59, Train, Epoch : 7, Step : 4160, Loss : 0.35093, Acc : 0.822, Sensitive_Loss : 0.13137, Sensitive_Acc : 16.400, Run Time : 9.08 sec
INFO:root:2024-04-10 10:33:08, Train, Epoch : 7, Step : 4170, Loss : 0.29758, Acc : 0.894, Sensitive_Loss : 0.10178, Sensitive_Acc : 17.100, Run Time : 9.58 sec
INFO:root:2024-04-10 10:33:17, Train, Epoch : 7, Step : 4180, Loss : 0.35731, Acc : 0.847, Sensitive_Loss : 0.13506, Sensitive_Acc : 15.700, Run Time : 9.21 sec
INFO:root:2024-04-10 10:33:26, Train, Epoch : 7, Step : 4190, Loss : 0.40574, Acc : 0.859, Sensitive_Loss : 0.15794, Sensitive_Acc : 16.300, Run Time : 8.73 sec
INFO:root:2024-04-10 10:33:35, Train, Epoch : 7, Step : 4200, Loss : 0.31952, Acc : 0.850, Sensitive_Loss : 0.13027, Sensitive_Acc : 17.500, Run Time : 8.59 sec
INFO:root:2024-04-10 10:35:09, Dev, Step : 4200, Loss : 0.52842, Acc : 0.783, Auc : 0.861, Sensitive_Loss : 0.16132, Sensitive_Acc : 16.221, Sensitive_Auc : 0.985, Mean auc: 0.861, Run Time : 93.96 sec
INFO:root:2024-04-10 10:35:15, Train, Epoch : 7, Step : 4210, Loss : 0.36293, Acc : 0.863, Sensitive_Loss : 0.16186, Sensitive_Acc : 15.400, Run Time : 100.40 sec
INFO:root:2024-04-10 10:35:25, Train, Epoch : 7, Step : 4220, Loss : 0.39240, Acc : 0.834, Sensitive_Loss : 0.14564, Sensitive_Acc : 16.300, Run Time : 9.86 sec
INFO:root:2024-04-10 10:35:34, Train, Epoch : 7, Step : 4230, Loss : 0.35680, Acc : 0.819, Sensitive_Loss : 0.13064, Sensitive_Acc : 16.200, Run Time : 8.74 sec
INFO:root:2024-04-10 10:35:42, Train, Epoch : 7, Step : 4240, Loss : 0.36552, Acc : 0.850, Sensitive_Loss : 0.11515, Sensitive_Acc : 16.500, Run Time : 8.64 sec
INFO:root:2024-04-10 10:35:51, Train, Epoch : 7, Step : 4250, Loss : 0.33649, Acc : 0.866, Sensitive_Loss : 0.13588, Sensitive_Acc : 17.600, Run Time : 8.89 sec
INFO:root:2024-04-10 10:36:00, Train, Epoch : 7, Step : 4260, Loss : 0.30804, Acc : 0.872, Sensitive_Loss : 0.09768, Sensitive_Acc : 14.700, Run Time : 9.09 sec
INFO:root:2024-04-10 10:36:09, Train, Epoch : 7, Step : 4270, Loss : 0.32963, Acc : 0.850, Sensitive_Loss : 0.13474, Sensitive_Acc : 15.900, Run Time : 9.16 sec
INFO:root:2024-04-10 10:36:18, Train, Epoch : 7, Step : 4280, Loss : 0.30910, Acc : 0.859, Sensitive_Loss : 0.13215, Sensitive_Acc : 15.300, Run Time : 8.63 sec
INFO:root:2024-04-10 10:36:27, Train, Epoch : 7, Step : 4290, Loss : 0.35810, Acc : 0.838, Sensitive_Loss : 0.16596, Sensitive_Acc : 17.700, Run Time : 8.94 sec
INFO:root:2024-04-10 10:36:37, Train, Epoch : 7, Step : 4300, Loss : 0.29981, Acc : 0.859, Sensitive_Loss : 0.10840, Sensitive_Acc : 16.600, Run Time : 9.52 sec
INFO:root:2024-04-10 10:38:11, Dev, Step : 4300, Loss : 0.52043, Acc : 0.784, Auc : 0.862, Sensitive_Loss : 0.18026, Sensitive_Acc : 16.293, Sensitive_Auc : 0.987, Mean auc: 0.862, Run Time : 94.11 sec
INFO:root:2024-04-10 10:38:11, Best, Step : 4300, Loss : 0.52043, Acc : 0.784, Auc : 0.862, Sensitive_Loss : 0.18026, Sensitive_Acc : 16.293, Sensitive_Auc : 0.987, Best Auc : 0.862
INFO:root:2024-04-10 10:38:18, Train, Epoch : 7, Step : 4310, Loss : 0.30586, Acc : 0.863, Sensitive_Loss : 0.12876, Sensitive_Acc : 17.600, Run Time : 101.52 sec
INFO:root:2024-04-10 10:38:28, Train, Epoch : 7, Step : 4320, Loss : 0.34152, Acc : 0.847, Sensitive_Loss : 0.13026, Sensitive_Acc : 16.000, Run Time : 9.96 sec
INFO:root:2024-04-10 10:38:37, Train, Epoch : 7, Step : 4330, Loss : 0.40959, Acc : 0.787, Sensitive_Loss : 0.15601, Sensitive_Acc : 16.200, Run Time : 9.41 sec
INFO:root:2024-04-10 10:38:47, Train, Epoch : 7, Step : 4340, Loss : 0.38769, Acc : 0.822, Sensitive_Loss : 0.17608, Sensitive_Acc : 16.800, Run Time : 9.47 sec
INFO:root:2024-04-10 10:38:56, Train, Epoch : 7, Step : 4350, Loss : 0.29030, Acc : 0.863, Sensitive_Loss : 0.12615, Sensitive_Acc : 15.700, Run Time : 9.25 sec
INFO:root:2024-04-10 10:39:06, Train, Epoch : 7, Step : 4360, Loss : 0.35507, Acc : 0.856, Sensitive_Loss : 0.14378, Sensitive_Acc : 14.500, Run Time : 9.59 sec
INFO:root:2024-04-10 10:39:16, Train, Epoch : 7, Step : 4370, Loss : 0.39138, Acc : 0.822, Sensitive_Loss : 0.17263, Sensitive_Acc : 17.500, Run Time : 9.87 sec
INFO:root:2024-04-10 10:39:25, Train, Epoch : 7, Step : 4380, Loss : 0.31625, Acc : 0.866, Sensitive_Loss : 0.12141, Sensitive_Acc : 16.400, Run Time : 9.90 sec
INFO:root:2024-04-10 10:39:35, Train, Epoch : 7, Step : 4390, Loss : 0.33672, Acc : 0.853, Sensitive_Loss : 0.09698, Sensitive_Acc : 16.900, Run Time : 9.95 sec
INFO:root:2024-04-10 10:39:45, Train, Epoch : 7, Step : 4400, Loss : 0.30386, Acc : 0.863, Sensitive_Loss : 0.12671, Sensitive_Acc : 16.000, Run Time : 9.25 sec
INFO:root:2024-04-10 10:41:19, Dev, Step : 4400, Loss : 0.53826, Acc : 0.777, Auc : 0.859, Sensitive_Loss : 0.16723, Sensitive_Acc : 16.207, Sensitive_Auc : 0.984, Mean auc: 0.859, Run Time : 94.16 sec
INFO:root:2024-04-10 10:41:26, Train, Epoch : 7, Step : 4410, Loss : 0.32928, Acc : 0.834, Sensitive_Loss : 0.13907, Sensitive_Acc : 15.800, Run Time : 100.88 sec
INFO:root:2024-04-10 10:41:35, Train, Epoch : 7, Step : 4420, Loss : 0.30727, Acc : 0.884, Sensitive_Loss : 0.15652, Sensitive_Acc : 17.200, Run Time : 9.35 sec
INFO:root:2024-04-10 10:41:48, Train, Epoch : 7, Step : 4430, Loss : 0.35354, Acc : 0.819, Sensitive_Loss : 0.15007, Sensitive_Acc : 15.900, Run Time : 13.28 sec
INFO:root:2024-04-10 10:41:58, Train, Epoch : 7, Step : 4440, Loss : 0.34457, Acc : 0.816, Sensitive_Loss : 0.10058, Sensitive_Acc : 15.200, Run Time : 10.15 sec
INFO:root:2024-04-10 10:42:08, Train, Epoch : 7, Step : 4450, Loss : 0.31870, Acc : 0.863, Sensitive_Loss : 0.11174, Sensitive_Acc : 16.400, Run Time : 9.39 sec
INFO:root:2024-04-10 10:42:18, Train, Epoch : 7, Step : 4460, Loss : 0.37090, Acc : 0.844, Sensitive_Loss : 0.10051, Sensitive_Acc : 15.300, Run Time : 10.74 sec
INFO:root:2024-04-10 10:42:28, Train, Epoch : 7, Step : 4470, Loss : 0.38020, Acc : 0.866, Sensitive_Loss : 0.12170, Sensitive_Acc : 15.800, Run Time : 9.52 sec
INFO:root:2024-04-10 10:42:37, Train, Epoch : 7, Step : 4480, Loss : 0.31926, Acc : 0.850, Sensitive_Loss : 0.10407, Sensitive_Acc : 16.700, Run Time : 8.84 sec
INFO:root:2024-04-10 10:42:46, Train, Epoch : 7, Step : 4490, Loss : 0.32600, Acc : 0.850, Sensitive_Loss : 0.14777, Sensitive_Acc : 15.400, Run Time : 8.87 sec
INFO:root:2024-04-10 10:42:56, Train, Epoch : 7, Step : 4500, Loss : 0.36633, Acc : 0.816, Sensitive_Loss : 0.13509, Sensitive_Acc : 16.600, Run Time : 9.82 sec
INFO:root:2024-04-10 10:44:30, Dev, Step : 4500, Loss : 0.55479, Acc : 0.777, Auc : 0.858, Sensitive_Loss : 0.17213, Sensitive_Acc : 16.321, Sensitive_Auc : 0.984, Mean auc: 0.858, Run Time : 94.47 sec
INFO:root:2024-04-10 10:46:07
INFO:root:y_pred: [0.25648928 0.06248364 0.9057539  ... 0.76206017 0.67458355 0.07781495]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9961144e-01 2.7830305e-04 5.6940526e-01 9.9981326e-01 9.9947542e-01
 9.7762877e-01 9.9910718e-01 9.2439924e-04 9.9663770e-01 9.9895656e-01
 7.7260160e-01 4.1608500e-01 2.8988323e-04 9.8179287e-01 9.9996889e-01
 9.9988377e-01 9.8908335e-01 9.6455717e-01 9.9680030e-01 9.9542409e-01
 9.9980134e-01 3.5223147e-01 9.9506110e-01 9.4101220e-01 9.6696866e-01
 3.0686878e-02 9.9095309e-01 5.6993723e-02 9.9953008e-01 6.6752266e-03
 3.6673201e-03 1.4105830e-01 1.3131112e-02 9.9914658e-01 5.7186833e-05
 9.9675047e-01 6.2033749e-04 9.9983609e-01 7.7704035e-02 9.9541277e-01
 9.9959391e-01 1.5673004e-04 1.6050084e-02 7.0155167e-04 2.2529812e-01
 3.1420729e-01 9.9491817e-01 9.8996490e-01 8.9152431e-01 9.8248154e-01
 1.2596066e-03 7.1045023e-01 4.0891893e-02 8.4081906e-01 9.8889953e-01
 9.0735860e-02 9.2954552e-01 9.9806577e-01 9.9925655e-01 1.0110926e-02
 3.8833695e-03 9.9571866e-01 4.2653042e-01 9.9964190e-01 9.6311551e-01
 4.4095598e-02 9.4405401e-01 6.5890169e-01 9.9979383e-01 9.9181491e-01
 2.4514101e-03 8.5705566e-01 9.5668173e-01 9.9754840e-01 9.9174458e-01
 1.5050590e-03 5.1777905e-01 6.0858494e-03 2.4300839e-03 9.9997663e-01
 9.0323262e-02 9.9755126e-01 9.9949622e-01 9.9943358e-01 1.1098021e-01
 9.9999380e-01 1.1686968e-03 7.7361524e-02 9.9860519e-01 9.8831171e-01
 4.6705391e-02 5.0029111e-01 5.8405817e-02 8.7057126e-01 2.5711018e-01
 9.9994981e-01 6.5598418e-03 9.9436569e-01 9.7244400e-01 6.9321650e-03
 1.1506649e-03 3.2676333e-01 9.9863213e-01 9.9992180e-01 9.3768460e-01
 9.3978041e-01 9.9886250e-01 2.6202323e-02 2.5134951e-01 9.8930478e-01
 3.4586462e-06 2.5198117e-03 5.9196621e-01 9.9973768e-01 9.8632103e-01
 6.6660857e-04 9.8872143e-01 9.6144732e-03 9.9925011e-01 2.5231215e-01
 9.9897313e-01 9.9972385e-01 2.2705778e-01 8.7167865e-01 4.9967942e-01
 1.7064583e-02 1.4827934e-01 2.6136020e-04 9.3991059e-01 9.9725240e-01
 2.9041342e-02 1.4616275e-03 6.9860453e-03 7.7754945e-01 9.9869245e-01
 9.9454367e-01 9.6804827e-01 5.2029288e-01 1.1494632e-01 9.7701198e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 10:46:07, Dev, Step : 4508, Loss : 0.56120, Acc : 0.775, Auc : 0.858, Sensitive_Loss : 0.16839, Sensitive_Acc : 16.307, Sensitive_Auc : 0.983, Mean auc: 0.858, Run Time : 92.66 sec
INFO:root:2024-04-10 10:46:12, Train, Epoch : 8, Step : 4510, Loss : 0.12678, Acc : 0.141, Sensitive_Loss : 0.02316, Sensitive_Acc : 3.400, Run Time : 3.82 sec
INFO:root:2024-04-10 10:46:23, Train, Epoch : 8, Step : 4520, Loss : 0.28738, Acc : 0.881, Sensitive_Loss : 0.15083, Sensitive_Acc : 17.200, Run Time : 10.76 sec
INFO:root:2024-04-10 10:46:33, Train, Epoch : 8, Step : 4530, Loss : 0.36326, Acc : 0.816, Sensitive_Loss : 0.13075, Sensitive_Acc : 16.900, Run Time : 9.69 sec
INFO:root:2024-04-10 10:46:43, Train, Epoch : 8, Step : 4540, Loss : 0.29910, Acc : 0.887, Sensitive_Loss : 0.15912, Sensitive_Acc : 13.200, Run Time : 10.29 sec
INFO:root:2024-04-10 10:46:55, Train, Epoch : 8, Step : 4550, Loss : 0.31144, Acc : 0.838, Sensitive_Loss : 0.11426, Sensitive_Acc : 15.800, Run Time : 12.15 sec
INFO:root:2024-04-10 10:47:05, Train, Epoch : 8, Step : 4560, Loss : 0.29257, Acc : 0.856, Sensitive_Loss : 0.15201, Sensitive_Acc : 15.400, Run Time : 9.68 sec
INFO:root:2024-04-10 10:47:14, Train, Epoch : 8, Step : 4570, Loss : 0.32194, Acc : 0.875, Sensitive_Loss : 0.11368, Sensitive_Acc : 16.900, Run Time : 9.23 sec
INFO:root:2024-04-10 10:47:25, Train, Epoch : 8, Step : 4580, Loss : 0.30529, Acc : 0.847, Sensitive_Loss : 0.12687, Sensitive_Acc : 16.700, Run Time : 11.34 sec
INFO:root:2024-04-10 10:47:34, Train, Epoch : 8, Step : 4590, Loss : 0.32504, Acc : 0.834, Sensitive_Loss : 0.17429, Sensitive_Acc : 14.800, Run Time : 8.65 sec
INFO:root:2024-04-10 10:47:43, Train, Epoch : 8, Step : 4600, Loss : 0.29662, Acc : 0.875, Sensitive_Loss : 0.11982, Sensitive_Acc : 15.700, Run Time : 8.76 sec
INFO:root:2024-04-10 10:49:16, Dev, Step : 4600, Loss : 0.55241, Acc : 0.773, Auc : 0.857, Sensitive_Loss : 0.17940, Sensitive_Acc : 16.250, Sensitive_Auc : 0.984, Mean auc: 0.857, Run Time : 93.22 sec
INFO:root:2024-04-10 10:49:23, Train, Epoch : 8, Step : 4610, Loss : 0.29404, Acc : 0.838, Sensitive_Loss : 0.12955, Sensitive_Acc : 15.500, Run Time : 100.31 sec
INFO:root:2024-04-10 10:49:32, Train, Epoch : 8, Step : 4620, Loss : 0.26581, Acc : 0.869, Sensitive_Loss : 0.12322, Sensitive_Acc : 15.300, Run Time : 9.56 sec
INFO:root:2024-04-10 10:49:45, Train, Epoch : 8, Step : 4630, Loss : 0.30026, Acc : 0.856, Sensitive_Loss : 0.11965, Sensitive_Acc : 15.500, Run Time : 12.09 sec
INFO:root:2024-04-10 10:49:57, Train, Epoch : 8, Step : 4640, Loss : 0.28663, Acc : 0.878, Sensitive_Loss : 0.15567, Sensitive_Acc : 16.500, Run Time : 12.45 sec
INFO:root:2024-04-10 10:50:07, Train, Epoch : 8, Step : 4650, Loss : 0.33255, Acc : 0.878, Sensitive_Loss : 0.12888, Sensitive_Acc : 17.000, Run Time : 10.00 sec
INFO:root:2024-04-10 10:50:18, Train, Epoch : 8, Step : 4660, Loss : 0.28251, Acc : 0.863, Sensitive_Loss : 0.10397, Sensitive_Acc : 16.900, Run Time : 10.78 sec
INFO:root:2024-04-10 10:50:31, Train, Epoch : 8, Step : 4670, Loss : 0.33390, Acc : 0.856, Sensitive_Loss : 0.18180, Sensitive_Acc : 15.300, Run Time : 12.78 sec
INFO:root:2024-04-10 10:50:41, Train, Epoch : 8, Step : 4680, Loss : 0.37430, Acc : 0.806, Sensitive_Loss : 0.11952, Sensitive_Acc : 15.300, Run Time : 10.02 sec
INFO:root:2024-04-10 10:50:51, Train, Epoch : 8, Step : 4690, Loss : 0.24990, Acc : 0.887, Sensitive_Loss : 0.12607, Sensitive_Acc : 15.800, Run Time : 10.34 sec
INFO:root:2024-04-10 10:51:03, Train, Epoch : 8, Step : 4700, Loss : 0.30455, Acc : 0.859, Sensitive_Loss : 0.16128, Sensitive_Acc : 17.900, Run Time : 12.08 sec
INFO:root:2024-04-10 10:52:48, Dev, Step : 4700, Loss : 0.53748, Acc : 0.779, Auc : 0.860, Sensitive_Loss : 0.16606, Sensitive_Acc : 16.250, Sensitive_Auc : 0.983, Mean auc: 0.860, Run Time : 105.46 sec
INFO:root:2024-04-10 10:52:55, Train, Epoch : 8, Step : 4710, Loss : 0.30902, Acc : 0.853, Sensitive_Loss : 0.12495, Sensitive_Acc : 16.300, Run Time : 112.31 sec
INFO:root:2024-04-10 10:53:07, Train, Epoch : 8, Step : 4720, Loss : 0.33530, Acc : 0.856, Sensitive_Loss : 0.12917, Sensitive_Acc : 17.500, Run Time : 11.83 sec
INFO:root:2024-04-10 10:53:20, Train, Epoch : 8, Step : 4730, Loss : 0.32324, Acc : 0.878, Sensitive_Loss : 0.13184, Sensitive_Acc : 16.000, Run Time : 13.17 sec
INFO:root:2024-04-10 10:53:30, Train, Epoch : 8, Step : 4740, Loss : 0.31921, Acc : 0.859, Sensitive_Loss : 0.13054, Sensitive_Acc : 17.500, Run Time : 10.08 sec
INFO:root:2024-04-10 10:53:42, Train, Epoch : 8, Step : 4750, Loss : 0.35057, Acc : 0.838, Sensitive_Loss : 0.10995, Sensitive_Acc : 15.900, Run Time : 11.11 sec
INFO:root:2024-04-10 10:53:54, Train, Epoch : 8, Step : 4760, Loss : 0.24267, Acc : 0.884, Sensitive_Loss : 0.09973, Sensitive_Acc : 15.700, Run Time : 12.43 sec
INFO:root:2024-04-10 10:54:04, Train, Epoch : 8, Step : 4770, Loss : 0.31649, Acc : 0.856, Sensitive_Loss : 0.10207, Sensitive_Acc : 16.200, Run Time : 9.66 sec
INFO:root:2024-04-10 10:54:14, Train, Epoch : 8, Step : 4780, Loss : 0.29089, Acc : 0.844, Sensitive_Loss : 0.11012, Sensitive_Acc : 14.400, Run Time : 10.19 sec
INFO:root:2024-04-10 10:54:27, Train, Epoch : 8, Step : 4790, Loss : 0.31809, Acc : 0.863, Sensitive_Loss : 0.11014, Sensitive_Acc : 15.700, Run Time : 13.32 sec
INFO:root:2024-04-10 10:54:37, Train, Epoch : 8, Step : 4800, Loss : 0.24264, Acc : 0.906, Sensitive_Loss : 0.14167, Sensitive_Acc : 17.900, Run Time : 10.26 sec
INFO:root:2024-04-10 10:56:12, Dev, Step : 4800, Loss : 0.55101, Acc : 0.782, Auc : 0.859, Sensitive_Loss : 0.18388, Sensitive_Acc : 16.264, Sensitive_Auc : 0.985, Mean auc: 0.859, Run Time : 94.36 sec
INFO:root:2024-04-10 10:56:18, Train, Epoch : 8, Step : 4810, Loss : 0.31905, Acc : 0.859, Sensitive_Loss : 0.12762, Sensitive_Acc : 15.400, Run Time : 100.99 sec
INFO:root:2024-04-10 10:56:30, Train, Epoch : 8, Step : 4820, Loss : 0.38670, Acc : 0.850, Sensitive_Loss : 0.12113, Sensitive_Acc : 17.300, Run Time : 11.97 sec
INFO:root:2024-04-10 10:56:43, Train, Epoch : 8, Step : 4830, Loss : 0.28169, Acc : 0.869, Sensitive_Loss : 0.13769, Sensitive_Acc : 16.100, Run Time : 12.56 sec
INFO:root:2024-04-10 10:56:53, Train, Epoch : 8, Step : 4840, Loss : 0.36178, Acc : 0.847, Sensitive_Loss : 0.10253, Sensitive_Acc : 15.200, Run Time : 9.66 sec
INFO:root:2024-04-10 10:57:03, Train, Epoch : 8, Step : 4850, Loss : 0.32481, Acc : 0.856, Sensitive_Loss : 0.07096, Sensitive_Acc : 16.700, Run Time : 10.51 sec
INFO:root:2024-04-10 10:57:16, Train, Epoch : 8, Step : 4860, Loss : 0.36318, Acc : 0.875, Sensitive_Loss : 0.10641, Sensitive_Acc : 16.500, Run Time : 13.42 sec
INFO:root:2024-04-10 10:57:26, Train, Epoch : 8, Step : 4870, Loss : 0.30845, Acc : 0.853, Sensitive_Loss : 0.14185, Sensitive_Acc : 14.800, Run Time : 9.85 sec
INFO:root:2024-04-10 10:57:38, Train, Epoch : 8, Step : 4880, Loss : 0.33425, Acc : 0.872, Sensitive_Loss : 0.12849, Sensitive_Acc : 16.600, Run Time : 11.66 sec
INFO:root:2024-04-10 10:57:51, Train, Epoch : 8, Step : 4890, Loss : 0.33861, Acc : 0.831, Sensitive_Loss : 0.12687, Sensitive_Acc : 16.200, Run Time : 12.87 sec
INFO:root:2024-04-10 10:58:00, Train, Epoch : 8, Step : 4900, Loss : 0.37285, Acc : 0.869, Sensitive_Loss : 0.12692, Sensitive_Acc : 17.000, Run Time : 9.28 sec
INFO:root:2024-04-10 10:59:33, Dev, Step : 4900, Loss : 0.55224, Acc : 0.777, Auc : 0.855, Sensitive_Loss : 0.15943, Sensitive_Acc : 16.264, Sensitive_Auc : 0.987, Mean auc: 0.855, Run Time : 93.28 sec
INFO:root:2024-04-10 10:59:40, Train, Epoch : 8, Step : 4910, Loss : 0.31488, Acc : 0.894, Sensitive_Loss : 0.11610, Sensitive_Acc : 14.900, Run Time : 100.03 sec
INFO:root:2024-04-10 10:59:50, Train, Epoch : 8, Step : 4920, Loss : 0.30673, Acc : 0.850, Sensitive_Loss : 0.12138, Sensitive_Acc : 16.100, Run Time : 10.16 sec
INFO:root:2024-04-10 11:00:01, Train, Epoch : 8, Step : 4930, Loss : 0.30268, Acc : 0.872, Sensitive_Loss : 0.10305, Sensitive_Acc : 17.200, Run Time : 10.22 sec
INFO:root:2024-04-10 11:00:12, Train, Epoch : 8, Step : 4940, Loss : 0.38459, Acc : 0.844, Sensitive_Loss : 0.11968, Sensitive_Acc : 16.900, Run Time : 11.01 sec
INFO:root:2024-04-10 11:00:21, Train, Epoch : 8, Step : 4950, Loss : 0.29285, Acc : 0.875, Sensitive_Loss : 0.13050, Sensitive_Acc : 17.500, Run Time : 9.58 sec
INFO:root:2024-04-10 11:00:32, Train, Epoch : 8, Step : 4960, Loss : 0.29947, Acc : 0.875, Sensitive_Loss : 0.17435, Sensitive_Acc : 16.500, Run Time : 10.56 sec
INFO:root:2024-04-10 11:00:44, Train, Epoch : 8, Step : 4970, Loss : 0.29853, Acc : 0.869, Sensitive_Loss : 0.11292, Sensitive_Acc : 14.700, Run Time : 12.38 sec
INFO:root:2024-04-10 11:00:56, Train, Epoch : 8, Step : 4980, Loss : 0.29151, Acc : 0.875, Sensitive_Loss : 0.14235, Sensitive_Acc : 16.800, Run Time : 11.47 sec
INFO:root:2024-04-10 11:01:05, Train, Epoch : 8, Step : 4990, Loss : 0.38022, Acc : 0.825, Sensitive_Loss : 0.17118, Sensitive_Acc : 15.700, Run Time : 9.39 sec
INFO:root:2024-04-10 11:01:17, Train, Epoch : 8, Step : 5000, Loss : 0.34538, Acc : 0.866, Sensitive_Loss : 0.14042, Sensitive_Acc : 16.800, Run Time : 11.93 sec
INFO:root:2024-04-10 11:02:51, Dev, Step : 5000, Loss : 0.54924, Acc : 0.782, Auc : 0.858, Sensitive_Loss : 0.16689, Sensitive_Acc : 16.264, Sensitive_Auc : 0.987, Mean auc: 0.858, Run Time : 94.42 sec
INFO:root:2024-04-10 11:03:00, Train, Epoch : 8, Step : 5010, Loss : 0.28848, Acc : 0.884, Sensitive_Loss : 0.14262, Sensitive_Acc : 14.900, Run Time : 103.47 sec
INFO:root:2024-04-10 11:03:12, Train, Epoch : 8, Step : 5020, Loss : 0.32686, Acc : 0.844, Sensitive_Loss : 0.12494, Sensitive_Acc : 17.800, Run Time : 11.61 sec
INFO:root:2024-04-10 11:03:21, Train, Epoch : 8, Step : 5030, Loss : 0.29697, Acc : 0.863, Sensitive_Loss : 0.10717, Sensitive_Acc : 17.500, Run Time : 9.55 sec
INFO:root:2024-04-10 11:03:31, Train, Epoch : 8, Step : 5040, Loss : 0.31110, Acc : 0.891, Sensitive_Loss : 0.12376, Sensitive_Acc : 15.800, Run Time : 9.73 sec
INFO:root:2024-04-10 11:03:41, Train, Epoch : 8, Step : 5050, Loss : 0.24386, Acc : 0.887, Sensitive_Loss : 0.10733, Sensitive_Acc : 16.800, Run Time : 9.95 sec
INFO:root:2024-04-10 11:03:53, Train, Epoch : 8, Step : 5060, Loss : 0.27911, Acc : 0.891, Sensitive_Loss : 0.13830, Sensitive_Acc : 14.600, Run Time : 11.98 sec
INFO:root:2024-04-10 11:04:03, Train, Epoch : 8, Step : 5070, Loss : 0.26868, Acc : 0.853, Sensitive_Loss : 0.17852, Sensitive_Acc : 15.400, Run Time : 10.02 sec
INFO:root:2024-04-10 11:04:14, Train, Epoch : 8, Step : 5080, Loss : 0.29331, Acc : 0.894, Sensitive_Loss : 0.09205, Sensitive_Acc : 16.400, Run Time : 10.66 sec
INFO:root:2024-04-10 11:04:23, Train, Epoch : 8, Step : 5090, Loss : 0.30240, Acc : 0.884, Sensitive_Loss : 0.09123, Sensitive_Acc : 15.000, Run Time : 9.53 sec
INFO:root:2024-04-10 11:04:33, Train, Epoch : 8, Step : 5100, Loss : 0.29599, Acc : 0.875, Sensitive_Loss : 0.10517, Sensitive_Acc : 15.200, Run Time : 9.41 sec
INFO:root:2024-04-10 11:06:07, Dev, Step : 5100, Loss : 0.55880, Acc : 0.783, Auc : 0.861, Sensitive_Loss : 0.16975, Sensitive_Acc : 16.193, Sensitive_Auc : 0.984, Mean auc: 0.861, Run Time : 94.00 sec
INFO:root:2024-04-10 11:06:14, Train, Epoch : 8, Step : 5110, Loss : 0.35689, Acc : 0.825, Sensitive_Loss : 0.10019, Sensitive_Acc : 16.000, Run Time : 100.78 sec
INFO:root:2024-04-10 11:06:23, Train, Epoch : 8, Step : 5120, Loss : 0.32727, Acc : 0.869, Sensitive_Loss : 0.10128, Sensitive_Acc : 17.000, Run Time : 9.74 sec
INFO:root:2024-04-10 11:06:33, Train, Epoch : 8, Step : 5130, Loss : 0.36222, Acc : 0.847, Sensitive_Loss : 0.11611, Sensitive_Acc : 16.900, Run Time : 9.43 sec
INFO:root:2024-04-10 11:06:42, Train, Epoch : 8, Step : 5140, Loss : 0.33389, Acc : 0.859, Sensitive_Loss : 0.13914, Sensitive_Acc : 14.900, Run Time : 9.42 sec
INFO:root:2024-04-10 11:06:52, Train, Epoch : 8, Step : 5150, Loss : 0.37244, Acc : 0.834, Sensitive_Loss : 0.11437, Sensitive_Acc : 15.200, Run Time : 9.83 sec
INFO:root:2024-04-10 11:08:28
INFO:root:y_pred: [0.15678665 0.03614533 0.9088661  ... 0.6842797  0.6236173  0.07828455]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9968243e-01 3.3286499e-04 6.5058786e-01 9.9994218e-01 9.9938905e-01
 9.8345274e-01 9.9973077e-01 1.3217038e-03 9.9942762e-01 9.9912876e-01
 8.1106490e-01 2.5416470e-01 2.6627013e-04 9.7969925e-01 9.9999499e-01
 9.9992943e-01 9.8466510e-01 9.7143292e-01 9.9772006e-01 9.9778545e-01
 9.9992740e-01 5.1568514e-01 9.9597198e-01 9.1423768e-01 9.8201215e-01
 2.1167323e-02 9.9831831e-01 8.1954405e-02 9.9967742e-01 5.8370717e-03
 2.8329501e-03 2.8665516e-01 6.3130155e-02 9.9967647e-01 3.7428388e-05
 9.9742270e-01 3.9256277e-04 9.9990165e-01 5.1924422e-02 9.9706405e-01
 9.9943131e-01 1.9895156e-04 7.5250189e-03 2.1203067e-04 1.4387424e-01
 5.3112298e-01 9.9641776e-01 9.8863834e-01 9.3628794e-01 9.9251091e-01
 1.6602768e-03 6.9074768e-01 7.9540394e-02 9.5148146e-01 9.9034131e-01
 6.3498124e-02 9.4342738e-01 9.9896026e-01 9.9980778e-01 9.8778959e-03
 3.1076712e-03 9.9743205e-01 4.3712991e-01 9.9947768e-01 9.6849281e-01
 4.8244987e-02 9.3507516e-01 7.6762432e-01 9.9993718e-01 9.9612254e-01
 2.7604995e-03 7.7759045e-01 9.4551373e-01 9.9848217e-01 9.9596596e-01
 3.3299602e-03 5.6757241e-01 1.7998844e-02 2.4091857e-03 9.9999678e-01
 5.8374032e-02 9.9839526e-01 9.9974507e-01 9.9975330e-01 1.2244098e-01
 9.9999702e-01 2.1300046e-03 9.1172270e-02 9.9931002e-01 9.9370331e-01
 1.2854159e-01 6.0704035e-01 3.7910458e-02 9.1160506e-01 4.6346352e-01
 9.9994802e-01 8.7569375e-03 9.9544048e-01 9.7673744e-01 5.3846030e-03
 1.1020467e-03 3.5024011e-01 9.9972922e-01 9.9996567e-01 9.5525128e-01
 9.8017335e-01 9.9854976e-01 3.4844730e-02 3.8947299e-01 9.9525356e-01
 8.5104500e-07 3.8042776e-03 6.7488837e-01 9.9994326e-01 9.9068576e-01
 5.0579367e-04 9.9268568e-01 1.0727605e-02 9.9958342e-01 2.8484467e-01
 9.9961060e-01 9.9988043e-01 2.9015639e-01 7.9098660e-01 7.2103888e-01
 1.2529531e-02 1.4996396e-01 2.6356027e-04 9.3959063e-01 9.9887139e-01
 3.3403680e-02 2.0869472e-03 6.0563767e-03 6.9911271e-01 9.9945885e-01
 9.9326575e-01 9.9319822e-01 5.6283587e-01 2.9076236e-01 9.8460788e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 11:08:28, Dev, Step : 5152, Loss : 0.60267, Acc : 0.767, Auc : 0.855, Sensitive_Loss : 0.18279, Sensitive_Acc : 16.293, Sensitive_Auc : 0.981, Mean auc: 0.855, Run Time : 93.80 sec
INFO:root:2024-04-10 11:08:38, Train, Epoch : 9, Step : 5160, Loss : 0.25652, Acc : 0.694, Sensitive_Loss : 0.07330, Sensitive_Acc : 12.600, Run Time : 9.10 sec
INFO:root:2024-04-10 11:08:48, Train, Epoch : 9, Step : 5170, Loss : 0.30500, Acc : 0.872, Sensitive_Loss : 0.14592, Sensitive_Acc : 17.000, Run Time : 10.41 sec
INFO:root:2024-04-10 11:08:58, Train, Epoch : 9, Step : 5180, Loss : 0.23558, Acc : 0.919, Sensitive_Loss : 0.12068, Sensitive_Acc : 15.300, Run Time : 10.42 sec
INFO:root:2024-04-10 11:09:08, Train, Epoch : 9, Step : 5190, Loss : 0.24219, Acc : 0.878, Sensitive_Loss : 0.14196, Sensitive_Acc : 16.600, Run Time : 9.95 sec
INFO:root:2024-04-10 11:09:18, Train, Epoch : 9, Step : 5200, Loss : 0.34770, Acc : 0.859, Sensitive_Loss : 0.09124, Sensitive_Acc : 15.500, Run Time : 10.03 sec
INFO:root:2024-04-10 11:10:53, Dev, Step : 5200, Loss : 0.54736, Acc : 0.778, Auc : 0.854, Sensitive_Loss : 0.15303, Sensitive_Acc : 16.221, Sensitive_Auc : 0.983, Mean auc: 0.854, Run Time : 94.21 sec
INFO:root:2024-04-10 11:10:59, Train, Epoch : 9, Step : 5210, Loss : 0.33216, Acc : 0.859, Sensitive_Loss : 0.13017, Sensitive_Acc : 16.400, Run Time : 101.10 sec
INFO:root:2024-04-10 11:11:11, Train, Epoch : 9, Step : 5220, Loss : 0.25050, Acc : 0.900, Sensitive_Loss : 0.09107, Sensitive_Acc : 16.600, Run Time : 11.05 sec
INFO:root:2024-04-10 11:11:21, Train, Epoch : 9, Step : 5230, Loss : 0.28878, Acc : 0.875, Sensitive_Loss : 0.10868, Sensitive_Acc : 18.000, Run Time : 10.36 sec
INFO:root:2024-04-10 11:11:31, Train, Epoch : 9, Step : 5240, Loss : 0.29142, Acc : 0.878, Sensitive_Loss : 0.12065, Sensitive_Acc : 15.100, Run Time : 10.11 sec
INFO:root:2024-04-10 11:11:45, Train, Epoch : 9, Step : 5250, Loss : 0.24860, Acc : 0.887, Sensitive_Loss : 0.10950, Sensitive_Acc : 16.700, Run Time : 14.03 sec
INFO:root:2024-04-10 11:11:55, Train, Epoch : 9, Step : 5260, Loss : 0.31659, Acc : 0.872, Sensitive_Loss : 0.14308, Sensitive_Acc : 17.000, Run Time : 9.94 sec
INFO:root:2024-04-10 11:12:05, Train, Epoch : 9, Step : 5270, Loss : 0.34181, Acc : 0.859, Sensitive_Loss : 0.10245, Sensitive_Acc : 16.800, Run Time : 9.84 sec
INFO:root:2024-04-10 11:12:17, Train, Epoch : 9, Step : 5280, Loss : 0.29429, Acc : 0.841, Sensitive_Loss : 0.11404, Sensitive_Acc : 15.800, Run Time : 11.87 sec
INFO:root:2024-04-10 11:12:27, Train, Epoch : 9, Step : 5290, Loss : 0.26566, Acc : 0.903, Sensitive_Loss : 0.12981, Sensitive_Acc : 17.000, Run Time : 10.17 sec
INFO:root:2024-04-10 11:12:37, Train, Epoch : 9, Step : 5300, Loss : 0.32751, Acc : 0.866, Sensitive_Loss : 0.11135, Sensitive_Acc : 16.100, Run Time : 9.65 sec
INFO:root:2024-04-10 11:14:17, Dev, Step : 5300, Loss : 0.54828, Acc : 0.785, Auc : 0.858, Sensitive_Loss : 0.13603, Sensitive_Acc : 16.250, Sensitive_Auc : 0.987, Mean auc: 0.858, Run Time : 100.25 sec
INFO:root:2024-04-10 11:14:24, Train, Epoch : 9, Step : 5310, Loss : 0.24668, Acc : 0.887, Sensitive_Loss : 0.09582, Sensitive_Acc : 15.900, Run Time : 107.00 sec
INFO:root:2024-04-10 11:14:33, Train, Epoch : 9, Step : 5320, Loss : 0.27633, Acc : 0.866, Sensitive_Loss : 0.10881, Sensitive_Acc : 15.800, Run Time : 9.97 sec
INFO:root:2024-04-10 11:14:45, Train, Epoch : 9, Step : 5330, Loss : 0.29395, Acc : 0.859, Sensitive_Loss : 0.12780, Sensitive_Acc : 17.100, Run Time : 11.37 sec
INFO:root:2024-04-10 11:14:55, Train, Epoch : 9, Step : 5340, Loss : 0.31995, Acc : 0.859, Sensitive_Loss : 0.10600, Sensitive_Acc : 16.200, Run Time : 9.93 sec
INFO:root:2024-04-10 11:15:04, Train, Epoch : 9, Step : 5350, Loss : 0.27644, Acc : 0.881, Sensitive_Loss : 0.11984, Sensitive_Acc : 15.300, Run Time : 9.70 sec
INFO:root:2024-04-10 11:15:16, Train, Epoch : 9, Step : 5360, Loss : 0.29508, Acc : 0.881, Sensitive_Loss : 0.13109, Sensitive_Acc : 16.100, Run Time : 11.93 sec
INFO:root:2024-04-10 11:15:27, Train, Epoch : 9, Step : 5370, Loss : 0.29407, Acc : 0.878, Sensitive_Loss : 0.12197, Sensitive_Acc : 15.500, Run Time : 10.98 sec
INFO:root:2024-04-10 11:15:37, Train, Epoch : 9, Step : 5380, Loss : 0.28170, Acc : 0.872, Sensitive_Loss : 0.12136, Sensitive_Acc : 15.000, Run Time : 9.98 sec
INFO:root:2024-04-10 11:15:50, Train, Epoch : 9, Step : 5390, Loss : 0.24763, Acc : 0.891, Sensitive_Loss : 0.11768, Sensitive_Acc : 17.400, Run Time : 12.44 sec
INFO:root:2024-04-10 11:16:00, Train, Epoch : 9, Step : 5400, Loss : 0.29774, Acc : 0.850, Sensitive_Loss : 0.13126, Sensitive_Acc : 16.500, Run Time : 10.58 sec
INFO:root:2024-04-10 11:17:35, Dev, Step : 5400, Loss : 0.55635, Acc : 0.781, Auc : 0.857, Sensitive_Loss : 0.20134, Sensitive_Acc : 16.321, Sensitive_Auc : 0.983, Mean auc: 0.857, Run Time : 94.27 sec
INFO:root:2024-04-10 11:17:44, Train, Epoch : 9, Step : 5410, Loss : 0.35011, Acc : 0.847, Sensitive_Loss : 0.11588, Sensitive_Acc : 16.300, Run Time : 103.21 sec
INFO:root:2024-04-10 11:17:57, Train, Epoch : 9, Step : 5420, Loss : 0.37345, Acc : 0.841, Sensitive_Loss : 0.16938, Sensitive_Acc : 16.100, Run Time : 13.23 sec
INFO:root:2024-04-10 11:18:06, Train, Epoch : 9, Step : 5430, Loss : 0.26514, Acc : 0.863, Sensitive_Loss : 0.11164, Sensitive_Acc : 15.800, Run Time : 9.48 sec
INFO:root:2024-04-10 11:18:20, Train, Epoch : 9, Step : 5440, Loss : 0.36137, Acc : 0.834, Sensitive_Loss : 0.14254, Sensitive_Acc : 18.300, Run Time : 13.74 sec
INFO:root:2024-04-10 11:18:31, Train, Epoch : 9, Step : 5450, Loss : 0.26596, Acc : 0.863, Sensitive_Loss : 0.12854, Sensitive_Acc : 16.800, Run Time : 10.88 sec
INFO:root:2024-04-10 11:18:42, Train, Epoch : 9, Step : 5460, Loss : 0.24008, Acc : 0.909, Sensitive_Loss : 0.12482, Sensitive_Acc : 16.400, Run Time : 11.31 sec
INFO:root:2024-04-10 11:19:00, Train, Epoch : 9, Step : 5470, Loss : 0.31946, Acc : 0.875, Sensitive_Loss : 0.11829, Sensitive_Acc : 16.100, Run Time : 18.12 sec
INFO:root:2024-04-10 11:19:11, Train, Epoch : 9, Step : 5480, Loss : 0.29810, Acc : 0.884, Sensitive_Loss : 0.11981, Sensitive_Acc : 15.700, Run Time : 10.52 sec
INFO:root:2024-04-10 11:19:21, Train, Epoch : 9, Step : 5490, Loss : 0.29700, Acc : 0.884, Sensitive_Loss : 0.10180, Sensitive_Acc : 17.000, Run Time : 10.47 sec
INFO:root:2024-04-10 11:19:37, Train, Epoch : 9, Step : 5500, Loss : 0.26055, Acc : 0.881, Sensitive_Loss : 0.11408, Sensitive_Acc : 16.000, Run Time : 16.12 sec
INFO:root:2024-04-10 11:21:12, Dev, Step : 5500, Loss : 0.57748, Acc : 0.775, Auc : 0.857, Sensitive_Loss : 0.17350, Sensitive_Acc : 16.236, Sensitive_Auc : 0.984, Mean auc: 0.857, Run Time : 94.15 sec
INFO:root:2024-04-10 11:21:22, Train, Epoch : 9, Step : 5510, Loss : 0.33695, Acc : 0.859, Sensitive_Loss : 0.12370, Sensitive_Acc : 16.500, Run Time : 104.60 sec
INFO:root:2024-04-10 11:21:34, Train, Epoch : 9, Step : 5520, Loss : 0.30321, Acc : 0.819, Sensitive_Loss : 0.12272, Sensitive_Acc : 16.800, Run Time : 11.89 sec
INFO:root:2024-04-10 11:21:50, Train, Epoch : 9, Step : 5530, Loss : 0.25266, Acc : 0.891, Sensitive_Loss : 0.10013, Sensitive_Acc : 15.400, Run Time : 15.88 sec
INFO:root:2024-04-10 11:22:03, Train, Epoch : 9, Step : 5540, Loss : 0.34216, Acc : 0.850, Sensitive_Loss : 0.11811, Sensitive_Acc : 16.100, Run Time : 13.47 sec
INFO:root:2024-04-10 11:22:16, Train, Epoch : 9, Step : 5550, Loss : 0.28263, Acc : 0.856, Sensitive_Loss : 0.11319, Sensitive_Acc : 14.600, Run Time : 12.94 sec
INFO:root:2024-04-10 11:22:36, Train, Epoch : 9, Step : 5560, Loss : 0.26972, Acc : 0.872, Sensitive_Loss : 0.11681, Sensitive_Acc : 14.900, Run Time : 19.62 sec
INFO:root:2024-04-10 11:22:47, Train, Epoch : 9, Step : 5570, Loss : 0.31253, Acc : 0.853, Sensitive_Loss : 0.14171, Sensitive_Acc : 17.600, Run Time : 11.58 sec
INFO:root:2024-04-10 11:23:03, Train, Epoch : 9, Step : 5580, Loss : 0.31296, Acc : 0.884, Sensitive_Loss : 0.13359, Sensitive_Acc : 16.900, Run Time : 15.42 sec
INFO:root:2024-04-10 11:23:15, Train, Epoch : 9, Step : 5590, Loss : 0.30765, Acc : 0.869, Sensitive_Loss : 0.11505, Sensitive_Acc : 16.200, Run Time : 12.20 sec
INFO:root:2024-04-10 11:23:26, Train, Epoch : 9, Step : 5600, Loss : 0.28550, Acc : 0.881, Sensitive_Loss : 0.16426, Sensitive_Acc : 17.100, Run Time : 11.30 sec
INFO:root:2024-04-10 11:25:35, Dev, Step : 5600, Loss : 0.54868, Acc : 0.783, Auc : 0.859, Sensitive_Loss : 0.18047, Sensitive_Acc : 16.293, Sensitive_Auc : 0.987, Mean auc: 0.859, Run Time : 128.13 sec
INFO:root:2024-04-10 11:25:46, Train, Epoch : 9, Step : 5610, Loss : 0.35281, Acc : 0.850, Sensitive_Loss : 0.10093, Sensitive_Acc : 14.600, Run Time : 139.75 sec
INFO:root:2024-04-10 11:26:07, Train, Epoch : 9, Step : 5620, Loss : 0.35530, Acc : 0.856, Sensitive_Loss : 0.11111, Sensitive_Acc : 16.000, Run Time : 20.94 sec
INFO:root:2024-04-10 11:26:21, Train, Epoch : 9, Step : 5630, Loss : 0.29032, Acc : 0.887, Sensitive_Loss : 0.10879, Sensitive_Acc : 16.200, Run Time : 14.22 sec
INFO:root:2024-04-10 11:26:49, Train, Epoch : 9, Step : 5640, Loss : 0.30005, Acc : 0.881, Sensitive_Loss : 0.13148, Sensitive_Acc : 16.700, Run Time : 27.55 sec
INFO:root:2024-04-10 11:27:09, Train, Epoch : 9, Step : 5650, Loss : 0.26973, Acc : 0.853, Sensitive_Loss : 0.13492, Sensitive_Acc : 17.300, Run Time : 20.26 sec
INFO:root:2024-04-10 11:27:40, Train, Epoch : 9, Step : 5660, Loss : 0.26162, Acc : 0.881, Sensitive_Loss : 0.12940, Sensitive_Acc : 15.900, Run Time : 30.93 sec
INFO:root:2024-04-10 11:28:06, Train, Epoch : 9, Step : 5670, Loss : 0.28694, Acc : 0.878, Sensitive_Loss : 0.12790, Sensitive_Acc : 16.000, Run Time : 25.61 sec
INFO:root:2024-04-10 11:28:21, Train, Epoch : 9, Step : 5680, Loss : 0.38320, Acc : 0.828, Sensitive_Loss : 0.13965, Sensitive_Acc : 16.500, Run Time : 15.79 sec
INFO:root:2024-04-10 11:28:38, Train, Epoch : 9, Step : 5690, Loss : 0.33621, Acc : 0.853, Sensitive_Loss : 0.09941, Sensitive_Acc : 17.100, Run Time : 17.02 sec
INFO:root:2024-04-10 11:28:53, Train, Epoch : 9, Step : 5700, Loss : 0.26101, Acc : 0.869, Sensitive_Loss : 0.09909, Sensitive_Acc : 16.500, Run Time : 15.06 sec
INFO:root:2024-04-10 11:31:57, Dev, Step : 5700, Loss : 0.56474, Acc : 0.776, Auc : 0.852, Sensitive_Loss : 0.17174, Sensitive_Acc : 16.336, Sensitive_Auc : 0.986, Mean auc: 0.852, Run Time : 183.58 sec
INFO:root:2024-04-10 11:32:04, Train, Epoch : 9, Step : 5710, Loss : 0.34020, Acc : 0.872, Sensitive_Loss : 0.12922, Sensitive_Acc : 15.400, Run Time : 190.48 sec
INFO:root:2024-04-10 11:32:15, Train, Epoch : 9, Step : 5720, Loss : 0.37184, Acc : 0.853, Sensitive_Loss : 0.10148, Sensitive_Acc : 15.600, Run Time : 10.98 sec
INFO:root:2024-04-10 11:32:28, Train, Epoch : 9, Step : 5730, Loss : 0.29523, Acc : 0.872, Sensitive_Loss : 0.09139, Sensitive_Acc : 16.100, Run Time : 13.19 sec
INFO:root:2024-04-10 11:32:38, Train, Epoch : 9, Step : 5740, Loss : 0.28296, Acc : 0.878, Sensitive_Loss : 0.13305, Sensitive_Acc : 15.000, Run Time : 9.97 sec
INFO:root:2024-04-10 11:32:50, Train, Epoch : 9, Step : 5750, Loss : 0.34678, Acc : 0.853, Sensitive_Loss : 0.08253, Sensitive_Acc : 15.500, Run Time : 11.87 sec
INFO:root:2024-04-10 11:33:02, Train, Epoch : 9, Step : 5760, Loss : 0.25333, Acc : 0.881, Sensitive_Loss : 0.12853, Sensitive_Acc : 17.100, Run Time : 12.37 sec
INFO:root:2024-04-10 11:33:12, Train, Epoch : 9, Step : 5770, Loss : 0.28377, Acc : 0.872, Sensitive_Loss : 0.07208, Sensitive_Acc : 16.200, Run Time : 9.35 sec
INFO:root:2024-04-10 11:33:23, Train, Epoch : 9, Step : 5780, Loss : 0.26472, Acc : 0.884, Sensitive_Loss : 0.12111, Sensitive_Acc : 17.000, Run Time : 11.46 sec
INFO:root:2024-04-10 11:33:37, Train, Epoch : 9, Step : 5790, Loss : 0.29948, Acc : 0.872, Sensitive_Loss : 0.11031, Sensitive_Acc : 15.800, Run Time : 13.44 sec
INFO:root:2024-04-10 11:35:16
INFO:root:y_pred: [0.20965376 0.14286272 0.83503616 ... 0.79300064 0.62184507 0.02600247]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.9974948e-01 4.9275387e-04 6.6251630e-01 9.9980038e-01 9.9966562e-01
 9.8282683e-01 9.9971384e-01 5.3884933e-04 9.9787486e-01 9.9929559e-01
 7.3415941e-01 2.4051343e-01 2.5607480e-04 9.8077422e-01 9.9999630e-01
 9.9990964e-01 9.6585566e-01 9.5459050e-01 9.9551624e-01 9.9867046e-01
 9.9989617e-01 7.5637656e-01 9.9546790e-01 9.2609793e-01 9.9172759e-01
 1.1292150e-02 9.9754936e-01 4.8909992e-02 9.9962163e-01 6.3542365e-03
 2.2923793e-03 1.5033606e-01 3.0456567e-02 9.9977332e-01 8.1815888e-05
 9.9639267e-01 3.1047253e-04 9.9977010e-01 7.6370038e-02 9.9691916e-01
 9.9965990e-01 1.4991587e-04 6.5802387e-03 3.2991861e-04 1.5015072e-01
 4.9927971e-01 9.9584889e-01 9.9085367e-01 9.2387390e-01 9.8763716e-01
 9.5524435e-04 5.7477736e-01 3.4956124e-02 8.7067890e-01 9.9098951e-01
 4.6930742e-02 9.2706645e-01 9.9803931e-01 9.9945968e-01 1.4772924e-02
 1.8825210e-03 9.9681216e-01 4.1677263e-01 9.9959749e-01 9.7996026e-01
 3.2658298e-02 9.0803963e-01 6.0842609e-01 9.9995875e-01 9.9220651e-01
 5.1567429e-03 7.8849936e-01 9.6823299e-01 9.9814010e-01 9.9210852e-01
 4.0974263e-03 2.9831231e-01 1.7918779e-02 1.5948754e-03 9.9998569e-01
 3.6325287e-02 9.9695349e-01 9.9950683e-01 9.9966967e-01 1.8178055e-01
 9.9999571e-01 1.4778706e-03 1.1102962e-01 9.9710053e-01 9.9093741e-01
 3.5817474e-02 6.0076416e-01 2.2162780e-02 9.0690398e-01 1.5764511e-01
 9.9997830e-01 1.0789457e-02 9.9428940e-01 9.7807795e-01 1.6068730e-03
 5.3767778e-04 3.3407864e-01 9.9954140e-01 9.9994814e-01 9.1813534e-01
 9.5319927e-01 9.9804354e-01 1.7115165e-02 3.5494858e-01 9.9437654e-01
 7.1140306e-07 2.9306933e-03 5.7204783e-01 9.9992430e-01 9.8464382e-01
 6.7866989e-04 9.9109966e-01 4.0847459e-03 9.9939466e-01 3.0129594e-01
 9.9922311e-01 9.9987972e-01 1.3752754e-01 7.6466018e-01 5.3432661e-01
 8.0736568e-03 1.7947274e-01 2.4614928e-04 9.1718823e-01 9.9810070e-01
 1.8817998e-02 3.3753321e-03 9.4185714e-03 6.6795522e-01 9.9831808e-01
 9.9366921e-01 9.8416704e-01 6.5324450e-01 2.1685541e-01 9.4904149e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 11:35:16, Dev, Step : 5796, Loss : 0.59324, Acc : 0.771, Auc : 0.852, Sensitive_Loss : 0.16335, Sensitive_Acc : 16.264, Sensitive_Auc : 0.984, Mean auc: 0.852, Run Time : 93.87 sec
INFO:root:2024-04-10 11:35:22, Train, Epoch : 10, Step : 5800, Loss : 0.10592, Acc : 0.359, Sensitive_Loss : 0.05358, Sensitive_Acc : 6.500, Run Time : 4.78 sec
INFO:root:2024-04-10 11:36:56, Dev, Step : 5800, Loss : 0.58845, Acc : 0.771, Auc : 0.852, Sensitive_Loss : 0.16680, Sensitive_Acc : 16.321, Sensitive_Auc : 0.984, Mean auc: 0.852, Run Time : 93.85 sec
INFO:root:2024-04-10 11:37:02, Train, Epoch : 10, Step : 5810, Loss : 0.23461, Acc : 0.903, Sensitive_Loss : 0.10852, Sensitive_Acc : 14.700, Run Time : 100.50 sec
INFO:root:2024-04-10 11:37:12, Train, Epoch : 10, Step : 5820, Loss : 0.26690, Acc : 0.872, Sensitive_Loss : 0.12031, Sensitive_Acc : 16.600, Run Time : 9.66 sec
INFO:root:2024-04-10 11:37:22, Train, Epoch : 10, Step : 5830, Loss : 0.24330, Acc : 0.884, Sensitive_Loss : 0.13815, Sensitive_Acc : 14.900, Run Time : 10.15 sec
INFO:root:2024-04-10 11:37:35, Train, Epoch : 10, Step : 5840, Loss : 0.32862, Acc : 0.853, Sensitive_Loss : 0.09373, Sensitive_Acc : 15.900, Run Time : 12.41 sec
INFO:root:2024-04-10 11:37:46, Train, Epoch : 10, Step : 5850, Loss : 0.26335, Acc : 0.887, Sensitive_Loss : 0.10667, Sensitive_Acc : 17.200, Run Time : 11.54 sec
INFO:root:2024-04-10 11:37:56, Train, Epoch : 10, Step : 5860, Loss : 0.23990, Acc : 0.909, Sensitive_Loss : 0.11945, Sensitive_Acc : 15.500, Run Time : 10.32 sec
INFO:root:2024-04-10 11:38:10, Train, Epoch : 10, Step : 5870, Loss : 0.31853, Acc : 0.881, Sensitive_Loss : 0.14533, Sensitive_Acc : 15.700, Run Time : 13.59 sec
INFO:root:2024-04-10 11:38:21, Train, Epoch : 10, Step : 5880, Loss : 0.27319, Acc : 0.878, Sensitive_Loss : 0.08921, Sensitive_Acc : 16.900, Run Time : 10.90 sec
INFO:root:2024-04-10 11:38:32, Train, Epoch : 10, Step : 5890, Loss : 0.29499, Acc : 0.891, Sensitive_Loss : 0.09674, Sensitive_Acc : 15.800, Run Time : 10.85 sec
INFO:root:2024-04-10 11:38:46, Train, Epoch : 10, Step : 5900, Loss : 0.29165, Acc : 0.869, Sensitive_Loss : 0.07954, Sensitive_Acc : 17.200, Run Time : 13.98 sec
INFO:root:2024-04-10 11:40:22, Dev, Step : 5900, Loss : 0.55061, Acc : 0.779, Auc : 0.854, Sensitive_Loss : 0.16175, Sensitive_Acc : 16.293, Sensitive_Auc : 0.985, Mean auc: 0.854, Run Time : 95.95 sec
INFO:root:2024-04-10 11:40:32, Train, Epoch : 10, Step : 5910, Loss : 0.22327, Acc : 0.875, Sensitive_Loss : 0.10726, Sensitive_Acc : 16.900, Run Time : 106.21 sec
INFO:root:2024-04-10 11:40:45, Train, Epoch : 10, Step : 5920, Loss : 0.24169, Acc : 0.916, Sensitive_Loss : 0.15144, Sensitive_Acc : 16.900, Run Time : 12.88 sec
INFO:root:2024-04-10 11:41:03, Train, Epoch : 10, Step : 5930, Loss : 0.26554, Acc : 0.897, Sensitive_Loss : 0.15890, Sensitive_Acc : 14.500, Run Time : 18.30 sec
INFO:root:2024-04-10 11:41:15, Train, Epoch : 10, Step : 5940, Loss : 0.29394, Acc : 0.875, Sensitive_Loss : 0.14160, Sensitive_Acc : 14.500, Run Time : 11.41 sec
INFO:root:2024-04-10 11:41:30, Train, Epoch : 10, Step : 5950, Loss : 0.29694, Acc : 0.887, Sensitive_Loss : 0.10782, Sensitive_Acc : 17.400, Run Time : 15.34 sec
INFO:root:2024-04-10 11:41:43, Train, Epoch : 10, Step : 5960, Loss : 0.24029, Acc : 0.912, Sensitive_Loss : 0.09617, Sensitive_Acc : 16.600, Run Time : 13.59 sec
INFO:root:2024-04-10 11:41:57, Train, Epoch : 10, Step : 5970, Loss : 0.25578, Acc : 0.856, Sensitive_Loss : 0.09181, Sensitive_Acc : 15.900, Run Time : 13.22 sec
INFO:root:2024-04-10 11:42:15, Train, Epoch : 10, Step : 5980, Loss : 0.24223, Acc : 0.894, Sensitive_Loss : 0.10031, Sensitive_Acc : 16.100, Run Time : 17.88 sec
INFO:root:2024-04-10 11:42:26, Train, Epoch : 10, Step : 5990, Loss : 0.32536, Acc : 0.853, Sensitive_Loss : 0.13515, Sensitive_Acc : 16.100, Run Time : 11.02 sec
INFO:root:2024-04-10 11:42:37, Train, Epoch : 10, Step : 6000, Loss : 0.24879, Acc : 0.881, Sensitive_Loss : 0.18587, Sensitive_Acc : 17.700, Run Time : 11.17 sec
INFO:root:2024-04-10 11:44:39, Dev, Step : 6000, Loss : 0.59286, Acc : 0.775, Auc : 0.852, Sensitive_Loss : 0.14826, Sensitive_Acc : 16.321, Sensitive_Auc : 0.984, Mean auc: 0.852, Run Time : 122.21 sec
INFO:root:2024-04-10 11:44:47, Train, Epoch : 10, Step : 6010, Loss : 0.25194, Acc : 0.866, Sensitive_Loss : 0.18190, Sensitive_Acc : 15.800, Run Time : 130.32 sec
INFO:root:2024-04-10 11:45:02, Train, Epoch : 10, Step : 6020, Loss : 0.32108, Acc : 0.856, Sensitive_Loss : 0.10563, Sensitive_Acc : 15.100, Run Time : 14.85 sec
INFO:root:2024-04-10 11:45:17, Train, Epoch : 10, Step : 6030, Loss : 0.27512, Acc : 0.866, Sensitive_Loss : 0.11113, Sensitive_Acc : 15.900, Run Time : 14.83 sec
INFO:root:2024-04-10 11:45:28, Train, Epoch : 10, Step : 6040, Loss : 0.30633, Acc : 0.866, Sensitive_Loss : 0.10636, Sensitive_Acc : 16.000, Run Time : 11.67 sec
INFO:root:2024-04-10 11:45:44, Train, Epoch : 10, Step : 6050, Loss : 0.23747, Acc : 0.878, Sensitive_Loss : 0.09326, Sensitive_Acc : 15.700, Run Time : 15.33 sec
INFO:root:2024-04-10 11:45:55, Train, Epoch : 10, Step : 6060, Loss : 0.28097, Acc : 0.872, Sensitive_Loss : 0.12604, Sensitive_Acc : 15.600, Run Time : 11.12 sec
INFO:root:2024-04-10 11:46:06, Train, Epoch : 10, Step : 6070, Loss : 0.29217, Acc : 0.863, Sensitive_Loss : 0.10878, Sensitive_Acc : 15.700, Run Time : 11.17 sec
INFO:root:2024-04-10 11:46:23, Train, Epoch : 10, Step : 6080, Loss : 0.28913, Acc : 0.875, Sensitive_Loss : 0.11843, Sensitive_Acc : 17.900, Run Time : 16.56 sec
INFO:root:2024-04-10 11:46:35, Train, Epoch : 10, Step : 6090, Loss : 0.28821, Acc : 0.863, Sensitive_Loss : 0.12164, Sensitive_Acc : 16.000, Run Time : 12.61 sec
INFO:root:2024-04-10 11:46:53, Train, Epoch : 10, Step : 6100, Loss : 0.29197, Acc : 0.850, Sensitive_Loss : 0.13004, Sensitive_Acc : 15.100, Run Time : 18.21 sec
INFO:root:2024-04-10 11:48:31, Dev, Step : 6100, Loss : 0.57184, Acc : 0.776, Auc : 0.855, Sensitive_Loss : 0.15585, Sensitive_Acc : 16.364, Sensitive_Auc : 0.987, Mean auc: 0.855, Run Time : 97.12 sec
INFO:root:2024-04-10 11:48:40, Train, Epoch : 10, Step : 6110, Loss : 0.26581, Acc : 0.900, Sensitive_Loss : 0.15573, Sensitive_Acc : 17.100, Run Time : 106.68 sec
INFO:root:2024-04-10 11:48:53, Train, Epoch : 10, Step : 6120, Loss : 0.25626, Acc : 0.881, Sensitive_Loss : 0.12945, Sensitive_Acc : 16.800, Run Time : 12.68 sec
INFO:root:2024-04-10 11:49:04, Train, Epoch : 10, Step : 6130, Loss : 0.28610, Acc : 0.881, Sensitive_Loss : 0.15085, Sensitive_Acc : 16.600, Run Time : 10.98 sec
INFO:root:2024-04-10 11:49:17, Train, Epoch : 10, Step : 6140, Loss : 0.21840, Acc : 0.894, Sensitive_Loss : 0.11605, Sensitive_Acc : 14.800, Run Time : 13.63 sec
INFO:root:2024-04-10 11:49:30, Train, Epoch : 10, Step : 6150, Loss : 0.24720, Acc : 0.887, Sensitive_Loss : 0.16562, Sensitive_Acc : 16.200, Run Time : 12.40 sec
INFO:root:2024-04-10 11:49:40, Train, Epoch : 10, Step : 6160, Loss : 0.26329, Acc : 0.875, Sensitive_Loss : 0.13033, Sensitive_Acc : 16.400, Run Time : 9.86 sec
INFO:root:2024-04-10 11:49:53, Train, Epoch : 10, Step : 6170, Loss : 0.30482, Acc : 0.884, Sensitive_Loss : 0.17304, Sensitive_Acc : 14.700, Run Time : 13.04 sec
INFO:root:2024-04-10 11:50:04, Train, Epoch : 10, Step : 6180, Loss : 0.33399, Acc : 0.881, Sensitive_Loss : 0.11653, Sensitive_Acc : 15.800, Run Time : 11.42 sec
INFO:root:2024-04-10 11:50:13, Train, Epoch : 10, Step : 6190, Loss : 0.28453, Acc : 0.884, Sensitive_Loss : 0.08966, Sensitive_Acc : 15.300, Run Time : 9.32 sec
INFO:root:2024-04-10 11:50:29, Train, Epoch : 10, Step : 6200, Loss : 0.22481, Acc : 0.894, Sensitive_Loss : 0.12470, Sensitive_Acc : 18.300, Run Time : 15.12 sec
INFO:root:2024-04-10 11:52:04, Dev, Step : 6200, Loss : 0.59353, Acc : 0.770, Auc : 0.853, Sensitive_Loss : 0.16052, Sensitive_Acc : 16.321, Sensitive_Auc : 0.987, Mean auc: 0.853, Run Time : 95.66 sec
INFO:root:2024-04-10 11:52:13, Train, Epoch : 10, Step : 6210, Loss : 0.32184, Acc : 0.838, Sensitive_Loss : 0.11140, Sensitive_Acc : 17.000, Run Time : 104.15 sec
INFO:root:2024-04-10 11:52:25, Train, Epoch : 10, Step : 6220, Loss : 0.25342, Acc : 0.900, Sensitive_Loss : 0.09826, Sensitive_Acc : 18.700, Run Time : 12.11 sec
INFO:root:2024-04-10 11:52:37, Train, Epoch : 10, Step : 6230, Loss : 0.23855, Acc : 0.903, Sensitive_Loss : 0.12407, Sensitive_Acc : 16.600, Run Time : 12.40 sec
INFO:root:2024-04-10 11:52:49, Train, Epoch : 10, Step : 6240, Loss : 0.27257, Acc : 0.884, Sensitive_Loss : 0.10868, Sensitive_Acc : 14.500, Run Time : 11.97 sec
INFO:root:2024-04-10 11:52:59, Train, Epoch : 10, Step : 6250, Loss : 0.26607, Acc : 0.863, Sensitive_Loss : 0.14971, Sensitive_Acc : 15.500, Run Time : 9.63 sec
INFO:root:2024-04-10 11:53:14, Train, Epoch : 10, Step : 6260, Loss : 0.24529, Acc : 0.891, Sensitive_Loss : 0.11608, Sensitive_Acc : 16.400, Run Time : 15.22 sec
INFO:root:2024-04-10 11:53:25, Train, Epoch : 10, Step : 6270, Loss : 0.24893, Acc : 0.884, Sensitive_Loss : 0.14969, Sensitive_Acc : 16.500, Run Time : 10.64 sec
INFO:root:2024-04-10 11:53:35, Train, Epoch : 10, Step : 6280, Loss : 0.29109, Acc : 0.884, Sensitive_Loss : 0.11340, Sensitive_Acc : 16.800, Run Time : 10.18 sec
INFO:root:2024-04-10 11:53:46, Train, Epoch : 10, Step : 6290, Loss : 0.29807, Acc : 0.884, Sensitive_Loss : 0.13529, Sensitive_Acc : 16.000, Run Time : 10.82 sec
INFO:root:2024-04-10 11:53:58, Train, Epoch : 10, Step : 6300, Loss : 0.31880, Acc : 0.856, Sensitive_Loss : 0.09195, Sensitive_Acc : 16.100, Run Time : 12.33 sec
INFO:root:2024-04-10 11:55:33, Dev, Step : 6300, Loss : 0.60901, Acc : 0.772, Auc : 0.853, Sensitive_Loss : 0.17081, Sensitive_Acc : 16.307, Sensitive_Auc : 0.988, Mean auc: 0.853, Run Time : 94.59 sec
INFO:root:2024-04-10 11:55:42, Train, Epoch : 10, Step : 6310, Loss : 0.29528, Acc : 0.887, Sensitive_Loss : 0.09270, Sensitive_Acc : 16.600, Run Time : 104.45 sec
INFO:root:2024-04-10 11:55:53, Train, Epoch : 10, Step : 6320, Loss : 0.30652, Acc : 0.863, Sensitive_Loss : 0.12760, Sensitive_Acc : 16.700, Run Time : 10.45 sec
INFO:root:2024-04-10 11:56:05, Train, Epoch : 10, Step : 6330, Loss : 0.25057, Acc : 0.891, Sensitive_Loss : 0.10252, Sensitive_Acc : 15.900, Run Time : 12.43 sec
INFO:root:2024-04-10 11:56:16, Train, Epoch : 10, Step : 6340, Loss : 0.26338, Acc : 0.894, Sensitive_Loss : 0.12467, Sensitive_Acc : 17.700, Run Time : 11.03 sec
INFO:root:2024-04-10 11:56:27, Train, Epoch : 10, Step : 6350, Loss : 0.29024, Acc : 0.869, Sensitive_Loss : 0.11792, Sensitive_Acc : 16.800, Run Time : 10.60 sec
INFO:root:2024-04-10 11:56:39, Train, Epoch : 10, Step : 6360, Loss : 0.29763, Acc : 0.869, Sensitive_Loss : 0.08779, Sensitive_Acc : 15.200, Run Time : 11.72 sec
INFO:root:2024-04-10 11:56:48, Train, Epoch : 10, Step : 6370, Loss : 0.29864, Acc : 0.881, Sensitive_Loss : 0.12359, Sensitive_Acc : 18.700, Run Time : 9.27 sec
INFO:root:2024-04-10 11:57:01, Train, Epoch : 10, Step : 6380, Loss : 0.26730, Acc : 0.884, Sensitive_Loss : 0.09754, Sensitive_Acc : 16.800, Run Time : 12.88 sec
INFO:root:2024-04-10 11:57:14, Train, Epoch : 10, Step : 6390, Loss : 0.27861, Acc : 0.850, Sensitive_Loss : 0.12159, Sensitive_Acc : 14.700, Run Time : 12.81 sec
INFO:root:2024-04-10 11:57:23, Train, Epoch : 10, Step : 6400, Loss : 0.28537, Acc : 0.878, Sensitive_Loss : 0.15081, Sensitive_Acc : 18.900, Run Time : 9.60 sec
INFO:root:2024-04-10 11:59:12, Dev, Step : 6400, Loss : 0.57344, Acc : 0.777, Auc : 0.851, Sensitive_Loss : 0.15904, Sensitive_Acc : 16.293, Sensitive_Auc : 0.987, Mean auc: 0.851, Run Time : 108.28 sec
INFO:root:2024-04-10 11:59:21, Train, Epoch : 10, Step : 6410, Loss : 0.25718, Acc : 0.897, Sensitive_Loss : 0.09225, Sensitive_Acc : 17.100, Run Time : 117.49 sec
INFO:root:2024-04-10 11:59:34, Train, Epoch : 10, Step : 6420, Loss : 0.30465, Acc : 0.866, Sensitive_Loss : 0.09347, Sensitive_Acc : 15.000, Run Time : 12.96 sec
INFO:root:2024-04-10 11:59:46, Train, Epoch : 10, Step : 6430, Loss : 0.28710, Acc : 0.844, Sensitive_Loss : 0.14408, Sensitive_Acc : 16.700, Run Time : 11.93 sec
INFO:root:2024-04-10 11:59:57, Train, Epoch : 10, Step : 6440, Loss : 0.26996, Acc : 0.875, Sensitive_Loss : 0.11484, Sensitive_Acc : 15.700, Run Time : 11.39 sec
INFO:root:2024-04-10 12:01:32
INFO:root:y_pred: [0.23341286 0.10401424 0.9490113  ... 0.77152467 0.70921886 0.01226747]
INFO:root:y_true: [0. 0. 1. ... 0. 1. 0.]
INFO:root:sensitive_y_pred: [9.99823749e-01 4.01413301e-04 3.02115619e-01 9.99872327e-01
 9.99295712e-01 9.78892565e-01 9.99670148e-01 6.76368305e-04
 9.92731035e-01 9.95546460e-01 5.60682058e-01 3.73986751e-01
 1.52778544e-03 9.84767497e-01 9.99982119e-01 9.99938250e-01
 9.88111854e-01 9.79466200e-01 9.97981429e-01 9.98289049e-01
 9.98980820e-01 3.38106006e-01 9.95335281e-01 9.02633369e-01
 9.89970863e-01 1.25324996e-02 9.95906472e-01 3.94532271e-02
 9.99698400e-01 3.35400621e-03 1.84369716e-03 1.45918712e-01
 1.71583090e-02 9.98040974e-01 4.79561677e-05 9.96524513e-01
 2.70797551e-04 9.99836802e-01 1.16410881e-01 9.98685181e-01
 9.99531865e-01 5.06915894e-05 9.25261341e-03 2.85872607e-04
 1.28074348e-01 6.48320675e-01 9.96737063e-01 9.96497989e-01
 9.14148986e-01 9.81754601e-01 1.01615791e-03 5.76922596e-01
 3.78771238e-02 6.57012939e-01 9.90448058e-01 9.45090577e-02
 9.51233566e-01 9.98902082e-01 9.92848873e-01 1.65230706e-02
 1.47727109e-03 9.96365309e-01 3.52658778e-01 9.99611914e-01
 9.85963643e-01 3.12263593e-02 9.23766494e-01 7.37726450e-01
 9.99928355e-01 9.91592884e-01 8.19028448e-03 8.61102104e-01
 9.56599236e-01 9.98628855e-01 9.96737659e-01 7.44959665e-03
 8.27286184e-01 5.02308179e-03 1.19616592e-03 9.99507666e-01
 8.26821253e-02 9.98135567e-01 9.99736369e-01 9.99885321e-01
 8.95338729e-02 9.99986529e-01 1.62965723e-03 6.89560995e-02
 9.96235311e-01 9.95563984e-01 9.63737443e-02 3.13495040e-01
 6.25395849e-02 7.67476439e-01 1.88385129e-01 9.99879479e-01
 7.47693842e-03 9.93117929e-01 9.85676765e-01 2.12725857e-03
 4.89179802e-04 2.54846603e-01 9.99494433e-01 9.99944210e-01
 9.73831058e-01 8.94374847e-01 9.99029398e-01 9.97792557e-03
 3.95449311e-01 9.92338538e-01 1.07195575e-07 4.74878633e-03
 6.98266625e-01 9.99867320e-01 9.87325668e-01 1.03776704e-03
 9.85273540e-01 1.39445881e-03 9.99365032e-01 2.83529639e-01
 9.97862637e-01 9.99648809e-01 3.05766702e-01 7.58612037e-01
 4.25999820e-01 8.30835290e-03 2.20573366e-01 5.03706862e-04
 8.93249631e-01 9.97977793e-01 8.14935714e-02 2.97900662e-03
 4.12356481e-03 5.76971471e-01 9.98698711e-01 9.92030978e-01
 9.84064639e-01 8.68530333e-01 3.33575696e-01 9.70924497e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.
 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.
 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0.
 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0.
 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-10 12:01:32, Dev, Step : 6440, Loss : 0.60117, Acc : 0.769, Auc : 0.850, Sensitive_Loss : 0.16451, Sensitive_Acc : 16.364, Sensitive_Auc : 0.988, Mean auc: 0.850, Run Time : 94.54 sec
