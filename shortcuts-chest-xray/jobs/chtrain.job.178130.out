Running on desktop22:
stdin: is not a tty
Activating chexpert environment...
3
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_val.csv",
    "pred_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Biased_Sex_1_pos01.csv",
    "pred_model": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Biased_Sex_1_pos011.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.1,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-26 18:50:43, Train, Epoch : 1, Step : 10, Loss : 0.73651, Acc : 0.578, Sensitive_Loss : 0.61476, Sensitive_Acc : 17.200, Run Time : 10.26 sec
INFO:root:2024-04-26 18:50:51, Train, Epoch : 1, Step : 20, Loss : 0.65356, Acc : 0.609, Sensitive_Loss : 0.61254, Sensitive_Acc : 16.200, Run Time : 7.86 sec
INFO:root:2024-04-26 18:50:59, Train, Epoch : 1, Step : 30, Loss : 0.60136, Acc : 0.700, Sensitive_Loss : 0.52618, Sensitive_Acc : 15.200, Run Time : 8.02 sec
INFO:root:2024-04-26 18:51:06, Train, Epoch : 1, Step : 40, Loss : 0.61309, Acc : 0.700, Sensitive_Loss : 0.53652, Sensitive_Acc : 16.500, Run Time : 7.17 sec
INFO:root:2024-04-26 18:51:14, Train, Epoch : 1, Step : 50, Loss : 0.54065, Acc : 0.691, Sensitive_Loss : 0.44252, Sensitive_Acc : 16.200, Run Time : 7.60 sec
INFO:root:2024-04-26 18:51:22, Train, Epoch : 1, Step : 60, Loss : 0.53799, Acc : 0.759, Sensitive_Loss : 0.43648, Sensitive_Acc : 15.600, Run Time : 7.89 sec
INFO:root:2024-04-26 18:51:30, Train, Epoch : 1, Step : 70, Loss : 0.56455, Acc : 0.734, Sensitive_Loss : 0.40762, Sensitive_Acc : 16.100, Run Time : 8.50 sec
INFO:root:2024-04-26 18:51:38, Train, Epoch : 1, Step : 80, Loss : 0.60368, Acc : 0.731, Sensitive_Loss : 0.29841, Sensitive_Acc : 17.400, Run Time : 7.86 sec
INFO:root:2024-04-26 18:51:46, Train, Epoch : 1, Step : 90, Loss : 0.48553, Acc : 0.753, Sensitive_Loss : 0.41007, Sensitive_Acc : 16.400, Run Time : 7.86 sec
INFO:root:2024-04-26 18:51:54, Train, Epoch : 1, Step : 100, Loss : 0.50050, Acc : 0.753, Sensitive_Loss : 0.30653, Sensitive_Acc : 16.800, Run Time : 8.01 sec
INFO:root:2024-04-26 18:53:37, Dev, Step : 100, Loss : 0.53072, Acc : 0.757, Auc : 0.837, Sensitive_Loss : 0.32789, Sensitive_Acc : 16.850, Sensitive_Auc : 0.925, Mean auc: 0.837, Run Time : 103.69 sec
INFO:root:2024-04-26 18:53:38, Best, Step : 100, Loss : 0.53072, Acc : 0.757, Auc : 0.837, Sensitive_Loss : 0.32789, Sensitive_Acc : 16.850, Sensitive_Auc : 0.925, Best Auc : 0.837
INFO:root:2024-04-26 18:53:44, Train, Epoch : 1, Step : 110, Loss : 0.55372, Acc : 0.734, Sensitive_Loss : 0.29621, Sensitive_Acc : 16.500, Run Time : 110.44 sec
INFO:root:2024-04-26 18:53:52, Train, Epoch : 1, Step : 120, Loss : 0.56380, Acc : 0.759, Sensitive_Loss : 0.28928, Sensitive_Acc : 15.800, Run Time : 7.86 sec
INFO:root:2024-04-26 18:54:00, Train, Epoch : 1, Step : 130, Loss : 0.54152, Acc : 0.784, Sensitive_Loss : 0.30832, Sensitive_Acc : 16.400, Run Time : 8.25 sec
INFO:root:2024-04-26 18:54:08, Train, Epoch : 1, Step : 140, Loss : 0.49083, Acc : 0.812, Sensitive_Loss : 0.29068, Sensitive_Acc : 15.400, Run Time : 8.10 sec
INFO:root:2024-04-26 18:54:16, Train, Epoch : 1, Step : 150, Loss : 0.48108, Acc : 0.762, Sensitive_Loss : 0.30929, Sensitive_Acc : 15.000, Run Time : 7.96 sec
INFO:root:2024-04-26 18:54:24, Train, Epoch : 1, Step : 160, Loss : 0.46256, Acc : 0.775, Sensitive_Loss : 0.29909, Sensitive_Acc : 16.000, Run Time : 7.86 sec
INFO:root:2024-04-26 18:54:32, Train, Epoch : 1, Step : 170, Loss : 0.55046, Acc : 0.753, Sensitive_Loss : 0.27666, Sensitive_Acc : 15.800, Run Time : 7.93 sec
INFO:root:2024-04-26 18:54:40, Train, Epoch : 1, Step : 180, Loss : 0.49502, Acc : 0.772, Sensitive_Loss : 0.28553, Sensitive_Acc : 16.100, Run Time : 7.73 sec
INFO:root:2024-04-26 18:54:48, Train, Epoch : 1, Step : 190, Loss : 0.45207, Acc : 0.803, Sensitive_Loss : 0.23466, Sensitive_Acc : 15.100, Run Time : 8.03 sec
INFO:root:2024-04-26 18:54:56, Train, Epoch : 1, Step : 200, Loss : 0.46566, Acc : 0.784, Sensitive_Loss : 0.27756, Sensitive_Acc : 16.100, Run Time : 7.78 sec
INFO:root:2024-04-26 18:56:30, Dev, Step : 200, Loss : 0.58870, Acc : 0.744, Auc : 0.869, Sensitive_Loss : 0.27766, Sensitive_Acc : 16.907, Sensitive_Auc : 0.973, Mean auc: 0.869, Run Time : 93.89 sec
INFO:root:2024-04-26 18:56:30, Best, Step : 200, Loss : 0.58870, Acc : 0.744, Auc : 0.869, Sensitive_Loss : 0.27766, Sensitive_Acc : 16.907, Sensitive_Auc : 0.973, Best Auc : 0.869
INFO:root:2024-04-26 18:56:36, Train, Epoch : 1, Step : 210, Loss : 0.50858, Acc : 0.762, Sensitive_Loss : 0.27417, Sensitive_Acc : 16.700, Run Time : 100.38 sec
INFO:root:2024-04-26 18:56:44, Train, Epoch : 1, Step : 220, Loss : 0.45526, Acc : 0.784, Sensitive_Loss : 0.27319, Sensitive_Acc : 16.300, Run Time : 8.24 sec
INFO:root:2024-04-26 18:56:52, Train, Epoch : 1, Step : 230, Loss : 0.44458, Acc : 0.778, Sensitive_Loss : 0.27435, Sensitive_Acc : 16.500, Run Time : 8.08 sec
INFO:root:2024-04-26 18:57:01, Train, Epoch : 1, Step : 240, Loss : 0.48323, Acc : 0.844, Sensitive_Loss : 0.20050, Sensitive_Acc : 15.600, Run Time : 8.13 sec
INFO:root:2024-04-26 18:57:08, Train, Epoch : 1, Step : 250, Loss : 0.47231, Acc : 0.772, Sensitive_Loss : 0.23606, Sensitive_Acc : 15.500, Run Time : 7.93 sec
INFO:root:2024-04-26 18:57:17, Train, Epoch : 1, Step : 260, Loss : 0.59830, Acc : 0.747, Sensitive_Loss : 0.24658, Sensitive_Acc : 16.500, Run Time : 8.08 sec
INFO:root:2024-04-26 18:57:25, Train, Epoch : 1, Step : 270, Loss : 0.43641, Acc : 0.778, Sensitive_Loss : 0.25588, Sensitive_Acc : 16.300, Run Time : 8.39 sec
INFO:root:2024-04-26 18:57:33, Train, Epoch : 1, Step : 280, Loss : 0.38766, Acc : 0.844, Sensitive_Loss : 0.17603, Sensitive_Acc : 14.900, Run Time : 8.36 sec
INFO:root:2024-04-26 18:57:41, Train, Epoch : 1, Step : 290, Loss : 0.49384, Acc : 0.794, Sensitive_Loss : 0.20107, Sensitive_Acc : 16.600, Run Time : 7.98 sec
INFO:root:2024-04-26 18:57:50, Train, Epoch : 1, Step : 300, Loss : 0.48545, Acc : 0.781, Sensitive_Loss : 0.23083, Sensitive_Acc : 14.900, Run Time : 8.49 sec
INFO:root:2024-04-26 18:59:24, Dev, Step : 300, Loss : 0.46946, Acc : 0.792, Auc : 0.870, Sensitive_Loss : 0.23567, Sensitive_Acc : 16.779, Sensitive_Auc : 0.986, Mean auc: 0.870, Run Time : 93.89 sec
INFO:root:2024-04-26 18:59:24, Best, Step : 300, Loss : 0.46946, Acc : 0.792, Auc : 0.870, Sensitive_Loss : 0.23567, Sensitive_Acc : 16.779, Sensitive_Auc : 0.986, Best Auc : 0.870
INFO:root:2024-04-26 18:59:30, Train, Epoch : 1, Step : 310, Loss : 0.47565, Acc : 0.784, Sensitive_Loss : 0.25988, Sensitive_Acc : 17.600, Run Time : 100.22 sec
INFO:root:2024-04-26 18:59:38, Train, Epoch : 1, Step : 320, Loss : 0.43551, Acc : 0.787, Sensitive_Loss : 0.20809, Sensitive_Acc : 15.900, Run Time : 8.29 sec
INFO:root:2024-04-26 18:59:46, Train, Epoch : 1, Step : 330, Loss : 0.50567, Acc : 0.781, Sensitive_Loss : 0.18795, Sensitive_Acc : 16.700, Run Time : 7.76 sec
INFO:root:2024-04-26 18:59:54, Train, Epoch : 1, Step : 340, Loss : 0.53948, Acc : 0.744, Sensitive_Loss : 0.26288, Sensitive_Acc : 17.100, Run Time : 8.21 sec
INFO:root:2024-04-26 19:00:02, Train, Epoch : 1, Step : 350, Loss : 0.43765, Acc : 0.803, Sensitive_Loss : 0.21387, Sensitive_Acc : 15.700, Run Time : 8.11 sec
INFO:root:2024-04-26 19:00:11, Train, Epoch : 1, Step : 360, Loss : 0.43485, Acc : 0.791, Sensitive_Loss : 0.25679, Sensitive_Acc : 14.400, Run Time : 8.27 sec
INFO:root:2024-04-26 19:00:19, Train, Epoch : 1, Step : 370, Loss : 0.42841, Acc : 0.781, Sensitive_Loss : 0.21832, Sensitive_Acc : 16.000, Run Time : 7.88 sec
INFO:root:2024-04-26 19:00:26, Train, Epoch : 1, Step : 380, Loss : 0.47406, Acc : 0.738, Sensitive_Loss : 0.23830, Sensitive_Acc : 15.300, Run Time : 7.82 sec
INFO:root:2024-04-26 19:00:35, Train, Epoch : 1, Step : 390, Loss : 0.45192, Acc : 0.772, Sensitive_Loss : 0.20353, Sensitive_Acc : 16.600, Run Time : 9.04 sec
INFO:root:2024-04-26 19:00:44, Train, Epoch : 1, Step : 400, Loss : 0.48710, Acc : 0.784, Sensitive_Loss : 0.21898, Sensitive_Acc : 15.400, Run Time : 8.18 sec
INFO:root:2024-04-26 19:02:17, Dev, Step : 400, Loss : 0.53255, Acc : 0.773, Auc : 0.873, Sensitive_Loss : 0.25680, Sensitive_Acc : 16.821, Sensitive_Auc : 0.985, Mean auc: 0.873, Run Time : 93.93 sec
INFO:root:2024-04-26 19:02:18, Best, Step : 400, Loss : 0.53255, Acc : 0.773, Auc : 0.873, Sensitive_Loss : 0.25680, Sensitive_Acc : 16.821, Sensitive_Auc : 0.985, Best Auc : 0.873
INFO:root:2024-04-26 19:02:24, Train, Epoch : 1, Step : 410, Loss : 0.47231, Acc : 0.756, Sensitive_Loss : 0.20697, Sensitive_Acc : 15.500, Run Time : 100.52 sec
INFO:root:2024-04-26 19:02:32, Train, Epoch : 1, Step : 420, Loss : 0.46841, Acc : 0.756, Sensitive_Loss : 0.27978, Sensitive_Acc : 16.600, Run Time : 8.25 sec
INFO:root:2024-04-26 19:02:41, Train, Epoch : 1, Step : 430, Loss : 0.47718, Acc : 0.816, Sensitive_Loss : 0.17729, Sensitive_Acc : 15.700, Run Time : 8.67 sec
INFO:root:2024-04-26 19:02:49, Train, Epoch : 1, Step : 440, Loss : 0.47370, Acc : 0.809, Sensitive_Loss : 0.18810, Sensitive_Acc : 16.400, Run Time : 8.32 sec
INFO:root:2024-04-26 19:02:58, Train, Epoch : 1, Step : 450, Loss : 0.46186, Acc : 0.772, Sensitive_Loss : 0.18455, Sensitive_Acc : 15.500, Run Time : 8.48 sec
INFO:root:2024-04-26 19:03:06, Train, Epoch : 1, Step : 460, Loss : 0.42860, Acc : 0.844, Sensitive_Loss : 0.18919, Sensitive_Acc : 15.200, Run Time : 8.23 sec
INFO:root:2024-04-26 19:03:14, Train, Epoch : 1, Step : 470, Loss : 0.52190, Acc : 0.787, Sensitive_Loss : 0.15395, Sensitive_Acc : 16.500, Run Time : 8.34 sec
INFO:root:2024-04-26 19:03:23, Train, Epoch : 1, Step : 480, Loss : 0.51870, Acc : 0.750, Sensitive_Loss : 0.18432, Sensitive_Acc : 15.500, Run Time : 8.97 sec
INFO:root:2024-04-26 19:03:32, Train, Epoch : 1, Step : 490, Loss : 0.55314, Acc : 0.769, Sensitive_Loss : 0.20891, Sensitive_Acc : 16.000, Run Time : 8.56 sec
INFO:root:2024-04-26 19:03:40, Train, Epoch : 1, Step : 500, Loss : 0.48731, Acc : 0.778, Sensitive_Loss : 0.23645, Sensitive_Acc : 16.800, Run Time : 8.45 sec
INFO:root:2024-04-26 19:05:14, Dev, Step : 500, Loss : 0.46681, Acc : 0.795, Auc : 0.870, Sensitive_Loss : 0.22633, Sensitive_Acc : 16.879, Sensitive_Auc : 0.983, Mean auc: 0.870, Run Time : 93.69 sec
INFO:root:2024-04-26 19:05:20, Train, Epoch : 1, Step : 510, Loss : 0.51195, Acc : 0.772, Sensitive_Loss : 0.20747, Sensitive_Acc : 15.000, Run Time : 99.59 sec
INFO:root:2024-04-26 19:05:29, Train, Epoch : 1, Step : 520, Loss : 0.45312, Acc : 0.784, Sensitive_Loss : 0.21772, Sensitive_Acc : 15.900, Run Time : 8.83 sec
INFO:root:2024-04-26 19:05:38, Train, Epoch : 1, Step : 530, Loss : 0.41880, Acc : 0.806, Sensitive_Loss : 0.17491, Sensitive_Acc : 15.300, Run Time : 8.78 sec
INFO:root:2024-04-26 19:05:46, Train, Epoch : 1, Step : 540, Loss : 0.43628, Acc : 0.822, Sensitive_Loss : 0.23981, Sensitive_Acc : 16.800, Run Time : 8.77 sec
INFO:root:2024-04-26 19:05:56, Train, Epoch : 1, Step : 550, Loss : 0.47689, Acc : 0.778, Sensitive_Loss : 0.16247, Sensitive_Acc : 16.800, Run Time : 9.23 sec
INFO:root:2024-04-26 19:06:04, Train, Epoch : 1, Step : 560, Loss : 0.44851, Acc : 0.809, Sensitive_Loss : 0.18768, Sensitive_Acc : 15.700, Run Time : 8.59 sec
INFO:root:2024-04-26 19:06:13, Train, Epoch : 1, Step : 570, Loss : 0.45197, Acc : 0.841, Sensitive_Loss : 0.15899, Sensitive_Acc : 17.300, Run Time : 8.49 sec
INFO:root:2024-04-26 19:06:21, Train, Epoch : 1, Step : 580, Loss : 0.53031, Acc : 0.775, Sensitive_Loss : 0.17136, Sensitive_Acc : 16.300, Run Time : 8.43 sec
INFO:root:2024-04-26 19:06:30, Train, Epoch : 1, Step : 590, Loss : 0.41156, Acc : 0.812, Sensitive_Loss : 0.21224, Sensitive_Acc : 16.000, Run Time : 9.18 sec
INFO:root:2024-04-26 19:06:39, Train, Epoch : 1, Step : 600, Loss : 0.46882, Acc : 0.797, Sensitive_Loss : 0.17895, Sensitive_Acc : 16.000, Run Time : 9.11 sec
INFO:root:2024-04-26 19:08:13, Dev, Step : 600, Loss : 0.50345, Acc : 0.781, Auc : 0.881, Sensitive_Loss : 0.18074, Sensitive_Acc : 16.736, Sensitive_Auc : 0.987, Mean auc: 0.881, Run Time : 93.50 sec
INFO:root:2024-04-26 19:08:14, Best, Step : 600, Loss : 0.50345, Acc : 0.781, Auc : 0.881, Sensitive_Loss : 0.18074, Sensitive_Acc : 16.736, Sensitive_Auc : 0.987, Best Auc : 0.881
INFO:root:2024-04-26 19:08:20, Train, Epoch : 1, Step : 610, Loss : 0.45455, Acc : 0.766, Sensitive_Loss : 0.15346, Sensitive_Acc : 15.000, Run Time : 101.01 sec
INFO:root:2024-04-26 19:08:30, Train, Epoch : 1, Step : 620, Loss : 0.45994, Acc : 0.806, Sensitive_Loss : 0.15549, Sensitive_Acc : 18.600, Run Time : 9.16 sec
INFO:root:2024-04-26 19:10:08
INFO:root:y_pred: [0.35172543 0.9596619  0.07771689 ... 0.62192845 0.01374923 0.7863895 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [8.12752664e-01 1.92776578e-03 9.36546102e-02 6.65621599e-03
 9.96597707e-01 3.50129034e-04 9.99993205e-01 9.99086857e-01
 1.60809495e-02 7.11899638e-01 9.98914123e-01 9.99150634e-01
 9.85747874e-01 9.54446256e-01 5.70051260e-02 8.76778305e-01
 9.99645114e-01 1.25052109e-01 8.32330346e-01 8.92072737e-01
 9.97211158e-01 1.69380516e-01 9.94185507e-01 9.79193091e-01
 9.98285115e-01 9.84482944e-01 2.16641407e-02 9.98147249e-01
 9.92184341e-01 1.61357403e-01 8.61494690e-02 4.23532575e-01
 5.14876604e-01 1.29327565e-01 3.35454904e-02 9.25889798e-03
 1.18939728e-01 5.30283060e-03 9.98467505e-01 9.99389052e-01
 3.19398736e-04 2.38166051e-03 9.32507813e-01 4.09793761e-03
 9.99975085e-01 9.88123477e-01 9.98712659e-01 9.96685803e-01
 2.13314611e-02 9.87108827e-01 9.95978951e-01 3.66312731e-03
 2.78019190e-01 8.79976724e-04 2.45128147e-04 2.14023497e-02
 2.78902799e-01 2.71288794e-03 1.27818331e-03 9.56066791e-03
 2.85481755e-02 3.92486900e-02 4.28658538e-02 9.12431240e-01
 3.95440191e-01 9.99874234e-01 6.43386692e-02 9.99609530e-01
 9.40051079e-01 7.34815598e-01 6.76441908e-01 5.91108620e-01
 7.26210093e-03 3.14101011e-01 1.36232242e-01 5.22660464e-03
 1.04095131e-01 4.00310755e-02 5.49033936e-03 9.94096994e-01
 9.97615814e-01 7.36728683e-03 3.10982578e-02 3.97101697e-03
 9.14162457e-01 8.82911444e-01 1.14240095e-01 2.03639381e-02
 9.24163878e-01 9.99284923e-01 9.99986053e-01 1.05014816e-02
 2.55095381e-02 9.98920202e-01 2.17075139e-01 6.65529305e-03
 9.98753786e-01 9.99371707e-01 4.42817621e-03 3.74298930e-01
 9.77904558e-01 9.63113546e-01 9.89889085e-01 9.84807014e-01
 1.57577614e-03 1.40533000e-01 9.93972242e-01 9.97631431e-01
 9.85452831e-01 3.89458637e-06 9.28887248e-01 9.99308228e-01
 2.27523491e-01 9.99531627e-01 9.97320831e-01 9.96977687e-01
 5.30007422e-01 9.95461643e-01 9.74152312e-02 1.35740647e-02
 9.98184264e-01 9.91944194e-01 2.19900525e-04 9.81744051e-01
 9.99957204e-01 7.48836517e-01 9.78119254e-01 1.32428497e-01
 1.90059930e-01 8.88541758e-01 9.95318830e-01 1.07788406e-01
 1.19977938e-02 2.78062616e-02 9.99434054e-01 9.69944060e-01
 9.89140630e-01 1.27935000e-02 2.73461908e-01 9.96039271e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 19:10:08, Dev, Step : 626, Loss : 0.44627, Acc : 0.797, Auc : 0.881, Sensitive_Loss : 0.16416, Sensitive_Acc : 16.850, Sensitive_Auc : 0.990, Mean auc: 0.881, Run Time : 92.69 sec
INFO:root:2024-04-26 19:10:08, Best, Step : 626, Loss : 0.44627, Acc : 0.797,Auc : 0.881, Best Auc : 0.881, Sensitive_Loss : 0.16416, Sensitive_Acc : 16.850, Sensitive_Auc : 0.990
INFO:root:2024-04-26 19:10:13, Train, Epoch : 2, Step : 630, Loss : 0.15894, Acc : 0.316, Sensitive_Loss : 0.06758, Sensitive_Acc : 6.800, Run Time : 3.75 sec
INFO:root:2024-04-26 19:10:21, Train, Epoch : 2, Step : 640, Loss : 0.51608, Acc : 0.806, Sensitive_Loss : 0.11498, Sensitive_Acc : 15.400, Run Time : 7.63 sec
INFO:root:2024-04-26 19:10:28, Train, Epoch : 2, Step : 650, Loss : 0.46402, Acc : 0.759, Sensitive_Loss : 0.19649, Sensitive_Acc : 16.100, Run Time : 7.21 sec
INFO:root:2024-04-26 19:10:36, Train, Epoch : 2, Step : 660, Loss : 0.36853, Acc : 0.831, Sensitive_Loss : 0.18696, Sensitive_Acc : 14.200, Run Time : 7.88 sec
INFO:root:2024-04-26 19:10:43, Train, Epoch : 2, Step : 670, Loss : 0.38028, Acc : 0.806, Sensitive_Loss : 0.16485, Sensitive_Acc : 15.300, Run Time : 7.06 sec
INFO:root:2024-04-26 19:10:50, Train, Epoch : 2, Step : 680, Loss : 0.52013, Acc : 0.750, Sensitive_Loss : 0.14751, Sensitive_Acc : 15.700, Run Time : 7.65 sec
INFO:root:2024-04-26 19:10:57, Train, Epoch : 2, Step : 690, Loss : 0.41633, Acc : 0.841, Sensitive_Loss : 0.16555, Sensitive_Acc : 16.300, Run Time : 7.08 sec
INFO:root:2024-04-26 19:11:04, Train, Epoch : 2, Step : 700, Loss : 0.42095, Acc : 0.806, Sensitive_Loss : 0.17599, Sensitive_Acc : 15.700, Run Time : 7.02 sec
INFO:root:2024-04-26 19:12:39, Dev, Step : 700, Loss : 0.44665, Acc : 0.802, Auc : 0.883, Sensitive_Loss : 0.14505, Sensitive_Acc : 16.936, Sensitive_Auc : 0.997, Mean auc: 0.883, Run Time : 94.58 sec
INFO:root:2024-04-26 19:12:40, Best, Step : 700, Loss : 0.44665, Acc : 0.802, Auc : 0.883, Sensitive_Loss : 0.14505, Sensitive_Acc : 16.936, Sensitive_Auc : 0.997, Best Auc : 0.883
INFO:root:2024-04-26 19:12:46, Train, Epoch : 2, Step : 710, Loss : 0.45519, Acc : 0.812, Sensitive_Loss : 0.17752, Sensitive_Acc : 16.400, Run Time : 101.19 sec
INFO:root:2024-04-26 19:12:53, Train, Epoch : 2, Step : 720, Loss : 0.39138, Acc : 0.831, Sensitive_Loss : 0.11914, Sensitive_Acc : 15.400, Run Time : 7.44 sec
INFO:root:2024-04-26 19:13:01, Train, Epoch : 2, Step : 730, Loss : 0.41025, Acc : 0.806, Sensitive_Loss : 0.18205, Sensitive_Acc : 15.200, Run Time : 7.57 sec
INFO:root:2024-04-26 19:13:08, Train, Epoch : 2, Step : 740, Loss : 0.42443, Acc : 0.831, Sensitive_Loss : 0.10879, Sensitive_Acc : 17.200, Run Time : 7.17 sec
INFO:root:2024-04-26 19:13:16, Train, Epoch : 2, Step : 750, Loss : 0.50070, Acc : 0.794, Sensitive_Loss : 0.16968, Sensitive_Acc : 16.400, Run Time : 7.74 sec
INFO:root:2024-04-26 19:13:23, Train, Epoch : 2, Step : 760, Loss : 0.43973, Acc : 0.816, Sensitive_Loss : 0.13525, Sensitive_Acc : 14.400, Run Time : 7.03 sec
INFO:root:2024-04-26 19:13:30, Train, Epoch : 2, Step : 770, Loss : 0.44665, Acc : 0.781, Sensitive_Loss : 0.17072, Sensitive_Acc : 17.000, Run Time : 7.50 sec
INFO:root:2024-04-26 19:13:38, Train, Epoch : 2, Step : 780, Loss : 0.43386, Acc : 0.787, Sensitive_Loss : 0.13581, Sensitive_Acc : 17.800, Run Time : 7.58 sec
INFO:root:2024-04-26 19:13:46, Train, Epoch : 2, Step : 790, Loss : 0.44834, Acc : 0.812, Sensitive_Loss : 0.17645, Sensitive_Acc : 18.000, Run Time : 8.07 sec
INFO:root:2024-04-26 19:13:53, Train, Epoch : 2, Step : 800, Loss : 0.44970, Acc : 0.822, Sensitive_Loss : 0.16189, Sensitive_Acc : 15.200, Run Time : 7.48 sec
INFO:root:2024-04-26 19:15:27, Dev, Step : 800, Loss : 0.51901, Acc : 0.774, Auc : 0.890, Sensitive_Loss : 0.25127, Sensitive_Acc : 16.693, Sensitive_Auc : 0.991, Mean auc: 0.890, Run Time : 94.01 sec
INFO:root:2024-04-26 19:15:28, Best, Step : 800, Loss : 0.51901, Acc : 0.774, Auc : 0.890, Sensitive_Loss : 0.25127, Sensitive_Acc : 16.693, Sensitive_Auc : 0.991, Best Auc : 0.890
INFO:root:2024-04-26 19:15:33, Train, Epoch : 2, Step : 810, Loss : 0.45739, Acc : 0.787, Sensitive_Loss : 0.13994, Sensitive_Acc : 15.600, Run Time : 100.09 sec
INFO:root:2024-04-26 19:15:41, Train, Epoch : 2, Step : 820, Loss : 0.41778, Acc : 0.828, Sensitive_Loss : 0.17904, Sensitive_Acc : 16.200, Run Time : 7.92 sec
INFO:root:2024-04-26 19:15:49, Train, Epoch : 2, Step : 830, Loss : 0.45329, Acc : 0.803, Sensitive_Loss : 0.17779, Sensitive_Acc : 15.900, Run Time : 7.54 sec
INFO:root:2024-04-26 19:15:56, Train, Epoch : 2, Step : 840, Loss : 0.42586, Acc : 0.809, Sensitive_Loss : 0.17951, Sensitive_Acc : 16.500, Run Time : 7.47 sec
INFO:root:2024-04-26 19:16:04, Train, Epoch : 2, Step : 850, Loss : 0.42785, Acc : 0.806, Sensitive_Loss : 0.16842, Sensitive_Acc : 17.000, Run Time : 7.49 sec
INFO:root:2024-04-26 19:16:11, Train, Epoch : 2, Step : 860, Loss : 0.43509, Acc : 0.834, Sensitive_Loss : 0.15359, Sensitive_Acc : 16.800, Run Time : 7.58 sec
INFO:root:2024-04-26 19:16:19, Train, Epoch : 2, Step : 870, Loss : 0.40789, Acc : 0.787, Sensitive_Loss : 0.13266, Sensitive_Acc : 17.000, Run Time : 7.54 sec
INFO:root:2024-04-26 19:16:26, Train, Epoch : 2, Step : 880, Loss : 0.42403, Acc : 0.806, Sensitive_Loss : 0.12881, Sensitive_Acc : 16.400, Run Time : 7.43 sec
INFO:root:2024-04-26 19:16:34, Train, Epoch : 2, Step : 890, Loss : 0.38600, Acc : 0.809, Sensitive_Loss : 0.15382, Sensitive_Acc : 15.900, Run Time : 7.49 sec
INFO:root:2024-04-26 19:16:41, Train, Epoch : 2, Step : 900, Loss : 0.42872, Acc : 0.800, Sensitive_Loss : 0.13373, Sensitive_Acc : 14.600, Run Time : 7.69 sec
INFO:root:2024-04-26 19:18:16, Dev, Step : 900, Loss : 0.46641, Acc : 0.798, Auc : 0.885, Sensitive_Loss : 0.18271, Sensitive_Acc : 16.693, Sensitive_Auc : 0.986, Mean auc: 0.885, Run Time : 94.10 sec
INFO:root:2024-04-26 19:18:21, Train, Epoch : 2, Step : 910, Loss : 0.40742, Acc : 0.806, Sensitive_Loss : 0.14364, Sensitive_Acc : 15.000, Run Time : 99.80 sec
INFO:root:2024-04-26 19:18:29, Train, Epoch : 2, Step : 920, Loss : 0.43842, Acc : 0.794, Sensitive_Loss : 0.17722, Sensitive_Acc : 16.600, Run Time : 7.64 sec
INFO:root:2024-04-26 19:18:36, Train, Epoch : 2, Step : 930, Loss : 0.36192, Acc : 0.838, Sensitive_Loss : 0.12833, Sensitive_Acc : 16.900, Run Time : 7.52 sec
INFO:root:2024-04-26 19:18:44, Train, Epoch : 2, Step : 940, Loss : 0.43158, Acc : 0.803, Sensitive_Loss : 0.14138, Sensitive_Acc : 15.900, Run Time : 7.59 sec
INFO:root:2024-04-26 19:18:52, Train, Epoch : 2, Step : 950, Loss : 0.46097, Acc : 0.809, Sensitive_Loss : 0.11532, Sensitive_Acc : 17.000, Run Time : 7.77 sec
INFO:root:2024-04-26 19:18:59, Train, Epoch : 2, Step : 960, Loss : 0.40366, Acc : 0.819, Sensitive_Loss : 0.18771, Sensitive_Acc : 15.500, Run Time : 7.29 sec
INFO:root:2024-04-26 19:19:07, Train, Epoch : 2, Step : 970, Loss : 0.39676, Acc : 0.803, Sensitive_Loss : 0.11491, Sensitive_Acc : 16.400, Run Time : 8.06 sec
INFO:root:2024-04-26 19:19:14, Train, Epoch : 2, Step : 980, Loss : 0.40754, Acc : 0.819, Sensitive_Loss : 0.14363, Sensitive_Acc : 16.300, Run Time : 7.28 sec
INFO:root:2024-04-26 19:19:22, Train, Epoch : 2, Step : 990, Loss : 0.48068, Acc : 0.747, Sensitive_Loss : 0.15421, Sensitive_Acc : 16.000, Run Time : 7.41 sec
INFO:root:2024-04-26 19:19:29, Train, Epoch : 2, Step : 1000, Loss : 0.46545, Acc : 0.806, Sensitive_Loss : 0.13058, Sensitive_Acc : 16.000, Run Time : 7.53 sec
INFO:root:2024-04-26 19:21:03, Dev, Step : 1000, Loss : 0.44043, Acc : 0.807, Auc : 0.887, Sensitive_Loss : 0.13961, Sensitive_Acc : 16.836, Sensitive_Auc : 0.985, Mean auc: 0.887, Run Time : 94.08 sec
INFO:root:2024-04-26 19:21:09, Train, Epoch : 2, Step : 1010, Loss : 0.43424, Acc : 0.784, Sensitive_Loss : 0.16833, Sensitive_Acc : 18.200, Run Time : 99.75 sec
INFO:root:2024-04-26 19:21:17, Train, Epoch : 2, Step : 1020, Loss : 0.44567, Acc : 0.803, Sensitive_Loss : 0.12977, Sensitive_Acc : 16.200, Run Time : 7.40 sec
INFO:root:2024-04-26 19:21:25, Train, Epoch : 2, Step : 1030, Loss : 0.49865, Acc : 0.797, Sensitive_Loss : 0.14422, Sensitive_Acc : 16.200, Run Time : 8.01 sec
INFO:root:2024-04-26 19:21:32, Train, Epoch : 2, Step : 1040, Loss : 0.39627, Acc : 0.822, Sensitive_Loss : 0.18305, Sensitive_Acc : 17.300, Run Time : 7.43 sec
INFO:root:2024-04-26 19:21:39, Train, Epoch : 2, Step : 1050, Loss : 0.35211, Acc : 0.847, Sensitive_Loss : 0.15285, Sensitive_Acc : 15.000, Run Time : 7.47 sec
INFO:root:2024-04-26 19:21:47, Train, Epoch : 2, Step : 1060, Loss : 0.38443, Acc : 0.853, Sensitive_Loss : 0.11394, Sensitive_Acc : 15.100, Run Time : 7.67 sec
INFO:root:2024-04-26 19:21:55, Train, Epoch : 2, Step : 1070, Loss : 0.49463, Acc : 0.819, Sensitive_Loss : 0.11183, Sensitive_Acc : 15.800, Run Time : 7.99 sec
INFO:root:2024-04-26 19:22:03, Train, Epoch : 2, Step : 1080, Loss : 0.50088, Acc : 0.797, Sensitive_Loss : 0.10110, Sensitive_Acc : 16.600, Run Time : 7.42 sec
INFO:root:2024-04-26 19:22:10, Train, Epoch : 2, Step : 1090, Loss : 0.43988, Acc : 0.800, Sensitive_Loss : 0.11625, Sensitive_Acc : 15.800, Run Time : 7.69 sec
INFO:root:2024-04-26 19:22:18, Train, Epoch : 2, Step : 1100, Loss : 0.39096, Acc : 0.856, Sensitive_Loss : 0.13740, Sensitive_Acc : 17.300, Run Time : 7.84 sec
INFO:root:2024-04-26 19:23:52, Dev, Step : 1100, Loss : 0.43074, Acc : 0.814, Auc : 0.889, Sensitive_Loss : 0.11742, Sensitive_Acc : 16.850, Sensitive_Auc : 0.997, Mean auc: 0.889, Run Time : 93.60 sec
INFO:root:2024-04-26 19:23:57, Train, Epoch : 2, Step : 1110, Loss : 0.53735, Acc : 0.775, Sensitive_Loss : 0.13789, Sensitive_Acc : 15.700, Run Time : 99.34 sec
INFO:root:2024-04-26 19:24:05, Train, Epoch : 2, Step : 1120, Loss : 0.38771, Acc : 0.800, Sensitive_Loss : 0.12961, Sensitive_Acc : 16.600, Run Time : 7.34 sec
INFO:root:2024-04-26 19:24:12, Train, Epoch : 2, Step : 1130, Loss : 0.44830, Acc : 0.797, Sensitive_Loss : 0.10107, Sensitive_Acc : 15.800, Run Time : 7.73 sec
INFO:root:2024-04-26 19:24:20, Train, Epoch : 2, Step : 1140, Loss : 0.40081, Acc : 0.834, Sensitive_Loss : 0.10462, Sensitive_Acc : 16.500, Run Time : 7.26 sec
INFO:root:2024-04-26 19:24:27, Train, Epoch : 2, Step : 1150, Loss : 0.48359, Acc : 0.797, Sensitive_Loss : 0.11674, Sensitive_Acc : 14.300, Run Time : 7.18 sec
INFO:root:2024-04-26 19:24:34, Train, Epoch : 2, Step : 1160, Loss : 0.46311, Acc : 0.781, Sensitive_Loss : 0.18695, Sensitive_Acc : 15.400, Run Time : 7.57 sec
INFO:root:2024-04-26 19:24:42, Train, Epoch : 2, Step : 1170, Loss : 0.41715, Acc : 0.791, Sensitive_Loss : 0.13096, Sensitive_Acc : 15.900, Run Time : 7.70 sec
INFO:root:2024-04-26 19:24:50, Train, Epoch : 2, Step : 1180, Loss : 0.39370, Acc : 0.834, Sensitive_Loss : 0.14087, Sensitive_Acc : 17.000, Run Time : 7.36 sec
INFO:root:2024-04-26 19:24:57, Train, Epoch : 2, Step : 1190, Loss : 0.37514, Acc : 0.834, Sensitive_Loss : 0.13921, Sensitive_Acc : 16.500, Run Time : 7.17 sec
INFO:root:2024-04-26 19:25:04, Train, Epoch : 2, Step : 1200, Loss : 0.47093, Acc : 0.803, Sensitive_Loss : 0.11514, Sensitive_Acc : 15.900, Run Time : 7.49 sec
INFO:root:2024-04-26 19:26:38, Dev, Step : 1200, Loss : 0.43103, Acc : 0.810, Auc : 0.890, Sensitive_Loss : 0.13320, Sensitive_Acc : 16.864, Sensitive_Auc : 0.991, Mean auc: 0.890, Run Time : 93.70 sec
INFO:root:2024-04-26 19:26:39, Best, Step : 1200, Loss : 0.43103, Acc : 0.810, Auc : 0.890, Sensitive_Loss : 0.13320, Sensitive_Acc : 16.864, Sensitive_Auc : 0.991, Best Auc : 0.890
INFO:root:2024-04-26 19:26:44, Train, Epoch : 2, Step : 1210, Loss : 0.44665, Acc : 0.794, Sensitive_Loss : 0.14212, Sensitive_Acc : 16.400, Run Time : 99.71 sec
INFO:root:2024-04-26 19:26:52, Train, Epoch : 2, Step : 1220, Loss : 0.41995, Acc : 0.809, Sensitive_Loss : 0.15916, Sensitive_Acc : 16.000, Run Time : 7.74 sec
INFO:root:2024-04-26 19:27:00, Train, Epoch : 2, Step : 1230, Loss : 0.50851, Acc : 0.784, Sensitive_Loss : 0.14561, Sensitive_Acc : 16.700, Run Time : 8.00 sec
INFO:root:2024-04-26 19:27:08, Train, Epoch : 2, Step : 1240, Loss : 0.37625, Acc : 0.838, Sensitive_Loss : 0.12794, Sensitive_Acc : 18.200, Run Time : 7.95 sec
INFO:root:2024-04-26 19:27:15, Train, Epoch : 2, Step : 1250, Loss : 0.43655, Acc : 0.781, Sensitive_Loss : 0.12961, Sensitive_Acc : 16.900, Run Time : 7.13 sec
INFO:root:2024-04-26 19:28:48
INFO:root:y_pred: [0.08514956 0.9605111  0.08121394 ... 0.52104247 0.03999146 0.60726875]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.9412715e-01 4.6870648e-03 2.9057410e-02 1.9069368e-03 9.9993324e-01
 3.3955625e-04 9.9998820e-01 9.9991012e-01 3.8508985e-02 9.3945962e-01
 9.9724591e-01 9.9993777e-01 9.9657154e-01 9.9849379e-01 4.0590653e-01
 9.8724395e-01 9.9990296e-01 4.7709188e-01 7.5950933e-01 8.8258010e-01
 9.9986339e-01 4.2605746e-01 9.9900985e-01 9.9829382e-01 9.9947125e-01
 9.9814045e-01 2.1240124e-02 9.9998367e-01 9.9807847e-01 7.5316834e-01
 5.0300281e-02 3.9391074e-01 6.2241113e-01 5.6181711e-01 4.2493470e-02
 1.4852231e-02 3.0801019e-01 1.1897060e-02 9.9966681e-01 9.9977535e-01
 2.7570804e-04 9.2401094e-04 9.9364424e-01 1.0610790e-03 9.9999142e-01
 9.9609548e-01 9.9940455e-01 9.9677593e-01 3.6116973e-02 9.9902475e-01
 9.9930048e-01 2.6121470e-03 6.8778388e-02 3.8172929e-03 4.7852346e-04
 1.0147999e-02 2.0587671e-01 1.4118889e-02 1.8474136e-03 1.5765831e-02
 1.5146321e-01 9.6326746e-02 5.1841909e-01 9.0673971e-01 9.3010181e-01
 9.9997127e-01 8.2216971e-02 9.9982160e-01 9.9971360e-01 8.6679339e-01
 8.4473693e-01 9.6007299e-01 6.2249405e-03 4.5564495e-02 1.3756604e-02
 8.0450512e-03 1.5853520e-01 8.2086079e-02 1.3759134e-02 9.9925488e-01
 9.9980706e-01 3.6689602e-02 4.6911147e-01 6.0249541e-02 9.7941166e-01
 9.2923671e-01 2.3732004e-01 5.8393371e-01 9.9924433e-01 9.9987030e-01
 9.9999750e-01 1.8565872e-01 1.4853273e-01 9.9933332e-01 5.6136310e-01
 6.8795569e-03 9.9989140e-01 9.9992454e-01 1.5157171e-03 1.1519253e-01
 9.9953735e-01 9.9771202e-01 9.7241229e-01 9.9647838e-01 1.1888406e-04
 1.0771908e-01 9.9818265e-01 9.9985206e-01 9.9367023e-01 1.2226303e-06
 9.9952543e-01 9.9988210e-01 1.0833603e-01 9.9993038e-01 9.9975115e-01
 9.9981850e-01 9.7248864e-01 9.9989164e-01 1.4155400e-01 2.9721358e-03
 9.9946898e-01 9.9908757e-01 3.1887175e-04 9.8401064e-01 9.9999714e-01
 8.2126731e-01 9.9827850e-01 8.0629485e-03 8.6651649e-03 9.9292541e-01
 9.9985206e-01 6.9530964e-02 9.0927118e-03 9.8526003e-03 9.9990177e-01
 9.9467695e-01 9.9935287e-01 8.4280595e-03 6.1267450e-02 9.9144942e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 19:28:48, Dev, Step : 1252, Loss : 0.48395, Acc : 0.786, Auc : 0.886, Sensitive_Loss : 0.20561, Sensitive_Acc : 16.807, Sensitive_Auc : 0.990, Mean auc: 0.886, Run Time : 92.38 sec
INFO:root:2024-04-26 19:28:56, Train, Epoch : 3, Step : 1260, Loss : 0.30427, Acc : 0.688, Sensitive_Loss : 0.09161, Sensitive_Acc : 12.200, Run Time : 6.84 sec
INFO:root:2024-04-26 19:29:03, Train, Epoch : 3, Step : 1270, Loss : 0.37937, Acc : 0.816, Sensitive_Loss : 0.13166, Sensitive_Acc : 16.000, Run Time : 7.25 sec
INFO:root:2024-04-26 19:29:11, Train, Epoch : 3, Step : 1280, Loss : 0.36186, Acc : 0.838, Sensitive_Loss : 0.08439, Sensitive_Acc : 16.900, Run Time : 7.27 sec
INFO:root:2024-04-26 19:29:18, Train, Epoch : 3, Step : 1290, Loss : 0.33328, Acc : 0.838, Sensitive_Loss : 0.11546, Sensitive_Acc : 16.900, Run Time : 7.48 sec
INFO:root:2024-04-26 19:29:26, Train, Epoch : 3, Step : 1300, Loss : 0.38179, Acc : 0.841, Sensitive_Loss : 0.10192, Sensitive_Acc : 15.900, Run Time : 7.60 sec
INFO:root:2024-04-26 19:30:59, Dev, Step : 1300, Loss : 0.42802, Acc : 0.816, Auc : 0.897, Sensitive_Loss : 0.10442, Sensitive_Acc : 16.821, Sensitive_Auc : 0.992, Mean auc: 0.897, Run Time : 93.52 sec
INFO:root:2024-04-26 19:31:00, Best, Step : 1300, Loss : 0.42802, Acc : 0.816, Auc : 0.897, Sensitive_Loss : 0.10442, Sensitive_Acc : 16.821, Sensitive_Auc : 0.992, Best Auc : 0.897
INFO:root:2024-04-26 19:31:05, Train, Epoch : 3, Step : 1310, Loss : 0.36468, Acc : 0.838, Sensitive_Loss : 0.10590, Sensitive_Acc : 15.300, Run Time : 99.80 sec
INFO:root:2024-04-26 19:31:13, Train, Epoch : 3, Step : 1320, Loss : 0.40762, Acc : 0.838, Sensitive_Loss : 0.13774, Sensitive_Acc : 15.600, Run Time : 7.55 sec
INFO:root:2024-04-26 19:31:21, Train, Epoch : 3, Step : 1330, Loss : 0.28594, Acc : 0.887, Sensitive_Loss : 0.08626, Sensitive_Acc : 16.100, Run Time : 7.87 sec
INFO:root:2024-04-26 19:31:29, Train, Epoch : 3, Step : 1340, Loss : 0.37185, Acc : 0.841, Sensitive_Loss : 0.09205, Sensitive_Acc : 16.800, Run Time : 7.82 sec
INFO:root:2024-04-26 19:31:36, Train, Epoch : 3, Step : 1350, Loss : 0.40261, Acc : 0.822, Sensitive_Loss : 0.11646, Sensitive_Acc : 14.800, Run Time : 7.51 sec
INFO:root:2024-04-26 19:31:44, Train, Epoch : 3, Step : 1360, Loss : 0.35434, Acc : 0.828, Sensitive_Loss : 0.11945, Sensitive_Acc : 17.600, Run Time : 7.62 sec
INFO:root:2024-04-26 19:31:51, Train, Epoch : 3, Step : 1370, Loss : 0.32688, Acc : 0.850, Sensitive_Loss : 0.08282, Sensitive_Acc : 16.200, Run Time : 7.52 sec
INFO:root:2024-04-26 19:32:00, Train, Epoch : 3, Step : 1380, Loss : 0.36317, Acc : 0.853, Sensitive_Loss : 0.13887, Sensitive_Acc : 18.300, Run Time : 8.18 sec
INFO:root:2024-04-26 19:32:07, Train, Epoch : 3, Step : 1390, Loss : 0.30214, Acc : 0.856, Sensitive_Loss : 0.10790, Sensitive_Acc : 15.700, Run Time : 7.22 sec
INFO:root:2024-04-26 19:32:14, Train, Epoch : 3, Step : 1400, Loss : 0.39699, Acc : 0.838, Sensitive_Loss : 0.09017, Sensitive_Acc : 18.000, Run Time : 7.61 sec
INFO:root:2024-04-26 19:33:48, Dev, Step : 1400, Loss : 0.42069, Acc : 0.821, Auc : 0.902, Sensitive_Loss : 0.11911, Sensitive_Acc : 16.879, Sensitive_Auc : 0.991, Mean auc: 0.902, Run Time : 94.04 sec
INFO:root:2024-04-26 19:33:49, Best, Step : 1400, Loss : 0.42069, Acc : 0.821, Auc : 0.902, Sensitive_Loss : 0.11911, Sensitive_Acc : 16.879, Sensitive_Auc : 0.991, Best Auc : 0.902
INFO:root:2024-04-26 19:33:55, Train, Epoch : 3, Step : 1410, Loss : 0.36474, Acc : 0.825, Sensitive_Loss : 0.08356, Sensitive_Acc : 16.000, Run Time : 100.20 sec
INFO:root:2024-04-26 19:34:02, Train, Epoch : 3, Step : 1420, Loss : 0.28288, Acc : 0.863, Sensitive_Loss : 0.10489, Sensitive_Acc : 17.700, Run Time : 7.62 sec
INFO:root:2024-04-26 19:34:10, Train, Epoch : 3, Step : 1430, Loss : 0.29556, Acc : 0.872, Sensitive_Loss : 0.09040, Sensitive_Acc : 17.100, Run Time : 7.73 sec
INFO:root:2024-04-26 19:34:17, Train, Epoch : 3, Step : 1440, Loss : 0.40249, Acc : 0.816, Sensitive_Loss : 0.10728, Sensitive_Acc : 16.800, Run Time : 7.20 sec
INFO:root:2024-04-26 19:34:25, Train, Epoch : 3, Step : 1450, Loss : 0.37693, Acc : 0.819, Sensitive_Loss : 0.11252, Sensitive_Acc : 16.600, Run Time : 7.79 sec
INFO:root:2024-04-26 19:34:33, Train, Epoch : 3, Step : 1460, Loss : 0.33461, Acc : 0.875, Sensitive_Loss : 0.06794, Sensitive_Acc : 16.500, Run Time : 7.76 sec
INFO:root:2024-04-26 19:34:40, Train, Epoch : 3, Step : 1470, Loss : 0.35236, Acc : 0.863, Sensitive_Loss : 0.07862, Sensitive_Acc : 16.400, Run Time : 7.49 sec
INFO:root:2024-04-26 19:34:48, Train, Epoch : 3, Step : 1480, Loss : 0.43028, Acc : 0.819, Sensitive_Loss : 0.12708, Sensitive_Acc : 17.300, Run Time : 7.34 sec
INFO:root:2024-04-26 19:34:55, Train, Epoch : 3, Step : 1490, Loss : 0.40512, Acc : 0.850, Sensitive_Loss : 0.08848, Sensitive_Acc : 15.400, Run Time : 7.64 sec
INFO:root:2024-04-26 19:35:03, Train, Epoch : 3, Step : 1500, Loss : 0.31751, Acc : 0.866, Sensitive_Loss : 0.08898, Sensitive_Acc : 17.100, Run Time : 7.59 sec
INFO:root:2024-04-26 19:36:36, Dev, Step : 1500, Loss : 0.41519, Acc : 0.823, Auc : 0.905, Sensitive_Loss : 0.12292, Sensitive_Acc : 16.879, Sensitive_Auc : 0.992, Mean auc: 0.905, Run Time : 93.65 sec
INFO:root:2024-04-26 19:36:37, Best, Step : 1500, Loss : 0.41519, Acc : 0.823, Auc : 0.905, Sensitive_Loss : 0.12292, Sensitive_Acc : 16.879, Sensitive_Auc : 0.992, Best Auc : 0.905
INFO:root:2024-04-26 19:36:43, Train, Epoch : 3, Step : 1510, Loss : 0.36677, Acc : 0.850, Sensitive_Loss : 0.10339, Sensitive_Acc : 16.800, Run Time : 99.98 sec
INFO:root:2024-04-26 19:36:51, Train, Epoch : 3, Step : 1520, Loss : 0.37496, Acc : 0.853, Sensitive_Loss : 0.08649, Sensitive_Acc : 16.900, Run Time : 7.83 sec
INFO:root:2024-04-26 19:36:59, Train, Epoch : 3, Step : 1530, Loss : 0.35122, Acc : 0.847, Sensitive_Loss : 0.07363, Sensitive_Acc : 15.100, Run Time : 8.20 sec
INFO:root:2024-04-26 19:37:06, Train, Epoch : 3, Step : 1540, Loss : 0.35301, Acc : 0.863, Sensitive_Loss : 0.11449, Sensitive_Acc : 16.700, Run Time : 7.34 sec
INFO:root:2024-04-26 19:37:14, Train, Epoch : 3, Step : 1550, Loss : 0.37963, Acc : 0.856, Sensitive_Loss : 0.08237, Sensitive_Acc : 16.700, Run Time : 7.92 sec
INFO:root:2024-04-26 19:37:22, Train, Epoch : 3, Step : 1560, Loss : 0.43410, Acc : 0.806, Sensitive_Loss : 0.10257, Sensitive_Acc : 16.200, Run Time : 7.71 sec
INFO:root:2024-04-26 19:37:29, Train, Epoch : 3, Step : 1570, Loss : 0.30292, Acc : 0.866, Sensitive_Loss : 0.12695, Sensitive_Acc : 13.900, Run Time : 7.45 sec
INFO:root:2024-04-26 19:37:37, Train, Epoch : 3, Step : 1580, Loss : 0.33027, Acc : 0.881, Sensitive_Loss : 0.07910, Sensitive_Acc : 16.100, Run Time : 7.72 sec
INFO:root:2024-04-26 19:37:45, Train, Epoch : 3, Step : 1590, Loss : 0.37770, Acc : 0.869, Sensitive_Loss : 0.07615, Sensitive_Acc : 15.900, Run Time : 7.79 sec
INFO:root:2024-04-26 19:37:52, Train, Epoch : 3, Step : 1600, Loss : 0.32866, Acc : 0.853, Sensitive_Loss : 0.11493, Sensitive_Acc : 16.400, Run Time : 7.44 sec
INFO:root:2024-04-26 19:39:26, Dev, Step : 1600, Loss : 0.42532, Acc : 0.819, Auc : 0.905, Sensitive_Loss : 0.12347, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.905, Run Time : 94.13 sec
INFO:root:2024-04-26 19:39:32, Train, Epoch : 3, Step : 1610, Loss : 0.29866, Acc : 0.859, Sensitive_Loss : 0.11118, Sensitive_Acc : 15.800, Run Time : 99.69 sec
INFO:root:2024-04-26 19:39:40, Train, Epoch : 3, Step : 1620, Loss : 0.30891, Acc : 0.856, Sensitive_Loss : 0.09665, Sensitive_Acc : 17.600, Run Time : 7.90 sec
INFO:root:2024-04-26 19:39:47, Train, Epoch : 3, Step : 1630, Loss : 0.36398, Acc : 0.844, Sensitive_Loss : 0.08226, Sensitive_Acc : 15.700, Run Time : 7.39 sec
INFO:root:2024-04-26 19:39:55, Train, Epoch : 3, Step : 1640, Loss : 0.35502, Acc : 0.853, Sensitive_Loss : 0.12631, Sensitive_Acc : 15.800, Run Time : 7.51 sec
INFO:root:2024-04-26 19:40:02, Train, Epoch : 3, Step : 1650, Loss : 0.37377, Acc : 0.838, Sensitive_Loss : 0.06555, Sensitive_Acc : 17.900, Run Time : 7.43 sec
INFO:root:2024-04-26 19:40:10, Train, Epoch : 3, Step : 1660, Loss : 0.30737, Acc : 0.856, Sensitive_Loss : 0.06930, Sensitive_Acc : 16.100, Run Time : 7.57 sec
INFO:root:2024-04-26 19:40:17, Train, Epoch : 3, Step : 1670, Loss : 0.30920, Acc : 0.863, Sensitive_Loss : 0.07684, Sensitive_Acc : 16.100, Run Time : 7.25 sec
INFO:root:2024-04-26 19:40:24, Train, Epoch : 3, Step : 1680, Loss : 0.35194, Acc : 0.863, Sensitive_Loss : 0.09711, Sensitive_Acc : 17.900, Run Time : 7.41 sec
INFO:root:2024-04-26 19:40:32, Train, Epoch : 3, Step : 1690, Loss : 0.33943, Acc : 0.831, Sensitive_Loss : 0.16644, Sensitive_Acc : 16.500, Run Time : 7.47 sec
INFO:root:2024-04-26 19:40:39, Train, Epoch : 3, Step : 1700, Loss : 0.45573, Acc : 0.831, Sensitive_Loss : 0.09291, Sensitive_Acc : 16.000, Run Time : 7.26 sec
INFO:root:2024-04-26 19:42:13, Dev, Step : 1700, Loss : 0.41618, Acc : 0.823, Auc : 0.907, Sensitive_Loss : 0.11052, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.907, Run Time : 93.93 sec
INFO:root:2024-04-26 19:42:14, Best, Step : 1700, Loss : 0.41618, Acc : 0.823, Auc : 0.907, Sensitive_Loss : 0.11052, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Best Auc : 0.907
INFO:root:2024-04-26 19:42:19, Train, Epoch : 3, Step : 1710, Loss : 0.47421, Acc : 0.797, Sensitive_Loss : 0.11974, Sensitive_Acc : 16.900, Run Time : 100.18 sec
INFO:root:2024-04-26 19:42:27, Train, Epoch : 3, Step : 1720, Loss : 0.41466, Acc : 0.841, Sensitive_Loss : 0.08282, Sensitive_Acc : 16.600, Run Time : 7.52 sec
INFO:root:2024-04-26 19:42:34, Train, Epoch : 3, Step : 1730, Loss : 0.36975, Acc : 0.822, Sensitive_Loss : 0.11677, Sensitive_Acc : 16.100, Run Time : 7.75 sec
INFO:root:2024-04-26 19:42:42, Train, Epoch : 3, Step : 1740, Loss : 0.33860, Acc : 0.866, Sensitive_Loss : 0.09763, Sensitive_Acc : 14.900, Run Time : 7.35 sec
INFO:root:2024-04-26 19:42:49, Train, Epoch : 3, Step : 1750, Loss : 0.33724, Acc : 0.853, Sensitive_Loss : 0.10535, Sensitive_Acc : 16.900, Run Time : 7.43 sec
INFO:root:2024-04-26 19:42:57, Train, Epoch : 3, Step : 1760, Loss : 0.36836, Acc : 0.825, Sensitive_Loss : 0.09724, Sensitive_Acc : 15.400, Run Time : 7.65 sec
INFO:root:2024-04-26 19:43:04, Train, Epoch : 3, Step : 1770, Loss : 0.33177, Acc : 0.853, Sensitive_Loss : 0.10136, Sensitive_Acc : 15.700, Run Time : 7.54 sec
INFO:root:2024-04-26 19:43:12, Train, Epoch : 3, Step : 1780, Loss : 0.35589, Acc : 0.803, Sensitive_Loss : 0.11737, Sensitive_Acc : 16.500, Run Time : 7.57 sec
INFO:root:2024-04-26 19:43:19, Train, Epoch : 3, Step : 1790, Loss : 0.34320, Acc : 0.838, Sensitive_Loss : 0.11408, Sensitive_Acc : 16.300, Run Time : 7.29 sec
INFO:root:2024-04-26 19:43:27, Train, Epoch : 3, Step : 1800, Loss : 0.39132, Acc : 0.844, Sensitive_Loss : 0.06446, Sensitive_Acc : 16.700, Run Time : 7.38 sec
INFO:root:2024-04-26 19:45:01, Dev, Step : 1800, Loss : 0.40800, Acc : 0.825, Auc : 0.907, Sensitive_Loss : 0.11752, Sensitive_Acc : 16.864, Sensitive_Auc : 0.992, Mean auc: 0.907, Run Time : 93.86 sec
INFO:root:2024-04-26 19:45:01, Best, Step : 1800, Loss : 0.40800, Acc : 0.825, Auc : 0.907, Sensitive_Loss : 0.11752, Sensitive_Acc : 16.864, Sensitive_Auc : 0.992, Best Auc : 0.907
INFO:root:2024-04-26 19:45:07, Train, Epoch : 3, Step : 1810, Loss : 0.39748, Acc : 0.825, Sensitive_Loss : 0.09801, Sensitive_Acc : 17.700, Run Time : 100.17 sec
INFO:root:2024-04-26 19:45:15, Train, Epoch : 3, Step : 1820, Loss : 0.46271, Acc : 0.816, Sensitive_Loss : 0.08474, Sensitive_Acc : 16.800, Run Time : 7.83 sec
INFO:root:2024-04-26 19:45:22, Train, Epoch : 3, Step : 1830, Loss : 0.32036, Acc : 0.869, Sensitive_Loss : 0.09537, Sensitive_Acc : 16.900, Run Time : 7.41 sec
INFO:root:2024-04-26 19:45:30, Train, Epoch : 3, Step : 1840, Loss : 0.37650, Acc : 0.828, Sensitive_Loss : 0.11275, Sensitive_Acc : 17.200, Run Time : 7.77 sec
INFO:root:2024-04-26 19:45:37, Train, Epoch : 3, Step : 1850, Loss : 0.33047, Acc : 0.875, Sensitive_Loss : 0.08919, Sensitive_Acc : 16.700, Run Time : 6.83 sec
INFO:root:2024-04-26 19:45:44, Train, Epoch : 3, Step : 1860, Loss : 0.26935, Acc : 0.878, Sensitive_Loss : 0.12586, Sensitive_Acc : 16.600, Run Time : 6.90 sec
INFO:root:2024-04-26 19:45:51, Train, Epoch : 3, Step : 1870, Loss : 0.38020, Acc : 0.828, Sensitive_Loss : 0.07052, Sensitive_Acc : 16.200, Run Time : 7.19 sec
INFO:root:2024-04-26 19:47:29
INFO:root:y_pred: [0.07530462 0.907305   0.05697294 ... 0.7472753  0.01538576 0.89770156]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.88426983e-01 3.50693095e-04 9.05513950e-03 3.28655675e-04
 9.99952435e-01 1.33275695e-04 9.99994159e-01 9.99845386e-01
 8.13732948e-03 8.40049446e-01 9.99077320e-01 9.99972582e-01
 9.93531168e-01 9.96718466e-01 6.74847290e-02 9.66566801e-01
 9.99914289e-01 2.44910233e-02 6.33185267e-01 9.41342294e-01
 9.99570787e-01 1.17287181e-01 9.99240279e-01 9.98432338e-01
 9.99652028e-01 9.93901372e-01 7.15522503e-04 9.99978065e-01
 9.98398840e-01 4.41626728e-01 2.51099048e-03 2.90215075e-01
 2.74798453e-01 1.71971723e-01 1.18811108e-01 9.57870972e-04
 1.75308082e-02 1.13617699e-03 9.99910474e-01 9.99826610e-01
 7.40095202e-05 1.19024154e-03 9.93510067e-01 3.01981956e-04
 9.99994040e-01 9.96632159e-01 9.99687552e-01 9.94245112e-01
 1.60547055e-03 9.98723686e-01 9.99239922e-01 1.10951788e-03
 4.50160652e-01 5.34380786e-04 3.14706092e-04 3.02695725e-02
 8.82064551e-02 4.38739546e-02 1.39997562e-03 9.35347751e-03
 1.92320198e-02 2.28930980e-01 4.95886318e-02 9.51941967e-01
 7.59006083e-01 9.99950051e-01 6.19968921e-02 9.99920487e-01
 9.99131978e-01 7.82168388e-01 7.96344876e-01 7.68863618e-01
 1.66799675e-03 9.65238810e-02 6.34717150e-03 1.22698070e-03
 1.51889950e-01 3.47219557e-02 2.37815781e-03 9.99574721e-01
 9.99848485e-01 3.09781125e-03 8.05256441e-02 2.12409478e-02
 9.47934270e-01 9.74051356e-01 2.23736223e-02 7.23462775e-02
 9.98488307e-01 9.99907136e-01 9.99997854e-01 2.74208393e-02
 3.75698321e-02 9.97972667e-01 2.33172953e-01 6.92912610e-03
 9.99542594e-01 9.99609649e-01 3.12662334e-04 4.26368378e-02
 9.98405993e-01 9.93204117e-01 9.87679720e-01 9.98572469e-01
 7.52727094e-04 1.89736858e-02 9.85281885e-01 9.99732435e-01
 9.92765546e-01 5.22966729e-06 9.99049246e-01 9.99918580e-01
 4.40666154e-02 9.99947429e-01 9.99396920e-01 9.99626517e-01
 8.85041475e-01 9.99579728e-01 1.83603764e-02 4.57351618e-02
 9.99761879e-01 9.96623397e-01 1.75115958e-04 9.79986131e-01
 9.99997020e-01 6.29134357e-01 9.88284588e-01 7.75995944e-03
 1.97371133e-02 9.92764771e-01 9.99916434e-01 2.19607316e-02
 6.44603185e-03 1.60833858e-02 9.99710977e-01 9.95515883e-01
 9.98208523e-01 3.35726561e-03 1.27344355e-02 9.96492684e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 19:47:29, Dev, Step : 1878, Loss : 0.40862, Acc : 0.824, Auc : 0.907, Sensitive_Loss : 0.12356, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.907, Run Time : 92.93 sec
INFO:root:2024-04-26 19:47:30, Best, Step : 1878, Loss : 0.40862, Acc : 0.824,Auc : 0.907, Best Auc : 0.907, Sensitive_Loss : 0.12356, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993
INFO:root:2024-04-26 19:47:34, Train, Epoch : 4, Step : 1880, Loss : 0.07677, Acc : 0.169, Sensitive_Loss : 0.01609, Sensitive_Acc : 3.000, Run Time : 2.83 sec
INFO:root:2024-04-26 19:47:40, Train, Epoch : 4, Step : 1890, Loss : 0.30080, Acc : 0.841, Sensitive_Loss : 0.11152, Sensitive_Acc : 16.700, Run Time : 6.76 sec
INFO:root:2024-04-26 19:47:47, Train, Epoch : 4, Step : 1900, Loss : 0.29724, Acc : 0.850, Sensitive_Loss : 0.08876, Sensitive_Acc : 16.100, Run Time : 7.02 sec
INFO:root:2024-04-26 19:49:22, Dev, Step : 1900, Loss : 0.43877, Acc : 0.812, Auc : 0.908, Sensitive_Loss : 0.14888, Sensitive_Acc : 16.836, Sensitive_Auc : 0.991, Mean auc: 0.908, Run Time : 94.09 sec
INFO:root:2024-04-26 19:49:22, Best, Step : 1900, Loss : 0.43877, Acc : 0.812, Auc : 0.908, Sensitive_Loss : 0.14888, Sensitive_Acc : 16.836, Sensitive_Auc : 0.991, Best Auc : 0.908
INFO:root:2024-04-26 19:49:28, Train, Epoch : 4, Step : 1910, Loss : 0.27926, Acc : 0.863, Sensitive_Loss : 0.09725, Sensitive_Acc : 16.200, Run Time : 100.26 sec
INFO:root:2024-04-26 19:49:35, Train, Epoch : 4, Step : 1920, Loss : 0.35200, Acc : 0.872, Sensitive_Loss : 0.07977, Sensitive_Acc : 17.300, Run Time : 6.99 sec
INFO:root:2024-04-26 19:49:42, Train, Epoch : 4, Step : 1930, Loss : 0.35752, Acc : 0.859, Sensitive_Loss : 0.11935, Sensitive_Acc : 16.900, Run Time : 7.53 sec
INFO:root:2024-04-26 19:49:49, Train, Epoch : 4, Step : 1940, Loss : 0.31323, Acc : 0.856, Sensitive_Loss : 0.09466, Sensitive_Acc : 16.700, Run Time : 7.08 sec
INFO:root:2024-04-26 19:49:57, Train, Epoch : 4, Step : 1950, Loss : 0.37795, Acc : 0.825, Sensitive_Loss : 0.06463, Sensitive_Acc : 16.500, Run Time : 7.63 sec
INFO:root:2024-04-26 19:50:04, Train, Epoch : 4, Step : 1960, Loss : 0.26480, Acc : 0.881, Sensitive_Loss : 0.10201, Sensitive_Acc : 16.800, Run Time : 7.11 sec
INFO:root:2024-04-26 19:50:11, Train, Epoch : 4, Step : 1970, Loss : 0.31418, Acc : 0.866, Sensitive_Loss : 0.08270, Sensitive_Acc : 16.300, Run Time : 6.95 sec
INFO:root:2024-04-26 19:50:18, Train, Epoch : 4, Step : 1980, Loss : 0.29835, Acc : 0.863, Sensitive_Loss : 0.07656, Sensitive_Acc : 17.100, Run Time : 7.36 sec
INFO:root:2024-04-26 19:50:25, Train, Epoch : 4, Step : 1990, Loss : 0.36470, Acc : 0.853, Sensitive_Loss : 0.07825, Sensitive_Acc : 15.600, Run Time : 6.67 sec
INFO:root:2024-04-26 19:50:32, Train, Epoch : 4, Step : 2000, Loss : 0.35854, Acc : 0.838, Sensitive_Loss : 0.07094, Sensitive_Acc : 15.000, Run Time : 7.34 sec
INFO:root:2024-04-26 19:52:06, Dev, Step : 2000, Loss : 0.40729, Acc : 0.830, Auc : 0.908, Sensitive_Loss : 0.11095, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.908, Run Time : 93.56 sec
INFO:root:2024-04-26 19:52:12, Train, Epoch : 4, Step : 2010, Loss : 0.33933, Acc : 0.853, Sensitive_Loss : 0.10812, Sensitive_Acc : 17.300, Run Time : 99.48 sec
INFO:root:2024-04-26 19:52:19, Train, Epoch : 4, Step : 2020, Loss : 0.32731, Acc : 0.831, Sensitive_Loss : 0.08279, Sensitive_Acc : 16.400, Run Time : 6.79 sec
INFO:root:2024-04-26 19:52:26, Train, Epoch : 4, Step : 2030, Loss : 0.34714, Acc : 0.859, Sensitive_Loss : 0.09551, Sensitive_Acc : 15.600, Run Time : 7.28 sec
INFO:root:2024-04-26 19:52:33, Train, Epoch : 4, Step : 2040, Loss : 0.42085, Acc : 0.841, Sensitive_Loss : 0.08613, Sensitive_Acc : 15.200, Run Time : 7.12 sec
INFO:root:2024-04-26 19:52:40, Train, Epoch : 4, Step : 2050, Loss : 0.34229, Acc : 0.872, Sensitive_Loss : 0.09483, Sensitive_Acc : 16.900, Run Time : 7.15 sec
INFO:root:2024-04-26 19:52:47, Train, Epoch : 4, Step : 2060, Loss : 0.34051, Acc : 0.866, Sensitive_Loss : 0.09796, Sensitive_Acc : 15.900, Run Time : 7.16 sec
INFO:root:2024-04-26 19:52:55, Train, Epoch : 4, Step : 2070, Loss : 0.32167, Acc : 0.838, Sensitive_Loss : 0.08607, Sensitive_Acc : 16.600, Run Time : 7.33 sec
INFO:root:2024-04-26 19:53:02, Train, Epoch : 4, Step : 2080, Loss : 0.36497, Acc : 0.850, Sensitive_Loss : 0.10638, Sensitive_Acc : 16.700, Run Time : 7.73 sec
INFO:root:2024-04-26 19:53:09, Train, Epoch : 4, Step : 2090, Loss : 0.32013, Acc : 0.847, Sensitive_Loss : 0.11659, Sensitive_Acc : 16.300, Run Time : 6.83 sec
INFO:root:2024-04-26 19:53:17, Train, Epoch : 4, Step : 2100, Loss : 0.32776, Acc : 0.856, Sensitive_Loss : 0.09612, Sensitive_Acc : 16.100, Run Time : 7.62 sec
INFO:root:2024-04-26 19:54:50, Dev, Step : 2100, Loss : 0.41220, Acc : 0.827, Auc : 0.907, Sensitive_Loss : 0.11472, Sensitive_Acc : 16.864, Sensitive_Auc : 0.992, Mean auc: 0.907, Run Time : 93.24 sec
INFO:root:2024-04-26 19:54:56, Train, Epoch : 4, Step : 2110, Loss : 0.32754, Acc : 0.859, Sensitive_Loss : 0.09757, Sensitive_Acc : 15.600, Run Time : 98.76 sec
INFO:root:2024-04-26 19:55:03, Train, Epoch : 4, Step : 2120, Loss : 0.39188, Acc : 0.859, Sensitive_Loss : 0.09722, Sensitive_Acc : 18.200, Run Time : 7.43 sec
INFO:root:2024-04-26 19:55:10, Train, Epoch : 4, Step : 2130, Loss : 0.31373, Acc : 0.866, Sensitive_Loss : 0.11373, Sensitive_Acc : 15.900, Run Time : 7.37 sec
INFO:root:2024-04-26 19:55:18, Train, Epoch : 4, Step : 2140, Loss : 0.33179, Acc : 0.856, Sensitive_Loss : 0.08471, Sensitive_Acc : 15.100, Run Time : 7.18 sec
INFO:root:2024-04-26 19:55:25, Train, Epoch : 4, Step : 2150, Loss : 0.37464, Acc : 0.847, Sensitive_Loss : 0.08148, Sensitive_Acc : 15.300, Run Time : 7.22 sec
INFO:root:2024-04-26 19:55:32, Train, Epoch : 4, Step : 2160, Loss : 0.34240, Acc : 0.838, Sensitive_Loss : 0.09620, Sensitive_Acc : 16.600, Run Time : 7.22 sec
INFO:root:2024-04-26 19:55:39, Train, Epoch : 4, Step : 2170, Loss : 0.33778, Acc : 0.856, Sensitive_Loss : 0.09315, Sensitive_Acc : 15.300, Run Time : 7.33 sec
INFO:root:2024-04-26 19:55:46, Train, Epoch : 4, Step : 2180, Loss : 0.32925, Acc : 0.834, Sensitive_Loss : 0.08295, Sensitive_Acc : 16.200, Run Time : 7.06 sec
INFO:root:2024-04-26 19:55:54, Train, Epoch : 4, Step : 2190, Loss : 0.38833, Acc : 0.847, Sensitive_Loss : 0.08148, Sensitive_Acc : 16.700, Run Time : 7.47 sec
INFO:root:2024-04-26 19:56:01, Train, Epoch : 4, Step : 2200, Loss : 0.35858, Acc : 0.838, Sensitive_Loss : 0.08129, Sensitive_Acc : 17.300, Run Time : 7.09 sec
INFO:root:2024-04-26 19:57:35, Dev, Step : 2200, Loss : 0.41267, Acc : 0.828, Auc : 0.907, Sensitive_Loss : 0.11024, Sensitive_Acc : 16.864, Sensitive_Auc : 0.993, Mean auc: 0.907, Run Time : 93.73 sec
INFO:root:2024-04-26 19:57:40, Train, Epoch : 4, Step : 2210, Loss : 0.27314, Acc : 0.850, Sensitive_Loss : 0.09320, Sensitive_Acc : 15.800, Run Time : 99.37 sec
INFO:root:2024-04-26 19:57:48, Train, Epoch : 4, Step : 2220, Loss : 0.29784, Acc : 0.847, Sensitive_Loss : 0.06524, Sensitive_Acc : 16.300, Run Time : 7.26 sec
INFO:root:2024-04-26 19:57:55, Train, Epoch : 4, Step : 2230, Loss : 0.40573, Acc : 0.831, Sensitive_Loss : 0.09078, Sensitive_Acc : 17.400, Run Time : 7.14 sec
INFO:root:2024-04-26 19:58:02, Train, Epoch : 4, Step : 2240, Loss : 0.31994, Acc : 0.869, Sensitive_Loss : 0.07423, Sensitive_Acc : 16.100, Run Time : 7.38 sec
INFO:root:2024-04-26 19:58:10, Train, Epoch : 4, Step : 2250, Loss : 0.29302, Acc : 0.856, Sensitive_Loss : 0.07746, Sensitive_Acc : 16.100, Run Time : 7.99 sec
INFO:root:2024-04-26 19:58:17, Train, Epoch : 4, Step : 2260, Loss : 0.32765, Acc : 0.834, Sensitive_Loss : 0.08988, Sensitive_Acc : 15.600, Run Time : 6.65 sec
INFO:root:2024-04-26 19:58:24, Train, Epoch : 4, Step : 2270, Loss : 0.33735, Acc : 0.859, Sensitive_Loss : 0.10139, Sensitive_Acc : 16.500, Run Time : 7.56 sec
INFO:root:2024-04-26 19:58:31, Train, Epoch : 4, Step : 2280, Loss : 0.32065, Acc : 0.872, Sensitive_Loss : 0.07249, Sensitive_Acc : 15.500, Run Time : 7.13 sec
INFO:root:2024-04-26 19:58:39, Train, Epoch : 4, Step : 2290, Loss : 0.32061, Acc : 0.869, Sensitive_Loss : 0.08560, Sensitive_Acc : 17.300, Run Time : 7.18 sec
INFO:root:2024-04-26 19:58:46, Train, Epoch : 4, Step : 2300, Loss : 0.34485, Acc : 0.841, Sensitive_Loss : 0.07233, Sensitive_Acc : 17.500, Run Time : 7.31 sec
INFO:root:2024-04-26 20:00:20, Dev, Step : 2300, Loss : 0.41647, Acc : 0.822, Auc : 0.910, Sensitive_Loss : 0.12892, Sensitive_Acc : 16.864, Sensitive_Auc : 0.991, Mean auc: 0.910, Run Time : 94.23 sec
INFO:root:2024-04-26 20:00:21, Best, Step : 2300, Loss : 0.41647, Acc : 0.822, Auc : 0.910, Sensitive_Loss : 0.12892, Sensitive_Acc : 16.864, Sensitive_Auc : 0.991, Best Auc : 0.910
INFO:root:2024-04-26 20:00:27, Train, Epoch : 4, Step : 2310, Loss : 0.29480, Acc : 0.859, Sensitive_Loss : 0.14183, Sensitive_Acc : 15.400, Run Time : 100.91 sec
INFO:root:2024-04-26 20:00:34, Train, Epoch : 4, Step : 2320, Loss : 0.31231, Acc : 0.847, Sensitive_Loss : 0.09990, Sensitive_Acc : 17.100, Run Time : 7.42 sec
INFO:root:2024-04-26 20:00:41, Train, Epoch : 4, Step : 2330, Loss : 0.33483, Acc : 0.850, Sensitive_Loss : 0.08814, Sensitive_Acc : 15.200, Run Time : 6.63 sec
INFO:root:2024-04-26 20:00:48, Train, Epoch : 4, Step : 2340, Loss : 0.36945, Acc : 0.869, Sensitive_Loss : 0.08935, Sensitive_Acc : 17.900, Run Time : 7.50 sec
INFO:root:2024-04-26 20:00:56, Train, Epoch : 4, Step : 2350, Loss : 0.39449, Acc : 0.838, Sensitive_Loss : 0.07755, Sensitive_Acc : 16.200, Run Time : 7.51 sec
INFO:root:2024-04-26 20:01:03, Train, Epoch : 4, Step : 2360, Loss : 0.36195, Acc : 0.859, Sensitive_Loss : 0.08051, Sensitive_Acc : 16.800, Run Time : 7.05 sec
INFO:root:2024-04-26 20:01:11, Train, Epoch : 4, Step : 2370, Loss : 0.34499, Acc : 0.853, Sensitive_Loss : 0.06534, Sensitive_Acc : 15.500, Run Time : 7.55 sec
INFO:root:2024-04-26 20:01:18, Train, Epoch : 4, Step : 2380, Loss : 0.32282, Acc : 0.872, Sensitive_Loss : 0.08815, Sensitive_Acc : 16.700, Run Time : 7.12 sec
INFO:root:2024-04-26 20:01:25, Train, Epoch : 4, Step : 2390, Loss : 0.32129, Acc : 0.881, Sensitive_Loss : 0.11103, Sensitive_Acc : 15.300, Run Time : 7.17 sec
INFO:root:2024-04-26 20:01:32, Train, Epoch : 4, Step : 2400, Loss : 0.35555, Acc : 0.872, Sensitive_Loss : 0.07995, Sensitive_Acc : 16.700, Run Time : 7.53 sec
INFO:root:2024-04-26 20:03:07, Dev, Step : 2400, Loss : 0.41055, Acc : 0.823, Auc : 0.909, Sensitive_Loss : 0.11716, Sensitive_Acc : 16.921, Sensitive_Auc : 0.992, Mean auc: 0.909, Run Time : 94.32 sec
INFO:root:2024-04-26 20:03:13, Train, Epoch : 4, Step : 2410, Loss : 0.28230, Acc : 0.856, Sensitive_Loss : 0.11544, Sensitive_Acc : 15.700, Run Time : 100.58 sec
INFO:root:2024-04-26 20:03:21, Train, Epoch : 4, Step : 2420, Loss : 0.31459, Acc : 0.869, Sensitive_Loss : 0.09686, Sensitive_Acc : 16.400, Run Time : 7.81 sec
INFO:root:2024-04-26 20:03:28, Train, Epoch : 4, Step : 2430, Loss : 0.34679, Acc : 0.856, Sensitive_Loss : 0.06347, Sensitive_Acc : 15.100, Run Time : 6.83 sec
INFO:root:2024-04-26 20:03:36, Train, Epoch : 4, Step : 2440, Loss : 0.38290, Acc : 0.834, Sensitive_Loss : 0.08241, Sensitive_Acc : 15.400, Run Time : 7.91 sec
INFO:root:2024-04-26 20:03:43, Train, Epoch : 4, Step : 2450, Loss : 0.35473, Acc : 0.822, Sensitive_Loss : 0.10786, Sensitive_Acc : 17.600, Run Time : 7.19 sec
INFO:root:2024-04-26 20:03:50, Train, Epoch : 4, Step : 2460, Loss : 0.28364, Acc : 0.831, Sensitive_Loss : 0.08609, Sensitive_Acc : 16.300, Run Time : 7.34 sec
INFO:root:2024-04-26 20:03:58, Train, Epoch : 4, Step : 2470, Loss : 0.34774, Acc : 0.847, Sensitive_Loss : 0.08246, Sensitive_Acc : 16.700, Run Time : 7.51 sec
INFO:root:2024-04-26 20:04:06, Train, Epoch : 4, Step : 2480, Loss : 0.36561, Acc : 0.844, Sensitive_Loss : 0.10194, Sensitive_Acc : 14.900, Run Time : 7.98 sec
INFO:root:2024-04-26 20:04:14, Train, Epoch : 4, Step : 2490, Loss : 0.32033, Acc : 0.859, Sensitive_Loss : 0.08336, Sensitive_Acc : 17.600, Run Time : 8.59 sec
INFO:root:2024-04-26 20:04:23, Train, Epoch : 4, Step : 2500, Loss : 0.29878, Acc : 0.875, Sensitive_Loss : 0.09050, Sensitive_Acc : 17.100, Run Time : 8.90 sec
INFO:root:2024-04-26 20:05:58, Dev, Step : 2500, Loss : 0.41155, Acc : 0.826, Auc : 0.910, Sensitive_Loss : 0.12941, Sensitive_Acc : 16.921, Sensitive_Auc : 0.991, Mean auc: 0.910, Run Time : 94.86 sec
INFO:root:2024-04-26 20:05:59, Best, Step : 2500, Loss : 0.41155, Acc : 0.826, Auc : 0.910, Sensitive_Loss : 0.12941, Sensitive_Acc : 16.921, Sensitive_Auc : 0.991, Best Auc : 0.910
INFO:root:2024-04-26 20:07:32
INFO:root:y_pred: [0.11604125 0.9550963  0.03795937 ... 0.8223096  0.00493437 0.9356306 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.8287427e-01 3.5937619e-04 1.6158687e-02 5.0487876e-04 9.9992907e-01
 1.5151333e-04 9.9999726e-01 9.9991405e-01 8.6022308e-03 9.1259181e-01
 9.9969149e-01 9.9998593e-01 9.9520135e-01 9.9628758e-01 1.0622453e-01
 9.7832286e-01 9.9994469e-01 2.2757700e-02 3.2917991e-01 9.5356232e-01
 9.9941969e-01 9.0594441e-02 9.9966300e-01 9.9845314e-01 9.9968648e-01
 9.9601799e-01 3.4721423e-04 9.9999166e-01 9.9545997e-01 3.8905796e-01
 4.2618494e-03 2.6623958e-01 2.2467981e-01 1.2777647e-01 3.4593046e-02
 7.5551326e-04 9.1901841e-03 9.9521526e-04 9.9996388e-01 9.9982834e-01
 4.2278007e-05 5.8036268e-04 9.9569380e-01 2.5113745e-04 9.9999738e-01
 9.9876499e-01 9.9984896e-01 9.9576169e-01 3.1870797e-03 9.9881834e-01
 9.9979526e-01 6.9993886e-04 4.2606768e-01 5.7681929e-04 1.7327776e-04
 2.3236133e-02 7.6995462e-02 2.3660252e-02 1.0851006e-03 6.6329357e-03
 1.4982926e-02 2.5715354e-01 1.0646493e-01 9.7467208e-01 6.1096889e-01
 9.9997175e-01 6.7866646e-02 9.9998736e-01 9.9965298e-01 7.7526188e-01
 8.8921398e-01 7.9270959e-01 1.0912098e-03 7.2721355e-02 7.9682795e-03
 1.1377886e-03 7.7395149e-02 1.8617479e-02 5.4990291e-03 9.9974376e-01
 9.9987781e-01 5.3413101e-03 9.3296267e-02 5.9724380e-03 9.7292656e-01
 9.8536563e-01 2.3819758e-02 9.2342347e-02 9.9822730e-01 9.9994910e-01
 9.9999881e-01 1.7320877e-02 1.2326100e-02 9.9917322e-01 1.7011181e-01
 7.1324622e-03 9.9979180e-01 9.9962008e-01 2.4793539e-04 1.5194321e-02
 9.9779248e-01 9.9524057e-01 9.9546981e-01 9.9883634e-01 5.3033006e-04
 7.3678405e-03 9.8845321e-01 9.9971443e-01 9.9451506e-01 8.5513429e-06
 9.9944013e-01 9.9992847e-01 7.0220269e-02 9.9996328e-01 9.9928647e-01
 9.9976474e-01 8.8662684e-01 9.9951482e-01 2.1323079e-02 4.2275544e-02
 9.9966359e-01 9.9757558e-01 1.8278701e-04 9.9001241e-01 9.9999821e-01
 6.9960636e-01 9.8331833e-01 1.1742525e-02 1.0098496e-02 9.9235922e-01
 9.9991345e-01 1.9372545e-02 1.7269114e-02 9.0946965e-03 9.9975568e-01
 9.9727947e-01 9.9911910e-01 1.6725614e-03 1.2429491e-02 9.9755329e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 20:07:32, Dev, Step : 2504, Loss : 0.40525, Acc : 0.830, Auc : 0.910, Sensitive_Loss : 0.12069, Sensitive_Acc : 16.864, Sensitive_Auc : 0.991, Mean auc: 0.910, Run Time : 91.84 sec
INFO:root:2024-04-26 20:07:33, Best, Step : 2504, Loss : 0.40525, Acc : 0.830,Auc : 0.910, Best Auc : 0.910, Sensitive_Loss : 0.12069, Sensitive_Acc : 16.864, Sensitive_Auc : 0.991
INFO:root:2024-04-26 20:07:39, Train, Epoch : 5, Step : 2510, Loss : 0.19991, Acc : 0.516, Sensitive_Loss : 0.04245, Sensitive_Acc : 10.400, Run Time : 5.52 sec
INFO:root:2024-04-26 20:07:47, Train, Epoch : 5, Step : 2520, Loss : 0.30194, Acc : 0.878, Sensitive_Loss : 0.12305, Sensitive_Acc : 17.200, Run Time : 7.79 sec
INFO:root:2024-04-26 20:07:54, Train, Epoch : 5, Step : 2530, Loss : 0.29313, Acc : 0.869, Sensitive_Loss : 0.07901, Sensitive_Acc : 17.000, Run Time : 7.14 sec
INFO:root:2024-04-26 20:08:01, Train, Epoch : 5, Step : 2540, Loss : 0.25098, Acc : 0.903, Sensitive_Loss : 0.07454, Sensitive_Acc : 17.100, Run Time : 7.50 sec
INFO:root:2024-04-26 20:08:09, Train, Epoch : 5, Step : 2550, Loss : 0.30469, Acc : 0.878, Sensitive_Loss : 0.08469, Sensitive_Acc : 16.700, Run Time : 7.52 sec
INFO:root:2024-04-26 20:08:17, Train, Epoch : 5, Step : 2560, Loss : 0.27796, Acc : 0.878, Sensitive_Loss : 0.12083, Sensitive_Acc : 17.800, Run Time : 7.75 sec
INFO:root:2024-04-26 20:08:24, Train, Epoch : 5, Step : 2570, Loss : 0.31429, Acc : 0.859, Sensitive_Loss : 0.07197, Sensitive_Acc : 15.300, Run Time : 7.54 sec
INFO:root:2024-04-26 20:08:32, Train, Epoch : 5, Step : 2580, Loss : 0.34827, Acc : 0.847, Sensitive_Loss : 0.07989, Sensitive_Acc : 16.600, Run Time : 7.65 sec
INFO:root:2024-04-26 20:08:40, Train, Epoch : 5, Step : 2590, Loss : 0.33254, Acc : 0.859, Sensitive_Loss : 0.07956, Sensitive_Acc : 15.500, Run Time : 8.30 sec
INFO:root:2024-04-26 20:08:48, Train, Epoch : 5, Step : 2600, Loss : 0.37052, Acc : 0.844, Sensitive_Loss : 0.06224, Sensitive_Acc : 16.700, Run Time : 7.50 sec
INFO:root:2024-04-26 20:10:21, Dev, Step : 2600, Loss : 0.41033, Acc : 0.832, Auc : 0.911, Sensitive_Loss : 0.11327, Sensitive_Acc : 16.907, Sensitive_Auc : 0.992, Mean auc: 0.911, Run Time : 93.47 sec
INFO:root:2024-04-26 20:10:22, Best, Step : 2600, Loss : 0.41033, Acc : 0.832, Auc : 0.911, Sensitive_Loss : 0.11327, Sensitive_Acc : 16.907, Sensitive_Auc : 0.992, Best Auc : 0.911
INFO:root:2024-04-26 20:10:27, Train, Epoch : 5, Step : 2610, Loss : 0.32382, Acc : 0.850, Sensitive_Loss : 0.10180, Sensitive_Acc : 17.300, Run Time : 99.77 sec
INFO:root:2024-04-26 20:10:36, Train, Epoch : 5, Step : 2620, Loss : 0.38923, Acc : 0.841, Sensitive_Loss : 0.09443, Sensitive_Acc : 17.900, Run Time : 8.05 sec
INFO:root:2024-04-26 20:10:43, Train, Epoch : 5, Step : 2630, Loss : 0.35488, Acc : 0.841, Sensitive_Loss : 0.09689, Sensitive_Acc : 16.800, Run Time : 7.80 sec
INFO:root:2024-04-26 20:10:51, Train, Epoch : 5, Step : 2640, Loss : 0.26388, Acc : 0.903, Sensitive_Loss : 0.07925, Sensitive_Acc : 16.600, Run Time : 7.32 sec
INFO:root:2024-04-26 20:10:58, Train, Epoch : 5, Step : 2650, Loss : 0.27215, Acc : 0.875, Sensitive_Loss : 0.09538, Sensitive_Acc : 16.400, Run Time : 7.53 sec
INFO:root:2024-04-26 20:11:06, Train, Epoch : 5, Step : 2660, Loss : 0.25373, Acc : 0.897, Sensitive_Loss : 0.04991, Sensitive_Acc : 14.900, Run Time : 7.96 sec
INFO:root:2024-04-26 20:11:14, Train, Epoch : 5, Step : 2670, Loss : 0.33608, Acc : 0.856, Sensitive_Loss : 0.07967, Sensitive_Acc : 16.400, Run Time : 7.51 sec
INFO:root:2024-04-26 20:11:22, Train, Epoch : 5, Step : 2680, Loss : 0.30792, Acc : 0.866, Sensitive_Loss : 0.07879, Sensitive_Acc : 16.700, Run Time : 8.17 sec
INFO:root:2024-04-26 20:11:31, Train, Epoch : 5, Step : 2690, Loss : 0.29624, Acc : 0.881, Sensitive_Loss : 0.06216, Sensitive_Acc : 17.300, Run Time : 9.50 sec
INFO:root:2024-04-26 20:11:39, Train, Epoch : 5, Step : 2700, Loss : 0.30209, Acc : 0.878, Sensitive_Loss : 0.06647, Sensitive_Acc : 15.900, Run Time : 7.94 sec
INFO:root:2024-04-26 20:13:13, Dev, Step : 2700, Loss : 0.40388, Acc : 0.831, Auc : 0.910, Sensitive_Loss : 0.11510, Sensitive_Acc : 16.864, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 93.64 sec
INFO:root:2024-04-26 20:13:19, Train, Epoch : 5, Step : 2710, Loss : 0.25975, Acc : 0.916, Sensitive_Loss : 0.09929, Sensitive_Acc : 17.600, Run Time : 99.40 sec
INFO:root:2024-04-26 20:13:27, Train, Epoch : 5, Step : 2720, Loss : 0.27624, Acc : 0.850, Sensitive_Loss : 0.12095, Sensitive_Acc : 15.000, Run Time : 7.86 sec
INFO:root:2024-04-26 20:13:34, Train, Epoch : 5, Step : 2730, Loss : 0.34042, Acc : 0.847, Sensitive_Loss : 0.08198, Sensitive_Acc : 17.000, Run Time : 7.93 sec
INFO:root:2024-04-26 20:13:42, Train, Epoch : 5, Step : 2740, Loss : 0.25805, Acc : 0.872, Sensitive_Loss : 0.07835, Sensitive_Acc : 15.700, Run Time : 7.86 sec
INFO:root:2024-04-26 20:13:50, Train, Epoch : 5, Step : 2750, Loss : 0.34410, Acc : 0.863, Sensitive_Loss : 0.08636, Sensitive_Acc : 17.900, Run Time : 8.07 sec
INFO:root:2024-04-26 20:13:58, Train, Epoch : 5, Step : 2760, Loss : 0.25848, Acc : 0.894, Sensitive_Loss : 0.06006, Sensitive_Acc : 15.100, Run Time : 7.76 sec
INFO:root:2024-04-26 20:14:07, Train, Epoch : 5, Step : 2770, Loss : 0.30040, Acc : 0.887, Sensitive_Loss : 0.05844, Sensitive_Acc : 16.700, Run Time : 8.42 sec
INFO:root:2024-04-26 20:14:15, Train, Epoch : 5, Step : 2780, Loss : 0.29545, Acc : 0.863, Sensitive_Loss : 0.06703, Sensitive_Acc : 17.600, Run Time : 8.16 sec
INFO:root:2024-04-26 20:14:23, Train, Epoch : 5, Step : 2790, Loss : 0.32528, Acc : 0.847, Sensitive_Loss : 0.08251, Sensitive_Acc : 16.500, Run Time : 7.95 sec
INFO:root:2024-04-26 20:14:30, Train, Epoch : 5, Step : 2800, Loss : 0.39327, Acc : 0.841, Sensitive_Loss : 0.13705, Sensitive_Acc : 17.100, Run Time : 7.68 sec
INFO:root:2024-04-26 20:16:04, Dev, Step : 2800, Loss : 0.44002, Acc : 0.815, Auc : 0.907, Sensitive_Loss : 0.13649, Sensitive_Acc : 16.893, Sensitive_Auc : 0.993, Mean auc: 0.907, Run Time : 94.04 sec
INFO:root:2024-04-26 20:16:10, Train, Epoch : 5, Step : 2810, Loss : 0.31219, Acc : 0.894, Sensitive_Loss : 0.07288, Sensitive_Acc : 17.200, Run Time : 99.97 sec
INFO:root:2024-04-26 20:16:19, Train, Epoch : 5, Step : 2820, Loss : 0.32381, Acc : 0.856, Sensitive_Loss : 0.08520, Sensitive_Acc : 17.300, Run Time : 8.48 sec
INFO:root:2024-04-26 20:16:27, Train, Epoch : 5, Step : 2830, Loss : 0.37375, Acc : 0.834, Sensitive_Loss : 0.13992, Sensitive_Acc : 15.800, Run Time : 8.03 sec
INFO:root:2024-04-26 20:16:34, Train, Epoch : 5, Step : 2840, Loss : 0.27256, Acc : 0.863, Sensitive_Loss : 0.11631, Sensitive_Acc : 15.400, Run Time : 7.57 sec
INFO:root:2024-04-26 20:16:42, Train, Epoch : 5, Step : 2850, Loss : 0.29160, Acc : 0.872, Sensitive_Loss : 0.11396, Sensitive_Acc : 16.900, Run Time : 7.77 sec
INFO:root:2024-04-26 20:16:50, Train, Epoch : 5, Step : 2860, Loss : 0.28061, Acc : 0.897, Sensitive_Loss : 0.07097, Sensitive_Acc : 17.400, Run Time : 8.03 sec
INFO:root:2024-04-26 20:16:58, Train, Epoch : 5, Step : 2870, Loss : 0.29854, Acc : 0.863, Sensitive_Loss : 0.13093, Sensitive_Acc : 15.900, Run Time : 8.20 sec
INFO:root:2024-04-26 20:17:06, Train, Epoch : 5, Step : 2880, Loss : 0.31364, Acc : 0.887, Sensitive_Loss : 0.07509, Sensitive_Acc : 16.200, Run Time : 8.00 sec
INFO:root:2024-04-26 20:17:14, Train, Epoch : 5, Step : 2890, Loss : 0.31642, Acc : 0.881, Sensitive_Loss : 0.09920, Sensitive_Acc : 16.600, Run Time : 7.76 sec
INFO:root:2024-04-26 20:17:22, Train, Epoch : 5, Step : 2900, Loss : 0.30994, Acc : 0.866, Sensitive_Loss : 0.08771, Sensitive_Acc : 17.400, Run Time : 7.63 sec
INFO:root:2024-04-26 20:18:56, Dev, Step : 2900, Loss : 0.43087, Acc : 0.822, Auc : 0.910, Sensitive_Loss : 0.13577, Sensitive_Acc : 16.864, Sensitive_Auc : 0.994, Mean auc: 0.910, Run Time : 93.96 sec
INFO:root:2024-04-26 20:19:02, Train, Epoch : 5, Step : 2910, Loss : 0.30684, Acc : 0.869, Sensitive_Loss : 0.07295, Sensitive_Acc : 16.800, Run Time : 99.89 sec
INFO:root:2024-04-26 20:19:09, Train, Epoch : 5, Step : 2920, Loss : 0.28277, Acc : 0.872, Sensitive_Loss : 0.09496, Sensitive_Acc : 15.600, Run Time : 7.59 sec
INFO:root:2024-04-26 20:19:17, Train, Epoch : 5, Step : 2930, Loss : 0.31716, Acc : 0.863, Sensitive_Loss : 0.10381, Sensitive_Acc : 16.500, Run Time : 7.90 sec
INFO:root:2024-04-26 20:19:25, Train, Epoch : 5, Step : 2940, Loss : 0.28633, Acc : 0.881, Sensitive_Loss : 0.07533, Sensitive_Acc : 17.900, Run Time : 7.43 sec
INFO:root:2024-04-26 20:19:32, Train, Epoch : 5, Step : 2950, Loss : 0.32286, Acc : 0.859, Sensitive_Loss : 0.08556, Sensitive_Acc : 18.700, Run Time : 7.90 sec
INFO:root:2024-04-26 20:19:40, Train, Epoch : 5, Step : 2960, Loss : 0.30705, Acc : 0.841, Sensitive_Loss : 0.10452, Sensitive_Acc : 14.900, Run Time : 7.33 sec
INFO:root:2024-04-26 20:19:47, Train, Epoch : 5, Step : 2970, Loss : 0.32237, Acc : 0.866, Sensitive_Loss : 0.10519, Sensitive_Acc : 18.400, Run Time : 7.35 sec
INFO:root:2024-04-26 20:19:55, Train, Epoch : 5, Step : 2980, Loss : 0.35352, Acc : 0.847, Sensitive_Loss : 0.08863, Sensitive_Acc : 15.700, Run Time : 7.65 sec
INFO:root:2024-04-26 20:20:02, Train, Epoch : 5, Step : 2990, Loss : 0.29012, Acc : 0.881, Sensitive_Loss : 0.08961, Sensitive_Acc : 16.700, Run Time : 7.56 sec
INFO:root:2024-04-26 20:20:10, Train, Epoch : 5, Step : 3000, Loss : 0.37327, Acc : 0.844, Sensitive_Loss : 0.11271, Sensitive_Acc : 15.000, Run Time : 7.81 sec
INFO:root:2024-04-26 20:21:44, Dev, Step : 3000, Loss : 0.40699, Acc : 0.833, Auc : 0.911, Sensitive_Loss : 0.09884, Sensitive_Acc : 16.921, Sensitive_Auc : 0.992, Mean auc: 0.911, Run Time : 93.86 sec
INFO:root:2024-04-26 20:21:45, Best, Step : 3000, Loss : 0.40699, Acc : 0.833, Auc : 0.911, Sensitive_Loss : 0.09884, Sensitive_Acc : 16.921, Sensitive_Auc : 0.992, Best Auc : 0.911
INFO:root:2024-04-26 20:21:51, Train, Epoch : 5, Step : 3010, Loss : 0.31591, Acc : 0.856, Sensitive_Loss : 0.09933, Sensitive_Acc : 15.800, Run Time : 100.48 sec
INFO:root:2024-04-26 20:21:59, Train, Epoch : 5, Step : 3020, Loss : 0.34106, Acc : 0.859, Sensitive_Loss : 0.05203, Sensitive_Acc : 17.100, Run Time : 7.90 sec
INFO:root:2024-04-26 20:22:06, Train, Epoch : 5, Step : 3030, Loss : 0.29257, Acc : 0.894, Sensitive_Loss : 0.08053, Sensitive_Acc : 16.400, Run Time : 7.80 sec
INFO:root:2024-04-26 20:22:14, Train, Epoch : 5, Step : 3040, Loss : 0.33377, Acc : 0.853, Sensitive_Loss : 0.07861, Sensitive_Acc : 15.600, Run Time : 7.58 sec
INFO:root:2024-04-26 20:22:21, Train, Epoch : 5, Step : 3050, Loss : 0.30960, Acc : 0.850, Sensitive_Loss : 0.06884, Sensitive_Acc : 14.800, Run Time : 7.35 sec
INFO:root:2024-04-26 20:22:29, Train, Epoch : 5, Step : 3060, Loss : 0.39466, Acc : 0.831, Sensitive_Loss : 0.08584, Sensitive_Acc : 15.700, Run Time : 7.69 sec
INFO:root:2024-04-26 20:22:37, Train, Epoch : 5, Step : 3070, Loss : 0.34906, Acc : 0.838, Sensitive_Loss : 0.11549, Sensitive_Acc : 15.600, Run Time : 7.56 sec
INFO:root:2024-04-26 20:22:44, Train, Epoch : 5, Step : 3080, Loss : 0.28982, Acc : 0.881, Sensitive_Loss : 0.05773, Sensitive_Acc : 16.500, Run Time : 7.73 sec
INFO:root:2024-04-26 20:22:52, Train, Epoch : 5, Step : 3090, Loss : 0.32180, Acc : 0.887, Sensitive_Loss : 0.09694, Sensitive_Acc : 16.100, Run Time : 7.63 sec
INFO:root:2024-04-26 20:22:59, Train, Epoch : 5, Step : 3100, Loss : 0.30348, Acc : 0.866, Sensitive_Loss : 0.06382, Sensitive_Acc : 16.300, Run Time : 7.46 sec
INFO:root:2024-04-26 20:24:33, Dev, Step : 3100, Loss : 0.41234, Acc : 0.828, Auc : 0.911, Sensitive_Loss : 0.10882, Sensitive_Acc : 16.907, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 94.09 sec
INFO:root:2024-04-26 20:24:34, Best, Step : 3100, Loss : 0.41234, Acc : 0.828, Auc : 0.911, Sensitive_Loss : 0.10882, Sensitive_Acc : 16.907, Sensitive_Auc : 0.993, Best Auc : 0.911
INFO:root:2024-04-26 20:24:40, Train, Epoch : 5, Step : 3110, Loss : 0.37053, Acc : 0.834, Sensitive_Loss : 0.11759, Sensitive_Acc : 15.700, Run Time : 100.57 sec
INFO:root:2024-04-26 20:24:48, Train, Epoch : 5, Step : 3120, Loss : 0.28306, Acc : 0.863, Sensitive_Loss : 0.11999, Sensitive_Acc : 16.000, Run Time : 7.99 sec
INFO:root:2024-04-26 20:24:55, Train, Epoch : 5, Step : 3130, Loss : 0.32532, Acc : 0.856, Sensitive_Loss : 0.10577, Sensitive_Acc : 15.200, Run Time : 7.01 sec
INFO:root:2024-04-26 20:29:02
INFO:root:y_pred: [0.05533979 0.86483765 0.01945817 ... 0.6854013  0.00309826 0.9413799 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.83741045e-01 9.27994319e-04 5.78662101e-03 7.64194352e-04
 9.99948502e-01 4.42323624e-04 9.99997258e-01 9.99861598e-01
 4.62477654e-03 9.41359639e-01 9.99445379e-01 9.99985576e-01
 9.94691670e-01 9.98123348e-01 6.68151602e-02 9.73256648e-01
 9.99961376e-01 2.83369701e-02 4.50906187e-01 9.47456777e-01
 9.99748766e-01 1.80611193e-01 9.99729455e-01 9.97483671e-01
 9.99712527e-01 9.95195687e-01 1.36140283e-04 9.99995232e-01
 9.96441066e-01 3.54340404e-01 1.69718580e-03 2.34482408e-01
 4.68481869e-01 1.50115892e-01 1.08558342e-01 7.88511534e-04
 3.19375694e-02 8.88747803e-04 9.99968171e-01 9.99764860e-01
 4.14684946e-05 1.84089749e-03 9.96453404e-01 4.59836854e-04
 9.99997377e-01 9.99051273e-01 9.99857903e-01 9.94140565e-01
 2.27965694e-03 9.98918414e-01 9.99876618e-01 6.48365531e-04
 4.62223530e-01 4.31602675e-04 2.22528339e-04 1.71089470e-02
 4.34437692e-02 2.36938056e-02 9.46681015e-04 1.21946214e-02
 1.23629877e-02 2.02869117e-01 9.56666023e-02 9.71852243e-01
 6.12000167e-01 9.99959111e-01 3.39053012e-02 9.99987125e-01
 9.99394536e-01 7.92180300e-01 9.22467411e-01 6.75302148e-01
 3.39974620e-04 9.47814882e-02 4.94160922e-03 3.43929348e-03
 6.47598580e-02 4.04706411e-02 1.14521412e-02 9.99808371e-01
 9.99889731e-01 2.32552644e-03 6.08244315e-02 6.42205123e-03
 9.77845967e-01 9.89138901e-01 3.37370038e-02 8.19614455e-02
 9.98084903e-01 9.99974966e-01 9.99998808e-01 2.23438554e-02
 2.65943110e-02 9.99270499e-01 1.55427441e-01 8.04711785e-03
 9.99640107e-01 9.99618888e-01 1.14619259e-04 1.51092780e-03
 9.97599423e-01 9.94558990e-01 9.96167839e-01 9.99119341e-01
 9.81067191e-04 3.06927669e-03 9.94842350e-01 9.99708354e-01
 9.96077836e-01 6.25928305e-06 9.99755681e-01 9.99896646e-01
 3.79737690e-02 9.99974370e-01 9.99526381e-01 9.99907851e-01
 8.85684907e-01 9.99621630e-01 2.09240876e-02 2.78558321e-02
 9.99735773e-01 9.97634053e-01 1.20912686e-04 9.88251984e-01
 9.99998689e-01 7.64743567e-01 9.73067641e-01 9.11544077e-03
 1.48556149e-02 9.92833257e-01 9.99939203e-01 1.92889329e-02
 1.67578980e-02 8.41437001e-03 9.99701083e-01 9.98851299e-01
 9.98307228e-01 1.55329658e-03 6.50211377e-03 9.97180939e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 20:29:03, Dev, Step : 3130, Loss : 0.46855, Acc : 0.807, Auc : 0.910, Sensitive_Loss : 0.12197, Sensitive_Acc : 16.864, Sensitive_Auc : 0.992, Mean auc: 0.910, Run Time : 247.06 sec
INFO:root:2024-04-26 20:29:35, Train, Epoch : 6, Step : 3140, Loss : 0.27599, Acc : 0.887, Sensitive_Loss : 0.11393, Sensitive_Acc : 15.700, Run Time : 29.85 sec
INFO:root:2024-04-26 20:29:58, Train, Epoch : 6, Step : 3150, Loss : 0.24340, Acc : 0.909, Sensitive_Loss : 0.06977, Sensitive_Acc : 15.700, Run Time : 23.77 sec
INFO:root:2024-04-26 20:30:06, Train, Epoch : 6, Step : 3160, Loss : 0.31652, Acc : 0.875, Sensitive_Loss : 0.07031, Sensitive_Acc : 15.200, Run Time : 7.27 sec
INFO:root:2024-04-26 20:30:13, Train, Epoch : 6, Step : 3170, Loss : 0.31248, Acc : 0.856, Sensitive_Loss : 0.09925, Sensitive_Acc : 16.700, Run Time : 7.35 sec
INFO:root:2024-04-26 20:30:21, Train, Epoch : 6, Step : 3180, Loss : 0.29612, Acc : 0.863, Sensitive_Loss : 0.07421, Sensitive_Acc : 15.000, Run Time : 7.95 sec
INFO:root:2024-04-26 20:30:28, Train, Epoch : 6, Step : 3190, Loss : 0.32313, Acc : 0.853, Sensitive_Loss : 0.09748, Sensitive_Acc : 16.100, Run Time : 7.47 sec
INFO:root:2024-04-26 20:30:37, Train, Epoch : 6, Step : 3200, Loss : 0.33986, Acc : 0.875, Sensitive_Loss : 0.11125, Sensitive_Acc : 18.300, Run Time : 8.36 sec
INFO:root:2024-04-26 20:32:10, Dev, Step : 3200, Loss : 0.40727, Acc : 0.825, Auc : 0.911, Sensitive_Loss : 0.12318, Sensitive_Acc : 16.864, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 93.52 sec
INFO:root:2024-04-26 20:32:16, Train, Epoch : 6, Step : 3210, Loss : 0.29429, Acc : 0.878, Sensitive_Loss : 0.06413, Sensitive_Acc : 16.700, Run Time : 98.84 sec
INFO:root:2024-04-26 20:32:23, Train, Epoch : 6, Step : 3220, Loss : 0.31349, Acc : 0.881, Sensitive_Loss : 0.07520, Sensitive_Acc : 16.000, Run Time : 7.81 sec
INFO:root:2024-04-26 20:32:31, Train, Epoch : 6, Step : 3230, Loss : 0.33735, Acc : 0.841, Sensitive_Loss : 0.08625, Sensitive_Acc : 16.400, Run Time : 7.89 sec
INFO:root:2024-04-26 20:32:39, Train, Epoch : 6, Step : 3240, Loss : 0.31589, Acc : 0.872, Sensitive_Loss : 0.06763, Sensitive_Acc : 15.500, Run Time : 7.50 sec
INFO:root:2024-04-26 20:32:46, Train, Epoch : 6, Step : 3250, Loss : 0.34125, Acc : 0.838, Sensitive_Loss : 0.09312, Sensitive_Acc : 17.700, Run Time : 7.53 sec
INFO:root:2024-04-26 20:32:54, Train, Epoch : 6, Step : 3260, Loss : 0.31146, Acc : 0.872, Sensitive_Loss : 0.07664, Sensitive_Acc : 16.500, Run Time : 7.55 sec
INFO:root:2024-04-26 20:33:01, Train, Epoch : 6, Step : 3270, Loss : 0.24018, Acc : 0.872, Sensitive_Loss : 0.12870, Sensitive_Acc : 18.300, Run Time : 7.24 sec
INFO:root:2024-04-26 20:33:09, Train, Epoch : 6, Step : 3280, Loss : 0.32053, Acc : 0.878, Sensitive_Loss : 0.08964, Sensitive_Acc : 16.300, Run Time : 7.79 sec
INFO:root:2024-04-26 20:33:16, Train, Epoch : 6, Step : 3290, Loss : 0.29668, Acc : 0.866, Sensitive_Loss : 0.07102, Sensitive_Acc : 16.000, Run Time : 7.52 sec
INFO:root:2024-04-26 20:33:24, Train, Epoch : 6, Step : 3300, Loss : 0.32556, Acc : 0.872, Sensitive_Loss : 0.07847, Sensitive_Acc : 16.100, Run Time : 7.79 sec
INFO:root:2024-04-26 20:34:58, Dev, Step : 3300, Loss : 0.39472, Acc : 0.832, Auc : 0.912, Sensitive_Loss : 0.09742, Sensitive_Acc : 16.907, Sensitive_Auc : 0.995, Mean auc: 0.912, Run Time : 93.98 sec
INFO:root:2024-04-26 20:34:59, Best, Step : 3300, Loss : 0.39472, Acc : 0.832, Auc : 0.912, Sensitive_Loss : 0.09742, Sensitive_Acc : 16.907, Sensitive_Auc : 0.995, Best Auc : 0.912
INFO:root:2024-04-26 20:35:05, Train, Epoch : 6, Step : 3310, Loss : 0.31113, Acc : 0.853, Sensitive_Loss : 0.07372, Sensitive_Acc : 16.500, Run Time : 100.46 sec
INFO:root:2024-04-26 20:35:13, Train, Epoch : 6, Step : 3320, Loss : 0.23021, Acc : 0.906, Sensitive_Loss : 0.09403, Sensitive_Acc : 16.200, Run Time : 8.40 sec
INFO:root:2024-04-26 20:35:21, Train, Epoch : 6, Step : 3330, Loss : 0.30331, Acc : 0.866, Sensitive_Loss : 0.07827, Sensitive_Acc : 16.400, Run Time : 8.25 sec
INFO:root:2024-04-26 20:35:29, Train, Epoch : 6, Step : 3340, Loss : 0.32753, Acc : 0.844, Sensitive_Loss : 0.08744, Sensitive_Acc : 16.100, Run Time : 7.36 sec
INFO:root:2024-04-26 20:35:36, Train, Epoch : 6, Step : 3350, Loss : 0.24143, Acc : 0.912, Sensitive_Loss : 0.07910, Sensitive_Acc : 17.000, Run Time : 7.47 sec
INFO:root:2024-04-26 20:35:44, Train, Epoch : 6, Step : 3360, Loss : 0.25038, Acc : 0.906, Sensitive_Loss : 0.08629, Sensitive_Acc : 17.000, Run Time : 7.97 sec
INFO:root:2024-04-26 20:35:52, Train, Epoch : 6, Step : 3370, Loss : 0.27646, Acc : 0.891, Sensitive_Loss : 0.10598, Sensitive_Acc : 16.800, Run Time : 8.12 sec
INFO:root:2024-04-26 20:36:00, Train, Epoch : 6, Step : 3380, Loss : 0.25376, Acc : 0.897, Sensitive_Loss : 0.07323, Sensitive_Acc : 16.500, Run Time : 7.73 sec
INFO:root:2024-04-26 20:36:08, Train, Epoch : 6, Step : 3390, Loss : 0.27095, Acc : 0.884, Sensitive_Loss : 0.08053, Sensitive_Acc : 17.000, Run Time : 7.76 sec
INFO:root:2024-04-26 20:36:16, Train, Epoch : 6, Step : 3400, Loss : 0.26700, Acc : 0.884, Sensitive_Loss : 0.07429, Sensitive_Acc : 16.500, Run Time : 7.88 sec
INFO:root:2024-04-26 20:37:49, Dev, Step : 3400, Loss : 0.40603, Acc : 0.832, Auc : 0.911, Sensitive_Loss : 0.10601, Sensitive_Acc : 16.907, Sensitive_Auc : 0.993, Mean auc: 0.911, Run Time : 93.77 sec
INFO:root:2024-04-26 20:37:55, Train, Epoch : 6, Step : 3410, Loss : 0.27327, Acc : 0.900, Sensitive_Loss : 0.10026, Sensitive_Acc : 16.600, Run Time : 99.52 sec
INFO:root:2024-04-26 20:38:04, Train, Epoch : 6, Step : 3420, Loss : 0.31049, Acc : 0.878, Sensitive_Loss : 0.07435, Sensitive_Acc : 16.400, Run Time : 8.53 sec
INFO:root:2024-04-26 20:38:11, Train, Epoch : 6, Step : 3430, Loss : 0.27550, Acc : 0.872, Sensitive_Loss : 0.09209, Sensitive_Acc : 17.000, Run Time : 7.55 sec
INFO:root:2024-04-26 20:38:19, Train, Epoch : 6, Step : 3440, Loss : 0.33003, Acc : 0.859, Sensitive_Loss : 0.04881, Sensitive_Acc : 17.400, Run Time : 8.23 sec
INFO:root:2024-04-26 20:38:26, Train, Epoch : 6, Step : 3450, Loss : 0.28559, Acc : 0.853, Sensitive_Loss : 0.05353, Sensitive_Acc : 17.200, Run Time : 6.93 sec
INFO:root:2024-04-26 20:38:35, Train, Epoch : 6, Step : 3460, Loss : 0.31296, Acc : 0.881, Sensitive_Loss : 0.11027, Sensitive_Acc : 15.400, Run Time : 8.52 sec
INFO:root:2024-04-26 20:38:43, Train, Epoch : 6, Step : 3470, Loss : 0.32192, Acc : 0.869, Sensitive_Loss : 0.10001, Sensitive_Acc : 16.900, Run Time : 7.93 sec
INFO:root:2024-04-26 20:38:50, Train, Epoch : 6, Step : 3480, Loss : 0.34717, Acc : 0.863, Sensitive_Loss : 0.11055, Sensitive_Acc : 16.200, Run Time : 7.66 sec
INFO:root:2024-04-26 20:38:58, Train, Epoch : 6, Step : 3490, Loss : 0.34883, Acc : 0.850, Sensitive_Loss : 0.11554, Sensitive_Acc : 16.300, Run Time : 7.73 sec
INFO:root:2024-04-26 20:39:06, Train, Epoch : 6, Step : 3500, Loss : 0.32816, Acc : 0.872, Sensitive_Loss : 0.05861, Sensitive_Acc : 18.100, Run Time : 7.96 sec
INFO:root:2024-04-26 20:40:40, Dev, Step : 3500, Loss : 0.40288, Acc : 0.829, Auc : 0.909, Sensitive_Loss : 0.09440, Sensitive_Acc : 16.864, Sensitive_Auc : 0.993, Mean auc: 0.909, Run Time : 93.93 sec
INFO:root:2024-04-26 20:40:46, Train, Epoch : 6, Step : 3510, Loss : 0.30918, Acc : 0.850, Sensitive_Loss : 0.13003, Sensitive_Acc : 17.200, Run Time : 100.21 sec
INFO:root:2024-04-26 20:40:54, Train, Epoch : 6, Step : 3520, Loss : 0.32866, Acc : 0.863, Sensitive_Loss : 0.10016, Sensitive_Acc : 16.900, Run Time : 7.63 sec
INFO:root:2024-04-26 20:41:02, Train, Epoch : 6, Step : 3530, Loss : 0.26476, Acc : 0.900, Sensitive_Loss : 0.08255, Sensitive_Acc : 18.200, Run Time : 7.92 sec
INFO:root:2024-04-26 20:41:10, Train, Epoch : 6, Step : 3540, Loss : 0.32071, Acc : 0.866, Sensitive_Loss : 0.09302, Sensitive_Acc : 17.600, Run Time : 7.90 sec
INFO:root:2024-04-26 20:41:18, Train, Epoch : 6, Step : 3550, Loss : 0.27878, Acc : 0.875, Sensitive_Loss : 0.09779, Sensitive_Acc : 16.100, Run Time : 8.26 sec
INFO:root:2024-04-26 20:41:26, Train, Epoch : 6, Step : 3560, Loss : 0.30281, Acc : 0.875, Sensitive_Loss : 0.08964, Sensitive_Acc : 15.800, Run Time : 7.83 sec
INFO:root:2024-04-26 20:41:34, Train, Epoch : 6, Step : 3570, Loss : 0.35634, Acc : 0.869, Sensitive_Loss : 0.08879, Sensitive_Acc : 16.400, Run Time : 7.85 sec
INFO:root:2024-04-26 20:41:42, Train, Epoch : 6, Step : 3580, Loss : 0.25364, Acc : 0.869, Sensitive_Loss : 0.11609, Sensitive_Acc : 18.500, Run Time : 7.95 sec
INFO:root:2024-04-26 20:41:49, Train, Epoch : 6, Step : 3590, Loss : 0.25617, Acc : 0.897, Sensitive_Loss : 0.09743, Sensitive_Acc : 14.500, Run Time : 7.67 sec
INFO:root:2024-04-26 20:41:57, Train, Epoch : 6, Step : 3600, Loss : 0.33472, Acc : 0.859, Sensitive_Loss : 0.09448, Sensitive_Acc : 17.500, Run Time : 8.10 sec
INFO:root:2024-04-26 20:43:41, Dev, Step : 3600, Loss : 0.41297, Acc : 0.827, Auc : 0.910, Sensitive_Loss : 0.10850, Sensitive_Acc : 16.864, Sensitive_Auc : 0.991, Mean auc: 0.910, Run Time : 104.03 sec
INFO:root:2024-04-26 20:43:47, Train, Epoch : 6, Step : 3610, Loss : 0.23896, Acc : 0.887, Sensitive_Loss : 0.07175, Sensitive_Acc : 16.000, Run Time : 109.53 sec
INFO:root:2024-04-26 20:43:54, Train, Epoch : 6, Step : 3620, Loss : 0.29215, Acc : 0.887, Sensitive_Loss : 0.08127, Sensitive_Acc : 17.200, Run Time : 7.33 sec
INFO:root:2024-04-26 20:44:02, Train, Epoch : 6, Step : 3630, Loss : 0.29940, Acc : 0.859, Sensitive_Loss : 0.09247, Sensitive_Acc : 17.600, Run Time : 7.44 sec
INFO:root:2024-04-26 20:44:10, Train, Epoch : 6, Step : 3640, Loss : 0.29302, Acc : 0.878, Sensitive_Loss : 0.08994, Sensitive_Acc : 16.800, Run Time : 8.08 sec
INFO:root:2024-04-26 20:44:17, Train, Epoch : 6, Step : 3650, Loss : 0.30593, Acc : 0.875, Sensitive_Loss : 0.06571, Sensitive_Acc : 17.600, Run Time : 7.44 sec
INFO:root:2024-04-26 20:44:25, Train, Epoch : 6, Step : 3660, Loss : 0.23930, Acc : 0.894, Sensitive_Loss : 0.10179, Sensitive_Acc : 17.900, Run Time : 7.58 sec
INFO:root:2024-04-26 20:44:32, Train, Epoch : 6, Step : 3670, Loss : 0.34787, Acc : 0.825, Sensitive_Loss : 0.09455, Sensitive_Acc : 16.700, Run Time : 6.96 sec
INFO:root:2024-04-26 20:44:39, Train, Epoch : 6, Step : 3680, Loss : 0.27469, Acc : 0.894, Sensitive_Loss : 0.05792, Sensitive_Acc : 16.000, Run Time : 7.58 sec
INFO:root:2024-04-26 20:44:47, Train, Epoch : 6, Step : 3690, Loss : 0.29789, Acc : 0.878, Sensitive_Loss : 0.08785, Sensitive_Acc : 18.400, Run Time : 7.15 sec
INFO:root:2024-04-26 20:44:54, Train, Epoch : 6, Step : 3700, Loss : 0.28816, Acc : 0.894, Sensitive_Loss : 0.06481, Sensitive_Acc : 15.800, Run Time : 7.20 sec
INFO:root:2024-04-26 20:46:28, Dev, Step : 3700, Loss : 0.42544, Acc : 0.821, Auc : 0.910, Sensitive_Loss : 0.11771, Sensitive_Acc : 16.879, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 94.18 sec
INFO:root:2024-04-26 20:46:34, Train, Epoch : 6, Step : 3710, Loss : 0.23542, Acc : 0.878, Sensitive_Loss : 0.06447, Sensitive_Acc : 15.800, Run Time : 99.98 sec
INFO:root:2024-04-26 20:46:41, Train, Epoch : 6, Step : 3720, Loss : 0.31191, Acc : 0.881, Sensitive_Loss : 0.09783, Sensitive_Acc : 15.900, Run Time : 7.08 sec
INFO:root:2024-04-26 20:46:48, Train, Epoch : 6, Step : 3730, Loss : 0.26706, Acc : 0.884, Sensitive_Loss : 0.10426, Sensitive_Acc : 15.200, Run Time : 6.83 sec
INFO:root:2024-04-26 20:46:55, Train, Epoch : 6, Step : 3740, Loss : 0.27499, Acc : 0.872, Sensitive_Loss : 0.08331, Sensitive_Acc : 15.200, Run Time : 6.95 sec
INFO:root:2024-04-26 20:47:02, Train, Epoch : 6, Step : 3750, Loss : 0.29867, Acc : 0.859, Sensitive_Loss : 0.08865, Sensitive_Acc : 16.600, Run Time : 7.44 sec
INFO:root:2024-04-26 20:48:38
INFO:root:y_pred: [0.03695291 0.9625962  0.02266898 ... 0.81852585 0.00221416 0.9463554 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.83404338e-01 1.37700327e-03 1.23281833e-02 5.71944751e-04
 9.99974132e-01 4.00709250e-04 9.99991179e-01 9.99802768e-01
 3.58970487e-03 9.06402946e-01 9.99504447e-01 9.99982357e-01
 9.97004807e-01 9.96988356e-01 8.58724490e-02 9.74886835e-01
 9.99932289e-01 3.41078304e-02 5.03121257e-01 9.54493046e-01
 9.99475420e-01 3.10776591e-01 9.99823630e-01 9.97237682e-01
 9.99598444e-01 9.96682107e-01 7.02715552e-05 9.99991894e-01
 9.96496022e-01 3.54130447e-01 9.02742380e-04 2.98102051e-01
 1.19978078e-01 9.10755098e-02 1.16602987e-01 9.18750593e-04
 1.66166928e-02 3.39016900e-04 9.99951720e-01 9.99730170e-01
 4.44676007e-05 1.05349848e-03 9.95820165e-01 5.23951952e-04
 9.99992013e-01 9.99383330e-01 9.99790251e-01 9.95632052e-01
 3.60137294e-03 9.98488665e-01 9.99856710e-01 3.31248943e-04
 6.33266926e-01 5.50700584e-04 1.07239379e-04 1.56325586e-02
 4.66193929e-02 4.87447195e-02 2.23827362e-03 2.38818582e-02
 3.09892446e-02 8.61328021e-02 1.64484844e-01 9.67780173e-01
 2.97063410e-01 9.99938250e-01 6.62557930e-02 9.99976516e-01
 9.99386072e-01 5.47585070e-01 9.07120407e-01 7.05254436e-01
 4.29756648e-04 1.33956671e-01 9.93270427e-03 2.15327670e-03
 1.16348036e-01 5.38143590e-02 7.20412144e-03 9.99756753e-01
 9.99801695e-01 1.99268991e-03 1.85893804e-01 7.97278527e-03
 9.83193636e-01 9.80608106e-01 1.61411166e-02 8.75522792e-02
 9.98144507e-01 9.99965668e-01 9.99998450e-01 2.27260049e-02
 1.79958828e-02 9.99000847e-01 1.64485916e-01 4.98882867e-03
 9.99508619e-01 9.99338567e-01 1.18844961e-04 2.99407123e-03
 9.97501552e-01 9.93029654e-01 9.94179249e-01 9.97983098e-01
 2.10400741e-03 1.74508970e-02 9.85928476e-01 9.99670386e-01
 9.94242609e-01 8.88235081e-06 9.99589741e-01 9.99961734e-01
 5.70074990e-02 9.99964356e-01 9.99426007e-01 9.99868512e-01
 9.23845649e-01 9.98989880e-01 2.44607478e-02 2.57444549e-02
 9.99764383e-01 9.98141766e-01 2.98689702e-04 9.94836688e-01
 9.99996901e-01 5.93150735e-01 9.80552852e-01 1.02975313e-02
 2.41505969e-02 9.95630980e-01 9.99944925e-01 2.28942130e-02
 1.45956175e-02 3.19859036e-03 9.99619365e-01 9.98808742e-01
 9.98987377e-01 8.51113233e-04 1.54592143e-02 9.96391833e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 20:48:38, Dev, Step : 3756, Loss : 0.43699, Acc : 0.818, Auc : 0.909, Sensitive_Loss : 0.11852, Sensitive_Acc : 16.936, Sensitive_Auc : 0.992, Mean auc: 0.909, Run Time : 91.85 sec
INFO:root:2024-04-26 20:48:43, Train, Epoch : 7, Step : 3760, Loss : 0.09241, Acc : 0.359, Sensitive_Loss : 0.03221, Sensitive_Acc : 7.700, Run Time : 3.95 sec
INFO:root:2024-04-26 20:48:50, Train, Epoch : 7, Step : 3770, Loss : 0.28990, Acc : 0.863, Sensitive_Loss : 0.12295, Sensitive_Acc : 16.200, Run Time : 7.08 sec
INFO:root:2024-04-26 20:48:57, Train, Epoch : 7, Step : 3780, Loss : 0.28296, Acc : 0.878, Sensitive_Loss : 0.07174, Sensitive_Acc : 15.400, Run Time : 6.96 sec
INFO:root:2024-04-26 20:49:04, Train, Epoch : 7, Step : 3790, Loss : 0.28669, Acc : 0.863, Sensitive_Loss : 0.07204, Sensitive_Acc : 17.000, Run Time : 7.26 sec
INFO:root:2024-04-26 20:49:11, Train, Epoch : 7, Step : 3800, Loss : 0.26726, Acc : 0.884, Sensitive_Loss : 0.07053, Sensitive_Acc : 16.200, Run Time : 7.10 sec
INFO:root:2024-04-26 20:50:45, Dev, Step : 3800, Loss : 0.42108, Acc : 0.822, Auc : 0.908, Sensitive_Loss : 0.10439, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.908, Run Time : 93.75 sec
INFO:root:2024-04-26 20:50:50, Train, Epoch : 7, Step : 3810, Loss : 0.30815, Acc : 0.894, Sensitive_Loss : 0.07507, Sensitive_Acc : 16.900, Run Time : 99.36 sec
INFO:root:2024-04-26 20:50:58, Train, Epoch : 7, Step : 3820, Loss : 0.22478, Acc : 0.903, Sensitive_Loss : 0.05950, Sensitive_Acc : 16.400, Run Time : 7.26 sec
INFO:root:2024-04-26 20:51:05, Train, Epoch : 7, Step : 3830, Loss : 0.28240, Acc : 0.891, Sensitive_Loss : 0.05979, Sensitive_Acc : 17.000, Run Time : 7.15 sec
INFO:root:2024-04-26 20:51:12, Train, Epoch : 7, Step : 3840, Loss : 0.24244, Acc : 0.884, Sensitive_Loss : 0.09751, Sensitive_Acc : 17.300, Run Time : 7.29 sec
INFO:root:2024-04-26 20:51:19, Train, Epoch : 7, Step : 3850, Loss : 0.28451, Acc : 0.875, Sensitive_Loss : 0.08533, Sensitive_Acc : 15.900, Run Time : 7.21 sec
INFO:root:2024-04-26 20:51:27, Train, Epoch : 7, Step : 3860, Loss : 0.25849, Acc : 0.863, Sensitive_Loss : 0.07635, Sensitive_Acc : 16.900, Run Time : 7.33 sec
INFO:root:2024-04-26 20:51:34, Train, Epoch : 7, Step : 3870, Loss : 0.25254, Acc : 0.859, Sensitive_Loss : 0.11809, Sensitive_Acc : 18.300, Run Time : 7.20 sec
INFO:root:2024-04-26 20:51:41, Train, Epoch : 7, Step : 3880, Loss : 0.29261, Acc : 0.878, Sensitive_Loss : 0.06950, Sensitive_Acc : 16.900, Run Time : 7.77 sec
INFO:root:2024-04-26 20:51:48, Train, Epoch : 7, Step : 3890, Loss : 0.30570, Acc : 0.869, Sensitive_Loss : 0.08061, Sensitive_Acc : 16.000, Run Time : 7.00 sec
INFO:root:2024-04-26 20:51:56, Train, Epoch : 7, Step : 3900, Loss : 0.26700, Acc : 0.884, Sensitive_Loss : 0.05598, Sensitive_Acc : 16.500, Run Time : 7.43 sec
INFO:root:2024-04-26 20:53:29, Dev, Step : 3900, Loss : 0.41247, Acc : 0.829, Auc : 0.908, Sensitive_Loss : 0.11477, Sensitive_Acc : 16.936, Sensitive_Auc : 0.992, Mean auc: 0.908, Run Time : 93.35 sec
INFO:root:2024-04-26 20:53:35, Train, Epoch : 7, Step : 3910, Loss : 0.23524, Acc : 0.919, Sensitive_Loss : 0.08156, Sensitive_Acc : 16.100, Run Time : 98.71 sec
INFO:root:2024-04-26 20:53:42, Train, Epoch : 7, Step : 3920, Loss : 0.29832, Acc : 0.875, Sensitive_Loss : 0.06806, Sensitive_Acc : 16.900, Run Time : 7.19 sec
INFO:root:2024-04-26 20:53:49, Train, Epoch : 7, Step : 3930, Loss : 0.27901, Acc : 0.866, Sensitive_Loss : 0.10112, Sensitive_Acc : 16.700, Run Time : 7.33 sec
INFO:root:2024-04-26 20:53:56, Train, Epoch : 7, Step : 3940, Loss : 0.28438, Acc : 0.875, Sensitive_Loss : 0.07630, Sensitive_Acc : 16.000, Run Time : 6.86 sec
INFO:root:2024-04-26 20:54:04, Train, Epoch : 7, Step : 3950, Loss : 0.26987, Acc : 0.897, Sensitive_Loss : 0.11113, Sensitive_Acc : 16.100, Run Time : 7.59 sec
INFO:root:2024-04-26 20:54:11, Train, Epoch : 7, Step : 3960, Loss : 0.34379, Acc : 0.850, Sensitive_Loss : 0.07011, Sensitive_Acc : 16.200, Run Time : 6.94 sec
INFO:root:2024-04-26 20:54:18, Train, Epoch : 7, Step : 3970, Loss : 0.24433, Acc : 0.881, Sensitive_Loss : 0.09048, Sensitive_Acc : 15.700, Run Time : 7.15 sec
INFO:root:2024-04-26 20:54:24, Train, Epoch : 7, Step : 3980, Loss : 0.27278, Acc : 0.887, Sensitive_Loss : 0.07249, Sensitive_Acc : 15.600, Run Time : 6.57 sec
INFO:root:2024-04-26 20:54:32, Train, Epoch : 7, Step : 3990, Loss : 0.24579, Acc : 0.884, Sensitive_Loss : 0.10713, Sensitive_Acc : 15.900, Run Time : 8.11 sec
INFO:root:2024-04-26 20:54:38, Train, Epoch : 7, Step : 4000, Loss : 0.27113, Acc : 0.866, Sensitive_Loss : 0.06934, Sensitive_Acc : 16.000, Run Time : 6.09 sec
INFO:root:2024-04-26 20:56:12, Dev, Step : 4000, Loss : 0.41796, Acc : 0.829, Auc : 0.909, Sensitive_Loss : 0.10840, Sensitive_Acc : 16.821, Sensitive_Auc : 0.992, Mean auc: 0.909, Run Time : 93.68 sec
INFO:root:2024-04-26 20:56:18, Train, Epoch : 7, Step : 4010, Loss : 0.28992, Acc : 0.887, Sensitive_Loss : 0.09651, Sensitive_Acc : 15.200, Run Time : 99.42 sec
INFO:root:2024-04-26 20:56:25, Train, Epoch : 7, Step : 4020, Loss : 0.27454, Acc : 0.894, Sensitive_Loss : 0.06900, Sensitive_Acc : 15.600, Run Time : 7.04 sec
INFO:root:2024-04-26 20:56:32, Train, Epoch : 7, Step : 4030, Loss : 0.30249, Acc : 0.872, Sensitive_Loss : 0.10706, Sensitive_Acc : 16.200, Run Time : 7.17 sec
INFO:root:2024-04-26 20:56:39, Train, Epoch : 7, Step : 4040, Loss : 0.30619, Acc : 0.875, Sensitive_Loss : 0.06869, Sensitive_Acc : 17.600, Run Time : 7.29 sec
INFO:root:2024-04-26 20:56:47, Train, Epoch : 7, Step : 4050, Loss : 0.31971, Acc : 0.866, Sensitive_Loss : 0.08033, Sensitive_Acc : 17.600, Run Time : 7.31 sec
INFO:root:2024-04-26 20:56:53, Train, Epoch : 7, Step : 4060, Loss : 0.24709, Acc : 0.887, Sensitive_Loss : 0.10945, Sensitive_Acc : 16.800, Run Time : 6.80 sec
INFO:root:2024-04-26 20:57:01, Train, Epoch : 7, Step : 4070, Loss : 0.28987, Acc : 0.881, Sensitive_Loss : 0.06031, Sensitive_Acc : 17.800, Run Time : 7.38 sec
INFO:root:2024-04-26 20:57:08, Train, Epoch : 7, Step : 4080, Loss : 0.26717, Acc : 0.872, Sensitive_Loss : 0.07887, Sensitive_Acc : 16.800, Run Time : 7.06 sec
INFO:root:2024-04-26 20:57:15, Train, Epoch : 7, Step : 4090, Loss : 0.28173, Acc : 0.866, Sensitive_Loss : 0.05847, Sensitive_Acc : 15.700, Run Time : 7.08 sec
INFO:root:2024-04-26 20:57:22, Train, Epoch : 7, Step : 4100, Loss : 0.28757, Acc : 0.847, Sensitive_Loss : 0.08238, Sensitive_Acc : 15.500, Run Time : 7.01 sec
INFO:root:2024-04-26 20:58:56, Dev, Step : 4100, Loss : 0.43151, Acc : 0.824, Auc : 0.908, Sensitive_Loss : 0.11055, Sensitive_Acc : 16.821, Sensitive_Auc : 0.993, Mean auc: 0.908, Run Time : 93.72 sec
INFO:root:2024-04-26 20:59:02, Train, Epoch : 7, Step : 4110, Loss : 0.26656, Acc : 0.878, Sensitive_Loss : 0.09532, Sensitive_Acc : 17.300, Run Time : 99.54 sec
INFO:root:2024-04-26 20:59:09, Train, Epoch : 7, Step : 4120, Loss : 0.32599, Acc : 0.859, Sensitive_Loss : 0.05959, Sensitive_Acc : 17.500, Run Time : 7.06 sec
INFO:root:2024-04-26 20:59:16, Train, Epoch : 7, Step : 4130, Loss : 0.31299, Acc : 0.875, Sensitive_Loss : 0.07700, Sensitive_Acc : 16.200, Run Time : 7.48 sec
INFO:root:2024-04-26 20:59:23, Train, Epoch : 7, Step : 4140, Loss : 0.23753, Acc : 0.887, Sensitive_Loss : 0.08387, Sensitive_Acc : 16.000, Run Time : 6.95 sec
INFO:root:2024-04-26 20:59:31, Train, Epoch : 7, Step : 4150, Loss : 0.24461, Acc : 0.922, Sensitive_Loss : 0.08608, Sensitive_Acc : 15.700, Run Time : 7.45 sec
INFO:root:2024-04-26 20:59:38, Train, Epoch : 7, Step : 4160, Loss : 0.27166, Acc : 0.875, Sensitive_Loss : 0.09903, Sensitive_Acc : 13.800, Run Time : 7.17 sec
INFO:root:2024-04-26 20:59:45, Train, Epoch : 7, Step : 4170, Loss : 0.29347, Acc : 0.878, Sensitive_Loss : 0.06202, Sensitive_Acc : 17.100, Run Time : 7.45 sec
INFO:root:2024-04-26 20:59:53, Train, Epoch : 7, Step : 4180, Loss : 0.22952, Acc : 0.909, Sensitive_Loss : 0.07393, Sensitive_Acc : 15.900, Run Time : 7.84 sec
INFO:root:2024-04-26 21:00:00, Train, Epoch : 7, Step : 4190, Loss : 0.28974, Acc : 0.875, Sensitive_Loss : 0.08116, Sensitive_Acc : 16.200, Run Time : 7.19 sec
INFO:root:2024-04-26 21:00:07, Train, Epoch : 7, Step : 4200, Loss : 0.30992, Acc : 0.859, Sensitive_Loss : 0.08400, Sensitive_Acc : 16.900, Run Time : 7.32 sec
INFO:root:2024-04-26 21:01:41, Dev, Step : 4200, Loss : 0.42499, Acc : 0.831, Auc : 0.908, Sensitive_Loss : 0.11903, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.908, Run Time : 93.74 sec
INFO:root:2024-04-26 21:01:47, Train, Epoch : 7, Step : 4210, Loss : 0.26299, Acc : 0.912, Sensitive_Loss : 0.07490, Sensitive_Acc : 15.900, Run Time : 99.18 sec
INFO:root:2024-04-26 21:01:54, Train, Epoch : 7, Step : 4220, Loss : 0.21671, Acc : 0.906, Sensitive_Loss : 0.11210, Sensitive_Acc : 16.000, Run Time : 7.30 sec
INFO:root:2024-04-26 21:02:02, Train, Epoch : 7, Step : 4230, Loss : 0.27016, Acc : 0.884, Sensitive_Loss : 0.07878, Sensitive_Acc : 15.400, Run Time : 7.85 sec
INFO:root:2024-04-26 21:02:09, Train, Epoch : 7, Step : 4240, Loss : 0.26185, Acc : 0.881, Sensitive_Loss : 0.06312, Sensitive_Acc : 16.200, Run Time : 7.46 sec
INFO:root:2024-04-26 21:02:17, Train, Epoch : 7, Step : 4250, Loss : 0.25210, Acc : 0.872, Sensitive_Loss : 0.09388, Sensitive_Acc : 16.300, Run Time : 7.29 sec
INFO:root:2024-04-26 21:02:24, Train, Epoch : 7, Step : 4260, Loss : 0.25839, Acc : 0.878, Sensitive_Loss : 0.14773, Sensitive_Acc : 17.200, Run Time : 7.83 sec
INFO:root:2024-04-26 21:02:32, Train, Epoch : 7, Step : 4270, Loss : 0.26974, Acc : 0.875, Sensitive_Loss : 0.10894, Sensitive_Acc : 15.000, Run Time : 7.27 sec
INFO:root:2024-04-26 21:02:39, Train, Epoch : 7, Step : 4280, Loss : 0.29371, Acc : 0.872, Sensitive_Loss : 0.09506, Sensitive_Acc : 18.100, Run Time : 7.47 sec
INFO:root:2024-04-26 21:02:47, Train, Epoch : 7, Step : 4290, Loss : 0.20794, Acc : 0.916, Sensitive_Loss : 0.07570, Sensitive_Acc : 17.000, Run Time : 7.57 sec
INFO:root:2024-04-26 21:02:54, Train, Epoch : 7, Step : 4300, Loss : 0.30576, Acc : 0.891, Sensitive_Loss : 0.06390, Sensitive_Acc : 18.200, Run Time : 7.24 sec
INFO:root:2024-04-26 21:04:28, Dev, Step : 4300, Loss : 0.42365, Acc : 0.823, Auc : 0.905, Sensitive_Loss : 0.10122, Sensitive_Acc : 16.821, Sensitive_Auc : 0.993, Mean auc: 0.905, Run Time : 94.10 sec
INFO:root:2024-04-26 21:04:34, Train, Epoch : 7, Step : 4310, Loss : 0.26965, Acc : 0.887, Sensitive_Loss : 0.08822, Sensitive_Acc : 17.400, Run Time : 99.98 sec
INFO:root:2024-04-26 21:04:41, Train, Epoch : 7, Step : 4320, Loss : 0.31226, Acc : 0.859, Sensitive_Loss : 0.07417, Sensitive_Acc : 17.400, Run Time : 7.23 sec
INFO:root:2024-04-26 21:04:50, Train, Epoch : 7, Step : 4330, Loss : 0.30953, Acc : 0.872, Sensitive_Loss : 0.11464, Sensitive_Acc : 17.500, Run Time : 8.53 sec
INFO:root:2024-04-26 21:04:57, Train, Epoch : 7, Step : 4340, Loss : 0.25791, Acc : 0.912, Sensitive_Loss : 0.06500, Sensitive_Acc : 14.900, Run Time : 7.46 sec
INFO:root:2024-04-26 21:05:05, Train, Epoch : 7, Step : 4350, Loss : 0.33609, Acc : 0.872, Sensitive_Loss : 0.08250, Sensitive_Acc : 15.700, Run Time : 8.03 sec
INFO:root:2024-04-26 21:05:13, Train, Epoch : 7, Step : 4360, Loss : 0.27906, Acc : 0.891, Sensitive_Loss : 0.06681, Sensitive_Acc : 16.400, Run Time : 7.60 sec
INFO:root:2024-04-26 21:05:21, Train, Epoch : 7, Step : 4370, Loss : 0.25536, Acc : 0.878, Sensitive_Loss : 0.09529, Sensitive_Acc : 16.300, Run Time : 8.16 sec
INFO:root:2024-04-26 21:05:29, Train, Epoch : 7, Step : 4380, Loss : 0.23021, Acc : 0.875, Sensitive_Loss : 0.12647, Sensitive_Acc : 16.200, Run Time : 7.94 sec
INFO:root:2024-04-26 21:07:03
INFO:root:y_pred: [0.03448388 0.97858834 0.01530753 ... 0.82889444 0.00108846 0.96026343]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.90031719e-01 1.17944193e-03 9.55571607e-03 3.48056317e-04
 9.99970436e-01 2.98856612e-04 9.99994040e-01 9.99919772e-01
 8.60657170e-03 9.68031228e-01 9.99646187e-01 9.99990106e-01
 9.93060827e-01 9.93996859e-01 2.65994705e-02 9.90782559e-01
 9.99940872e-01 2.00446118e-02 6.62024856e-01 9.73042607e-01
 9.99227285e-01 2.01333791e-01 9.99705732e-01 9.97568786e-01
 9.99556601e-01 9.97862399e-01 9.74082068e-05 9.99991059e-01
 9.87094820e-01 1.80017948e-01 6.54638279e-04 3.80082339e-01
 1.84490249e-01 1.07356064e-01 5.62663339e-02 1.34219253e-03
 7.33325398e-03 4.80850838e-04 9.99984503e-01 9.99716699e-01
 7.01581957e-05 7.80200935e-04 9.97655511e-01 5.37408749e-04
 9.99990821e-01 9.99755204e-01 9.99886513e-01 9.97552216e-01
 3.10005783e-03 9.98182416e-01 9.99905467e-01 1.00887308e-04
 1.84571043e-01 5.48570766e-04 1.51547880e-04 2.60828380e-02
 8.47427621e-02 6.17007073e-03 2.82306504e-03 2.14131922e-02
 3.35311070e-02 1.01123706e-01 5.16818427e-02 9.81453180e-01
 1.61217242e-01 9.99945521e-01 5.68704270e-02 9.99987006e-01
 9.99675751e-01 3.15774947e-01 9.36237574e-01 6.58812821e-01
 2.00205235e-04 1.32425994e-01 8.76683928e-03 3.17823119e-03
 1.06502779e-01 6.28892630e-02 4.86094505e-03 9.99881744e-01
 9.99919295e-01 3.20719020e-03 3.43706816e-01 5.15116611e-03
 9.89966393e-01 9.93592680e-01 8.95252544e-03 5.21437190e-02
 9.97022331e-01 9.99983191e-01 9.99998689e-01 1.45604964e-02
 6.30563730e-03 9.99276102e-01 2.52031922e-01 1.02675892e-02
 9.99708235e-01 9.99446094e-01 7.49040628e-05 1.76818250e-03
 9.96085167e-01 9.95795965e-01 9.95838284e-01 9.98340368e-01
 1.52347074e-03 9.04251821e-03 9.91276145e-01 9.99724209e-01
 9.93378222e-01 3.48151261e-05 9.99547541e-01 9.99945641e-01
 6.64858595e-02 9.99971867e-01 9.99127805e-01 9.99891877e-01
 9.41401541e-01 9.97685075e-01 2.81377994e-02 4.10618819e-02
 9.99751747e-01 9.98284876e-01 1.05908497e-04 9.96716321e-01
 9.99998093e-01 5.44069886e-01 9.86449659e-01 4.42937436e-03
 2.23307814e-02 9.96627212e-01 9.99926925e-01 1.12512801e-02
 2.46609692e-02 2.15368252e-03 9.99556243e-01 9.98184025e-01
 9.99102712e-01 9.52967268e-04 1.80660915e-02 9.97170389e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 21:07:03, Dev, Step : 4382, Loss : 0.43196, Acc : 0.825, Auc : 0.909, Sensitive_Loss : 0.12008, Sensitive_Acc : 16.821, Sensitive_Auc : 0.994, Mean auc: 0.909, Run Time : 92.68 sec
INFO:root:2024-04-26 21:07:10, Train, Epoch : 8, Step : 4390, Loss : 0.16294, Acc : 0.738, Sensitive_Loss : 0.05477, Sensitive_Acc : 12.900, Run Time : 6.39 sec
INFO:root:2024-04-26 21:07:18, Train, Epoch : 8, Step : 4400, Loss : 0.22930, Acc : 0.903, Sensitive_Loss : 0.12626, Sensitive_Acc : 16.000, Run Time : 7.42 sec
INFO:root:2024-04-26 21:08:51, Dev, Step : 4400, Loss : 0.41931, Acc : 0.828, Auc : 0.910, Sensitive_Loss : 0.10830, Sensitive_Acc : 16.821, Sensitive_Auc : 0.994, Mean auc: 0.910, Run Time : 93.21 sec
INFO:root:2024-04-26 21:08:57, Train, Epoch : 8, Step : 4410, Loss : 0.27209, Acc : 0.878, Sensitive_Loss : 0.09311, Sensitive_Acc : 16.300, Run Time : 98.76 sec
INFO:root:2024-04-26 21:09:04, Train, Epoch : 8, Step : 4420, Loss : 0.30649, Acc : 0.875, Sensitive_Loss : 0.09102, Sensitive_Acc : 16.400, Run Time : 7.74 sec
INFO:root:2024-04-26 21:09:11, Train, Epoch : 8, Step : 4430, Loss : 0.22847, Acc : 0.894, Sensitive_Loss : 0.07957, Sensitive_Acc : 16.900, Run Time : 6.90 sec
INFO:root:2024-04-26 21:09:18, Train, Epoch : 8, Step : 4440, Loss : 0.25575, Acc : 0.897, Sensitive_Loss : 0.06772, Sensitive_Acc : 17.400, Run Time : 7.27 sec
INFO:root:2024-04-26 21:09:26, Train, Epoch : 8, Step : 4450, Loss : 0.23906, Acc : 0.894, Sensitive_Loss : 0.07447, Sensitive_Acc : 16.000, Run Time : 7.34 sec
INFO:root:2024-04-26 21:09:33, Train, Epoch : 8, Step : 4460, Loss : 0.24401, Acc : 0.900, Sensitive_Loss : 0.10086, Sensitive_Acc : 16.900, Run Time : 7.28 sec
INFO:root:2024-04-26 21:09:41, Train, Epoch : 8, Step : 4470, Loss : 0.25623, Acc : 0.894, Sensitive_Loss : 0.07396, Sensitive_Acc : 16.900, Run Time : 7.51 sec
INFO:root:2024-04-26 21:09:48, Train, Epoch : 8, Step : 4480, Loss : 0.25534, Acc : 0.869, Sensitive_Loss : 0.10267, Sensitive_Acc : 17.300, Run Time : 7.33 sec
INFO:root:2024-04-26 21:09:56, Train, Epoch : 8, Step : 4490, Loss : 0.22225, Acc : 0.903, Sensitive_Loss : 0.09282, Sensitive_Acc : 17.700, Run Time : 7.73 sec
INFO:root:2024-04-26 21:10:03, Train, Epoch : 8, Step : 4500, Loss : 0.25732, Acc : 0.887, Sensitive_Loss : 0.06762, Sensitive_Acc : 17.000, Run Time : 7.58 sec
INFO:root:2024-04-26 21:11:36, Dev, Step : 4500, Loss : 0.42399, Acc : 0.826, Auc : 0.909, Sensitive_Loss : 0.10436, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.909, Run Time : 93.31 sec
INFO:root:2024-04-26 21:11:42, Train, Epoch : 8, Step : 4510, Loss : 0.22108, Acc : 0.906, Sensitive_Loss : 0.09456, Sensitive_Acc : 15.400, Run Time : 98.81 sec
INFO:root:2024-04-26 21:11:49, Train, Epoch : 8, Step : 4520, Loss : 0.20613, Acc : 0.916, Sensitive_Loss : 0.07017, Sensitive_Acc : 15.900, Run Time : 7.34 sec
INFO:root:2024-04-26 21:11:57, Train, Epoch : 8, Step : 4530, Loss : 0.27368, Acc : 0.863, Sensitive_Loss : 0.11418, Sensitive_Acc : 17.200, Run Time : 7.72 sec
INFO:root:2024-04-26 21:12:04, Train, Epoch : 8, Step : 4540, Loss : 0.24525, Acc : 0.894, Sensitive_Loss : 0.10590, Sensitive_Acc : 17.300, Run Time : 7.27 sec
INFO:root:2024-04-26 21:12:12, Train, Epoch : 8, Step : 4550, Loss : 0.22325, Acc : 0.903, Sensitive_Loss : 0.11320, Sensitive_Acc : 15.600, Run Time : 7.34 sec
INFO:root:2024-04-26 21:12:19, Train, Epoch : 8, Step : 4560, Loss : 0.20936, Acc : 0.922, Sensitive_Loss : 0.09333, Sensitive_Acc : 17.100, Run Time : 7.44 sec
INFO:root:2024-04-26 21:12:26, Train, Epoch : 8, Step : 4570, Loss : 0.26342, Acc : 0.903, Sensitive_Loss : 0.08692, Sensitive_Acc : 17.200, Run Time : 7.22 sec
INFO:root:2024-04-26 21:12:34, Train, Epoch : 8, Step : 4580, Loss : 0.28548, Acc : 0.887, Sensitive_Loss : 0.07690, Sensitive_Acc : 15.400, Run Time : 7.73 sec
INFO:root:2024-04-26 21:12:41, Train, Epoch : 8, Step : 4590, Loss : 0.21961, Acc : 0.884, Sensitive_Loss : 0.07371, Sensitive_Acc : 14.900, Run Time : 7.27 sec
INFO:root:2024-04-26 21:12:49, Train, Epoch : 8, Step : 4600, Loss : 0.27367, Acc : 0.884, Sensitive_Loss : 0.09032, Sensitive_Acc : 16.500, Run Time : 8.00 sec
INFO:root:2024-04-26 21:14:23, Dev, Step : 4600, Loss : 0.47563, Acc : 0.811, Auc : 0.905, Sensitive_Loss : 0.12257, Sensitive_Acc : 16.907, Sensitive_Auc : 0.992, Mean auc: 0.905, Run Time : 93.56 sec
INFO:root:2024-04-26 21:14:28, Train, Epoch : 8, Step : 4610, Loss : 0.23565, Acc : 0.887, Sensitive_Loss : 0.08887, Sensitive_Acc : 16.500, Run Time : 99.04 sec
INFO:root:2024-04-26 21:14:36, Train, Epoch : 8, Step : 4620, Loss : 0.26588, Acc : 0.869, Sensitive_Loss : 0.08875, Sensitive_Acc : 18.400, Run Time : 7.75 sec
INFO:root:2024-04-26 21:14:44, Train, Epoch : 8, Step : 4630, Loss : 0.25652, Acc : 0.900, Sensitive_Loss : 0.04629, Sensitive_Acc : 17.600, Run Time : 7.87 sec
INFO:root:2024-04-26 21:14:51, Train, Epoch : 8, Step : 4640, Loss : 0.28220, Acc : 0.881, Sensitive_Loss : 0.09325, Sensitive_Acc : 18.000, Run Time : 7.33 sec
INFO:root:2024-04-26 21:14:59, Train, Epoch : 8, Step : 4650, Loss : 0.22774, Acc : 0.897, Sensitive_Loss : 0.05481, Sensitive_Acc : 16.100, Run Time : 7.44 sec
INFO:root:2024-04-26 21:15:06, Train, Epoch : 8, Step : 4660, Loss : 0.28248, Acc : 0.884, Sensitive_Loss : 0.08323, Sensitive_Acc : 16.500, Run Time : 7.58 sec
INFO:root:2024-04-26 21:15:14, Train, Epoch : 8, Step : 4670, Loss : 0.20990, Acc : 0.903, Sensitive_Loss : 0.09013, Sensitive_Acc : 16.000, Run Time : 7.56 sec
INFO:root:2024-04-26 21:15:21, Train, Epoch : 8, Step : 4680, Loss : 0.25058, Acc : 0.897, Sensitive_Loss : 0.08327, Sensitive_Acc : 16.300, Run Time : 7.01 sec
INFO:root:2024-04-26 21:15:28, Train, Epoch : 8, Step : 4690, Loss : 0.28304, Acc : 0.884, Sensitive_Loss : 0.07036, Sensitive_Acc : 16.300, Run Time : 7.41 sec
INFO:root:2024-04-26 21:15:36, Train, Epoch : 8, Step : 4700, Loss : 0.23519, Acc : 0.912, Sensitive_Loss : 0.06144, Sensitive_Acc : 15.100, Run Time : 8.18 sec
INFO:root:2024-04-26 21:17:10, Dev, Step : 4700, Loss : 0.44321, Acc : 0.818, Auc : 0.905, Sensitive_Loss : 0.11814, Sensitive_Acc : 16.821, Sensitive_Auc : 0.993, Mean auc: 0.905, Run Time : 93.94 sec
INFO:root:2024-04-26 21:17:16, Train, Epoch : 8, Step : 4710, Loss : 0.18844, Acc : 0.934, Sensitive_Loss : 0.06069, Sensitive_Acc : 17.200, Run Time : 99.48 sec
INFO:root:2024-04-26 21:17:24, Train, Epoch : 8, Step : 4720, Loss : 0.22957, Acc : 0.906, Sensitive_Loss : 0.07375, Sensitive_Acc : 18.200, Run Time : 7.80 sec
INFO:root:2024-04-26 21:17:31, Train, Epoch : 8, Step : 4730, Loss : 0.21982, Acc : 0.900, Sensitive_Loss : 0.07986, Sensitive_Acc : 17.200, Run Time : 7.69 sec
INFO:root:2024-04-26 21:17:39, Train, Epoch : 8, Step : 4740, Loss : 0.20656, Acc : 0.912, Sensitive_Loss : 0.05291, Sensitive_Acc : 15.500, Run Time : 7.13 sec
INFO:root:2024-04-26 21:17:46, Train, Epoch : 8, Step : 4750, Loss : 0.31265, Acc : 0.859, Sensitive_Loss : 0.09368, Sensitive_Acc : 16.800, Run Time : 7.50 sec
INFO:root:2024-04-26 21:17:54, Train, Epoch : 8, Step : 4760, Loss : 0.25650, Acc : 0.897, Sensitive_Loss : 0.07329, Sensitive_Acc : 16.000, Run Time : 7.86 sec
INFO:root:2024-04-26 21:18:02, Train, Epoch : 8, Step : 4770, Loss : 0.24804, Acc : 0.894, Sensitive_Loss : 0.09501, Sensitive_Acc : 15.400, Run Time : 7.76 sec
INFO:root:2024-04-26 21:18:09, Train, Epoch : 8, Step : 4780, Loss : 0.29079, Acc : 0.884, Sensitive_Loss : 0.07014, Sensitive_Acc : 16.700, Run Time : 7.76 sec
INFO:root:2024-04-26 21:18:17, Train, Epoch : 8, Step : 4790, Loss : 0.30028, Acc : 0.863, Sensitive_Loss : 0.11114, Sensitive_Acc : 15.600, Run Time : 7.49 sec
INFO:root:2024-04-26 21:18:25, Train, Epoch : 8, Step : 4800, Loss : 0.28216, Acc : 0.869, Sensitive_Loss : 0.05749, Sensitive_Acc : 17.200, Run Time : 7.60 sec
INFO:root:2024-04-26 21:19:58, Dev, Step : 4800, Loss : 0.47984, Acc : 0.809, Auc : 0.901, Sensitive_Loss : 0.11095, Sensitive_Acc : 16.821, Sensitive_Auc : 0.995, Mean auc: 0.901, Run Time : 93.88 sec
INFO:root:2024-04-26 21:20:05, Train, Epoch : 8, Step : 4810, Loss : 0.22795, Acc : 0.919, Sensitive_Loss : 0.07107, Sensitive_Acc : 16.200, Run Time : 99.95 sec
INFO:root:2024-04-26 21:20:12, Train, Epoch : 8, Step : 4820, Loss : 0.25373, Acc : 0.909, Sensitive_Loss : 0.06114, Sensitive_Acc : 15.600, Run Time : 7.50 sec
INFO:root:2024-04-26 21:20:20, Train, Epoch : 8, Step : 4830, Loss : 0.25998, Acc : 0.903, Sensitive_Loss : 0.05990, Sensitive_Acc : 16.300, Run Time : 7.69 sec
INFO:root:2024-04-26 21:20:28, Train, Epoch : 8, Step : 4840, Loss : 0.32503, Acc : 0.847, Sensitive_Loss : 0.09485, Sensitive_Acc : 15.900, Run Time : 8.11 sec
INFO:root:2024-04-26 21:20:35, Train, Epoch : 8, Step : 4850, Loss : 0.23780, Acc : 0.897, Sensitive_Loss : 0.06437, Sensitive_Acc : 16.200, Run Time : 7.42 sec
INFO:root:2024-04-26 21:20:42, Train, Epoch : 8, Step : 4860, Loss : 0.22822, Acc : 0.887, Sensitive_Loss : 0.07152, Sensitive_Acc : 16.300, Run Time : 7.21 sec
INFO:root:2024-04-26 21:20:50, Train, Epoch : 8, Step : 4870, Loss : 0.24272, Acc : 0.903, Sensitive_Loss : 0.07934, Sensitive_Acc : 17.400, Run Time : 7.69 sec
INFO:root:2024-04-26 21:20:57, Train, Epoch : 8, Step : 4880, Loss : 0.27932, Acc : 0.894, Sensitive_Loss : 0.06391, Sensitive_Acc : 17.300, Run Time : 7.36 sec
INFO:root:2024-04-26 21:21:05, Train, Epoch : 8, Step : 4890, Loss : 0.24623, Acc : 0.903, Sensitive_Loss : 0.06854, Sensitive_Acc : 15.500, Run Time : 7.58 sec
INFO:root:2024-04-26 21:21:13, Train, Epoch : 8, Step : 4900, Loss : 0.23621, Acc : 0.887, Sensitive_Loss : 0.08885, Sensitive_Acc : 15.900, Run Time : 7.70 sec
INFO:root:2024-04-26 21:22:47, Dev, Step : 4900, Loss : 0.45927, Acc : 0.821, Auc : 0.903, Sensitive_Loss : 0.10581, Sensitive_Acc : 16.821, Sensitive_Auc : 0.995, Mean auc: 0.903, Run Time : 93.78 sec
INFO:root:2024-04-26 21:22:52, Train, Epoch : 8, Step : 4910, Loss : 0.29003, Acc : 0.894, Sensitive_Loss : 0.06294, Sensitive_Acc : 16.100, Run Time : 99.43 sec
INFO:root:2024-04-26 21:23:00, Train, Epoch : 8, Step : 4920, Loss : 0.22319, Acc : 0.912, Sensitive_Loss : 0.17040, Sensitive_Acc : 17.100, Run Time : 7.97 sec
INFO:root:2024-04-26 21:23:08, Train, Epoch : 8, Step : 4930, Loss : 0.23579, Acc : 0.900, Sensitive_Loss : 0.08083, Sensitive_Acc : 15.700, Run Time : 7.52 sec
INFO:root:2024-04-26 21:23:14, Train, Epoch : 8, Step : 4940, Loss : 0.26051, Acc : 0.894, Sensitive_Loss : 0.09443, Sensitive_Acc : 16.700, Run Time : 6.80 sec
INFO:root:2024-04-26 21:23:22, Train, Epoch : 8, Step : 4950, Loss : 0.20695, Acc : 0.928, Sensitive_Loss : 0.11886, Sensitive_Acc : 17.900, Run Time : 7.54 sec
INFO:root:2024-04-26 21:23:30, Train, Epoch : 8, Step : 4960, Loss : 0.25628, Acc : 0.900, Sensitive_Loss : 0.06599, Sensitive_Acc : 16.600, Run Time : 7.80 sec
INFO:root:2024-04-26 21:23:37, Train, Epoch : 8, Step : 4970, Loss : 0.24385, Acc : 0.875, Sensitive_Loss : 0.07828, Sensitive_Acc : 16.400, Run Time : 7.48 sec
INFO:root:2024-04-26 21:23:45, Train, Epoch : 8, Step : 4980, Loss : 0.26516, Acc : 0.881, Sensitive_Loss : 0.07970, Sensitive_Acc : 16.300, Run Time : 7.44 sec
INFO:root:2024-04-26 21:23:52, Train, Epoch : 8, Step : 4990, Loss : 0.22461, Acc : 0.903, Sensitive_Loss : 0.10745, Sensitive_Acc : 14.800, Run Time : 7.71 sec
INFO:root:2024-04-26 21:24:00, Train, Epoch : 8, Step : 5000, Loss : 0.29717, Acc : 0.878, Sensitive_Loss : 0.07042, Sensitive_Acc : 18.500, Run Time : 7.37 sec
INFO:root:2024-04-26 21:25:33, Dev, Step : 5000, Loss : 0.44192, Acc : 0.823, Auc : 0.908, Sensitive_Loss : 0.09710, Sensitive_Acc : 16.864, Sensitive_Auc : 0.995, Mean auc: 0.908, Run Time : 93.53 sec
INFO:root:2024-04-26 21:27:09
INFO:root:y_pred: [0.05456897 0.96410674 0.00622575 ... 0.7499436  0.00161004 0.97807515]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.70733285e-01 3.21381376e-04 1.37504013e-02 3.65150627e-04
 9.99861360e-01 7.66173471e-04 9.99977708e-01 9.99911427e-01
 1.86874846e-03 9.22606409e-01 9.97764945e-01 9.99983072e-01
 9.95660484e-01 9.96890247e-01 4.01047841e-02 9.83332872e-01
 9.99914169e-01 2.10568141e-02 6.30866706e-01 8.67151976e-01
 9.99206960e-01 2.69459069e-01 9.99523282e-01 9.97765183e-01
 9.99017358e-01 9.96258736e-01 4.41957236e-05 9.99989867e-01
 9.95545268e-01 1.31838769e-01 1.12605665e-03 5.72129041e-02
 1.07936963e-01 3.93243879e-02 3.20625715e-02 7.87448662e-04
 1.36200795e-02 4.49134910e-04 9.99898434e-01 9.99513984e-01
 2.83567460e-05 4.31349152e-04 9.92766500e-01 4.55398607e-04
 9.99955416e-01 9.98765945e-01 9.99815166e-01 9.93231416e-01
 6.48357812e-03 9.97765422e-01 9.99581635e-01 2.69332755e-04
 6.23211265e-02 1.12653640e-03 1.26119776e-04 4.79214601e-02
 5.94846979e-02 5.68595855e-03 3.75844981e-03 3.32139060e-02
 2.85383798e-02 6.94400072e-02 2.30626110e-02 9.03815210e-01
 2.14403689e-01 9.99774277e-01 3.07696313e-02 9.99924660e-01
 9.99587953e-01 2.61096865e-01 8.69593740e-01 5.25602758e-01
 2.97603576e-04 7.69548267e-02 7.49984756e-03 2.56004999e-03
 5.23230433e-02 3.23080719e-02 9.85212158e-03 9.99751866e-01
 9.99851823e-01 2.95024551e-03 2.07655266e-01 5.25120366e-03
 9.56758380e-01 9.64613616e-01 1.00012124e-02 2.39433404e-02
 9.95312929e-01 9.99920607e-01 9.99997735e-01 1.64213218e-02
 6.13793172e-03 9.97691870e-01 1.91648886e-01 3.48294433e-03
 9.99746025e-01 9.98558819e-01 1.00094541e-04 2.59440974e-03
 9.96730685e-01 9.93632495e-01 9.86789048e-01 9.95439589e-01
 1.78891909e-03 2.44967337e-03 9.92910326e-01 9.99697566e-01
 9.85115945e-01 5.68893620e-06 9.99544084e-01 9.99213338e-01
 3.13067548e-02 9.99950647e-01 9.99440134e-01 9.99813378e-01
 8.87450755e-01 9.96971846e-01 1.63516831e-02 8.76183901e-03
 9.99473274e-01 9.98372614e-01 1.06622472e-04 9.93279994e-01
 9.99994516e-01 5.85513294e-01 9.84935284e-01 4.71931696e-03
 3.27555626e-03 9.88574386e-01 9.99907017e-01 8.41581821e-03
 1.02709867e-02 3.41273355e-03 9.99341071e-01 9.93197203e-01
 9.98955369e-01 6.41224207e-04 1.09691536e-02 9.89928067e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 21:27:09, Dev, Step : 5008, Loss : 0.44897, Acc : 0.824, Auc : 0.907, Sensitive_Loss : 0.10215, Sensitive_Acc : 16.821, Sensitive_Auc : 0.994, Mean auc: 0.907, Run Time : 91.98 sec
INFO:root:2024-04-26 21:27:14, Train, Epoch : 9, Step : 5010, Loss : 0.04104, Acc : 0.181, Sensitive_Loss : 0.01277, Sensitive_Acc : 3.100, Run Time : 2.78 sec
INFO:root:2024-04-26 21:27:21, Train, Epoch : 9, Step : 5020, Loss : 0.29044, Acc : 0.878, Sensitive_Loss : 0.09653, Sensitive_Acc : 16.000, Run Time : 6.96 sec
INFO:root:2024-04-26 21:27:28, Train, Epoch : 9, Step : 5030, Loss : 0.24797, Acc : 0.884, Sensitive_Loss : 0.09303, Sensitive_Acc : 16.600, Run Time : 7.49 sec
INFO:root:2024-04-26 21:27:35, Train, Epoch : 9, Step : 5040, Loss : 0.17779, Acc : 0.925, Sensitive_Loss : 0.05297, Sensitive_Acc : 15.100, Run Time : 7.06 sec
INFO:root:2024-04-26 21:27:42, Train, Epoch : 9, Step : 5050, Loss : 0.23273, Acc : 0.891, Sensitive_Loss : 0.11819, Sensitive_Acc : 16.800, Run Time : 6.70 sec
INFO:root:2024-04-26 21:27:50, Train, Epoch : 9, Step : 5060, Loss : 0.22444, Acc : 0.906, Sensitive_Loss : 0.09844, Sensitive_Acc : 16.300, Run Time : 7.88 sec
INFO:root:2024-04-26 21:27:57, Train, Epoch : 9, Step : 5070, Loss : 0.21340, Acc : 0.912, Sensitive_Loss : 0.06205, Sensitive_Acc : 17.100, Run Time : 7.58 sec
INFO:root:2024-04-26 21:28:05, Train, Epoch : 9, Step : 5080, Loss : 0.22584, Acc : 0.909, Sensitive_Loss : 0.09074, Sensitive_Acc : 14.800, Run Time : 7.60 sec
INFO:root:2024-04-26 21:28:12, Train, Epoch : 9, Step : 5090, Loss : 0.21989, Acc : 0.906, Sensitive_Loss : 0.07056, Sensitive_Acc : 15.800, Run Time : 6.82 sec
INFO:root:2024-04-26 21:28:19, Train, Epoch : 9, Step : 5100, Loss : 0.18798, Acc : 0.925, Sensitive_Loss : 0.07124, Sensitive_Acc : 16.000, Run Time : 7.18 sec
INFO:root:2024-04-26 21:29:52, Dev, Step : 5100, Loss : 0.45030, Acc : 0.820, Auc : 0.907, Sensitive_Loss : 0.10922, Sensitive_Acc : 16.821, Sensitive_Auc : 0.994, Mean auc: 0.907, Run Time : 93.16 sec
INFO:root:2024-04-26 21:29:58, Train, Epoch : 9, Step : 5110, Loss : 0.25398, Acc : 0.912, Sensitive_Loss : 0.05433, Sensitive_Acc : 14.500, Run Time : 98.78 sec
INFO:root:2024-04-26 21:30:06, Train, Epoch : 9, Step : 5120, Loss : 0.23938, Acc : 0.897, Sensitive_Loss : 0.08335, Sensitive_Acc : 15.400, Run Time : 8.09 sec
INFO:root:2024-04-26 21:30:13, Train, Epoch : 9, Step : 5130, Loss : 0.25549, Acc : 0.878, Sensitive_Loss : 0.09170, Sensitive_Acc : 16.100, Run Time : 6.94 sec
INFO:root:2024-04-26 21:30:20, Train, Epoch : 9, Step : 5140, Loss : 0.24057, Acc : 0.894, Sensitive_Loss : 0.07227, Sensitive_Acc : 16.600, Run Time : 7.09 sec
INFO:root:2024-04-26 21:30:27, Train, Epoch : 9, Step : 5150, Loss : 0.22701, Acc : 0.903, Sensitive_Loss : 0.10524, Sensitive_Acc : 16.000, Run Time : 7.36 sec
INFO:root:2024-04-26 21:30:34, Train, Epoch : 9, Step : 5160, Loss : 0.21243, Acc : 0.878, Sensitive_Loss : 0.08111, Sensitive_Acc : 14.800, Run Time : 7.16 sec
INFO:root:2024-04-26 21:30:42, Train, Epoch : 9, Step : 5170, Loss : 0.21067, Acc : 0.891, Sensitive_Loss : 0.10071, Sensitive_Acc : 15.800, Run Time : 7.54 sec
INFO:root:2024-04-26 21:30:49, Train, Epoch : 9, Step : 5180, Loss : 0.28418, Acc : 0.856, Sensitive_Loss : 0.09343, Sensitive_Acc : 16.000, Run Time : 7.08 sec
INFO:root:2024-04-26 21:30:57, Train, Epoch : 9, Step : 5190, Loss : 0.18518, Acc : 0.919, Sensitive_Loss : 0.09461, Sensitive_Acc : 16.900, Run Time : 7.54 sec
INFO:root:2024-04-26 21:31:04, Train, Epoch : 9, Step : 5200, Loss : 0.21302, Acc : 0.909, Sensitive_Loss : 0.10125, Sensitive_Acc : 16.100, Run Time : 7.20 sec
INFO:root:2024-04-26 21:32:38, Dev, Step : 5200, Loss : 0.43856, Acc : 0.825, Auc : 0.905, Sensitive_Loss : 0.12019, Sensitive_Acc : 16.821, Sensitive_Auc : 0.993, Mean auc: 0.905, Run Time : 93.96 sec
INFO:root:2024-04-26 21:32:43, Train, Epoch : 9, Step : 5210, Loss : 0.18937, Acc : 0.925, Sensitive_Loss : 0.08715, Sensitive_Acc : 16.100, Run Time : 99.44 sec
INFO:root:2024-04-26 21:32:51, Train, Epoch : 9, Step : 5220, Loss : 0.20166, Acc : 0.916, Sensitive_Loss : 0.08552, Sensitive_Acc : 18.400, Run Time : 7.35 sec
INFO:root:2024-04-26 21:32:58, Train, Epoch : 9, Step : 5230, Loss : 0.21736, Acc : 0.928, Sensitive_Loss : 0.06750, Sensitive_Acc : 15.700, Run Time : 7.49 sec
INFO:root:2024-04-26 21:33:06, Train, Epoch : 9, Step : 5240, Loss : 0.23651, Acc : 0.891, Sensitive_Loss : 0.10228, Sensitive_Acc : 16.100, Run Time : 7.67 sec
INFO:root:2024-04-26 21:33:13, Train, Epoch : 9, Step : 5250, Loss : 0.22562, Acc : 0.925, Sensitive_Loss : 0.04313, Sensitive_Acc : 16.800, Run Time : 7.48 sec
INFO:root:2024-04-26 21:33:21, Train, Epoch : 9, Step : 5260, Loss : 0.19548, Acc : 0.916, Sensitive_Loss : 0.05665, Sensitive_Acc : 17.100, Run Time : 7.52 sec
INFO:root:2024-04-26 21:33:28, Train, Epoch : 9, Step : 5270, Loss : 0.20071, Acc : 0.909, Sensitive_Loss : 0.09645, Sensitive_Acc : 15.800, Run Time : 7.29 sec
INFO:root:2024-04-26 21:33:35, Train, Epoch : 9, Step : 5280, Loss : 0.29839, Acc : 0.869, Sensitive_Loss : 0.06217, Sensitive_Acc : 15.100, Run Time : 7.13 sec
INFO:root:2024-04-26 21:33:42, Train, Epoch : 9, Step : 5290, Loss : 0.22109, Acc : 0.916, Sensitive_Loss : 0.06441, Sensitive_Acc : 16.000, Run Time : 7.39 sec
INFO:root:2024-04-26 21:33:50, Train, Epoch : 9, Step : 5300, Loss : 0.24365, Acc : 0.900, Sensitive_Loss : 0.09453, Sensitive_Acc : 17.000, Run Time : 7.56 sec
INFO:root:2024-04-26 21:35:23, Dev, Step : 5300, Loss : 0.46970, Acc : 0.819, Auc : 0.904, Sensitive_Loss : 0.10968, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.904, Run Time : 93.40 sec
INFO:root:2024-04-26 21:35:29, Train, Epoch : 9, Step : 5310, Loss : 0.24449, Acc : 0.912, Sensitive_Loss : 0.08951, Sensitive_Acc : 16.600, Run Time : 99.11 sec
INFO:root:2024-04-26 21:35:37, Train, Epoch : 9, Step : 5320, Loss : 0.25559, Acc : 0.887, Sensitive_Loss : 0.06792, Sensitive_Acc : 15.300, Run Time : 7.58 sec
INFO:root:2024-04-26 21:35:44, Train, Epoch : 9, Step : 5330, Loss : 0.20741, Acc : 0.906, Sensitive_Loss : 0.07305, Sensitive_Acc : 16.900, Run Time : 7.45 sec
INFO:root:2024-04-26 21:35:51, Train, Epoch : 9, Step : 5340, Loss : 0.21869, Acc : 0.903, Sensitive_Loss : 0.08194, Sensitive_Acc : 16.200, Run Time : 7.15 sec
INFO:root:2024-04-26 21:35:59, Train, Epoch : 9, Step : 5350, Loss : 0.20469, Acc : 0.925, Sensitive_Loss : 0.07727, Sensitive_Acc : 15.800, Run Time : 7.35 sec
INFO:root:2024-04-26 21:36:07, Train, Epoch : 9, Step : 5360, Loss : 0.23373, Acc : 0.894, Sensitive_Loss : 0.07572, Sensitive_Acc : 15.600, Run Time : 7.84 sec
INFO:root:2024-04-26 21:36:14, Train, Epoch : 9, Step : 5370, Loss : 0.19892, Acc : 0.919, Sensitive_Loss : 0.06486, Sensitive_Acc : 16.600, Run Time : 7.35 sec
INFO:root:2024-04-26 21:36:21, Train, Epoch : 9, Step : 5380, Loss : 0.20465, Acc : 0.906, Sensitive_Loss : 0.08280, Sensitive_Acc : 16.000, Run Time : 7.20 sec
INFO:root:2024-04-26 21:36:28, Train, Epoch : 9, Step : 5390, Loss : 0.28919, Acc : 0.891, Sensitive_Loss : 0.08450, Sensitive_Acc : 17.000, Run Time : 7.42 sec
INFO:root:2024-04-26 21:36:36, Train, Epoch : 9, Step : 5400, Loss : 0.23332, Acc : 0.875, Sensitive_Loss : 0.06923, Sensitive_Acc : 17.400, Run Time : 7.51 sec
INFO:root:2024-04-26 21:38:09, Dev, Step : 5400, Loss : 0.43930, Acc : 0.826, Auc : 0.904, Sensitive_Loss : 0.11445, Sensitive_Acc : 16.821, Sensitive_Auc : 0.992, Mean auc: 0.904, Run Time : 93.49 sec
INFO:root:2024-04-26 21:38:15, Train, Epoch : 9, Step : 5410, Loss : 0.27801, Acc : 0.881, Sensitive_Loss : 0.09003, Sensitive_Acc : 17.300, Run Time : 99.07 sec
INFO:root:2024-04-26 21:38:23, Train, Epoch : 9, Step : 5420, Loss : 0.22088, Acc : 0.912, Sensitive_Loss : 0.06506, Sensitive_Acc : 16.500, Run Time : 7.82 sec
INFO:root:2024-04-26 21:38:30, Train, Epoch : 9, Step : 5430, Loss : 0.24975, Acc : 0.912, Sensitive_Loss : 0.06997, Sensitive_Acc : 16.400, Run Time : 7.09 sec
INFO:root:2024-04-26 21:38:37, Train, Epoch : 9, Step : 5440, Loss : 0.17342, Acc : 0.906, Sensitive_Loss : 0.07374, Sensitive_Acc : 16.200, Run Time : 7.00 sec
INFO:root:2024-04-26 21:38:44, Train, Epoch : 9, Step : 5450, Loss : 0.19474, Acc : 0.912, Sensitive_Loss : 0.08255, Sensitive_Acc : 16.700, Run Time : 7.44 sec
INFO:root:2024-04-26 21:38:52, Train, Epoch : 9, Step : 5460, Loss : 0.20269, Acc : 0.894, Sensitive_Loss : 0.09855, Sensitive_Acc : 16.900, Run Time : 7.44 sec
INFO:root:2024-04-26 21:38:59, Train, Epoch : 9, Step : 5470, Loss : 0.27880, Acc : 0.881, Sensitive_Loss : 0.05166, Sensitive_Acc : 15.600, Run Time : 7.23 sec
INFO:root:2024-04-26 21:39:06, Train, Epoch : 9, Step : 5480, Loss : 0.23022, Acc : 0.916, Sensitive_Loss : 0.09027, Sensitive_Acc : 17.100, Run Time : 7.37 sec
INFO:root:2024-04-26 21:39:14, Train, Epoch : 9, Step : 5490, Loss : 0.27845, Acc : 0.881, Sensitive_Loss : 0.08494, Sensitive_Acc : 16.600, Run Time : 7.26 sec
INFO:root:2024-04-26 21:39:21, Train, Epoch : 9, Step : 5500, Loss : 0.22517, Acc : 0.897, Sensitive_Loss : 0.05698, Sensitive_Acc : 15.300, Run Time : 7.39 sec
INFO:root:2024-04-26 21:40:55, Dev, Step : 5500, Loss : 0.44117, Acc : 0.824, Auc : 0.903, Sensitive_Loss : 0.10576, Sensitive_Acc : 16.836, Sensitive_Auc : 0.993, Mean auc: 0.903, Run Time : 93.54 sec
INFO:root:2024-04-26 21:41:00, Train, Epoch : 9, Step : 5510, Loss : 0.23334, Acc : 0.906, Sensitive_Loss : 0.09313, Sensitive_Acc : 17.200, Run Time : 99.17 sec
INFO:root:2024-04-26 21:41:07, Train, Epoch : 9, Step : 5520, Loss : 0.22841, Acc : 0.909, Sensitive_Loss : 0.06284, Sensitive_Acc : 16.900, Run Time : 6.96 sec
INFO:root:2024-04-26 21:41:15, Train, Epoch : 9, Step : 5530, Loss : 0.25484, Acc : 0.875, Sensitive_Loss : 0.08087, Sensitive_Acc : 16.500, Run Time : 7.34 sec
INFO:root:2024-04-26 21:41:22, Train, Epoch : 9, Step : 5540, Loss : 0.28066, Acc : 0.900, Sensitive_Loss : 0.09920, Sensitive_Acc : 16.400, Run Time : 7.18 sec
INFO:root:2024-04-26 21:41:29, Train, Epoch : 9, Step : 5550, Loss : 0.24149, Acc : 0.903, Sensitive_Loss : 0.05223, Sensitive_Acc : 16.500, Run Time : 7.17 sec
INFO:root:2024-04-26 21:41:36, Train, Epoch : 9, Step : 5560, Loss : 0.29816, Acc : 0.887, Sensitive_Loss : 0.09075, Sensitive_Acc : 18.200, Run Time : 7.47 sec
INFO:root:2024-04-26 21:41:44, Train, Epoch : 9, Step : 5570, Loss : 0.25134, Acc : 0.881, Sensitive_Loss : 0.07480, Sensitive_Acc : 15.900, Run Time : 7.21 sec
INFO:root:2024-04-26 21:41:51, Train, Epoch : 9, Step : 5580, Loss : 0.21404, Acc : 0.894, Sensitive_Loss : 0.06477, Sensitive_Acc : 15.900, Run Time : 7.20 sec
INFO:root:2024-04-26 21:41:58, Train, Epoch : 9, Step : 5590, Loss : 0.22201, Acc : 0.925, Sensitive_Loss : 0.05727, Sensitive_Acc : 16.300, Run Time : 7.12 sec
INFO:root:2024-04-26 21:42:05, Train, Epoch : 9, Step : 5600, Loss : 0.23270, Acc : 0.897, Sensitive_Loss : 0.13170, Sensitive_Acc : 15.000, Run Time : 7.50 sec
INFO:root:2024-04-26 21:43:38, Dev, Step : 5600, Loss : 0.45447, Acc : 0.823, Auc : 0.905, Sensitive_Loss : 0.10074, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.905, Run Time : 93.06 sec
INFO:root:2024-04-26 21:43:44, Train, Epoch : 9, Step : 5610, Loss : 0.24749, Acc : 0.909, Sensitive_Loss : 0.09175, Sensitive_Acc : 16.600, Run Time : 98.64 sec
INFO:root:2024-04-26 21:43:51, Train, Epoch : 9, Step : 5620, Loss : 0.23905, Acc : 0.894, Sensitive_Loss : 0.06200, Sensitive_Acc : 17.100, Run Time : 7.17 sec
INFO:root:2024-04-26 21:43:58, Train, Epoch : 9, Step : 5630, Loss : 0.25602, Acc : 0.900, Sensitive_Loss : 0.08282, Sensitive_Acc : 15.700, Run Time : 7.28 sec
INFO:root:2024-04-26 21:45:34
INFO:root:y_pred: [5.8164667e-02 9.8125356e-01 5.6147412e-03 ... 7.9255259e-01 3.4708163e-04
 9.9271315e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.8275119e-01 5.1801599e-04 2.0156164e-02 5.7017029e-04 9.9999738e-01
 9.7442616e-04 9.9999595e-01 9.9987113e-01 1.5870384e-03 9.5158315e-01
 9.9978894e-01 9.9999285e-01 9.9653131e-01 9.9614644e-01 1.6623270e-02
 9.8842645e-01 9.9996352e-01 2.2522844e-02 6.8285686e-01 9.3272907e-01
 9.9901140e-01 1.3606058e-01 9.9971420e-01 9.9626595e-01 9.9930871e-01
 9.9703848e-01 8.0621241e-05 9.9998963e-01 9.9202538e-01 1.4049862e-01
 5.7844585e-04 2.8578228e-01 8.8701040e-02 4.3479502e-02 5.2764285e-02
 5.9095793e-04 7.3954081e-03 2.9597603e-04 9.9998486e-01 9.9951398e-01
 3.3465371e-05 4.2432945e-04 9.9788123e-01 4.2290782e-04 9.9999559e-01
 9.9968839e-01 9.9990809e-01 9.9134773e-01 5.1190448e-03 9.9809998e-01
 9.9993372e-01 3.5271980e-04 2.2151396e-01 1.3291106e-03 1.6638031e-04
 3.7332643e-02 8.4077254e-02 2.3627777e-03 6.1765658e-03 3.6921415e-02
 1.9348167e-02 5.3091511e-02 1.0020162e-02 9.7718561e-01 8.6754240e-02
 9.9994016e-01 3.0081624e-02 9.9998701e-01 9.9970716e-01 3.6262518e-01
 9.1917479e-01 3.3974504e-01 5.1942316e-04 1.2749392e-01 5.7688877e-03
 3.6749055e-03 6.7232251e-02 3.0411059e-02 8.1008151e-03 9.9981159e-01
 9.9996781e-01 2.4958896e-03 2.3431012e-01 3.6521151e-03 9.7726929e-01
 9.9310851e-01 1.2402315e-02 2.0322444e-02 9.9771428e-01 9.9998283e-01
 9.9999917e-01 2.3200817e-02 6.4229551e-03 9.9957305e-01 1.8352351e-01
 6.0560065e-03 9.9987376e-01 9.9946982e-01 4.6878609e-05 2.5003073e-03
 9.9653399e-01 9.9438983e-01 9.9690956e-01 9.9735105e-01 1.4119988e-03
 3.8125014e-03 9.8934644e-01 9.9971634e-01 9.9021578e-01 1.2525946e-05
 9.9939930e-01 9.9992049e-01 3.1379826e-02 9.9997818e-01 9.9921238e-01
 9.9987495e-01 8.2081127e-01 9.9626786e-01 1.7325632e-02 1.0981147e-02
 9.9981958e-01 9.9870062e-01 1.0664098e-04 9.9619484e-01 9.9999881e-01
 6.1242253e-01 9.8616469e-01 8.3104204e-03 1.8719876e-02 9.9560839e-01
 9.9992418e-01 9.7064869e-03 1.3193052e-02 7.7398443e-03 9.9961638e-01
 9.9706370e-01 9.9892777e-01 4.0153868e-04 7.4835056e-03 9.9152863e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 21:45:34, Dev, Step : 5634, Loss : 0.46018, Acc : 0.822, Auc : 0.907, Sensitive_Loss : 0.10635, Sensitive_Acc : 16.850, Sensitive_Auc : 0.994, Mean auc: 0.907, Run Time : 92.87 sec
INFO:root:2024-04-26 21:45:40, Train, Epoch : 10, Step : 5640, Loss : 0.13119, Acc : 0.547, Sensitive_Loss : 0.04187, Sensitive_Acc : 9.900, Run Time : 5.39 sec
INFO:root:2024-04-26 21:45:48, Train, Epoch : 10, Step : 5650, Loss : 0.18064, Acc : 0.938, Sensitive_Loss : 0.06842, Sensitive_Acc : 17.200, Run Time : 7.14 sec
INFO:root:2024-04-26 21:45:55, Train, Epoch : 10, Step : 5660, Loss : 0.22323, Acc : 0.906, Sensitive_Loss : 0.08446, Sensitive_Acc : 16.100, Run Time : 7.37 sec
INFO:root:2024-04-26 21:46:02, Train, Epoch : 10, Step : 5670, Loss : 0.19420, Acc : 0.903, Sensitive_Loss : 0.07152, Sensitive_Acc : 16.500, Run Time : 7.04 sec
INFO:root:2024-04-26 21:46:09, Train, Epoch : 10, Step : 5680, Loss : 0.20093, Acc : 0.922, Sensitive_Loss : 0.06329, Sensitive_Acc : 15.000, Run Time : 6.83 sec
INFO:root:2024-04-26 21:46:16, Train, Epoch : 10, Step : 5690, Loss : 0.22375, Acc : 0.906, Sensitive_Loss : 0.06951, Sensitive_Acc : 17.200, Run Time : 7.46 sec
INFO:root:2024-04-26 21:46:24, Train, Epoch : 10, Step : 5700, Loss : 0.23953, Acc : 0.897, Sensitive_Loss : 0.08104, Sensitive_Acc : 16.300, Run Time : 7.33 sec
INFO:root:2024-04-26 21:47:57, Dev, Step : 5700, Loss : 0.47140, Acc : 0.822, Auc : 0.906, Sensitive_Loss : 0.10431, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.906, Run Time : 92.98 sec
INFO:root:2024-04-26 21:48:02, Train, Epoch : 10, Step : 5710, Loss : 0.18708, Acc : 0.919, Sensitive_Loss : 0.11445, Sensitive_Acc : 18.100, Run Time : 98.40 sec
INFO:root:2024-04-26 21:48:10, Train, Epoch : 10, Step : 5720, Loss : 0.16694, Acc : 0.919, Sensitive_Loss : 0.08603, Sensitive_Acc : 16.000, Run Time : 7.69 sec
INFO:root:2024-04-26 21:48:17, Train, Epoch : 10, Step : 5730, Loss : 0.19675, Acc : 0.906, Sensitive_Loss : 0.06628, Sensitive_Acc : 16.500, Run Time : 6.91 sec
INFO:root:2024-04-26 21:48:24, Train, Epoch : 10, Step : 5740, Loss : 0.17859, Acc : 0.922, Sensitive_Loss : 0.06270, Sensitive_Acc : 17.000, Run Time : 7.40 sec
INFO:root:2024-04-26 21:48:31, Train, Epoch : 10, Step : 5750, Loss : 0.22833, Acc : 0.903, Sensitive_Loss : 0.07033, Sensitive_Acc : 17.800, Run Time : 7.04 sec
INFO:root:2024-04-26 21:48:38, Train, Epoch : 10, Step : 5760, Loss : 0.17533, Acc : 0.944, Sensitive_Loss : 0.04578, Sensitive_Acc : 15.500, Run Time : 6.91 sec
INFO:root:2024-04-26 21:48:45, Train, Epoch : 10, Step : 5770, Loss : 0.20854, Acc : 0.887, Sensitive_Loss : 0.06839, Sensitive_Acc : 16.800, Run Time : 7.02 sec
INFO:root:2024-04-26 21:48:52, Train, Epoch : 10, Step : 5780, Loss : 0.25552, Acc : 0.916, Sensitive_Loss : 0.11951, Sensitive_Acc : 15.500, Run Time : 7.38 sec
INFO:root:2024-04-26 21:49:00, Train, Epoch : 10, Step : 5790, Loss : 0.26497, Acc : 0.894, Sensitive_Loss : 0.08194, Sensitive_Acc : 16.200, Run Time : 7.49 sec
INFO:root:2024-04-26 21:49:07, Train, Epoch : 10, Step : 5800, Loss : 0.22005, Acc : 0.916, Sensitive_Loss : 0.11402, Sensitive_Acc : 14.200, Run Time : 6.77 sec
INFO:root:2024-04-26 21:50:40, Dev, Step : 5800, Loss : 0.48044, Acc : 0.819, Auc : 0.903, Sensitive_Loss : 0.10091, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.903, Run Time : 93.22 sec
INFO:root:2024-04-26 21:50:45, Train, Epoch : 10, Step : 5810, Loss : 0.18650, Acc : 0.931, Sensitive_Loss : 0.06721, Sensitive_Acc : 16.300, Run Time : 98.69 sec
INFO:root:2024-04-26 21:50:53, Train, Epoch : 10, Step : 5820, Loss : 0.24103, Acc : 0.906, Sensitive_Loss : 0.09038, Sensitive_Acc : 16.500, Run Time : 7.73 sec
INFO:root:2024-04-26 21:51:00, Train, Epoch : 10, Step : 5830, Loss : 0.25910, Acc : 0.884, Sensitive_Loss : 0.06376, Sensitive_Acc : 16.200, Run Time : 7.00 sec
INFO:root:2024-04-26 21:51:07, Train, Epoch : 10, Step : 5840, Loss : 0.24715, Acc : 0.891, Sensitive_Loss : 0.07704, Sensitive_Acc : 16.400, Run Time : 7.05 sec
INFO:root:2024-04-26 21:51:14, Train, Epoch : 10, Step : 5850, Loss : 0.18167, Acc : 0.963, Sensitive_Loss : 0.06840, Sensitive_Acc : 15.700, Run Time : 6.89 sec
INFO:root:2024-04-26 21:51:21, Train, Epoch : 10, Step : 5860, Loss : 0.22856, Acc : 0.906, Sensitive_Loss : 0.06226, Sensitive_Acc : 16.800, Run Time : 7.25 sec
INFO:root:2024-04-26 21:51:29, Train, Epoch : 10, Step : 5870, Loss : 0.18993, Acc : 0.903, Sensitive_Loss : 0.08696, Sensitive_Acc : 15.500, Run Time : 7.57 sec
INFO:root:2024-04-26 21:51:36, Train, Epoch : 10, Step : 5880, Loss : 0.21079, Acc : 0.916, Sensitive_Loss : 0.12283, Sensitive_Acc : 17.200, Run Time : 7.18 sec
INFO:root:2024-04-26 21:51:43, Train, Epoch : 10, Step : 5890, Loss : 0.21395, Acc : 0.925, Sensitive_Loss : 0.07438, Sensitive_Acc : 16.400, Run Time : 6.91 sec
INFO:root:2024-04-26 21:51:50, Train, Epoch : 10, Step : 5900, Loss : 0.20846, Acc : 0.912, Sensitive_Loss : 0.09158, Sensitive_Acc : 16.900, Run Time : 7.03 sec
INFO:root:2024-04-26 21:53:23, Dev, Step : 5900, Loss : 0.47727, Acc : 0.820, Auc : 0.904, Sensitive_Loss : 0.11336, Sensitive_Acc : 16.850, Sensitive_Auc : 0.995, Mean auc: 0.904, Run Time : 93.23 sec
INFO:root:2024-04-26 21:53:28, Train, Epoch : 10, Step : 5910, Loss : 0.20018, Acc : 0.897, Sensitive_Loss : 0.08797, Sensitive_Acc : 17.300, Run Time : 98.48 sec
INFO:root:2024-04-26 21:53:36, Train, Epoch : 10, Step : 5920, Loss : 0.20469, Acc : 0.887, Sensitive_Loss : 0.11149, Sensitive_Acc : 16.000, Run Time : 7.21 sec
INFO:root:2024-04-26 21:53:43, Train, Epoch : 10, Step : 5930, Loss : 0.21087, Acc : 0.916, Sensitive_Loss : 0.07935, Sensitive_Acc : 16.000, Run Time : 7.27 sec
INFO:root:2024-04-26 21:53:50, Train, Epoch : 10, Step : 5940, Loss : 0.19768, Acc : 0.912, Sensitive_Loss : 0.06948, Sensitive_Acc : 15.400, Run Time : 7.53 sec
INFO:root:2024-04-26 21:53:57, Train, Epoch : 10, Step : 5950, Loss : 0.21708, Acc : 0.903, Sensitive_Loss : 0.07312, Sensitive_Acc : 16.300, Run Time : 6.86 sec
INFO:root:2024-04-26 21:54:04, Train, Epoch : 10, Step : 5960, Loss : 0.23022, Acc : 0.909, Sensitive_Loss : 0.08120, Sensitive_Acc : 17.200, Run Time : 6.99 sec
INFO:root:2024-04-26 21:54:11, Train, Epoch : 10, Step : 5970, Loss : 0.18193, Acc : 0.922, Sensitive_Loss : 0.07169, Sensitive_Acc : 14.900, Run Time : 7.18 sec
INFO:root:2024-04-26 21:54:19, Train, Epoch : 10, Step : 5980, Loss : 0.21605, Acc : 0.919, Sensitive_Loss : 0.08540, Sensitive_Acc : 15.000, Run Time : 7.41 sec
INFO:root:2024-04-26 21:54:27, Train, Epoch : 10, Step : 5990, Loss : 0.27515, Acc : 0.884, Sensitive_Loss : 0.07407, Sensitive_Acc : 15.500, Run Time : 7.64 sec
INFO:root:2024-04-26 21:54:33, Train, Epoch : 10, Step : 6000, Loss : 0.24076, Acc : 0.900, Sensitive_Loss : 0.06547, Sensitive_Acc : 14.600, Run Time : 6.90 sec
INFO:root:2024-04-26 21:56:07, Dev, Step : 6000, Loss : 0.50157, Acc : 0.814, Auc : 0.900, Sensitive_Loss : 0.10727, Sensitive_Acc : 16.879, Sensitive_Auc : 0.994, Mean auc: 0.900, Run Time : 93.53 sec
INFO:root:2024-04-26 21:56:13, Train, Epoch : 10, Step : 6010, Loss : 0.21663, Acc : 0.903, Sensitive_Loss : 0.09112, Sensitive_Acc : 16.900, Run Time : 99.20 sec
INFO:root:2024-04-26 21:56:20, Train, Epoch : 10, Step : 6020, Loss : 0.17339, Acc : 0.938, Sensitive_Loss : 0.07845, Sensitive_Acc : 15.700, Run Time : 7.06 sec
INFO:root:2024-04-26 21:56:27, Train, Epoch : 10, Step : 6030, Loss : 0.21907, Acc : 0.934, Sensitive_Loss : 0.06763, Sensitive_Acc : 16.200, Run Time : 7.26 sec
INFO:root:2024-04-26 21:56:34, Train, Epoch : 10, Step : 6040, Loss : 0.27892, Acc : 0.887, Sensitive_Loss : 0.06801, Sensitive_Acc : 17.300, Run Time : 7.06 sec
INFO:root:2024-04-26 21:56:41, Train, Epoch : 10, Step : 6050, Loss : 0.20768, Acc : 0.903, Sensitive_Loss : 0.04648, Sensitive_Acc : 16.600, Run Time : 7.29 sec
INFO:root:2024-04-26 21:56:48, Train, Epoch : 10, Step : 6060, Loss : 0.16371, Acc : 0.931, Sensitive_Loss : 0.06913, Sensitive_Acc : 16.600, Run Time : 7.01 sec
INFO:root:2024-04-26 21:56:56, Train, Epoch : 10, Step : 6070, Loss : 0.18553, Acc : 0.916, Sensitive_Loss : 0.07777, Sensitive_Acc : 15.200, Run Time : 7.28 sec
INFO:root:2024-04-26 21:57:03, Train, Epoch : 10, Step : 6080, Loss : 0.21349, Acc : 0.891, Sensitive_Loss : 0.08321, Sensitive_Acc : 16.900, Run Time : 7.64 sec
INFO:root:2024-04-26 21:57:10, Train, Epoch : 10, Step : 6090, Loss : 0.24427, Acc : 0.900, Sensitive_Loss : 0.08504, Sensitive_Acc : 17.700, Run Time : 6.85 sec
INFO:root:2024-04-26 21:57:18, Train, Epoch : 10, Step : 6100, Loss : 0.22228, Acc : 0.900, Sensitive_Loss : 0.05586, Sensitive_Acc : 15.900, Run Time : 7.47 sec
INFO:root:2024-04-26 21:58:51, Dev, Step : 6100, Loss : 0.46303, Acc : 0.824, Auc : 0.905, Sensitive_Loss : 0.11468, Sensitive_Acc : 16.821, Sensitive_Auc : 0.994, Mean auc: 0.905, Run Time : 93.51 sec
INFO:root:2024-04-26 21:58:56, Train, Epoch : 10, Step : 6110, Loss : 0.23190, Acc : 0.903, Sensitive_Loss : 0.05848, Sensitive_Acc : 15.900, Run Time : 98.87 sec
INFO:root:2024-04-26 21:59:04, Train, Epoch : 10, Step : 6120, Loss : 0.20285, Acc : 0.931, Sensitive_Loss : 0.07291, Sensitive_Acc : 15.600, Run Time : 7.59 sec
INFO:root:2024-04-26 21:59:11, Train, Epoch : 10, Step : 6130, Loss : 0.19654, Acc : 0.903, Sensitive_Loss : 0.07641, Sensitive_Acc : 16.300, Run Time : 7.40 sec
INFO:root:2024-04-26 21:59:19, Train, Epoch : 10, Step : 6140, Loss : 0.22809, Acc : 0.887, Sensitive_Loss : 0.07671, Sensitive_Acc : 15.300, Run Time : 7.15 sec
INFO:root:2024-04-26 21:59:26, Train, Epoch : 10, Step : 6150, Loss : 0.17143, Acc : 0.928, Sensitive_Loss : 0.07522, Sensitive_Acc : 16.400, Run Time : 7.69 sec
INFO:root:2024-04-26 21:59:33, Train, Epoch : 10, Step : 6160, Loss : 0.19378, Acc : 0.912, Sensitive_Loss : 0.10043, Sensitive_Acc : 15.500, Run Time : 6.85 sec
INFO:root:2024-04-26 21:59:41, Train, Epoch : 10, Step : 6170, Loss : 0.20180, Acc : 0.900, Sensitive_Loss : 0.08039, Sensitive_Acc : 17.000, Run Time : 7.63 sec
INFO:root:2024-04-26 21:59:48, Train, Epoch : 10, Step : 6180, Loss : 0.27676, Acc : 0.881, Sensitive_Loss : 0.04812, Sensitive_Acc : 16.800, Run Time : 6.95 sec
INFO:root:2024-04-26 21:59:55, Train, Epoch : 10, Step : 6190, Loss : 0.22893, Acc : 0.887, Sensitive_Loss : 0.12282, Sensitive_Acc : 16.500, Run Time : 7.45 sec
INFO:root:2024-04-26 22:00:02, Train, Epoch : 10, Step : 6200, Loss : 0.20835, Acc : 0.925, Sensitive_Loss : 0.07171, Sensitive_Acc : 17.100, Run Time : 6.95 sec
INFO:root:2024-04-26 22:01:36, Dev, Step : 6200, Loss : 0.46896, Acc : 0.821, Auc : 0.903, Sensitive_Loss : 0.09371, Sensitive_Acc : 16.893, Sensitive_Auc : 0.996, Mean auc: 0.903, Run Time : 93.77 sec
INFO:root:2024-04-26 22:01:41, Train, Epoch : 10, Step : 6210, Loss : 0.21578, Acc : 0.919, Sensitive_Loss : 0.07787, Sensitive_Acc : 17.400, Run Time : 99.31 sec
INFO:root:2024-04-26 22:01:49, Train, Epoch : 10, Step : 6220, Loss : 0.22731, Acc : 0.919, Sensitive_Loss : 0.09386, Sensitive_Acc : 16.900, Run Time : 7.62 sec
INFO:root:2024-04-26 22:01:56, Train, Epoch : 10, Step : 6230, Loss : 0.27950, Acc : 0.887, Sensitive_Loss : 0.07242, Sensitive_Acc : 17.300, Run Time : 7.27 sec
INFO:root:2024-04-26 22:02:04, Train, Epoch : 10, Step : 6240, Loss : 0.21006, Acc : 0.909, Sensitive_Loss : 0.06141, Sensitive_Acc : 14.300, Run Time : 7.25 sec
INFO:root:2024-04-26 22:02:11, Train, Epoch : 10, Step : 6250, Loss : 0.23367, Acc : 0.897, Sensitive_Loss : 0.04778, Sensitive_Acc : 16.500, Run Time : 7.10 sec
INFO:root:2024-04-26 22:02:18, Train, Epoch : 10, Step : 6260, Loss : 0.17768, Acc : 0.934, Sensitive_Loss : 0.06292, Sensitive_Acc : 16.000, Run Time : 7.08 sec
INFO:root:2024-04-26 22:03:50
INFO:root:y_pred: [5.2811898e-02 9.8731601e-01 5.6104036e-03 ... 8.6897665e-01 6.7293330e-04
 9.9507153e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.8030806e-01 1.1638558e-03 2.6117679e-02 9.7130536e-04 9.9999356e-01
 1.4215336e-03 9.9998856e-01 9.9985230e-01 1.6023575e-03 9.6612173e-01
 9.9961627e-01 9.9998832e-01 9.9472839e-01 9.9757284e-01 1.6222129e-02
 9.8961639e-01 9.9994743e-01 2.7668329e-02 5.9977251e-01 9.5282894e-01
 9.9879527e-01 2.5879529e-01 9.9966586e-01 9.9726403e-01 9.9827027e-01
 9.9414343e-01 3.3444434e-04 9.9998283e-01 9.9132818e-01 9.9058941e-02
 1.0368747e-03 1.5149273e-01 8.0099702e-02 2.9460751e-02 5.1271595e-02
 7.1870134e-04 9.2109321e-03 9.0718525e-04 9.9997807e-01 9.9907959e-01
 3.4079992e-05 9.8723872e-04 9.9792647e-01 1.7774116e-03 9.9998093e-01
 9.9953771e-01 9.9991131e-01 9.8967510e-01 6.4244429e-03 9.9689013e-01
 9.9989831e-01 8.0750982e-04 1.6661623e-01 1.7933086e-03 1.7808305e-04
 4.2395253e-02 8.6525030e-02 3.4136253e-03 5.7114288e-03 5.2750073e-02
 4.0238760e-02 3.5098009e-02 1.9549329e-02 9.7882885e-01 6.6234805e-02
 9.9994755e-01 2.4011696e-02 9.9998963e-01 9.9963415e-01 2.6743993e-01
 8.7919974e-01 3.7930000e-01 1.0072298e-03 1.6192476e-01 6.5755188e-03
 1.0605276e-02 2.7783383e-02 2.8577795e-02 1.1838385e-02 9.9982196e-01
 9.9989212e-01 3.7393493e-03 3.6618295e-01 8.7217791e-03 9.2619270e-01
 9.8716819e-01 3.6705684e-02 1.8767094e-02 9.9708420e-01 9.9997807e-01
 9.9999833e-01 1.7946687e-02 1.1373643e-02 9.9962425e-01 1.2847002e-01
 7.1129710e-03 9.9983704e-01 9.9892139e-01 1.2597640e-04 2.5462408e-03
 9.9344182e-01 9.9382389e-01 9.9703479e-01 9.9651933e-01 1.9075333e-03
 2.4668621e-03 9.8515391e-01 9.9975675e-01 9.8982656e-01 1.7511356e-05
 9.9887723e-01 9.9998045e-01 5.6184918e-02 9.9995875e-01 9.9916255e-01
 9.9986660e-01 9.0610629e-01 9.9660373e-01 2.9153449e-02 7.3065981e-03
 9.9986696e-01 9.9890685e-01 1.5745037e-04 9.9642676e-01 9.9999869e-01
 4.4491586e-01 9.8921669e-01 6.3939407e-03 1.4691963e-02 9.9536926e-01
 9.9990726e-01 1.0258343e-02 2.3057565e-02 8.5821860e-03 9.9956268e-01
 9.9871027e-01 9.9866557e-01 2.4875204e-04 1.5435513e-02 9.8851424e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 22:03:50, Dev, Step : 6260, Loss : 0.46492, Acc : 0.821, Auc : 0.900, Sensitive_Loss : 0.10728, Sensitive_Acc : 16.864, Sensitive_Auc : 0.994, Mean auc: 0.900, Run Time : 92.26 sec
