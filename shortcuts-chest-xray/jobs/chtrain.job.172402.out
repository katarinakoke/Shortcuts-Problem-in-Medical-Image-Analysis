Running on desktop22:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/random_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/random_dataset_val.csv",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.1,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-03-30 07:04:10, Train, Epoch : 1, Step : 10, Loss : 1.22793, Acc : 0.534, Sensitive_Loss : 0.78599, Sensitive_Acc : 14.200, Run Time : 15.03 sec
INFO:root:2024-03-30 07:04:23, Train, Epoch : 1, Step : 20, Loss : 0.93020, Acc : 0.537, Sensitive_Loss : 0.70697, Sensitive_Acc : 15.100, Run Time : 12.79 sec
INFO:root:2024-03-30 07:04:38, Train, Epoch : 1, Step : 30, Loss : 1.09750, Acc : 0.512, Sensitive_Loss : 0.79228, Sensitive_Acc : 15.800, Run Time : 15.37 sec
INFO:root:2024-03-30 07:04:51, Train, Epoch : 1, Step : 40, Loss : 1.47989, Acc : 0.512, Sensitive_Loss : 0.75383, Sensitive_Acc : 16.700, Run Time : 12.43 sec
INFO:root:2024-03-30 07:05:05, Train, Epoch : 1, Step : 50, Loss : 1.27570, Acc : 0.506, Sensitive_Loss : 0.68759, Sensitive_Acc : 17.400, Run Time : 14.16 sec
INFO:root:2024-03-30 07:05:18, Train, Epoch : 1, Step : 60, Loss : 1.12293, Acc : 0.487, Sensitive_Loss : 0.78072, Sensitive_Acc : 17.100, Run Time : 12.97 sec
INFO:root:2024-03-30 07:05:31, Train, Epoch : 1, Step : 70, Loss : 1.45879, Acc : 0.544, Sensitive_Loss : 0.73512, Sensitive_Acc : 16.900, Run Time : 13.04 sec
INFO:root:2024-03-30 07:05:46, Train, Epoch : 1, Step : 80, Loss : 0.82458, Acc : 0.519, Sensitive_Loss : 0.70133, Sensitive_Acc : 16.200, Run Time : 15.31 sec
INFO:root:2024-03-30 07:05:59, Train, Epoch : 1, Step : 90, Loss : 1.25105, Acc : 0.534, Sensitive_Loss : 0.67879, Sensitive_Acc : 16.300, Run Time : 12.73 sec
INFO:root:2024-03-30 07:06:11, Train, Epoch : 1, Step : 100, Loss : 1.31599, Acc : 0.516, Sensitive_Loss : 0.62328, Sensitive_Acc : 16.500, Run Time : 12.39 sec
INFO:root:2024-03-30 07:09:25, Dev, Step : 100, Loss : 1.12667, Acc : 0.727, Auc : 0.648, Sensitive_Loss : 0.69105, Sensitive_Acc : 15.645, Sensitive_Auc : 0.722, Mean auc: 0.648, Run Time : 193.66 sec
INFO:root:2024-03-30 07:09:26, Best, Step : 100, Loss : 1.12667, Acc : 0.727, Auc : 0.648, Sensitive_Loss : 0.69105, Sensitive_Acc : 15.645, Sensitive_Auc : 0.722, Best Auc : 0.648
INFO:root:2024-03-30 07:09:35, Train, Epoch : 1, Step : 110, Loss : 0.89168, Acc : 0.481, Sensitive_Loss : 0.77930, Sensitive_Acc : 16.600, Run Time : 203.57 sec
INFO:root:2024-03-30 07:09:52, Train, Epoch : 1, Step : 120, Loss : 1.25280, Acc : 0.541, Sensitive_Loss : 0.75212, Sensitive_Acc : 16.100, Run Time : 17.09 sec
INFO:root:2024-03-30 07:10:05, Train, Epoch : 1, Step : 130, Loss : 1.25699, Acc : 0.537, Sensitive_Loss : 0.68184, Sensitive_Acc : 17.800, Run Time : 13.25 sec
INFO:root:2024-03-30 07:10:17, Train, Epoch : 1, Step : 140, Loss : 1.03977, Acc : 0.547, Sensitive_Loss : 0.62876, Sensitive_Acc : 17.400, Run Time : 12.36 sec
INFO:root:2024-03-30 07:10:30, Train, Epoch : 1, Step : 150, Loss : 1.28361, Acc : 0.559, Sensitive_Loss : 0.61655, Sensitive_Acc : 15.400, Run Time : 12.59 sec
INFO:root:2024-03-30 07:10:43, Train, Epoch : 1, Step : 160, Loss : 1.03830, Acc : 0.522, Sensitive_Loss : 0.58027, Sensitive_Acc : 15.500, Run Time : 13.45 sec
INFO:root:2024-03-30 07:11:00, Train, Epoch : 1, Step : 170, Loss : 0.95712, Acc : 0.550, Sensitive_Loss : 0.63422, Sensitive_Acc : 17.800, Run Time : 16.51 sec
INFO:root:2024-03-30 07:11:13, Train, Epoch : 1, Step : 180, Loss : 1.25470, Acc : 0.591, Sensitive_Loss : 0.53501, Sensitive_Acc : 14.900, Run Time : 13.24 sec
INFO:root:2024-03-30 07:11:29, Train, Epoch : 1, Step : 190, Loss : 0.86804, Acc : 0.588, Sensitive_Loss : 0.63131, Sensitive_Acc : 14.900, Run Time : 15.66 sec
INFO:root:2024-03-30 07:11:44, Train, Epoch : 1, Step : 200, Loss : 1.15898, Acc : 0.581, Sensitive_Loss : 0.60487, Sensitive_Acc : 15.800, Run Time : 14.94 sec
INFO:root:2024-03-30 07:14:26, Dev, Step : 200, Loss : 1.11450, Acc : 0.528, Auc : 0.672, Sensitive_Loss : 0.58299, Sensitive_Acc : 16.440, Sensitive_Auc : 0.821, Mean auc: 0.672, Run Time : 162.18 sec
INFO:root:2024-03-30 07:14:27, Best, Step : 200, Loss : 1.11450, Acc : 0.528, Auc : 0.672, Sensitive_Loss : 0.58299, Sensitive_Acc : 16.440, Sensitive_Auc : 0.821, Best Auc : 0.672
INFO:root:2024-03-30 07:14:39, Train, Epoch : 1, Step : 210, Loss : 1.17791, Acc : 0.572, Sensitive_Loss : 0.60672, Sensitive_Acc : 19.100, Run Time : 174.70 sec
INFO:root:2024-03-30 07:14:51, Train, Epoch : 1, Step : 220, Loss : 1.07082, Acc : 0.528, Sensitive_Loss : 0.51910, Sensitive_Acc : 17.400, Run Time : 12.52 sec
INFO:root:2024-03-30 07:15:04, Train, Epoch : 1, Step : 230, Loss : 1.21430, Acc : 0.547, Sensitive_Loss : 0.60095, Sensitive_Acc : 14.000, Run Time : 13.21 sec
INFO:root:2024-03-30 07:15:18, Train, Epoch : 1, Step : 240, Loss : 1.05532, Acc : 0.591, Sensitive_Loss : 0.57111, Sensitive_Acc : 15.900, Run Time : 13.35 sec
INFO:root:2024-03-30 07:15:30, Train, Epoch : 1, Step : 250, Loss : 0.87876, Acc : 0.566, Sensitive_Loss : 0.55815, Sensitive_Acc : 15.000, Run Time : 12.06 sec
INFO:root:2024-03-30 07:15:43, Train, Epoch : 1, Step : 260, Loss : 1.37654, Acc : 0.566, Sensitive_Loss : 0.56921, Sensitive_Acc : 17.200, Run Time : 12.81 sec
INFO:root:2024-03-30 07:15:55, Train, Epoch : 1, Step : 270, Loss : 0.96369, Acc : 0.569, Sensitive_Loss : 0.50713, Sensitive_Acc : 16.600, Run Time : 12.19 sec
INFO:root:2024-03-30 07:16:06, Train, Epoch : 1, Step : 280, Loss : 1.24818, Acc : 0.572, Sensitive_Loss : 0.54697, Sensitive_Acc : 16.500, Run Time : 11.45 sec
INFO:root:2024-03-30 07:16:20, Train, Epoch : 1, Step : 290, Loss : 1.24283, Acc : 0.553, Sensitive_Loss : 0.46968, Sensitive_Acc : 13.300, Run Time : 14.09 sec
INFO:root:2024-03-30 07:16:37, Train, Epoch : 1, Step : 300, Loss : 1.11989, Acc : 0.600, Sensitive_Loss : 0.51856, Sensitive_Acc : 16.800, Run Time : 16.45 sec
INFO:root:2024-03-30 07:19:57, Dev, Step : 300, Loss : 1.06850, Acc : 0.643, Auc : 0.705, Sensitive_Loss : 0.58837, Sensitive_Acc : 16.000, Sensitive_Auc : 0.868, Mean auc: 0.705, Run Time : 200.34 sec
INFO:root:2024-03-30 07:19:58, Best, Step : 300, Loss : 1.06850, Acc : 0.643, Auc : 0.705, Sensitive_Loss : 0.58837, Sensitive_Acc : 16.000, Sensitive_Auc : 0.868, Best Auc : 0.705
INFO:root:2024-03-30 07:20:08, Train, Epoch : 1, Step : 310, Loss : 0.76407, Acc : 0.600, Sensitive_Loss : 0.50578, Sensitive_Acc : 15.300, Run Time : 211.57 sec
INFO:root:2024-03-30 07:20:27, Train, Epoch : 1, Step : 320, Loss : 1.05652, Acc : 0.644, Sensitive_Loss : 0.54077, Sensitive_Acc : 18.100, Run Time : 18.67 sec
INFO:root:2024-03-30 07:20:44, Train, Epoch : 1, Step : 330, Loss : 1.21545, Acc : 0.600, Sensitive_Loss : 0.45245, Sensitive_Acc : 15.800, Run Time : 17.16 sec
INFO:root:2024-03-30 07:21:01, Train, Epoch : 1, Step : 340, Loss : 1.17951, Acc : 0.578, Sensitive_Loss : 0.43695, Sensitive_Acc : 15.600, Run Time : 16.80 sec
INFO:root:2024-03-30 07:21:18, Train, Epoch : 1, Step : 350, Loss : 1.22121, Acc : 0.534, Sensitive_Loss : 0.54287, Sensitive_Acc : 16.600, Run Time : 16.68 sec
INFO:root:2024-03-30 07:21:37, Train, Epoch : 1, Step : 360, Loss : 1.26057, Acc : 0.575, Sensitive_Loss : 0.54190, Sensitive_Acc : 15.400, Run Time : 19.47 sec
INFO:root:2024-03-30 07:21:53, Train, Epoch : 1, Step : 370, Loss : 1.14837, Acc : 0.603, Sensitive_Loss : 0.54508, Sensitive_Acc : 16.300, Run Time : 16.21 sec
INFO:root:2024-03-30 07:22:10, Train, Epoch : 1, Step : 380, Loss : 1.08461, Acc : 0.594, Sensitive_Loss : 0.51349, Sensitive_Acc : 15.400, Run Time : 16.36 sec
INFO:root:2024-03-30 07:22:26, Train, Epoch : 1, Step : 390, Loss : 1.12291, Acc : 0.547, Sensitive_Loss : 0.54442, Sensitive_Acc : 15.000, Run Time : 16.46 sec
INFO:root:2024-03-30 07:22:43, Train, Epoch : 1, Step : 400, Loss : 1.07568, Acc : 0.637, Sensitive_Loss : 0.48485, Sensitive_Acc : 17.400, Run Time : 16.97 sec
INFO:root:2024-03-30 07:26:10, Dev, Step : 400, Loss : 1.26973, Acc : 0.272, Auc : 0.678, Sensitive_Loss : 0.60943, Sensitive_Acc : 15.631, Sensitive_Auc : 0.910, Mean auc: 0.678, Run Time : 207.22 sec
INFO:root:2024-03-30 07:26:20, Train, Epoch : 1, Step : 410, Loss : 1.28496, Acc : 0.572, Sensitive_Loss : 0.48598, Sensitive_Acc : 17.900, Run Time : 216.63 sec
INFO:root:2024-03-30 07:26:36, Train, Epoch : 1, Step : 420, Loss : 0.88676, Acc : 0.578, Sensitive_Loss : 0.50251, Sensitive_Acc : 15.600, Run Time : 15.88 sec
INFO:root:2024-03-30 07:26:48, Train, Epoch : 1, Step : 430, Loss : 1.33637, Acc : 0.578, Sensitive_Loss : 0.48389, Sensitive_Acc : 18.100, Run Time : 12.74 sec
INFO:root:2024-03-30 07:27:02, Train, Epoch : 1, Step : 440, Loss : 1.12777, Acc : 0.594, Sensitive_Loss : 0.47166, Sensitive_Acc : 16.800, Run Time : 13.64 sec
INFO:root:2024-03-30 07:27:17, Train, Epoch : 1, Step : 450, Loss : 1.05409, Acc : 0.606, Sensitive_Loss : 0.44769, Sensitive_Acc : 15.400, Run Time : 15.49 sec
INFO:root:2024-03-30 07:27:30, Train, Epoch : 1, Step : 460, Loss : 0.91399, Acc : 0.594, Sensitive_Loss : 0.42719, Sensitive_Acc : 14.200, Run Time : 12.68 sec
INFO:root:2024-03-30 07:27:45, Train, Epoch : 1, Step : 470, Loss : 1.09740, Acc : 0.581, Sensitive_Loss : 0.47195, Sensitive_Acc : 16.300, Run Time : 14.81 sec
INFO:root:2024-03-30 07:28:00, Train, Epoch : 1, Step : 480, Loss : 1.25342, Acc : 0.578, Sensitive_Loss : 0.41042, Sensitive_Acc : 14.000, Run Time : 14.86 sec
INFO:root:2024-03-30 07:28:12, Train, Epoch : 1, Step : 490, Loss : 0.99233, Acc : 0.616, Sensitive_Loss : 0.41065, Sensitive_Acc : 15.700, Run Time : 12.08 sec
INFO:root:2024-03-30 07:30:56
INFO:root:y_pred: [0.752493   0.11739349 0.4966995  ... 0.45600367 0.57983786 0.5476606 ]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [1.91279184e-02 5.69566488e-01 7.97192633e-01 2.21085362e-02
 9.05352890e-01 3.75153348e-02 1.83095917e-01 4.59849909e-02
 4.90608998e-03 1.33353844e-01 2.97274917e-01 2.51688901e-03
 6.52843237e-01 5.24537191e-02 8.41735601e-01 8.50879967e-01
 4.33997251e-03 1.66326687e-01 7.72831500e-01 3.36260915e-01
 8.67835991e-03 3.15290466e-02 8.53700221e-01 2.55439103e-01
 5.68842776e-02 9.17979658e-01 7.34780170e-03 7.90990472e-01
 9.37495753e-03 2.47696772e-01 8.67610395e-01 6.61730289e-01
 3.95700429e-03 3.86562711e-03 3.81409228e-01 2.12988686e-02
 6.99113170e-03 2.72032917e-01 8.90099108e-01 4.13152099e-01
 1.39182201e-02 8.96413624e-02 5.61055839e-01 9.60326552e-01
 2.42969006e-01 2.69238085e-01 4.15106444e-03 1.85145780e-01
 8.37738037e-01 5.10049462e-01 5.05966723e-01 2.94374824e-01
 6.83378339e-01 6.97237015e-01 4.84441578e-01 3.89682800e-01
 2.61084408e-01 3.65677290e-02 9.20288146e-01 4.84676287e-02
 2.08082586e-03 2.73302849e-02 2.70679779e-02 7.84066916e-01
 8.33517015e-01 8.64692509e-01 3.93697232e-01 9.46654797e-01
 8.71712804e-01 7.65813470e-01 9.10429776e-01 6.86364155e-03
 3.52966040e-02 3.81492704e-01 4.28592235e-01 5.86887915e-03
 8.85988474e-01 3.79776442e-03 5.48346221e-01 1.28487960e-01
 1.94518656e-01 1.39609082e-02 4.80134338e-02 4.98392403e-01
 6.62243187e-01 1.57277718e-01 7.62874305e-01 1.85324401e-01
 2.20259428e-02 3.78724158e-01 1.52117321e-02 1.59757122e-01
 1.43533871e-02 4.04903293e-03 7.75277913e-01 2.60003388e-01
 6.55662641e-02 2.90285051e-02 4.93188620e-01 6.60662293e-01
 9.50196326e-01 6.06509438e-03 8.14378381e-01 1.86949521e-01
 9.20932353e-01 6.84096515e-01 1.33607134e-01 1.05266728e-01
 2.06545126e-02 1.72224361e-02 8.89927440e-04 1.34605825e-01
 1.07181035e-01 3.25202779e-03 8.37559253e-03 8.24140385e-03
 5.79511523e-01 4.16119059e-04 3.51353958e-02 6.93913409e-03
 4.80530649e-01 5.62798558e-03 9.37077701e-01 1.33469636e-02
 1.73369244e-01 9.84022319e-01 7.43253250e-03 5.55409908e-01
 9.17408407e-01 8.41748476e-01 9.73993810e-05 9.16332722e-01
 6.89643025e-02 9.61025178e-01 5.36147237e-01 4.24923390e-01
 4.13755178e-01 4.12251204e-01 1.05134733e-02 2.65136778e-01
 8.04812491e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-30 07:30:56, Dev, Step : 492, Loss : 1.25474, Acc : 0.381, Auc : 0.718, Sensitive_Loss : 0.43134, Sensitive_Acc : 17.319, Sensitive_Auc : 0.928, Mean auc: 0.718, Run Time : 161.48 sec
INFO:root:2024-03-30 07:30:57, Best, Step : 492, Loss : 1.25474, Acc : 0.381,Auc : 0.718, Best Auc : 0.718, Sensitive_Loss : 0.43134, Sensitive_Acc : 17.319, Sensitive_Auc : 0.928
INFO:root:2024-03-30 07:31:06, Train, Epoch : 2, Step : 500, Loss : 0.76393, Acc : 0.497, Sensitive_Loss : 0.30180, Sensitive_Acc : 10.700, Run Time : 8.55 sec
INFO:root:2024-03-30 07:32:44, Dev, Step : 500, Loss : 1.18859, Acc : 0.419, Auc : 0.720, Sensitive_Loss : 0.39134, Sensitive_Acc : 17.050, Sensitive_Auc : 0.935, Mean auc: 0.720, Run Time : 97.89 sec
INFO:root:2024-03-30 07:32:45, Best, Step : 500, Loss : 1.18859, Acc : 0.419, Auc : 0.720, Sensitive_Loss : 0.39134, Sensitive_Acc : 17.050, Sensitive_Auc : 0.935, Best Auc : 0.720
INFO:root:2024-03-30 07:32:52, Train, Epoch : 2, Step : 510, Loss : 0.90219, Acc : 0.637, Sensitive_Loss : 0.36011, Sensitive_Acc : 15.800, Run Time : 106.01 sec
INFO:root:2024-03-30 07:33:03, Train, Epoch : 2, Step : 520, Loss : 1.00142, Acc : 0.588, Sensitive_Loss : 0.39596, Sensitive_Acc : 14.700, Run Time : 11.25 sec
INFO:root:2024-03-30 07:33:14, Train, Epoch : 2, Step : 530, Loss : 0.92386, Acc : 0.647, Sensitive_Loss : 0.40415, Sensitive_Acc : 16.200, Run Time : 10.93 sec
INFO:root:2024-03-30 07:33:28, Train, Epoch : 2, Step : 540, Loss : 1.08604, Acc : 0.653, Sensitive_Loss : 0.41849, Sensitive_Acc : 14.600, Run Time : 13.64 sec
INFO:root:2024-03-30 07:33:40, Train, Epoch : 2, Step : 550, Loss : 0.94384, Acc : 0.647, Sensitive_Loss : 0.44269, Sensitive_Acc : 12.600, Run Time : 12.21 sec
INFO:root:2024-03-30 07:33:51, Train, Epoch : 2, Step : 560, Loss : 1.06594, Acc : 0.637, Sensitive_Loss : 0.36193, Sensitive_Acc : 18.200, Run Time : 10.66 sec
INFO:root:2024-03-30 07:34:02, Train, Epoch : 2, Step : 570, Loss : 1.16106, Acc : 0.609, Sensitive_Loss : 0.35679, Sensitive_Acc : 15.400, Run Time : 11.10 sec
INFO:root:2024-03-30 07:34:14, Train, Epoch : 2, Step : 580, Loss : 0.91483, Acc : 0.597, Sensitive_Loss : 0.32936, Sensitive_Acc : 16.500, Run Time : 12.30 sec
INFO:root:2024-03-30 07:34:26, Train, Epoch : 2, Step : 590, Loss : 1.24043, Acc : 0.622, Sensitive_Loss : 0.34029, Sensitive_Acc : 16.300, Run Time : 11.44 sec
INFO:root:2024-03-30 07:34:41, Train, Epoch : 2, Step : 600, Loss : 0.95772, Acc : 0.622, Sensitive_Loss : 0.35551, Sensitive_Acc : 17.600, Run Time : 15.75 sec
INFO:root:2024-03-30 07:37:25, Dev, Step : 600, Loss : 1.19574, Acc : 0.860, Auc : 0.677, Sensitive_Loss : 0.53472, Sensitive_Acc : 15.773, Sensitive_Auc : 0.953, Mean auc: 0.677, Run Time : 163.63 sec
INFO:root:2024-03-30 07:37:33, Train, Epoch : 2, Step : 610, Loss : 0.96672, Acc : 0.659, Sensitive_Loss : 0.35237, Sensitive_Acc : 16.500, Run Time : 171.69 sec
INFO:root:2024-03-30 07:37:43, Train, Epoch : 2, Step : 620, Loss : 0.96445, Acc : 0.662, Sensitive_Loss : 0.32607, Sensitive_Acc : 17.200, Run Time : 10.35 sec
INFO:root:2024-03-30 07:37:56, Train, Epoch : 2, Step : 630, Loss : 0.87713, Acc : 0.662, Sensitive_Loss : 0.38026, Sensitive_Acc : 15.400, Run Time : 12.89 sec
INFO:root:2024-03-30 07:38:06, Train, Epoch : 2, Step : 640, Loss : 1.02336, Acc : 0.628, Sensitive_Loss : 0.40790, Sensitive_Acc : 15.600, Run Time : 9.60 sec
INFO:root:2024-03-30 07:38:16, Train, Epoch : 2, Step : 650, Loss : 1.02890, Acc : 0.650, Sensitive_Loss : 0.40501, Sensitive_Acc : 17.700, Run Time : 9.66 sec
INFO:root:2024-03-30 07:38:26, Train, Epoch : 2, Step : 660, Loss : 1.05260, Acc : 0.647, Sensitive_Loss : 0.35419, Sensitive_Acc : 16.100, Run Time : 10.20 sec
INFO:root:2024-03-30 07:38:36, Train, Epoch : 2, Step : 670, Loss : 1.14347, Acc : 0.647, Sensitive_Loss : 0.39391, Sensitive_Acc : 16.000, Run Time : 10.60 sec
INFO:root:2024-03-30 07:38:47, Train, Epoch : 2, Step : 680, Loss : 1.19250, Acc : 0.631, Sensitive_Loss : 0.30692, Sensitive_Acc : 16.100, Run Time : 10.24 sec
INFO:root:2024-03-30 07:38:57, Train, Epoch : 2, Step : 690, Loss : 0.97824, Acc : 0.637, Sensitive_Loss : 0.37145, Sensitive_Acc : 15.600, Run Time : 10.19 sec
INFO:root:2024-03-30 07:39:08, Train, Epoch : 2, Step : 700, Loss : 1.08563, Acc : 0.625, Sensitive_Loss : 0.39051, Sensitive_Acc : 17.700, Run Time : 11.08 sec
INFO:root:2024-03-30 07:41:47, Dev, Step : 700, Loss : 1.06936, Acc : 0.553, Auc : 0.714, Sensitive_Loss : 0.43378, Sensitive_Acc : 16.426, Sensitive_Auc : 0.971, Mean auc: 0.714, Run Time : 158.76 sec
INFO:root:2024-03-30 07:41:54, Train, Epoch : 2, Step : 710, Loss : 0.84950, Acc : 0.653, Sensitive_Loss : 0.30782, Sensitive_Acc : 15.600, Run Time : 166.04 sec
INFO:root:2024-03-30 07:42:05, Train, Epoch : 2, Step : 720, Loss : 0.85182, Acc : 0.641, Sensitive_Loss : 0.33431, Sensitive_Acc : 15.600, Run Time : 10.92 sec
INFO:root:2024-03-30 07:42:14, Train, Epoch : 2, Step : 730, Loss : 0.90848, Acc : 0.622, Sensitive_Loss : 0.36256, Sensitive_Acc : 16.500, Run Time : 9.56 sec
INFO:root:2024-03-30 07:42:24, Train, Epoch : 2, Step : 740, Loss : 1.11166, Acc : 0.625, Sensitive_Loss : 0.31820, Sensitive_Acc : 15.700, Run Time : 9.96 sec
INFO:root:2024-03-30 07:42:35, Train, Epoch : 2, Step : 750, Loss : 1.20630, Acc : 0.631, Sensitive_Loss : 0.38673, Sensitive_Acc : 18.700, Run Time : 10.89 sec
INFO:root:2024-03-30 07:42:45, Train, Epoch : 2, Step : 760, Loss : 1.02138, Acc : 0.678, Sensitive_Loss : 0.36273, Sensitive_Acc : 18.100, Run Time : 9.69 sec
INFO:root:2024-03-30 07:42:55, Train, Epoch : 2, Step : 770, Loss : 1.11578, Acc : 0.656, Sensitive_Loss : 0.34383, Sensitive_Acc : 18.200, Run Time : 9.54 sec
INFO:root:2024-03-30 07:43:05, Train, Epoch : 2, Step : 780, Loss : 1.05414, Acc : 0.641, Sensitive_Loss : 0.37430, Sensitive_Acc : 16.000, Run Time : 10.95 sec
INFO:root:2024-03-30 07:43:16, Train, Epoch : 2, Step : 790, Loss : 1.06988, Acc : 0.644, Sensitive_Loss : 0.36323, Sensitive_Acc : 16.300, Run Time : 10.80 sec
INFO:root:2024-03-30 07:43:26, Train, Epoch : 2, Step : 800, Loss : 1.09351, Acc : 0.656, Sensitive_Loss : 0.31907, Sensitive_Acc : 17.900, Run Time : 10.16 sec
INFO:root:2024-03-30 07:46:02, Dev, Step : 800, Loss : 1.07120, Acc : 0.618, Auc : 0.718, Sensitive_Loss : 0.31146, Sensitive_Acc : 17.050, Sensitive_Auc : 0.966, Mean auc: 0.718, Run Time : 155.98 sec
INFO:root:2024-03-30 07:46:10, Train, Epoch : 2, Step : 810, Loss : 1.30990, Acc : 0.644, Sensitive_Loss : 0.37387, Sensitive_Acc : 16.400, Run Time : 163.93 sec
INFO:root:2024-03-30 07:46:20, Train, Epoch : 2, Step : 820, Loss : 1.08848, Acc : 0.628, Sensitive_Loss : 0.30082, Sensitive_Acc : 16.200, Run Time : 10.05 sec
INFO:root:2024-03-30 07:46:31, Train, Epoch : 2, Step : 830, Loss : 1.13503, Acc : 0.675, Sensitive_Loss : 0.40178, Sensitive_Acc : 16.100, Run Time : 10.16 sec
INFO:root:2024-03-30 07:46:44, Train, Epoch : 2, Step : 840, Loss : 1.27622, Acc : 0.653, Sensitive_Loss : 0.35110, Sensitive_Acc : 15.100, Run Time : 13.09 sec
INFO:root:2024-03-30 07:46:55, Train, Epoch : 2, Step : 850, Loss : 0.96333, Acc : 0.659, Sensitive_Loss : 0.38404, Sensitive_Acc : 18.500, Run Time : 11.17 sec
INFO:root:2024-03-30 07:47:05, Train, Epoch : 2, Step : 860, Loss : 1.02034, Acc : 0.678, Sensitive_Loss : 0.33934, Sensitive_Acc : 17.300, Run Time : 9.79 sec
INFO:root:2024-03-30 07:47:15, Train, Epoch : 2, Step : 870, Loss : 1.11722, Acc : 0.662, Sensitive_Loss : 0.29899, Sensitive_Acc : 16.900, Run Time : 9.91 sec
INFO:root:2024-03-30 07:47:25, Train, Epoch : 2, Step : 880, Loss : 1.12040, Acc : 0.584, Sensitive_Loss : 0.40994, Sensitive_Acc : 17.000, Run Time : 10.17 sec
INFO:root:2024-03-30 07:47:34, Train, Epoch : 2, Step : 890, Loss : 1.10435, Acc : 0.666, Sensitive_Loss : 0.30709, Sensitive_Acc : 17.900, Run Time : 9.65 sec
INFO:root:2024-03-30 07:47:44, Train, Epoch : 2, Step : 900, Loss : 0.74225, Acc : 0.684, Sensitive_Loss : 0.37292, Sensitive_Acc : 17.000, Run Time : 9.75 sec
INFO:root:2024-03-30 07:50:18, Dev, Step : 900, Loss : 1.01708, Acc : 0.599, Auc : 0.745, Sensitive_Loss : 0.33271, Sensitive_Acc : 16.624, Sensitive_Auc : 0.987, Mean auc: 0.745, Run Time : 153.79 sec
INFO:root:2024-03-30 07:50:19, Best, Step : 900, Loss : 1.01708, Acc : 0.599, Auc : 0.745, Sensitive_Loss : 0.33271, Sensitive_Acc : 16.624, Sensitive_Auc : 0.987, Best Auc : 0.745
INFO:root:2024-03-30 07:50:25, Train, Epoch : 2, Step : 910, Loss : 1.13727, Acc : 0.603, Sensitive_Loss : 0.29647, Sensitive_Acc : 15.800, Run Time : 161.17 sec
INFO:root:2024-03-30 07:50:35, Train, Epoch : 2, Step : 920, Loss : 0.87194, Acc : 0.650, Sensitive_Loss : 0.33945, Sensitive_Acc : 17.300, Run Time : 10.05 sec
INFO:root:2024-03-30 07:50:46, Train, Epoch : 2, Step : 930, Loss : 0.96370, Acc : 0.669, Sensitive_Loss : 0.24451, Sensitive_Acc : 18.400, Run Time : 10.97 sec
INFO:root:2024-03-30 07:51:00, Train, Epoch : 2, Step : 940, Loss : 0.99832, Acc : 0.641, Sensitive_Loss : 0.28898, Sensitive_Acc : 14.100, Run Time : 13.55 sec
INFO:root:2024-03-30 07:51:09, Train, Epoch : 2, Step : 950, Loss : 0.96576, Acc : 0.669, Sensitive_Loss : 0.26553, Sensitive_Acc : 17.300, Run Time : 9.46 sec
INFO:root:2024-03-30 07:51:19, Train, Epoch : 2, Step : 960, Loss : 0.97066, Acc : 0.669, Sensitive_Loss : 0.27382, Sensitive_Acc : 17.700, Run Time : 10.12 sec
INFO:root:2024-03-30 07:51:32, Train, Epoch : 2, Step : 970, Loss : 0.98620, Acc : 0.678, Sensitive_Loss : 0.27787, Sensitive_Acc : 18.300, Run Time : 12.98 sec
INFO:root:2024-03-30 07:51:42, Train, Epoch : 2, Step : 980, Loss : 0.94341, Acc : 0.625, Sensitive_Loss : 0.28763, Sensitive_Acc : 15.600, Run Time : 9.66 sec
INFO:root:2024-03-30 07:54:19
INFO:root:y_pred: [0.6061549  0.53121185 0.3857071  ... 0.9269189  0.99050796 0.31059164]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [2.59838370e-03 1.97588831e-01 9.13803995e-01 1.37218535e-01
 9.90705073e-01 5.38919680e-03 4.16273251e-02 1.56355295e-02
 2.77485810e-02 1.02898695e-01 1.23549215e-01 2.20977552e-02
 4.64027315e-01 7.70384399e-03 8.82461548e-01 7.70882189e-01
 5.23798401e-04 3.09938975e-02 9.46583509e-01 3.51529926e-01
 4.51300964e-02 1.81155968e-02 9.99683261e-01 4.50627714e-01
 2.54871864e-02 7.59986520e-01 1.47078268e-03 9.94536757e-01
 2.66226963e-03 7.34915137e-01 9.97743487e-01 8.17806780e-01
 2.41713156e-03 2.10993784e-03 4.36647058e-01 8.16013385e-03
 1.73856243e-02 8.09199288e-02 7.50884950e-01 6.57923579e-01
 1.67567749e-02 2.29994184e-03 9.11427081e-01 9.89782989e-01
 3.17592382e-01 3.58974822e-02 1.24245873e-02 1.60067856e-01
 9.96756494e-01 9.14186656e-01 9.48695958e-01 3.92192721e-01
 9.70242500e-01 9.39934075e-01 3.71757954e-01 5.70473731e-01
 5.74391365e-01 1.82856634e-01 9.98532534e-01 3.28314491e-02
 8.15539679e-04 3.28524292e-01 6.39515460e-01 9.81839716e-01
 8.49248886e-01 9.96917605e-01 4.17738467e-01 9.99523520e-01
 8.06658685e-01 8.09772193e-01 9.98101890e-01 1.39543330e-02
 2.71589565e-03 3.95110935e-01 7.81255245e-01 6.50677225e-03
 9.71657991e-01 3.79354198e-04 9.97146547e-01 3.03900868e-01
 2.80606747e-01 1.09025696e-02 2.25637585e-01 5.15124917e-01
 6.26276433e-01 7.14552104e-02 9.06548023e-01 2.77824342e-01
 4.32693074e-03 4.73206565e-02 6.66666683e-03 4.04057130e-02
 1.15480712e-02 6.50446396e-03 9.94053662e-01 5.69258869e-01
 2.33298197e-01 6.83556357e-03 8.70180845e-01 6.30502045e-01
 9.02142406e-01 1.23340441e-02 9.89267886e-01 6.00042529e-02
 9.92899016e-02 8.63411546e-01 4.35180664e-02 1.10190306e-02
 3.19760549e-03 8.26753862e-03 2.69499072e-03 5.45610011e-01
 2.07546860e-01 1.78152812e-03 2.39660054e-01 6.12514373e-03
 9.68556404e-01 6.02971762e-03 8.19708332e-02 6.22320315e-03
 2.43621767e-01 3.28977429e-03 7.98727632e-01 4.82729962e-03
 1.22832619e-02 9.97909069e-01 9.34990775e-03 8.78654540e-01
 5.42057514e-01 9.14686263e-01 4.88527512e-05 9.82320786e-01
 1.15281232e-02 9.98590291e-01 9.97343004e-01 7.79171363e-02
 5.80882192e-01 6.92288131e-02 2.55455915e-02 8.07084799e-01
 8.17554653e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-30 07:54:19, Dev, Step : 984, Loss : 1.08786, Acc : 0.565, Auc : 0.726, Sensitive_Loss : 0.29445, Sensitive_Acc : 17.206, Sensitive_Auc : 0.988, Mean auc: 0.726, Run Time : 152.93 sec
INFO:root:2024-03-30 07:54:29, Train, Epoch : 3, Step : 990, Loss : 0.54715, Acc : 0.444, Sensitive_Loss : 0.18820, Sensitive_Acc : 9.500, Run Time : 8.86 sec
INFO:root:2024-03-30 07:54:41, Train, Epoch : 3, Step : 1000, Loss : 0.82724, Acc : 0.728, Sensitive_Loss : 0.31208, Sensitive_Acc : 18.600, Run Time : 12.42 sec
INFO:root:2024-03-30 07:56:22, Dev, Step : 1000, Loss : 1.03178, Acc : 0.630, Auc : 0.739, Sensitive_Loss : 0.29055, Sensitive_Acc : 16.809, Sensitive_Auc : 0.988, Mean auc: 0.739, Run Time : 100.97 sec
INFO:root:2024-03-30 07:56:30, Train, Epoch : 3, Step : 1010, Loss : 0.74540, Acc : 0.747, Sensitive_Loss : 0.32381, Sensitive_Acc : 15.100, Run Time : 108.74 sec
INFO:root:2024-03-30 07:56:42, Train, Epoch : 3, Step : 1020, Loss : 0.95944, Acc : 0.662, Sensitive_Loss : 0.29828, Sensitive_Acc : 16.900, Run Time : 11.90 sec
INFO:root:2024-03-30 07:56:55, Train, Epoch : 3, Step : 1030, Loss : 1.12423, Acc : 0.681, Sensitive_Loss : 0.30368, Sensitive_Acc : 17.700, Run Time : 13.27 sec
INFO:root:2024-03-30 07:57:07, Train, Epoch : 3, Step : 1040, Loss : 0.94871, Acc : 0.694, Sensitive_Loss : 0.30694, Sensitive_Acc : 16.200, Run Time : 11.89 sec
INFO:root:2024-03-30 07:57:23, Train, Epoch : 3, Step : 1050, Loss : 1.10300, Acc : 0.703, Sensitive_Loss : 0.29436, Sensitive_Acc : 16.400, Run Time : 16.02 sec
INFO:root:2024-03-30 07:57:34, Train, Epoch : 3, Step : 1060, Loss : 0.80933, Acc : 0.688, Sensitive_Loss : 0.27430, Sensitive_Acc : 15.800, Run Time : 11.35 sec
INFO:root:2024-03-30 07:57:46, Train, Epoch : 3, Step : 1070, Loss : 0.79402, Acc : 0.672, Sensitive_Loss : 0.25470, Sensitive_Acc : 16.600, Run Time : 11.56 sec
INFO:root:2024-03-30 07:57:58, Train, Epoch : 3, Step : 1080, Loss : 1.01676, Acc : 0.694, Sensitive_Loss : 0.28209, Sensitive_Acc : 17.000, Run Time : 11.94 sec
INFO:root:2024-03-30 07:58:09, Train, Epoch : 3, Step : 1090, Loss : 1.08232, Acc : 0.678, Sensitive_Loss : 0.29397, Sensitive_Acc : 16.800, Run Time : 11.03 sec
INFO:root:2024-03-30 07:58:20, Train, Epoch : 3, Step : 1100, Loss : 1.01667, Acc : 0.722, Sensitive_Loss : 0.24434, Sensitive_Acc : 17.300, Run Time : 11.38 sec
INFO:root:2024-03-30 08:01:24, Dev, Step : 1100, Loss : 0.96949, Acc : 0.675, Auc : 0.766, Sensitive_Loss : 0.27936, Sensitive_Acc : 16.837, Sensitive_Auc : 0.987, Mean auc: 0.766, Run Time : 184.18 sec
INFO:root:2024-03-30 08:01:25, Best, Step : 1100, Loss : 0.96949, Acc : 0.675, Auc : 0.766, Sensitive_Loss : 0.27936, Sensitive_Acc : 16.837, Sensitive_Auc : 0.987, Best Auc : 0.766
INFO:root:2024-03-30 08:01:37, Train, Epoch : 3, Step : 1110, Loss : 1.02868, Acc : 0.666, Sensitive_Loss : 0.23168, Sensitive_Acc : 16.300, Run Time : 196.72 sec
INFO:root:2024-03-30 08:01:50, Train, Epoch : 3, Step : 1120, Loss : 0.81122, Acc : 0.738, Sensitive_Loss : 0.30860, Sensitive_Acc : 14.600, Run Time : 12.54 sec
INFO:root:2024-03-30 08:02:02, Train, Epoch : 3, Step : 1130, Loss : 0.77284, Acc : 0.662, Sensitive_Loss : 0.29238, Sensitive_Acc : 17.200, Run Time : 12.45 sec
INFO:root:2024-03-30 08:02:15, Train, Epoch : 3, Step : 1140, Loss : 0.63878, Acc : 0.709, Sensitive_Loss : 0.22157, Sensitive_Acc : 14.400, Run Time : 12.72 sec
INFO:root:2024-03-30 08:02:26, Train, Epoch : 3, Step : 1150, Loss : 0.94815, Acc : 0.706, Sensitive_Loss : 0.29289, Sensitive_Acc : 16.700, Run Time : 11.24 sec
INFO:root:2024-03-30 08:02:38, Train, Epoch : 3, Step : 1160, Loss : 0.91671, Acc : 0.716, Sensitive_Loss : 0.26012, Sensitive_Acc : 16.600, Run Time : 11.77 sec
INFO:root:2024-03-30 08:02:50, Train, Epoch : 3, Step : 1170, Loss : 0.84198, Acc : 0.703, Sensitive_Loss : 0.23741, Sensitive_Acc : 17.300, Run Time : 12.27 sec
INFO:root:2024-03-30 08:03:01, Train, Epoch : 3, Step : 1180, Loss : 0.81982, Acc : 0.678, Sensitive_Loss : 0.28009, Sensitive_Acc : 13.000, Run Time : 11.34 sec
INFO:root:2024-03-30 08:03:15, Train, Epoch : 3, Step : 1190, Loss : 0.97391, Acc : 0.719, Sensitive_Loss : 0.26487, Sensitive_Acc : 15.800, Run Time : 13.60 sec
INFO:root:2024-03-30 08:03:27, Train, Epoch : 3, Step : 1200, Loss : 0.85092, Acc : 0.728, Sensitive_Loss : 0.22314, Sensitive_Acc : 16.100, Run Time : 11.79 sec
INFO:root:2024-03-30 08:05:43, Dev, Step : 1200, Loss : 0.95393, Acc : 0.717, Auc : 0.771, Sensitive_Loss : 0.27915, Sensitive_Acc : 16.851, Sensitive_Auc : 0.990, Mean auc: 0.771, Run Time : 136.21 sec
INFO:root:2024-03-30 08:05:44, Best, Step : 1200, Loss : 0.95393, Acc : 0.717, Auc : 0.771, Sensitive_Loss : 0.27915, Sensitive_Acc : 16.851, Sensitive_Auc : 0.990, Best Auc : 0.771
INFO:root:2024-03-30 08:05:51, Train, Epoch : 3, Step : 1210, Loss : 0.96795, Acc : 0.681, Sensitive_Loss : 0.33212, Sensitive_Acc : 15.600, Run Time : 144.78 sec
INFO:root:2024-03-30 08:06:03, Train, Epoch : 3, Step : 1220, Loss : 0.79348, Acc : 0.719, Sensitive_Loss : 0.30431, Sensitive_Acc : 16.300, Run Time : 12.01 sec
INFO:root:2024-03-30 08:06:24, Train, Epoch : 3, Step : 1230, Loss : 0.90855, Acc : 0.688, Sensitive_Loss : 0.20808, Sensitive_Acc : 18.100, Run Time : 20.43 sec
INFO:root:2024-03-30 08:06:36, Train, Epoch : 3, Step : 1240, Loss : 0.93550, Acc : 0.656, Sensitive_Loss : 0.24753, Sensitive_Acc : 16.500, Run Time : 11.92 sec
INFO:root:2024-03-30 08:06:47, Train, Epoch : 3, Step : 1250, Loss : 0.82666, Acc : 0.694, Sensitive_Loss : 0.29247, Sensitive_Acc : 16.600, Run Time : 11.57 sec
INFO:root:2024-03-30 08:07:04, Train, Epoch : 3, Step : 1260, Loss : 0.84234, Acc : 0.709, Sensitive_Loss : 0.22166, Sensitive_Acc : 16.800, Run Time : 16.25 sec
INFO:root:2024-03-30 08:07:16, Train, Epoch : 3, Step : 1270, Loss : 1.00670, Acc : 0.669, Sensitive_Loss : 0.34461, Sensitive_Acc : 16.200, Run Time : 12.49 sec
INFO:root:2024-03-30 08:07:31, Train, Epoch : 3, Step : 1280, Loss : 0.92196, Acc : 0.672, Sensitive_Loss : 0.27603, Sensitive_Acc : 19.100, Run Time : 14.62 sec
INFO:root:2024-03-30 08:07:45, Train, Epoch : 3, Step : 1290, Loss : 1.00849, Acc : 0.688, Sensitive_Loss : 0.24966, Sensitive_Acc : 17.700, Run Time : 13.91 sec
INFO:root:2024-03-30 08:07:58, Train, Epoch : 3, Step : 1300, Loss : 0.81760, Acc : 0.700, Sensitive_Loss : 0.23949, Sensitive_Acc : 16.200, Run Time : 12.98 sec
INFO:root:2024-03-30 08:10:24, Dev, Step : 1300, Loss : 0.95405, Acc : 0.674, Auc : 0.777, Sensitive_Loss : 0.26015, Sensitive_Acc : 17.021, Sensitive_Auc : 0.989, Mean auc: 0.777, Run Time : 146.46 sec
INFO:root:2024-03-30 08:10:25, Best, Step : 1300, Loss : 0.95405, Acc : 0.674, Auc : 0.777, Sensitive_Loss : 0.26015, Sensitive_Acc : 17.021, Sensitive_Auc : 0.989, Best Auc : 0.777
INFO:root:2024-03-30 08:10:32, Train, Epoch : 3, Step : 1310, Loss : 0.81421, Acc : 0.703, Sensitive_Loss : 0.22039, Sensitive_Acc : 17.000, Run Time : 154.80 sec
INFO:root:2024-03-30 08:10:47, Train, Epoch : 3, Step : 1320, Loss : 0.74001, Acc : 0.700, Sensitive_Loss : 0.24851, Sensitive_Acc : 17.500, Run Time : 14.30 sec
INFO:root:2024-03-30 08:10:58, Train, Epoch : 3, Step : 1330, Loss : 0.88781, Acc : 0.697, Sensitive_Loss : 0.25638, Sensitive_Acc : 17.900, Run Time : 11.34 sec
INFO:root:2024-03-30 08:11:10, Train, Epoch : 3, Step : 1340, Loss : 0.80120, Acc : 0.738, Sensitive_Loss : 0.30089, Sensitive_Acc : 18.600, Run Time : 12.26 sec
INFO:root:2024-03-30 08:11:24, Train, Epoch : 3, Step : 1350, Loss : 0.73070, Acc : 0.709, Sensitive_Loss : 0.22675, Sensitive_Acc : 16.500, Run Time : 13.21 sec
INFO:root:2024-03-30 08:11:35, Train, Epoch : 3, Step : 1360, Loss : 0.83736, Acc : 0.716, Sensitive_Loss : 0.26016, Sensitive_Acc : 17.100, Run Time : 11.54 sec
INFO:root:2024-03-30 08:11:50, Train, Epoch : 3, Step : 1370, Loss : 0.77113, Acc : 0.691, Sensitive_Loss : 0.25494, Sensitive_Acc : 16.100, Run Time : 15.02 sec
INFO:root:2024-03-30 08:12:02, Train, Epoch : 3, Step : 1380, Loss : 1.00131, Acc : 0.678, Sensitive_Loss : 0.27730, Sensitive_Acc : 17.900, Run Time : 11.86 sec
INFO:root:2024-03-30 08:12:14, Train, Epoch : 3, Step : 1390, Loss : 0.97983, Acc : 0.700, Sensitive_Loss : 0.20030, Sensitive_Acc : 17.200, Run Time : 12.40 sec
INFO:root:2024-03-30 08:12:28, Train, Epoch : 3, Step : 1400, Loss : 0.87353, Acc : 0.725, Sensitive_Loss : 0.26282, Sensitive_Acc : 14.500, Run Time : 13.48 sec
INFO:root:2024-03-30 08:15:09, Dev, Step : 1400, Loss : 0.93817, Acc : 0.726, Auc : 0.781, Sensitive_Loss : 0.26997, Sensitive_Acc : 16.879, Sensitive_Auc : 0.989, Mean auc: 0.781, Run Time : 161.47 sec
INFO:root:2024-03-30 08:15:10, Best, Step : 1400, Loss : 0.93817, Acc : 0.726, Auc : 0.781, Sensitive_Loss : 0.26997, Sensitive_Acc : 16.879, Sensitive_Auc : 0.989, Best Auc : 0.781
INFO:root:2024-03-30 08:15:18, Train, Epoch : 3, Step : 1410, Loss : 0.96328, Acc : 0.697, Sensitive_Loss : 0.26264, Sensitive_Acc : 15.500, Run Time : 170.00 sec
INFO:root:2024-03-30 08:15:30, Train, Epoch : 3, Step : 1420, Loss : 0.91999, Acc : 0.734, Sensitive_Loss : 0.21549, Sensitive_Acc : 17.400, Run Time : 11.66 sec
INFO:root:2024-03-30 08:15:41, Train, Epoch : 3, Step : 1430, Loss : 0.79873, Acc : 0.694, Sensitive_Loss : 0.35440, Sensitive_Acc : 16.800, Run Time : 11.69 sec
INFO:root:2024-03-30 08:15:57, Train, Epoch : 3, Step : 1440, Loss : 0.73702, Acc : 0.700, Sensitive_Loss : 0.23409, Sensitive_Acc : 17.400, Run Time : 16.07 sec
INFO:root:2024-03-30 08:16:09, Train, Epoch : 3, Step : 1450, Loss : 0.87493, Acc : 0.672, Sensitive_Loss : 0.28572, Sensitive_Acc : 15.500, Run Time : 11.50 sec
INFO:root:2024-03-30 08:16:21, Train, Epoch : 3, Step : 1460, Loss : 0.83338, Acc : 0.675, Sensitive_Loss : 0.25073, Sensitive_Acc : 15.500, Run Time : 12.52 sec
INFO:root:2024-03-30 08:16:36, Train, Epoch : 3, Step : 1470, Loss : 0.80293, Acc : 0.713, Sensitive_Loss : 0.26888, Sensitive_Acc : 16.700, Run Time : 14.28 sec
INFO:root:2024-03-30 08:18:57
INFO:root:y_pred: [0.22924095 0.11879564 0.35264423 ... 0.62924355 0.5553557  0.20485632]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [6.37103431e-03 2.23830536e-01 9.68889892e-01 9.74184796e-02
 9.94601727e-01 1.03635835e-02 2.04952031e-01 4.69904207e-02
 4.02410142e-02 9.48416218e-02 9.02588665e-02 2.32676975e-02
 9.54824507e-01 5.77134453e-03 9.86222327e-01 8.11834097e-01
 8.69175245e-04 2.57799089e-01 9.97909129e-01 5.49278975e-01
 1.84497222e-01 3.76004614e-02 9.99666452e-01 8.87346029e-01
 4.66313735e-02 7.44764090e-01 9.19299945e-03 9.99052346e-01
 1.95235983e-02 7.97042727e-01 9.99216914e-01 9.67890561e-01
 1.27632013e-02 9.81217716e-04 3.00366729e-01 2.55311038e-02
 2.58137472e-02 1.29681826e-01 9.50342298e-01 9.32276368e-01
 2.94057988e-02 1.23442886e-02 9.84970987e-01 9.98291314e-01
 3.16391736e-01 1.90316699e-02 7.09132776e-02 2.34238252e-01
 9.98910189e-01 6.71674311e-01 9.91682112e-01 5.21995842e-01
 9.94789004e-01 9.78564501e-01 5.51399827e-01 8.86347473e-01
 7.40536332e-01 4.56746332e-02 9.99868751e-01 7.85145983e-02
 1.78928778e-03 6.99420094e-01 8.82873476e-01 9.70061421e-01
 9.34128881e-01 9.99287784e-01 5.37591219e-01 9.99377191e-01
 9.74836349e-01 9.91684556e-01 9.94868875e-01 1.27505716e-02
 6.35706447e-03 6.84586704e-01 9.11160588e-01 1.83072966e-02
 9.86202955e-01 1.00922049e-03 9.93567765e-01 4.91036355e-01
 6.40193701e-01 4.44285050e-02 1.89695448e-01 5.41153550e-01
 9.25613880e-01 3.02523494e-01 9.78743970e-01 2.53051251e-01
 5.38980262e-03 5.24697006e-02 2.93379035e-02 1.72172949e-01
 1.69923101e-02 1.64260361e-02 9.96596158e-01 7.69641697e-01
 2.27188319e-01 1.77999362e-02 9.18438971e-01 9.20666814e-01
 9.79762495e-01 2.47290432e-02 9.89350259e-01 4.24809158e-02
 1.60546705e-01 9.69182491e-01 1.12592235e-01 1.70353577e-02
 6.47074450e-03 1.33139687e-02 3.25147435e-03 8.75377059e-01
 2.94576734e-01 1.21629490e-02 4.26756263e-01 1.42351422e-03
 9.94767308e-01 9.77079291e-03 3.82165670e-01 3.00505031e-02
 6.91813290e-01 1.49481036e-02 9.81067181e-01 1.03816940e-02
 3.71309370e-02 9.98740017e-01 3.01821232e-02 9.43113923e-01
 8.51907432e-01 9.81662095e-01 1.65621022e-04 9.96543825e-01
 1.11975269e-02 9.99357522e-01 9.91597116e-01 2.31319472e-01
 6.06059194e-01 2.22998410e-01 9.07884073e-03 9.87434924e-01
 9.52589512e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-30 08:18:57, Dev, Step : 1476, Loss : 0.93278, Acc : 0.782, Auc : 0.784, Sensitive_Loss : 0.27380, Sensitive_Acc : 16.908, Sensitive_Auc : 0.991, Mean auc: 0.784, Run Time : 134.04 sec
INFO:root:2024-03-30 08:18:58, Best, Step : 1476, Loss : 0.93278, Acc : 0.782,Auc : 0.784, Best Auc : 0.784, Sensitive_Loss : 0.27380, Sensitive_Acc : 16.908, Sensitive_Auc : 0.991
INFO:root:2024-03-30 08:19:03, Train, Epoch : 4, Step : 1480, Loss : 0.27770, Acc : 0.287, Sensitive_Loss : 0.12671, Sensitive_Acc : 6.900, Run Time : 4.76 sec
INFO:root:2024-03-30 08:19:14, Train, Epoch : 4, Step : 1490, Loss : 0.87267, Acc : 0.694, Sensitive_Loss : 0.21686, Sensitive_Acc : 15.600, Run Time : 10.76 sec
INFO:root:2024-03-30 08:19:27, Train, Epoch : 4, Step : 1500, Loss : 0.76961, Acc : 0.738, Sensitive_Loss : 0.29990, Sensitive_Acc : 17.400, Run Time : 12.80 sec
INFO:root:2024-03-30 08:21:03, Dev, Step : 1500, Loss : 0.92909, Acc : 0.768, Auc : 0.784, Sensitive_Loss : 0.28780, Sensitive_Acc : 16.766, Sensitive_Auc : 0.991, Mean auc: 0.784, Run Time : 96.62 sec
INFO:root:2024-03-30 08:21:11, Train, Epoch : 4, Step : 1510, Loss : 0.99665, Acc : 0.716, Sensitive_Loss : 0.24603, Sensitive_Acc : 18.600, Run Time : 104.11 sec
INFO:root:2024-03-30 08:21:22, Train, Epoch : 4, Step : 1520, Loss : 0.79289, Acc : 0.731, Sensitive_Loss : 0.29691, Sensitive_Acc : 17.600, Run Time : 11.41 sec
INFO:root:2024-03-30 08:21:34, Train, Epoch : 4, Step : 1530, Loss : 0.81730, Acc : 0.706, Sensitive_Loss : 0.26052, Sensitive_Acc : 17.700, Run Time : 11.86 sec
INFO:root:2024-03-30 08:21:47, Train, Epoch : 4, Step : 1540, Loss : 0.80308, Acc : 0.700, Sensitive_Loss : 0.24892, Sensitive_Acc : 15.700, Run Time : 12.93 sec
INFO:root:2024-03-30 08:21:58, Train, Epoch : 4, Step : 1550, Loss : 0.84893, Acc : 0.728, Sensitive_Loss : 0.27858, Sensitive_Acc : 15.500, Run Time : 11.23 sec
INFO:root:2024-03-30 08:22:10, Train, Epoch : 4, Step : 1560, Loss : 0.84454, Acc : 0.666, Sensitive_Loss : 0.20420, Sensitive_Acc : 18.100, Run Time : 11.61 sec
INFO:root:2024-03-30 08:22:27, Train, Epoch : 4, Step : 1570, Loss : 0.81929, Acc : 0.722, Sensitive_Loss : 0.25016, Sensitive_Acc : 16.700, Run Time : 17.06 sec
INFO:root:2024-03-30 08:22:38, Train, Epoch : 4, Step : 1580, Loss : 0.97713, Acc : 0.728, Sensitive_Loss : 0.25437, Sensitive_Acc : 17.000, Run Time : 10.91 sec
INFO:root:2024-03-30 08:22:52, Train, Epoch : 4, Step : 1590, Loss : 0.90744, Acc : 0.716, Sensitive_Loss : 0.17968, Sensitive_Acc : 15.500, Run Time : 13.87 sec
INFO:root:2024-03-30 08:23:03, Train, Epoch : 4, Step : 1600, Loss : 0.91193, Acc : 0.728, Sensitive_Loss : 0.26499, Sensitive_Acc : 13.700, Run Time : 11.39 sec
INFO:root:2024-03-30 08:24:42, Dev, Step : 1600, Loss : 0.93936, Acc : 0.766, Auc : 0.778, Sensitive_Loss : 0.26983, Sensitive_Acc : 16.865, Sensitive_Auc : 0.989, Mean auc: 0.778, Run Time : 98.62 sec
INFO:root:2024-03-30 08:24:50, Train, Epoch : 4, Step : 1610, Loss : 0.88267, Acc : 0.709, Sensitive_Loss : 0.27157, Sensitive_Acc : 16.800, Run Time : 106.42 sec
INFO:root:2024-03-30 08:25:01, Train, Epoch : 4, Step : 1620, Loss : 0.88223, Acc : 0.719, Sensitive_Loss : 0.22832, Sensitive_Acc : 17.300, Run Time : 11.56 sec
INFO:root:2024-03-30 08:25:13, Train, Epoch : 4, Step : 1630, Loss : 1.06259, Acc : 0.703, Sensitive_Loss : 0.31820, Sensitive_Acc : 16.800, Run Time : 11.36 sec
INFO:root:2024-03-30 08:25:24, Train, Epoch : 4, Step : 1640, Loss : 0.81026, Acc : 0.725, Sensitive_Loss : 0.25649, Sensitive_Acc : 13.600, Run Time : 11.14 sec
INFO:root:2024-03-30 08:25:35, Train, Epoch : 4, Step : 1650, Loss : 1.04434, Acc : 0.678, Sensitive_Loss : 0.25677, Sensitive_Acc : 16.600, Run Time : 11.19 sec
INFO:root:2024-03-30 08:25:49, Train, Epoch : 4, Step : 1660, Loss : 0.84148, Acc : 0.684, Sensitive_Loss : 0.28601, Sensitive_Acc : 14.500, Run Time : 14.52 sec
INFO:root:2024-03-30 08:26:02, Train, Epoch : 4, Step : 1670, Loss : 0.75651, Acc : 0.731, Sensitive_Loss : 0.32996, Sensitive_Acc : 15.800, Run Time : 12.34 sec
INFO:root:2024-03-30 08:26:13, Train, Epoch : 4, Step : 1680, Loss : 0.90648, Acc : 0.700, Sensitive_Loss : 0.21455, Sensitive_Acc : 17.300, Run Time : 11.52 sec
INFO:root:2024-03-30 08:26:28, Train, Epoch : 4, Step : 1690, Loss : 0.74036, Acc : 0.716, Sensitive_Loss : 0.27598, Sensitive_Acc : 15.700, Run Time : 14.85 sec
INFO:root:2024-03-30 08:26:39, Train, Epoch : 4, Step : 1700, Loss : 0.88070, Acc : 0.747, Sensitive_Loss : 0.18156, Sensitive_Acc : 17.200, Run Time : 10.77 sec
INFO:root:2024-03-30 08:28:43, Dev, Step : 1700, Loss : 0.93481, Acc : 0.780, Auc : 0.786, Sensitive_Loss : 0.24793, Sensitive_Acc : 17.121, Sensitive_Auc : 0.991, Mean auc: 0.786, Run Time : 123.84 sec
INFO:root:2024-03-30 08:28:43, Best, Step : 1700, Loss : 0.93481, Acc : 0.780, Auc : 0.786, Sensitive_Loss : 0.24793, Sensitive_Acc : 17.121, Sensitive_Auc : 0.991, Best Auc : 0.786
INFO:root:2024-03-30 08:28:52, Train, Epoch : 4, Step : 1710, Loss : 0.69271, Acc : 0.694, Sensitive_Loss : 0.20593, Sensitive_Acc : 18.300, Run Time : 133.22 sec
INFO:root:2024-03-30 08:29:02, Train, Epoch : 4, Step : 1720, Loss : 0.86624, Acc : 0.747, Sensitive_Loss : 0.27047, Sensitive_Acc : 15.600, Run Time : 10.10 sec
INFO:root:2024-03-30 08:29:13, Train, Epoch : 4, Step : 1730, Loss : 0.73492, Acc : 0.719, Sensitive_Loss : 0.29608, Sensitive_Acc : 16.700, Run Time : 10.51 sec
INFO:root:2024-03-30 08:29:23, Train, Epoch : 4, Step : 1740, Loss : 0.85892, Acc : 0.719, Sensitive_Loss : 0.21540, Sensitive_Acc : 15.900, Run Time : 10.40 sec
INFO:root:2024-03-30 08:29:34, Train, Epoch : 4, Step : 1750, Loss : 0.91743, Acc : 0.706, Sensitive_Loss : 0.22794, Sensitive_Acc : 17.100, Run Time : 11.03 sec
INFO:root:2024-03-30 08:29:45, Train, Epoch : 4, Step : 1760, Loss : 0.91482, Acc : 0.719, Sensitive_Loss : 0.26556, Sensitive_Acc : 13.900, Run Time : 10.74 sec
INFO:root:2024-03-30 08:29:56, Train, Epoch : 4, Step : 1770, Loss : 0.89149, Acc : 0.713, Sensitive_Loss : 0.25527, Sensitive_Acc : 17.900, Run Time : 11.24 sec
INFO:root:2024-03-30 08:30:10, Train, Epoch : 4, Step : 1780, Loss : 1.00140, Acc : 0.713, Sensitive_Loss : 0.25833, Sensitive_Acc : 15.000, Run Time : 13.88 sec
INFO:root:2024-03-30 08:30:20, Train, Epoch : 4, Step : 1790, Loss : 0.88468, Acc : 0.678, Sensitive_Loss : 0.24723, Sensitive_Acc : 18.900, Run Time : 10.33 sec
INFO:root:2024-03-30 08:30:31, Train, Epoch : 4, Step : 1800, Loss : 0.89351, Acc : 0.675, Sensitive_Loss : 0.20131, Sensitive_Acc : 16.100, Run Time : 10.85 sec
INFO:root:2024-03-30 08:32:40, Dev, Step : 1800, Loss : 0.93336, Acc : 0.780, Auc : 0.786, Sensitive_Loss : 0.26648, Sensitive_Acc : 16.809, Sensitive_Auc : 0.990, Mean auc: 0.786, Run Time : 129.22 sec
INFO:root:2024-03-30 08:32:48, Train, Epoch : 4, Step : 1810, Loss : 0.76042, Acc : 0.700, Sensitive_Loss : 0.24516, Sensitive_Acc : 16.500, Run Time : 136.31 sec
INFO:root:2024-03-30 08:32:58, Train, Epoch : 4, Step : 1820, Loss : 0.73429, Acc : 0.706, Sensitive_Loss : 0.25510, Sensitive_Acc : 15.500, Run Time : 10.26 sec
INFO:root:2024-03-30 08:33:11, Train, Epoch : 4, Step : 1830, Loss : 0.87463, Acc : 0.716, Sensitive_Loss : 0.28696, Sensitive_Acc : 17.300, Run Time : 13.45 sec
INFO:root:2024-03-30 08:33:22, Train, Epoch : 4, Step : 1840, Loss : 0.92612, Acc : 0.697, Sensitive_Loss : 0.23036, Sensitive_Acc : 16.000, Run Time : 10.37 sec
INFO:root:2024-03-30 08:33:32, Train, Epoch : 4, Step : 1850, Loss : 0.71455, Acc : 0.694, Sensitive_Loss : 0.25471, Sensitive_Acc : 16.900, Run Time : 10.52 sec
INFO:root:2024-03-30 08:33:43, Train, Epoch : 4, Step : 1860, Loss : 0.73294, Acc : 0.728, Sensitive_Loss : 0.21076, Sensitive_Acc : 17.600, Run Time : 11.32 sec
INFO:root:2024-03-30 08:33:54, Train, Epoch : 4, Step : 1870, Loss : 0.73597, Acc : 0.703, Sensitive_Loss : 0.20570, Sensitive_Acc : 16.700, Run Time : 10.71 sec
INFO:root:2024-03-30 08:34:04, Train, Epoch : 4, Step : 1880, Loss : 0.89194, Acc : 0.722, Sensitive_Loss : 0.25452, Sensitive_Acc : 15.500, Run Time : 10.25 sec
INFO:root:2024-03-30 08:34:15, Train, Epoch : 4, Step : 1890, Loss : 0.76507, Acc : 0.681, Sensitive_Loss : 0.23725, Sensitive_Acc : 17.700, Run Time : 10.13 sec
INFO:root:2024-03-30 08:34:26, Train, Epoch : 4, Step : 1900, Loss : 0.67703, Acc : 0.713, Sensitive_Loss : 0.25837, Sensitive_Acc : 15.400, Run Time : 11.16 sec
INFO:root:2024-03-30 08:36:30, Dev, Step : 1900, Loss : 0.91921, Acc : 0.757, Auc : 0.790, Sensitive_Loss : 0.27552, Sensitive_Acc : 16.723, Sensitive_Auc : 0.992, Mean auc: 0.790, Run Time : 124.61 sec
INFO:root:2024-03-30 08:36:31, Best, Step : 1900, Loss : 0.91921, Acc : 0.757, Auc : 0.790, Sensitive_Loss : 0.27552, Sensitive_Acc : 16.723, Sensitive_Auc : 0.992, Best Auc : 0.790
INFO:root:2024-03-30 08:36:39, Train, Epoch : 4, Step : 1910, Loss : 0.69857, Acc : 0.719, Sensitive_Loss : 0.26108, Sensitive_Acc : 16.100, Run Time : 133.00 sec
INFO:root:2024-03-30 08:36:49, Train, Epoch : 4, Step : 1920, Loss : 1.16385, Acc : 0.659, Sensitive_Loss : 0.23294, Sensitive_Acc : 16.900, Run Time : 10.26 sec
INFO:root:2024-03-30 08:36:59, Train, Epoch : 4, Step : 1930, Loss : 0.81197, Acc : 0.706, Sensitive_Loss : 0.29199, Sensitive_Acc : 16.900, Run Time : 10.55 sec
INFO:root:2024-03-30 08:37:11, Train, Epoch : 4, Step : 1940, Loss : 0.84108, Acc : 0.713, Sensitive_Loss : 0.23474, Sensitive_Acc : 18.100, Run Time : 11.65 sec
INFO:root:2024-03-30 08:37:23, Train, Epoch : 4, Step : 1950, Loss : 0.74058, Acc : 0.762, Sensitive_Loss : 0.23182, Sensitive_Acc : 18.400, Run Time : 12.11 sec
INFO:root:2024-03-30 08:37:33, Train, Epoch : 4, Step : 1960, Loss : 0.95526, Acc : 0.713, Sensitive_Loss : 0.29013, Sensitive_Acc : 17.000, Run Time : 10.12 sec
INFO:root:2024-03-30 08:39:17
INFO:root:y_pred: [0.21059729 0.14032151 0.33389854 ... 0.6166824  0.6459527  0.19539915]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [2.9648552e-03 2.7679294e-01 9.6301699e-01 7.1655877e-02 9.9518591e-01
 8.6985175e-03 1.6499990e-01 1.9351888e-02 2.3305988e-02 6.6693388e-02
 3.6267627e-02 1.3312389e-02 9.5044804e-01 5.3630336e-03 9.9423742e-01
 8.9730954e-01 4.0232239e-04 2.0298475e-01 9.9907613e-01 5.1206994e-01
 1.3932367e-01 2.8368149e-02 9.9989510e-01 8.9908820e-01 5.3580135e-02
 6.5495396e-01 4.2287875e-03 9.9945968e-01 1.9784946e-02 9.0493041e-01
 9.9956220e-01 9.8422235e-01 7.9728682e-03 2.3582084e-04 2.3958211e-01
 1.7596351e-02 4.8434440e-02 1.0635689e-01 8.9302492e-01 9.6021497e-01
 1.9259434e-02 1.1347011e-02 9.8794448e-01 9.9865097e-01 3.7674659e-01
 2.3210213e-02 2.4824299e-02 2.5708848e-01 9.9916339e-01 7.5273705e-01
 9.9370688e-01 4.3509972e-01 9.9838066e-01 9.9662423e-01 6.2012762e-01
 8.3218437e-01 7.6800221e-01 2.3729615e-02 9.9990535e-01 6.8034880e-02
 1.4381635e-03 7.2589016e-01 9.4934469e-01 9.6351862e-01 9.3527317e-01
 9.9929702e-01 6.3189489e-01 9.9951839e-01 9.7372460e-01 9.8155171e-01
 9.9496979e-01 5.3524477e-03 5.5526257e-03 5.2733082e-01 9.3641901e-01
 2.2041926e-02 9.9139118e-01 4.7166352e-04 9.9869627e-01 4.2936575e-01
 5.2454716e-01 3.1436879e-02 1.9895563e-01 5.2194285e-01 9.4778639e-01
 2.7575219e-01 9.8376793e-01 1.6721617e-01 7.3157940e-03 1.4382005e-02
 2.0795904e-02 2.0884165e-01 1.6161848e-02 1.1914567e-02 9.9870872e-01
 6.0524076e-01 1.6494472e-01 1.3098678e-02 9.4389743e-01 9.5751929e-01
 9.7020298e-01 2.1270365e-02 9.9394894e-01 4.5642521e-02 1.2028259e-01
 9.8506016e-01 1.1052217e-01 1.2884818e-02 2.4989531e-03 9.0619838e-03
 2.2828064e-03 9.1556942e-01 2.7977803e-01 9.5319208e-03 5.6471747e-01
 1.6056793e-03 9.9551976e-01 1.3344407e-02 5.5247235e-01 1.4160958e-02
 6.2960392e-01 1.0714127e-02 9.8460686e-01 9.4609829e-03 3.2138973e-02
 9.9942809e-01 1.7428240e-02 9.7649562e-01 8.2287842e-01 9.8157352e-01
 4.4491735e-05 9.9804747e-01 4.5420825e-03 9.9941373e-01 9.9457377e-01
 1.7780642e-01 6.9422603e-01 2.3130181e-01 2.7492153e-03 9.8552471e-01
 9.5714200e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-30 08:39:17, Dev, Step : 1968, Loss : 0.91290, Acc : 0.753, Auc : 0.795, Sensitive_Loss : 0.25955, Sensitive_Acc : 16.695, Sensitive_Auc : 0.992, Mean auc: 0.795, Run Time : 95.43 sec
INFO:root:2024-03-30 08:39:18, Best, Step : 1968, Loss : 0.91290, Acc : 0.753,Auc : 0.795, Best Auc : 0.795, Sensitive_Loss : 0.25955, Sensitive_Acc : 16.695, Sensitive_Auc : 0.992
INFO:root:2024-03-30 08:39:21, Train, Epoch : 5, Step : 1970, Loss : 0.17883, Acc : 0.134, Sensitive_Loss : 0.05171, Sensitive_Acc : 3.500, Run Time : 2.78 sec
INFO:root:2024-03-30 08:39:31, Train, Epoch : 5, Step : 1980, Loss : 0.76583, Acc : 0.750, Sensitive_Loss : 0.25463, Sensitive_Acc : 16.900, Run Time : 10.15 sec
INFO:root:2024-03-30 08:39:42, Train, Epoch : 5, Step : 1990, Loss : 0.83987, Acc : 0.713, Sensitive_Loss : 0.26310, Sensitive_Acc : 13.800, Run Time : 10.14 sec
INFO:root:2024-03-30 08:39:52, Train, Epoch : 5, Step : 2000, Loss : 0.73463, Acc : 0.738, Sensitive_Loss : 0.24069, Sensitive_Acc : 17.300, Run Time : 10.00 sec
INFO:root:2024-03-30 08:41:28, Dev, Step : 2000, Loss : 0.91183, Acc : 0.746, Auc : 0.795, Sensitive_Loss : 0.27430, Sensitive_Acc : 16.723, Sensitive_Auc : 0.992, Mean auc: 0.795, Run Time : 96.46 sec
INFO:root:2024-03-30 08:41:36, Train, Epoch : 5, Step : 2010, Loss : 0.80465, Acc : 0.713, Sensitive_Loss : 0.22710, Sensitive_Acc : 16.400, Run Time : 104.22 sec
INFO:root:2024-03-30 08:41:48, Train, Epoch : 5, Step : 2020, Loss : 0.71922, Acc : 0.688, Sensitive_Loss : 0.21066, Sensitive_Acc : 15.700, Run Time : 12.10 sec
INFO:root:2024-03-30 08:42:00, Train, Epoch : 5, Step : 2030, Loss : 0.97389, Acc : 0.719, Sensitive_Loss : 0.22913, Sensitive_Acc : 14.700, Run Time : 11.55 sec
INFO:root:2024-03-30 08:42:11, Train, Epoch : 5, Step : 2040, Loss : 0.76491, Acc : 0.753, Sensitive_Loss : 0.21805, Sensitive_Acc : 16.900, Run Time : 11.59 sec
INFO:root:2024-03-30 08:42:24, Train, Epoch : 5, Step : 2050, Loss : 0.77443, Acc : 0.691, Sensitive_Loss : 0.23598, Sensitive_Acc : 15.800, Run Time : 12.48 sec
INFO:root:2024-03-30 08:42:35, Train, Epoch : 5, Step : 2060, Loss : 0.90464, Acc : 0.688, Sensitive_Loss : 0.18338, Sensitive_Acc : 17.400, Run Time : 11.86 sec
INFO:root:2024-03-30 08:42:46, Train, Epoch : 5, Step : 2070, Loss : 0.71616, Acc : 0.697, Sensitive_Loss : 0.21827, Sensitive_Acc : 15.100, Run Time : 10.94 sec
INFO:root:2024-03-30 08:42:57, Train, Epoch : 5, Step : 2080, Loss : 0.89957, Acc : 0.709, Sensitive_Loss : 0.24108, Sensitive_Acc : 15.900, Run Time : 10.95 sec
INFO:root:2024-03-30 08:43:08, Train, Epoch : 5, Step : 2090, Loss : 0.57299, Acc : 0.753, Sensitive_Loss : 0.26420, Sensitive_Acc : 14.600, Run Time : 11.13 sec
INFO:root:2024-03-30 08:43:19, Train, Epoch : 5, Step : 2100, Loss : 0.68301, Acc : 0.722, Sensitive_Loss : 0.16104, Sensitive_Acc : 16.000, Run Time : 10.48 sec
INFO:root:2024-03-30 08:45:23, Dev, Step : 2100, Loss : 0.92485, Acc : 0.741, Auc : 0.790, Sensitive_Loss : 0.25598, Sensitive_Acc : 16.766, Sensitive_Auc : 0.990, Mean auc: 0.790, Run Time : 123.76 sec
INFO:root:2024-03-30 08:45:30, Train, Epoch : 5, Step : 2110, Loss : 0.87255, Acc : 0.706, Sensitive_Loss : 0.22991, Sensitive_Acc : 16.000, Run Time : 130.91 sec
INFO:root:2024-03-30 08:45:41, Train, Epoch : 5, Step : 2120, Loss : 0.78369, Acc : 0.728, Sensitive_Loss : 0.19675, Sensitive_Acc : 17.300, Run Time : 11.19 sec
INFO:root:2024-03-30 08:45:51, Train, Epoch : 5, Step : 2130, Loss : 0.59302, Acc : 0.772, Sensitive_Loss : 0.23081, Sensitive_Acc : 16.400, Run Time : 9.67 sec
INFO:root:2024-03-30 08:46:01, Train, Epoch : 5, Step : 2140, Loss : 0.86198, Acc : 0.722, Sensitive_Loss : 0.23280, Sensitive_Acc : 15.400, Run Time : 10.23 sec
INFO:root:2024-03-30 08:46:11, Train, Epoch : 5, Step : 2150, Loss : 0.77934, Acc : 0.722, Sensitive_Loss : 0.23622, Sensitive_Acc : 17.600, Run Time : 9.68 sec
INFO:root:2024-03-30 08:46:22, Train, Epoch : 5, Step : 2160, Loss : 0.62087, Acc : 0.716, Sensitive_Loss : 0.23491, Sensitive_Acc : 15.900, Run Time : 11.50 sec
INFO:root:2024-03-30 08:46:34, Train, Epoch : 5, Step : 2170, Loss : 0.84777, Acc : 0.747, Sensitive_Loss : 0.23837, Sensitive_Acc : 19.000, Run Time : 12.11 sec
INFO:root:2024-03-30 08:46:44, Train, Epoch : 5, Step : 2180, Loss : 0.93589, Acc : 0.722, Sensitive_Loss : 0.22033, Sensitive_Acc : 17.300, Run Time : 9.84 sec
INFO:root:2024-03-30 08:46:54, Train, Epoch : 5, Step : 2190, Loss : 0.93967, Acc : 0.694, Sensitive_Loss : 0.26369, Sensitive_Acc : 18.100, Run Time : 10.20 sec
INFO:root:2024-03-30 08:47:04, Train, Epoch : 5, Step : 2200, Loss : 0.76323, Acc : 0.709, Sensitive_Loss : 0.24491, Sensitive_Acc : 14.700, Run Time : 10.19 sec
INFO:root:2024-03-30 08:48:45, Dev, Step : 2200, Loss : 0.92045, Acc : 0.770, Auc : 0.791, Sensitive_Loss : 0.27341, Sensitive_Acc : 16.695, Sensitive_Auc : 0.991, Mean auc: 0.791, Run Time : 100.28 sec
INFO:root:2024-03-30 08:48:54, Train, Epoch : 5, Step : 2210, Loss : 1.01786, Acc : 0.684, Sensitive_Loss : 0.21823, Sensitive_Acc : 15.800, Run Time : 109.73 sec
INFO:root:2024-03-30 08:49:06, Train, Epoch : 5, Step : 2220, Loss : 0.71001, Acc : 0.703, Sensitive_Loss : 0.30966, Sensitive_Acc : 15.300, Run Time : 11.82 sec
INFO:root:2024-03-30 08:49:17, Train, Epoch : 5, Step : 2230, Loss : 0.82528, Acc : 0.741, Sensitive_Loss : 0.22316, Sensitive_Acc : 17.600, Run Time : 10.95 sec
INFO:root:2024-03-30 08:49:28, Train, Epoch : 5, Step : 2240, Loss : 0.80791, Acc : 0.691, Sensitive_Loss : 0.27420, Sensitive_Acc : 17.500, Run Time : 10.87 sec
INFO:root:2024-03-30 08:49:39, Train, Epoch : 5, Step : 2250, Loss : 0.92701, Acc : 0.728, Sensitive_Loss : 0.21795, Sensitive_Acc : 15.500, Run Time : 11.62 sec
INFO:root:2024-03-30 08:49:50, Train, Epoch : 5, Step : 2260, Loss : 0.68854, Acc : 0.738, Sensitive_Loss : 0.20529, Sensitive_Acc : 14.700, Run Time : 10.77 sec
INFO:root:2024-03-30 08:50:01, Train, Epoch : 5, Step : 2270, Loss : 0.84298, Acc : 0.734, Sensitive_Loss : 0.20535, Sensitive_Acc : 17.600, Run Time : 10.34 sec
INFO:root:2024-03-30 08:50:11, Train, Epoch : 5, Step : 2280, Loss : 0.88253, Acc : 0.719, Sensitive_Loss : 0.27590, Sensitive_Acc : 16.500, Run Time : 10.58 sec
INFO:root:2024-03-30 08:50:22, Train, Epoch : 5, Step : 2290, Loss : 0.77552, Acc : 0.772, Sensitive_Loss : 0.25577, Sensitive_Acc : 16.500, Run Time : 10.65 sec
INFO:root:2024-03-30 08:50:32, Train, Epoch : 5, Step : 2300, Loss : 0.77094, Acc : 0.753, Sensitive_Loss : 0.24703, Sensitive_Acc : 16.600, Run Time : 10.36 sec
INFO:root:2024-03-30 08:52:10, Dev, Step : 2300, Loss : 0.91680, Acc : 0.756, Auc : 0.791, Sensitive_Loss : 0.27066, Sensitive_Acc : 16.695, Sensitive_Auc : 0.990, Mean auc: 0.791, Run Time : 97.88 sec
INFO:root:2024-03-30 08:52:18, Train, Epoch : 5, Step : 2310, Loss : 0.74597, Acc : 0.759, Sensitive_Loss : 0.21752, Sensitive_Acc : 17.300, Run Time : 105.40 sec
INFO:root:2024-03-30 08:52:29, Train, Epoch : 5, Step : 2320, Loss : 0.75920, Acc : 0.703, Sensitive_Loss : 0.22915, Sensitive_Acc : 16.400, Run Time : 11.41 sec
INFO:root:2024-03-30 08:52:40, Train, Epoch : 5, Step : 2330, Loss : 0.76009, Acc : 0.719, Sensitive_Loss : 0.23018, Sensitive_Acc : 16.500, Run Time : 11.18 sec
INFO:root:2024-03-30 08:52:52, Train, Epoch : 5, Step : 2340, Loss : 0.84395, Acc : 0.747, Sensitive_Loss : 0.24735, Sensitive_Acc : 14.900, Run Time : 12.16 sec
INFO:root:2024-03-30 08:53:03, Train, Epoch : 5, Step : 2350, Loss : 0.78477, Acc : 0.756, Sensitive_Loss : 0.23916, Sensitive_Acc : 15.300, Run Time : 10.54 sec
INFO:root:2024-03-30 08:53:14, Train, Epoch : 5, Step : 2360, Loss : 0.94124, Acc : 0.734, Sensitive_Loss : 0.24486, Sensitive_Acc : 18.000, Run Time : 11.19 sec
INFO:root:2024-03-30 08:53:25, Train, Epoch : 5, Step : 2370, Loss : 0.79309, Acc : 0.713, Sensitive_Loss : 0.21393, Sensitive_Acc : 16.600, Run Time : 10.76 sec
INFO:root:2024-03-30 08:53:36, Train, Epoch : 5, Step : 2380, Loss : 0.79544, Acc : 0.734, Sensitive_Loss : 0.22108, Sensitive_Acc : 17.200, Run Time : 11.34 sec
INFO:root:2024-03-30 08:53:47, Train, Epoch : 5, Step : 2390, Loss : 0.81515, Acc : 0.750, Sensitive_Loss : 0.27254, Sensitive_Acc : 14.100, Run Time : 10.91 sec
INFO:root:2024-03-30 08:53:58, Train, Epoch : 5, Step : 2400, Loss : 0.88944, Acc : 0.738, Sensitive_Loss : 0.28862, Sensitive_Acc : 17.100, Run Time : 11.27 sec
INFO:root:2024-03-30 08:56:01, Dev, Step : 2400, Loss : 0.91323, Acc : 0.752, Auc : 0.796, Sensitive_Loss : 0.28020, Sensitive_Acc : 16.695, Sensitive_Auc : 0.992, Mean auc: 0.796, Run Time : 123.10 sec
INFO:root:2024-03-30 08:56:02, Best, Step : 2400, Loss : 0.91323, Acc : 0.752, Auc : 0.796, Sensitive_Loss : 0.28020, Sensitive_Acc : 16.695, Sensitive_Auc : 0.992, Best Auc : 0.796
INFO:root:2024-03-30 08:56:09, Train, Epoch : 5, Step : 2410, Loss : 0.84503, Acc : 0.753, Sensitive_Loss : 0.17644, Sensitive_Acc : 15.000, Run Time : 130.76 sec
INFO:root:2024-03-30 08:56:20, Train, Epoch : 5, Step : 2420, Loss : 0.85520, Acc : 0.738, Sensitive_Loss : 0.19240, Sensitive_Acc : 15.900, Run Time : 10.52 sec
INFO:root:2024-03-30 08:56:30, Train, Epoch : 5, Step : 2430, Loss : 0.73365, Acc : 0.750, Sensitive_Loss : 0.21540, Sensitive_Acc : 14.700, Run Time : 10.44 sec
INFO:root:2024-03-30 08:56:40, Train, Epoch : 5, Step : 2440, Loss : 0.79517, Acc : 0.725, Sensitive_Loss : 0.23000, Sensitive_Acc : 17.200, Run Time : 9.87 sec
INFO:root:2024-03-30 08:56:52, Train, Epoch : 5, Step : 2450, Loss : 0.88148, Acc : 0.744, Sensitive_Loss : 0.24262, Sensitive_Acc : 16.100, Run Time : 11.62 sec
INFO:root:2024-03-30 08:57:01, Train, Epoch : 5, Step : 2460, Loss : 0.90423, Acc : 0.759, Sensitive_Loss : 0.23616, Sensitive_Acc : 16.800, Run Time : 9.95 sec
INFO:root:2024-03-30 08:58:37
INFO:root:y_pred: [0.17020278 0.17515586 0.20147233 ... 0.6093153  0.4089126  0.11921002]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [2.51849974e-03 1.45424351e-01 9.69123304e-01 3.49354818e-02
 9.97674048e-01 4.06697067e-03 1.88380867e-01 2.35219635e-02
 2.04105116e-02 6.82732835e-02 2.86947135e-02 1.02063185e-02
 9.82697070e-01 5.76437172e-03 9.97815251e-01 8.96819949e-01
 3.31443502e-04 1.93837687e-01 9.99382019e-01 5.10802805e-01
 1.86269209e-01 3.86781879e-02 9.99876499e-01 9.16356146e-01
 3.44181769e-02 7.02546239e-01 3.30882869e-03 9.99831915e-01
 1.65851880e-02 8.80990088e-01 9.99592483e-01 9.81480837e-01
 1.41579695e-02 1.41068725e-04 2.37971157e-01 1.99725572e-02
 6.43304437e-02 1.49332374e-01 9.43554699e-01 9.40780938e-01
 1.47476625e-02 1.09996190e-02 9.87265468e-01 9.98859286e-01
 3.00991386e-01 4.50406410e-02 8.40188656e-03 3.39926600e-01
 9.99496818e-01 8.23972821e-01 9.96450543e-01 5.14217317e-01
 9.98991191e-01 9.95658159e-01 7.06255913e-01 9.23916459e-01
 8.82457018e-01 1.86350122e-02 9.99972939e-01 9.30606052e-02
 1.72645075e-03 5.75855792e-01 9.54205215e-01 9.61807370e-01
 9.56430614e-01 9.99575436e-01 6.39460564e-01 9.99578655e-01
 9.85019565e-01 9.83902276e-01 9.96447563e-01 5.22252033e-03
 9.20281094e-03 6.19650722e-01 9.34926867e-01 2.82068346e-02
 9.93889093e-01 6.58467121e-04 9.99007165e-01 3.54415506e-01
 6.09285712e-01 3.35847437e-02 2.36175761e-01 5.99116921e-01
 9.71464753e-01 3.87435138e-01 9.89285767e-01 2.77933955e-01
 2.78487094e-02 1.58950575e-02 2.10727975e-02 2.46363699e-01
 2.19432432e-02 1.54633801e-02 9.98261988e-01 6.98449135e-01
 2.23047003e-01 6.12202426e-03 9.42631304e-01 9.57269728e-01
 9.64761376e-01 2.71614268e-02 9.96206641e-01 6.36001527e-02
 1.72398999e-01 9.85458493e-01 1.23020187e-01 1.57263391e-02
 2.02633673e-03 9.54774953e-03 1.33324426e-03 9.53995705e-01
 1.90053686e-01 7.11279921e-03 4.80758220e-01 7.52039545e-04
 9.97616529e-01 1.09975282e-02 6.77807570e-01 6.56416826e-03
 6.42647564e-01 1.02292830e-02 9.93631542e-01 8.88346694e-03
 3.26412357e-02 9.99700308e-01 1.58637092e-02 9.82834220e-01
 8.12668681e-01 9.87925768e-01 5.99772029e-05 9.97667491e-01
 6.69685937e-03 9.99430597e-01 9.92488265e-01 2.03511432e-01
 6.11099660e-01 1.60754517e-01 3.62295564e-03 9.82794821e-01
 9.76884544e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-30 08:58:37, Dev, Step : 2460, Loss : 0.92582, Acc : 0.798, Auc : 0.794, Sensitive_Loss : 0.26302, Sensitive_Acc : 16.809, Sensitive_Auc : 0.993, Mean auc: 0.794, Run Time : 95.31 sec
INFO:root:2024-03-30 08:58:49, Train, Epoch : 6, Step : 2470, Loss : 0.76327, Acc : 0.762, Sensitive_Loss : 0.19875, Sensitive_Acc : 17.600, Run Time : 10.80 sec
INFO:root:2024-03-30 08:59:00, Train, Epoch : 6, Step : 2480, Loss : 0.87633, Acc : 0.750, Sensitive_Loss : 0.18272, Sensitive_Acc : 18.000, Run Time : 11.71 sec
INFO:root:2024-03-30 08:59:11, Train, Epoch : 6, Step : 2490, Loss : 0.65897, Acc : 0.731, Sensitive_Loss : 0.28138, Sensitive_Acc : 16.600, Run Time : 11.07 sec
INFO:root:2024-03-30 08:59:22, Train, Epoch : 6, Step : 2500, Loss : 0.82423, Acc : 0.719, Sensitive_Loss : 0.28014, Sensitive_Acc : 17.800, Run Time : 10.57 sec
INFO:root:2024-03-30 09:00:59, Dev, Step : 2500, Loss : 0.91096, Acc : 0.746, Auc : 0.795, Sensitive_Loss : 0.27288, Sensitive_Acc : 16.723, Sensitive_Auc : 0.993, Mean auc: 0.795, Run Time : 97.17 sec
INFO:root:2024-03-30 09:01:07, Train, Epoch : 6, Step : 2510, Loss : 0.89320, Acc : 0.750, Sensitive_Loss : 0.22775, Sensitive_Acc : 17.000, Run Time : 104.90 sec
INFO:root:2024-03-30 09:01:19, Train, Epoch : 6, Step : 2520, Loss : 0.89645, Acc : 0.750, Sensitive_Loss : 0.22446, Sensitive_Acc : 17.000, Run Time : 11.69 sec
INFO:root:2024-03-30 09:01:32, Train, Epoch : 6, Step : 2530, Loss : 0.61770, Acc : 0.709, Sensitive_Loss : 0.21375, Sensitive_Acc : 17.400, Run Time : 13.46 sec
INFO:root:2024-03-30 09:01:43, Train, Epoch : 6, Step : 2540, Loss : 0.61963, Acc : 0.722, Sensitive_Loss : 0.24536, Sensitive_Acc : 18.300, Run Time : 11.30 sec
INFO:root:2024-03-30 09:01:55, Train, Epoch : 6, Step : 2550, Loss : 0.69879, Acc : 0.741, Sensitive_Loss : 0.24102, Sensitive_Acc : 15.300, Run Time : 11.43 sec
INFO:root:2024-03-30 09:02:07, Train, Epoch : 6, Step : 2560, Loss : 0.65587, Acc : 0.744, Sensitive_Loss : 0.23161, Sensitive_Acc : 16.400, Run Time : 11.93 sec
INFO:root:2024-03-30 09:02:17, Train, Epoch : 6, Step : 2570, Loss : 0.81889, Acc : 0.762, Sensitive_Loss : 0.22943, Sensitive_Acc : 16.800, Run Time : 10.61 sec
INFO:root:2024-03-30 09:02:28, Train, Epoch : 6, Step : 2580, Loss : 0.71941, Acc : 0.750, Sensitive_Loss : 0.26470, Sensitive_Acc : 14.500, Run Time : 10.69 sec
INFO:root:2024-03-30 09:02:40, Train, Epoch : 6, Step : 2590, Loss : 0.61595, Acc : 0.756, Sensitive_Loss : 0.20348, Sensitive_Acc : 15.300, Run Time : 11.79 sec
INFO:root:2024-03-30 09:02:51, Train, Epoch : 6, Step : 2600, Loss : 0.65054, Acc : 0.716, Sensitive_Loss : 0.24016, Sensitive_Acc : 17.100, Run Time : 10.84 sec
INFO:root:2024-03-30 09:05:00, Dev, Step : 2600, Loss : 0.91610, Acc : 0.725, Auc : 0.795, Sensitive_Loss : 0.27839, Sensitive_Acc : 16.638, Sensitive_Auc : 0.992, Mean auc: 0.795, Run Time : 129.65 sec
INFO:root:2024-03-30 09:05:07, Train, Epoch : 6, Step : 2610, Loss : 0.69588, Acc : 0.734, Sensitive_Loss : 0.24573, Sensitive_Acc : 15.600, Run Time : 136.56 sec
INFO:root:2024-03-30 09:05:17, Train, Epoch : 6, Step : 2620, Loss : 0.79411, Acc : 0.728, Sensitive_Loss : 0.21698, Sensitive_Acc : 17.200, Run Time : 10.15 sec
INFO:root:2024-03-30 09:05:28, Train, Epoch : 6, Step : 2630, Loss : 0.69764, Acc : 0.759, Sensitive_Loss : 0.24958, Sensitive_Acc : 17.300, Run Time : 11.07 sec
INFO:root:2024-03-30 09:05:40, Train, Epoch : 6, Step : 2640, Loss : 0.66784, Acc : 0.728, Sensitive_Loss : 0.24485, Sensitive_Acc : 17.600, Run Time : 12.06 sec
INFO:root:2024-03-30 09:05:50, Train, Epoch : 6, Step : 2650, Loss : 0.78069, Acc : 0.750, Sensitive_Loss : 0.26372, Sensitive_Acc : 15.900, Run Time : 10.00 sec
INFO:root:2024-03-30 09:06:00, Train, Epoch : 6, Step : 2660, Loss : 0.78815, Acc : 0.719, Sensitive_Loss : 0.24984, Sensitive_Acc : 16.800, Run Time : 10.02 sec
INFO:root:2024-03-30 09:06:12, Train, Epoch : 6, Step : 2670, Loss : 1.01754, Acc : 0.700, Sensitive_Loss : 0.19441, Sensitive_Acc : 16.100, Run Time : 11.53 sec
INFO:root:2024-03-30 09:06:22, Train, Epoch : 6, Step : 2680, Loss : 0.72272, Acc : 0.744, Sensitive_Loss : 0.22039, Sensitive_Acc : 19.000, Run Time : 10.20 sec
INFO:root:2024-03-30 09:06:32, Train, Epoch : 6, Step : 2690, Loss : 0.70828, Acc : 0.772, Sensitive_Loss : 0.27583, Sensitive_Acc : 16.200, Run Time : 9.72 sec
INFO:root:2024-03-30 09:06:43, Train, Epoch : 6, Step : 2700, Loss : 0.83183, Acc : 0.747, Sensitive_Loss : 0.24185, Sensitive_Acc : 18.400, Run Time : 11.16 sec
INFO:root:2024-03-30 09:08:23, Dev, Step : 2700, Loss : 0.92241, Acc : 0.765, Auc : 0.788, Sensitive_Loss : 0.29625, Sensitive_Acc : 16.752, Sensitive_Auc : 0.992, Mean auc: 0.788, Run Time : 100.18 sec
INFO:root:2024-03-30 09:08:31, Train, Epoch : 6, Step : 2710, Loss : 0.82811, Acc : 0.744, Sensitive_Loss : 0.26876, Sensitive_Acc : 14.700, Run Time : 107.53 sec
INFO:root:2024-03-30 09:08:41, Train, Epoch : 6, Step : 2720, Loss : 0.89953, Acc : 0.741, Sensitive_Loss : 0.20085, Sensitive_Acc : 16.600, Run Time : 9.87 sec
INFO:root:2024-03-30 09:08:51, Train, Epoch : 6, Step : 2730, Loss : 0.75505, Acc : 0.750, Sensitive_Loss : 0.21039, Sensitive_Acc : 18.000, Run Time : 10.25 sec
INFO:root:2024-03-30 09:09:01, Train, Epoch : 6, Step : 2740, Loss : 0.75652, Acc : 0.772, Sensitive_Loss : 0.17689, Sensitive_Acc : 15.900, Run Time : 10.42 sec
INFO:root:2024-03-30 09:09:11, Train, Epoch : 6, Step : 2750, Loss : 0.63453, Acc : 0.759, Sensitive_Loss : 0.22412, Sensitive_Acc : 15.200, Run Time : 10.13 sec
INFO:root:2024-03-30 09:09:22, Train, Epoch : 6, Step : 2760, Loss : 0.80494, Acc : 0.716, Sensitive_Loss : 0.20633, Sensitive_Acc : 16.000, Run Time : 10.23 sec
INFO:root:2024-03-30 09:09:34, Train, Epoch : 6, Step : 2770, Loss : 0.91786, Acc : 0.694, Sensitive_Loss : 0.17457, Sensitive_Acc : 15.400, Run Time : 12.47 sec
INFO:root:2024-03-30 09:09:46, Train, Epoch : 6, Step : 2780, Loss : 0.59868, Acc : 0.700, Sensitive_Loss : 0.25535, Sensitive_Acc : 19.900, Run Time : 12.41 sec
INFO:root:2024-03-30 09:09:56, Train, Epoch : 6, Step : 2790, Loss : 0.60121, Acc : 0.731, Sensitive_Loss : 0.20672, Sensitive_Acc : 16.800, Run Time : 10.01 sec
INFO:root:2024-03-30 09:10:07, Train, Epoch : 6, Step : 2800, Loss : 0.82721, Acc : 0.719, Sensitive_Loss : 0.23198, Sensitive_Acc : 18.300, Run Time : 11.04 sec
INFO:root:2024-03-30 09:11:45, Dev, Step : 2800, Loss : 0.91245, Acc : 0.785, Auc : 0.795, Sensitive_Loss : 0.24656, Sensitive_Acc : 16.695, Sensitive_Auc : 0.993, Mean auc: 0.795, Run Time : 97.37 sec
INFO:root:2024-03-30 09:11:52, Train, Epoch : 6, Step : 2810, Loss : 0.71002, Acc : 0.750, Sensitive_Loss : 0.19862, Sensitive_Acc : 17.200, Run Time : 104.83 sec
INFO:root:2024-03-30 09:12:03, Train, Epoch : 6, Step : 2820, Loss : 0.78534, Acc : 0.750, Sensitive_Loss : 0.18010, Sensitive_Acc : 15.400, Run Time : 10.88 sec
INFO:root:2024-03-30 09:12:13, Train, Epoch : 6, Step : 2830, Loss : 0.78937, Acc : 0.750, Sensitive_Loss : 0.17590, Sensitive_Acc : 14.100, Run Time : 10.27 sec
INFO:root:2024-03-30 09:12:24, Train, Epoch : 6, Step : 2840, Loss : 0.89207, Acc : 0.738, Sensitive_Loss : 0.22581, Sensitive_Acc : 15.300, Run Time : 10.79 sec
INFO:root:2024-03-30 09:12:36, Train, Epoch : 6, Step : 2850, Loss : 0.62591, Acc : 0.753, Sensitive_Loss : 0.16672, Sensitive_Acc : 16.700, Run Time : 11.33 sec
INFO:root:2024-03-30 09:12:46, Train, Epoch : 6, Step : 2860, Loss : 0.64544, Acc : 0.759, Sensitive_Loss : 0.20962, Sensitive_Acc : 16.800, Run Time : 10.70 sec
INFO:root:2024-03-30 09:12:57, Train, Epoch : 6, Step : 2870, Loss : 0.72761, Acc : 0.769, Sensitive_Loss : 0.24245, Sensitive_Acc : 15.000, Run Time : 10.77 sec
INFO:root:2024-03-30 09:13:08, Train, Epoch : 6, Step : 2880, Loss : 0.71699, Acc : 0.722, Sensitive_Loss : 0.21307, Sensitive_Acc : 18.700, Run Time : 10.63 sec
INFO:root:2024-03-30 09:13:19, Train, Epoch : 6, Step : 2890, Loss : 0.92619, Acc : 0.744, Sensitive_Loss : 0.22228, Sensitive_Acc : 15.400, Run Time : 10.94 sec
INFO:root:2024-03-30 09:13:29, Train, Epoch : 6, Step : 2900, Loss : 0.81278, Acc : 0.722, Sensitive_Loss : 0.20875, Sensitive_Acc : 16.100, Run Time : 10.71 sec
INFO:root:2024-03-30 09:15:18, Dev, Step : 2900, Loss : 0.93587, Acc : 0.713, Auc : 0.781, Sensitive_Loss : 0.23825, Sensitive_Acc : 16.823, Sensitive_Auc : 0.993, Mean auc: 0.781, Run Time : 108.72 sec
INFO:root:2024-03-30 09:15:25, Train, Epoch : 6, Step : 2910, Loss : 0.82352, Acc : 0.731, Sensitive_Loss : 0.29398, Sensitive_Acc : 18.500, Run Time : 116.08 sec
INFO:root:2024-03-30 09:15:36, Train, Epoch : 6, Step : 2920, Loss : 0.78903, Acc : 0.719, Sensitive_Loss : 0.16926, Sensitive_Acc : 16.200, Run Time : 10.91 sec
INFO:root:2024-03-30 09:15:47, Train, Epoch : 6, Step : 2930, Loss : 0.81847, Acc : 0.741, Sensitive_Loss : 0.19909, Sensitive_Acc : 17.000, Run Time : 10.65 sec
INFO:root:2024-03-30 09:15:59, Train, Epoch : 6, Step : 2940, Loss : 0.68509, Acc : 0.766, Sensitive_Loss : 0.20169, Sensitive_Acc : 17.900, Run Time : 11.97 sec
INFO:root:2024-03-30 09:16:11, Train, Epoch : 6, Step : 2950, Loss : 0.78431, Acc : 0.756, Sensitive_Loss : 0.26215, Sensitive_Acc : 13.900, Run Time : 12.28 sec
INFO:root:2024-03-30 09:17:49
INFO:root:y_pred: [0.32349014 0.16214056 0.3139238  ... 0.58470935 0.514276   0.22601373]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [3.22719919e-03 1.23376340e-01 9.70095158e-01 2.72950493e-02
 9.96040940e-01 2.90761259e-03 8.86479393e-02 2.29913648e-02
 2.05979850e-02 3.58429551e-02 1.17369825e-02 7.78264645e-03
 9.83945072e-01 1.68520969e-03 9.97127593e-01 8.97247076e-01
 5.15511550e-04 1.45798281e-01 9.99592960e-01 5.89549124e-01
 1.16638586e-01 3.44421268e-02 9.99793828e-01 9.37125027e-01
 9.84875392e-03 6.70517981e-01 5.58581576e-03 9.99535799e-01
 1.67820361e-02 8.73821437e-01 9.99781907e-01 9.86425757e-01
 1.19572096e-02 2.15419874e-04 3.00179780e-01 3.62833031e-02
 3.22984718e-02 4.12009321e-02 9.79148030e-01 9.47919488e-01
 1.15958368e-02 1.95258167e-02 9.92131293e-01 9.98768866e-01
 1.68378070e-01 2.06878800e-02 1.88933685e-02 1.90893039e-01
 9.97947395e-01 8.27753425e-01 9.95715439e-01 3.85364830e-01
 9.98696864e-01 9.93551493e-01 5.36155403e-01 9.41560566e-01
 8.78045678e-01 2.11206134e-02 9.99990463e-01 7.00829476e-02
 6.87393069e-04 7.13316798e-01 9.28557634e-01 9.43415999e-01
 9.52910483e-01 9.99531627e-01 6.53896928e-01 9.99526858e-01
 9.85025704e-01 9.90551710e-01 9.88643110e-01 5.04303537e-03
 1.14782071e-02 6.78504646e-01 9.43824053e-01 2.92951670e-02
 9.94738877e-01 4.66805359e-04 9.98854280e-01 3.60575914e-01
 5.37728131e-01 3.40051875e-02 1.27387822e-01 7.28519559e-01
 9.79810357e-01 3.66893917e-01 9.80627477e-01 1.02868967e-01
 9.91032925e-03 1.72324777e-02 1.90216228e-02 2.95074552e-01
 8.67636222e-03 1.06677692e-02 9.95081186e-01 7.67537653e-01
 1.85483620e-01 9.79611650e-03 9.29904401e-01 9.21498656e-01
 9.69264925e-01 2.05860790e-02 9.94025290e-01 2.06565578e-02
 1.47284552e-01 9.81196702e-01 9.34587419e-02 1.87253859e-02
 1.91484345e-03 8.47826898e-03 1.14806625e-03 9.37138438e-01
 2.34664500e-01 8.47291108e-03 3.83544713e-01 5.77213301e-04
 9.96479332e-01 3.83225968e-03 6.82278275e-01 5.62729454e-03
 5.90222180e-01 1.57313868e-02 9.94112194e-01 5.56646334e-03
 2.59063728e-02 9.99527812e-01 2.71639600e-02 9.66013730e-01
 7.09240913e-01 9.91283417e-01 3.90292480e-05 9.97993350e-01
 3.93033540e-03 9.99811828e-01 9.90542471e-01 1.29995003e-01
 4.94997621e-01 1.77141264e-01 5.61092468e-03 9.84085679e-01
 9.65488672e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-30 09:17:49, Dev, Step : 2952, Loss : 0.91200, Acc : 0.729, Auc : 0.796, Sensitive_Loss : 0.24684, Sensitive_Acc : 16.723, Sensitive_Auc : 0.993, Mean auc: 0.796, Run Time : 95.63 sec
INFO:root:2024-03-30 09:17:50, Best, Step : 2952, Loss : 0.91200, Acc : 0.729,Auc : 0.796, Best Auc : 0.796, Sensitive_Loss : 0.24684, Sensitive_Acc : 16.723, Sensitive_Auc : 0.993
INFO:root:2024-03-30 09:18:00, Train, Epoch : 7, Step : 2960, Loss : 0.53664, Acc : 0.578, Sensitive_Loss : 0.17275, Sensitive_Acc : 13.200, Run Time : 9.39 sec
INFO:root:2024-03-30 09:18:12, Train, Epoch : 7, Step : 2970, Loss : 0.61450, Acc : 0.784, Sensitive_Loss : 0.22908, Sensitive_Acc : 16.900, Run Time : 11.92 sec
INFO:root:2024-03-30 09:18:23, Train, Epoch : 7, Step : 2980, Loss : 0.83493, Acc : 0.725, Sensitive_Loss : 0.27812, Sensitive_Acc : 19.100, Run Time : 11.34 sec
INFO:root:2024-03-30 09:18:33, Train, Epoch : 7, Step : 2990, Loss : 0.79495, Acc : 0.728, Sensitive_Loss : 0.24329, Sensitive_Acc : 15.500, Run Time : 10.18 sec
INFO:root:2024-03-30 09:18:46, Train, Epoch : 7, Step : 3000, Loss : 0.76790, Acc : 0.762, Sensitive_Loss : 0.19630, Sensitive_Acc : 17.100, Run Time : 12.59 sec
INFO:root:2024-03-30 09:20:23, Dev, Step : 3000, Loss : 0.92134, Acc : 0.757, Auc : 0.788, Sensitive_Loss : 0.25561, Sensitive_Acc : 16.809, Sensitive_Auc : 0.993, Mean auc: 0.788, Run Time : 96.81 sec
INFO:root:2024-03-30 09:20:32, Train, Epoch : 7, Step : 3010, Loss : 0.86830, Acc : 0.738, Sensitive_Loss : 0.22696, Sensitive_Acc : 15.900, Run Time : 105.74 sec
INFO:root:2024-03-30 09:20:43, Train, Epoch : 7, Step : 3020, Loss : 0.80067, Acc : 0.731, Sensitive_Loss : 0.24075, Sensitive_Acc : 15.900, Run Time : 11.63 sec
INFO:root:2024-03-30 09:20:56, Train, Epoch : 7, Step : 3030, Loss : 0.80136, Acc : 0.753, Sensitive_Loss : 0.24298, Sensitive_Acc : 13.600, Run Time : 12.70 sec
INFO:root:2024-03-30 09:21:07, Train, Epoch : 7, Step : 3040, Loss : 0.61865, Acc : 0.719, Sensitive_Loss : 0.25735, Sensitive_Acc : 17.100, Run Time : 11.50 sec
INFO:root:2024-03-30 09:21:19, Train, Epoch : 7, Step : 3050, Loss : 0.89773, Acc : 0.709, Sensitive_Loss : 0.16652, Sensitive_Acc : 15.500, Run Time : 11.34 sec
INFO:root:2024-03-30 09:21:30, Train, Epoch : 7, Step : 3060, Loss : 0.60054, Acc : 0.741, Sensitive_Loss : 0.25563, Sensitive_Acc : 18.600, Run Time : 10.86 sec
INFO:root:2024-03-30 09:21:41, Train, Epoch : 7, Step : 3070, Loss : 0.73425, Acc : 0.762, Sensitive_Loss : 0.19431, Sensitive_Acc : 16.100, Run Time : 11.05 sec
INFO:root:2024-03-30 09:21:51, Train, Epoch : 7, Step : 3080, Loss : 0.61788, Acc : 0.734, Sensitive_Loss : 0.18809, Sensitive_Acc : 15.800, Run Time : 10.86 sec
INFO:root:2024-03-30 09:22:02, Train, Epoch : 7, Step : 3090, Loss : 0.78177, Acc : 0.750, Sensitive_Loss : 0.23313, Sensitive_Acc : 17.800, Run Time : 10.79 sec
INFO:root:2024-03-30 09:22:13, Train, Epoch : 7, Step : 3100, Loss : 0.73094, Acc : 0.750, Sensitive_Loss : 0.30056, Sensitive_Acc : 17.400, Run Time : 10.52 sec
INFO:root:2024-03-30 09:24:10, Dev, Step : 3100, Loss : 0.90542, Acc : 0.767, Auc : 0.795, Sensitive_Loss : 0.25741, Sensitive_Acc : 16.695, Sensitive_Auc : 0.992, Mean auc: 0.795, Run Time : 117.48 sec
INFO:root:2024-03-30 09:24:18, Train, Epoch : 7, Step : 3110, Loss : 0.66743, Acc : 0.772, Sensitive_Loss : 0.20390, Sensitive_Acc : 16.900, Run Time : 125.26 sec
INFO:root:2024-03-30 09:24:29, Train, Epoch : 7, Step : 3120, Loss : 0.72919, Acc : 0.762, Sensitive_Loss : 0.20990, Sensitive_Acc : 16.800, Run Time : 10.68 sec
INFO:root:2024-03-30 09:24:39, Train, Epoch : 7, Step : 3130, Loss : 0.92092, Acc : 0.738, Sensitive_Loss : 0.20071, Sensitive_Acc : 16.300, Run Time : 10.28 sec
INFO:root:2024-03-30 09:24:49, Train, Epoch : 7, Step : 3140, Loss : 0.70170, Acc : 0.728, Sensitive_Loss : 0.23435, Sensitive_Acc : 17.000, Run Time : 9.92 sec
INFO:root:2024-03-30 09:25:00, Train, Epoch : 7, Step : 3150, Loss : 0.78247, Acc : 0.769, Sensitive_Loss : 0.23639, Sensitive_Acc : 17.100, Run Time : 10.70 sec
INFO:root:2024-03-30 09:25:10, Train, Epoch : 7, Step : 3160, Loss : 0.78857, Acc : 0.731, Sensitive_Loss : 0.25306, Sensitive_Acc : 15.200, Run Time : 10.51 sec
INFO:root:2024-03-30 09:25:20, Train, Epoch : 7, Step : 3170, Loss : 0.77057, Acc : 0.759, Sensitive_Loss : 0.21512, Sensitive_Acc : 16.200, Run Time : 9.88 sec
INFO:root:2024-03-30 09:25:31, Train, Epoch : 7, Step : 3180, Loss : 0.72451, Acc : 0.734, Sensitive_Loss : 0.18861, Sensitive_Acc : 19.100, Run Time : 10.48 sec
INFO:root:2024-03-30 09:25:41, Train, Epoch : 7, Step : 3190, Loss : 0.85206, Acc : 0.744, Sensitive_Loss : 0.25280, Sensitive_Acc : 16.800, Run Time : 10.46 sec
INFO:root:2024-03-30 09:25:51, Train, Epoch : 7, Step : 3200, Loss : 0.61927, Acc : 0.753, Sensitive_Loss : 0.23277, Sensitive_Acc : 16.600, Run Time : 10.31 sec
INFO:root:2024-03-30 09:27:51, Dev, Step : 3200, Loss : 0.92648, Acc : 0.769, Auc : 0.785, Sensitive_Loss : 0.26268, Sensitive_Acc : 16.610, Sensitive_Auc : 0.992, Mean auc: 0.785, Run Time : 119.32 sec
INFO:root:2024-03-30 09:27:58, Train, Epoch : 7, Step : 3210, Loss : 0.76124, Acc : 0.734, Sensitive_Loss : 0.18437, Sensitive_Acc : 17.600, Run Time : 126.55 sec
INFO:root:2024-03-30 09:28:08, Train, Epoch : 7, Step : 3220, Loss : 0.67651, Acc : 0.731, Sensitive_Loss : 0.17651, Sensitive_Acc : 18.000, Run Time : 10.32 sec
INFO:root:2024-03-30 09:28:19, Train, Epoch : 7, Step : 3230, Loss : 0.85279, Acc : 0.756, Sensitive_Loss : 0.20169, Sensitive_Acc : 17.400, Run Time : 10.60 sec
INFO:root:2024-03-30 09:28:29, Train, Epoch : 7, Step : 3240, Loss : 0.69769, Acc : 0.759, Sensitive_Loss : 0.15994, Sensitive_Acc : 17.100, Run Time : 10.20 sec
INFO:root:2024-03-30 09:28:39, Train, Epoch : 7, Step : 3250, Loss : 0.73169, Acc : 0.756, Sensitive_Loss : 0.23802, Sensitive_Acc : 17.000, Run Time : 10.21 sec
INFO:root:2024-03-30 09:28:50, Train, Epoch : 7, Step : 3260, Loss : 0.75742, Acc : 0.734, Sensitive_Loss : 0.21242, Sensitive_Acc : 15.500, Run Time : 10.56 sec
INFO:root:2024-03-30 09:29:00, Train, Epoch : 7, Step : 3270, Loss : 0.76969, Acc : 0.728, Sensitive_Loss : 0.21231, Sensitive_Acc : 15.600, Run Time : 10.37 sec
INFO:root:2024-03-30 09:29:10, Train, Epoch : 7, Step : 3280, Loss : 0.67589, Acc : 0.769, Sensitive_Loss : 0.17354, Sensitive_Acc : 16.300, Run Time : 10.17 sec
INFO:root:2024-03-30 09:29:20, Train, Epoch : 7, Step : 3290, Loss : 0.71858, Acc : 0.797, Sensitive_Loss : 0.18960, Sensitive_Acc : 16.800, Run Time : 9.48 sec
INFO:root:2024-03-30 09:29:32, Train, Epoch : 7, Step : 3300, Loss : 0.89222, Acc : 0.744, Sensitive_Loss : 0.15017, Sensitive_Acc : 16.700, Run Time : 11.86 sec
INFO:root:2024-03-30 09:31:09, Dev, Step : 3300, Loss : 0.94451, Acc : 0.698, Auc : 0.782, Sensitive_Loss : 0.25144, Sensitive_Acc : 16.752, Sensitive_Auc : 0.993, Mean auc: 0.782, Run Time : 97.04 sec
INFO:root:2024-03-30 09:31:16, Train, Epoch : 7, Step : 3310, Loss : 0.76071, Acc : 0.784, Sensitive_Loss : 0.18070, Sensitive_Acc : 15.200, Run Time : 104.50 sec
INFO:root:2024-03-30 09:31:27, Train, Epoch : 7, Step : 3320, Loss : 0.77758, Acc : 0.769, Sensitive_Loss : 0.20876, Sensitive_Acc : 15.300, Run Time : 10.88 sec
INFO:root:2024-03-30 09:31:38, Train, Epoch : 7, Step : 3330, Loss : 0.67976, Acc : 0.716, Sensitive_Loss : 0.29153, Sensitive_Acc : 16.900, Run Time : 11.02 sec
INFO:root:2024-03-30 09:31:49, Train, Epoch : 7, Step : 3340, Loss : 0.73296, Acc : 0.728, Sensitive_Loss : 0.19391, Sensitive_Acc : 16.900, Run Time : 10.72 sec
INFO:root:2024-03-30 09:31:59, Train, Epoch : 7, Step : 3350, Loss : 0.72865, Acc : 0.772, Sensitive_Loss : 0.20825, Sensitive_Acc : 18.500, Run Time : 10.56 sec
INFO:root:2024-03-30 09:32:10, Train, Epoch : 7, Step : 3360, Loss : 0.62041, Acc : 0.753, Sensitive_Loss : 0.21358, Sensitive_Acc : 17.900, Run Time : 10.34 sec
INFO:root:2024-03-30 09:32:20, Train, Epoch : 7, Step : 3370, Loss : 0.86632, Acc : 0.734, Sensitive_Loss : 0.19948, Sensitive_Acc : 14.800, Run Time : 10.69 sec
INFO:root:2024-03-30 09:32:31, Train, Epoch : 7, Step : 3380, Loss : 0.73179, Acc : 0.728, Sensitive_Loss : 0.17651, Sensitive_Acc : 16.900, Run Time : 10.67 sec
INFO:root:2024-03-30 09:32:43, Train, Epoch : 7, Step : 3390, Loss : 0.78864, Acc : 0.747, Sensitive_Loss : 0.24828, Sensitive_Acc : 16.500, Run Time : 12.39 sec
INFO:root:2024-03-30 09:32:56, Train, Epoch : 7, Step : 3400, Loss : 0.64468, Acc : 0.787, Sensitive_Loss : 0.23105, Sensitive_Acc : 17.000, Run Time : 12.31 sec
INFO:root:2024-03-30 09:34:44, Dev, Step : 3400, Loss : 0.92548, Acc : 0.731, Auc : 0.790, Sensitive_Loss : 0.28801, Sensitive_Acc : 16.667, Sensitive_Auc : 0.992, Mean auc: 0.790, Run Time : 108.27 sec
INFO:root:2024-03-30 09:34:51, Train, Epoch : 7, Step : 3410, Loss : 0.68419, Acc : 0.753, Sensitive_Loss : 0.13864, Sensitive_Acc : 18.000, Run Time : 115.51 sec
INFO:root:2024-03-30 09:35:02, Train, Epoch : 7, Step : 3420, Loss : 0.66770, Acc : 0.716, Sensitive_Loss : 0.19849, Sensitive_Acc : 15.300, Run Time : 10.93 sec
INFO:root:2024-03-30 09:35:14, Train, Epoch : 7, Step : 3430, Loss : 0.95686, Acc : 0.747, Sensitive_Loss : 0.18937, Sensitive_Acc : 15.800, Run Time : 11.60 sec
INFO:root:2024-03-30 09:35:25, Train, Epoch : 7, Step : 3440, Loss : 0.62401, Acc : 0.753, Sensitive_Loss : 0.21570, Sensitive_Acc : 18.500, Run Time : 10.82 sec
INFO:root:2024-03-30 09:37:04
INFO:root:y_pred: [0.27653503 0.1814945  0.3855232  ... 0.5343582  0.7287386  0.22249614]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [5.5986680e-03 2.1235716e-01 9.7298867e-01 2.7005496e-02 9.9862540e-01
 3.2921482e-03 8.3674073e-02 3.7720278e-02 5.7667252e-02 5.3362835e-02
 1.3939492e-02 1.7528031e-02 9.9081928e-01 2.2183298e-03 9.9862218e-01
 9.2313933e-01 3.5077034e-04 2.5868222e-01 9.9947292e-01 7.3388195e-01
 2.8176093e-01 6.1372433e-02 9.9966824e-01 9.6715742e-01 1.8438634e-02
 8.7148327e-01 8.0655077e-03 9.9985087e-01 1.7153597e-02 9.4313747e-01
 9.9985147e-01 9.9104148e-01 1.6420659e-02 2.9034237e-04 2.8502250e-01
 4.1225277e-02 3.4979206e-02 8.3887763e-02 9.8026556e-01 9.6955305e-01
 1.6305108e-02 4.6383049e-02 9.9416047e-01 9.9855274e-01 2.4983744e-01
 3.8907029e-02 2.1571001e-02 3.5852718e-01 9.9948031e-01 9.0648580e-01
 9.9808866e-01 6.1878878e-01 9.9920589e-01 9.9729508e-01 7.6398802e-01
 9.6644217e-01 9.3109745e-01 1.4008645e-02 9.9998939e-01 1.2436358e-01
 1.9615684e-03 7.3369795e-01 9.7031063e-01 9.6375829e-01 9.4913512e-01
 9.9973613e-01 7.8863567e-01 9.9967015e-01 9.8853624e-01 9.9300450e-01
 9.9639904e-01 7.5502419e-03 1.1888503e-02 8.5158378e-01 9.7349995e-01
 3.1426623e-02 9.9687469e-01 6.6652155e-04 9.9917030e-01 5.9074777e-01
 5.2664465e-01 7.6192178e-02 2.2158097e-01 7.2682273e-01 9.9316204e-01
 4.3417966e-01 9.9287379e-01 2.0749891e-01 2.4754714e-02 5.0793160e-02
 3.9469771e-02 4.0978929e-01 2.0973673e-02 1.1522147e-02 9.9912208e-01
 8.0829513e-01 2.5215304e-01 1.5994821e-02 9.6379495e-01 9.3113196e-01
 9.8164898e-01 2.4339518e-02 9.9772865e-01 8.3079174e-02 2.3815165e-01
 9.9252558e-01 1.5725425e-01 2.4778938e-02 4.9008271e-03 8.6211655e-03
 3.1201094e-03 9.8326474e-01 3.2045823e-01 6.3657421e-03 6.0030878e-01
 8.3200855e-04 9.9842441e-01 1.4724863e-02 8.5567498e-01 7.9984451e-03
 6.6116673e-01 3.1208640e-02 9.9857473e-01 1.0653582e-02 2.7871989e-02
 9.9970585e-01 4.3257162e-02 9.9392891e-01 7.1597749e-01 9.9454790e-01
 7.4950287e-05 9.9885130e-01 7.3323334e-03 9.9982589e-01 9.8893505e-01
 1.6235779e-01 6.3056707e-01 3.1694359e-01 6.6926861e-03 9.9150217e-01
 9.7928119e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-30 09:37:04, Dev, Step : 3444, Loss : 0.93786, Acc : 0.686, Auc : 0.794, Sensitive_Loss : 0.28648, Sensitive_Acc : 16.667, Sensitive_Auc : 0.993, Mean auc: 0.794, Run Time : 95.43 sec
INFO:root:2024-03-30 09:37:12, Train, Epoch : 8, Step : 3450, Loss : 0.43970, Acc : 0.475, Sensitive_Loss : 0.17180, Sensitive_Acc : 8.400, Run Time : 7.09 sec
INFO:root:2024-03-30 09:37:23, Train, Epoch : 8, Step : 3460, Loss : 0.67567, Acc : 0.753, Sensitive_Loss : 0.19266, Sensitive_Acc : 16.700, Run Time : 10.78 sec
INFO:root:2024-03-30 09:37:34, Train, Epoch : 8, Step : 3470, Loss : 0.92831, Acc : 0.741, Sensitive_Loss : 0.20786, Sensitive_Acc : 14.500, Run Time : 11.47 sec
INFO:root:2024-03-30 09:37:45, Train, Epoch : 8, Step : 3480, Loss : 0.71662, Acc : 0.750, Sensitive_Loss : 0.21993, Sensitive_Acc : 16.200, Run Time : 10.75 sec
INFO:root:2024-03-30 09:37:56, Train, Epoch : 8, Step : 3490, Loss : 0.61151, Acc : 0.750, Sensitive_Loss : 0.16965, Sensitive_Acc : 17.400, Run Time : 10.48 sec
INFO:root:2024-03-30 09:38:06, Train, Epoch : 8, Step : 3500, Loss : 0.60673, Acc : 0.769, Sensitive_Loss : 0.22079, Sensitive_Acc : 19.400, Run Time : 10.56 sec
INFO:root:2024-03-30 09:39:58, Dev, Step : 3500, Loss : 0.91592, Acc : 0.731, Auc : 0.797, Sensitive_Loss : 0.25500, Sensitive_Acc : 16.709, Sensitive_Auc : 0.993, Mean auc: 0.797, Run Time : 111.30 sec
INFO:root:2024-03-30 09:39:58, Best, Step : 3500, Loss : 0.91592, Acc : 0.731, Auc : 0.797, Sensitive_Loss : 0.25500, Sensitive_Acc : 16.709, Sensitive_Auc : 0.993, Best Auc : 0.797
INFO:root:2024-03-30 09:40:06, Train, Epoch : 8, Step : 3510, Loss : 0.60466, Acc : 0.716, Sensitive_Loss : 0.22302, Sensitive_Acc : 16.600, Run Time : 119.52 sec
INFO:root:2024-03-30 09:40:17, Train, Epoch : 8, Step : 3520, Loss : 0.70568, Acc : 0.756, Sensitive_Loss : 0.18320, Sensitive_Acc : 17.700, Run Time : 10.96 sec
INFO:root:2024-03-30 09:40:28, Train, Epoch : 8, Step : 3530, Loss : 0.68849, Acc : 0.750, Sensitive_Loss : 0.24745, Sensitive_Acc : 18.400, Run Time : 11.72 sec
INFO:root:2024-03-30 09:40:39, Train, Epoch : 8, Step : 3540, Loss : 0.76117, Acc : 0.778, Sensitive_Loss : 0.23176, Sensitive_Acc : 15.100, Run Time : 10.31 sec
INFO:root:2024-03-30 09:40:50, Train, Epoch : 8, Step : 3550, Loss : 0.71309, Acc : 0.784, Sensitive_Loss : 0.28194, Sensitive_Acc : 15.800, Run Time : 11.78 sec
INFO:root:2024-03-30 09:41:03, Train, Epoch : 8, Step : 3560, Loss : 0.75262, Acc : 0.744, Sensitive_Loss : 0.22227, Sensitive_Acc : 16.500, Run Time : 12.20 sec
INFO:root:2024-03-30 09:41:14, Train, Epoch : 8, Step : 3570, Loss : 0.65306, Acc : 0.766, Sensitive_Loss : 0.18670, Sensitive_Acc : 18.000, Run Time : 10.98 sec
INFO:root:2024-03-30 09:41:25, Train, Epoch : 8, Step : 3580, Loss : 0.72216, Acc : 0.787, Sensitive_Loss : 0.22041, Sensitive_Acc : 14.000, Run Time : 11.35 sec
INFO:root:2024-03-30 09:41:36, Train, Epoch : 8, Step : 3590, Loss : 0.67113, Acc : 0.784, Sensitive_Loss : 0.20830, Sensitive_Acc : 19.300, Run Time : 10.52 sec
INFO:root:2024-03-30 09:41:46, Train, Epoch : 8, Step : 3600, Loss : 0.76226, Acc : 0.750, Sensitive_Loss : 0.19239, Sensitive_Acc : 17.300, Run Time : 10.14 sec
INFO:root:2024-03-30 09:43:38, Dev, Step : 3600, Loss : 0.92033, Acc : 0.730, Auc : 0.796, Sensitive_Loss : 0.24964, Sensitive_Acc : 16.738, Sensitive_Auc : 0.993, Mean auc: 0.796, Run Time : 111.85 sec
INFO:root:2024-03-30 09:43:47, Train, Epoch : 8, Step : 3610, Loss : 0.70194, Acc : 0.716, Sensitive_Loss : 0.19267, Sensitive_Acc : 15.900, Run Time : 120.85 sec
INFO:root:2024-03-30 09:43:57, Train, Epoch : 8, Step : 3620, Loss : 0.63375, Acc : 0.719, Sensitive_Loss : 0.20110, Sensitive_Acc : 17.800, Run Time : 10.82 sec
INFO:root:2024-03-30 09:44:07, Train, Epoch : 8, Step : 3630, Loss : 0.67941, Acc : 0.784, Sensitive_Loss : 0.23368, Sensitive_Acc : 16.500, Run Time : 10.06 sec
INFO:root:2024-03-30 09:44:17, Train, Epoch : 8, Step : 3640, Loss : 0.54069, Acc : 0.719, Sensitive_Loss : 0.20761, Sensitive_Acc : 17.600, Run Time : 9.90 sec
INFO:root:2024-03-30 09:44:28, Train, Epoch : 8, Step : 3650, Loss : 0.70047, Acc : 0.750, Sensitive_Loss : 0.22546, Sensitive_Acc : 16.800, Run Time : 10.57 sec
INFO:root:2024-03-30 09:44:38, Train, Epoch : 8, Step : 3660, Loss : 0.68097, Acc : 0.775, Sensitive_Loss : 0.18045, Sensitive_Acc : 17.400, Run Time : 10.45 sec
INFO:root:2024-03-30 09:44:48, Train, Epoch : 8, Step : 3670, Loss : 0.81278, Acc : 0.738, Sensitive_Loss : 0.21628, Sensitive_Acc : 16.100, Run Time : 9.83 sec
INFO:root:2024-03-30 09:44:59, Train, Epoch : 8, Step : 3680, Loss : 0.73306, Acc : 0.722, Sensitive_Loss : 0.17909, Sensitive_Acc : 16.600, Run Time : 10.43 sec
INFO:root:2024-03-30 09:45:09, Train, Epoch : 8, Step : 3690, Loss : 0.66301, Acc : 0.750, Sensitive_Loss : 0.21034, Sensitive_Acc : 17.300, Run Time : 10.25 sec
INFO:root:2024-03-30 09:45:19, Train, Epoch : 8, Step : 3700, Loss : 0.66388, Acc : 0.747, Sensitive_Loss : 0.20680, Sensitive_Acc : 17.700, Run Time : 10.42 sec
INFO:root:2024-03-30 09:46:56, Dev, Step : 3700, Loss : 0.92419, Acc : 0.765, Auc : 0.790, Sensitive_Loss : 0.25095, Sensitive_Acc : 16.695, Sensitive_Auc : 0.992, Mean auc: 0.790, Run Time : 97.09 sec
INFO:root:2024-03-30 09:47:04, Train, Epoch : 8, Step : 3710, Loss : 0.76450, Acc : 0.731, Sensitive_Loss : 0.24731, Sensitive_Acc : 16.200, Run Time : 104.56 sec
INFO:root:2024-03-30 09:47:18, Train, Epoch : 8, Step : 3720, Loss : 0.48635, Acc : 0.784, Sensitive_Loss : 0.19146, Sensitive_Acc : 16.500, Run Time : 13.83 sec
INFO:root:2024-03-30 09:47:28, Train, Epoch : 8, Step : 3730, Loss : 0.70291, Acc : 0.778, Sensitive_Loss : 0.26243, Sensitive_Acc : 17.800, Run Time : 10.65 sec
INFO:root:2024-03-30 09:47:39, Train, Epoch : 8, Step : 3740, Loss : 0.75144, Acc : 0.769, Sensitive_Loss : 0.21414, Sensitive_Acc : 16.600, Run Time : 10.73 sec
INFO:root:2024-03-30 09:47:50, Train, Epoch : 8, Step : 3750, Loss : 0.76336, Acc : 0.725, Sensitive_Loss : 0.22630, Sensitive_Acc : 17.100, Run Time : 10.73 sec
INFO:root:2024-03-30 09:48:01, Train, Epoch : 8, Step : 3760, Loss : 0.69090, Acc : 0.775, Sensitive_Loss : 0.20323, Sensitive_Acc : 14.500, Run Time : 10.91 sec
INFO:root:2024-03-30 09:48:12, Train, Epoch : 8, Step : 3770, Loss : 0.76316, Acc : 0.747, Sensitive_Loss : 0.21725, Sensitive_Acc : 18.100, Run Time : 11.72 sec
INFO:root:2024-03-30 09:48:26, Train, Epoch : 8, Step : 3780, Loss : 0.78198, Acc : 0.775, Sensitive_Loss : 0.21151, Sensitive_Acc : 16.300, Run Time : 13.12 sec
INFO:root:2024-03-30 09:48:36, Train, Epoch : 8, Step : 3790, Loss : 0.69897, Acc : 0.766, Sensitive_Loss : 0.21331, Sensitive_Acc : 16.800, Run Time : 10.06 sec
INFO:root:2024-03-30 09:48:46, Train, Epoch : 8, Step : 3800, Loss : 0.56684, Acc : 0.775, Sensitive_Loss : 0.18706, Sensitive_Acc : 16.400, Run Time : 10.45 sec
INFO:root:2024-03-30 09:50:23, Dev, Step : 3800, Loss : 0.93661, Acc : 0.743, Auc : 0.782, Sensitive_Loss : 0.25850, Sensitive_Acc : 16.809, Sensitive_Auc : 0.993, Mean auc: 0.782, Run Time : 97.34 sec
INFO:root:2024-03-30 09:50:31, Train, Epoch : 8, Step : 3810, Loss : 0.73771, Acc : 0.731, Sensitive_Loss : 0.22176, Sensitive_Acc : 18.000, Run Time : 104.70 sec
INFO:root:2024-03-30 09:50:43, Train, Epoch : 8, Step : 3820, Loss : 0.66992, Acc : 0.784, Sensitive_Loss : 0.20414, Sensitive_Acc : 17.400, Run Time : 12.65 sec
INFO:root:2024-03-30 09:50:54, Train, Epoch : 8, Step : 3830, Loss : 0.72678, Acc : 0.778, Sensitive_Loss : 0.22287, Sensitive_Acc : 14.900, Run Time : 10.90 sec
INFO:root:2024-03-30 09:51:05, Train, Epoch : 8, Step : 3840, Loss : 0.70841, Acc : 0.759, Sensitive_Loss : 0.20738, Sensitive_Acc : 15.900, Run Time : 10.68 sec
INFO:root:2024-03-30 09:51:18, Train, Epoch : 8, Step : 3850, Loss : 0.70462, Acc : 0.784, Sensitive_Loss : 0.15435, Sensitive_Acc : 17.300, Run Time : 12.83 sec
INFO:root:2024-03-30 09:51:29, Train, Epoch : 8, Step : 3860, Loss : 0.88601, Acc : 0.744, Sensitive_Loss : 0.15495, Sensitive_Acc : 17.400, Run Time : 11.28 sec
INFO:root:2024-03-30 09:51:40, Train, Epoch : 8, Step : 3870, Loss : 0.56485, Acc : 0.759, Sensitive_Loss : 0.20934, Sensitive_Acc : 15.900, Run Time : 10.86 sec
INFO:root:2024-03-30 09:51:51, Train, Epoch : 8, Step : 3880, Loss : 0.82501, Acc : 0.744, Sensitive_Loss : 0.17584, Sensitive_Acc : 15.800, Run Time : 10.99 sec
INFO:root:2024-03-30 09:52:02, Train, Epoch : 8, Step : 3890, Loss : 0.75829, Acc : 0.759, Sensitive_Loss : 0.23271, Sensitive_Acc : 16.600, Run Time : 11.39 sec
INFO:root:2024-03-30 09:52:13, Train, Epoch : 8, Step : 3900, Loss : 0.67960, Acc : 0.781, Sensitive_Loss : 0.17247, Sensitive_Acc : 15.900, Run Time : 10.43 sec
INFO:root:2024-03-30 09:53:51, Dev, Step : 3900, Loss : 0.91428, Acc : 0.726, Auc : 0.797, Sensitive_Loss : 0.24888, Sensitive_Acc : 16.780, Sensitive_Auc : 0.993, Mean auc: 0.797, Run Time : 97.80 sec
INFO:root:2024-03-30 09:53:51, Best, Step : 3900, Loss : 0.91428, Acc : 0.726, Auc : 0.797, Sensitive_Loss : 0.24888, Sensitive_Acc : 16.780, Sensitive_Auc : 0.993, Best Auc : 0.797
INFO:root:2024-03-30 09:53:59, Train, Epoch : 8, Step : 3910, Loss : 0.83713, Acc : 0.762, Sensitive_Loss : 0.16222, Sensitive_Acc : 16.300, Run Time : 106.50 sec
INFO:root:2024-03-30 09:54:12, Train, Epoch : 8, Step : 3920, Loss : 0.67512, Acc : 0.803, Sensitive_Loss : 0.20616, Sensitive_Acc : 18.900, Run Time : 12.52 sec
INFO:root:2024-03-30 09:54:23, Train, Epoch : 8, Step : 3930, Loss : 0.85809, Acc : 0.734, Sensitive_Loss : 0.23167, Sensitive_Acc : 16.600, Run Time : 10.95 sec
INFO:root:2024-03-30 09:56:05
INFO:root:y_pred: [0.26130936 0.15285708 0.28042346 ... 0.36630338 0.5457518  0.23504522]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [4.68249107e-03 6.00763112e-02 9.57768202e-01 1.77722648e-02
 9.96822000e-01 2.68841628e-03 4.79902811e-02 2.52406411e-02
 4.44360487e-02 2.43304558e-02 1.07445959e-02 1.82293169e-02
 9.89830077e-01 1.29692431e-03 9.99335587e-01 9.58962977e-01
 5.59284003e-04 1.96192518e-01 9.99745071e-01 6.55073404e-01
 1.01259351e-01 3.89721766e-02 9.99556005e-01 9.42447066e-01
 1.32043874e-02 7.72158086e-01 4.90248296e-03 9.99757946e-01
 8.96624010e-03 8.96513224e-01 9.99855399e-01 9.95958149e-01
 1.22586135e-02 1.30388289e-04 1.41606584e-01 3.41871418e-02
 4.39893305e-02 1.06855087e-01 9.76704717e-01 9.55646336e-01
 2.18450259e-02 1.66943464e-02 9.91604388e-01 9.98775065e-01
 1.53438240e-01 8.46894234e-02 8.31836276e-03 2.53639251e-01
 9.99531865e-01 9.24411178e-01 9.95704353e-01 4.84873086e-01
 9.99073029e-01 9.98696983e-01 6.76635265e-01 9.56628263e-01
 9.29726183e-01 8.53449572e-03 9.99994636e-01 7.90162534e-02
 1.02217298e-03 6.95111096e-01 9.76282954e-01 9.51374233e-01
 9.45365906e-01 9.99491692e-01 8.01207185e-01 9.99681592e-01
 9.90116179e-01 9.88524497e-01 9.95150447e-01 5.07800560e-03
 1.25470348e-02 7.39665866e-01 9.52008963e-01 4.75773774e-02
 9.95729029e-01 3.68474954e-04 9.99607027e-01 3.86901110e-01
 4.88263309e-01 5.99425919e-02 2.13861674e-01 6.04250431e-01
 9.96175528e-01 4.30659860e-01 9.92415786e-01 2.04711482e-01
 2.06023287e-02 1.33233229e-02 3.76712829e-02 4.46414948e-01
 1.53313354e-02 1.67288054e-02 9.98453021e-01 8.27377439e-01
 2.53348529e-01 1.02035394e-02 9.58200455e-01 9.08978105e-01
 9.71374571e-01 8.85147415e-03 9.98981893e-01 6.99037313e-02
 1.33180544e-01 9.93990183e-01 1.31258056e-01 2.53569186e-02
 2.25133612e-03 4.75371536e-03 2.04091077e-03 9.58583593e-01
 3.00236791e-01 3.02250683e-03 4.58911061e-01 5.94785437e-04
 9.97797489e-01 6.62621623e-03 7.94114053e-01 6.61796983e-03
 5.68809509e-01 1.22578284e-02 9.98132050e-01 8.15423857e-03
 2.34195087e-02 9.99688029e-01 2.53788251e-02 9.92334247e-01
 5.66359580e-01 9.88343835e-01 3.53047362e-05 9.99182284e-01
 9.41923354e-03 9.99701798e-01 9.83799577e-01 1.29548073e-01
 6.82956338e-01 1.47545204e-01 6.55891187e-03 9.69815135e-01
 9.81963396e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-30 09:56:05, Dev, Step : 3936, Loss : 0.92501, Acc : 0.754, Auc : 0.789, Sensitive_Loss : 0.25391, Sensitive_Acc : 16.823, Sensitive_Auc : 0.993, Mean auc: 0.789, Run Time : 95.77 sec
INFO:root:2024-03-30 09:56:11, Train, Epoch : 9, Step : 3940, Loss : 0.27500, Acc : 0.328, Sensitive_Loss : 0.10588, Sensitive_Acc : 5.400, Run Time : 5.23 sec
INFO:root:2024-03-30 09:56:28, Train, Epoch : 9, Step : 3950, Loss : 0.61947, Acc : 0.766, Sensitive_Loss : 0.26182, Sensitive_Acc : 16.700, Run Time : 16.96 sec
INFO:root:2024-03-30 09:56:39, Train, Epoch : 9, Step : 3960, Loss : 0.69739, Acc : 0.753, Sensitive_Loss : 0.20691, Sensitive_Acc : 18.900, Run Time : 11.23 sec
INFO:root:2024-03-30 09:56:52, Train, Epoch : 9, Step : 3970, Loss : 0.63276, Acc : 0.803, Sensitive_Loss : 0.17058, Sensitive_Acc : 14.900, Run Time : 13.40 sec
INFO:root:2024-03-30 09:57:03, Train, Epoch : 9, Step : 3980, Loss : 0.53211, Acc : 0.747, Sensitive_Loss : 0.22537, Sensitive_Acc : 17.300, Run Time : 11.05 sec
INFO:root:2024-03-30 09:57:14, Train, Epoch : 9, Step : 3990, Loss : 0.73042, Acc : 0.766, Sensitive_Loss : 0.18799, Sensitive_Acc : 15.900, Run Time : 10.92 sec
INFO:root:2024-03-30 09:57:26, Train, Epoch : 9, Step : 4000, Loss : 0.78226, Acc : 0.722, Sensitive_Loss : 0.28892, Sensitive_Acc : 18.200, Run Time : 12.05 sec
INFO:root:2024-03-30 09:59:10, Dev, Step : 4000, Loss : 0.94298, Acc : 0.730, Auc : 0.783, Sensitive_Loss : 0.25747, Sensitive_Acc : 16.823, Sensitive_Auc : 0.991, Mean auc: 0.783, Run Time : 104.04 sec
INFO:root:2024-03-30 09:59:18, Train, Epoch : 9, Step : 4010, Loss : 0.63743, Acc : 0.756, Sensitive_Loss : 0.21521, Sensitive_Acc : 16.400, Run Time : 111.40 sec
INFO:root:2024-03-30 09:59:29, Train, Epoch : 9, Step : 4020, Loss : 0.57943, Acc : 0.769, Sensitive_Loss : 0.19669, Sensitive_Acc : 16.500, Run Time : 11.34 sec
INFO:root:2024-03-30 09:59:40, Train, Epoch : 9, Step : 4030, Loss : 0.60698, Acc : 0.734, Sensitive_Loss : 0.16865, Sensitive_Acc : 14.700, Run Time : 11.10 sec
INFO:root:2024-03-30 09:59:54, Train, Epoch : 9, Step : 4040, Loss : 0.72184, Acc : 0.769, Sensitive_Loss : 0.20887, Sensitive_Acc : 18.100, Run Time : 14.31 sec
INFO:root:2024-03-30 10:00:05, Train, Epoch : 9, Step : 4050, Loss : 0.71699, Acc : 0.753, Sensitive_Loss : 0.25887, Sensitive_Acc : 18.200, Run Time : 10.35 sec
INFO:root:2024-03-30 10:00:15, Train, Epoch : 9, Step : 4060, Loss : 0.59950, Acc : 0.766, Sensitive_Loss : 0.22471, Sensitive_Acc : 17.500, Run Time : 10.64 sec
INFO:root:2024-03-30 10:00:28, Train, Epoch : 9, Step : 4070, Loss : 0.63593, Acc : 0.766, Sensitive_Loss : 0.16067, Sensitive_Acc : 16.400, Run Time : 12.74 sec
INFO:root:2024-03-30 10:00:42, Train, Epoch : 9, Step : 4080, Loss : 0.61619, Acc : 0.775, Sensitive_Loss : 0.21567, Sensitive_Acc : 16.700, Run Time : 13.36 sec
INFO:root:2024-03-30 10:00:52, Train, Epoch : 9, Step : 4090, Loss : 0.70792, Acc : 0.797, Sensitive_Loss : 0.16670, Sensitive_Acc : 13.500, Run Time : 10.29 sec
INFO:root:2024-03-30 10:01:05, Train, Epoch : 9, Step : 4100, Loss : 0.63918, Acc : 0.756, Sensitive_Loss : 0.19267, Sensitive_Acc : 16.200, Run Time : 12.72 sec
INFO:root:2024-03-30 10:02:42, Dev, Step : 4100, Loss : 0.94504, Acc : 0.719, Auc : 0.786, Sensitive_Loss : 0.22143, Sensitive_Acc : 16.894, Sensitive_Auc : 0.992, Mean auc: 0.786, Run Time : 97.09 sec
INFO:root:2024-03-30 10:02:49, Train, Epoch : 9, Step : 4110, Loss : 0.70794, Acc : 0.759, Sensitive_Loss : 0.19877, Sensitive_Acc : 17.500, Run Time : 104.72 sec
INFO:root:2024-03-30 10:03:01, Train, Epoch : 9, Step : 4120, Loss : 0.57016, Acc : 0.781, Sensitive_Loss : 0.21990, Sensitive_Acc : 16.800, Run Time : 11.29 sec
INFO:root:2024-03-30 10:03:13, Train, Epoch : 9, Step : 4130, Loss : 0.72341, Acc : 0.741, Sensitive_Loss : 0.21600, Sensitive_Acc : 18.200, Run Time : 11.99 sec
INFO:root:2024-03-30 10:03:24, Train, Epoch : 9, Step : 4140, Loss : 0.56506, Acc : 0.812, Sensitive_Loss : 0.18939, Sensitive_Acc : 15.800, Run Time : 11.30 sec
INFO:root:2024-03-30 10:03:34, Train, Epoch : 9, Step : 4150, Loss : 0.60179, Acc : 0.797, Sensitive_Loss : 0.22590, Sensitive_Acc : 17.200, Run Time : 10.40 sec
INFO:root:2024-03-30 10:03:45, Train, Epoch : 9, Step : 4160, Loss : 0.49059, Acc : 0.775, Sensitive_Loss : 0.18272, Sensitive_Acc : 17.800, Run Time : 11.08 sec
INFO:root:2024-03-30 10:03:56, Train, Epoch : 9, Step : 4170, Loss : 0.65567, Acc : 0.762, Sensitive_Loss : 0.20362, Sensitive_Acc : 16.200, Run Time : 10.64 sec
INFO:root:2024-03-30 10:04:07, Train, Epoch : 9, Step : 4180, Loss : 0.55525, Acc : 0.816, Sensitive_Loss : 0.27418, Sensitive_Acc : 16.200, Run Time : 10.66 sec
INFO:root:2024-03-30 10:04:19, Train, Epoch : 9, Step : 4190, Loss : 0.74259, Acc : 0.753, Sensitive_Loss : 0.19065, Sensitive_Acc : 17.600, Run Time : 12.65 sec
INFO:root:2024-03-30 10:04:32, Train, Epoch : 9, Step : 4200, Loss : 0.77666, Acc : 0.772, Sensitive_Loss : 0.16389, Sensitive_Acc : 14.500, Run Time : 12.93 sec
INFO:root:2024-03-30 10:06:10, Dev, Step : 4200, Loss : 0.95140, Acc : 0.790, Auc : 0.784, Sensitive_Loss : 0.23034, Sensitive_Acc : 16.936, Sensitive_Auc : 0.993, Mean auc: 0.784, Run Time : 97.97 sec
INFO:root:2024-03-30 10:06:18, Train, Epoch : 9, Step : 4210, Loss : 0.51849, Acc : 0.812, Sensitive_Loss : 0.15594, Sensitive_Acc : 17.500, Run Time : 105.35 sec
INFO:root:2024-03-30 10:06:28, Train, Epoch : 9, Step : 4220, Loss : 0.68314, Acc : 0.797, Sensitive_Loss : 0.19265, Sensitive_Acc : 14.400, Run Time : 10.87 sec
INFO:root:2024-03-30 10:06:39, Train, Epoch : 9, Step : 4230, Loss : 0.72702, Acc : 0.784, Sensitive_Loss : 0.15853, Sensitive_Acc : 15.800, Run Time : 10.98 sec
INFO:root:2024-03-30 10:06:50, Train, Epoch : 9, Step : 4240, Loss : 0.49438, Acc : 0.759, Sensitive_Loss : 0.23536, Sensitive_Acc : 13.700, Run Time : 10.84 sec
INFO:root:2024-03-30 10:07:01, Train, Epoch : 9, Step : 4250, Loss : 0.67533, Acc : 0.762, Sensitive_Loss : 0.19119, Sensitive_Acc : 15.000, Run Time : 10.30 sec
INFO:root:2024-03-30 10:07:12, Train, Epoch : 9, Step : 4260, Loss : 0.62622, Acc : 0.787, Sensitive_Loss : 0.22224, Sensitive_Acc : 14.300, Run Time : 10.98 sec
INFO:root:2024-03-30 10:07:22, Train, Epoch : 9, Step : 4270, Loss : 0.72191, Acc : 0.766, Sensitive_Loss : 0.19294, Sensitive_Acc : 16.500, Run Time : 10.24 sec
INFO:root:2024-03-30 10:07:32, Train, Epoch : 9, Step : 4280, Loss : 0.75283, Acc : 0.772, Sensitive_Loss : 0.19086, Sensitive_Acc : 16.300, Run Time : 10.60 sec
INFO:root:2024-03-30 10:07:47, Train, Epoch : 9, Step : 4290, Loss : 0.64559, Acc : 0.797, Sensitive_Loss : 0.19296, Sensitive_Acc : 16.500, Run Time : 14.21 sec
INFO:root:2024-03-30 10:07:58, Train, Epoch : 9, Step : 4300, Loss : 0.75511, Acc : 0.766, Sensitive_Loss : 0.17119, Sensitive_Acc : 17.400, Run Time : 11.51 sec
INFO:root:2024-03-30 10:09:35, Dev, Step : 4300, Loss : 0.94147, Acc : 0.741, Auc : 0.791, Sensitive_Loss : 0.22678, Sensitive_Acc : 16.766, Sensitive_Auc : 0.993, Mean auc: 0.791, Run Time : 97.41 sec
INFO:root:2024-03-30 10:09:43, Train, Epoch : 9, Step : 4310, Loss : 0.64292, Acc : 0.787, Sensitive_Loss : 0.18880, Sensitive_Acc : 16.900, Run Time : 104.75 sec
INFO:root:2024-03-30 10:09:55, Train, Epoch : 9, Step : 4320, Loss : 0.64180, Acc : 0.756, Sensitive_Loss : 0.16644, Sensitive_Acc : 17.700, Run Time : 12.18 sec
INFO:root:2024-03-30 10:10:08, Train, Epoch : 9, Step : 4330, Loss : 0.61250, Acc : 0.806, Sensitive_Loss : 0.15949, Sensitive_Acc : 17.400, Run Time : 13.09 sec
INFO:root:2024-03-30 10:10:19, Train, Epoch : 9, Step : 4340, Loss : 0.70369, Acc : 0.766, Sensitive_Loss : 0.19558, Sensitive_Acc : 18.100, Run Time : 10.74 sec
INFO:root:2024-03-30 10:10:29, Train, Epoch : 9, Step : 4350, Loss : 0.59292, Acc : 0.769, Sensitive_Loss : 0.19145, Sensitive_Acc : 16.300, Run Time : 10.45 sec
INFO:root:2024-03-30 10:10:44, Train, Epoch : 9, Step : 4360, Loss : 0.67634, Acc : 0.762, Sensitive_Loss : 0.23799, Sensitive_Acc : 17.600, Run Time : 14.56 sec
INFO:root:2024-03-30 10:10:55, Train, Epoch : 9, Step : 4370, Loss : 0.62463, Acc : 0.816, Sensitive_Loss : 0.17820, Sensitive_Acc : 16.600, Run Time : 11.59 sec
INFO:root:2024-03-30 10:11:06, Train, Epoch : 9, Step : 4380, Loss : 0.77334, Acc : 0.766, Sensitive_Loss : 0.19148, Sensitive_Acc : 15.900, Run Time : 10.30 sec
INFO:root:2024-03-30 10:11:19, Train, Epoch : 9, Step : 4390, Loss : 0.54334, Acc : 0.800, Sensitive_Loss : 0.21660, Sensitive_Acc : 16.700, Run Time : 13.58 sec
INFO:root:2024-03-30 10:11:30, Train, Epoch : 9, Step : 4400, Loss : 0.67989, Acc : 0.772, Sensitive_Loss : 0.21701, Sensitive_Acc : 18.700, Run Time : 10.37 sec
INFO:root:2024-03-30 10:13:48, Dev, Step : 4400, Loss : 0.96087, Acc : 0.798, Auc : 0.784, Sensitive_Loss : 0.23059, Sensitive_Acc : 16.894, Sensitive_Auc : 0.993, Mean auc: 0.784, Run Time : 138.75 sec
INFO:root:2024-03-30 10:13:57, Train, Epoch : 9, Step : 4410, Loss : 0.63553, Acc : 0.819, Sensitive_Loss : 0.19811, Sensitive_Acc : 14.100, Run Time : 146.81 sec
INFO:root:2024-03-30 10:14:07, Train, Epoch : 9, Step : 4420, Loss : 0.71297, Acc : 0.778, Sensitive_Loss : 0.16292, Sensitive_Acc : 17.100, Run Time : 10.97 sec
INFO:root:2024-03-30 10:15:52
INFO:root:y_pred: [0.18944052 0.2524894  0.3461548  ... 0.38717    0.4589858  0.15482654]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [3.37209809e-03 9.27718431e-02 9.30130482e-01 1.05362125e-02
 9.95507121e-01 2.24913843e-03 1.05526792e-02 7.67713506e-03
 9.16499179e-03 7.52800703e-03 3.84930475e-03 7.08612753e-03
 9.81922925e-01 6.50923175e-04 9.98257220e-01 8.51362824e-01
 3.99040175e-04 1.16997063e-01 9.98661876e-01 5.72471976e-01
 4.85563502e-02 1.65220313e-02 9.99136031e-01 9.18678641e-01
 2.06841785e-03 7.80319035e-01 1.10019115e-03 9.99098897e-01
 2.54421798e-03 7.13422060e-01 9.99241352e-01 9.90422964e-01
 3.61464405e-03 5.74114601e-05 6.25560805e-02 1.16392886e-02
 1.29168304e-02 3.97223942e-02 9.30468202e-01 9.34500277e-01
 4.24820976e-03 9.49246250e-03 9.88563299e-01 9.93976057e-01
 1.14185333e-01 2.43348312e-02 3.31169995e-03 1.67412087e-01
 9.99200761e-01 9.11630929e-01 9.92880940e-01 2.62301981e-01
 9.96702015e-01 9.96903002e-01 5.34221709e-01 9.06163096e-01
 8.34033906e-01 6.14811014e-03 9.99985695e-01 9.77260992e-02
 2.72066158e-04 3.49103063e-01 9.46909070e-01 9.51488197e-01
 8.97696137e-01 9.99076247e-01 6.35606766e-01 9.99408841e-01
 9.84072268e-01 9.77603137e-01 9.89154160e-01 2.29473482e-03
 4.01580287e-03 2.79341012e-01 9.59926784e-01 1.66725684e-02
 9.91462111e-01 1.95973073e-04 9.99010801e-01 2.49931350e-01
 3.14046234e-01 2.17272788e-02 6.96762279e-02 4.55814511e-01
 9.84432697e-01 2.76822865e-01 9.84972715e-01 7.88792744e-02
 1.23309195e-02 7.38348765e-03 7.68921943e-03 1.36534274e-01
 7.50623504e-03 1.98679529e-02 9.98235583e-01 4.97735620e-01
 9.52326879e-02 7.32596125e-03 8.94343257e-01 9.13493216e-01
 9.01602387e-01 5.86542999e-03 9.96513307e-01 2.00985353e-02
 5.81587069e-02 9.84329224e-01 5.25438748e-02 9.41591058e-03
 1.04590482e-03 3.13512795e-03 1.00014661e-03 9.57422972e-01
 1.35812700e-01 1.80810213e-03 1.43367290e-01 2.82429595e-04
 9.95481133e-01 4.47499845e-03 4.54520673e-01 2.21273443e-03
 2.91899979e-01 4.59711347e-03 9.97165978e-01 5.37840882e-03
 3.60866939e-03 9.99708116e-01 1.55702885e-02 9.75732923e-01
 4.81611997e-01 9.96406257e-01 1.77273869e-05 9.97400761e-01
 4.18424234e-03 9.98895288e-01 9.52046812e-01 8.54535699e-02
 4.02064830e-01 6.47757873e-02 3.69138154e-03 9.15423632e-01
 9.04490948e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-30 10:15:52, Dev, Step : 4428, Loss : 0.95674, Acc : 0.754, Auc : 0.780, Sensitive_Loss : 0.20991, Sensitive_Acc : 17.121, Sensitive_Auc : 0.993, Mean auc: 0.780, Run Time : 95.52 sec
INFO:root:2024-03-30 10:15:56, Train, Epoch : 10, Step : 4430, Loss : 0.11079, Acc : 0.159, Sensitive_Loss : 0.02222, Sensitive_Acc : 2.700, Run Time : 2.85 sec
INFO:root:2024-03-30 10:16:06, Train, Epoch : 10, Step : 4440, Loss : 0.69039, Acc : 0.772, Sensitive_Loss : 0.22360, Sensitive_Acc : 16.600, Run Time : 10.42 sec
INFO:root:2024-03-30 10:16:20, Train, Epoch : 10, Step : 4450, Loss : 0.65647, Acc : 0.772, Sensitive_Loss : 0.17483, Sensitive_Acc : 16.800, Run Time : 13.56 sec
INFO:root:2024-03-30 10:16:33, Train, Epoch : 10, Step : 4460, Loss : 0.69864, Acc : 0.775, Sensitive_Loss : 0.17025, Sensitive_Acc : 16.300, Run Time : 13.34 sec
INFO:root:2024-03-30 10:16:45, Train, Epoch : 10, Step : 4470, Loss : 0.55700, Acc : 0.781, Sensitive_Loss : 0.20770, Sensitive_Acc : 17.000, Run Time : 11.84 sec
INFO:root:2024-03-30 10:16:58, Train, Epoch : 10, Step : 4480, Loss : 0.48910, Acc : 0.812, Sensitive_Loss : 0.20867, Sensitive_Acc : 16.300, Run Time : 13.02 sec
INFO:root:2024-03-30 10:17:10, Train, Epoch : 10, Step : 4490, Loss : 0.59036, Acc : 0.831, Sensitive_Loss : 0.17346, Sensitive_Acc : 14.200, Run Time : 12.23 sec
INFO:root:2024-03-30 10:17:20, Train, Epoch : 10, Step : 4500, Loss : 0.60476, Acc : 0.778, Sensitive_Loss : 0.20099, Sensitive_Acc : 14.900, Run Time : 10.37 sec
INFO:root:2024-03-30 10:19:02, Dev, Step : 4500, Loss : 0.93716, Acc : 0.784, Auc : 0.793, Sensitive_Loss : 0.23356, Sensitive_Acc : 16.922, Sensitive_Auc : 0.993, Mean auc: 0.793, Run Time : 101.50 sec
INFO:root:2024-03-30 10:19:09, Train, Epoch : 10, Step : 4510, Loss : 0.66338, Acc : 0.766, Sensitive_Loss : 0.16481, Sensitive_Acc : 17.300, Run Time : 108.90 sec
INFO:root:2024-03-30 10:19:20, Train, Epoch : 10, Step : 4520, Loss : 0.62418, Acc : 0.759, Sensitive_Loss : 0.18404, Sensitive_Acc : 14.700, Run Time : 11.04 sec
INFO:root:2024-03-30 10:19:31, Train, Epoch : 10, Step : 4530, Loss : 0.38822, Acc : 0.769, Sensitive_Loss : 0.14948, Sensitive_Acc : 17.200, Run Time : 10.93 sec
INFO:root:2024-03-30 10:19:45, Train, Epoch : 10, Step : 4540, Loss : 0.56791, Acc : 0.784, Sensitive_Loss : 0.16697, Sensitive_Acc : 15.600, Run Time : 14.06 sec
INFO:root:2024-03-30 10:19:56, Train, Epoch : 10, Step : 4550, Loss : 0.56480, Acc : 0.791, Sensitive_Loss : 0.22766, Sensitive_Acc : 16.200, Run Time : 11.14 sec
INFO:root:2024-03-30 10:20:07, Train, Epoch : 10, Step : 4560, Loss : 0.66585, Acc : 0.794, Sensitive_Loss : 0.18064, Sensitive_Acc : 17.300, Run Time : 10.55 sec
INFO:root:2024-03-30 10:20:18, Train, Epoch : 10, Step : 4570, Loss : 0.65210, Acc : 0.816, Sensitive_Loss : 0.17376, Sensitive_Acc : 16.300, Run Time : 10.88 sec
INFO:root:2024-03-30 10:20:28, Train, Epoch : 10, Step : 4580, Loss : 0.52062, Acc : 0.800, Sensitive_Loss : 0.17115, Sensitive_Acc : 15.400, Run Time : 10.51 sec
INFO:root:2024-03-30 10:20:39, Train, Epoch : 10, Step : 4590, Loss : 0.61097, Acc : 0.794, Sensitive_Loss : 0.16833, Sensitive_Acc : 15.100, Run Time : 10.65 sec
INFO:root:2024-03-30 10:20:51, Train, Epoch : 10, Step : 4600, Loss : 0.60907, Acc : 0.819, Sensitive_Loss : 0.17427, Sensitive_Acc : 16.300, Run Time : 11.68 sec
INFO:root:2024-03-30 10:22:28, Dev, Step : 4600, Loss : 0.96792, Acc : 0.785, Auc : 0.781, Sensitive_Loss : 0.21875, Sensitive_Acc : 16.865, Sensitive_Auc : 0.993, Mean auc: 0.781, Run Time : 96.83 sec
INFO:root:2024-03-30 10:22:35, Train, Epoch : 10, Step : 4610, Loss : 0.48057, Acc : 0.791, Sensitive_Loss : 0.20423, Sensitive_Acc : 17.000, Run Time : 104.26 sec
INFO:root:2024-03-30 10:22:49, Train, Epoch : 10, Step : 4620, Loss : 0.55742, Acc : 0.816, Sensitive_Loss : 0.28424, Sensitive_Acc : 15.800, Run Time : 13.74 sec
INFO:root:2024-03-30 10:22:59, Train, Epoch : 10, Step : 4630, Loss : 0.74292, Acc : 0.744, Sensitive_Loss : 0.19049, Sensitive_Acc : 17.200, Run Time : 10.64 sec
INFO:root:2024-03-30 10:23:10, Train, Epoch : 10, Step : 4640, Loss : 0.51762, Acc : 0.769, Sensitive_Loss : 0.20390, Sensitive_Acc : 17.100, Run Time : 10.53 sec
INFO:root:2024-03-30 10:23:21, Train, Epoch : 10, Step : 4650, Loss : 0.64068, Acc : 0.778, Sensitive_Loss : 0.21337, Sensitive_Acc : 17.900, Run Time : 11.18 sec
INFO:root:2024-03-30 10:23:32, Train, Epoch : 10, Step : 4660, Loss : 0.63848, Acc : 0.772, Sensitive_Loss : 0.19883, Sensitive_Acc : 16.600, Run Time : 10.59 sec
INFO:root:2024-03-30 10:23:43, Train, Epoch : 10, Step : 4670, Loss : 0.74650, Acc : 0.797, Sensitive_Loss : 0.15057, Sensitive_Acc : 16.100, Run Time : 11.00 sec
INFO:root:2024-03-30 10:23:55, Train, Epoch : 10, Step : 4680, Loss : 0.61168, Acc : 0.822, Sensitive_Loss : 0.19950, Sensitive_Acc : 16.400, Run Time : 12.26 sec
INFO:root:2024-03-30 10:24:06, Train, Epoch : 10, Step : 4690, Loss : 0.75131, Acc : 0.778, Sensitive_Loss : 0.18072, Sensitive_Acc : 15.100, Run Time : 10.76 sec
INFO:root:2024-03-30 10:24:16, Train, Epoch : 10, Step : 4700, Loss : 0.56344, Acc : 0.784, Sensitive_Loss : 0.22225, Sensitive_Acc : 15.500, Run Time : 10.25 sec
INFO:root:2024-03-30 10:25:53, Dev, Step : 4700, Loss : 0.94041, Acc : 0.731, Auc : 0.794, Sensitive_Loss : 0.25216, Sensitive_Acc : 16.794, Sensitive_Auc : 0.992, Mean auc: 0.794, Run Time : 97.19 sec
INFO:root:2024-03-30 10:26:00, Train, Epoch : 10, Step : 4710, Loss : 0.62875, Acc : 0.778, Sensitive_Loss : 0.19960, Sensitive_Acc : 14.900, Run Time : 104.45 sec
INFO:root:2024-03-30 10:26:13, Train, Epoch : 10, Step : 4720, Loss : 0.58715, Acc : 0.797, Sensitive_Loss : 0.16921, Sensitive_Acc : 17.200, Run Time : 12.16 sec
INFO:root:2024-03-30 10:26:25, Train, Epoch : 10, Step : 4730, Loss : 0.69620, Acc : 0.778, Sensitive_Loss : 0.17253, Sensitive_Acc : 14.700, Run Time : 12.40 sec
INFO:root:2024-03-30 10:26:36, Train, Epoch : 10, Step : 4740, Loss : 0.48228, Acc : 0.759, Sensitive_Loss : 0.15701, Sensitive_Acc : 17.600, Run Time : 11.03 sec
INFO:root:2024-03-30 10:26:48, Train, Epoch : 10, Step : 4750, Loss : 0.61710, Acc : 0.819, Sensitive_Loss : 0.24713, Sensitive_Acc : 16.200, Run Time : 12.11 sec
INFO:root:2024-03-30 10:26:59, Train, Epoch : 10, Step : 4760, Loss : 0.51020, Acc : 0.809, Sensitive_Loss : 0.16448, Sensitive_Acc : 17.500, Run Time : 10.95 sec
INFO:root:2024-03-30 10:27:10, Train, Epoch : 10, Step : 4770, Loss : 0.66463, Acc : 0.819, Sensitive_Loss : 0.18590, Sensitive_Acc : 14.800, Run Time : 10.58 sec
INFO:root:2024-03-30 10:27:20, Train, Epoch : 10, Step : 4780, Loss : 0.48578, Acc : 0.784, Sensitive_Loss : 0.13447, Sensitive_Acc : 15.300, Run Time : 10.45 sec
INFO:root:2024-03-30 10:27:34, Train, Epoch : 10, Step : 4790, Loss : 0.60327, Acc : 0.791, Sensitive_Loss : 0.16342, Sensitive_Acc : 15.300, Run Time : 13.80 sec
INFO:root:2024-03-30 10:27:45, Train, Epoch : 10, Step : 4800, Loss : 0.74254, Acc : 0.769, Sensitive_Loss : 0.25692, Sensitive_Acc : 18.800, Run Time : 10.82 sec
INFO:root:2024-03-30 10:29:22, Dev, Step : 4800, Loss : 1.01553, Acc : 0.802, Auc : 0.764, Sensitive_Loss : 0.24149, Sensitive_Acc : 16.865, Sensitive_Auc : 0.992, Mean auc: 0.764, Run Time : 97.69 sec
INFO:root:2024-03-30 10:29:29, Train, Epoch : 10, Step : 4810, Loss : 0.49857, Acc : 0.778, Sensitive_Loss : 0.22649, Sensitive_Acc : 17.000, Run Time : 104.79 sec
INFO:root:2024-03-30 10:29:40, Train, Epoch : 10, Step : 4820, Loss : 0.73995, Acc : 0.791, Sensitive_Loss : 0.17208, Sensitive_Acc : 18.600, Run Time : 10.65 sec
INFO:root:2024-03-30 10:29:54, Train, Epoch : 10, Step : 4830, Loss : 0.59722, Acc : 0.812, Sensitive_Loss : 0.14853, Sensitive_Acc : 16.300, Run Time : 14.05 sec
INFO:root:2024-03-30 10:30:04, Train, Epoch : 10, Step : 4840, Loss : 0.70306, Acc : 0.806, Sensitive_Loss : 0.23398, Sensitive_Acc : 16.300, Run Time : 9.93 sec
INFO:root:2024-03-30 10:30:14, Train, Epoch : 10, Step : 4850, Loss : 0.63326, Acc : 0.781, Sensitive_Loss : 0.23534, Sensitive_Acc : 14.800, Run Time : 10.22 sec
INFO:root:2024-03-30 10:30:25, Train, Epoch : 10, Step : 4860, Loss : 0.62019, Acc : 0.856, Sensitive_Loss : 0.18337, Sensitive_Acc : 15.200, Run Time : 10.63 sec
INFO:root:2024-03-30 10:30:36, Train, Epoch : 10, Step : 4870, Loss : 0.68256, Acc : 0.766, Sensitive_Loss : 0.20639, Sensitive_Acc : 16.800, Run Time : 11.39 sec
INFO:root:2024-03-30 10:30:47, Train, Epoch : 10, Step : 4880, Loss : 0.51585, Acc : 0.794, Sensitive_Loss : 0.22062, Sensitive_Acc : 16.200, Run Time : 10.70 sec
INFO:root:2024-03-30 10:30:58, Train, Epoch : 10, Step : 4890, Loss : 0.59436, Acc : 0.812, Sensitive_Loss : 0.17250, Sensitive_Acc : 17.300, Run Time : 11.27 sec
INFO:root:2024-03-30 10:31:11, Train, Epoch : 10, Step : 4900, Loss : 0.61537, Acc : 0.791, Sensitive_Loss : 0.20753, Sensitive_Acc : 15.900, Run Time : 13.04 sec
INFO:root:2024-03-30 10:32:52, Dev, Step : 4900, Loss : 0.94965, Acc : 0.762, Auc : 0.792, Sensitive_Loss : 0.27051, Sensitive_Acc : 16.709, Sensitive_Auc : 0.992, Mean auc: 0.792, Run Time : 100.80 sec
INFO:root:2024-03-30 10:32:59, Train, Epoch : 10, Step : 4910, Loss : 0.50409, Acc : 0.806, Sensitive_Loss : 0.18482, Sensitive_Acc : 17.900, Run Time : 108.16 sec
INFO:root:2024-03-30 10:33:10, Train, Epoch : 10, Step : 4920, Loss : 0.59900, Acc : 0.797, Sensitive_Loss : 0.14541, Sensitive_Acc : 17.800, Run Time : 10.67 sec
INFO:root:2024-03-30 10:34:45
INFO:root:y_pred: [0.13733403 0.12844712 0.28646985 ... 0.19942161 0.47521994 0.15155154]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [4.87288507e-03 1.18627980e-01 9.72874343e-01 1.96406338e-02
 9.96691108e-01 2.86234473e-03 2.62046177e-02 2.16638651e-02
 3.66738290e-02 6.87736506e-03 6.87278714e-03 1.53650222e-02
 9.93124783e-01 1.14392408e-03 9.99238849e-01 9.28789437e-01
 3.08702787e-04 1.26879901e-01 9.99839306e-01 8.70791674e-01
 1.10766120e-01 1.64515395e-02 9.99797285e-01 9.61666703e-01
 5.17979870e-03 7.27026820e-01 3.43217677e-03 9.99747455e-01
 8.64068884e-03 8.83052528e-01 9.99816477e-01 9.97858226e-01
 5.77175478e-03 6.59582947e-05 1.09652646e-01 3.36792767e-02
 1.63737778e-02 5.30403964e-02 9.75104094e-01 9.48790848e-01
 1.79036073e-02 1.17879193e-02 9.93764281e-01 9.98428106e-01
 9.86438170e-02 2.91304067e-02 1.92348659e-02 2.40044922e-01
 9.99687195e-01 9.61274683e-01 9.96599376e-01 4.70832497e-01
 9.99160767e-01 9.99300480e-01 5.63008964e-01 9.38483894e-01
 9.32932794e-01 1.80663411e-02 9.99996662e-01 1.39588684e-01
 5.85790025e-04 5.78869641e-01 9.83973563e-01 9.81358826e-01
 9.65911269e-01 9.99777257e-01 8.86954129e-01 9.99657273e-01
 9.95149791e-01 9.92111444e-01 9.93848860e-01 5.57773514e-03
 1.74237899e-02 5.37608564e-01 9.56429839e-01 3.43888178e-02
 9.96982634e-01 2.85592512e-04 9.99861956e-01 3.28879356e-01
 4.56148624e-01 3.14136147e-02 9.81292799e-02 6.12165332e-01
 9.93468285e-01 2.68336594e-01 9.93231535e-01 1.32229015e-01
 3.47707532e-02 8.78432579e-03 2.07228307e-02 3.34802866e-01
 1.56296007e-02 1.54535677e-02 9.99492764e-01 7.29416728e-01
 2.09568635e-01 1.34918392e-02 9.64754760e-01 9.18981969e-01
 9.75648940e-01 5.40789729e-03 9.99481142e-01 4.92137223e-02
 6.13374859e-02 9.92961586e-01 7.34271184e-02 1.63082220e-02
 2.07968964e-03 1.14299599e-02 2.18718546e-03 9.73405838e-01
 4.86075252e-01 2.31954246e-03 2.99073070e-01 5.27089403e-04
 9.97371912e-01 1.43762063e-02 6.76038921e-01 4.12733201e-03
 5.25616109e-01 1.26685286e-02 9.99065340e-01 9.18886904e-03
 5.10315318e-03 9.99837399e-01 1.24792289e-02 9.80475724e-01
 5.63515663e-01 9.97937679e-01 4.04002021e-05 9.98936594e-01
 1.48025462e-02 9.99807537e-01 9.96034443e-01 1.66145831e-01
 7.56758571e-01 1.51376665e-01 7.71298073e-03 9.70339477e-01
 9.67772424e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-30 10:34:45, Dev, Step : 4920, Loss : 0.97038, Acc : 0.812, Auc : 0.789, Sensitive_Loss : 0.24374, Sensitive_Acc : 16.823, Sensitive_Auc : 0.992, Mean auc: 0.789, Run Time : 95.02 sec
