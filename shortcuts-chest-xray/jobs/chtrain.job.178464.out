Running on desktop22:
stdin: is not a tty
/home/pmen/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
1
Using the specified args:
Namespace(cfg_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_pmen.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_train.csv",
    "dev_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_val.csv",
    "pred_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Biased_Sex_1_pos01.csv",
    "pred_model": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Biased_Sex_1_pos011.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 1,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-28 06:20:26, Train, Epoch : 1, Step : 10, Loss : 0.67995, Acc : 0.600, Sensitive_Loss : 0.69056, Sensitive_Acc : 15.000, Run Time : 29.32 sec
INFO:root:2024-04-28 06:20:39, Train, Epoch : 1, Step : 20, Loss : 0.62324, Acc : 0.647, Sensitive_Loss : 0.65857, Sensitive_Acc : 15.200, Run Time : 12.73 sec
INFO:root:2024-04-28 06:22:27, Train, Epoch : 1, Step : 30, Loss : 0.60602, Acc : 0.691, Sensitive_Loss : 0.59468, Sensitive_Acc : 15.700, Run Time : 108.02 sec
INFO:root:2024-04-28 06:22:41, Train, Epoch : 1, Step : 40, Loss : 0.52739, Acc : 0.738, Sensitive_Loss : 0.62397, Sensitive_Acc : 15.100, Run Time : 13.54 sec
INFO:root:2024-04-28 06:23:00, Train, Epoch : 1, Step : 50, Loss : 0.48940, Acc : 0.762, Sensitive_Loss : 0.61547, Sensitive_Acc : 16.300, Run Time : 19.11 sec
INFO:root:2024-04-28 06:23:13, Train, Epoch : 1, Step : 60, Loss : 0.55857, Acc : 0.753, Sensitive_Loss : 0.58733, Sensitive_Acc : 16.000, Run Time : 12.87 sec
INFO:root:2024-04-28 06:23:20, Train, Epoch : 1, Step : 70, Loss : 0.50501, Acc : 0.778, Sensitive_Loss : 0.53883, Sensitive_Acc : 16.200, Run Time : 7.48 sec
INFO:root:2024-04-28 06:23:28, Train, Epoch : 1, Step : 80, Loss : 0.47791, Acc : 0.756, Sensitive_Loss : 0.57306, Sensitive_Acc : 16.500, Run Time : 7.31 sec
INFO:root:2024-04-28 06:23:35, Train, Epoch : 1, Step : 90, Loss : 0.49558, Acc : 0.781, Sensitive_Loss : 0.46963, Sensitive_Acc : 17.100, Run Time : 7.40 sec
INFO:root:2024-04-28 06:23:42, Train, Epoch : 1, Step : 100, Loss : 0.45886, Acc : 0.809, Sensitive_Loss : 0.49103, Sensitive_Acc : 16.600, Run Time : 7.51 sec
INFO:root:2024-04-28 06:25:21, Dev, Step : 100, Loss : 0.55306, Acc : 0.755, Auc : 0.853, Sensitive_Loss : 0.48707, Sensitive_Acc : 16.679, Sensitive_Auc : 0.916, Mean auc: 0.853, Run Time : 98.36 sec
INFO:root:2024-04-28 06:25:22, Best, Step : 100, Loss : 0.55306, Acc : 0.755, Auc : 0.853, Sensitive_Loss : 0.48707, Sensitive_Acc : 16.679, Sensitive_Auc : 0.916, Best Auc : 0.853
INFO:root:2024-04-28 06:25:27, Train, Epoch : 1, Step : 110, Loss : 0.43959, Acc : 0.784, Sensitive_Loss : 0.48163, Sensitive_Acc : 18.100, Run Time : 105.00 sec
INFO:root:2024-04-28 06:25:35, Train, Epoch : 1, Step : 120, Loss : 0.59530, Acc : 0.744, Sensitive_Loss : 0.41752, Sensitive_Acc : 14.500, Run Time : 7.31 sec
INFO:root:2024-04-28 06:25:42, Train, Epoch : 1, Step : 130, Loss : 0.55920, Acc : 0.750, Sensitive_Loss : 0.50261, Sensitive_Acc : 16.000, Run Time : 7.09 sec
INFO:root:2024-04-28 06:25:49, Train, Epoch : 1, Step : 140, Loss : 0.52889, Acc : 0.731, Sensitive_Loss : 0.45700, Sensitive_Acc : 15.800, Run Time : 7.66 sec
INFO:root:2024-04-28 06:25:57, Train, Epoch : 1, Step : 150, Loss : 0.45289, Acc : 0.759, Sensitive_Loss : 0.45034, Sensitive_Acc : 15.900, Run Time : 7.58 sec
INFO:root:2024-04-28 06:26:04, Train, Epoch : 1, Step : 160, Loss : 0.46415, Acc : 0.766, Sensitive_Loss : 0.44235, Sensitive_Acc : 16.300, Run Time : 7.26 sec
INFO:root:2024-04-28 06:26:12, Train, Epoch : 1, Step : 170, Loss : 0.48513, Acc : 0.728, Sensitive_Loss : 0.41822, Sensitive_Acc : 16.000, Run Time : 7.37 sec
INFO:root:2024-04-28 06:26:19, Train, Epoch : 1, Step : 180, Loss : 0.49573, Acc : 0.762, Sensitive_Loss : 0.39552, Sensitive_Acc : 15.200, Run Time : 7.78 sec
INFO:root:2024-04-28 06:26:27, Train, Epoch : 1, Step : 190, Loss : 0.47016, Acc : 0.766, Sensitive_Loss : 0.38092, Sensitive_Acc : 17.200, Run Time : 7.44 sec
INFO:root:2024-04-28 06:26:34, Train, Epoch : 1, Step : 200, Loss : 0.62422, Acc : 0.734, Sensitive_Loss : 0.39532, Sensitive_Acc : 15.000, Run Time : 7.32 sec
INFO:root:2024-04-28 06:28:10, Dev, Step : 200, Loss : 0.51825, Acc : 0.759, Auc : 0.856, Sensitive_Loss : 0.43559, Sensitive_Acc : 16.621, Sensitive_Auc : 0.951, Mean auc: 0.856, Run Time : 95.98 sec
INFO:root:2024-04-28 06:28:11, Best, Step : 200, Loss : 0.51825, Acc : 0.759, Auc : 0.856, Sensitive_Loss : 0.43559, Sensitive_Acc : 16.621, Sensitive_Auc : 0.951, Best Auc : 0.856
INFO:root:2024-04-28 06:28:17, Train, Epoch : 1, Step : 210, Loss : 0.49590, Acc : 0.784, Sensitive_Loss : 0.38230, Sensitive_Acc : 16.600, Run Time : 102.64 sec
INFO:root:2024-04-28 06:28:24, Train, Epoch : 1, Step : 220, Loss : 0.48270, Acc : 0.791, Sensitive_Loss : 0.38161, Sensitive_Acc : 17.400, Run Time : 7.59 sec
INFO:root:2024-04-28 06:28:31, Train, Epoch : 1, Step : 230, Loss : 0.44111, Acc : 0.794, Sensitive_Loss : 0.39039, Sensitive_Acc : 18.000, Run Time : 6.74 sec
INFO:root:2024-04-28 06:28:39, Train, Epoch : 1, Step : 240, Loss : 0.55680, Acc : 0.778, Sensitive_Loss : 0.34778, Sensitive_Acc : 15.400, Run Time : 7.68 sec
INFO:root:2024-04-28 06:28:47, Train, Epoch : 1, Step : 250, Loss : 0.49698, Acc : 0.766, Sensitive_Loss : 0.37045, Sensitive_Acc : 17.500, Run Time : 7.71 sec
INFO:root:2024-04-28 06:28:54, Train, Epoch : 1, Step : 260, Loss : 0.51400, Acc : 0.759, Sensitive_Loss : 0.44692, Sensitive_Acc : 17.000, Run Time : 7.07 sec
INFO:root:2024-04-28 06:29:01, Train, Epoch : 1, Step : 270, Loss : 0.38178, Acc : 0.819, Sensitive_Loss : 0.36870, Sensitive_Acc : 16.700, Run Time : 7.43 sec
INFO:root:2024-04-28 06:29:09, Train, Epoch : 1, Step : 280, Loss : 0.54813, Acc : 0.756, Sensitive_Loss : 0.30468, Sensitive_Acc : 16.900, Run Time : 7.41 sec
INFO:root:2024-04-28 06:29:16, Train, Epoch : 1, Step : 290, Loss : 0.53074, Acc : 0.756, Sensitive_Loss : 0.38834, Sensitive_Acc : 16.500, Run Time : 7.42 sec
INFO:root:2024-04-28 06:29:23, Train, Epoch : 1, Step : 300, Loss : 0.46349, Acc : 0.781, Sensitive_Loss : 0.31812, Sensitive_Acc : 15.100, Run Time : 7.23 sec
INFO:root:2024-04-28 06:30:59, Dev, Step : 300, Loss : 0.58037, Acc : 0.731, Auc : 0.868, Sensitive_Loss : 0.44683, Sensitive_Acc : 16.664, Sensitive_Auc : 0.954, Mean auc: 0.868, Run Time : 95.98 sec
INFO:root:2024-04-28 06:31:00, Best, Step : 300, Loss : 0.58037, Acc : 0.731, Auc : 0.868, Sensitive_Loss : 0.44683, Sensitive_Acc : 16.664, Sensitive_Auc : 0.954, Best Auc : 0.868
INFO:root:2024-04-28 06:31:05, Train, Epoch : 1, Step : 310, Loss : 0.50258, Acc : 0.750, Sensitive_Loss : 0.37107, Sensitive_Acc : 15.700, Run Time : 102.30 sec
INFO:root:2024-04-28 06:31:13, Train, Epoch : 1, Step : 320, Loss : 0.50561, Acc : 0.781, Sensitive_Loss : 0.34431, Sensitive_Acc : 15.800, Run Time : 7.35 sec
INFO:root:2024-04-28 06:31:20, Train, Epoch : 1, Step : 330, Loss : 0.47326, Acc : 0.750, Sensitive_Loss : 0.39824, Sensitive_Acc : 15.800, Run Time : 7.11 sec
INFO:root:2024-04-28 06:31:27, Train, Epoch : 1, Step : 340, Loss : 0.50606, Acc : 0.762, Sensitive_Loss : 0.39817, Sensitive_Acc : 15.700, Run Time : 7.47 sec
INFO:root:2024-04-28 06:31:35, Train, Epoch : 1, Step : 350, Loss : 0.57020, Acc : 0.775, Sensitive_Loss : 0.35563, Sensitive_Acc : 15.700, Run Time : 7.75 sec
INFO:root:2024-04-28 06:31:42, Train, Epoch : 1, Step : 360, Loss : 0.52834, Acc : 0.747, Sensitive_Loss : 0.31446, Sensitive_Acc : 15.000, Run Time : 7.24 sec
INFO:root:2024-04-28 06:31:50, Train, Epoch : 1, Step : 370, Loss : 0.51592, Acc : 0.759, Sensitive_Loss : 0.36479, Sensitive_Acc : 15.000, Run Time : 7.79 sec
INFO:root:2024-04-28 06:31:58, Train, Epoch : 1, Step : 380, Loss : 0.52937, Acc : 0.794, Sensitive_Loss : 0.35822, Sensitive_Acc : 15.800, Run Time : 7.73 sec
INFO:root:2024-04-28 06:32:05, Train, Epoch : 1, Step : 390, Loss : 0.47732, Acc : 0.791, Sensitive_Loss : 0.32569, Sensitive_Acc : 16.300, Run Time : 6.98 sec
INFO:root:2024-04-28 06:32:12, Train, Epoch : 1, Step : 400, Loss : 0.50784, Acc : 0.766, Sensitive_Loss : 0.32852, Sensitive_Acc : 14.200, Run Time : 7.49 sec
INFO:root:2024-04-28 06:33:48, Dev, Step : 400, Loss : 0.48009, Acc : 0.791, Auc : 0.863, Sensitive_Loss : 0.33141, Sensitive_Acc : 16.921, Sensitive_Auc : 0.958, Mean auc: 0.863, Run Time : 95.58 sec
INFO:root:2024-04-28 06:33:54, Train, Epoch : 1, Step : 410, Loss : 0.52055, Acc : 0.772, Sensitive_Loss : 0.35900, Sensitive_Acc : 15.000, Run Time : 101.66 sec
INFO:root:2024-04-28 06:34:01, Train, Epoch : 1, Step : 420, Loss : 0.48147, Acc : 0.769, Sensitive_Loss : 0.29393, Sensitive_Acc : 15.300, Run Time : 6.96 sec
INFO:root:2024-04-28 06:34:08, Train, Epoch : 1, Step : 430, Loss : 0.43049, Acc : 0.806, Sensitive_Loss : 0.33091, Sensitive_Acc : 15.700, Run Time : 6.95 sec
INFO:root:2024-04-28 06:34:16, Train, Epoch : 1, Step : 440, Loss : 0.52802, Acc : 0.766, Sensitive_Loss : 0.34631, Sensitive_Acc : 15.700, Run Time : 7.68 sec
INFO:root:2024-04-28 06:34:24, Train, Epoch : 1, Step : 450, Loss : 0.55036, Acc : 0.731, Sensitive_Loss : 0.33667, Sensitive_Acc : 15.700, Run Time : 7.89 sec
INFO:root:2024-04-28 06:34:31, Train, Epoch : 1, Step : 460, Loss : 0.50088, Acc : 0.791, Sensitive_Loss : 0.32755, Sensitive_Acc : 16.800, Run Time : 7.21 sec
INFO:root:2024-04-28 06:34:38, Train, Epoch : 1, Step : 470, Loss : 0.48580, Acc : 0.778, Sensitive_Loss : 0.32259, Sensitive_Acc : 15.800, Run Time : 7.17 sec
INFO:root:2024-04-28 06:34:46, Train, Epoch : 1, Step : 480, Loss : 0.48450, Acc : 0.784, Sensitive_Loss : 0.28322, Sensitive_Acc : 15.500, Run Time : 7.74 sec
INFO:root:2024-04-28 06:34:53, Train, Epoch : 1, Step : 490, Loss : 0.41166, Acc : 0.778, Sensitive_Loss : 0.30205, Sensitive_Acc : 15.500, Run Time : 7.27 sec
INFO:root:2024-04-28 06:35:00, Train, Epoch : 1, Step : 500, Loss : 0.54039, Acc : 0.778, Sensitive_Loss : 0.27115, Sensitive_Acc : 14.700, Run Time : 7.05 sec
INFO:root:2024-04-28 06:36:37, Dev, Step : 500, Loss : 0.47263, Acc : 0.785, Auc : 0.872, Sensitive_Loss : 0.29141, Sensitive_Acc : 16.664, Sensitive_Auc : 0.958, Mean auc: 0.872, Run Time : 96.74 sec
INFO:root:2024-04-28 06:36:37, Best, Step : 500, Loss : 0.47263, Acc : 0.785, Auc : 0.872, Sensitive_Loss : 0.29141, Sensitive_Acc : 16.664, Sensitive_Auc : 0.958, Best Auc : 0.872
INFO:root:2024-04-28 06:36:43, Train, Epoch : 1, Step : 510, Loss : 0.47808, Acc : 0.762, Sensitive_Loss : 0.35300, Sensitive_Acc : 15.400, Run Time : 103.07 sec
INFO:root:2024-04-28 06:36:51, Train, Epoch : 1, Step : 520, Loss : 0.46851, Acc : 0.800, Sensitive_Loss : 0.31685, Sensitive_Acc : 17.000, Run Time : 7.58 sec
INFO:root:2024-04-28 06:36:58, Train, Epoch : 1, Step : 530, Loss : 0.41475, Acc : 0.794, Sensitive_Loss : 0.23868, Sensitive_Acc : 16.000, Run Time : 7.14 sec
INFO:root:2024-04-28 06:37:05, Train, Epoch : 1, Step : 540, Loss : 0.58505, Acc : 0.731, Sensitive_Loss : 0.33863, Sensitive_Acc : 17.900, Run Time : 7.63 sec
INFO:root:2024-04-28 06:37:13, Train, Epoch : 1, Step : 550, Loss : 0.42567, Acc : 0.800, Sensitive_Loss : 0.25618, Sensitive_Acc : 16.300, Run Time : 7.64 sec
INFO:root:2024-04-28 06:37:21, Train, Epoch : 1, Step : 560, Loss : 0.49107, Acc : 0.772, Sensitive_Loss : 0.26623, Sensitive_Acc : 14.800, Run Time : 7.50 sec
INFO:root:2024-04-28 06:37:28, Train, Epoch : 1, Step : 570, Loss : 0.43703, Acc : 0.822, Sensitive_Loss : 0.25211, Sensitive_Acc : 14.700, Run Time : 7.65 sec
INFO:root:2024-04-28 06:37:36, Train, Epoch : 1, Step : 580, Loss : 0.45384, Acc : 0.775, Sensitive_Loss : 0.30583, Sensitive_Acc : 18.400, Run Time : 7.52 sec
INFO:root:2024-04-28 06:37:43, Train, Epoch : 1, Step : 590, Loss : 0.47667, Acc : 0.800, Sensitive_Loss : 0.22518, Sensitive_Acc : 16.900, Run Time : 7.26 sec
INFO:root:2024-04-28 06:37:50, Train, Epoch : 1, Step : 600, Loss : 0.40448, Acc : 0.809, Sensitive_Loss : 0.28387, Sensitive_Acc : 15.900, Run Time : 7.18 sec
INFO:root:2024-04-28 06:39:27, Dev, Step : 600, Loss : 0.50072, Acc : 0.779, Auc : 0.874, Sensitive_Loss : 0.24775, Sensitive_Acc : 16.950, Sensitive_Auc : 0.987, Mean auc: 0.874, Run Time : 96.91 sec
INFO:root:2024-04-28 06:39:28, Best, Step : 600, Loss : 0.50072, Acc : 0.779, Auc : 0.874, Sensitive_Loss : 0.24775, Sensitive_Acc : 16.950, Sensitive_Auc : 0.987, Best Auc : 0.874
INFO:root:2024-04-28 06:39:33, Train, Epoch : 1, Step : 610, Loss : 0.45969, Acc : 0.750, Sensitive_Loss : 0.24891, Sensitive_Acc : 16.900, Run Time : 103.19 sec
INFO:root:2024-04-28 06:39:41, Train, Epoch : 1, Step : 620, Loss : 0.46340, Acc : 0.772, Sensitive_Loss : 0.28463, Sensitive_Acc : 15.500, Run Time : 8.11 sec
INFO:root:2024-04-28 06:41:20
INFO:root:y_pred: [0.0418216  0.69845873 0.0349084  ... 0.68299127 0.0029242  0.6393212 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.89572823e-01 4.63285251e-03 4.81314719e-01 2.97191972e-03
 9.44516063e-01 4.22681719e-02 9.82176900e-01 9.17608798e-01
 1.81012556e-01 8.20422232e-01 9.41322982e-01 9.96645749e-01
 9.80033994e-01 5.08795083e-01 6.99650422e-02 8.87446642e-01
 9.82057095e-01 4.76373360e-03 7.58339405e-01 9.89980638e-01
 8.99789095e-01 5.97128212e-01 9.81505811e-01 7.94849634e-01
 9.79535043e-01 7.15830445e-01 7.07809851e-02 9.84818697e-01
 8.17952633e-01 6.12795711e-01 2.32954800e-01 8.37273061e-01
 1.17060229e-01 6.85617700e-02 5.91678560e-01 1.42860226e-02
 4.70338851e-01 9.26934108e-02 9.71353769e-01 9.40232337e-01
 2.83037126e-02 1.53590098e-01 9.06690121e-01 4.24941862e-03
 9.96175170e-01 9.62699234e-01 9.51824903e-01 9.86028075e-01
 4.62706536e-02 8.21991324e-01 9.82364833e-01 7.05434754e-02
 1.93946674e-01 4.35406901e-02 4.01491212e-04 2.49249578e-01
 6.67498559e-02 3.90512943e-01 1.24995876e-03 3.53383988e-01
 4.45419084e-03 4.75757688e-01 2.18574688e-01 9.04028594e-01
 1.03831917e-01 9.76699471e-01 1.06007621e-01 9.75700438e-01
 8.21461916e-01 8.15697134e-01 4.28824782e-01 6.34698272e-01
 1.84379697e-01 2.15732083e-01 3.40537220e-01 4.89280140e-03
 6.46187484e-01 1.04255088e-01 2.02016421e-02 9.69142199e-01
 9.45617616e-01 4.97490342e-04 1.63386837e-01 3.91248390e-02
 9.56631839e-01 8.69352996e-01 8.52762349e-03 4.38858479e-01
 9.65807199e-01 9.93844748e-01 9.89876807e-01 3.69938090e-02
 9.54309329e-02 9.79562879e-01 6.31256849e-02 1.14186205e-01
 9.73590791e-01 9.43777740e-01 1.51542677e-02 6.91016540e-02
 9.34970140e-01 7.35139012e-01 9.78459001e-01 9.68914092e-01
 3.55000556e-01 9.49319422e-01 6.24314010e-01 8.96238029e-01
 9.41936970e-01 1.44675598e-01 7.39818215e-01 9.61998761e-01
 1.36861755e-02 9.82708037e-01 9.02899504e-01 9.70879793e-01
 9.47667301e-01 9.83175397e-01 4.00640890e-02 4.69979942e-01
 9.73258317e-01 9.71566498e-01 2.13198690e-03 9.39332902e-01
 9.85072315e-01 4.94571894e-01 9.84141409e-01 9.91263688e-02
 4.26990300e-01 9.93455827e-01 9.13328469e-01 2.39143148e-01
 3.17222685e-01 1.93363633e-02 9.55341816e-01 9.90864933e-01
 8.14586222e-01 1.47919446e-01 4.44231592e-02 9.02887225e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 06:41:20, Dev, Step : 626, Loss : 0.50390, Acc : 0.765, Auc : 0.870, Sensitive_Loss : 0.27165, Sensitive_Acc : 16.636, Sensitive_Auc : 0.974, Mean auc: 0.870, Run Time : 94.45 sec
INFO:root:2024-04-28 06:41:25, Train, Epoch : 2, Step : 630, Loss : 0.16875, Acc : 0.341, Sensitive_Loss : 0.07855, Sensitive_Acc : 6.600, Run Time : 4.36 sec
INFO:root:2024-04-28 06:41:32, Train, Epoch : 2, Step : 640, Loss : 0.37681, Acc : 0.822, Sensitive_Loss : 0.28270, Sensitive_Acc : 15.200, Run Time : 7.25 sec
INFO:root:2024-04-28 06:41:39, Train, Epoch : 2, Step : 650, Loss : 0.41300, Acc : 0.825, Sensitive_Loss : 0.20686, Sensitive_Acc : 16.100, Run Time : 7.28 sec
INFO:root:2024-04-28 06:41:47, Train, Epoch : 2, Step : 660, Loss : 0.45026, Acc : 0.838, Sensitive_Loss : 0.22459, Sensitive_Acc : 15.500, Run Time : 7.53 sec
INFO:root:2024-04-28 06:41:54, Train, Epoch : 2, Step : 670, Loss : 0.39918, Acc : 0.831, Sensitive_Loss : 0.23021, Sensitive_Acc : 16.500, Run Time : 7.16 sec
INFO:root:2024-04-28 06:42:02, Train, Epoch : 2, Step : 680, Loss : 0.34070, Acc : 0.853, Sensitive_Loss : 0.21884, Sensitive_Acc : 15.800, Run Time : 7.87 sec
INFO:root:2024-04-28 06:42:09, Train, Epoch : 2, Step : 690, Loss : 0.49047, Acc : 0.784, Sensitive_Loss : 0.23047, Sensitive_Acc : 16.400, Run Time : 7.13 sec
INFO:root:2024-04-28 06:42:16, Train, Epoch : 2, Step : 700, Loss : 0.47464, Acc : 0.800, Sensitive_Loss : 0.27297, Sensitive_Acc : 16.200, Run Time : 6.97 sec
INFO:root:2024-04-28 06:43:52, Dev, Step : 700, Loss : 0.50532, Acc : 0.770, Auc : 0.874, Sensitive_Loss : 0.23257, Sensitive_Acc : 16.607, Sensitive_Auc : 0.972, Mean auc: 0.874, Run Time : 95.95 sec
INFO:root:2024-04-28 06:43:53, Best, Step : 700, Loss : 0.50532, Acc : 0.770, Auc : 0.874, Sensitive_Loss : 0.23257, Sensitive_Acc : 16.607, Sensitive_Auc : 0.972, Best Auc : 0.874
INFO:root:2024-04-28 06:43:58, Train, Epoch : 2, Step : 710, Loss : 0.50926, Acc : 0.759, Sensitive_Loss : 0.24574, Sensitive_Acc : 16.300, Run Time : 102.07 sec
INFO:root:2024-04-28 06:44:06, Train, Epoch : 2, Step : 720, Loss : 0.49644, Acc : 0.794, Sensitive_Loss : 0.19220, Sensitive_Acc : 16.000, Run Time : 7.62 sec
INFO:root:2024-04-28 06:44:13, Train, Epoch : 2, Step : 730, Loss : 0.43061, Acc : 0.787, Sensitive_Loss : 0.28374, Sensitive_Acc : 15.800, Run Time : 7.39 sec
INFO:root:2024-04-28 06:44:20, Train, Epoch : 2, Step : 740, Loss : 0.48229, Acc : 0.784, Sensitive_Loss : 0.28147, Sensitive_Acc : 17.500, Run Time : 7.21 sec
INFO:root:2024-04-28 06:44:28, Train, Epoch : 2, Step : 750, Loss : 0.39779, Acc : 0.812, Sensitive_Loss : 0.21268, Sensitive_Acc : 16.700, Run Time : 7.66 sec
INFO:root:2024-04-28 06:44:35, Train, Epoch : 2, Step : 760, Loss : 0.38670, Acc : 0.806, Sensitive_Loss : 0.22907, Sensitive_Acc : 15.000, Run Time : 7.26 sec
INFO:root:2024-04-28 06:44:42, Train, Epoch : 2, Step : 770, Loss : 0.47756, Acc : 0.781, Sensitive_Loss : 0.24416, Sensitive_Acc : 14.300, Run Time : 7.01 sec
INFO:root:2024-04-28 06:44:50, Train, Epoch : 2, Step : 780, Loss : 0.37980, Acc : 0.834, Sensitive_Loss : 0.21502, Sensitive_Acc : 14.300, Run Time : 7.26 sec
INFO:root:2024-04-28 06:44:57, Train, Epoch : 2, Step : 790, Loss : 0.43735, Acc : 0.819, Sensitive_Loss : 0.21507, Sensitive_Acc : 18.200, Run Time : 7.26 sec
INFO:root:2024-04-28 06:45:04, Train, Epoch : 2, Step : 800, Loss : 0.43901, Acc : 0.803, Sensitive_Loss : 0.23069, Sensitive_Acc : 16.500, Run Time : 7.56 sec
INFO:root:2024-04-28 06:46:40, Dev, Step : 800, Loss : 0.51203, Acc : 0.779, Auc : 0.868, Sensitive_Loss : 0.22668, Sensitive_Acc : 16.764, Sensitive_Auc : 0.961, Mean auc: 0.868, Run Time : 95.35 sec
INFO:root:2024-04-28 06:46:46, Train, Epoch : 2, Step : 810, Loss : 0.45410, Acc : 0.828, Sensitive_Loss : 0.21265, Sensitive_Acc : 17.600, Run Time : 101.13 sec
INFO:root:2024-04-28 06:46:53, Train, Epoch : 2, Step : 820, Loss : 0.42894, Acc : 0.806, Sensitive_Loss : 0.22902, Sensitive_Acc : 17.100, Run Time : 7.40 sec
INFO:root:2024-04-28 06:47:00, Train, Epoch : 2, Step : 830, Loss : 0.46361, Acc : 0.781, Sensitive_Loss : 0.25425, Sensitive_Acc : 15.200, Run Time : 6.83 sec
INFO:root:2024-04-28 06:47:07, Train, Epoch : 2, Step : 840, Loss : 0.44672, Acc : 0.806, Sensitive_Loss : 0.23558, Sensitive_Acc : 15.400, Run Time : 7.45 sec
INFO:root:2024-04-28 06:47:15, Train, Epoch : 2, Step : 850, Loss : 0.45346, Acc : 0.809, Sensitive_Loss : 0.22252, Sensitive_Acc : 15.400, Run Time : 7.40 sec
INFO:root:2024-04-28 06:47:22, Train, Epoch : 2, Step : 860, Loss : 0.41137, Acc : 0.787, Sensitive_Loss : 0.22769, Sensitive_Acc : 18.000, Run Time : 7.38 sec
INFO:root:2024-04-28 06:47:29, Train, Epoch : 2, Step : 870, Loss : 0.45492, Acc : 0.787, Sensitive_Loss : 0.21932, Sensitive_Acc : 15.800, Run Time : 7.35 sec
INFO:root:2024-04-28 06:47:37, Train, Epoch : 2, Step : 880, Loss : 0.41978, Acc : 0.797, Sensitive_Loss : 0.21056, Sensitive_Acc : 16.200, Run Time : 7.23 sec
INFO:root:2024-04-28 06:47:44, Train, Epoch : 2, Step : 890, Loss : 0.40332, Acc : 0.797, Sensitive_Loss : 0.24105, Sensitive_Acc : 16.100, Run Time : 7.33 sec
INFO:root:2024-04-28 06:47:51, Train, Epoch : 2, Step : 900, Loss : 0.41598, Acc : 0.812, Sensitive_Loss : 0.23971, Sensitive_Acc : 17.000, Run Time : 7.18 sec
INFO:root:2024-04-28 06:49:27, Dev, Step : 900, Loss : 0.56429, Acc : 0.759, Auc : 0.885, Sensitive_Loss : 0.22670, Sensitive_Acc : 16.807, Sensitive_Auc : 0.979, Mean auc: 0.885, Run Time : 95.49 sec
INFO:root:2024-04-28 06:49:27, Best, Step : 900, Loss : 0.56429, Acc : 0.759, Auc : 0.885, Sensitive_Loss : 0.22670, Sensitive_Acc : 16.807, Sensitive_Auc : 0.979, Best Auc : 0.885
INFO:root:2024-04-28 06:49:33, Train, Epoch : 2, Step : 910, Loss : 0.46235, Acc : 0.819, Sensitive_Loss : 0.16072, Sensitive_Acc : 15.700, Run Time : 101.74 sec
INFO:root:2024-04-28 06:49:40, Train, Epoch : 2, Step : 920, Loss : 0.44667, Acc : 0.794, Sensitive_Loss : 0.23416, Sensitive_Acc : 16.200, Run Time : 7.37 sec
INFO:root:2024-04-28 06:49:48, Train, Epoch : 2, Step : 930, Loss : 0.51020, Acc : 0.781, Sensitive_Loss : 0.19686, Sensitive_Acc : 15.000, Run Time : 7.38 sec
INFO:root:2024-04-28 06:49:55, Train, Epoch : 2, Step : 940, Loss : 0.42336, Acc : 0.800, Sensitive_Loss : 0.21826, Sensitive_Acc : 15.800, Run Time : 7.60 sec
INFO:root:2024-04-28 06:50:02, Train, Epoch : 2, Step : 950, Loss : 0.46514, Acc : 0.797, Sensitive_Loss : 0.19720, Sensitive_Acc : 15.700, Run Time : 6.91 sec
INFO:root:2024-04-28 06:50:09, Train, Epoch : 2, Step : 960, Loss : 0.43506, Acc : 0.775, Sensitive_Loss : 0.19081, Sensitive_Acc : 17.700, Run Time : 7.12 sec
INFO:root:2024-04-28 06:50:17, Train, Epoch : 2, Step : 970, Loss : 0.41779, Acc : 0.819, Sensitive_Loss : 0.22923, Sensitive_Acc : 16.200, Run Time : 7.51 sec
INFO:root:2024-04-28 06:50:24, Train, Epoch : 2, Step : 980, Loss : 0.43866, Acc : 0.772, Sensitive_Loss : 0.22133, Sensitive_Acc : 15.600, Run Time : 7.58 sec
INFO:root:2024-04-28 06:50:32, Train, Epoch : 2, Step : 990, Loss : 0.50133, Acc : 0.759, Sensitive_Loss : 0.21088, Sensitive_Acc : 16.400, Run Time : 7.38 sec
INFO:root:2024-04-28 06:50:38, Train, Epoch : 2, Step : 1000, Loss : 0.51390, Acc : 0.781, Sensitive_Loss : 0.20619, Sensitive_Acc : 15.800, Run Time : 6.49 sec
INFO:root:2024-04-28 06:52:14, Dev, Step : 1000, Loss : 0.46130, Acc : 0.797, Auc : 0.876, Sensitive_Loss : 0.18832, Sensitive_Acc : 16.850, Sensitive_Auc : 0.985, Mean auc: 0.876, Run Time : 96.03 sec
INFO:root:2024-04-28 06:52:20, Train, Epoch : 2, Step : 1010, Loss : 0.40719, Acc : 0.828, Sensitive_Loss : 0.16357, Sensitive_Acc : 16.000, Run Time : 102.17 sec
INFO:root:2024-04-28 06:52:27, Train, Epoch : 2, Step : 1020, Loss : 0.45647, Acc : 0.797, Sensitive_Loss : 0.16494, Sensitive_Acc : 15.500, Run Time : 6.68 sec
INFO:root:2024-04-28 06:52:34, Train, Epoch : 2, Step : 1030, Loss : 0.43196, Acc : 0.797, Sensitive_Loss : 0.18544, Sensitive_Acc : 18.100, Run Time : 7.35 sec
INFO:root:2024-04-28 06:52:42, Train, Epoch : 2, Step : 1040, Loss : 0.49031, Acc : 0.738, Sensitive_Loss : 0.19631, Sensitive_Acc : 15.000, Run Time : 7.44 sec
INFO:root:2024-04-28 06:52:49, Train, Epoch : 2, Step : 1050, Loss : 0.44302, Acc : 0.838, Sensitive_Loss : 0.20588, Sensitive_Acc : 16.500, Run Time : 6.85 sec
INFO:root:2024-04-28 06:52:57, Train, Epoch : 2, Step : 1060, Loss : 0.46393, Acc : 0.794, Sensitive_Loss : 0.20505, Sensitive_Acc : 16.800, Run Time : 7.85 sec
INFO:root:2024-04-28 06:53:03, Train, Epoch : 2, Step : 1070, Loss : 0.46225, Acc : 0.803, Sensitive_Loss : 0.22534, Sensitive_Acc : 17.000, Run Time : 6.65 sec
INFO:root:2024-04-28 06:53:10, Train, Epoch : 2, Step : 1080, Loss : 0.42210, Acc : 0.822, Sensitive_Loss : 0.14699, Sensitive_Acc : 17.600, Run Time : 7.13 sec
INFO:root:2024-04-28 06:53:18, Train, Epoch : 2, Step : 1090, Loss : 0.41413, Acc : 0.822, Sensitive_Loss : 0.20294, Sensitive_Acc : 16.200, Run Time : 7.77 sec
INFO:root:2024-04-28 06:53:25, Train, Epoch : 2, Step : 1100, Loss : 0.45497, Acc : 0.809, Sensitive_Loss : 0.17878, Sensitive_Acc : 16.700, Run Time : 7.07 sec
INFO:root:2024-04-28 06:55:01, Dev, Step : 1100, Loss : 0.46722, Acc : 0.803, Auc : 0.891, Sensitive_Loss : 0.18544, Sensitive_Acc : 16.793, Sensitive_Auc : 0.983, Mean auc: 0.891, Run Time : 95.89 sec
INFO:root:2024-04-28 06:55:02, Best, Step : 1100, Loss : 0.46722, Acc : 0.803, Auc : 0.891, Sensitive_Loss : 0.18544, Sensitive_Acc : 16.793, Sensitive_Auc : 0.983, Best Auc : 0.891
INFO:root:2024-04-28 06:55:07, Train, Epoch : 2, Step : 1110, Loss : 0.50014, Acc : 0.791, Sensitive_Loss : 0.15781, Sensitive_Acc : 16.200, Run Time : 102.24 sec
INFO:root:2024-04-28 06:55:15, Train, Epoch : 2, Step : 1120, Loss : 0.38899, Acc : 0.812, Sensitive_Loss : 0.16008, Sensitive_Acc : 15.000, Run Time : 7.22 sec
INFO:root:2024-04-28 06:55:22, Train, Epoch : 2, Step : 1130, Loss : 0.47260, Acc : 0.781, Sensitive_Loss : 0.17282, Sensitive_Acc : 16.800, Run Time : 7.06 sec
INFO:root:2024-04-28 06:55:29, Train, Epoch : 2, Step : 1140, Loss : 0.42829, Acc : 0.819, Sensitive_Loss : 0.17602, Sensitive_Acc : 14.400, Run Time : 7.57 sec
INFO:root:2024-04-28 06:55:37, Train, Epoch : 2, Step : 1150, Loss : 0.45542, Acc : 0.787, Sensitive_Loss : 0.19049, Sensitive_Acc : 16.800, Run Time : 7.34 sec
INFO:root:2024-04-28 06:55:44, Train, Epoch : 2, Step : 1160, Loss : 0.38669, Acc : 0.834, Sensitive_Loss : 0.16463, Sensitive_Acc : 15.700, Run Time : 7.71 sec
INFO:root:2024-04-28 06:55:51, Train, Epoch : 2, Step : 1170, Loss : 0.33556, Acc : 0.828, Sensitive_Loss : 0.14918, Sensitive_Acc : 17.200, Run Time : 6.59 sec
INFO:root:2024-04-28 06:55:58, Train, Epoch : 2, Step : 1180, Loss : 0.47041, Acc : 0.778, Sensitive_Loss : 0.19853, Sensitive_Acc : 16.200, Run Time : 7.55 sec
INFO:root:2024-04-28 06:56:05, Train, Epoch : 2, Step : 1190, Loss : 0.39481, Acc : 0.819, Sensitive_Loss : 0.16541, Sensitive_Acc : 17.400, Run Time : 6.92 sec
INFO:root:2024-04-28 06:56:13, Train, Epoch : 2, Step : 1200, Loss : 0.40613, Acc : 0.834, Sensitive_Loss : 0.14877, Sensitive_Acc : 16.900, Run Time : 7.15 sec
INFO:root:2024-04-28 06:57:48, Dev, Step : 1200, Loss : 0.44167, Acc : 0.805, Auc : 0.894, Sensitive_Loss : 0.15231, Sensitive_Acc : 16.721, Sensitive_Auc : 0.979, Mean auc: 0.894, Run Time : 95.93 sec
INFO:root:2024-04-28 06:57:49, Best, Step : 1200, Loss : 0.44167, Acc : 0.805, Auc : 0.894, Sensitive_Loss : 0.15231, Sensitive_Acc : 16.721, Sensitive_Auc : 0.979, Best Auc : 0.894
INFO:root:2024-04-28 06:57:55, Train, Epoch : 2, Step : 1210, Loss : 0.40882, Acc : 0.825, Sensitive_Loss : 0.15640, Sensitive_Acc : 16.900, Run Time : 102.81 sec
INFO:root:2024-04-28 06:58:02, Train, Epoch : 2, Step : 1220, Loss : 0.47791, Acc : 0.781, Sensitive_Loss : 0.18082, Sensitive_Acc : 16.100, Run Time : 6.57 sec
INFO:root:2024-04-28 06:58:09, Train, Epoch : 2, Step : 1230, Loss : 0.43034, Acc : 0.784, Sensitive_Loss : 0.16695, Sensitive_Acc : 16.000, Run Time : 7.51 sec
INFO:root:2024-04-28 06:58:16, Train, Epoch : 2, Step : 1240, Loss : 0.46951, Acc : 0.797, Sensitive_Loss : 0.20420, Sensitive_Acc : 16.700, Run Time : 6.99 sec
INFO:root:2024-04-28 06:58:24, Train, Epoch : 2, Step : 1250, Loss : 0.45574, Acc : 0.803, Sensitive_Loss : 0.15140, Sensitive_Acc : 16.400, Run Time : 7.57 sec
INFO:root:2024-04-28 06:59:58
INFO:root:y_pred: [0.11249881 0.85463107 0.18298963 ... 0.62457436 0.0925765  0.62070376]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.8449916e-01 1.3237737e-02 3.5845983e-01 4.3078964e-03 9.8274302e-01
 4.1331651e-04 9.9845779e-01 9.7327381e-01 1.6938684e-04 9.6744311e-01
 9.6614802e-01 9.9860746e-01 9.9762386e-01 8.2757151e-01 4.3309841e-02
 7.5918400e-01 9.9410927e-01 1.2562543e-03 2.0671820e-02 9.2570806e-01
 9.7362161e-01 9.2607653e-03 9.9879175e-01 9.4982249e-01 9.9227762e-01
 9.5295417e-01 1.3322885e-03 9.9621034e-01 9.4658715e-01 3.0566087e-01
 3.1131056e-01 7.5689018e-01 6.6277449e-04 2.5993651e-01 4.0438544e-02
 5.6355931e-02 2.8485132e-02 1.3341419e-02 9.5048392e-01 9.9254906e-01
 4.2390035e-05 7.7628290e-05 9.1219079e-01 2.3548859e-03 9.9873799e-01
 9.8800755e-01 9.7947538e-01 9.9382138e-01 5.4857717e-03 9.4305062e-01
 9.9462974e-01 2.0539867e-02 1.3775064e-01 7.4825069e-04 2.0392842e-04
 1.1020682e-01 2.0952821e-02 1.6643526e-02 2.5048295e-02 5.5792850e-01
 4.2895619e-02 8.9956515e-02 3.0740076e-01 8.0329597e-01 3.1286687e-01
 9.9624819e-01 4.1312543e-03 9.9610388e-01 7.9723614e-01 5.8364558e-01
 8.6188412e-01 7.9510075e-01 4.4915959e-02 6.7635842e-02 3.2577454e-03
 7.0361397e-04 1.0409654e-03 5.3131010e-02 6.1537125e-03 9.9407226e-01
 9.9463153e-01 1.9235912e-03 8.8768959e-01 1.8925745e-03 8.8321728e-01
 8.6720163e-01 5.3409096e-03 9.0261595e-03 9.6733618e-01 9.8792750e-01
 9.9828857e-01 2.2181803e-02 9.8900488e-03 9.9491864e-01 5.9429798e-02
 3.3429771e-04 9.8969191e-01 9.9107414e-01 1.6531297e-03 1.1289320e-01
 9.8359007e-01 9.2982554e-01 9.7848219e-01 9.9572325e-01 1.1420228e-02
 8.5228884e-01 8.7401456e-01 9.7141963e-01 8.5794139e-01 3.6020314e-03
 8.0627489e-01 5.2311987e-01 2.2345606e-02 9.9746239e-01 9.9276662e-01
 9.9587554e-01 9.4591361e-01 9.9721187e-01 3.1050283e-02 1.0072241e-01
 9.8277032e-01 9.9866033e-01 3.9355364e-04 9.3172622e-01 9.9831223e-01
 8.0268371e-01 9.7954285e-01 4.0730555e-03 2.4712276e-01 9.9060214e-01
 9.9200660e-01 1.0741364e-03 6.3488592e-04 1.3689472e-02 9.9324507e-01
 9.7878188e-01 9.7360557e-01 1.0583711e-04 3.5436414e-03 9.2230445e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 06:59:58, Dev, Step : 1252, Loss : 0.43893, Acc : 0.808, Auc : 0.891, Sensitive_Loss : 0.16590, Sensitive_Acc : 16.736, Sensitive_Auc : 0.980, Mean auc: 0.891, Run Time : 93.45 sec
INFO:root:2024-04-28 07:00:06, Train, Epoch : 3, Step : 1260, Loss : 0.29038, Acc : 0.659, Sensitive_Loss : 0.14240, Sensitive_Acc : 12.900, Run Time : 6.84 sec
INFO:root:2024-04-28 07:00:13, Train, Epoch : 3, Step : 1270, Loss : 0.38570, Acc : 0.822, Sensitive_Loss : 0.13955, Sensitive_Acc : 17.100, Run Time : 6.88 sec
INFO:root:2024-04-28 07:00:20, Train, Epoch : 3, Step : 1280, Loss : 0.45136, Acc : 0.791, Sensitive_Loss : 0.16211, Sensitive_Acc : 16.800, Run Time : 7.43 sec
INFO:root:2024-04-28 07:00:27, Train, Epoch : 3, Step : 1290, Loss : 0.43288, Acc : 0.825, Sensitive_Loss : 0.16176, Sensitive_Acc : 14.500, Run Time : 7.06 sec
INFO:root:2024-04-28 07:00:34, Train, Epoch : 3, Step : 1300, Loss : 0.37578, Acc : 0.819, Sensitive_Loss : 0.13167, Sensitive_Acc : 16.700, Run Time : 6.95 sec
INFO:root:2024-04-28 07:02:09, Dev, Step : 1300, Loss : 0.42142, Acc : 0.821, Auc : 0.897, Sensitive_Loss : 0.14742, Sensitive_Acc : 16.721, Sensitive_Auc : 0.985, Mean auc: 0.897, Run Time : 95.09 sec
INFO:root:2024-04-28 07:02:10, Best, Step : 1300, Loss : 0.42142, Acc : 0.821, Auc : 0.897, Sensitive_Loss : 0.14742, Sensitive_Acc : 16.721, Sensitive_Auc : 0.985, Best Auc : 0.897
INFO:root:2024-04-28 07:02:16, Train, Epoch : 3, Step : 1310, Loss : 0.32108, Acc : 0.844, Sensitive_Loss : 0.19210, Sensitive_Acc : 16.700, Run Time : 101.72 sec
INFO:root:2024-04-28 07:02:23, Train, Epoch : 3, Step : 1320, Loss : 0.37767, Acc : 0.866, Sensitive_Loss : 0.13613, Sensitive_Acc : 13.600, Run Time : 7.10 sec
INFO:root:2024-04-28 07:02:31, Train, Epoch : 3, Step : 1330, Loss : 0.35495, Acc : 0.859, Sensitive_Loss : 0.14555, Sensitive_Acc : 18.000, Run Time : 7.59 sec
INFO:root:2024-04-28 07:02:38, Train, Epoch : 3, Step : 1340, Loss : 0.36320, Acc : 0.869, Sensitive_Loss : 0.12514, Sensitive_Acc : 15.600, Run Time : 7.03 sec
INFO:root:2024-04-28 07:02:44, Train, Epoch : 3, Step : 1350, Loss : 0.31433, Acc : 0.884, Sensitive_Loss : 0.11289, Sensitive_Acc : 16.100, Run Time : 6.55 sec
INFO:root:2024-04-28 07:02:52, Train, Epoch : 3, Step : 1360, Loss : 0.46071, Acc : 0.797, Sensitive_Loss : 0.12861, Sensitive_Acc : 14.600, Run Time : 7.50 sec
INFO:root:2024-04-28 07:02:59, Train, Epoch : 3, Step : 1370, Loss : 0.31444, Acc : 0.872, Sensitive_Loss : 0.14644, Sensitive_Acc : 16.700, Run Time : 7.30 sec
INFO:root:2024-04-28 07:03:06, Train, Epoch : 3, Step : 1380, Loss : 0.42227, Acc : 0.800, Sensitive_Loss : 0.16876, Sensitive_Acc : 16.400, Run Time : 6.60 sec
INFO:root:2024-04-28 07:03:13, Train, Epoch : 3, Step : 1390, Loss : 0.41921, Acc : 0.797, Sensitive_Loss : 0.15255, Sensitive_Acc : 15.200, Run Time : 7.51 sec
INFO:root:2024-04-28 07:03:20, Train, Epoch : 3, Step : 1400, Loss : 0.35371, Acc : 0.841, Sensitive_Loss : 0.15649, Sensitive_Acc : 16.200, Run Time : 6.77 sec
INFO:root:2024-04-28 07:04:55, Dev, Step : 1400, Loss : 0.41754, Acc : 0.823, Auc : 0.902, Sensitive_Loss : 0.15522, Sensitive_Acc : 16.693, Sensitive_Auc : 0.985, Mean auc: 0.902, Run Time : 95.25 sec
INFO:root:2024-04-28 07:04:56, Best, Step : 1400, Loss : 0.41754, Acc : 0.823, Auc : 0.902, Sensitive_Loss : 0.15522, Sensitive_Acc : 16.693, Sensitive_Auc : 0.985, Best Auc : 0.902
INFO:root:2024-04-28 07:05:01, Train, Epoch : 3, Step : 1410, Loss : 0.31944, Acc : 0.866, Sensitive_Loss : 0.12468, Sensitive_Acc : 17.000, Run Time : 101.39 sec
INFO:root:2024-04-28 07:05:09, Train, Epoch : 3, Step : 1420, Loss : 0.34449, Acc : 0.844, Sensitive_Loss : 0.12333, Sensitive_Acc : 15.800, Run Time : 7.30 sec
INFO:root:2024-04-28 07:05:16, Train, Epoch : 3, Step : 1430, Loss : 0.35716, Acc : 0.819, Sensitive_Loss : 0.11947, Sensitive_Acc : 16.300, Run Time : 7.06 sec
INFO:root:2024-04-28 07:05:23, Train, Epoch : 3, Step : 1440, Loss : 0.31771, Acc : 0.847, Sensitive_Loss : 0.13764, Sensitive_Acc : 16.800, Run Time : 7.36 sec
INFO:root:2024-04-28 07:05:30, Train, Epoch : 3, Step : 1450, Loss : 0.41418, Acc : 0.825, Sensitive_Loss : 0.11054, Sensitive_Acc : 15.800, Run Time : 6.88 sec
INFO:root:2024-04-28 07:05:37, Train, Epoch : 3, Step : 1460, Loss : 0.37025, Acc : 0.859, Sensitive_Loss : 0.11691, Sensitive_Acc : 17.000, Run Time : 7.17 sec
INFO:root:2024-04-28 07:05:44, Train, Epoch : 3, Step : 1470, Loss : 0.39297, Acc : 0.841, Sensitive_Loss : 0.12258, Sensitive_Acc : 16.700, Run Time : 6.94 sec
INFO:root:2024-04-28 07:05:51, Train, Epoch : 3, Step : 1480, Loss : 0.33533, Acc : 0.881, Sensitive_Loss : 0.13258, Sensitive_Acc : 16.000, Run Time : 7.11 sec
INFO:root:2024-04-28 07:05:58, Train, Epoch : 3, Step : 1490, Loss : 0.39481, Acc : 0.822, Sensitive_Loss : 0.12259, Sensitive_Acc : 16.800, Run Time : 6.83 sec
INFO:root:2024-04-28 07:06:05, Train, Epoch : 3, Step : 1500, Loss : 0.32749, Acc : 0.884, Sensitive_Loss : 0.10010, Sensitive_Acc : 16.300, Run Time : 7.41 sec
INFO:root:2024-04-28 07:07:40, Dev, Step : 1500, Loss : 0.40724, Acc : 0.827, Auc : 0.903, Sensitive_Loss : 0.14602, Sensitive_Acc : 16.721, Sensitive_Auc : 0.987, Mean auc: 0.903, Run Time : 95.01 sec
INFO:root:2024-04-28 07:07:41, Best, Step : 1500, Loss : 0.40724, Acc : 0.827, Auc : 0.903, Sensitive_Loss : 0.14602, Sensitive_Acc : 16.721, Sensitive_Auc : 0.987, Best Auc : 0.903
INFO:root:2024-04-28 07:07:47, Train, Epoch : 3, Step : 1510, Loss : 0.34721, Acc : 0.850, Sensitive_Loss : 0.12972, Sensitive_Acc : 16.000, Run Time : 101.15 sec
INFO:root:2024-04-28 07:07:54, Train, Epoch : 3, Step : 1520, Loss : 0.32254, Acc : 0.838, Sensitive_Loss : 0.12365, Sensitive_Acc : 15.800, Run Time : 7.08 sec
INFO:root:2024-04-28 07:08:01, Train, Epoch : 3, Step : 1530, Loss : 0.37324, Acc : 0.812, Sensitive_Loss : 0.13505, Sensitive_Acc : 15.400, Run Time : 7.09 sec
INFO:root:2024-04-28 07:08:08, Train, Epoch : 3, Step : 1540, Loss : 0.34453, Acc : 0.825, Sensitive_Loss : 0.14814, Sensitive_Acc : 16.300, Run Time : 7.48 sec
INFO:root:2024-04-28 07:08:15, Train, Epoch : 3, Step : 1550, Loss : 0.44032, Acc : 0.809, Sensitive_Loss : 0.10249, Sensitive_Acc : 17.300, Run Time : 6.97 sec
INFO:root:2024-04-28 07:08:22, Train, Epoch : 3, Step : 1560, Loss : 0.34955, Acc : 0.856, Sensitive_Loss : 0.14718, Sensitive_Acc : 16.700, Run Time : 6.97 sec
INFO:root:2024-04-28 07:08:29, Train, Epoch : 3, Step : 1570, Loss : 0.34593, Acc : 0.844, Sensitive_Loss : 0.15725, Sensitive_Acc : 14.900, Run Time : 7.15 sec
INFO:root:2024-04-28 07:08:37, Train, Epoch : 3, Step : 1580, Loss : 0.35982, Acc : 0.869, Sensitive_Loss : 0.15154, Sensitive_Acc : 15.500, Run Time : 7.38 sec
INFO:root:2024-04-28 07:08:44, Train, Epoch : 3, Step : 1590, Loss : 0.42535, Acc : 0.812, Sensitive_Loss : 0.13341, Sensitive_Acc : 18.900, Run Time : 6.96 sec
INFO:root:2024-04-28 07:08:51, Train, Epoch : 3, Step : 1600, Loss : 0.46357, Acc : 0.806, Sensitive_Loss : 0.15496, Sensitive_Acc : 16.200, Run Time : 7.70 sec
INFO:root:2024-04-28 07:10:25, Dev, Step : 1600, Loss : 0.40950, Acc : 0.825, Auc : 0.903, Sensitive_Loss : 0.14436, Sensitive_Acc : 16.779, Sensitive_Auc : 0.987, Mean auc: 0.903, Run Time : 94.04 sec
INFO:root:2024-04-28 07:10:26, Best, Step : 1600, Loss : 0.40950, Acc : 0.825, Auc : 0.903, Sensitive_Loss : 0.14436, Sensitive_Acc : 16.779, Sensitive_Auc : 0.987, Best Auc : 0.903
INFO:root:2024-04-28 07:10:32, Train, Epoch : 3, Step : 1610, Loss : 0.41172, Acc : 0.831, Sensitive_Loss : 0.10855, Sensitive_Acc : 17.600, Run Time : 100.23 sec
INFO:root:2024-04-28 07:10:39, Train, Epoch : 3, Step : 1620, Loss : 0.35508, Acc : 0.816, Sensitive_Loss : 0.11275, Sensitive_Acc : 15.200, Run Time : 7.16 sec
INFO:root:2024-04-28 07:10:46, Train, Epoch : 3, Step : 1630, Loss : 0.29106, Acc : 0.887, Sensitive_Loss : 0.14912, Sensitive_Acc : 15.700, Run Time : 7.10 sec
INFO:root:2024-04-28 07:10:53, Train, Epoch : 3, Step : 1640, Loss : 0.34357, Acc : 0.841, Sensitive_Loss : 0.13154, Sensitive_Acc : 17.300, Run Time : 7.11 sec
INFO:root:2024-04-28 07:11:00, Train, Epoch : 3, Step : 1650, Loss : 0.38383, Acc : 0.838, Sensitive_Loss : 0.10314, Sensitive_Acc : 16.600, Run Time : 6.84 sec
INFO:root:2024-04-28 07:11:07, Train, Epoch : 3, Step : 1660, Loss : 0.34574, Acc : 0.838, Sensitive_Loss : 0.11628, Sensitive_Acc : 16.000, Run Time : 7.25 sec
INFO:root:2024-04-28 07:11:14, Train, Epoch : 3, Step : 1670, Loss : 0.32753, Acc : 0.869, Sensitive_Loss : 0.17400, Sensitive_Acc : 17.300, Run Time : 7.22 sec
INFO:root:2024-04-28 07:11:21, Train, Epoch : 3, Step : 1680, Loss : 0.35587, Acc : 0.834, Sensitive_Loss : 0.12275, Sensitive_Acc : 16.300, Run Time : 6.80 sec
INFO:root:2024-04-28 07:11:28, Train, Epoch : 3, Step : 1690, Loss : 0.39522, Acc : 0.847, Sensitive_Loss : 0.14469, Sensitive_Acc : 16.400, Run Time : 7.47 sec
INFO:root:2024-04-28 07:11:36, Train, Epoch : 3, Step : 1700, Loss : 0.37961, Acc : 0.838, Sensitive_Loss : 0.16072, Sensitive_Acc : 16.100, Run Time : 7.58 sec
INFO:root:2024-04-28 07:13:09, Dev, Step : 1700, Loss : 0.40368, Acc : 0.827, Auc : 0.905, Sensitive_Loss : 0.14406, Sensitive_Acc : 16.736, Sensitive_Auc : 0.985, Mean auc: 0.905, Run Time : 93.30 sec
INFO:root:2024-04-28 07:13:10, Best, Step : 1700, Loss : 0.40368, Acc : 0.827, Auc : 0.905, Sensitive_Loss : 0.14406, Sensitive_Acc : 16.736, Sensitive_Auc : 0.985, Best Auc : 0.905
INFO:root:2024-04-28 07:13:15, Train, Epoch : 3, Step : 1710, Loss : 0.35766, Acc : 0.841, Sensitive_Loss : 0.12932, Sensitive_Acc : 17.300, Run Time : 99.26 sec
INFO:root:2024-04-28 07:13:23, Train, Epoch : 3, Step : 1720, Loss : 0.39428, Acc : 0.809, Sensitive_Loss : 0.16665, Sensitive_Acc : 15.500, Run Time : 7.52 sec
INFO:root:2024-04-28 07:13:30, Train, Epoch : 3, Step : 1730, Loss : 0.37075, Acc : 0.859, Sensitive_Loss : 0.10246, Sensitive_Acc : 17.900, Run Time : 7.14 sec
INFO:root:2024-04-28 07:13:37, Train, Epoch : 3, Step : 1740, Loss : 0.32764, Acc : 0.841, Sensitive_Loss : 0.13337, Sensitive_Acc : 16.600, Run Time : 7.42 sec
INFO:root:2024-04-28 07:13:44, Train, Epoch : 3, Step : 1750, Loss : 0.33171, Acc : 0.847, Sensitive_Loss : 0.10310, Sensitive_Acc : 16.800, Run Time : 6.58 sec
INFO:root:2024-04-28 07:13:51, Train, Epoch : 3, Step : 1760, Loss : 0.42527, Acc : 0.800, Sensitive_Loss : 0.16999, Sensitive_Acc : 15.000, Run Time : 6.95 sec
INFO:root:2024-04-28 07:13:58, Train, Epoch : 3, Step : 1770, Loss : 0.31890, Acc : 0.834, Sensitive_Loss : 0.13371, Sensitive_Acc : 15.700, Run Time : 7.11 sec
INFO:root:2024-04-28 07:14:06, Train, Epoch : 3, Step : 1780, Loss : 0.33893, Acc : 0.841, Sensitive_Loss : 0.13357, Sensitive_Acc : 16.000, Run Time : 7.54 sec
INFO:root:2024-04-28 07:14:13, Train, Epoch : 3, Step : 1790, Loss : 0.35705, Acc : 0.841, Sensitive_Loss : 0.11497, Sensitive_Acc : 16.600, Run Time : 7.05 sec
INFO:root:2024-04-28 07:14:20, Train, Epoch : 3, Step : 1800, Loss : 0.35446, Acc : 0.838, Sensitive_Loss : 0.10906, Sensitive_Acc : 16.900, Run Time : 7.31 sec
INFO:root:2024-04-28 07:15:53, Dev, Step : 1800, Loss : 0.41445, Acc : 0.825, Auc : 0.906, Sensitive_Loss : 0.13931, Sensitive_Acc : 16.750, Sensitive_Auc : 0.985, Mean auc: 0.906, Run Time : 93.55 sec
INFO:root:2024-04-28 07:15:55, Best, Step : 1800, Loss : 0.41445, Acc : 0.825, Auc : 0.906, Sensitive_Loss : 0.13931, Sensitive_Acc : 16.750, Sensitive_Auc : 0.985, Best Auc : 0.906
INFO:root:2024-04-28 07:16:00, Train, Epoch : 3, Step : 1810, Loss : 0.36099, Acc : 0.831, Sensitive_Loss : 0.12088, Sensitive_Acc : 17.100, Run Time : 100.56 sec
INFO:root:2024-04-28 07:16:08, Train, Epoch : 3, Step : 1820, Loss : 0.39847, Acc : 0.825, Sensitive_Loss : 0.15702, Sensitive_Acc : 15.200, Run Time : 7.60 sec
INFO:root:2024-04-28 07:16:15, Train, Epoch : 3, Step : 1830, Loss : 0.31259, Acc : 0.850, Sensitive_Loss : 0.11878, Sensitive_Acc : 16.800, Run Time : 6.72 sec
INFO:root:2024-04-28 07:16:22, Train, Epoch : 3, Step : 1840, Loss : 0.40333, Acc : 0.828, Sensitive_Loss : 0.10869, Sensitive_Acc : 16.800, Run Time : 6.91 sec
INFO:root:2024-04-28 07:16:29, Train, Epoch : 3, Step : 1850, Loss : 0.33048, Acc : 0.872, Sensitive_Loss : 0.09604, Sensitive_Acc : 15.200, Run Time : 7.43 sec
INFO:root:2024-04-28 07:16:36, Train, Epoch : 3, Step : 1860, Loss : 0.35317, Acc : 0.872, Sensitive_Loss : 0.10514, Sensitive_Acc : 16.600, Run Time : 6.76 sec
INFO:root:2024-04-28 07:16:43, Train, Epoch : 3, Step : 1870, Loss : 0.41867, Acc : 0.806, Sensitive_Loss : 0.10341, Sensitive_Acc : 14.900, Run Time : 7.26 sec
INFO:root:2024-04-28 07:18:21
INFO:root:y_pred: [0.06908996 0.765425   0.0893814  ... 0.71104527 0.0241972  0.81405395]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.88877714e-01 7.76488101e-03 3.20211798e-01 4.64844890e-03
 9.91073072e-01 9.98725416e-04 9.94826734e-01 9.89364266e-01
 2.38046539e-03 8.83702159e-01 9.59173024e-01 9.99419212e-01
 9.94497716e-01 8.78480256e-01 3.43659744e-02 7.60956168e-01
 9.97693360e-01 9.17210523e-03 5.66456057e-02 9.75920677e-01
 9.81129467e-01 3.12942490e-02 9.99511957e-01 9.53434110e-01
 9.95227575e-01 9.51014757e-01 1.95159926e-03 9.95595276e-01
 9.68200922e-01 5.69024861e-01 2.01196849e-01 6.47268176e-01
 5.00082504e-03 1.91380858e-01 3.24194193e-01 4.11258899e-02
 9.02344659e-02 2.73743290e-02 9.74025786e-01 9.90158379e-01
 4.73239161e-05 2.86863098e-04 9.70115364e-01 2.16393359e-03
 9.99419928e-01 9.89503860e-01 9.82257903e-01 9.90844429e-01
 2.27739699e-02 9.66342747e-01 9.97838676e-01 5.07172272e-02
 4.33068782e-01 1.92099041e-03 5.83345245e-04 1.56694442e-01
 6.72188075e-03 1.11544669e-01 3.01269279e-03 5.02376139e-01
 1.91411376e-02 1.53458998e-01 1.76606074e-01 9.61118102e-01
 3.23310673e-01 9.93365586e-01 2.42296811e-02 9.96964872e-01
 8.67626965e-01 4.19061124e-01 9.12428558e-01 7.32322991e-01
 1.00793513e-02 2.97806233e-01 2.00421549e-03 6.92125352e-04
 6.95313327e-03 7.34001324e-02 4.57454380e-03 9.94590998e-01
 9.98191297e-01 3.60807776e-03 5.69921076e-01 1.95044791e-03
 9.48352337e-01 9.03852403e-01 1.80281177e-02 1.88781973e-02
 8.99412274e-01 9.94459510e-01 9.98212218e-01 4.14969362e-02
 4.92780609e-03 9.95060146e-01 1.11726046e-01 4.56182432e-04
 9.94972944e-01 9.89596307e-01 2.95611448e-03 2.88250856e-02
 9.82415676e-01 9.27380860e-01 9.90768731e-01 9.96827781e-01
 1.48582608e-02 4.66129184e-01 8.79405797e-01 9.76030827e-01
 9.16593730e-01 1.34130113e-03 8.72833550e-01 9.45513308e-01
 1.20487645e-01 9.97884452e-01 9.91932452e-01 9.96849000e-01
 9.48505282e-01 9.98457670e-01 1.53930053e-01 2.05742747e-01
 9.91039455e-01 9.97448981e-01 5.55395207e-04 9.33346629e-01
 9.98342395e-01 5.10905147e-01 9.89142954e-01 1.16982022e-02
 1.16846509e-01 9.89378393e-01 9.89918530e-01 9.65407398e-03
 1.01416353e-02 8.77388641e-02 9.71117735e-01 9.92234468e-01
 9.71953511e-01 8.36317427e-04 1.95338968e-02 9.67077792e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 07:18:21, Dev, Step : 1878, Loss : 0.41355, Acc : 0.821, Auc : 0.905, Sensitive_Loss : 0.14923, Sensitive_Acc : 16.707, Sensitive_Auc : 0.984, Mean auc: 0.905, Run Time : 92.34 sec
INFO:root:2024-04-28 07:18:25, Train, Epoch : 4, Step : 1880, Loss : 0.08182, Acc : 0.150, Sensitive_Loss : 0.02495, Sensitive_Acc : 3.300, Run Time : 2.89 sec
INFO:root:2024-04-28 07:18:32, Train, Epoch : 4, Step : 1890, Loss : 0.31229, Acc : 0.866, Sensitive_Loss : 0.09325, Sensitive_Acc : 16.500, Run Time : 7.13 sec
INFO:root:2024-04-28 07:18:39, Train, Epoch : 4, Step : 1900, Loss : 0.27768, Acc : 0.891, Sensitive_Loss : 0.09328, Sensitive_Acc : 16.300, Run Time : 7.13 sec
INFO:root:2024-04-28 07:20:12, Dev, Step : 1900, Loss : 0.44023, Acc : 0.809, Auc : 0.905, Sensitive_Loss : 0.15953, Sensitive_Acc : 16.779, Sensitive_Auc : 0.984, Mean auc: 0.905, Run Time : 93.48 sec
INFO:root:2024-04-28 07:20:18, Train, Epoch : 4, Step : 1910, Loss : 0.30077, Acc : 0.869, Sensitive_Loss : 0.11825, Sensitive_Acc : 15.300, Run Time : 99.23 sec
INFO:root:2024-04-28 07:20:25, Train, Epoch : 4, Step : 1920, Loss : 0.31524, Acc : 0.884, Sensitive_Loss : 0.12518, Sensitive_Acc : 16.000, Run Time : 6.99 sec
INFO:root:2024-04-28 07:20:32, Train, Epoch : 4, Step : 1930, Loss : 0.35093, Acc : 0.856, Sensitive_Loss : 0.11695, Sensitive_Acc : 14.200, Run Time : 7.07 sec
INFO:root:2024-04-28 07:20:39, Train, Epoch : 4, Step : 1940, Loss : 0.33989, Acc : 0.853, Sensitive_Loss : 0.15528, Sensitive_Acc : 15.700, Run Time : 7.09 sec
INFO:root:2024-04-28 07:20:47, Train, Epoch : 4, Step : 1950, Loss : 0.37850, Acc : 0.819, Sensitive_Loss : 0.08528, Sensitive_Acc : 16.200, Run Time : 7.66 sec
INFO:root:2024-04-28 07:20:54, Train, Epoch : 4, Step : 1960, Loss : 0.35563, Acc : 0.844, Sensitive_Loss : 0.12845, Sensitive_Acc : 15.300, Run Time : 6.76 sec
INFO:root:2024-04-28 07:21:00, Train, Epoch : 4, Step : 1970, Loss : 0.37964, Acc : 0.841, Sensitive_Loss : 0.10073, Sensitive_Acc : 15.600, Run Time : 6.74 sec
INFO:root:2024-04-28 07:21:07, Train, Epoch : 4, Step : 1980, Loss : 0.36348, Acc : 0.828, Sensitive_Loss : 0.13021, Sensitive_Acc : 14.900, Run Time : 6.86 sec
INFO:root:2024-04-28 07:21:15, Train, Epoch : 4, Step : 1990, Loss : 0.36378, Acc : 0.866, Sensitive_Loss : 0.09405, Sensitive_Acc : 17.000, Run Time : 7.36 sec
INFO:root:2024-04-28 07:21:22, Train, Epoch : 4, Step : 2000, Loss : 0.42523, Acc : 0.828, Sensitive_Loss : 0.14478, Sensitive_Acc : 15.300, Run Time : 7.45 sec
INFO:root:2024-04-28 07:22:56, Dev, Step : 2000, Loss : 0.42086, Acc : 0.817, Auc : 0.907, Sensitive_Loss : 0.14494, Sensitive_Acc : 16.807, Sensitive_Auc : 0.986, Mean auc: 0.907, Run Time : 94.20 sec
INFO:root:2024-04-28 07:22:57, Best, Step : 2000, Loss : 0.42086, Acc : 0.817, Auc : 0.907, Sensitive_Loss : 0.14494, Sensitive_Acc : 16.807, Sensitive_Auc : 0.986, Best Auc : 0.907
INFO:root:2024-04-28 07:23:03, Train, Epoch : 4, Step : 2010, Loss : 0.26418, Acc : 0.887, Sensitive_Loss : 0.13942, Sensitive_Acc : 17.500, Run Time : 100.71 sec
INFO:root:2024-04-28 07:23:09, Train, Epoch : 4, Step : 2020, Loss : 0.39857, Acc : 0.803, Sensitive_Loss : 0.10900, Sensitive_Acc : 16.900, Run Time : 6.65 sec
INFO:root:2024-04-28 07:23:17, Train, Epoch : 4, Step : 2030, Loss : 0.35272, Acc : 0.866, Sensitive_Loss : 0.12992, Sensitive_Acc : 15.700, Run Time : 7.26 sec
INFO:root:2024-04-28 07:23:24, Train, Epoch : 4, Step : 2040, Loss : 0.37630, Acc : 0.850, Sensitive_Loss : 0.13660, Sensitive_Acc : 16.500, Run Time : 7.67 sec
INFO:root:2024-04-28 07:23:31, Train, Epoch : 4, Step : 2050, Loss : 0.41121, Acc : 0.825, Sensitive_Loss : 0.10309, Sensitive_Acc : 16.800, Run Time : 6.21 sec
INFO:root:2024-04-28 07:23:38, Train, Epoch : 4, Step : 2060, Loss : 0.36339, Acc : 0.838, Sensitive_Loss : 0.10792, Sensitive_Acc : 17.100, Run Time : 7.44 sec
INFO:root:2024-04-28 07:23:45, Train, Epoch : 4, Step : 2070, Loss : 0.33022, Acc : 0.844, Sensitive_Loss : 0.09287, Sensitive_Acc : 16.700, Run Time : 7.33 sec
INFO:root:2024-04-28 07:23:52, Train, Epoch : 4, Step : 2080, Loss : 0.31577, Acc : 0.875, Sensitive_Loss : 0.16417, Sensitive_Acc : 16.600, Run Time : 7.00 sec
INFO:root:2024-04-28 07:24:00, Train, Epoch : 4, Step : 2090, Loss : 0.39422, Acc : 0.828, Sensitive_Loss : 0.12533, Sensitive_Acc : 16.800, Run Time : 7.24 sec
INFO:root:2024-04-28 07:24:06, Train, Epoch : 4, Step : 2100, Loss : 0.36789, Acc : 0.831, Sensitive_Loss : 0.11484, Sensitive_Acc : 15.700, Run Time : 6.67 sec
INFO:root:2024-04-28 07:25:41, Dev, Step : 2100, Loss : 0.40627, Acc : 0.825, Auc : 0.907, Sensitive_Loss : 0.13790, Sensitive_Acc : 16.793, Sensitive_Auc : 0.986, Mean auc: 0.907, Run Time : 94.52 sec
INFO:root:2024-04-28 07:25:41, Best, Step : 2100, Loss : 0.40627, Acc : 0.825, Auc : 0.907, Sensitive_Loss : 0.13790, Sensitive_Acc : 16.793, Sensitive_Auc : 0.986, Best Auc : 0.907
INFO:root:2024-04-28 07:25:47, Train, Epoch : 4, Step : 2110, Loss : 0.32415, Acc : 0.838, Sensitive_Loss : 0.12939, Sensitive_Acc : 15.000, Run Time : 100.53 sec
INFO:root:2024-04-28 07:25:54, Train, Epoch : 4, Step : 2120, Loss : 0.26960, Acc : 0.866, Sensitive_Loss : 0.13094, Sensitive_Acc : 15.800, Run Time : 7.13 sec
INFO:root:2024-04-28 07:26:01, Train, Epoch : 4, Step : 2130, Loss : 0.31782, Acc : 0.872, Sensitive_Loss : 0.14345, Sensitive_Acc : 15.400, Run Time : 7.33 sec
INFO:root:2024-04-28 07:26:09, Train, Epoch : 4, Step : 2140, Loss : 0.29640, Acc : 0.909, Sensitive_Loss : 0.12058, Sensitive_Acc : 14.700, Run Time : 7.30 sec
INFO:root:2024-04-28 07:26:15, Train, Epoch : 4, Step : 2150, Loss : 0.33652, Acc : 0.875, Sensitive_Loss : 0.11875, Sensitive_Acc : 16.800, Run Time : 6.93 sec
INFO:root:2024-04-28 07:26:23, Train, Epoch : 4, Step : 2160, Loss : 0.28942, Acc : 0.863, Sensitive_Loss : 0.10610, Sensitive_Acc : 15.800, Run Time : 7.11 sec
INFO:root:2024-04-28 07:26:30, Train, Epoch : 4, Step : 2170, Loss : 0.28212, Acc : 0.866, Sensitive_Loss : 0.17132, Sensitive_Acc : 17.300, Run Time : 7.02 sec
INFO:root:2024-04-28 07:26:37, Train, Epoch : 4, Step : 2180, Loss : 0.37261, Acc : 0.816, Sensitive_Loss : 0.16227, Sensitive_Acc : 16.000, Run Time : 7.45 sec
INFO:root:2024-04-28 07:26:44, Train, Epoch : 4, Step : 2190, Loss : 0.31006, Acc : 0.853, Sensitive_Loss : 0.10714, Sensitive_Acc : 15.200, Run Time : 7.07 sec
INFO:root:2024-04-28 07:26:51, Train, Epoch : 4, Step : 2200, Loss : 0.37276, Acc : 0.850, Sensitive_Loss : 0.10040, Sensitive_Acc : 15.000, Run Time : 6.99 sec
INFO:root:2024-04-28 07:28:25, Dev, Step : 2200, Loss : 0.41451, Acc : 0.824, Auc : 0.906, Sensitive_Loss : 0.14426, Sensitive_Acc : 16.850, Sensitive_Auc : 0.986, Mean auc: 0.906, Run Time : 94.10 sec
INFO:root:2024-04-28 07:28:31, Train, Epoch : 4, Step : 2210, Loss : 0.33750, Acc : 0.838, Sensitive_Loss : 0.12513, Sensitive_Acc : 15.900, Run Time : 99.69 sec
INFO:root:2024-04-28 07:28:38, Train, Epoch : 4, Step : 2220, Loss : 0.39573, Acc : 0.856, Sensitive_Loss : 0.11630, Sensitive_Acc : 17.000, Run Time : 7.16 sec
INFO:root:2024-04-28 07:28:45, Train, Epoch : 4, Step : 2230, Loss : 0.27666, Acc : 0.897, Sensitive_Loss : 0.14396, Sensitive_Acc : 15.300, Run Time : 6.89 sec
INFO:root:2024-04-28 07:28:52, Train, Epoch : 4, Step : 2240, Loss : 0.39683, Acc : 0.844, Sensitive_Loss : 0.11974, Sensitive_Acc : 16.300, Run Time : 6.95 sec
INFO:root:2024-04-28 07:28:59, Train, Epoch : 4, Step : 2250, Loss : 0.30445, Acc : 0.866, Sensitive_Loss : 0.12045, Sensitive_Acc : 18.200, Run Time : 7.18 sec
INFO:root:2024-04-28 07:29:07, Train, Epoch : 4, Step : 2260, Loss : 0.30772, Acc : 0.863, Sensitive_Loss : 0.15156, Sensitive_Acc : 16.200, Run Time : 7.89 sec
INFO:root:2024-04-28 07:29:13, Train, Epoch : 4, Step : 2270, Loss : 0.30471, Acc : 0.859, Sensitive_Loss : 0.12051, Sensitive_Acc : 15.300, Run Time : 6.62 sec
INFO:root:2024-04-28 07:29:21, Train, Epoch : 4, Step : 2280, Loss : 0.37032, Acc : 0.847, Sensitive_Loss : 0.09321, Sensitive_Acc : 14.600, Run Time : 7.44 sec
INFO:root:2024-04-28 07:29:28, Train, Epoch : 4, Step : 2290, Loss : 0.34792, Acc : 0.834, Sensitive_Loss : 0.13473, Sensitive_Acc : 16.000, Run Time : 6.70 sec
INFO:root:2024-04-28 07:29:35, Train, Epoch : 4, Step : 2300, Loss : 0.42208, Acc : 0.794, Sensitive_Loss : 0.16759, Sensitive_Acc : 17.700, Run Time : 7.12 sec
INFO:root:2024-04-28 07:31:08, Dev, Step : 2300, Loss : 0.42801, Acc : 0.814, Auc : 0.908, Sensitive_Loss : 0.15329, Sensitive_Acc : 16.764, Sensitive_Auc : 0.986, Mean auc: 0.908, Run Time : 93.39 sec
INFO:root:2024-04-28 07:31:09, Best, Step : 2300, Loss : 0.42801, Acc : 0.814, Auc : 0.908, Sensitive_Loss : 0.15329, Sensitive_Acc : 16.764, Sensitive_Auc : 0.986, Best Auc : 0.908
INFO:root:2024-04-28 07:31:15, Train, Epoch : 4, Step : 2310, Loss : 0.29091, Acc : 0.872, Sensitive_Loss : 0.09173, Sensitive_Acc : 17.000, Run Time : 99.77 sec
INFO:root:2024-04-28 07:31:22, Train, Epoch : 4, Step : 2320, Loss : 0.39655, Acc : 0.828, Sensitive_Loss : 0.12693, Sensitive_Acc : 16.100, Run Time : 6.98 sec
INFO:root:2024-04-28 07:31:29, Train, Epoch : 4, Step : 2330, Loss : 0.40737, Acc : 0.819, Sensitive_Loss : 0.13580, Sensitive_Acc : 16.700, Run Time : 7.19 sec
INFO:root:2024-04-28 07:31:36, Train, Epoch : 4, Step : 2340, Loss : 0.34999, Acc : 0.834, Sensitive_Loss : 0.08659, Sensitive_Acc : 17.100, Run Time : 7.36 sec
INFO:root:2024-04-28 07:31:43, Train, Epoch : 4, Step : 2350, Loss : 0.37405, Acc : 0.844, Sensitive_Loss : 0.12887, Sensitive_Acc : 17.200, Run Time : 7.17 sec
INFO:root:2024-04-28 07:31:50, Train, Epoch : 4, Step : 2360, Loss : 0.36548, Acc : 0.866, Sensitive_Loss : 0.12004, Sensitive_Acc : 16.600, Run Time : 6.98 sec
INFO:root:2024-04-28 07:31:57, Train, Epoch : 4, Step : 2370, Loss : 0.32884, Acc : 0.863, Sensitive_Loss : 0.11017, Sensitive_Acc : 15.300, Run Time : 7.06 sec
INFO:root:2024-04-28 07:32:05, Train, Epoch : 4, Step : 2380, Loss : 0.35779, Acc : 0.859, Sensitive_Loss : 0.10819, Sensitive_Acc : 16.500, Run Time : 7.30 sec
INFO:root:2024-04-28 07:32:11, Train, Epoch : 4, Step : 2390, Loss : 0.31131, Acc : 0.875, Sensitive_Loss : 0.08468, Sensitive_Acc : 17.400, Run Time : 6.85 sec
INFO:root:2024-04-28 07:32:19, Train, Epoch : 4, Step : 2400, Loss : 0.29972, Acc : 0.872, Sensitive_Loss : 0.11282, Sensitive_Acc : 17.300, Run Time : 7.14 sec
INFO:root:2024-04-28 07:33:53, Dev, Step : 2400, Loss : 0.40772, Acc : 0.828, Auc : 0.907, Sensitive_Loss : 0.13987, Sensitive_Acc : 16.850, Sensitive_Auc : 0.986, Mean auc: 0.907, Run Time : 94.17 sec
INFO:root:2024-04-28 07:33:58, Train, Epoch : 4, Step : 2410, Loss : 0.32355, Acc : 0.841, Sensitive_Loss : 0.13760, Sensitive_Acc : 16.900, Run Time : 99.59 sec
INFO:root:2024-04-28 07:34:05, Train, Epoch : 4, Step : 2420, Loss : 0.35645, Acc : 0.825, Sensitive_Loss : 0.11600, Sensitive_Acc : 16.400, Run Time : 7.03 sec
INFO:root:2024-04-28 07:34:13, Train, Epoch : 4, Step : 2430, Loss : 0.32798, Acc : 0.856, Sensitive_Loss : 0.09944, Sensitive_Acc : 15.700, Run Time : 7.42 sec
INFO:root:2024-04-28 07:34:20, Train, Epoch : 4, Step : 2440, Loss : 0.31793, Acc : 0.859, Sensitive_Loss : 0.15349, Sensitive_Acc : 16.700, Run Time : 7.29 sec
INFO:root:2024-04-28 07:34:27, Train, Epoch : 4, Step : 2450, Loss : 0.32411, Acc : 0.859, Sensitive_Loss : 0.10131, Sensitive_Acc : 16.100, Run Time : 6.97 sec
INFO:root:2024-04-28 07:34:34, Train, Epoch : 4, Step : 2460, Loss : 0.28077, Acc : 0.887, Sensitive_Loss : 0.11370, Sensitive_Acc : 16.600, Run Time : 7.39 sec
INFO:root:2024-04-28 07:34:41, Train, Epoch : 4, Step : 2470, Loss : 0.31847, Acc : 0.856, Sensitive_Loss : 0.11370, Sensitive_Acc : 16.000, Run Time : 6.73 sec
INFO:root:2024-04-28 07:34:48, Train, Epoch : 4, Step : 2480, Loss : 0.32430, Acc : 0.853, Sensitive_Loss : 0.12091, Sensitive_Acc : 15.800, Run Time : 7.26 sec
INFO:root:2024-04-28 07:34:55, Train, Epoch : 4, Step : 2490, Loss : 0.36976, Acc : 0.834, Sensitive_Loss : 0.14312, Sensitive_Acc : 15.400, Run Time : 7.07 sec
INFO:root:2024-04-28 07:35:03, Train, Epoch : 4, Step : 2500, Loss : 0.36443, Acc : 0.856, Sensitive_Loss : 0.10530, Sensitive_Acc : 15.900, Run Time : 7.21 sec
INFO:root:2024-04-28 07:36:37, Dev, Step : 2500, Loss : 0.41580, Acc : 0.825, Auc : 0.908, Sensitive_Loss : 0.14890, Sensitive_Acc : 16.793, Sensitive_Auc : 0.985, Mean auc: 0.908, Run Time : 94.14 sec
INFO:root:2024-04-28 07:36:37, Best, Step : 2500, Loss : 0.41580, Acc : 0.825, Auc : 0.908, Sensitive_Loss : 0.14890, Sensitive_Acc : 16.793, Sensitive_Auc : 0.985, Best Auc : 0.908
INFO:root:2024-04-28 07:38:10
INFO:root:y_pred: [0.11004305 0.7847656  0.0454006  ... 0.77992874 0.01342304 0.80888796]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.78987157e-01 2.74237338e-03 2.78008550e-01 4.55046818e-03
 9.96415973e-01 2.12654704e-03 9.96962726e-01 9.93537366e-01
 1.20397098e-03 8.77016842e-01 9.71433520e-01 9.99431193e-01
 9.94926810e-01 9.05390322e-01 2.64704116e-02 8.52298915e-01
 9.98170376e-01 1.02337925e-02 6.42037392e-02 9.76079643e-01
 9.82228100e-01 4.70809266e-02 9.99761045e-01 9.61789608e-01
 9.97665763e-01 9.77798998e-01 2.13537365e-03 9.96095836e-01
 9.78921711e-01 4.89961654e-01 5.31073250e-02 4.49277431e-01
 3.25460359e-03 2.19337955e-01 1.34411484e-01 9.36537981e-02
 6.69780821e-02 2.53722239e-02 9.84406233e-01 9.93162870e-01
 7.89203987e-05 1.43872472e-04 9.80723441e-01 3.07352957e-03
 9.99593437e-01 9.91493165e-01 9.88543153e-01 9.92656648e-01
 3.67263593e-02 9.81401503e-01 9.98317480e-01 4.44752984e-02
 3.29477698e-01 4.05263249e-03 8.33025901e-04 1.03930861e-01
 1.28639899e-02 1.57164410e-01 3.35887307e-03 4.37665373e-01
 2.18066573e-02 8.67110714e-02 5.69085069e-02 9.63912547e-01
 1.45274341e-01 9.93531168e-01 2.83003300e-02 9.97375727e-01
 9.14408922e-01 1.87102094e-01 9.36311245e-01 7.61350870e-01
 9.45476908e-03 2.93053895e-01 2.92904652e-03 1.42266916e-03
 4.33297688e-03 7.52821192e-02 6.22234773e-03 9.96710896e-01
 9.98517931e-01 4.96848347e-03 6.56474650e-01 4.54479130e-03
 9.59024370e-01 9.41320181e-01 3.07416376e-02 2.97762305e-02
 9.09512460e-01 9.93651092e-01 9.98739064e-01 1.91528779e-02
 2.80650239e-03 9.95271981e-01 5.78289367e-02 2.93198624e-04
 9.92939591e-01 9.95501697e-01 4.61399369e-03 2.82349382e-02
 9.87673104e-01 9.50205922e-01 9.92566466e-01 9.95753884e-01
 8.63641687e-03 6.02727979e-02 9.23320234e-01 9.83396113e-01
 9.19397771e-01 6.63635961e-04 9.10392344e-01 9.84001398e-01
 1.17969751e-01 9.98193085e-01 9.93348181e-01 9.96035993e-01
 9.67274189e-01 9.98124540e-01 1.70196772e-01 9.74667743e-02
 9.93675053e-01 9.96873975e-01 8.53818608e-04 9.49958324e-01
 9.98305678e-01 6.95474207e-01 9.86057818e-01 8.68806336e-03
 3.18903066e-02 9.89555120e-01 9.91416872e-01 1.13212503e-02
 1.35272611e-02 1.52839720e-01 9.81962144e-01 9.92628217e-01
 9.71595824e-01 8.21477792e-04 7.51230121e-02 9.85862672e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 07:38:10, Dev, Step : 2504, Loss : 0.41319, Acc : 0.825, Auc : 0.908, Sensitive_Loss : 0.14425, Sensitive_Acc : 16.793, Sensitive_Auc : 0.986, Mean auc: 0.908, Run Time : 91.50 sec
INFO:root:2024-04-28 07:38:11, Best, Step : 2504, Loss : 0.41319, Acc : 0.825,Auc : 0.908, Best Auc : 0.908, Sensitive_Loss : 0.14425, Sensitive_Acc : 16.793, Sensitive_Auc : 0.986
INFO:root:2024-04-28 07:38:17, Train, Epoch : 5, Step : 2510, Loss : 0.23294, Acc : 0.487, Sensitive_Loss : 0.09484, Sensitive_Acc : 10.300, Run Time : 5.39 sec
INFO:root:2024-04-28 07:38:24, Train, Epoch : 5, Step : 2520, Loss : 0.30965, Acc : 0.841, Sensitive_Loss : 0.11323, Sensitive_Acc : 16.100, Run Time : 7.04 sec
INFO:root:2024-04-28 07:38:31, Train, Epoch : 5, Step : 2530, Loss : 0.35990, Acc : 0.825, Sensitive_Loss : 0.10791, Sensitive_Acc : 17.100, Run Time : 7.03 sec
INFO:root:2024-04-28 07:38:39, Train, Epoch : 5, Step : 2540, Loss : 0.32761, Acc : 0.872, Sensitive_Loss : 0.12776, Sensitive_Acc : 16.500, Run Time : 7.11 sec
INFO:root:2024-04-28 07:38:46, Train, Epoch : 5, Step : 2550, Loss : 0.30407, Acc : 0.859, Sensitive_Loss : 0.13488, Sensitive_Acc : 17.100, Run Time : 7.27 sec
INFO:root:2024-04-28 07:38:53, Train, Epoch : 5, Step : 2560, Loss : 0.31266, Acc : 0.859, Sensitive_Loss : 0.13329, Sensitive_Acc : 16.400, Run Time : 7.53 sec
INFO:root:2024-04-28 07:39:00, Train, Epoch : 5, Step : 2570, Loss : 0.26119, Acc : 0.900, Sensitive_Loss : 0.10215, Sensitive_Acc : 16.700, Run Time : 6.91 sec
INFO:root:2024-04-28 07:39:07, Train, Epoch : 5, Step : 2580, Loss : 0.32518, Acc : 0.881, Sensitive_Loss : 0.12357, Sensitive_Acc : 16.900, Run Time : 7.10 sec
INFO:root:2024-04-28 07:39:14, Train, Epoch : 5, Step : 2590, Loss : 0.37275, Acc : 0.853, Sensitive_Loss : 0.10882, Sensitive_Acc : 15.300, Run Time : 6.91 sec
INFO:root:2024-04-28 07:39:22, Train, Epoch : 5, Step : 2600, Loss : 0.29798, Acc : 0.884, Sensitive_Loss : 0.09998, Sensitive_Acc : 17.300, Run Time : 7.25 sec
INFO:root:2024-04-28 07:40:55, Dev, Step : 2600, Loss : 0.40209, Acc : 0.828, Auc : 0.909, Sensitive_Loss : 0.12567, Sensitive_Acc : 16.807, Sensitive_Auc : 0.986, Mean auc: 0.909, Run Time : 93.54 sec
INFO:root:2024-04-28 07:40:56, Best, Step : 2600, Loss : 0.40209, Acc : 0.828, Auc : 0.909, Sensitive_Loss : 0.12567, Sensitive_Acc : 16.807, Sensitive_Auc : 0.986, Best Auc : 0.909
INFO:root:2024-04-28 07:41:01, Train, Epoch : 5, Step : 2610, Loss : 0.30657, Acc : 0.853, Sensitive_Loss : 0.12148, Sensitive_Acc : 15.500, Run Time : 99.79 sec
INFO:root:2024-04-28 07:41:08, Train, Epoch : 5, Step : 2620, Loss : 0.30521, Acc : 0.866, Sensitive_Loss : 0.13652, Sensitive_Acc : 16.600, Run Time : 6.85 sec
INFO:root:2024-04-28 07:41:15, Train, Epoch : 5, Step : 2630, Loss : 0.32758, Acc : 0.838, Sensitive_Loss : 0.13968, Sensitive_Acc : 15.800, Run Time : 6.99 sec
INFO:root:2024-04-28 07:41:23, Train, Epoch : 5, Step : 2640, Loss : 0.31591, Acc : 0.872, Sensitive_Loss : 0.09357, Sensitive_Acc : 15.700, Run Time : 7.36 sec
INFO:root:2024-04-28 07:41:30, Train, Epoch : 5, Step : 2650, Loss : 0.31118, Acc : 0.847, Sensitive_Loss : 0.13372, Sensitive_Acc : 15.400, Run Time : 7.31 sec
INFO:root:2024-04-28 07:41:37, Train, Epoch : 5, Step : 2660, Loss : 0.32286, Acc : 0.875, Sensitive_Loss : 0.09516, Sensitive_Acc : 17.100, Run Time : 7.06 sec
INFO:root:2024-04-28 07:41:44, Train, Epoch : 5, Step : 2670, Loss : 0.37929, Acc : 0.844, Sensitive_Loss : 0.10944, Sensitive_Acc : 15.900, Run Time : 7.02 sec
INFO:root:2024-04-28 07:41:51, Train, Epoch : 5, Step : 2680, Loss : 0.33431, Acc : 0.850, Sensitive_Loss : 0.08209, Sensitive_Acc : 17.600, Run Time : 7.23 sec
INFO:root:2024-04-28 07:41:58, Train, Epoch : 5, Step : 2690, Loss : 0.30386, Acc : 0.850, Sensitive_Loss : 0.14692, Sensitive_Acc : 16.600, Run Time : 7.13 sec
INFO:root:2024-04-28 07:42:05, Train, Epoch : 5, Step : 2700, Loss : 0.29496, Acc : 0.872, Sensitive_Loss : 0.09868, Sensitive_Acc : 16.500, Run Time : 7.17 sec
INFO:root:2024-04-28 07:43:39, Dev, Step : 2700, Loss : 0.42388, Acc : 0.814, Auc : 0.905, Sensitive_Loss : 0.13937, Sensitive_Acc : 16.750, Sensitive_Auc : 0.988, Mean auc: 0.905, Run Time : 93.78 sec
INFO:root:2024-04-28 07:43:45, Train, Epoch : 5, Step : 2710, Loss : 0.30455, Acc : 0.872, Sensitive_Loss : 0.11619, Sensitive_Acc : 17.100, Run Time : 99.46 sec
INFO:root:2024-04-28 07:43:52, Train, Epoch : 5, Step : 2720, Loss : 0.29366, Acc : 0.859, Sensitive_Loss : 0.09680, Sensitive_Acc : 16.100, Run Time : 7.45 sec
INFO:root:2024-04-28 07:44:00, Train, Epoch : 5, Step : 2730, Loss : 0.29980, Acc : 0.866, Sensitive_Loss : 0.10679, Sensitive_Acc : 14.400, Run Time : 7.30 sec
INFO:root:2024-04-28 07:44:07, Train, Epoch : 5, Step : 2740, Loss : 0.30949, Acc : 0.863, Sensitive_Loss : 0.11876, Sensitive_Acc : 16.300, Run Time : 6.98 sec
INFO:root:2024-04-28 07:44:13, Train, Epoch : 5, Step : 2750, Loss : 0.30780, Acc : 0.869, Sensitive_Loss : 0.09865, Sensitive_Acc : 15.300, Run Time : 6.66 sec
INFO:root:2024-04-28 07:44:21, Train, Epoch : 5, Step : 2760, Loss : 0.32253, Acc : 0.872, Sensitive_Loss : 0.10838, Sensitive_Acc : 17.000, Run Time : 7.16 sec
INFO:root:2024-04-28 07:44:28, Train, Epoch : 5, Step : 2770, Loss : 0.35999, Acc : 0.838, Sensitive_Loss : 0.14823, Sensitive_Acc : 15.700, Run Time : 7.33 sec
INFO:root:2024-04-28 07:44:35, Train, Epoch : 5, Step : 2780, Loss : 0.33764, Acc : 0.863, Sensitive_Loss : 0.08703, Sensitive_Acc : 16.800, Run Time : 6.95 sec
INFO:root:2024-04-28 07:44:42, Train, Epoch : 5, Step : 2790, Loss : 0.30757, Acc : 0.863, Sensitive_Loss : 0.12749, Sensitive_Acc : 17.700, Run Time : 7.24 sec
INFO:root:2024-04-28 07:44:49, Train, Epoch : 5, Step : 2800, Loss : 0.32290, Acc : 0.872, Sensitive_Loss : 0.11748, Sensitive_Acc : 15.400, Run Time : 6.74 sec
INFO:root:2024-04-28 07:46:23, Dev, Step : 2800, Loss : 0.41509, Acc : 0.824, Auc : 0.907, Sensitive_Loss : 0.13424, Sensitive_Acc : 16.750, Sensitive_Auc : 0.986, Mean auc: 0.907, Run Time : 94.02 sec
INFO:root:2024-04-28 07:46:28, Train, Epoch : 5, Step : 2810, Loss : 0.32032, Acc : 0.847, Sensitive_Loss : 0.09605, Sensitive_Acc : 17.100, Run Time : 99.57 sec
INFO:root:2024-04-28 07:46:36, Train, Epoch : 5, Step : 2820, Loss : 0.30035, Acc : 0.891, Sensitive_Loss : 0.13167, Sensitive_Acc : 15.300, Run Time : 7.33 sec
INFO:root:2024-04-28 07:46:43, Train, Epoch : 5, Step : 2830, Loss : 0.32402, Acc : 0.853, Sensitive_Loss : 0.10490, Sensitive_Acc : 14.900, Run Time : 6.96 sec
INFO:root:2024-04-28 07:46:50, Train, Epoch : 5, Step : 2840, Loss : 0.28200, Acc : 0.891, Sensitive_Loss : 0.10774, Sensitive_Acc : 16.000, Run Time : 7.29 sec
INFO:root:2024-04-28 07:46:57, Train, Epoch : 5, Step : 2850, Loss : 0.30877, Acc : 0.863, Sensitive_Loss : 0.10406, Sensitive_Acc : 16.200, Run Time : 7.09 sec
INFO:root:2024-04-28 07:47:04, Train, Epoch : 5, Step : 2860, Loss : 0.31306, Acc : 0.878, Sensitive_Loss : 0.09102, Sensitive_Acc : 16.300, Run Time : 7.23 sec
INFO:root:2024-04-28 07:47:11, Train, Epoch : 5, Step : 2870, Loss : 0.30087, Acc : 0.872, Sensitive_Loss : 0.11139, Sensitive_Acc : 16.300, Run Time : 7.09 sec
INFO:root:2024-04-28 07:47:19, Train, Epoch : 5, Step : 2880, Loss : 0.33243, Acc : 0.872, Sensitive_Loss : 0.09568, Sensitive_Acc : 16.500, Run Time : 7.44 sec
INFO:root:2024-04-28 07:47:26, Train, Epoch : 5, Step : 2890, Loss : 0.31159, Acc : 0.869, Sensitive_Loss : 0.12767, Sensitive_Acc : 16.100, Run Time : 7.23 sec
INFO:root:2024-04-28 07:47:33, Train, Epoch : 5, Step : 2900, Loss : 0.39467, Acc : 0.834, Sensitive_Loss : 0.10225, Sensitive_Acc : 16.400, Run Time : 7.23 sec
INFO:root:2024-04-28 07:49:07, Dev, Step : 2900, Loss : 0.40554, Acc : 0.826, Auc : 0.906, Sensitive_Loss : 0.12111, Sensitive_Acc : 16.879, Sensitive_Auc : 0.986, Mean auc: 0.906, Run Time : 93.34 sec
INFO:root:2024-04-28 07:49:12, Train, Epoch : 5, Step : 2910, Loss : 0.34061, Acc : 0.866, Sensitive_Loss : 0.08423, Sensitive_Acc : 16.800, Run Time : 98.73 sec
INFO:root:2024-04-28 07:49:19, Train, Epoch : 5, Step : 2920, Loss : 0.34725, Acc : 0.844, Sensitive_Loss : 0.09544, Sensitive_Acc : 16.700, Run Time : 7.45 sec
INFO:root:2024-04-28 07:49:26, Train, Epoch : 5, Step : 2930, Loss : 0.35673, Acc : 0.859, Sensitive_Loss : 0.11623, Sensitive_Acc : 16.100, Run Time : 6.90 sec
INFO:root:2024-04-28 07:49:34, Train, Epoch : 5, Step : 2940, Loss : 0.33597, Acc : 0.841, Sensitive_Loss : 0.08310, Sensitive_Acc : 15.800, Run Time : 7.24 sec
INFO:root:2024-04-28 07:49:40, Train, Epoch : 5, Step : 2950, Loss : 0.31382, Acc : 0.847, Sensitive_Loss : 0.12091, Sensitive_Acc : 17.200, Run Time : 6.91 sec
INFO:root:2024-04-28 07:49:48, Train, Epoch : 5, Step : 2960, Loss : 0.34278, Acc : 0.850, Sensitive_Loss : 0.10086, Sensitive_Acc : 16.300, Run Time : 7.25 sec
INFO:root:2024-04-28 07:49:55, Train, Epoch : 5, Step : 2970, Loss : 0.26757, Acc : 0.881, Sensitive_Loss : 0.09285, Sensitive_Acc : 16.600, Run Time : 7.16 sec
INFO:root:2024-04-28 07:50:02, Train, Epoch : 5, Step : 2980, Loss : 0.32143, Acc : 0.844, Sensitive_Loss : 0.10665, Sensitive_Acc : 16.200, Run Time : 7.16 sec
INFO:root:2024-04-28 07:50:10, Train, Epoch : 5, Step : 2990, Loss : 0.35632, Acc : 0.831, Sensitive_Loss : 0.12479, Sensitive_Acc : 14.700, Run Time : 7.67 sec
INFO:root:2024-04-28 07:50:16, Train, Epoch : 5, Step : 3000, Loss : 0.33845, Acc : 0.850, Sensitive_Loss : 0.14923, Sensitive_Acc : 16.100, Run Time : 6.41 sec
INFO:root:2024-04-28 07:51:51, Dev, Step : 3000, Loss : 0.44052, Acc : 0.809, Auc : 0.906, Sensitive_Loss : 0.14465, Sensitive_Acc : 16.750, Sensitive_Auc : 0.988, Mean auc: 0.906, Run Time : 94.64 sec
INFO:root:2024-04-28 07:51:56, Train, Epoch : 5, Step : 3010, Loss : 0.33990, Acc : 0.863, Sensitive_Loss : 0.13272, Sensitive_Acc : 17.400, Run Time : 100.02 sec
INFO:root:2024-04-28 07:52:03, Train, Epoch : 5, Step : 3020, Loss : 0.28534, Acc : 0.872, Sensitive_Loss : 0.09090, Sensitive_Acc : 15.200, Run Time : 6.94 sec
INFO:root:2024-04-28 07:52:11, Train, Epoch : 5, Step : 3030, Loss : 0.30377, Acc : 0.881, Sensitive_Loss : 0.09707, Sensitive_Acc : 16.900, Run Time : 7.56 sec
INFO:root:2024-04-28 07:52:18, Train, Epoch : 5, Step : 3040, Loss : 0.39855, Acc : 0.800, Sensitive_Loss : 0.11278, Sensitive_Acc : 15.400, Run Time : 7.08 sec
INFO:root:2024-04-28 07:52:25, Train, Epoch : 5, Step : 3050, Loss : 0.32826, Acc : 0.859, Sensitive_Loss : 0.10389, Sensitive_Acc : 15.500, Run Time : 7.14 sec
INFO:root:2024-04-28 07:52:32, Train, Epoch : 5, Step : 3060, Loss : 0.30784, Acc : 0.878, Sensitive_Loss : 0.08032, Sensitive_Acc : 16.100, Run Time : 7.16 sec
INFO:root:2024-04-28 07:52:39, Train, Epoch : 5, Step : 3070, Loss : 0.36666, Acc : 0.844, Sensitive_Loss : 0.08761, Sensitive_Acc : 16.200, Run Time : 7.01 sec
INFO:root:2024-04-28 07:52:46, Train, Epoch : 5, Step : 3080, Loss : 0.39124, Acc : 0.856, Sensitive_Loss : 0.12415, Sensitive_Acc : 16.500, Run Time : 7.14 sec
INFO:root:2024-04-28 07:52:53, Train, Epoch : 5, Step : 3090, Loss : 0.32095, Acc : 0.863, Sensitive_Loss : 0.09655, Sensitive_Acc : 14.600, Run Time : 7.15 sec
INFO:root:2024-04-28 07:53:00, Train, Epoch : 5, Step : 3100, Loss : 0.33315, Acc : 0.831, Sensitive_Loss : 0.14438, Sensitive_Acc : 15.700, Run Time : 6.54 sec
INFO:root:2024-04-28 07:54:34, Dev, Step : 3100, Loss : 0.42072, Acc : 0.822, Auc : 0.906, Sensitive_Loss : 0.14350, Sensitive_Acc : 16.864, Sensitive_Auc : 0.988, Mean auc: 0.906, Run Time : 93.91 sec
INFO:root:2024-04-28 07:54:40, Train, Epoch : 5, Step : 3110, Loss : 0.33972, Acc : 0.866, Sensitive_Loss : 0.07834, Sensitive_Acc : 16.200, Run Time : 99.81 sec
INFO:root:2024-04-28 07:54:46, Train, Epoch : 5, Step : 3120, Loss : 0.29969, Acc : 0.875, Sensitive_Loss : 0.11270, Sensitive_Acc : 17.000, Run Time : 6.70 sec
INFO:root:2024-04-28 07:54:53, Train, Epoch : 5, Step : 3130, Loss : 0.31636, Acc : 0.872, Sensitive_Loss : 0.12835, Sensitive_Acc : 15.100, Run Time : 6.87 sec
INFO:root:2024-04-28 07:56:25
INFO:root:y_pred: [0.0530261  0.84778404 0.03306819 ... 0.8559115  0.01609802 0.8732851 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.88534629e-01 1.48066320e-03 1.56282961e-01 7.33956753e-04
 9.96034205e-01 3.35331890e-04 9.94730353e-01 9.95562196e-01
 2.59370805e-04 8.27948868e-01 9.46612895e-01 9.99181807e-01
 9.92971718e-01 9.49367642e-01 1.42454281e-02 8.17508042e-01
 9.97622073e-01 5.32441121e-03 5.58022819e-02 9.33932722e-01
 9.81646955e-01 4.39852551e-02 9.99324679e-01 9.58644569e-01
 9.98259604e-01 9.81254578e-01 1.21668575e-03 9.96848643e-01
 9.83017027e-01 6.57893479e-01 3.94310988e-03 2.39932597e-01
 1.48101442e-03 1.22820415e-01 1.76640451e-01 3.61579694e-02
 1.47306686e-02 8.31090380e-03 9.76586878e-01 9.91297364e-01
 1.77921702e-05 1.66115151e-05 9.73583341e-01 7.13101763e-04
 9.99298930e-01 9.90136981e-01 9.82186019e-01 9.79675889e-01
 2.40990296e-02 9.71527755e-01 9.96172011e-01 2.09591854e-02
 4.20342833e-01 2.28073518e-03 6.97269279e-05 2.38763951e-02
 4.42608865e-03 3.19355905e-01 1.07417523e-03 3.49797189e-01
 1.11942068e-02 1.64063156e-01 1.28521929e-02 9.26269829e-01
 2.62452457e-02 9.93101478e-01 3.23388837e-02 9.91528153e-01
 9.09785509e-01 1.61143675e-01 8.91015768e-01 7.47807682e-01
 9.72889597e-04 2.14023620e-01 9.67984204e-04 4.73786349e-04
 7.07444549e-03 1.16559111e-01 1.71018520e-03 9.97489333e-01
 9.98218238e-01 1.57814787e-03 3.33004981e-01 1.50601030e-03
 9.54451442e-01 9.20807004e-01 1.48368338e-02 3.53714749e-02
 8.92185092e-01 9.93967295e-01 9.98382092e-01 1.95293836e-02
 1.28846534e-03 9.91499007e-01 7.47837592e-03 1.99794842e-04
 9.83753979e-01 9.95259345e-01 9.58671793e-04 3.93082295e-03
 9.86518323e-01 9.40398097e-01 9.85951543e-01 9.93075252e-01
 2.02575931e-03 4.05443907e-02 9.02139604e-01 9.82350230e-01
 9.18615937e-01 7.96404202e-05 8.97164762e-01 9.85981464e-01
 7.19580650e-02 9.96852577e-01 9.93108511e-01 9.95181859e-01
 9.71802831e-01 9.98571157e-01 9.14395079e-02 6.22640997e-02
 9.94412601e-01 9.96827781e-01 2.31203594e-04 9.26653326e-01
 9.97500122e-01 3.87517542e-01 9.87639129e-01 2.55027669e-03
 1.49735343e-02 9.86278594e-01 9.88622010e-01 4.33310634e-03
 7.97613803e-03 6.14695735e-02 9.61515665e-01 9.90384936e-01
 9.66455996e-01 1.17670489e-03 3.83811854e-02 9.83369529e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 07:56:25, Dev, Step : 3130, Loss : 0.40183, Acc : 0.824, Auc : 0.908, Sensitive_Loss : 0.12650, Sensitive_Acc : 16.821, Sensitive_Auc : 0.989, Mean auc: 0.908, Run Time : 91.88 sec
INFO:root:2024-04-28 07:56:34, Train, Epoch : 6, Step : 3140, Loss : 0.30296, Acc : 0.853, Sensitive_Loss : 0.09830, Sensitive_Acc : 15.900, Run Time : 8.02 sec
INFO:root:2024-04-28 07:56:41, Train, Epoch : 6, Step : 3150, Loss : 0.34763, Acc : 0.875, Sensitive_Loss : 0.13357, Sensitive_Acc : 15.700, Run Time : 7.31 sec
INFO:root:2024-04-28 07:56:49, Train, Epoch : 6, Step : 3160, Loss : 0.28173, Acc : 0.894, Sensitive_Loss : 0.08451, Sensitive_Acc : 17.000, Run Time : 7.22 sec
INFO:root:2024-04-28 07:56:56, Train, Epoch : 6, Step : 3170, Loss : 0.31830, Acc : 0.853, Sensitive_Loss : 0.07948, Sensitive_Acc : 16.800, Run Time : 7.05 sec
INFO:root:2024-04-28 07:57:03, Train, Epoch : 6, Step : 3180, Loss : 0.30793, Acc : 0.869, Sensitive_Loss : 0.10971, Sensitive_Acc : 18.100, Run Time : 7.64 sec
INFO:root:2024-04-28 07:57:11, Train, Epoch : 6, Step : 3190, Loss : 0.26688, Acc : 0.866, Sensitive_Loss : 0.10307, Sensitive_Acc : 17.100, Run Time : 7.26 sec
INFO:root:2024-04-28 07:57:18, Train, Epoch : 6, Step : 3200, Loss : 0.28607, Acc : 0.859, Sensitive_Loss : 0.13832, Sensitive_Acc : 15.500, Run Time : 7.05 sec
INFO:root:2024-04-28 07:58:52, Dev, Step : 3200, Loss : 0.43484, Acc : 0.814, Auc : 0.906, Sensitive_Loss : 0.13602, Sensitive_Acc : 16.764, Sensitive_Auc : 0.990, Mean auc: 0.906, Run Time : 94.42 sec
INFO:root:2024-04-28 07:58:58, Train, Epoch : 6, Step : 3210, Loss : 0.37742, Acc : 0.853, Sensitive_Loss : 0.13034, Sensitive_Acc : 17.300, Run Time : 99.87 sec
INFO:root:2024-04-28 07:59:05, Train, Epoch : 6, Step : 3220, Loss : 0.28600, Acc : 0.875, Sensitive_Loss : 0.07359, Sensitive_Acc : 16.100, Run Time : 7.24 sec
INFO:root:2024-04-28 07:59:12, Train, Epoch : 6, Step : 3230, Loss : 0.31212, Acc : 0.887, Sensitive_Loss : 0.11456, Sensitive_Acc : 16.800, Run Time : 7.64 sec
INFO:root:2024-04-28 07:59:19, Train, Epoch : 6, Step : 3240, Loss : 0.32502, Acc : 0.844, Sensitive_Loss : 0.08913, Sensitive_Acc : 15.600, Run Time : 6.46 sec
INFO:root:2024-04-28 07:59:26, Train, Epoch : 6, Step : 3250, Loss : 0.24937, Acc : 0.881, Sensitive_Loss : 0.12341, Sensitive_Acc : 16.600, Run Time : 7.43 sec
INFO:root:2024-04-28 07:59:33, Train, Epoch : 6, Step : 3260, Loss : 0.28930, Acc : 0.863, Sensitive_Loss : 0.12688, Sensitive_Acc : 15.700, Run Time : 7.08 sec
INFO:root:2024-04-28 07:59:41, Train, Epoch : 6, Step : 3270, Loss : 0.30715, Acc : 0.869, Sensitive_Loss : 0.10472, Sensitive_Acc : 16.300, Run Time : 7.39 sec
INFO:root:2024-04-28 07:59:48, Train, Epoch : 6, Step : 3280, Loss : 0.35312, Acc : 0.841, Sensitive_Loss : 0.09734, Sensitive_Acc : 15.900, Run Time : 6.83 sec
INFO:root:2024-04-28 07:59:55, Train, Epoch : 6, Step : 3290, Loss : 0.28221, Acc : 0.884, Sensitive_Loss : 0.10560, Sensitive_Acc : 16.600, Run Time : 7.00 sec
INFO:root:2024-04-28 08:00:03, Train, Epoch : 6, Step : 3300, Loss : 0.27938, Acc : 0.887, Sensitive_Loss : 0.12106, Sensitive_Acc : 16.400, Run Time : 8.02 sec
INFO:root:2024-04-28 08:01:36, Dev, Step : 3300, Loss : 0.41609, Acc : 0.824, Auc : 0.906, Sensitive_Loss : 0.13557, Sensitive_Acc : 16.793, Sensitive_Auc : 0.988, Mean auc: 0.906, Run Time : 93.36 sec
INFO:root:2024-04-28 08:01:42, Train, Epoch : 6, Step : 3310, Loss : 0.41639, Acc : 0.853, Sensitive_Loss : 0.12148, Sensitive_Acc : 15.700, Run Time : 99.08 sec
INFO:root:2024-04-28 08:01:49, Train, Epoch : 6, Step : 3320, Loss : 0.27694, Acc : 0.887, Sensitive_Loss : 0.10955, Sensitive_Acc : 17.000, Run Time : 7.26 sec
INFO:root:2024-04-28 08:01:57, Train, Epoch : 6, Step : 3330, Loss : 0.32811, Acc : 0.878, Sensitive_Loss : 0.08685, Sensitive_Acc : 15.400, Run Time : 7.55 sec
INFO:root:2024-04-28 08:02:03, Train, Epoch : 6, Step : 3340, Loss : 0.31362, Acc : 0.875, Sensitive_Loss : 0.09409, Sensitive_Acc : 15.700, Run Time : 6.84 sec
INFO:root:2024-04-28 08:02:11, Train, Epoch : 6, Step : 3350, Loss : 0.31398, Acc : 0.878, Sensitive_Loss : 0.11355, Sensitive_Acc : 15.900, Run Time : 7.15 sec
INFO:root:2024-04-28 08:02:18, Train, Epoch : 6, Step : 3360, Loss : 0.27187, Acc : 0.891, Sensitive_Loss : 0.09071, Sensitive_Acc : 16.600, Run Time : 7.01 sec
INFO:root:2024-04-28 08:02:25, Train, Epoch : 6, Step : 3370, Loss : 0.32547, Acc : 0.881, Sensitive_Loss : 0.07889, Sensitive_Acc : 17.100, Run Time : 7.27 sec
INFO:root:2024-04-28 08:02:32, Train, Epoch : 6, Step : 3380, Loss : 0.31572, Acc : 0.856, Sensitive_Loss : 0.09359, Sensitive_Acc : 14.900, Run Time : 7.33 sec
INFO:root:2024-04-28 08:02:39, Train, Epoch : 6, Step : 3390, Loss : 0.31891, Acc : 0.850, Sensitive_Loss : 0.14664, Sensitive_Acc : 14.500, Run Time : 7.30 sec
INFO:root:2024-04-28 08:02:46, Train, Epoch : 6, Step : 3400, Loss : 0.29341, Acc : 0.859, Sensitive_Loss : 0.10415, Sensitive_Acc : 15.900, Run Time : 6.32 sec
INFO:root:2024-04-28 08:04:20, Dev, Step : 3400, Loss : 0.41773, Acc : 0.820, Auc : 0.904, Sensitive_Loss : 0.12793, Sensitive_Acc : 16.779, Sensitive_Auc : 0.989, Mean auc: 0.904, Run Time : 94.24 sec
INFO:root:2024-04-28 08:04:26, Train, Epoch : 6, Step : 3410, Loss : 0.27996, Acc : 0.856, Sensitive_Loss : 0.11341, Sensitive_Acc : 16.700, Run Time : 99.79 sec
INFO:root:2024-04-28 08:04:33, Train, Epoch : 6, Step : 3420, Loss : 0.28497, Acc : 0.875, Sensitive_Loss : 0.09973, Sensitive_Acc : 16.300, Run Time : 6.99 sec
INFO:root:2024-04-28 08:04:40, Train, Epoch : 6, Step : 3430, Loss : 0.28884, Acc : 0.884, Sensitive_Loss : 0.14954, Sensitive_Acc : 16.500, Run Time : 7.31 sec
INFO:root:2024-04-28 08:04:47, Train, Epoch : 6, Step : 3440, Loss : 0.27143, Acc : 0.875, Sensitive_Loss : 0.11231, Sensitive_Acc : 15.500, Run Time : 7.19 sec
INFO:root:2024-04-28 08:04:54, Train, Epoch : 6, Step : 3450, Loss : 0.34221, Acc : 0.856, Sensitive_Loss : 0.10830, Sensitive_Acc : 17.700, Run Time : 6.77 sec
INFO:root:2024-04-28 08:05:01, Train, Epoch : 6, Step : 3460, Loss : 0.28682, Acc : 0.863, Sensitive_Loss : 0.12730, Sensitive_Acc : 16.600, Run Time : 7.12 sec
INFO:root:2024-04-28 08:05:09, Train, Epoch : 6, Step : 3470, Loss : 0.21648, Acc : 0.906, Sensitive_Loss : 0.08725, Sensitive_Acc : 16.900, Run Time : 7.91 sec
INFO:root:2024-04-28 08:05:15, Train, Epoch : 6, Step : 3480, Loss : 0.32200, Acc : 0.878, Sensitive_Loss : 0.09510, Sensitive_Acc : 16.000, Run Time : 6.58 sec
INFO:root:2024-04-28 08:05:23, Train, Epoch : 6, Step : 3490, Loss : 0.28487, Acc : 0.869, Sensitive_Loss : 0.10446, Sensitive_Acc : 16.100, Run Time : 7.51 sec
INFO:root:2024-04-28 08:05:30, Train, Epoch : 6, Step : 3500, Loss : 0.32808, Acc : 0.863, Sensitive_Loss : 0.11622, Sensitive_Acc : 17.000, Run Time : 6.64 sec
INFO:root:2024-04-28 08:07:04, Dev, Step : 3500, Loss : 0.41775, Acc : 0.821, Auc : 0.907, Sensitive_Loss : 0.14451, Sensitive_Acc : 16.721, Sensitive_Auc : 0.989, Mean auc: 0.907, Run Time : 94.06 sec
INFO:root:2024-04-28 08:07:09, Train, Epoch : 6, Step : 3510, Loss : 0.34087, Acc : 0.875, Sensitive_Loss : 0.09111, Sensitive_Acc : 16.400, Run Time : 99.74 sec
INFO:root:2024-04-28 08:07:17, Train, Epoch : 6, Step : 3520, Loss : 0.34680, Acc : 0.875, Sensitive_Loss : 0.07161, Sensitive_Acc : 15.600, Run Time : 7.28 sec
INFO:root:2024-04-28 08:07:23, Train, Epoch : 6, Step : 3530, Loss : 0.32389, Acc : 0.847, Sensitive_Loss : 0.11271, Sensitive_Acc : 17.400, Run Time : 6.76 sec
INFO:root:2024-04-28 08:07:31, Train, Epoch : 6, Step : 3540, Loss : 0.30942, Acc : 0.872, Sensitive_Loss : 0.09376, Sensitive_Acc : 15.700, Run Time : 7.51 sec
INFO:root:2024-04-28 08:07:37, Train, Epoch : 6, Step : 3550, Loss : 0.28501, Acc : 0.875, Sensitive_Loss : 0.13162, Sensitive_Acc : 16.400, Run Time : 6.57 sec
INFO:root:2024-04-28 08:07:45, Train, Epoch : 6, Step : 3560, Loss : 0.29844, Acc : 0.869, Sensitive_Loss : 0.10627, Sensitive_Acc : 14.500, Run Time : 7.35 sec
INFO:root:2024-04-28 08:07:52, Train, Epoch : 6, Step : 3570, Loss : 0.29907, Acc : 0.866, Sensitive_Loss : 0.12601, Sensitive_Acc : 15.500, Run Time : 7.16 sec
INFO:root:2024-04-28 08:07:59, Train, Epoch : 6, Step : 3580, Loss : 0.32005, Acc : 0.881, Sensitive_Loss : 0.12425, Sensitive_Acc : 14.900, Run Time : 7.55 sec
INFO:root:2024-04-28 08:08:07, Train, Epoch : 6, Step : 3590, Loss : 0.31711, Acc : 0.881, Sensitive_Loss : 0.08158, Sensitive_Acc : 15.900, Run Time : 7.42 sec
INFO:root:2024-04-28 08:08:14, Train, Epoch : 6, Step : 3600, Loss : 0.29012, Acc : 0.881, Sensitive_Loss : 0.12154, Sensitive_Acc : 16.100, Run Time : 6.97 sec
INFO:root:2024-04-28 08:09:48, Dev, Step : 3600, Loss : 0.40829, Acc : 0.826, Auc : 0.907, Sensitive_Loss : 0.12280, Sensitive_Acc : 16.821, Sensitive_Auc : 0.988, Mean auc: 0.907, Run Time : 93.88 sec
INFO:root:2024-04-28 08:09:54, Train, Epoch : 6, Step : 3610, Loss : 0.24744, Acc : 0.897, Sensitive_Loss : 0.07762, Sensitive_Acc : 16.100, Run Time : 99.93 sec
INFO:root:2024-04-28 08:10:01, Train, Epoch : 6, Step : 3620, Loss : 0.32937, Acc : 0.872, Sensitive_Loss : 0.08528, Sensitive_Acc : 14.800, Run Time : 6.82 sec
INFO:root:2024-04-28 08:10:08, Train, Epoch : 6, Step : 3630, Loss : 0.35620, Acc : 0.828, Sensitive_Loss : 0.18290, Sensitive_Acc : 18.800, Run Time : 6.96 sec
INFO:root:2024-04-28 08:10:15, Train, Epoch : 6, Step : 3640, Loss : 0.21990, Acc : 0.903, Sensitive_Loss : 0.09591, Sensitive_Acc : 16.300, Run Time : 7.25 sec
INFO:root:2024-04-28 08:10:22, Train, Epoch : 6, Step : 3650, Loss : 0.34068, Acc : 0.844, Sensitive_Loss : 0.13881, Sensitive_Acc : 14.400, Run Time : 6.97 sec
INFO:root:2024-04-28 08:10:29, Train, Epoch : 6, Step : 3660, Loss : 0.28304, Acc : 0.887, Sensitive_Loss : 0.06285, Sensitive_Acc : 17.300, Run Time : 7.13 sec
INFO:root:2024-04-28 08:10:36, Train, Epoch : 6, Step : 3670, Loss : 0.26280, Acc : 0.875, Sensitive_Loss : 0.10191, Sensitive_Acc : 15.100, Run Time : 7.55 sec
INFO:root:2024-04-28 08:10:43, Train, Epoch : 6, Step : 3680, Loss : 0.30803, Acc : 0.894, Sensitive_Loss : 0.09846, Sensitive_Acc : 17.000, Run Time : 6.90 sec
INFO:root:2024-04-28 08:10:50, Train, Epoch : 6, Step : 3690, Loss : 0.30895, Acc : 0.863, Sensitive_Loss : 0.12585, Sensitive_Acc : 16.800, Run Time : 6.82 sec
INFO:root:2024-04-28 08:10:57, Train, Epoch : 6, Step : 3700, Loss : 0.34853, Acc : 0.847, Sensitive_Loss : 0.11458, Sensitive_Acc : 17.100, Run Time : 6.97 sec
INFO:root:2024-04-28 08:12:32, Dev, Step : 3700, Loss : 0.42640, Acc : 0.821, Auc : 0.905, Sensitive_Loss : 0.13507, Sensitive_Acc : 16.836, Sensitive_Auc : 0.990, Mean auc: 0.905, Run Time : 94.44 sec
INFO:root:2024-04-28 08:12:37, Train, Epoch : 6, Step : 3710, Loss : 0.30501, Acc : 0.881, Sensitive_Loss : 0.11451, Sensitive_Acc : 16.600, Run Time : 99.89 sec
INFO:root:2024-04-28 08:12:44, Train, Epoch : 6, Step : 3720, Loss : 0.30088, Acc : 0.878, Sensitive_Loss : 0.11168, Sensitive_Acc : 15.900, Run Time : 7.22 sec
INFO:root:2024-04-28 08:12:51, Train, Epoch : 6, Step : 3730, Loss : 0.30839, Acc : 0.869, Sensitive_Loss : 0.14616, Sensitive_Acc : 14.900, Run Time : 7.05 sec
INFO:root:2024-04-28 08:12:59, Train, Epoch : 6, Step : 3740, Loss : 0.26141, Acc : 0.878, Sensitive_Loss : 0.12750, Sensitive_Acc : 13.800, Run Time : 7.75 sec
INFO:root:2024-04-28 08:13:06, Train, Epoch : 6, Step : 3750, Loss : 0.37342, Acc : 0.847, Sensitive_Loss : 0.12589, Sensitive_Acc : 15.500, Run Time : 6.56 sec
INFO:root:2024-04-28 08:14:42
INFO:root:y_pred: [0.02313594 0.79931426 0.01812521 ... 0.68550164 0.00746259 0.89191616]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.9004740e-01 1.3455049e-03 1.9989578e-01 1.5957822e-03 9.9563664e-01
 3.0157081e-04 9.9632984e-01 9.9701631e-01 1.3874695e-04 8.6323392e-01
 9.5114142e-01 9.9948323e-01 9.9642783e-01 9.6629900e-01 1.6120691e-02
 8.4285307e-01 9.9887234e-01 1.1632599e-02 1.7935145e-01 9.3799990e-01
 9.8840708e-01 2.0756291e-02 9.9966908e-01 9.7062892e-01 9.9956328e-01
 9.8846775e-01 2.3364279e-04 9.9777156e-01 9.8643118e-01 5.6099278e-01
 2.8783712e-03 1.9412565e-01 1.7290282e-03 5.3106278e-01 2.9796787e-02
 3.7871659e-02 1.9067632e-02 1.2063141e-02 9.8073775e-01 9.9618679e-01
 1.8298149e-05 1.4314457e-05 9.7705996e-01 1.4293410e-03 9.9944621e-01
 9.8655790e-01 9.9080557e-01 9.4844973e-01 2.0178944e-02 9.9068224e-01
 9.9694806e-01 5.0155815e-02 2.5823286e-01 1.0056046e-03 2.0387844e-04
 6.0556039e-02 4.8028757e-03 1.2358353e-01 1.6775379e-03 6.6119695e-01
 1.0514461e-02 5.6665983e-02 8.8974964e-03 9.4559991e-01 5.7166759e-02
 9.9501544e-01 6.6204402e-03 9.9243027e-01 9.0268052e-01 1.3344662e-01
 9.4352150e-01 7.0964855e-01 1.0850044e-03 1.4936051e-01 2.4990353e-04
 5.3810590e-04 3.4036783e-03 5.7591263e-02 3.4437219e-03 9.9827468e-01
 9.9897623e-01 4.0047262e-03 1.8774171e-01 1.9372008e-03 9.5447046e-01
 9.2114037e-01 1.3403974e-02 4.0107444e-02 8.8908070e-01 9.9519902e-01
 9.9882847e-01 1.1764802e-02 2.6088331e-03 9.9586749e-01 8.5195117e-03
 1.1423464e-04 9.7590780e-01 9.9721706e-01 1.3814061e-03 3.0658864e-03
 9.8813909e-01 9.7196680e-01 9.8806727e-01 9.9781030e-01 5.7297195e-03
 1.0797098e-02 9.3498355e-01 9.9112624e-01 9.2175514e-01 9.6981486e-05
 9.0255260e-01 9.8705626e-01 1.1161210e-01 9.9745947e-01 9.9773407e-01
 9.9780780e-01 9.4077349e-01 9.9918514e-01 1.4889832e-01 1.0481260e-02
 9.9482262e-01 9.9803787e-01 6.2696065e-04 9.2343599e-01 9.9864358e-01
 5.3297096e-01 9.9076241e-01 5.7815402e-03 5.1185470e-03 9.8425722e-01
 9.9284863e-01 3.6864900e-03 3.7256745e-03 1.0848490e-01 9.7275341e-01
 9.9397707e-01 9.7774839e-01 8.0426905e-04 4.5737378e-02 9.9219924e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 08:14:42, Dev, Step : 3756, Loss : 0.41901, Acc : 0.823, Auc : 0.908, Sensitive_Loss : 0.12450, Sensitive_Acc : 16.821, Sensitive_Auc : 0.989, Mean auc: 0.908, Run Time : 92.25 sec
INFO:root:2024-04-28 08:14:47, Train, Epoch : 7, Step : 3760, Loss : 0.13347, Acc : 0.347, Sensitive_Loss : 0.02917, Sensitive_Acc : 7.000, Run Time : 4.10 sec
INFO:root:2024-04-28 08:14:54, Train, Epoch : 7, Step : 3770, Loss : 0.31233, Acc : 0.841, Sensitive_Loss : 0.12627, Sensitive_Acc : 15.800, Run Time : 6.82 sec
INFO:root:2024-04-28 08:15:01, Train, Epoch : 7, Step : 3780, Loss : 0.24822, Acc : 0.897, Sensitive_Loss : 0.09904, Sensitive_Acc : 15.400, Run Time : 7.23 sec
INFO:root:2024-04-28 08:15:08, Train, Epoch : 7, Step : 3790, Loss : 0.29998, Acc : 0.863, Sensitive_Loss : 0.09934, Sensitive_Acc : 15.700, Run Time : 7.02 sec
INFO:root:2024-04-28 08:15:16, Train, Epoch : 7, Step : 3800, Loss : 0.31671, Acc : 0.878, Sensitive_Loss : 0.07353, Sensitive_Acc : 16.800, Run Time : 7.53 sec
INFO:root:2024-04-28 08:16:50, Dev, Step : 3800, Loss : 0.40936, Acc : 0.827, Auc : 0.909, Sensitive_Loss : 0.12128, Sensitive_Acc : 16.864, Sensitive_Auc : 0.990, Mean auc: 0.909, Run Time : 94.30 sec
INFO:root:2024-04-28 08:16:50, Best, Step : 3800, Loss : 0.40936, Acc : 0.827, Auc : 0.909, Sensitive_Loss : 0.12128, Sensitive_Acc : 16.864, Sensitive_Auc : 0.990, Best Auc : 0.909
INFO:root:2024-04-28 08:16:56, Train, Epoch : 7, Step : 3810, Loss : 0.29176, Acc : 0.881, Sensitive_Loss : 0.09513, Sensitive_Acc : 17.400, Run Time : 100.38 sec
INFO:root:2024-04-28 08:17:04, Train, Epoch : 7, Step : 3820, Loss : 0.35839, Acc : 0.838, Sensitive_Loss : 0.10743, Sensitive_Acc : 16.900, Run Time : 7.95 sec
INFO:root:2024-04-28 08:17:11, Train, Epoch : 7, Step : 3830, Loss : 0.28731, Acc : 0.866, Sensitive_Loss : 0.14143, Sensitive_Acc : 15.900, Run Time : 6.98 sec
INFO:root:2024-04-28 08:17:18, Train, Epoch : 7, Step : 3840, Loss : 0.24942, Acc : 0.884, Sensitive_Loss : 0.08503, Sensitive_Acc : 17.200, Run Time : 7.34 sec
INFO:root:2024-04-28 08:17:25, Train, Epoch : 7, Step : 3850, Loss : 0.30165, Acc : 0.869, Sensitive_Loss : 0.11321, Sensitive_Acc : 15.700, Run Time : 6.91 sec
INFO:root:2024-04-28 08:17:32, Train, Epoch : 7, Step : 3860, Loss : 0.26561, Acc : 0.875, Sensitive_Loss : 0.11874, Sensitive_Acc : 16.500, Run Time : 6.99 sec
INFO:root:2024-04-28 08:17:40, Train, Epoch : 7, Step : 3870, Loss : 0.28523, Acc : 0.872, Sensitive_Loss : 0.09264, Sensitive_Acc : 18.100, Run Time : 8.14 sec
INFO:root:2024-04-28 08:17:47, Train, Epoch : 7, Step : 3880, Loss : 0.25088, Acc : 0.884, Sensitive_Loss : 0.13586, Sensitive_Acc : 17.000, Run Time : 7.04 sec
INFO:root:2024-04-28 08:17:54, Train, Epoch : 7, Step : 3890, Loss : 0.36384, Acc : 0.856, Sensitive_Loss : 0.06413, Sensitive_Acc : 16.800, Run Time : 6.82 sec
INFO:root:2024-04-28 08:18:02, Train, Epoch : 7, Step : 3900, Loss : 0.31312, Acc : 0.869, Sensitive_Loss : 0.13923, Sensitive_Acc : 14.900, Run Time : 7.47 sec
INFO:root:2024-04-28 08:19:37, Dev, Step : 3900, Loss : 0.43976, Acc : 0.816, Auc : 0.905, Sensitive_Loss : 0.13561, Sensitive_Acc : 16.821, Sensitive_Auc : 0.988, Mean auc: 0.905, Run Time : 95.63 sec
INFO:root:2024-04-28 08:20:06, Train, Epoch : 7, Step : 3910, Loss : 0.24097, Acc : 0.900, Sensitive_Loss : 0.10245, Sensitive_Acc : 16.600, Run Time : 124.29 sec
INFO:root:2024-04-28 08:20:14, Train, Epoch : 7, Step : 3920, Loss : 0.31447, Acc : 0.878, Sensitive_Loss : 0.09820, Sensitive_Acc : 18.000, Run Time : 8.29 sec
INFO:root:2024-04-28 08:21:30, Train, Epoch : 7, Step : 3930, Loss : 0.29799, Acc : 0.875, Sensitive_Loss : 0.10783, Sensitive_Acc : 16.200, Run Time : 75.80 sec
INFO:root:2024-04-28 08:22:11, Train, Epoch : 7, Step : 3940, Loss : 0.36055, Acc : 0.847, Sensitive_Loss : 0.14565, Sensitive_Acc : 15.600, Run Time : 41.13 sec
INFO:root:2024-04-28 08:22:29, Train, Epoch : 7, Step : 3950, Loss : 0.27172, Acc : 0.891, Sensitive_Loss : 0.08336, Sensitive_Acc : 17.000, Run Time : 17.84 sec
INFO:root:2024-04-28 08:22:45, Train, Epoch : 7, Step : 3960, Loss : 0.25548, Acc : 0.903, Sensitive_Loss : 0.12668, Sensitive_Acc : 16.300, Run Time : 16.53 sec
INFO:root:2024-04-28 08:22:53, Train, Epoch : 7, Step : 3970, Loss : 0.34458, Acc : 0.875, Sensitive_Loss : 0.11587, Sensitive_Acc : 17.700, Run Time : 7.55 sec
INFO:root:2024-04-28 08:23:01, Train, Epoch : 7, Step : 3980, Loss : 0.25221, Acc : 0.894, Sensitive_Loss : 0.12222, Sensitive_Acc : 17.100, Run Time : 7.64 sec
INFO:root:2024-04-28 08:23:08, Train, Epoch : 7, Step : 3990, Loss : 0.26302, Acc : 0.856, Sensitive_Loss : 0.08068, Sensitive_Acc : 16.500, Run Time : 7.29 sec
INFO:root:2024-04-28 08:23:15, Train, Epoch : 7, Step : 4000, Loss : 0.25528, Acc : 0.912, Sensitive_Loss : 0.08349, Sensitive_Acc : 16.500, Run Time : 7.19 sec
INFO:root:2024-04-28 08:24:52, Dev, Step : 4000, Loss : 0.42304, Acc : 0.821, Auc : 0.905, Sensitive_Loss : 0.13297, Sensitive_Acc : 16.879, Sensitive_Auc : 0.990, Mean auc: 0.905, Run Time : 96.76 sec
INFO:root:2024-04-28 08:24:58, Train, Epoch : 7, Step : 4010, Loss : 0.28326, Acc : 0.859, Sensitive_Loss : 0.07118, Sensitive_Acc : 15.500, Run Time : 102.47 sec
INFO:root:2024-04-28 08:25:05, Train, Epoch : 7, Step : 4020, Loss : 0.24566, Acc : 0.894, Sensitive_Loss : 0.07572, Sensitive_Acc : 15.600, Run Time : 7.17 sec
INFO:root:2024-04-28 08:25:12, Train, Epoch : 7, Step : 4030, Loss : 0.25653, Acc : 0.897, Sensitive_Loss : 0.07411, Sensitive_Acc : 17.300, Run Time : 7.58 sec
INFO:root:2024-04-28 08:25:20, Train, Epoch : 7, Step : 4040, Loss : 0.24624, Acc : 0.912, Sensitive_Loss : 0.10376, Sensitive_Acc : 16.200, Run Time : 7.33 sec
INFO:root:2024-04-28 08:25:27, Train, Epoch : 7, Step : 4050, Loss : 0.33473, Acc : 0.869, Sensitive_Loss : 0.09498, Sensitive_Acc : 16.300, Run Time : 7.48 sec
INFO:root:2024-04-28 08:25:34, Train, Epoch : 7, Step : 4060, Loss : 0.34301, Acc : 0.825, Sensitive_Loss : 0.13839, Sensitive_Acc : 15.300, Run Time : 7.06 sec
INFO:root:2024-04-28 08:25:42, Train, Epoch : 7, Step : 4070, Loss : 0.33182, Acc : 0.850, Sensitive_Loss : 0.09370, Sensitive_Acc : 16.000, Run Time : 7.52 sec
INFO:root:2024-04-28 08:25:49, Train, Epoch : 7, Step : 4080, Loss : 0.31705, Acc : 0.847, Sensitive_Loss : 0.11203, Sensitive_Acc : 16.800, Run Time : 7.39 sec
INFO:root:2024-04-28 08:25:57, Train, Epoch : 7, Step : 4090, Loss : 0.30384, Acc : 0.875, Sensitive_Loss : 0.07292, Sensitive_Acc : 16.500, Run Time : 7.45 sec
INFO:root:2024-04-28 08:26:04, Train, Epoch : 7, Step : 4100, Loss : 0.30257, Acc : 0.878, Sensitive_Loss : 0.09893, Sensitive_Acc : 16.300, Run Time : 7.09 sec
INFO:root:2024-04-28 08:27:39, Dev, Step : 4100, Loss : 0.43040, Acc : 0.821, Auc : 0.900, Sensitive_Loss : 0.13724, Sensitive_Acc : 16.879, Sensitive_Auc : 0.988, Mean auc: 0.900, Run Time : 95.58 sec
INFO:root:2024-04-28 08:27:45, Train, Epoch : 7, Step : 4110, Loss : 0.29275, Acc : 0.869, Sensitive_Loss : 0.13789, Sensitive_Acc : 14.600, Run Time : 101.49 sec
INFO:root:2024-04-28 08:27:53, Train, Epoch : 7, Step : 4120, Loss : 0.24162, Acc : 0.859, Sensitive_Loss : 0.10433, Sensitive_Acc : 16.300, Run Time : 7.84 sec
INFO:root:2024-04-28 08:28:00, Train, Epoch : 7, Step : 4130, Loss : 0.32560, Acc : 0.872, Sensitive_Loss : 0.09189, Sensitive_Acc : 17.100, Run Time : 7.03 sec
INFO:root:2024-04-28 08:28:08, Train, Epoch : 7, Step : 4140, Loss : 0.22011, Acc : 0.919, Sensitive_Loss : 0.07298, Sensitive_Acc : 16.000, Run Time : 7.48 sec
INFO:root:2024-04-28 08:28:15, Train, Epoch : 7, Step : 4150, Loss : 0.29869, Acc : 0.878, Sensitive_Loss : 0.09178, Sensitive_Acc : 16.800, Run Time : 7.02 sec
INFO:root:2024-04-28 08:28:22, Train, Epoch : 7, Step : 4160, Loss : 0.26594, Acc : 0.875, Sensitive_Loss : 0.08877, Sensitive_Acc : 14.300, Run Time : 7.38 sec
INFO:root:2024-04-28 08:28:29, Train, Epoch : 7, Step : 4170, Loss : 0.31731, Acc : 0.859, Sensitive_Loss : 0.11277, Sensitive_Acc : 15.100, Run Time : 7.02 sec
INFO:root:2024-04-28 08:28:36, Train, Epoch : 7, Step : 4180, Loss : 0.24735, Acc : 0.887, Sensitive_Loss : 0.07207, Sensitive_Acc : 16.800, Run Time : 7.51 sec
INFO:root:2024-04-28 08:28:44, Train, Epoch : 7, Step : 4190, Loss : 0.30298, Acc : 0.872, Sensitive_Loss : 0.08440, Sensitive_Acc : 16.900, Run Time : 7.41 sec
INFO:root:2024-04-28 08:28:51, Train, Epoch : 7, Step : 4200, Loss : 0.27476, Acc : 0.887, Sensitive_Loss : 0.09034, Sensitive_Acc : 15.800, Run Time : 7.10 sec
INFO:root:2024-04-28 08:30:31, Dev, Step : 4200, Loss : 0.41115, Acc : 0.824, Auc : 0.907, Sensitive_Loss : 0.12398, Sensitive_Acc : 16.807, Sensitive_Auc : 0.989, Mean auc: 0.907, Run Time : 99.60 sec
INFO:root:2024-04-28 08:30:37, Train, Epoch : 7, Step : 4210, Loss : 0.28721, Acc : 0.878, Sensitive_Loss : 0.12672, Sensitive_Acc : 17.500, Run Time : 105.72 sec
INFO:root:2024-04-28 08:30:44, Train, Epoch : 7, Step : 4220, Loss : 0.30752, Acc : 0.875, Sensitive_Loss : 0.07769, Sensitive_Acc : 17.800, Run Time : 7.32 sec
INFO:root:2024-04-28 08:30:51, Train, Epoch : 7, Step : 4230, Loss : 0.29649, Acc : 0.903, Sensitive_Loss : 0.09686, Sensitive_Acc : 17.900, Run Time : 6.96 sec
INFO:root:2024-04-28 08:30:59, Train, Epoch : 7, Step : 4240, Loss : 0.25581, Acc : 0.897, Sensitive_Loss : 0.11521, Sensitive_Acc : 17.900, Run Time : 7.82 sec
INFO:root:2024-04-28 08:31:06, Train, Epoch : 7, Step : 4250, Loss : 0.30781, Acc : 0.856, Sensitive_Loss : 0.08182, Sensitive_Acc : 15.500, Run Time : 6.83 sec
INFO:root:2024-04-28 08:31:13, Train, Epoch : 7, Step : 4260, Loss : 0.21315, Acc : 0.925, Sensitive_Loss : 0.09373, Sensitive_Acc : 16.800, Run Time : 7.35 sec
INFO:root:2024-04-28 08:31:21, Train, Epoch : 7, Step : 4270, Loss : 0.31579, Acc : 0.863, Sensitive_Loss : 0.11041, Sensitive_Acc : 14.900, Run Time : 7.72 sec
INFO:root:2024-04-28 08:31:28, Train, Epoch : 7, Step : 4280, Loss : 0.30357, Acc : 0.884, Sensitive_Loss : 0.08622, Sensitive_Acc : 16.400, Run Time : 7.12 sec
INFO:root:2024-04-28 08:31:35, Train, Epoch : 7, Step : 4290, Loss : 0.26013, Acc : 0.863, Sensitive_Loss : 0.07741, Sensitive_Acc : 16.800, Run Time : 7.16 sec
INFO:root:2024-04-28 08:31:43, Train, Epoch : 7, Step : 4300, Loss : 0.27204, Acc : 0.872, Sensitive_Loss : 0.09816, Sensitive_Acc : 15.900, Run Time : 7.90 sec
INFO:root:2024-04-28 08:33:22, Dev, Step : 4300, Loss : 0.42419, Acc : 0.824, Auc : 0.906, Sensitive_Loss : 0.12269, Sensitive_Acc : 16.821, Sensitive_Auc : 0.989, Mean auc: 0.906, Run Time : 99.39 sec
INFO:root:2024-04-28 08:33:28, Train, Epoch : 7, Step : 4310, Loss : 0.23500, Acc : 0.887, Sensitive_Loss : 0.09122, Sensitive_Acc : 16.700, Run Time : 105.13 sec
INFO:root:2024-04-28 08:33:35, Train, Epoch : 7, Step : 4320, Loss : 0.27799, Acc : 0.878, Sensitive_Loss : 0.09638, Sensitive_Acc : 16.300, Run Time : 7.33 sec
INFO:root:2024-04-28 08:33:43, Train, Epoch : 7, Step : 4330, Loss : 0.29320, Acc : 0.863, Sensitive_Loss : 0.12570, Sensitive_Acc : 15.400, Run Time : 7.68 sec
INFO:root:2024-04-28 08:33:50, Train, Epoch : 7, Step : 4340, Loss : 0.27948, Acc : 0.906, Sensitive_Loss : 0.08707, Sensitive_Acc : 18.400, Run Time : 7.42 sec
INFO:root:2024-04-28 08:33:57, Train, Epoch : 7, Step : 4350, Loss : 0.32767, Acc : 0.856, Sensitive_Loss : 0.09548, Sensitive_Acc : 15.400, Run Time : 7.02 sec
INFO:root:2024-04-28 08:34:05, Train, Epoch : 7, Step : 4360, Loss : 0.33992, Acc : 0.856, Sensitive_Loss : 0.11769, Sensitive_Acc : 15.900, Run Time : 7.37 sec
INFO:root:2024-04-28 08:34:12, Train, Epoch : 7, Step : 4370, Loss : 0.34910, Acc : 0.869, Sensitive_Loss : 0.09541, Sensitive_Acc : 16.400, Run Time : 7.21 sec
INFO:root:2024-04-28 08:34:19, Train, Epoch : 7, Step : 4380, Loss : 0.33524, Acc : 0.844, Sensitive_Loss : 0.07345, Sensitive_Acc : 16.400, Run Time : 7.22 sec
INFO:root:2024-04-28 08:35:55
INFO:root:y_pred: [0.04848012 0.8762485  0.04714957 ... 0.7481098  0.00809313 0.84909177]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.87880945e-01 1.47108315e-03 6.54512644e-02 1.72251312e-04
 9.92853343e-01 9.85299994e-05 9.92769659e-01 9.95588064e-01
 1.27882422e-05 9.09233272e-01 9.32069123e-01 9.99077082e-01
 9.96427715e-01 9.59302902e-01 8.64887424e-03 8.97942185e-01
 9.99044240e-01 8.67417734e-03 6.94992095e-02 9.67186332e-01
 9.84686375e-01 1.46155590e-02 9.99475181e-01 9.62041020e-01
 9.99595582e-01 9.86258745e-01 1.47421772e-04 9.97290254e-01
 9.72936988e-01 5.11886299e-01 1.72349322e-03 5.55387914e-01
 4.29011474e-04 3.53056371e-01 2.40205321e-02 3.79999876e-02
 5.10347774e-03 4.51231748e-03 9.86466467e-01 9.94424939e-01
 1.14158975e-05 1.59405795e-06 9.84384775e-01 7.82867137e-04
 9.99439538e-01 9.89684582e-01 9.93612468e-01 9.89466727e-01
 9.66568291e-03 9.87781763e-01 9.97827590e-01 2.95054000e-02
 2.98910797e-01 8.48134456e-04 9.39123202e-05 2.63067018e-02
 3.11644818e-03 1.47228569e-01 1.04337058e-03 6.48928523e-01
 8.38589948e-03 1.13641508e-01 7.48666981e-03 9.57482338e-01
 5.42492792e-03 9.93822336e-01 5.67037566e-03 9.93478179e-01
 9.45698440e-01 1.20702483e-01 9.54803646e-01 7.75693893e-01
 9.52798000e-04 8.22404176e-02 1.97299742e-04 1.83914643e-04
 7.65116129e-04 5.88716939e-02 1.68912578e-03 9.98290479e-01
 9.99088645e-01 2.30170297e-03 3.43596518e-01 2.32793114e-04
 9.73215342e-01 9.07785892e-01 8.50680936e-03 1.50547158e-02
 8.99514019e-01 9.96513546e-01 9.98411536e-01 5.68861142e-03
 2.71242764e-03 9.96977448e-01 1.39516369e-02 2.39763656e-04
 9.73171651e-01 9.97775376e-01 4.38564515e-04 7.43527140e-04
 9.77905452e-01 9.73943770e-01 9.86063778e-01 9.97000396e-01
 2.53128703e-03 2.23253053e-02 9.42896843e-01 9.86149967e-01
 9.52384591e-01 6.01407737e-05 9.12565649e-01 9.79177296e-01
 4.20504883e-02 9.98124182e-01 9.97648180e-01 9.97425497e-01
 9.69331682e-01 9.98520315e-01 8.60074535e-02 2.63749734e-02
 9.92607355e-01 9.96377170e-01 2.62971676e-04 9.13339317e-01
 9.96403813e-01 3.51593167e-01 9.78322864e-01 3.38073424e-03
 3.35503742e-02 9.60089386e-01 9.81205940e-01 3.16319941e-03
 3.02475062e-03 2.72691268e-02 9.72895980e-01 9.94861424e-01
 9.73230243e-01 3.43560299e-04 2.83979345e-02 9.83900309e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 08:35:55, Dev, Step : 4382, Loss : 0.42363, Acc : 0.823, Auc : 0.904, Sensitive_Loss : 0.12277, Sensitive_Acc : 16.779, Sensitive_Auc : 0.988, Mean auc: 0.904, Run Time : 94.37 sec
INFO:root:2024-04-28 08:36:02, Train, Epoch : 8, Step : 4390, Loss : 0.19922, Acc : 0.719, Sensitive_Loss : 0.10289, Sensitive_Acc : 12.100, Run Time : 6.94 sec
INFO:root:2024-04-28 08:36:10, Train, Epoch : 8, Step : 4400, Loss : 0.24133, Acc : 0.909, Sensitive_Loss : 0.09918, Sensitive_Acc : 17.500, Run Time : 7.21 sec
INFO:root:2024-04-28 08:37:45, Dev, Step : 4400, Loss : 0.41646, Acc : 0.820, Auc : 0.903, Sensitive_Loss : 0.13076, Sensitive_Acc : 16.836, Sensitive_Auc : 0.988, Mean auc: 0.903, Run Time : 95.73 sec
INFO:root:2024-04-28 08:37:51, Train, Epoch : 8, Step : 4410, Loss : 0.30458, Acc : 0.878, Sensitive_Loss : 0.09470, Sensitive_Acc : 15.900, Run Time : 101.63 sec
INFO:root:2024-04-28 08:37:59, Train, Epoch : 8, Step : 4420, Loss : 0.27334, Acc : 0.894, Sensitive_Loss : 0.08950, Sensitive_Acc : 16.500, Run Time : 7.52 sec
INFO:root:2024-04-28 08:38:06, Train, Epoch : 8, Step : 4430, Loss : 0.29045, Acc : 0.891, Sensitive_Loss : 0.09947, Sensitive_Acc : 16.100, Run Time : 7.20 sec
INFO:root:2024-04-28 08:38:13, Train, Epoch : 8, Step : 4440, Loss : 0.25154, Acc : 0.894, Sensitive_Loss : 0.09246, Sensitive_Acc : 15.100, Run Time : 6.95 sec
INFO:root:2024-04-28 08:38:20, Train, Epoch : 8, Step : 4450, Loss : 0.27107, Acc : 0.887, Sensitive_Loss : 0.12860, Sensitive_Acc : 16.100, Run Time : 7.36 sec
INFO:root:2024-04-28 08:38:28, Train, Epoch : 8, Step : 4460, Loss : 0.28823, Acc : 0.872, Sensitive_Loss : 0.08993, Sensitive_Acc : 18.500, Run Time : 7.52 sec
INFO:root:2024-04-28 08:38:35, Train, Epoch : 8, Step : 4470, Loss : 0.25320, Acc : 0.875, Sensitive_Loss : 0.08565, Sensitive_Acc : 16.400, Run Time : 7.11 sec
INFO:root:2024-04-28 08:38:42, Train, Epoch : 8, Step : 4480, Loss : 0.25995, Acc : 0.909, Sensitive_Loss : 0.07928, Sensitive_Acc : 16.200, Run Time : 7.56 sec
INFO:root:2024-04-28 08:38:50, Train, Epoch : 8, Step : 4490, Loss : 0.30277, Acc : 0.850, Sensitive_Loss : 0.09886, Sensitive_Acc : 15.700, Run Time : 7.75 sec
INFO:root:2024-04-28 08:38:57, Train, Epoch : 8, Step : 4500, Loss : 0.25486, Acc : 0.903, Sensitive_Loss : 0.09093, Sensitive_Acc : 17.900, Run Time : 6.73 sec
INFO:root:2024-04-28 08:40:33, Dev, Step : 4500, Loss : 0.42046, Acc : 0.823, Auc : 0.906, Sensitive_Loss : 0.12840, Sensitive_Acc : 16.879, Sensitive_Auc : 0.989, Mean auc: 0.906, Run Time : 95.91 sec
INFO:root:2024-04-28 08:40:38, Train, Epoch : 8, Step : 4510, Loss : 0.25633, Acc : 0.887, Sensitive_Loss : 0.09731, Sensitive_Acc : 17.100, Run Time : 101.38 sec
INFO:root:2024-04-28 08:40:46, Train, Epoch : 8, Step : 4520, Loss : 0.21594, Acc : 0.912, Sensitive_Loss : 0.08022, Sensitive_Acc : 17.200, Run Time : 7.58 sec
INFO:root:2024-04-28 08:40:53, Train, Epoch : 8, Step : 4530, Loss : 0.36733, Acc : 0.853, Sensitive_Loss : 0.08373, Sensitive_Acc : 15.000, Run Time : 7.26 sec
INFO:root:2024-04-28 08:41:00, Train, Epoch : 8, Step : 4540, Loss : 0.33496, Acc : 0.875, Sensitive_Loss : 0.07844, Sensitive_Acc : 16.200, Run Time : 7.30 sec
INFO:root:2024-04-28 08:41:08, Train, Epoch : 8, Step : 4550, Loss : 0.32950, Acc : 0.887, Sensitive_Loss : 0.11114, Sensitive_Acc : 16.000, Run Time : 7.63 sec
INFO:root:2024-04-28 08:41:15, Train, Epoch : 8, Step : 4560, Loss : 0.27159, Acc : 0.869, Sensitive_Loss : 0.07712, Sensitive_Acc : 15.700, Run Time : 7.06 sec
INFO:root:2024-04-28 08:41:22, Train, Epoch : 8, Step : 4570, Loss : 0.29402, Acc : 0.887, Sensitive_Loss : 0.09578, Sensitive_Acc : 16.000, Run Time : 7.29 sec
INFO:root:2024-04-28 08:41:30, Train, Epoch : 8, Step : 4580, Loss : 0.27064, Acc : 0.859, Sensitive_Loss : 0.10657, Sensitive_Acc : 16.100, Run Time : 7.32 sec
INFO:root:2024-04-28 08:41:37, Train, Epoch : 8, Step : 4590, Loss : 0.28528, Acc : 0.884, Sensitive_Loss : 0.08562, Sensitive_Acc : 15.900, Run Time : 7.49 sec
INFO:root:2024-04-28 08:41:44, Train, Epoch : 8, Step : 4600, Loss : 0.22099, Acc : 0.916, Sensitive_Loss : 0.07559, Sensitive_Acc : 16.400, Run Time : 7.05 sec
INFO:root:2024-04-28 08:43:20, Dev, Step : 4600, Loss : 0.45312, Acc : 0.815, Auc : 0.904, Sensitive_Loss : 0.14608, Sensitive_Acc : 16.836, Sensitive_Auc : 0.990, Mean auc: 0.904, Run Time : 95.30 sec
INFO:root:2024-04-28 08:43:25, Train, Epoch : 8, Step : 4610, Loss : 0.29184, Acc : 0.878, Sensitive_Loss : 0.11007, Sensitive_Acc : 17.100, Run Time : 101.18 sec
INFO:root:2024-04-28 08:43:32, Train, Epoch : 8, Step : 4620, Loss : 0.28106, Acc : 0.869, Sensitive_Loss : 0.09879, Sensitive_Acc : 16.100, Run Time : 6.69 sec
INFO:root:2024-04-28 08:43:40, Train, Epoch : 8, Step : 4630, Loss : 0.28420, Acc : 0.872, Sensitive_Loss : 0.08387, Sensitive_Acc : 15.500, Run Time : 7.60 sec
INFO:root:2024-04-28 08:43:47, Train, Epoch : 8, Step : 4640, Loss : 0.31645, Acc : 0.878, Sensitive_Loss : 0.09907, Sensitive_Acc : 16.900, Run Time : 7.56 sec
INFO:root:2024-04-28 08:43:54, Train, Epoch : 8, Step : 4650, Loss : 0.21916, Acc : 0.925, Sensitive_Loss : 0.05995, Sensitive_Acc : 15.800, Run Time : 7.15 sec
INFO:root:2024-04-28 08:44:02, Train, Epoch : 8, Step : 4660, Loss : 0.33145, Acc : 0.847, Sensitive_Loss : 0.05863, Sensitive_Acc : 15.600, Run Time : 7.61 sec
INFO:root:2024-04-28 08:44:09, Train, Epoch : 8, Step : 4670, Loss : 0.29951, Acc : 0.863, Sensitive_Loss : 0.09627, Sensitive_Acc : 16.300, Run Time : 7.10 sec
INFO:root:2024-04-28 08:44:17, Train, Epoch : 8, Step : 4680, Loss : 0.21494, Acc : 0.912, Sensitive_Loss : 0.07608, Sensitive_Acc : 16.600, Run Time : 7.51 sec
INFO:root:2024-04-28 08:44:24, Train, Epoch : 8, Step : 4690, Loss : 0.31809, Acc : 0.853, Sensitive_Loss : 0.17929, Sensitive_Acc : 16.000, Run Time : 7.00 sec
INFO:root:2024-04-28 08:44:31, Train, Epoch : 8, Step : 4700, Loss : 0.22365, Acc : 0.919, Sensitive_Loss : 0.06477, Sensitive_Acc : 17.100, Run Time : 7.37 sec
INFO:root:2024-04-28 08:46:07, Dev, Step : 4700, Loss : 0.42552, Acc : 0.822, Auc : 0.906, Sensitive_Loss : 0.12407, Sensitive_Acc : 16.821, Sensitive_Auc : 0.989, Mean auc: 0.906, Run Time : 95.95 sec
INFO:root:2024-04-28 08:46:13, Train, Epoch : 8, Step : 4710, Loss : 0.25196, Acc : 0.903, Sensitive_Loss : 0.07547, Sensitive_Acc : 16.300, Run Time : 101.85 sec
INFO:root:2024-04-28 08:46:20, Train, Epoch : 8, Step : 4720, Loss : 0.31102, Acc : 0.894, Sensitive_Loss : 0.12121, Sensitive_Acc : 17.500, Run Time : 6.69 sec
INFO:root:2024-04-28 08:46:27, Train, Epoch : 8, Step : 4730, Loss : 0.23783, Acc : 0.863, Sensitive_Loss : 0.12313, Sensitive_Acc : 17.400, Run Time : 7.77 sec
INFO:root:2024-04-28 08:46:34, Train, Epoch : 8, Step : 4740, Loss : 0.24329, Acc : 0.903, Sensitive_Loss : 0.08871, Sensitive_Acc : 16.400, Run Time : 6.53 sec
INFO:root:2024-04-28 08:46:42, Train, Epoch : 8, Step : 4750, Loss : 0.31819, Acc : 0.850, Sensitive_Loss : 0.07803, Sensitive_Acc : 16.200, Run Time : 7.91 sec
INFO:root:2024-04-28 08:46:49, Train, Epoch : 8, Step : 4760, Loss : 0.26782, Acc : 0.903, Sensitive_Loss : 0.10495, Sensitive_Acc : 16.100, Run Time : 7.26 sec
INFO:root:2024-04-28 08:46:57, Train, Epoch : 8, Step : 4770, Loss : 0.23907, Acc : 0.887, Sensitive_Loss : 0.07265, Sensitive_Acc : 15.500, Run Time : 7.46 sec
INFO:root:2024-04-28 08:47:04, Train, Epoch : 8, Step : 4780, Loss : 0.27976, Acc : 0.872, Sensitive_Loss : 0.08160, Sensitive_Acc : 15.600, Run Time : 7.01 sec
INFO:root:2024-04-28 08:47:10, Train, Epoch : 8, Step : 4790, Loss : 0.31025, Acc : 0.856, Sensitive_Loss : 0.11021, Sensitive_Acc : 14.900, Run Time : 6.91 sec
INFO:root:2024-04-28 08:47:18, Train, Epoch : 8, Step : 4800, Loss : 0.29237, Acc : 0.869, Sensitive_Loss : 0.09114, Sensitive_Acc : 15.500, Run Time : 7.56 sec
INFO:root:2024-04-28 08:48:54, Dev, Step : 4800, Loss : 0.41870, Acc : 0.822, Auc : 0.904, Sensitive_Loss : 0.13058, Sensitive_Acc : 16.821, Sensitive_Auc : 0.989, Mean auc: 0.904, Run Time : 96.29 sec
INFO:root:2024-04-28 08:49:00, Train, Epoch : 8, Step : 4810, Loss : 0.30524, Acc : 0.869, Sensitive_Loss : 0.09691, Sensitive_Acc : 17.300, Run Time : 102.18 sec
INFO:root:2024-04-28 08:49:08, Train, Epoch : 8, Step : 4820, Loss : 0.21931, Acc : 0.916, Sensitive_Loss : 0.08289, Sensitive_Acc : 17.400, Run Time : 7.41 sec
INFO:root:2024-04-28 08:49:15, Train, Epoch : 8, Step : 4830, Loss : 0.32105, Acc : 0.850, Sensitive_Loss : 0.13678, Sensitive_Acc : 16.400, Run Time : 7.16 sec
INFO:root:2024-04-28 08:49:22, Train, Epoch : 8, Step : 4840, Loss : 0.23622, Acc : 0.894, Sensitive_Loss : 0.10963, Sensitive_Acc : 15.500, Run Time : 7.43 sec
INFO:root:2024-04-28 08:49:29, Train, Epoch : 8, Step : 4850, Loss : 0.32334, Acc : 0.853, Sensitive_Loss : 0.10955, Sensitive_Acc : 14.900, Run Time : 6.89 sec
INFO:root:2024-04-28 08:49:37, Train, Epoch : 8, Step : 4860, Loss : 0.22387, Acc : 0.887, Sensitive_Loss : 0.08027, Sensitive_Acc : 16.100, Run Time : 7.51 sec
INFO:root:2024-04-28 08:49:44, Train, Epoch : 8, Step : 4870, Loss : 0.23042, Acc : 0.894, Sensitive_Loss : 0.11747, Sensitive_Acc : 16.700, Run Time : 7.86 sec
INFO:root:2024-04-28 08:49:52, Train, Epoch : 8, Step : 4880, Loss : 0.22095, Acc : 0.897, Sensitive_Loss : 0.09963, Sensitive_Acc : 16.800, Run Time : 7.40 sec
INFO:root:2024-04-28 08:49:59, Train, Epoch : 8, Step : 4890, Loss : 0.30675, Acc : 0.881, Sensitive_Loss : 0.06420, Sensitive_Acc : 15.800, Run Time : 7.13 sec
INFO:root:2024-04-28 08:50:06, Train, Epoch : 8, Step : 4900, Loss : 0.29374, Acc : 0.866, Sensitive_Loss : 0.08956, Sensitive_Acc : 16.200, Run Time : 7.07 sec
INFO:root:2024-04-28 08:51:42, Dev, Step : 4900, Loss : 0.43385, Acc : 0.821, Auc : 0.903, Sensitive_Loss : 0.13116, Sensitive_Acc : 16.893, Sensitive_Auc : 0.989, Mean auc: 0.903, Run Time : 95.70 sec
INFO:root:2024-04-28 08:51:47, Train, Epoch : 8, Step : 4910, Loss : 0.20968, Acc : 0.925, Sensitive_Loss : 0.08569, Sensitive_Acc : 16.600, Run Time : 101.37 sec
INFO:root:2024-04-28 08:51:55, Train, Epoch : 8, Step : 4920, Loss : 0.24966, Acc : 0.891, Sensitive_Loss : 0.10708, Sensitive_Acc : 17.300, Run Time : 7.57 sec
INFO:root:2024-04-28 08:52:02, Train, Epoch : 8, Step : 4930, Loss : 0.30932, Acc : 0.884, Sensitive_Loss : 0.10898, Sensitive_Acc : 17.200, Run Time : 6.72 sec
INFO:root:2024-04-28 08:52:09, Train, Epoch : 8, Step : 4940, Loss : 0.22932, Acc : 0.878, Sensitive_Loss : 0.09662, Sensitive_Acc : 17.100, Run Time : 7.59 sec
INFO:root:2024-04-28 08:52:17, Train, Epoch : 8, Step : 4950, Loss : 0.28612, Acc : 0.863, Sensitive_Loss : 0.08794, Sensitive_Acc : 16.100, Run Time : 7.25 sec
INFO:root:2024-04-28 08:52:24, Train, Epoch : 8, Step : 4960, Loss : 0.22483, Acc : 0.903, Sensitive_Loss : 0.09422, Sensitive_Acc : 16.700, Run Time : 7.46 sec
INFO:root:2024-04-28 08:52:31, Train, Epoch : 8, Step : 4970, Loss : 0.24768, Acc : 0.906, Sensitive_Loss : 0.14309, Sensitive_Acc : 17.000, Run Time : 7.06 sec
INFO:root:2024-04-28 08:52:39, Train, Epoch : 8, Step : 4980, Loss : 0.27771, Acc : 0.897, Sensitive_Loss : 0.12785, Sensitive_Acc : 16.200, Run Time : 7.46 sec
INFO:root:2024-04-28 08:52:46, Train, Epoch : 8, Step : 4990, Loss : 0.32650, Acc : 0.887, Sensitive_Loss : 0.10179, Sensitive_Acc : 15.300, Run Time : 7.23 sec
INFO:root:2024-04-28 08:52:53, Train, Epoch : 8, Step : 5000, Loss : 0.20260, Acc : 0.916, Sensitive_Loss : 0.10889, Sensitive_Acc : 17.400, Run Time : 7.14 sec
INFO:root:2024-04-28 08:54:29, Dev, Step : 5000, Loss : 0.42241, Acc : 0.826, Auc : 0.901, Sensitive_Loss : 0.12884, Sensitive_Acc : 16.879, Sensitive_Auc : 0.989, Mean auc: 0.901, Run Time : 96.29 sec
INFO:root:2024-04-28 08:56:07
INFO:root:y_pred: [0.04981744 0.90162385 0.01543984 ... 0.83533204 0.00281702 0.88963175]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.84769821e-01 7.25633348e-04 1.13213457e-01 5.48810021e-05
 9.91449296e-01 4.51911401e-05 9.93260920e-01 9.95689571e-01
 2.65132876e-05 9.22280729e-01 9.32534575e-01 9.99033093e-01
 9.96934652e-01 9.57442105e-01 4.70277155e-03 9.01791215e-01
 9.98580694e-01 4.84687928e-03 2.93146037e-02 9.43927884e-01
 9.83933985e-01 2.34959051e-02 9.99280035e-01 9.76468563e-01
 9.99725640e-01 9.88103926e-01 1.63050165e-04 9.97097492e-01
 9.78506148e-01 5.38671792e-01 7.98762601e-04 1.45681873e-01
 3.59144673e-04 3.95064712e-01 1.35359233e-02 3.98878232e-02
 4.43631038e-03 2.14208895e-03 9.83281195e-01 9.94675279e-01
 7.60529474e-06 6.75351032e-07 9.57334399e-01 7.11738889e-04
 9.99520063e-01 9.92517352e-01 9.81714725e-01 9.80653167e-01
 8.35614558e-03 9.85371351e-01 9.97284293e-01 2.15197206e-02
 3.50282341e-01 5.21030044e-04 8.92627213e-05 1.47682950e-02
 6.59979042e-03 3.01838636e-01 1.75326352e-03 5.59601605e-01
 1.06720375e-02 4.22220007e-02 3.38236988e-03 9.26658213e-01
 1.52040785e-03 9.88425016e-01 6.53175078e-03 9.92047191e-01
 9.51641321e-01 1.63973153e-01 9.21227574e-01 7.92150736e-01
 4.14987502e-04 1.55302957e-01 1.30384811e-04 1.96587862e-04
 2.48282589e-03 8.10641721e-02 2.01784796e-03 9.98464465e-01
 9.98972774e-01 2.39344616e-03 3.10677886e-01 5.62871341e-04
 9.67275202e-01 9.33777213e-01 7.32919015e-03 1.44062983e-02
 9.17087317e-01 9.94024336e-01 9.97640729e-01 8.72916635e-03
 3.16170813e-03 9.96179104e-01 5.09637641e-03 1.87277066e-04
 9.72816467e-01 9.97418165e-01 5.13014325e-04 4.77866823e-04
 9.78202999e-01 9.69520926e-01 9.85897303e-01 9.96887863e-01
 1.26174872e-03 1.22158350e-02 9.34970558e-01 9.83443856e-01
 9.50525284e-01 1.24424041e-04 9.04354692e-01 9.81211901e-01
 2.15966348e-02 9.96932030e-01 9.95816529e-01 9.96208429e-01
 9.71270740e-01 9.98695076e-01 6.22079074e-02 3.85819860e-02
 9.89638925e-01 9.97089088e-01 1.44942242e-04 8.88301611e-01
 9.95537519e-01 2.16432422e-01 9.77423251e-01 1.60846347e-03
 7.57910963e-03 9.79262352e-01 9.84571338e-01 3.25172953e-03
 1.49885693e-03 2.60197483e-02 9.62511361e-01 9.86882150e-01
 9.77193952e-01 4.06988227e-04 2.38045342e-02 9.79989350e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 08:56:07, Dev, Step : 5008, Loss : 0.41991, Acc : 0.828, Auc : 0.903, Sensitive_Loss : 0.12135, Sensitive_Acc : 16.821, Sensitive_Auc : 0.990, Mean auc: 0.903, Run Time : 94.01 sec
INFO:root:2024-04-28 08:56:11, Train, Epoch : 9, Step : 5010, Loss : 0.03861, Acc : 0.184, Sensitive_Loss : 0.01499, Sensitive_Acc : 3.500, Run Time : 3.08 sec
INFO:root:2024-04-28 08:56:18, Train, Epoch : 9, Step : 5020, Loss : 0.25053, Acc : 0.916, Sensitive_Loss : 0.10262, Sensitive_Acc : 16.400, Run Time : 7.01 sec
INFO:root:2024-04-28 08:56:25, Train, Epoch : 9, Step : 5030, Loss : 0.19370, Acc : 0.884, Sensitive_Loss : 0.08765, Sensitive_Acc : 17.500, Run Time : 7.50 sec
INFO:root:2024-04-28 08:56:33, Train, Epoch : 9, Step : 5040, Loss : 0.24430, Acc : 0.887, Sensitive_Loss : 0.06351, Sensitive_Acc : 15.700, Run Time : 7.30 sec
INFO:root:2024-04-28 08:56:40, Train, Epoch : 9, Step : 5050, Loss : 0.26023, Acc : 0.906, Sensitive_Loss : 0.08073, Sensitive_Acc : 16.300, Run Time : 7.16 sec
INFO:root:2024-04-28 08:56:47, Train, Epoch : 9, Step : 5060, Loss : 0.26673, Acc : 0.897, Sensitive_Loss : 0.09341, Sensitive_Acc : 16.500, Run Time : 7.25 sec
INFO:root:2024-04-28 08:56:54, Train, Epoch : 9, Step : 5070, Loss : 0.22747, Acc : 0.891, Sensitive_Loss : 0.10177, Sensitive_Acc : 16.900, Run Time : 7.32 sec
INFO:root:2024-04-28 08:57:05, Train, Epoch : 9, Step : 5080, Loss : 0.25381, Acc : 0.887, Sensitive_Loss : 0.07413, Sensitive_Acc : 14.900, Run Time : 10.04 sec
INFO:root:2024-04-28 08:57:13, Train, Epoch : 9, Step : 5090, Loss : 0.24547, Acc : 0.891, Sensitive_Loss : 0.10103, Sensitive_Acc : 15.100, Run Time : 8.06 sec
INFO:root:2024-04-28 08:57:20, Train, Epoch : 9, Step : 5100, Loss : 0.19145, Acc : 0.925, Sensitive_Loss : 0.07224, Sensitive_Acc : 15.800, Run Time : 7.23 sec
INFO:root:2024-04-28 08:58:56, Dev, Step : 5100, Loss : 0.42814, Acc : 0.823, Auc : 0.904, Sensitive_Loss : 0.12350, Sensitive_Acc : 16.879, Sensitive_Auc : 0.990, Mean auc: 0.904, Run Time : 95.89 sec
INFO:root:2024-04-28 08:59:01, Train, Epoch : 9, Step : 5110, Loss : 0.27618, Acc : 0.881, Sensitive_Loss : 0.09257, Sensitive_Acc : 15.600, Run Time : 101.64 sec
INFO:root:2024-04-28 08:59:09, Train, Epoch : 9, Step : 5120, Loss : 0.23648, Acc : 0.903, Sensitive_Loss : 0.10192, Sensitive_Acc : 15.300, Run Time : 7.26 sec
INFO:root:2024-04-28 08:59:16, Train, Epoch : 9, Step : 5130, Loss : 0.21783, Acc : 0.916, Sensitive_Loss : 0.09843, Sensitive_Acc : 15.000, Run Time : 6.97 sec
INFO:root:2024-04-28 08:59:23, Train, Epoch : 9, Step : 5140, Loss : 0.26359, Acc : 0.897, Sensitive_Loss : 0.07430, Sensitive_Acc : 15.800, Run Time : 7.60 sec
INFO:root:2024-04-28 08:59:31, Train, Epoch : 9, Step : 5150, Loss : 0.32523, Acc : 0.881, Sensitive_Loss : 0.08867, Sensitive_Acc : 18.000, Run Time : 7.41 sec
INFO:root:2024-04-28 08:59:38, Train, Epoch : 9, Step : 5160, Loss : 0.23545, Acc : 0.900, Sensitive_Loss : 0.09489, Sensitive_Acc : 16.800, Run Time : 7.31 sec
INFO:root:2024-04-28 08:59:45, Train, Epoch : 9, Step : 5170, Loss : 0.25030, Acc : 0.906, Sensitive_Loss : 0.08483, Sensitive_Acc : 16.400, Run Time : 7.34 sec
INFO:root:2024-04-28 08:59:53, Train, Epoch : 9, Step : 5180, Loss : 0.24693, Acc : 0.906, Sensitive_Loss : 0.11427, Sensitive_Acc : 16.500, Run Time : 7.42 sec
INFO:root:2024-04-28 09:00:00, Train, Epoch : 9, Step : 5190, Loss : 0.26656, Acc : 0.884, Sensitive_Loss : 0.09040, Sensitive_Acc : 16.700, Run Time : 7.41 sec
INFO:root:2024-04-28 09:00:07, Train, Epoch : 9, Step : 5200, Loss : 0.25898, Acc : 0.887, Sensitive_Loss : 0.13438, Sensitive_Acc : 18.000, Run Time : 6.95 sec
INFO:root:2024-04-28 09:01:43, Dev, Step : 5200, Loss : 0.45782, Acc : 0.814, Auc : 0.899, Sensitive_Loss : 0.13108, Sensitive_Acc : 16.764, Sensitive_Auc : 0.989, Mean auc: 0.899, Run Time : 95.44 sec
INFO:root:2024-04-28 09:01:48, Train, Epoch : 9, Step : 5210, Loss : 0.21750, Acc : 0.916, Sensitive_Loss : 0.08478, Sensitive_Acc : 16.100, Run Time : 101.21 sec
INFO:root:2024-04-28 09:01:55, Train, Epoch : 9, Step : 5220, Loss : 0.25629, Acc : 0.909, Sensitive_Loss : 0.10534, Sensitive_Acc : 16.000, Run Time : 7.07 sec
INFO:root:2024-04-28 09:02:03, Train, Epoch : 9, Step : 5230, Loss : 0.22290, Acc : 0.897, Sensitive_Loss : 0.11563, Sensitive_Acc : 15.800, Run Time : 7.60 sec
INFO:root:2024-04-28 09:02:10, Train, Epoch : 9, Step : 5240, Loss : 0.24971, Acc : 0.887, Sensitive_Loss : 0.08051, Sensitive_Acc : 17.000, Run Time : 7.47 sec
INFO:root:2024-04-28 09:02:18, Train, Epoch : 9, Step : 5250, Loss : 0.27350, Acc : 0.894, Sensitive_Loss : 0.08511, Sensitive_Acc : 16.100, Run Time : 7.06 sec
INFO:root:2024-04-28 09:02:25, Train, Epoch : 9, Step : 5260, Loss : 0.28871, Acc : 0.878, Sensitive_Loss : 0.10377, Sensitive_Acc : 16.100, Run Time : 7.68 sec
INFO:root:2024-04-28 09:02:32, Train, Epoch : 9, Step : 5270, Loss : 0.30045, Acc : 0.866, Sensitive_Loss : 0.07744, Sensitive_Acc : 15.400, Run Time : 6.82 sec
INFO:root:2024-04-28 09:02:40, Train, Epoch : 9, Step : 5280, Loss : 0.22242, Acc : 0.897, Sensitive_Loss : 0.07916, Sensitive_Acc : 17.100, Run Time : 7.81 sec
INFO:root:2024-04-28 09:02:47, Train, Epoch : 9, Step : 5290, Loss : 0.22759, Acc : 0.894, Sensitive_Loss : 0.11394, Sensitive_Acc : 16.000, Run Time : 6.97 sec
INFO:root:2024-04-28 09:02:54, Train, Epoch : 9, Step : 5300, Loss : 0.24999, Acc : 0.878, Sensitive_Loss : 0.08889, Sensitive_Acc : 15.600, Run Time : 7.03 sec
INFO:root:2024-04-28 09:04:30, Dev, Step : 5300, Loss : 0.45012, Acc : 0.819, Auc : 0.901, Sensitive_Loss : 0.11975, Sensitive_Acc : 16.821, Sensitive_Auc : 0.990, Mean auc: 0.901, Run Time : 96.60 sec
INFO:root:2024-04-28 09:04:37, Train, Epoch : 9, Step : 5310, Loss : 0.26059, Acc : 0.912, Sensitive_Loss : 0.09047, Sensitive_Acc : 18.200, Run Time : 102.76 sec
INFO:root:2024-04-28 09:04:43, Train, Epoch : 9, Step : 5320, Loss : 0.24427, Acc : 0.925, Sensitive_Loss : 0.08368, Sensitive_Acc : 16.200, Run Time : 6.73 sec
INFO:root:2024-04-28 09:04:51, Train, Epoch : 9, Step : 5330, Loss : 0.24091, Acc : 0.894, Sensitive_Loss : 0.07289, Sensitive_Acc : 16.400, Run Time : 7.44 sec
INFO:root:2024-04-28 09:04:59, Train, Epoch : 9, Step : 5340, Loss : 0.27510, Acc : 0.897, Sensitive_Loss : 0.08532, Sensitive_Acc : 17.300, Run Time : 7.79 sec
INFO:root:2024-04-28 09:05:05, Train, Epoch : 9, Step : 5350, Loss : 0.28433, Acc : 0.884, Sensitive_Loss : 0.07159, Sensitive_Acc : 16.500, Run Time : 6.80 sec
INFO:root:2024-04-28 09:05:13, Train, Epoch : 9, Step : 5360, Loss : 0.18829, Acc : 0.922, Sensitive_Loss : 0.08978, Sensitive_Acc : 17.100, Run Time : 7.26 sec
INFO:root:2024-04-28 09:05:20, Train, Epoch : 9, Step : 5370, Loss : 0.23547, Acc : 0.900, Sensitive_Loss : 0.09250, Sensitive_Acc : 16.400, Run Time : 7.37 sec
INFO:root:2024-04-28 09:05:28, Train, Epoch : 9, Step : 5380, Loss : 0.31236, Acc : 0.869, Sensitive_Loss : 0.10716, Sensitive_Acc : 16.200, Run Time : 7.88 sec
INFO:root:2024-04-28 09:05:35, Train, Epoch : 9, Step : 5390, Loss : 0.28393, Acc : 0.884, Sensitive_Loss : 0.08540, Sensitive_Acc : 15.900, Run Time : 6.71 sec
INFO:root:2024-04-28 09:05:42, Train, Epoch : 9, Step : 5400, Loss : 0.29797, Acc : 0.875, Sensitive_Loss : 0.10508, Sensitive_Acc : 15.400, Run Time : 7.50 sec
INFO:root:2024-04-28 09:07:18, Dev, Step : 5400, Loss : 0.44111, Acc : 0.819, Auc : 0.899, Sensitive_Loss : 0.13083, Sensitive_Acc : 16.936, Sensitive_Auc : 0.989, Mean auc: 0.899, Run Time : 95.87 sec
INFO:root:2024-04-28 09:07:24, Train, Epoch : 9, Step : 5410, Loss : 0.25889, Acc : 0.897, Sensitive_Loss : 0.09072, Sensitive_Acc : 16.200, Run Time : 101.76 sec
INFO:root:2024-04-28 09:07:31, Train, Epoch : 9, Step : 5420, Loss : 0.26059, Acc : 0.900, Sensitive_Loss : 0.07882, Sensitive_Acc : 15.400, Run Time : 6.69 sec
INFO:root:2024-04-28 09:07:38, Train, Epoch : 9, Step : 5430, Loss : 0.29273, Acc : 0.875, Sensitive_Loss : 0.09622, Sensitive_Acc : 16.700, Run Time : 7.59 sec
INFO:root:2024-04-28 09:07:46, Train, Epoch : 9, Step : 5440, Loss : 0.22650, Acc : 0.912, Sensitive_Loss : 0.06128, Sensitive_Acc : 16.300, Run Time : 7.83 sec
INFO:root:2024-04-28 09:07:53, Train, Epoch : 9, Step : 5450, Loss : 0.25501, Acc : 0.881, Sensitive_Loss : 0.10805, Sensitive_Acc : 15.800, Run Time : 6.93 sec
INFO:root:2024-04-28 09:08:00, Train, Epoch : 9, Step : 5460, Loss : 0.19563, Acc : 0.919, Sensitive_Loss : 0.10665, Sensitive_Acc : 17.600, Run Time : 7.10 sec
INFO:root:2024-04-28 09:08:08, Train, Epoch : 9, Step : 5470, Loss : 0.20390, Acc : 0.922, Sensitive_Loss : 0.11509, Sensitive_Acc : 16.300, Run Time : 8.12 sec
INFO:root:2024-04-28 09:08:15, Train, Epoch : 9, Step : 5480, Loss : 0.27096, Acc : 0.875, Sensitive_Loss : 0.09348, Sensitive_Acc : 17.000, Run Time : 7.02 sec
INFO:root:2024-04-28 09:08:22, Train, Epoch : 9, Step : 5490, Loss : 0.20156, Acc : 0.909, Sensitive_Loss : 0.07303, Sensitive_Acc : 16.100, Run Time : 7.27 sec
INFO:root:2024-04-28 09:08:29, Train, Epoch : 9, Step : 5500, Loss : 0.26960, Acc : 0.884, Sensitive_Loss : 0.09194, Sensitive_Acc : 16.800, Run Time : 7.08 sec
INFO:root:2024-04-28 09:10:05, Dev, Step : 5500, Loss : 0.43933, Acc : 0.820, Auc : 0.900, Sensitive_Loss : 0.13847, Sensitive_Acc : 16.879, Sensitive_Auc : 0.989, Mean auc: 0.900, Run Time : 95.85 sec
INFO:root:2024-04-28 09:10:11, Train, Epoch : 9, Step : 5510, Loss : 0.25454, Acc : 0.903, Sensitive_Loss : 0.09289, Sensitive_Acc : 16.500, Run Time : 101.48 sec
INFO:root:2024-04-28 09:10:19, Train, Epoch : 9, Step : 5520, Loss : 0.24611, Acc : 0.909, Sensitive_Loss : 0.07484, Sensitive_Acc : 16.200, Run Time : 7.57 sec
INFO:root:2024-04-28 09:10:26, Train, Epoch : 9, Step : 5530, Loss : 0.26969, Acc : 0.903, Sensitive_Loss : 0.05653, Sensitive_Acc : 16.900, Run Time : 7.04 sec
INFO:root:2024-04-28 09:10:33, Train, Epoch : 9, Step : 5540, Loss : 0.32578, Acc : 0.881, Sensitive_Loss : 0.09107, Sensitive_Acc : 15.000, Run Time : 7.40 sec
INFO:root:2024-04-28 09:10:41, Train, Epoch : 9, Step : 5550, Loss : 0.24404, Acc : 0.897, Sensitive_Loss : 0.06176, Sensitive_Acc : 16.100, Run Time : 7.65 sec
INFO:root:2024-04-28 09:10:48, Train, Epoch : 9, Step : 5560, Loss : 0.25364, Acc : 0.884, Sensitive_Loss : 0.07888, Sensitive_Acc : 18.400, Run Time : 7.26 sec
INFO:root:2024-04-28 09:10:56, Train, Epoch : 9, Step : 5570, Loss : 0.25128, Acc : 0.887, Sensitive_Loss : 0.09832, Sensitive_Acc : 15.800, Run Time : 7.68 sec
INFO:root:2024-04-28 09:11:02, Train, Epoch : 9, Step : 5580, Loss : 0.22223, Acc : 0.897, Sensitive_Loss : 0.07713, Sensitive_Acc : 15.400, Run Time : 6.81 sec
INFO:root:2024-04-28 09:11:10, Train, Epoch : 9, Step : 5590, Loss : 0.26038, Acc : 0.872, Sensitive_Loss : 0.12096, Sensitive_Acc : 16.700, Run Time : 7.11 sec
INFO:root:2024-04-28 09:11:17, Train, Epoch : 9, Step : 5600, Loss : 0.24684, Acc : 0.900, Sensitive_Loss : 0.09880, Sensitive_Acc : 17.100, Run Time : 7.86 sec
INFO:root:2024-04-28 09:12:53, Dev, Step : 5600, Loss : 0.49352, Acc : 0.809, Auc : 0.900, Sensitive_Loss : 0.13517, Sensitive_Acc : 16.779, Sensitive_Auc : 0.988, Mean auc: 0.900, Run Time : 95.60 sec
INFO:root:2024-04-28 09:12:59, Train, Epoch : 9, Step : 5610, Loss : 0.26988, Acc : 0.866, Sensitive_Loss : 0.08625, Sensitive_Acc : 16.300, Run Time : 101.22 sec
INFO:root:2024-04-28 09:13:06, Train, Epoch : 9, Step : 5620, Loss : 0.26862, Acc : 0.894, Sensitive_Loss : 0.06893, Sensitive_Acc : 16.000, Run Time : 7.11 sec
INFO:root:2024-04-28 09:13:13, Train, Epoch : 9, Step : 5630, Loss : 0.22241, Acc : 0.891, Sensitive_Loss : 0.06850, Sensitive_Acc : 15.900, Run Time : 7.55 sec
INFO:root:2024-04-28 09:14:50
INFO:root:y_pred: [1.0670784e-02 9.2177624e-01 7.9912217e-03 ... 7.4431872e-01 8.9081185e-04
 8.0897969e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.95859921e-01 1.98198203e-03 1.47928551e-01 9.76756273e-05
 9.93328691e-01 1.07269334e-04 9.95401144e-01 9.97602880e-01
 4.28608691e-05 9.60840702e-01 9.64934647e-01 9.99490976e-01
 9.98701453e-01 9.51958537e-01 7.49969808e-03 9.41493809e-01
 9.99547184e-01 1.72521211e-02 1.02207094e-01 9.85343874e-01
 9.92223740e-01 3.44471224e-02 9.99658585e-01 9.84929502e-01
 9.99911308e-01 9.94563401e-01 1.27030551e-04 9.98502493e-01
 9.88400459e-01 7.13374376e-01 6.55320822e-04 4.36834365e-01
 6.18818391e-04 5.86668432e-01 4.70395349e-02 6.15791418e-02
 8.12447444e-03 6.44154102e-03 9.91814673e-01 9.96649802e-01
 1.27454468e-05 1.28396471e-06 9.91427302e-01 1.17058912e-03
 9.99612033e-01 9.98317719e-01 9.88050520e-01 9.97166216e-01
 9.67790466e-03 9.91224587e-01 9.98717666e-01 5.79740256e-02
 2.68756628e-01 6.71247719e-04 1.14263843e-04 5.56170605e-02
 8.20727460e-03 3.81934553e-01 2.13497947e-03 8.22560668e-01
 1.82293318e-02 5.37511930e-02 6.99227070e-03 9.72500682e-01
 2.21351488e-03 9.89003599e-01 9.26223956e-03 9.95570600e-01
 9.77707624e-01 1.85379028e-01 9.10687864e-01 8.87432575e-01
 1.13434740e-04 1.73850372e-01 1.69731225e-04 4.02995676e-04
 2.56468588e-03 6.51049614e-02 3.54662118e-03 9.99308348e-01
 9.99686599e-01 5.77723235e-03 4.02698219e-01 1.65962242e-03
 9.89047468e-01 9.76702750e-01 6.94817305e-03 4.16518599e-02
 9.32139635e-01 9.97671425e-01 9.98812675e-01 8.89718998e-03
 3.70120630e-03 9.98679101e-01 4.34536627e-03 6.51273236e-04
 9.86531198e-01 9.99071717e-01 1.12332846e-03 5.62785775e-04
 9.83114600e-01 9.80868936e-01 9.93999600e-01 9.96304274e-01
 1.17647403e-03 6.42938539e-02 9.63686764e-01 9.89021659e-01
 9.77364242e-01 1.01781028e-04 9.21024382e-01 9.87552166e-01
 5.51475100e-02 9.98513997e-01 9.97905731e-01 9.98475969e-01
 9.83713627e-01 9.99460638e-01 1.35155901e-01 8.94074664e-02
 9.96355176e-01 9.96278226e-01 3.16844729e-04 9.32976186e-01
 9.97541904e-01 3.61560464e-01 9.93486404e-01 2.18593725e-03
 2.20630057e-02 9.78794336e-01 9.89587307e-01 2.90494296e-03
 1.44180970e-03 6.62010834e-02 9.64676082e-01 9.94875252e-01
 9.83293772e-01 4.02685779e-04 3.52400951e-02 9.86236036e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 09:14:50, Dev, Step : 5634, Loss : 0.48586, Acc : 0.810, Auc : 0.898, Sensitive_Loss : 0.14583, Sensitive_Acc : 16.836, Sensitive_Auc : 0.988, Mean auc: 0.898, Run Time : 94.38 sec
INFO:root:2024-04-28 09:14:56, Train, Epoch : 10, Step : 5640, Loss : 0.13338, Acc : 0.550, Sensitive_Loss : 0.04892, Sensitive_Acc : 8.800, Run Time : 5.70 sec
INFO:root:2024-04-28 09:15:04, Train, Epoch : 10, Step : 5650, Loss : 0.24480, Acc : 0.875, Sensitive_Loss : 0.11184, Sensitive_Acc : 15.500, Run Time : 7.05 sec
INFO:root:2024-04-28 09:15:11, Train, Epoch : 10, Step : 5660, Loss : 0.28282, Acc : 0.887, Sensitive_Loss : 0.09051, Sensitive_Acc : 17.400, Run Time : 7.22 sec
INFO:root:2024-04-28 09:15:18, Train, Epoch : 10, Step : 5670, Loss : 0.24928, Acc : 0.903, Sensitive_Loss : 0.09103, Sensitive_Acc : 16.200, Run Time : 7.30 sec
INFO:root:2024-04-28 09:15:25, Train, Epoch : 10, Step : 5680, Loss : 0.25488, Acc : 0.884, Sensitive_Loss : 0.09797, Sensitive_Acc : 16.000, Run Time : 7.39 sec
INFO:root:2024-04-28 09:15:33, Train, Epoch : 10, Step : 5690, Loss : 0.20885, Acc : 0.897, Sensitive_Loss : 0.07966, Sensitive_Acc : 15.400, Run Time : 7.70 sec
INFO:root:2024-04-28 09:15:40, Train, Epoch : 10, Step : 5700, Loss : 0.19365, Acc : 0.912, Sensitive_Loss : 0.10370, Sensitive_Acc : 16.400, Run Time : 7.09 sec
INFO:root:2024-04-28 09:17:16, Dev, Step : 5700, Loss : 0.44959, Acc : 0.817, Auc : 0.897, Sensitive_Loss : 0.13774, Sensitive_Acc : 16.821, Sensitive_Auc : 0.989, Mean auc: 0.897, Run Time : 96.13 sec
INFO:root:2024-04-28 09:17:22, Train, Epoch : 10, Step : 5710, Loss : 0.29495, Acc : 0.866, Sensitive_Loss : 0.09214, Sensitive_Acc : 17.800, Run Time : 101.92 sec
INFO:root:2024-04-28 09:17:30, Train, Epoch : 10, Step : 5720, Loss : 0.22774, Acc : 0.887, Sensitive_Loss : 0.09606, Sensitive_Acc : 16.000, Run Time : 7.42 sec
INFO:root:2024-04-28 09:17:37, Train, Epoch : 10, Step : 5730, Loss : 0.21789, Acc : 0.909, Sensitive_Loss : 0.05659, Sensitive_Acc : 17.300, Run Time : 7.35 sec
INFO:root:2024-04-28 09:17:45, Train, Epoch : 10, Step : 5740, Loss : 0.24877, Acc : 0.903, Sensitive_Loss : 0.09909, Sensitive_Acc : 15.900, Run Time : 8.01 sec
INFO:root:2024-04-28 09:17:52, Train, Epoch : 10, Step : 5750, Loss : 0.22333, Acc : 0.900, Sensitive_Loss : 0.10290, Sensitive_Acc : 16.700, Run Time : 6.59 sec
INFO:root:2024-04-28 09:17:59, Train, Epoch : 10, Step : 5760, Loss : 0.19384, Acc : 0.906, Sensitive_Loss : 0.07660, Sensitive_Acc : 15.700, Run Time : 7.59 sec
INFO:root:2024-04-28 09:18:06, Train, Epoch : 10, Step : 5770, Loss : 0.24021, Acc : 0.884, Sensitive_Loss : 0.10922, Sensitive_Acc : 15.000, Run Time : 6.62 sec
INFO:root:2024-04-28 09:18:13, Train, Epoch : 10, Step : 5780, Loss : 0.25072, Acc : 0.906, Sensitive_Loss : 0.07647, Sensitive_Acc : 14.800, Run Time : 7.57 sec
INFO:root:2024-04-28 09:18:20, Train, Epoch : 10, Step : 5790, Loss : 0.25222, Acc : 0.894, Sensitive_Loss : 0.09143, Sensitive_Acc : 16.900, Run Time : 6.92 sec
INFO:root:2024-04-28 09:18:28, Train, Epoch : 10, Step : 5800, Loss : 0.25661, Acc : 0.909, Sensitive_Loss : 0.08798, Sensitive_Acc : 15.800, Run Time : 7.88 sec
INFO:root:2024-04-28 09:20:05, Dev, Step : 5800, Loss : 0.47862, Acc : 0.811, Auc : 0.898, Sensitive_Loss : 0.12356, Sensitive_Acc : 16.764, Sensitive_Auc : 0.990, Mean auc: 0.898, Run Time : 96.44 sec
INFO:root:2024-04-28 09:20:11, Train, Epoch : 10, Step : 5810, Loss : 0.24049, Acc : 0.887, Sensitive_Loss : 0.08892, Sensitive_Acc : 16.900, Run Time : 102.55 sec
INFO:root:2024-04-28 09:20:18, Train, Epoch : 10, Step : 5820, Loss : 0.20774, Acc : 0.909, Sensitive_Loss : 0.05811, Sensitive_Acc : 16.800, Run Time : 7.58 sec
INFO:root:2024-04-28 09:20:26, Train, Epoch : 10, Step : 5830, Loss : 0.26386, Acc : 0.869, Sensitive_Loss : 0.07959, Sensitive_Acc : 18.000, Run Time : 7.28 sec
INFO:root:2024-04-28 09:20:32, Train, Epoch : 10, Step : 5840, Loss : 0.26834, Acc : 0.894, Sensitive_Loss : 0.09851, Sensitive_Acc : 17.500, Run Time : 6.81 sec
INFO:root:2024-04-28 09:20:39, Train, Epoch : 10, Step : 5850, Loss : 0.20935, Acc : 0.916, Sensitive_Loss : 0.08344, Sensitive_Acc : 16.700, Run Time : 7.15 sec
INFO:root:2024-04-28 09:20:47, Train, Epoch : 10, Step : 5860, Loss : 0.21140, Acc : 0.909, Sensitive_Loss : 0.08964, Sensitive_Acc : 16.500, Run Time : 7.78 sec
INFO:root:2024-04-28 09:20:54, Train, Epoch : 10, Step : 5870, Loss : 0.23825, Acc : 0.906, Sensitive_Loss : 0.06979, Sensitive_Acc : 16.600, Run Time : 6.84 sec
INFO:root:2024-04-28 09:21:01, Train, Epoch : 10, Step : 5880, Loss : 0.18867, Acc : 0.934, Sensitive_Loss : 0.07524, Sensitive_Acc : 16.500, Run Time : 7.37 sec
INFO:root:2024-04-28 09:21:09, Train, Epoch : 10, Step : 5890, Loss : 0.22213, Acc : 0.903, Sensitive_Loss : 0.07175, Sensitive_Acc : 17.500, Run Time : 7.12 sec
INFO:root:2024-04-28 09:21:16, Train, Epoch : 10, Step : 5900, Loss : 0.28078, Acc : 0.878, Sensitive_Loss : 0.06749, Sensitive_Acc : 16.000, Run Time : 7.38 sec
INFO:root:2024-04-28 09:22:54, Dev, Step : 5900, Loss : 0.43825, Acc : 0.816, Auc : 0.897, Sensitive_Loss : 0.12666, Sensitive_Acc : 16.821, Sensitive_Auc : 0.987, Mean auc: 0.897, Run Time : 97.96 sec
INFO:root:2024-04-28 09:23:00, Train, Epoch : 10, Step : 5910, Loss : 0.20050, Acc : 0.909, Sensitive_Loss : 0.08835, Sensitive_Acc : 14.700, Run Time : 103.71 sec
INFO:root:2024-04-28 09:23:07, Train, Epoch : 10, Step : 5920, Loss : 0.22320, Acc : 0.909, Sensitive_Loss : 0.10436, Sensitive_Acc : 15.100, Run Time : 7.53 sec
INFO:root:2024-04-28 09:23:14, Train, Epoch : 10, Step : 5930, Loss : 0.26151, Acc : 0.887, Sensitive_Loss : 0.07843, Sensitive_Acc : 17.500, Run Time : 6.98 sec
INFO:root:2024-04-28 09:23:22, Train, Epoch : 10, Step : 5940, Loss : 0.26320, Acc : 0.891, Sensitive_Loss : 0.07936, Sensitive_Acc : 16.200, Run Time : 8.27 sec
INFO:root:2024-04-28 09:23:29, Train, Epoch : 10, Step : 5950, Loss : 0.22356, Acc : 0.912, Sensitive_Loss : 0.06930, Sensitive_Acc : 15.200, Run Time : 6.94 sec
INFO:root:2024-04-28 09:23:37, Train, Epoch : 10, Step : 5960, Loss : 0.18706, Acc : 0.938, Sensitive_Loss : 0.11736, Sensitive_Acc : 16.900, Run Time : 7.18 sec
INFO:root:2024-04-28 09:23:44, Train, Epoch : 10, Step : 5970, Loss : 0.26778, Acc : 0.878, Sensitive_Loss : 0.11583, Sensitive_Acc : 17.300, Run Time : 7.40 sec
INFO:root:2024-04-28 09:23:51, Train, Epoch : 10, Step : 5980, Loss : 0.24984, Acc : 0.887, Sensitive_Loss : 0.07472, Sensitive_Acc : 16.500, Run Time : 7.36 sec
INFO:root:2024-04-28 09:23:59, Train, Epoch : 10, Step : 5990, Loss : 0.20553, Acc : 0.922, Sensitive_Loss : 0.08434, Sensitive_Acc : 17.000, Run Time : 7.97 sec
INFO:root:2024-04-28 09:24:07, Train, Epoch : 10, Step : 6000, Loss : 0.22725, Acc : 0.906, Sensitive_Loss : 0.12539, Sensitive_Acc : 17.200, Run Time : 7.79 sec
INFO:root:2024-04-28 09:25:57, Dev, Step : 6000, Loss : 0.46836, Acc : 0.816, Auc : 0.899, Sensitive_Loss : 0.12159, Sensitive_Acc : 16.807, Sensitive_Auc : 0.988, Mean auc: 0.899, Run Time : 110.15 sec
INFO:root:2024-04-28 09:26:03, Train, Epoch : 10, Step : 6010, Loss : 0.22698, Acc : 0.909, Sensitive_Loss : 0.07969, Sensitive_Acc : 15.600, Run Time : 116.26 sec
INFO:root:2024-04-28 09:26:11, Train, Epoch : 10, Step : 6020, Loss : 0.26247, Acc : 0.900, Sensitive_Loss : 0.09471, Sensitive_Acc : 18.800, Run Time : 7.94 sec
INFO:root:2024-04-28 09:26:20, Train, Epoch : 10, Step : 6030, Loss : 0.20346, Acc : 0.903, Sensitive_Loss : 0.06426, Sensitive_Acc : 17.200, Run Time : 8.28 sec
INFO:root:2024-04-28 09:26:27, Train, Epoch : 10, Step : 6040, Loss : 0.25298, Acc : 0.887, Sensitive_Loss : 0.11390, Sensitive_Acc : 15.800, Run Time : 7.59 sec
INFO:root:2024-04-28 09:26:44, Train, Epoch : 10, Step : 6050, Loss : 0.21383, Acc : 0.931, Sensitive_Loss : 0.09054, Sensitive_Acc : 15.600, Run Time : 16.92 sec
INFO:root:2024-04-28 09:26:58, Train, Epoch : 10, Step : 6060, Loss : 0.19335, Acc : 0.941, Sensitive_Loss : 0.07995, Sensitive_Acc : 16.000, Run Time : 13.72 sec
INFO:root:2024-04-28 09:27:14, Train, Epoch : 10, Step : 6070, Loss : 0.22603, Acc : 0.903, Sensitive_Loss : 0.10654, Sensitive_Acc : 16.200, Run Time : 16.43 sec
INFO:root:2024-04-28 09:27:34, Train, Epoch : 10, Step : 6080, Loss : 0.20326, Acc : 0.922, Sensitive_Loss : 0.06238, Sensitive_Acc : 17.300, Run Time : 20.14 sec
INFO:root:2024-04-28 09:28:00, Train, Epoch : 10, Step : 6090, Loss : 0.20693, Acc : 0.909, Sensitive_Loss : 0.09347, Sensitive_Acc : 15.600, Run Time : 25.44 sec
INFO:root:2024-04-28 09:28:12, Train, Epoch : 10, Step : 6100, Loss : 0.20862, Acc : 0.919, Sensitive_Loss : 0.08105, Sensitive_Acc : 17.100, Run Time : 11.72 sec
INFO:root:2024-04-28 09:30:16, Dev, Step : 6100, Loss : 0.47735, Acc : 0.816, Auc : 0.903, Sensitive_Loss : 0.11571, Sensitive_Acc : 16.864, Sensitive_Auc : 0.989, Mean auc: 0.903, Run Time : 124.38 sec
INFO:root:2024-04-28 09:30:21, Train, Epoch : 10, Step : 6110, Loss : 0.19011, Acc : 0.928, Sensitive_Loss : 0.08161, Sensitive_Acc : 15.900, Run Time : 129.89 sec
INFO:root:2024-04-28 09:30:29, Train, Epoch : 10, Step : 6120, Loss : 0.25570, Acc : 0.894, Sensitive_Loss : 0.08340, Sensitive_Acc : 16.000, Run Time : 7.52 sec
INFO:root:2024-04-28 09:30:36, Train, Epoch : 10, Step : 6130, Loss : 0.19651, Acc : 0.906, Sensitive_Loss : 0.11970, Sensitive_Acc : 16.100, Run Time : 7.53 sec
INFO:root:2024-04-28 09:30:44, Train, Epoch : 10, Step : 6140, Loss : 0.18753, Acc : 0.919, Sensitive_Loss : 0.11535, Sensitive_Acc : 14.700, Run Time : 7.33 sec
INFO:root:2024-04-28 09:30:51, Train, Epoch : 10, Step : 6150, Loss : 0.25457, Acc : 0.894, Sensitive_Loss : 0.09422, Sensitive_Acc : 14.700, Run Time : 7.22 sec
INFO:root:2024-04-28 09:30:58, Train, Epoch : 10, Step : 6160, Loss : 0.29206, Acc : 0.859, Sensitive_Loss : 0.11064, Sensitive_Acc : 17.500, Run Time : 7.23 sec
INFO:root:2024-04-28 09:31:05, Train, Epoch : 10, Step : 6170, Loss : 0.20148, Acc : 0.912, Sensitive_Loss : 0.10204, Sensitive_Acc : 16.500, Run Time : 7.19 sec
INFO:root:2024-04-28 09:31:13, Train, Epoch : 10, Step : 6180, Loss : 0.22042, Acc : 0.912, Sensitive_Loss : 0.06405, Sensitive_Acc : 17.400, Run Time : 7.23 sec
INFO:root:2024-04-28 09:31:20, Train, Epoch : 10, Step : 6190, Loss : 0.24309, Acc : 0.894, Sensitive_Loss : 0.10270, Sensitive_Acc : 17.400, Run Time : 7.03 sec
INFO:root:2024-04-28 09:31:27, Train, Epoch : 10, Step : 6200, Loss : 0.24252, Acc : 0.912, Sensitive_Loss : 0.07795, Sensitive_Acc : 18.000, Run Time : 7.65 sec
INFO:root:2024-04-28 09:33:04, Dev, Step : 6200, Loss : 0.45120, Acc : 0.820, Auc : 0.904, Sensitive_Loss : 0.11965, Sensitive_Acc : 16.807, Sensitive_Auc : 0.990, Mean auc: 0.904, Run Time : 96.64 sec
INFO:root:2024-04-28 09:33:11, Train, Epoch : 10, Step : 6210, Loss : 0.24806, Acc : 0.897, Sensitive_Loss : 0.08931, Sensitive_Acc : 16.100, Run Time : 103.23 sec
INFO:root:2024-04-28 09:33:17, Train, Epoch : 10, Step : 6220, Loss : 0.18004, Acc : 0.925, Sensitive_Loss : 0.10058, Sensitive_Acc : 16.900, Run Time : 6.83 sec
INFO:root:2024-04-28 09:33:25, Train, Epoch : 10, Step : 6230, Loss : 0.20179, Acc : 0.894, Sensitive_Loss : 0.10396, Sensitive_Acc : 16.600, Run Time : 7.61 sec
INFO:root:2024-04-28 09:33:32, Train, Epoch : 10, Step : 6240, Loss : 0.20823, Acc : 0.900, Sensitive_Loss : 0.07743, Sensitive_Acc : 14.400, Run Time : 6.75 sec
INFO:root:2024-04-28 09:33:39, Train, Epoch : 10, Step : 6250, Loss : 0.21377, Acc : 0.912, Sensitive_Loss : 0.07834, Sensitive_Acc : 17.200, Run Time : 7.46 sec
INFO:root:2024-04-28 09:33:46, Train, Epoch : 10, Step : 6260, Loss : 0.20551, Acc : 0.925, Sensitive_Loss : 0.10184, Sensitive_Acc : 16.300, Run Time : 6.33 sec
INFO:root:2024-04-28 09:35:20
INFO:root:y_pred: [7.6536098e-03 9.4801658e-01 8.8958312e-03 ... 8.1803447e-01 8.6329592e-04
 9.2719412e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.95891333e-01 2.53410707e-03 3.65302190e-02 1.86039233e-05
 9.96062219e-01 2.40398967e-05 9.96286869e-01 9.98897910e-01
 8.10134370e-05 9.28839087e-01 9.80333269e-01 9.99236584e-01
 9.98930633e-01 9.65490460e-01 2.61145132e-03 9.46556687e-01
 9.99367416e-01 9.74844210e-03 8.47455934e-02 9.80792940e-01
 9.90938067e-01 2.65499465e-02 9.99788582e-01 9.87161815e-01
 9.99897361e-01 9.90713060e-01 1.13930255e-04 9.99008000e-01
 9.89268243e-01 7.11630881e-01 3.99976125e-04 9.55626965e-02
 4.65637422e-04 4.04526353e-01 1.93836726e-02 3.07664294e-02
 6.80152141e-03 7.69760227e-03 9.85888064e-01 9.98243332e-01
 1.49254511e-05 1.84283499e-06 9.85768974e-01 5.37707994e-04
 9.99635220e-01 9.97393370e-01 9.91624773e-01 9.98030841e-01
 5.83683280e-03 9.93697584e-01 9.98724043e-01 4.03218158e-02
 1.11459523e-01 2.58943124e-04 1.02956918e-04 4.30065393e-02
 4.51986305e-03 5.27313173e-01 2.08001910e-03 7.01813817e-01
 1.06587801e-02 9.21731349e-03 1.53730130e-02 9.80893850e-01
 2.42829858e-03 9.93892908e-01 3.91650479e-03 9.96514916e-01
 9.47848737e-01 2.63026863e-01 9.62531924e-01 8.78679752e-01
 1.83757773e-04 1.46885723e-01 1.19152137e-04 2.97423161e-04
 1.94365485e-03 5.80000579e-02 2.85570836e-03 9.99132812e-01
 9.98456597e-01 1.54935045e-03 4.50517446e-01 6.26056048e-04
 9.81483161e-01 9.54104722e-01 4.21119668e-03 2.04192884e-02
 9.37345743e-01 9.96924937e-01 9.98987019e-01 1.46390526e-02
 1.11345772e-03 9.98271823e-01 1.57632167e-03 2.75226368e-04
 9.81978118e-01 9.99521852e-01 4.60918847e-04 1.76377187e-03
 9.88174915e-01 9.80251729e-01 9.93491709e-01 9.94851053e-01
 1.11004210e-03 1.20292276e-01 9.55053329e-01 9.87624943e-01
 9.81649697e-01 1.59364092e-04 9.08522606e-01 9.94967520e-01
 3.74970548e-02 9.98074412e-01 9.98659492e-01 9.98720169e-01
 9.84480858e-01 9.99382854e-01 9.68059748e-02 6.70980886e-02
 9.94204104e-01 9.98067439e-01 1.55879927e-04 9.25949931e-01
 9.98001277e-01 2.52078205e-01 9.94905353e-01 1.29246595e-03
 7.47447321e-03 9.68173563e-01 9.83223140e-01 2.02357979e-03
 1.47714373e-03 3.34170014e-02 9.62816954e-01 9.92154419e-01
 9.86664534e-01 3.28199472e-04 2.83521861e-02 9.94100511e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-28 09:35:20, Dev, Step : 6260, Loss : 0.49138, Acc : 0.811, Auc : 0.899, Sensitive_Loss : 0.13106, Sensitive_Acc : 16.879, Sensitive_Auc : 0.989, Mean auc: 0.899, Run Time : 94.52 sec
