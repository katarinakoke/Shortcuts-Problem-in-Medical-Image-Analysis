Running on desktop22:
stdin: is not a tty
/home/pmen/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
0
Using the specified args:
Namespace(cfg_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_pmen.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_train.csv",
    "dev_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_val.csv",
    "pred_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Biased_Sex_1_pos01.csv",
    "pred_model": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Biased_Sex_1_pos011.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": -0.05,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-26 12:39:51, Train, Epoch : 1, Step : 10, Loss : 0.64606, Acc : 0.581, Sensitive_Loss : 0.72318, Sensitive_Acc : 16.800, Run Time : 8.13 sec
INFO:root:2024-04-26 12:39:58, Train, Epoch : 1, Step : 20, Loss : 0.61927, Acc : 0.709, Sensitive_Loss : 0.70119, Sensitive_Acc : 15.600, Run Time : 7.47 sec
INFO:root:2024-04-26 12:40:06, Train, Epoch : 1, Step : 30, Loss : 0.58690, Acc : 0.697, Sensitive_Loss : 0.84375, Sensitive_Acc : 15.700, Run Time : 7.45 sec
INFO:root:2024-04-26 12:40:14, Train, Epoch : 1, Step : 40, Loss : 0.60423, Acc : 0.731, Sensitive_Loss : 0.74797, Sensitive_Acc : 14.400, Run Time : 7.98 sec
INFO:root:2024-04-26 12:40:21, Train, Epoch : 1, Step : 50, Loss : 0.53575, Acc : 0.738, Sensitive_Loss : 0.88348, Sensitive_Acc : 15.700, Run Time : 7.22 sec
INFO:root:2024-04-26 12:40:28, Train, Epoch : 1, Step : 60, Loss : 0.54639, Acc : 0.734, Sensitive_Loss : 0.96678, Sensitive_Acc : 16.700, Run Time : 7.21 sec
INFO:root:2024-04-26 12:40:35, Train, Epoch : 1, Step : 70, Loss : 0.61688, Acc : 0.747, Sensitive_Loss : 1.19709, Sensitive_Acc : 16.300, Run Time : 6.90 sec
INFO:root:2024-04-26 12:40:43, Train, Epoch : 1, Step : 80, Loss : 0.48751, Acc : 0.781, Sensitive_Loss : 1.38796, Sensitive_Acc : 15.900, Run Time : 7.66 sec
INFO:root:2024-04-26 12:40:50, Train, Epoch : 1, Step : 90, Loss : 0.41302, Acc : 0.828, Sensitive_Loss : 1.72269, Sensitive_Acc : 15.700, Run Time : 7.28 sec
INFO:root:2024-04-26 12:40:57, Train, Epoch : 1, Step : 100, Loss : 0.59399, Acc : 0.738, Sensitive_Loss : 1.57208, Sensitive_Acc : 15.900, Run Time : 7.38 sec
INFO:root:2024-04-26 12:42:51, Dev, Step : 100, Loss : 0.51170, Acc : 0.762, Auc : 0.844, Sensitive_Loss : 1.75162, Sensitive_Acc : 15.286, Sensitive_Auc : 0.072, Mean auc: 0.844, Run Time : 113.56 sec
INFO:root:2024-04-26 12:42:52, Best, Step : 100, Loss : 0.51170, Acc : 0.762, Auc : 0.844, Sensitive_Loss : 1.75162, Sensitive_Acc : 15.286, Sensitive_Auc : 0.072, Best Auc : 0.844
INFO:root:2024-04-26 12:42:59, Train, Epoch : 1, Step : 110, Loss : 0.50707, Acc : 0.784, Sensitive_Loss : 1.84154, Sensitive_Acc : 16.000, Run Time : 121.53 sec
INFO:root:2024-04-26 12:43:09, Train, Epoch : 1, Step : 120, Loss : 0.52516, Acc : 0.722, Sensitive_Loss : 2.09786, Sensitive_Acc : 16.300, Run Time : 10.01 sec
INFO:root:2024-04-26 12:43:19, Train, Epoch : 1, Step : 130, Loss : 0.48722, Acc : 0.734, Sensitive_Loss : 2.29316, Sensitive_Acc : 16.200, Run Time : 10.20 sec
INFO:root:2024-04-26 12:43:29, Train, Epoch : 1, Step : 140, Loss : 0.50612, Acc : 0.728, Sensitive_Loss : 1.91828, Sensitive_Acc : 16.500, Run Time : 10.10 sec
INFO:root:2024-04-26 12:43:39, Train, Epoch : 1, Step : 150, Loss : 0.59423, Acc : 0.731, Sensitive_Loss : 1.80763, Sensitive_Acc : 16.100, Run Time : 9.38 sec
INFO:root:2024-04-26 12:43:48, Train, Epoch : 1, Step : 160, Loss : 0.56531, Acc : 0.703, Sensitive_Loss : 1.85458, Sensitive_Acc : 15.500, Run Time : 9.94 sec
INFO:root:2024-04-26 12:43:58, Train, Epoch : 1, Step : 170, Loss : 0.53156, Acc : 0.741, Sensitive_Loss : 1.65013, Sensitive_Acc : 14.500, Run Time : 9.60 sec
INFO:root:2024-04-26 12:44:07, Train, Epoch : 1, Step : 180, Loss : 0.62884, Acc : 0.738, Sensitive_Loss : 1.42042, Sensitive_Acc : 16.400, Run Time : 9.37 sec
INFO:root:2024-04-26 12:44:18, Train, Epoch : 1, Step : 190, Loss : 0.52894, Acc : 0.756, Sensitive_Loss : 1.39716, Sensitive_Acc : 16.300, Run Time : 10.42 sec
INFO:root:2024-04-26 12:44:29, Train, Epoch : 1, Step : 200, Loss : 0.52607, Acc : 0.750, Sensitive_Loss : 1.25206, Sensitive_Acc : 17.000, Run Time : 11.28 sec
INFO:root:2024-04-26 12:46:04, Dev, Step : 200, Loss : 0.52456, Acc : 0.775, Auc : 0.846, Sensitive_Loss : 1.06250, Sensitive_Acc : 15.371, Sensitive_Auc : 0.431, Mean auc: 0.846, Run Time : 94.97 sec
INFO:root:2024-04-26 12:46:05, Best, Step : 200, Loss : 0.52456, Acc : 0.775, Auc : 0.846, Sensitive_Loss : 1.06250, Sensitive_Acc : 15.371, Sensitive_Auc : 0.431, Best Auc : 0.846
INFO:root:2024-04-26 12:46:11, Train, Epoch : 1, Step : 210, Loss : 0.45915, Acc : 0.778, Sensitive_Loss : 1.11743, Sensitive_Acc : 16.800, Run Time : 101.68 sec
INFO:root:2024-04-26 12:46:20, Train, Epoch : 1, Step : 220, Loss : 0.49505, Acc : 0.766, Sensitive_Loss : 0.88973, Sensitive_Acc : 16.200, Run Time : 9.20 sec
INFO:root:2024-04-26 12:46:29, Train, Epoch : 1, Step : 230, Loss : 0.67225, Acc : 0.672, Sensitive_Loss : 0.73701, Sensitive_Acc : 14.600, Run Time : 8.69 sec
INFO:root:2024-04-26 12:46:38, Train, Epoch : 1, Step : 240, Loss : 0.46769, Acc : 0.756, Sensitive_Loss : 0.75582, Sensitive_Acc : 15.800, Run Time : 9.48 sec
INFO:root:2024-04-26 12:46:47, Train, Epoch : 1, Step : 250, Loss : 0.45719, Acc : 0.759, Sensitive_Loss : 0.70510, Sensitive_Acc : 15.100, Run Time : 9.06 sec
INFO:root:2024-04-26 12:46:56, Train, Epoch : 1, Step : 260, Loss : 0.48376, Acc : 0.769, Sensitive_Loss : 0.61983, Sensitive_Acc : 16.200, Run Time : 9.19 sec
INFO:root:2024-04-26 12:47:06, Train, Epoch : 1, Step : 270, Loss : 0.53998, Acc : 0.741, Sensitive_Loss : 0.54615, Sensitive_Acc : 15.700, Run Time : 9.74 sec
INFO:root:2024-04-26 12:47:15, Train, Epoch : 1, Step : 280, Loss : 0.42642, Acc : 0.778, Sensitive_Loss : 0.60971, Sensitive_Acc : 17.100, Run Time : 8.65 sec
INFO:root:2024-04-26 12:47:24, Train, Epoch : 1, Step : 290, Loss : 0.61069, Acc : 0.725, Sensitive_Loss : 0.50986, Sensitive_Acc : 15.800, Run Time : 8.75 sec
INFO:root:2024-04-26 12:47:33, Train, Epoch : 1, Step : 300, Loss : 0.53628, Acc : 0.756, Sensitive_Loss : 0.51500, Sensitive_Acc : 16.600, Run Time : 9.18 sec
INFO:root:2024-04-26 12:49:06, Dev, Step : 300, Loss : 0.54374, Acc : 0.760, Auc : 0.834, Sensitive_Loss : 0.68145, Sensitive_Acc : 15.679, Sensitive_Auc : 0.567, Mean auc: 0.834, Run Time : 93.17 sec
INFO:root:2024-04-26 12:49:11, Train, Epoch : 1, Step : 310, Loss : 0.54835, Acc : 0.728, Sensitive_Loss : 0.47086, Sensitive_Acc : 16.400, Run Time : 98.64 sec
INFO:root:2024-04-26 12:49:20, Train, Epoch : 1, Step : 320, Loss : 0.52973, Acc : 0.775, Sensitive_Loss : 0.35891, Sensitive_Acc : 16.300, Run Time : 8.87 sec
INFO:root:2024-04-26 12:49:30, Train, Epoch : 1, Step : 330, Loss : 0.43865, Acc : 0.828, Sensitive_Loss : 0.37090, Sensitive_Acc : 16.600, Run Time : 9.24 sec
INFO:root:2024-04-26 12:49:39, Train, Epoch : 1, Step : 340, Loss : 0.53381, Acc : 0.775, Sensitive_Loss : 0.32752, Sensitive_Acc : 16.400, Run Time : 9.04 sec
INFO:root:2024-04-26 12:49:47, Train, Epoch : 1, Step : 350, Loss : 0.46725, Acc : 0.784, Sensitive_Loss : 0.32621, Sensitive_Acc : 16.300, Run Time : 8.02 sec
INFO:root:2024-04-26 12:49:56, Train, Epoch : 1, Step : 360, Loss : 0.47949, Acc : 0.778, Sensitive_Loss : 0.35199, Sensitive_Acc : 17.300, Run Time : 9.27 sec
INFO:root:2024-04-26 12:50:04, Train, Epoch : 1, Step : 370, Loss : 0.46109, Acc : 0.781, Sensitive_Loss : 0.24456, Sensitive_Acc : 17.200, Run Time : 8.60 sec
INFO:root:2024-04-26 12:50:13, Train, Epoch : 1, Step : 380, Loss : 0.50377, Acc : 0.812, Sensitive_Loss : 0.25913, Sensitive_Acc : 16.700, Run Time : 8.71 sec
INFO:root:2024-04-26 12:50:21, Train, Epoch : 1, Step : 390, Loss : 0.44420, Acc : 0.794, Sensitive_Loss : 0.32657, Sensitive_Acc : 14.600, Run Time : 8.21 sec
INFO:root:2024-04-26 12:50:31, Train, Epoch : 1, Step : 400, Loss : 0.51475, Acc : 0.769, Sensitive_Loss : 0.33435, Sensitive_Acc : 15.900, Run Time : 9.61 sec
INFO:root:2024-04-26 12:52:05, Dev, Step : 400, Loss : 0.49889, Acc : 0.781, Auc : 0.850, Sensitive_Loss : 0.30449, Sensitive_Acc : 16.607, Sensitive_Auc : 0.952, Mean auc: 0.850, Run Time : 93.70 sec
INFO:root:2024-04-26 12:52:06, Best, Step : 400, Loss : 0.49889, Acc : 0.781, Auc : 0.850, Sensitive_Loss : 0.30449, Sensitive_Acc : 16.607, Sensitive_Auc : 0.952, Best Auc : 0.850
INFO:root:2024-04-26 12:52:12, Train, Epoch : 1, Step : 410, Loss : 0.50413, Acc : 0.778, Sensitive_Loss : 0.26673, Sensitive_Acc : 16.800, Run Time : 101.28 sec
INFO:root:2024-04-26 12:52:21, Train, Epoch : 1, Step : 420, Loss : 0.41382, Acc : 0.838, Sensitive_Loss : 0.21925, Sensitive_Acc : 15.200, Run Time : 8.51 sec
INFO:root:2024-04-26 12:52:29, Train, Epoch : 1, Step : 430, Loss : 0.42033, Acc : 0.819, Sensitive_Loss : 0.26669, Sensitive_Acc : 16.700, Run Time : 8.46 sec
INFO:root:2024-04-26 12:52:39, Train, Epoch : 1, Step : 440, Loss : 0.47101, Acc : 0.778, Sensitive_Loss : 0.23724, Sensitive_Acc : 16.600, Run Time : 9.44 sec
INFO:root:2024-04-26 12:52:48, Train, Epoch : 1, Step : 450, Loss : 0.54033, Acc : 0.756, Sensitive_Loss : 0.30557, Sensitive_Acc : 16.100, Run Time : 9.48 sec
INFO:root:2024-04-26 12:52:57, Train, Epoch : 1, Step : 460, Loss : 0.49713, Acc : 0.791, Sensitive_Loss : 0.25751, Sensitive_Acc : 15.500, Run Time : 8.39 sec
INFO:root:2024-04-26 12:53:05, Train, Epoch : 1, Step : 470, Loss : 0.51564, Acc : 0.791, Sensitive_Loss : 0.24936, Sensitive_Acc : 16.300, Run Time : 8.71 sec
INFO:root:2024-04-26 12:53:14, Train, Epoch : 1, Step : 480, Loss : 0.56771, Acc : 0.753, Sensitive_Loss : 0.25913, Sensitive_Acc : 16.000, Run Time : 8.73 sec
INFO:root:2024-04-26 12:53:23, Train, Epoch : 1, Step : 490, Loss : 0.47048, Acc : 0.759, Sensitive_Loss : 0.21143, Sensitive_Acc : 17.400, Run Time : 8.92 sec
INFO:root:2024-04-26 12:53:31, Train, Epoch : 1, Step : 500, Loss : 0.41064, Acc : 0.797, Sensitive_Loss : 0.19151, Sensitive_Acc : 17.500, Run Time : 7.96 sec
INFO:root:2024-04-26 12:55:04, Dev, Step : 500, Loss : 0.56461, Acc : 0.757, Auc : 0.873, Sensitive_Loss : 0.33323, Sensitive_Acc : 16.693, Sensitive_Auc : 0.946, Mean auc: 0.873, Run Time : 93.49 sec
INFO:root:2024-04-26 12:55:05, Best, Step : 500, Loss : 0.56461, Acc : 0.757, Auc : 0.873, Sensitive_Loss : 0.33323, Sensitive_Acc : 16.693, Sensitive_Auc : 0.946, Best Auc : 0.873
INFO:root:2024-04-26 12:55:12, Train, Epoch : 1, Step : 510, Loss : 0.42742, Acc : 0.791, Sensitive_Loss : 0.25197, Sensitive_Acc : 17.600, Run Time : 100.71 sec
INFO:root:2024-04-26 12:55:20, Train, Epoch : 1, Step : 520, Loss : 0.51413, Acc : 0.778, Sensitive_Loss : 0.25378, Sensitive_Acc : 15.800, Run Time : 8.75 sec
INFO:root:2024-04-26 12:55:29, Train, Epoch : 1, Step : 530, Loss : 0.44876, Acc : 0.769, Sensitive_Loss : 0.19758, Sensitive_Acc : 16.000, Run Time : 8.71 sec
INFO:root:2024-04-26 12:55:37, Train, Epoch : 1, Step : 540, Loss : 0.52919, Acc : 0.769, Sensitive_Loss : 0.23512, Sensitive_Acc : 15.400, Run Time : 8.38 sec
INFO:root:2024-04-26 12:55:46, Train, Epoch : 1, Step : 550, Loss : 0.44337, Acc : 0.787, Sensitive_Loss : 0.23382, Sensitive_Acc : 15.700, Run Time : 8.77 sec
INFO:root:2024-04-26 12:55:54, Train, Epoch : 1, Step : 560, Loss : 0.47506, Acc : 0.794, Sensitive_Loss : 0.23618, Sensitive_Acc : 16.300, Run Time : 8.29 sec
INFO:root:2024-04-26 12:56:03, Train, Epoch : 1, Step : 570, Loss : 0.40428, Acc : 0.825, Sensitive_Loss : 0.18621, Sensitive_Acc : 17.200, Run Time : 8.20 sec
INFO:root:2024-04-26 12:56:12, Train, Epoch : 1, Step : 580, Loss : 0.42414, Acc : 0.806, Sensitive_Loss : 0.22103, Sensitive_Acc : 16.000, Run Time : 8.94 sec
INFO:root:2024-04-26 12:56:20, Train, Epoch : 1, Step : 590, Loss : 0.52940, Acc : 0.750, Sensitive_Loss : 0.19273, Sensitive_Acc : 14.300, Run Time : 8.16 sec
INFO:root:2024-04-26 12:56:28, Train, Epoch : 1, Step : 600, Loss : 0.49971, Acc : 0.734, Sensitive_Loss : 0.22506, Sensitive_Acc : 16.600, Run Time : 7.80 sec
INFO:root:2024-04-26 12:58:02, Dev, Step : 600, Loss : 0.47927, Acc : 0.790, Auc : 0.873, Sensitive_Loss : 0.17926, Sensitive_Acc : 16.607, Sensitive_Auc : 0.986, Mean auc: 0.873, Run Time : 94.12 sec
INFO:root:2024-04-26 12:58:08, Train, Epoch : 1, Step : 610, Loss : 0.46257, Acc : 0.791, Sensitive_Loss : 0.22000, Sensitive_Acc : 16.700, Run Time : 99.96 sec
INFO:root:2024-04-26 12:58:16, Train, Epoch : 1, Step : 620, Loss : 0.45154, Acc : 0.806, Sensitive_Loss : 0.16903, Sensitive_Acc : 14.800, Run Time : 8.21 sec
INFO:root:2024-04-26 12:59:53
INFO:root:y_pred: [0.1260519  0.9501223  0.04545907 ... 0.81811136 0.01738152 0.7327017 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.38979089e-01 3.86233121e-04 2.76617050e-01 6.45169785e-05
 9.90208387e-01 1.96173671e-04 9.90145862e-01 8.05394948e-01
 2.84704642e-04 8.40870917e-01 9.91885424e-01 9.81490195e-01
 8.88304174e-01 7.26456523e-01 4.08121683e-02 7.80407488e-01
 9.66676831e-01 3.14042481e-05 8.27781022e-01 9.68648016e-01
 8.67283225e-01 2.08182603e-01 9.87878740e-01 7.19901204e-01
 9.57004189e-01 8.20455790e-01 1.91973906e-03 9.15483892e-01
 8.89306724e-01 8.80735159e-01 3.78341158e-03 6.74907982e-01
 3.06859370e-02 7.21591013e-03 8.16625655e-01 7.69879390e-03
 3.84454653e-02 5.35012747e-04 9.37625408e-01 8.31912398e-01
 3.23392596e-05 9.04320925e-03 8.66732419e-01 1.12544321e-05
 9.91064191e-01 9.44985092e-01 8.82412732e-01 9.45620418e-01
 2.37199548e-03 7.10744619e-01 9.71511900e-01 3.20404419e-03
 7.42956579e-01 9.58353980e-04 5.42803376e-04 5.09473793e-02
 1.36609720e-02 1.19195525e-02 2.60043103e-04 2.29800493e-01
 3.02120199e-04 4.30978328e-01 3.25367413e-02 8.62931788e-01
 6.45145169e-03 9.77510631e-01 1.22275643e-01 9.30269778e-01
 9.14125919e-01 3.36164922e-01 7.40696788e-01 6.98192179e-01
 2.98013981e-03 1.02939690e-02 1.45482132e-02 7.26603503e-06
 3.98135372e-02 7.17421174e-01 1.50317280e-03 9.10590768e-01
 9.68935668e-01 2.91616016e-04 2.04156175e-01 4.50417952e-04
 8.69466186e-01 6.81421518e-01 8.22529197e-03 3.01091094e-02
 8.73977363e-01 9.37813044e-01 9.75087464e-01 2.92820465e-02
 4.47736293e-01 9.31481957e-01 1.88083217e-01 1.10948589e-02
 9.68195021e-01 8.93690586e-01 1.71533742e-04 2.56272461e-02
 8.75162363e-01 7.49795318e-01 9.93447959e-01 9.37044322e-01
 7.19304616e-03 6.99839950e-01 7.72238135e-01 7.89782286e-01
 8.12199354e-01 6.98472955e-04 6.40183866e-01 9.55807030e-01
 6.84474246e-04 9.23433781e-01 8.33384812e-01 9.04622614e-01
 8.90401721e-01 9.64806676e-01 1.43178200e-04 6.72107637e-01
 9.58840728e-01 9.40548182e-01 3.58706340e-04 9.24120963e-01
 9.69524622e-01 4.17287141e-01 9.54401195e-01 3.52731794e-02
 1.14674605e-01 9.73757029e-01 9.82149303e-01 4.27776650e-02
 1.03118144e-01 6.80494923e-05 9.79549468e-01 9.91773427e-01
 8.50259721e-01 9.09002323e-04 2.16250643e-04 8.35889637e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 12:59:53, Dev, Step : 626, Loss : 0.47191, Acc : 0.791, Auc : 0.869, Sensitive_Loss : 0.17509, Sensitive_Acc : 16.650, Sensitive_Auc : 0.984, Mean auc: 0.869, Run Time : 91.98 sec
INFO:root:2024-04-26 12:59:59, Train, Epoch : 2, Step : 630, Loss : 0.21235, Acc : 0.322, Sensitive_Loss : 0.07651, Sensitive_Acc : 6.000, Run Time : 4.86 sec
INFO:root:2024-04-26 13:00:05, Train, Epoch : 2, Step : 640, Loss : 0.40180, Acc : 0.809, Sensitive_Loss : 0.15040, Sensitive_Acc : 15.700, Run Time : 6.20 sec
INFO:root:2024-04-26 13:00:12, Train, Epoch : 2, Step : 650, Loss : 0.43966, Acc : 0.759, Sensitive_Loss : 0.19953, Sensitive_Acc : 17.900, Run Time : 7.22 sec
INFO:root:2024-04-26 13:00:20, Train, Epoch : 2, Step : 660, Loss : 0.42296, Acc : 0.794, Sensitive_Loss : 0.19844, Sensitive_Acc : 17.600, Run Time : 7.48 sec
INFO:root:2024-04-26 13:00:27, Train, Epoch : 2, Step : 670, Loss : 0.50291, Acc : 0.772, Sensitive_Loss : 0.15069, Sensitive_Acc : 15.000, Run Time : 7.19 sec
INFO:root:2024-04-26 13:00:34, Train, Epoch : 2, Step : 680, Loss : 0.41260, Acc : 0.812, Sensitive_Loss : 0.16816, Sensitive_Acc : 16.200, Run Time : 6.91 sec
INFO:root:2024-04-26 13:00:41, Train, Epoch : 2, Step : 690, Loss : 0.49285, Acc : 0.778, Sensitive_Loss : 0.18135, Sensitive_Acc : 17.500, Run Time : 6.51 sec
INFO:root:2024-04-26 13:00:47, Train, Epoch : 2, Step : 700, Loss : 0.50386, Acc : 0.787, Sensitive_Loss : 0.16037, Sensitive_Acc : 15.800, Run Time : 6.76 sec
INFO:root:2024-04-26 13:02:20, Dev, Step : 700, Loss : 0.49990, Acc : 0.778, Auc : 0.871, Sensitive_Loss : 0.24969, Sensitive_Acc : 16.721, Sensitive_Auc : 0.981, Mean auc: 0.871, Run Time : 92.96 sec
INFO:root:2024-04-26 13:02:26, Train, Epoch : 2, Step : 710, Loss : 0.43816, Acc : 0.809, Sensitive_Loss : 0.16939, Sensitive_Acc : 16.700, Run Time : 98.59 sec
INFO:root:2024-04-26 13:02:33, Train, Epoch : 2, Step : 720, Loss : 0.39697, Acc : 0.812, Sensitive_Loss : 0.18265, Sensitive_Acc : 16.100, Run Time : 6.99 sec
INFO:root:2024-04-26 13:02:40, Train, Epoch : 2, Step : 730, Loss : 0.52061, Acc : 0.791, Sensitive_Loss : 0.17569, Sensitive_Acc : 16.200, Run Time : 7.08 sec
INFO:root:2024-04-26 13:02:47, Train, Epoch : 2, Step : 740, Loss : 0.46713, Acc : 0.781, Sensitive_Loss : 0.15357, Sensitive_Acc : 14.300, Run Time : 6.89 sec
INFO:root:2024-04-26 13:02:54, Train, Epoch : 2, Step : 750, Loss : 0.44152, Acc : 0.803, Sensitive_Loss : 0.18511, Sensitive_Acc : 15.700, Run Time : 7.29 sec
INFO:root:2024-04-26 13:03:01, Train, Epoch : 2, Step : 760, Loss : 0.42111, Acc : 0.781, Sensitive_Loss : 0.19190, Sensitive_Acc : 14.600, Run Time : 7.15 sec
INFO:root:2024-04-26 13:03:08, Train, Epoch : 2, Step : 770, Loss : 0.49424, Acc : 0.784, Sensitive_Loss : 0.20117, Sensitive_Acc : 16.400, Run Time : 6.84 sec
INFO:root:2024-04-26 13:03:15, Train, Epoch : 2, Step : 780, Loss : 0.46609, Acc : 0.819, Sensitive_Loss : 0.19061, Sensitive_Acc : 16.800, Run Time : 6.96 sec
INFO:root:2024-04-26 13:03:22, Train, Epoch : 2, Step : 790, Loss : 0.42991, Acc : 0.794, Sensitive_Loss : 0.16151, Sensitive_Acc : 17.400, Run Time : 6.99 sec
INFO:root:2024-04-26 13:03:29, Train, Epoch : 2, Step : 800, Loss : 0.40910, Acc : 0.800, Sensitive_Loss : 0.16896, Sensitive_Acc : 17.600, Run Time : 7.16 sec
INFO:root:2024-04-26 13:05:02, Dev, Step : 800, Loss : 0.49869, Acc : 0.803, Auc : 0.871, Sensitive_Loss : 0.15928, Sensitive_Acc : 16.693, Sensitive_Auc : 0.988, Mean auc: 0.871, Run Time : 92.34 sec
INFO:root:2024-04-26 13:05:07, Train, Epoch : 2, Step : 810, Loss : 0.46898, Acc : 0.787, Sensitive_Loss : 0.12986, Sensitive_Acc : 17.000, Run Time : 97.78 sec
INFO:root:2024-04-26 13:05:14, Train, Epoch : 2, Step : 820, Loss : 0.43234, Acc : 0.809, Sensitive_Loss : 0.16322, Sensitive_Acc : 15.100, Run Time : 7.11 sec
INFO:root:2024-04-26 13:05:21, Train, Epoch : 2, Step : 830, Loss : 0.39583, Acc : 0.841, Sensitive_Loss : 0.19112, Sensitive_Acc : 16.300, Run Time : 6.83 sec
INFO:root:2024-04-26 13:05:29, Train, Epoch : 2, Step : 840, Loss : 0.45666, Acc : 0.816, Sensitive_Loss : 0.15600, Sensitive_Acc : 14.900, Run Time : 7.61 sec
INFO:root:2024-04-26 13:05:35, Train, Epoch : 2, Step : 850, Loss : 0.42614, Acc : 0.803, Sensitive_Loss : 0.16539, Sensitive_Acc : 15.300, Run Time : 6.93 sec
INFO:root:2024-04-26 13:05:43, Train, Epoch : 2, Step : 860, Loss : 0.44252, Acc : 0.762, Sensitive_Loss : 0.17825, Sensitive_Acc : 15.500, Run Time : 7.17 sec
INFO:root:2024-04-26 13:05:50, Train, Epoch : 2, Step : 870, Loss : 0.43583, Acc : 0.822, Sensitive_Loss : 0.11329, Sensitive_Acc : 16.800, Run Time : 7.19 sec
INFO:root:2024-04-26 13:05:57, Train, Epoch : 2, Step : 880, Loss : 0.39251, Acc : 0.825, Sensitive_Loss : 0.16704, Sensitive_Acc : 16.600, Run Time : 7.02 sec
INFO:root:2024-04-26 13:06:04, Train, Epoch : 2, Step : 890, Loss : 0.43245, Acc : 0.809, Sensitive_Loss : 0.14964, Sensitive_Acc : 17.100, Run Time : 6.98 sec
INFO:root:2024-04-26 13:06:11, Train, Epoch : 2, Step : 900, Loss : 0.49948, Acc : 0.781, Sensitive_Loss : 0.15878, Sensitive_Acc : 14.600, Run Time : 7.07 sec
INFO:root:2024-04-26 13:07:43, Dev, Step : 900, Loss : 0.48010, Acc : 0.793, Auc : 0.883, Sensitive_Loss : 0.19729, Sensitive_Acc : 16.779, Sensitive_Auc : 0.987, Mean auc: 0.883, Run Time : 92.41 sec
INFO:root:2024-04-26 13:07:46, Best, Step : 900, Loss : 0.48010, Acc : 0.793, Auc : 0.883, Sensitive_Loss : 0.19729, Sensitive_Acc : 16.779, Sensitive_Auc : 0.987, Best Auc : 0.883
INFO:root:2024-04-26 13:07:51, Train, Epoch : 2, Step : 910, Loss : 0.34032, Acc : 0.853, Sensitive_Loss : 0.19954, Sensitive_Acc : 16.700, Run Time : 100.60 sec
INFO:root:2024-04-26 13:08:00, Train, Epoch : 2, Step : 920, Loss : 0.47534, Acc : 0.791, Sensitive_Loss : 0.18505, Sensitive_Acc : 16.500, Run Time : 8.11 sec
INFO:root:2024-04-26 13:08:08, Train, Epoch : 2, Step : 930, Loss : 0.50595, Acc : 0.794, Sensitive_Loss : 0.12423, Sensitive_Acc : 15.900, Run Time : 8.15 sec
INFO:root:2024-04-26 13:08:15, Train, Epoch : 2, Step : 940, Loss : 0.49290, Acc : 0.803, Sensitive_Loss : 0.12198, Sensitive_Acc : 15.500, Run Time : 7.32 sec
INFO:root:2024-04-26 13:08:23, Train, Epoch : 2, Step : 950, Loss : 0.42309, Acc : 0.819, Sensitive_Loss : 0.18362, Sensitive_Acc : 16.100, Run Time : 7.91 sec
INFO:root:2024-04-26 13:08:30, Train, Epoch : 2, Step : 960, Loss : 0.50242, Acc : 0.784, Sensitive_Loss : 0.20830, Sensitive_Acc : 16.700, Run Time : 7.00 sec
INFO:root:2024-04-26 13:08:38, Train, Epoch : 2, Step : 970, Loss : 0.45522, Acc : 0.784, Sensitive_Loss : 0.11802, Sensitive_Acc : 15.400, Run Time : 8.50 sec
INFO:root:2024-04-26 13:08:46, Train, Epoch : 2, Step : 980, Loss : 0.43492, Acc : 0.800, Sensitive_Loss : 0.12284, Sensitive_Acc : 16.400, Run Time : 7.11 sec
INFO:root:2024-04-26 13:08:53, Train, Epoch : 2, Step : 990, Loss : 0.50346, Acc : 0.772, Sensitive_Loss : 0.17757, Sensitive_Acc : 15.600, Run Time : 6.98 sec
INFO:root:2024-04-26 13:09:00, Train, Epoch : 2, Step : 1000, Loss : 0.42712, Acc : 0.781, Sensitive_Loss : 0.20215, Sensitive_Acc : 17.500, Run Time : 7.07 sec
INFO:root:2024-04-26 13:10:32, Dev, Step : 1000, Loss : 0.51480, Acc : 0.770, Auc : 0.882, Sensitive_Loss : 0.25644, Sensitive_Acc : 16.721, Sensitive_Auc : 0.963, Mean auc: 0.882, Run Time : 92.05 sec
INFO:root:2024-04-26 13:10:37, Train, Epoch : 2, Step : 1010, Loss : 0.43307, Acc : 0.819, Sensitive_Loss : 0.18938, Sensitive_Acc : 16.300, Run Time : 97.36 sec
INFO:root:2024-04-26 13:10:44, Train, Epoch : 2, Step : 1020, Loss : 0.40854, Acc : 0.819, Sensitive_Loss : 0.18101, Sensitive_Acc : 16.500, Run Time : 7.06 sec
INFO:root:2024-04-26 13:10:51, Train, Epoch : 2, Step : 1030, Loss : 0.51230, Acc : 0.781, Sensitive_Loss : 0.14517, Sensitive_Acc : 16.100, Run Time : 6.89 sec
INFO:root:2024-04-26 13:10:58, Train, Epoch : 2, Step : 1040, Loss : 0.44703, Acc : 0.809, Sensitive_Loss : 0.15805, Sensitive_Acc : 15.100, Run Time : 7.25 sec
INFO:root:2024-04-26 13:11:05, Train, Epoch : 2, Step : 1050, Loss : 0.48141, Acc : 0.809, Sensitive_Loss : 0.14848, Sensitive_Acc : 15.100, Run Time : 6.95 sec
INFO:root:2024-04-26 13:11:12, Train, Epoch : 2, Step : 1060, Loss : 0.49050, Acc : 0.800, Sensitive_Loss : 0.15263, Sensitive_Acc : 17.000, Run Time : 6.97 sec
INFO:root:2024-04-26 13:11:19, Train, Epoch : 2, Step : 1070, Loss : 0.38617, Acc : 0.825, Sensitive_Loss : 0.18310, Sensitive_Acc : 14.900, Run Time : 7.17 sec
INFO:root:2024-04-26 13:11:26, Train, Epoch : 2, Step : 1080, Loss : 0.37744, Acc : 0.819, Sensitive_Loss : 0.19276, Sensitive_Acc : 16.500, Run Time : 6.88 sec
INFO:root:2024-04-26 13:11:34, Train, Epoch : 2, Step : 1090, Loss : 0.38655, Acc : 0.812, Sensitive_Loss : 0.15019, Sensitive_Acc : 15.400, Run Time : 7.87 sec
INFO:root:2024-04-26 13:11:41, Train, Epoch : 2, Step : 1100, Loss : 0.41105, Acc : 0.819, Sensitive_Loss : 0.14992, Sensitive_Acc : 17.500, Run Time : 7.17 sec
INFO:root:2024-04-26 13:13:14, Dev, Step : 1100, Loss : 0.45864, Acc : 0.797, Auc : 0.887, Sensitive_Loss : 0.15662, Sensitive_Acc : 16.764, Sensitive_Auc : 0.979, Mean auc: 0.887, Run Time : 92.65 sec
INFO:root:2024-04-26 13:13:15, Best, Step : 1100, Loss : 0.45864, Acc : 0.797, Auc : 0.887, Sensitive_Loss : 0.15662, Sensitive_Acc : 16.764, Sensitive_Auc : 0.979, Best Auc : 0.887
INFO:root:2024-04-26 13:13:20, Train, Epoch : 2, Step : 1110, Loss : 0.39094, Acc : 0.831, Sensitive_Loss : 0.17089, Sensitive_Acc : 17.000, Run Time : 98.77 sec
INFO:root:2024-04-26 13:13:27, Train, Epoch : 2, Step : 1120, Loss : 0.37638, Acc : 0.834, Sensitive_Loss : 0.13928, Sensitive_Acc : 16.200, Run Time : 6.74 sec
INFO:root:2024-04-26 13:13:34, Train, Epoch : 2, Step : 1130, Loss : 0.46892, Acc : 0.800, Sensitive_Loss : 0.15766, Sensitive_Acc : 16.100, Run Time : 7.72 sec
INFO:root:2024-04-26 13:13:44, Train, Epoch : 2, Step : 1140, Loss : 0.42243, Acc : 0.809, Sensitive_Loss : 0.17110, Sensitive_Acc : 16.400, Run Time : 9.28 sec
INFO:root:2024-04-26 13:13:51, Train, Epoch : 2, Step : 1150, Loss : 0.41731, Acc : 0.822, Sensitive_Loss : 0.20888, Sensitive_Acc : 15.500, Run Time : 7.50 sec
INFO:root:2024-04-26 13:13:59, Train, Epoch : 2, Step : 1160, Loss : 0.49466, Acc : 0.791, Sensitive_Loss : 0.10500, Sensitive_Acc : 15.500, Run Time : 7.72 sec
INFO:root:2024-04-26 13:14:06, Train, Epoch : 2, Step : 1170, Loss : 0.40077, Acc : 0.806, Sensitive_Loss : 0.12976, Sensitive_Acc : 16.000, Run Time : 7.21 sec
INFO:root:2024-04-26 13:14:13, Train, Epoch : 2, Step : 1180, Loss : 0.33806, Acc : 0.863, Sensitive_Loss : 0.15425, Sensitive_Acc : 16.300, Run Time : 7.31 sec
INFO:root:2024-04-26 13:14:21, Train, Epoch : 2, Step : 1190, Loss : 0.43049, Acc : 0.797, Sensitive_Loss : 0.14793, Sensitive_Acc : 17.200, Run Time : 7.40 sec
INFO:root:2024-04-26 13:14:30, Train, Epoch : 2, Step : 1200, Loss : 0.39485, Acc : 0.838, Sensitive_Loss : 0.16240, Sensitive_Acc : 15.700, Run Time : 8.86 sec
INFO:root:2024-04-26 13:16:03, Dev, Step : 1200, Loss : 0.43598, Acc : 0.811, Auc : 0.892, Sensitive_Loss : 0.19286, Sensitive_Acc : 16.636, Sensitive_Auc : 0.984, Mean auc: 0.892, Run Time : 92.85 sec
INFO:root:2024-04-26 13:16:04, Best, Step : 1200, Loss : 0.43598, Acc : 0.811, Auc : 0.892, Sensitive_Loss : 0.19286, Sensitive_Acc : 16.636, Sensitive_Auc : 0.984, Best Auc : 0.892
INFO:root:2024-04-26 13:16:10, Train, Epoch : 2, Step : 1210, Loss : 0.42064, Acc : 0.825, Sensitive_Loss : 0.15459, Sensitive_Acc : 16.400, Run Time : 100.36 sec
INFO:root:2024-04-26 13:16:17, Train, Epoch : 2, Step : 1220, Loss : 0.39511, Acc : 0.816, Sensitive_Loss : 0.13315, Sensitive_Acc : 14.900, Run Time : 6.82 sec
INFO:root:2024-04-26 13:16:24, Train, Epoch : 2, Step : 1230, Loss : 0.39194, Acc : 0.828, Sensitive_Loss : 0.12563, Sensitive_Acc : 15.500, Run Time : 7.05 sec
INFO:root:2024-04-26 13:16:31, Train, Epoch : 2, Step : 1240, Loss : 0.44042, Acc : 0.850, Sensitive_Loss : 0.18045, Sensitive_Acc : 16.400, Run Time : 7.10 sec
INFO:root:2024-04-26 13:16:38, Train, Epoch : 2, Step : 1250, Loss : 0.39812, Acc : 0.822, Sensitive_Loss : 0.14722, Sensitive_Acc : 15.400, Run Time : 7.34 sec
INFO:root:2024-04-26 13:18:12
INFO:root:y_pred: [0.04845881 0.76468176 0.07877627 ... 0.67112845 0.02213124 0.77674013]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.91184115e-01 1.79701805e-04 5.73281169e-01 3.18547536e-04
 9.99080539e-01 1.84822449e-04 9.98314023e-01 9.92077410e-01
 7.49500748e-03 8.00204456e-01 9.95387614e-01 9.97712970e-01
 9.83328342e-01 9.85912502e-01 1.17890239e-01 8.63357425e-01
 9.99156952e-01 2.24197819e-03 6.78272009e-01 9.86382246e-01
 9.89014804e-01 4.22439516e-01 9.98134851e-01 9.47272241e-01
 9.93060350e-01 8.38509977e-01 5.96613463e-05 9.87589300e-01
 9.93568778e-01 8.86539221e-01 2.42862538e-01 4.07861024e-01
 1.27291888e-01 3.03439926e-02 2.18380898e-01 1.42581776e-01
 9.82552469e-02 4.44568954e-02 9.93012965e-01 9.85131800e-01
 1.43524987e-04 4.12611989e-04 9.66289222e-01 4.29378677e-04
 9.99060214e-01 9.89818752e-01 9.94538605e-01 9.89043117e-01
 3.46593908e-03 9.84084308e-01 9.98502493e-01 2.74093971e-02
 5.93968213e-01 2.84542376e-03 1.27949268e-02 1.13142893e-01
 4.72027004e-01 1.37778595e-02 1.45034626e-01 8.67927313e-01
 8.30595642e-02 4.99140143e-01 2.95489188e-03 9.71540511e-01
 4.65011179e-01 9.97879624e-01 2.84519768e-03 9.95161116e-01
 8.82972896e-01 9.24452841e-01 9.71101403e-01 9.02156353e-01
 3.06222532e-02 1.80236641e-02 2.05930807e-02 1.51231594e-04
 1.94623023e-02 5.24175882e-01 2.53489837e-02 9.92490947e-01
 9.97439504e-01 1.39797740e-02 9.16494191e-01 3.01745329e-02
 9.50201333e-01 4.98789638e-01 2.57507691e-05 2.79480368e-01
 9.73172963e-01 9.84095573e-01 9.96234715e-01 3.02927077e-01
 6.43347874e-02 9.96366739e-01 4.66037720e-01 3.06123216e-02
 9.94088769e-01 9.97632146e-01 4.00502654e-03 8.92997801e-01
 9.86680329e-01 9.65487123e-01 9.97204244e-01 9.92877662e-01
 1.42175153e-01 9.37799811e-01 8.07147861e-01 9.50637639e-01
 8.87224078e-01 1.66423633e-04 8.92699480e-01 9.98176932e-01
 4.35993932e-02 9.97662783e-01 9.82572556e-01 9.90497768e-01
 9.40270066e-01 9.94552791e-01 1.82978832e-03 6.66848004e-01
 9.97342765e-01 9.83663559e-01 8.42349790e-03 9.63522613e-01
 9.96453881e-01 2.08387896e-01 9.95528042e-01 1.87381744e-01
 2.81218054e-05 9.93513584e-01 9.96898532e-01 1.06029436e-01
 1.86123624e-02 2.01712996e-01 9.96221662e-01 9.98758316e-01
 9.59441483e-01 3.89430337e-02 3.04804146e-01 9.91491735e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 13:18:12, Dev, Step : 1252, Loss : 0.47358, Acc : 0.796, Auc : 0.897, Sensitive_Loss : 0.22779, Sensitive_Acc : 16.764, Sensitive_Auc : 0.979, Mean auc: 0.897, Run Time : 92.40 sec
INFO:root:2024-04-26 13:18:13, Best, Step : 1252, Loss : 0.47358, Acc : 0.796,Auc : 0.897, Best Auc : 0.897, Sensitive_Loss : 0.22779, Sensitive_Acc : 16.764, Sensitive_Auc : 0.979
INFO:root:2024-04-26 13:18:21, Train, Epoch : 3, Step : 1260, Loss : 0.34915, Acc : 0.666, Sensitive_Loss : 0.10068, Sensitive_Acc : 13.900, Run Time : 6.37 sec
INFO:root:2024-04-26 13:18:30, Train, Epoch : 3, Step : 1270, Loss : 0.36386, Acc : 0.831, Sensitive_Loss : 0.14068, Sensitive_Acc : 17.400, Run Time : 8.91 sec
INFO:root:2024-04-26 13:18:36, Train, Epoch : 3, Step : 1280, Loss : 0.43336, Acc : 0.806, Sensitive_Loss : 0.14742, Sensitive_Acc : 16.400, Run Time : 6.26 sec
INFO:root:2024-04-26 13:18:43, Train, Epoch : 3, Step : 1290, Loss : 0.38367, Acc : 0.834, Sensitive_Loss : 0.14475, Sensitive_Acc : 17.800, Run Time : 7.15 sec
INFO:root:2024-04-26 13:18:50, Train, Epoch : 3, Step : 1300, Loss : 0.36078, Acc : 0.834, Sensitive_Loss : 0.18242, Sensitive_Acc : 17.300, Run Time : 7.22 sec
INFO:root:2024-04-26 13:20:24, Dev, Step : 1300, Loss : 0.42528, Acc : 0.811, Auc : 0.899, Sensitive_Loss : 0.15116, Sensitive_Acc : 16.764, Sensitive_Auc : 0.982, Mean auc: 0.899, Run Time : 94.24 sec
INFO:root:2024-04-26 13:20:25, Best, Step : 1300, Loss : 0.42528, Acc : 0.811, Auc : 0.899, Sensitive_Loss : 0.15116, Sensitive_Acc : 16.764, Sensitive_Auc : 0.982, Best Auc : 0.899
INFO:root:2024-04-26 13:20:31, Train, Epoch : 3, Step : 1310, Loss : 0.36851, Acc : 0.828, Sensitive_Loss : 0.14071, Sensitive_Acc : 16.500, Run Time : 101.02 sec
INFO:root:2024-04-26 13:20:39, Train, Epoch : 3, Step : 1320, Loss : 0.40043, Acc : 0.803, Sensitive_Loss : 0.15352, Sensitive_Acc : 14.900, Run Time : 7.46 sec
INFO:root:2024-04-26 13:20:45, Train, Epoch : 3, Step : 1330, Loss : 0.35330, Acc : 0.834, Sensitive_Loss : 0.10725, Sensitive_Acc : 17.700, Run Time : 6.57 sec
INFO:root:2024-04-26 13:20:52, Train, Epoch : 3, Step : 1340, Loss : 0.37714, Acc : 0.819, Sensitive_Loss : 0.15248, Sensitive_Acc : 15.300, Run Time : 7.04 sec
INFO:root:2024-04-26 13:20:59, Train, Epoch : 3, Step : 1350, Loss : 0.30637, Acc : 0.856, Sensitive_Loss : 0.15271, Sensitive_Acc : 17.100, Run Time : 7.09 sec
INFO:root:2024-04-26 13:21:06, Train, Epoch : 3, Step : 1360, Loss : 0.35912, Acc : 0.841, Sensitive_Loss : 0.13514, Sensitive_Acc : 16.700, Run Time : 6.92 sec
INFO:root:2024-04-26 13:21:13, Train, Epoch : 3, Step : 1370, Loss : 0.40489, Acc : 0.819, Sensitive_Loss : 0.11656, Sensitive_Acc : 16.700, Run Time : 6.93 sec
INFO:root:2024-04-26 13:21:21, Train, Epoch : 3, Step : 1380, Loss : 0.36251, Acc : 0.834, Sensitive_Loss : 0.15525, Sensitive_Acc : 15.500, Run Time : 7.38 sec
INFO:root:2024-04-26 13:21:28, Train, Epoch : 3, Step : 1390, Loss : 0.37316, Acc : 0.803, Sensitive_Loss : 0.17066, Sensitive_Acc : 16.000, Run Time : 7.11 sec
INFO:root:2024-04-26 13:21:35, Train, Epoch : 3, Step : 1400, Loss : 0.39302, Acc : 0.822, Sensitive_Loss : 0.14212, Sensitive_Acc : 15.400, Run Time : 7.05 sec
INFO:root:2024-04-26 13:23:07, Dev, Step : 1400, Loss : 0.42575, Acc : 0.814, Auc : 0.903, Sensitive_Loss : 0.15364, Sensitive_Acc : 16.764, Sensitive_Auc : 0.984, Mean auc: 0.903, Run Time : 92.61 sec
INFO:root:2024-04-26 13:23:08, Best, Step : 1400, Loss : 0.42575, Acc : 0.814, Auc : 0.903, Sensitive_Loss : 0.15364, Sensitive_Acc : 16.764, Sensitive_Auc : 0.984, Best Auc : 0.903
INFO:root:2024-04-26 13:23:14, Train, Epoch : 3, Step : 1410, Loss : 0.39182, Acc : 0.816, Sensitive_Loss : 0.13566, Sensitive_Acc : 16.300, Run Time : 98.85 sec
INFO:root:2024-04-26 13:23:21, Train, Epoch : 3, Step : 1420, Loss : 0.34552, Acc : 0.828, Sensitive_Loss : 0.13693, Sensitive_Acc : 15.100, Run Time : 7.26 sec
INFO:root:2024-04-26 13:23:28, Train, Epoch : 3, Step : 1430, Loss : 0.39491, Acc : 0.838, Sensitive_Loss : 0.14816, Sensitive_Acc : 16.200, Run Time : 7.64 sec
INFO:root:2024-04-26 13:23:36, Train, Epoch : 3, Step : 1440, Loss : 0.32034, Acc : 0.853, Sensitive_Loss : 0.15916, Sensitive_Acc : 16.400, Run Time : 7.03 sec
INFO:root:2024-04-26 13:23:42, Train, Epoch : 3, Step : 1450, Loss : 0.36382, Acc : 0.834, Sensitive_Loss : 0.15901, Sensitive_Acc : 16.700, Run Time : 6.78 sec
INFO:root:2024-04-26 13:23:49, Train, Epoch : 3, Step : 1460, Loss : 0.39882, Acc : 0.834, Sensitive_Loss : 0.13345, Sensitive_Acc : 16.900, Run Time : 7.07 sec
INFO:root:2024-04-26 13:23:57, Train, Epoch : 3, Step : 1470, Loss : 0.37076, Acc : 0.853, Sensitive_Loss : 0.10413, Sensitive_Acc : 17.700, Run Time : 7.27 sec
INFO:root:2024-04-26 13:24:04, Train, Epoch : 3, Step : 1480, Loss : 0.34700, Acc : 0.828, Sensitive_Loss : 0.17897, Sensitive_Acc : 16.600, Run Time : 6.95 sec
INFO:root:2024-04-26 13:24:11, Train, Epoch : 3, Step : 1490, Loss : 0.37399, Acc : 0.847, Sensitive_Loss : 0.11574, Sensitive_Acc : 16.500, Run Time : 7.14 sec
INFO:root:2024-04-26 13:24:18, Train, Epoch : 3, Step : 1500, Loss : 0.45169, Acc : 0.806, Sensitive_Loss : 0.11619, Sensitive_Acc : 15.000, Run Time : 6.80 sec
INFO:root:2024-04-26 13:25:51, Dev, Step : 1500, Loss : 0.42375, Acc : 0.820, Auc : 0.903, Sensitive_Loss : 0.15748, Sensitive_Acc : 16.779, Sensitive_Auc : 0.987, Mean auc: 0.903, Run Time : 93.57 sec
INFO:root:2024-04-26 13:25:57, Train, Epoch : 3, Step : 1510, Loss : 0.37540, Acc : 0.841, Sensitive_Loss : 0.13184, Sensitive_Acc : 15.400, Run Time : 99.45 sec
INFO:root:2024-04-26 13:26:04, Train, Epoch : 3, Step : 1520, Loss : 0.39494, Acc : 0.816, Sensitive_Loss : 0.13134, Sensitive_Acc : 15.600, Run Time : 6.91 sec
INFO:root:2024-04-26 13:26:11, Train, Epoch : 3, Step : 1530, Loss : 0.38982, Acc : 0.844, Sensitive_Loss : 0.13576, Sensitive_Acc : 16.900, Run Time : 7.06 sec
INFO:root:2024-04-26 13:26:18, Train, Epoch : 3, Step : 1540, Loss : 0.38500, Acc : 0.844, Sensitive_Loss : 0.08592, Sensitive_Acc : 14.700, Run Time : 7.49 sec
INFO:root:2024-04-26 13:26:25, Train, Epoch : 3, Step : 1550, Loss : 0.45152, Acc : 0.809, Sensitive_Loss : 0.11499, Sensitive_Acc : 16.200, Run Time : 6.52 sec
INFO:root:2024-04-26 13:26:32, Train, Epoch : 3, Step : 1560, Loss : 0.42213, Acc : 0.809, Sensitive_Loss : 0.15112, Sensitive_Acc : 15.900, Run Time : 7.35 sec
INFO:root:2024-04-26 13:26:39, Train, Epoch : 3, Step : 1570, Loss : 0.26443, Acc : 0.891, Sensitive_Loss : 0.13786, Sensitive_Acc : 15.700, Run Time : 7.03 sec
INFO:root:2024-04-26 13:26:46, Train, Epoch : 3, Step : 1580, Loss : 0.41206, Acc : 0.822, Sensitive_Loss : 0.14158, Sensitive_Acc : 15.300, Run Time : 7.09 sec
INFO:root:2024-04-26 13:26:53, Train, Epoch : 3, Step : 1590, Loss : 0.36690, Acc : 0.859, Sensitive_Loss : 0.16203, Sensitive_Acc : 15.700, Run Time : 6.91 sec
INFO:root:2024-04-26 13:27:00, Train, Epoch : 3, Step : 1600, Loss : 0.41584, Acc : 0.825, Sensitive_Loss : 0.14422, Sensitive_Acc : 17.600, Run Time : 6.82 sec
INFO:root:2024-04-26 13:28:33, Dev, Step : 1600, Loss : 0.42212, Acc : 0.816, Auc : 0.904, Sensitive_Loss : 0.15017, Sensitive_Acc : 16.736, Sensitive_Auc : 0.986, Mean auc: 0.904, Run Time : 93.03 sec
INFO:root:2024-04-26 13:28:34, Best, Step : 1600, Loss : 0.42212, Acc : 0.816, Auc : 0.904, Sensitive_Loss : 0.15017, Sensitive_Acc : 16.736, Sensitive_Auc : 0.986, Best Auc : 0.904
INFO:root:2024-04-26 13:28:40, Train, Epoch : 3, Step : 1610, Loss : 0.33874, Acc : 0.847, Sensitive_Loss : 0.08001, Sensitive_Acc : 14.900, Run Time : 99.42 sec
INFO:root:2024-04-26 13:28:47, Train, Epoch : 3, Step : 1620, Loss : 0.30825, Acc : 0.847, Sensitive_Loss : 0.13684, Sensitive_Acc : 16.500, Run Time : 7.36 sec
INFO:root:2024-04-26 13:28:54, Train, Epoch : 3, Step : 1630, Loss : 0.27637, Acc : 0.850, Sensitive_Loss : 0.14717, Sensitive_Acc : 16.000, Run Time : 7.17 sec
INFO:root:2024-04-26 13:29:01, Train, Epoch : 3, Step : 1640, Loss : 0.36769, Acc : 0.834, Sensitive_Loss : 0.13816, Sensitive_Acc : 17.700, Run Time : 7.11 sec
INFO:root:2024-04-26 13:29:08, Train, Epoch : 3, Step : 1650, Loss : 0.37762, Acc : 0.819, Sensitive_Loss : 0.16511, Sensitive_Acc : 16.600, Run Time : 6.50 sec
INFO:root:2024-04-26 13:29:15, Train, Epoch : 3, Step : 1660, Loss : 0.34323, Acc : 0.853, Sensitive_Loss : 0.09663, Sensitive_Acc : 15.900, Run Time : 7.56 sec
INFO:root:2024-04-26 13:29:22, Train, Epoch : 3, Step : 1670, Loss : 0.37698, Acc : 0.841, Sensitive_Loss : 0.17074, Sensitive_Acc : 16.900, Run Time : 6.99 sec
INFO:root:2024-04-26 13:29:29, Train, Epoch : 3, Step : 1680, Loss : 0.44943, Acc : 0.787, Sensitive_Loss : 0.16718, Sensitive_Acc : 15.500, Run Time : 7.02 sec
INFO:root:2024-04-26 13:29:36, Train, Epoch : 3, Step : 1690, Loss : 0.34900, Acc : 0.825, Sensitive_Loss : 0.11941, Sensitive_Acc : 15.200, Run Time : 6.98 sec
INFO:root:2024-04-26 13:29:43, Train, Epoch : 3, Step : 1700, Loss : 0.33877, Acc : 0.850, Sensitive_Loss : 0.08543, Sensitive_Acc : 15.700, Run Time : 7.16 sec
INFO:root:2024-04-26 13:31:16, Dev, Step : 1700, Loss : 0.41643, Acc : 0.822, Auc : 0.905, Sensitive_Loss : 0.15341, Sensitive_Acc : 16.736, Sensitive_Auc : 0.985, Mean auc: 0.905, Run Time : 92.27 sec
INFO:root:2024-04-26 13:31:16, Best, Step : 1700, Loss : 0.41643, Acc : 0.822, Auc : 0.905, Sensitive_Loss : 0.15341, Sensitive_Acc : 16.736, Sensitive_Auc : 0.985, Best Auc : 0.905
INFO:root:2024-04-26 13:31:22, Train, Epoch : 3, Step : 1710, Loss : 0.39035, Acc : 0.831, Sensitive_Loss : 0.12014, Sensitive_Acc : 15.000, Run Time : 98.60 sec
INFO:root:2024-04-26 13:31:29, Train, Epoch : 3, Step : 1720, Loss : 0.33995, Acc : 0.863, Sensitive_Loss : 0.13387, Sensitive_Acc : 15.400, Run Time : 7.03 sec
INFO:root:2024-04-26 13:31:37, Train, Epoch : 3, Step : 1730, Loss : 0.36703, Acc : 0.828, Sensitive_Loss : 0.10441, Sensitive_Acc : 16.400, Run Time : 7.47 sec
INFO:root:2024-04-26 13:31:44, Train, Epoch : 3, Step : 1740, Loss : 0.41384, Acc : 0.828, Sensitive_Loss : 0.16686, Sensitive_Acc : 16.000, Run Time : 7.12 sec
INFO:root:2024-04-26 13:31:50, Train, Epoch : 3, Step : 1750, Loss : 0.40137, Acc : 0.806, Sensitive_Loss : 0.13630, Sensitive_Acc : 16.700, Run Time : 6.47 sec
INFO:root:2024-04-26 13:31:57, Train, Epoch : 3, Step : 1760, Loss : 0.37991, Acc : 0.825, Sensitive_Loss : 0.11412, Sensitive_Acc : 15.700, Run Time : 7.09 sec
INFO:root:2024-04-26 13:32:04, Train, Epoch : 3, Step : 1770, Loss : 0.35474, Acc : 0.822, Sensitive_Loss : 0.10086, Sensitive_Acc : 15.800, Run Time : 7.14 sec
INFO:root:2024-04-26 13:32:12, Train, Epoch : 3, Step : 1780, Loss : 0.33138, Acc : 0.866, Sensitive_Loss : 0.13419, Sensitive_Acc : 17.000, Run Time : 7.14 sec
INFO:root:2024-04-26 13:32:18, Train, Epoch : 3, Step : 1790, Loss : 0.38381, Acc : 0.847, Sensitive_Loss : 0.16355, Sensitive_Acc : 16.200, Run Time : 6.80 sec
INFO:root:2024-04-26 13:32:25, Train, Epoch : 3, Step : 1800, Loss : 0.33769, Acc : 0.863, Sensitive_Loss : 0.13801, Sensitive_Acc : 14.000, Run Time : 6.92 sec
INFO:root:2024-04-26 13:34:24, Dev, Step : 1800, Loss : 0.41299, Acc : 0.824, Auc : 0.906, Sensitive_Loss : 0.13898, Sensitive_Acc : 16.679, Sensitive_Auc : 0.987, Mean auc: 0.906, Run Time : 118.25 sec
INFO:root:2024-04-26 13:34:25, Best, Step : 1800, Loss : 0.41299, Acc : 0.824, Auc : 0.906, Sensitive_Loss : 0.13898, Sensitive_Acc : 16.679, Sensitive_Auc : 0.987, Best Auc : 0.906
INFO:root:2024-04-26 13:34:31, Train, Epoch : 3, Step : 1810, Loss : 0.32935, Acc : 0.878, Sensitive_Loss : 0.10210, Sensitive_Acc : 16.200, Run Time : 126.08 sec
INFO:root:2024-04-26 13:34:39, Train, Epoch : 3, Step : 1820, Loss : 0.37431, Acc : 0.853, Sensitive_Loss : 0.12480, Sensitive_Acc : 14.700, Run Time : 7.83 sec
INFO:root:2024-04-26 13:34:47, Train, Epoch : 3, Step : 1830, Loss : 0.35169, Acc : 0.831, Sensitive_Loss : 0.11612, Sensitive_Acc : 17.100, Run Time : 8.19 sec
INFO:root:2024-04-26 13:34:56, Train, Epoch : 3, Step : 1840, Loss : 0.39132, Acc : 0.828, Sensitive_Loss : 0.08885, Sensitive_Acc : 17.500, Run Time : 9.08 sec
INFO:root:2024-04-26 13:35:04, Train, Epoch : 3, Step : 1850, Loss : 0.33991, Acc : 0.859, Sensitive_Loss : 0.09833, Sensitive_Acc : 15.900, Run Time : 7.76 sec
INFO:root:2024-04-26 13:35:13, Train, Epoch : 3, Step : 1860, Loss : 0.36526, Acc : 0.831, Sensitive_Loss : 0.09405, Sensitive_Acc : 16.200, Run Time : 8.68 sec
INFO:root:2024-04-26 13:35:21, Train, Epoch : 3, Step : 1870, Loss : 0.42447, Acc : 0.819, Sensitive_Loss : 0.14156, Sensitive_Acc : 15.500, Run Time : 8.00 sec
INFO:root:2024-04-26 13:37:06
INFO:root:y_pred: [0.09940546 0.90209377 0.12723076 ... 0.80644727 0.00653803 0.8347367 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.78333592e-01 3.43692605e-04 4.46713746e-01 1.31656088e-06
 9.92636561e-01 3.40492363e-07 9.93116617e-01 9.75146711e-01
 7.19665876e-03 8.60202134e-01 9.90273893e-01 9.94793832e-01
 9.90200460e-01 9.53650057e-01 1.60584822e-02 9.23788548e-01
 9.98639286e-01 2.22625956e-03 7.91051090e-01 9.95693564e-01
 9.62615967e-01 6.06030487e-02 9.97444272e-01 9.30830300e-01
 9.86344218e-01 6.29132628e-01 2.44963053e-06 9.79265571e-01
 9.87645805e-01 3.81445169e-01 4.91575804e-03 5.36719620e-01
 5.75987808e-03 1.35711511e-03 2.49805581e-02 9.22225788e-03
 7.67468587e-02 2.39148294e-03 9.84145701e-01 9.55632687e-01
 5.43660065e-07 3.31147276e-05 9.67430711e-01 9.02640022e-06
 9.97310281e-01 9.89458799e-01 9.82079387e-01 9.96679783e-01
 6.33873715e-05 9.83281910e-01 9.98281002e-01 6.57463446e-04
 2.94194907e-01 9.05873094e-05 1.47742621e-05 1.03991730e-02
 1.05853006e-02 6.23916159e-04 2.75259814e-03 6.03774786e-01
 5.34963282e-03 1.86213404e-01 1.99726841e-04 9.85084593e-01
 7.99584538e-02 9.94270205e-01 1.51610493e-06 9.92336333e-01
 7.77089655e-01 7.37417996e-01 9.22381103e-01 7.06223190e-01
 8.02055758e-04 6.67314697e-03 1.59769016e-03 2.27834130e-06
 2.27671801e-04 1.05718404e-01 3.77790595e-04 9.82648492e-01
 9.96802092e-01 1.37869152e-04 9.02325988e-01 7.75650435e-04
 9.48158622e-01 6.65464342e-01 3.97772192e-06 2.01348871e-01
 9.14127886e-01 9.75767493e-01 9.83573020e-01 2.85010505e-02
 9.39228758e-03 9.95009422e-01 1.12172239e-01 3.27532999e-02
 9.93994296e-01 9.89047348e-01 5.41888185e-05 3.73113781e-01
 9.50522244e-01 9.36699450e-01 9.98152912e-01 9.90217626e-01
 1.86083522e-02 7.31069028e-01 6.88071966e-01 8.79330754e-01
 8.59685481e-01 1.27008491e-06 8.07200611e-01 9.90034878e-01
 2.63370108e-03 9.95201588e-01 9.35070455e-01 9.83126462e-01
 8.75664711e-01 9.89120364e-01 8.31377984e-05 2.11136401e-01
 9.95021462e-01 9.81300592e-01 9.23357729e-05 9.00227427e-01
 9.85197842e-01 2.96666380e-02 9.91282463e-01 3.99651341e-02
 1.31459936e-04 9.94577408e-01 9.93315518e-01 2.75470258e-04
 3.11242293e-05 2.22612955e-02 9.91226733e-01 9.98846531e-01
 9.45150137e-01 1.05323060e-03 5.26142586e-03 9.72805202e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 13:37:06, Dev, Step : 1878, Loss : 0.42303, Acc : 0.820, Auc : 0.905, Sensitive_Loss : 0.15609, Sensitive_Acc : 16.721, Sensitive_Auc : 0.987, Mean auc: 0.905, Run Time : 98.92 sec
INFO:root:2024-04-26 13:37:10, Train, Epoch : 4, Step : 1880, Loss : 0.08635, Acc : 0.169, Sensitive_Loss : 0.01922, Sensitive_Acc : 2.800, Run Time : 2.97 sec
INFO:root:2024-04-26 13:37:19, Train, Epoch : 4, Step : 1890, Loss : 0.32392, Acc : 0.875, Sensitive_Loss : 0.10470, Sensitive_Acc : 15.400, Run Time : 8.11 sec
INFO:root:2024-04-26 13:37:27, Train, Epoch : 4, Step : 1900, Loss : 0.33482, Acc : 0.847, Sensitive_Loss : 0.16399, Sensitive_Acc : 17.800, Run Time : 8.08 sec
INFO:root:2024-04-26 13:39:00, Dev, Step : 1900, Loss : 0.42143, Acc : 0.823, Auc : 0.905, Sensitive_Loss : 0.15891, Sensitive_Acc : 16.721, Sensitive_Auc : 0.987, Mean auc: 0.905, Run Time : 93.62 sec
INFO:root:2024-04-26 13:39:06, Train, Epoch : 4, Step : 1910, Loss : 0.36796, Acc : 0.847, Sensitive_Loss : 0.12057, Sensitive_Acc : 16.100, Run Time : 99.79 sec
INFO:root:2024-04-26 13:39:15, Train, Epoch : 4, Step : 1920, Loss : 0.36378, Acc : 0.831, Sensitive_Loss : 0.09156, Sensitive_Acc : 16.800, Run Time : 8.69 sec
INFO:root:2024-04-26 13:39:25, Train, Epoch : 4, Step : 1930, Loss : 0.28851, Acc : 0.878, Sensitive_Loss : 0.15084, Sensitive_Acc : 15.700, Run Time : 9.81 sec
INFO:root:2024-04-26 13:39:35, Train, Epoch : 4, Step : 1940, Loss : 0.27693, Acc : 0.887, Sensitive_Loss : 0.13444, Sensitive_Acc : 16.100, Run Time : 9.75 sec
INFO:root:2024-04-26 13:39:43, Train, Epoch : 4, Step : 1950, Loss : 0.37072, Acc : 0.831, Sensitive_Loss : 0.11439, Sensitive_Acc : 17.200, Run Time : 7.85 sec
INFO:root:2024-04-26 13:39:50, Train, Epoch : 4, Step : 1960, Loss : 0.34635, Acc : 0.834, Sensitive_Loss : 0.18041, Sensitive_Acc : 15.900, Run Time : 7.88 sec
INFO:root:2024-04-26 13:39:59, Train, Epoch : 4, Step : 1970, Loss : 0.28985, Acc : 0.872, Sensitive_Loss : 0.12587, Sensitive_Acc : 17.600, Run Time : 8.47 sec
INFO:root:2024-04-26 13:40:07, Train, Epoch : 4, Step : 1980, Loss : 0.26924, Acc : 0.900, Sensitive_Loss : 0.15695, Sensitive_Acc : 15.600, Run Time : 8.28 sec
INFO:root:2024-04-26 13:40:16, Train, Epoch : 4, Step : 1990, Loss : 0.35691, Acc : 0.850, Sensitive_Loss : 0.15491, Sensitive_Acc : 17.300, Run Time : 8.61 sec
INFO:root:2024-04-26 13:40:25, Train, Epoch : 4, Step : 2000, Loss : 0.33291, Acc : 0.856, Sensitive_Loss : 0.11408, Sensitive_Acc : 17.200, Run Time : 8.76 sec
INFO:root:2024-04-26 13:41:58, Dev, Step : 2000, Loss : 0.41989, Acc : 0.825, Auc : 0.904, Sensitive_Loss : 0.14996, Sensitive_Acc : 16.779, Sensitive_Auc : 0.987, Mean auc: 0.904, Run Time : 93.32 sec
INFO:root:2024-04-26 13:42:04, Train, Epoch : 4, Step : 2010, Loss : 0.33344, Acc : 0.841, Sensitive_Loss : 0.11502, Sensitive_Acc : 16.300, Run Time : 98.97 sec
INFO:root:2024-04-26 13:42:14, Train, Epoch : 4, Step : 2020, Loss : 0.30047, Acc : 0.866, Sensitive_Loss : 0.12455, Sensitive_Acc : 17.500, Run Time : 10.19 sec
INFO:root:2024-04-26 13:42:22, Train, Epoch : 4, Step : 2030, Loss : 0.41345, Acc : 0.806, Sensitive_Loss : 0.12598, Sensitive_Acc : 16.000, Run Time : 8.44 sec
INFO:root:2024-04-26 13:42:31, Train, Epoch : 4, Step : 2040, Loss : 0.40668, Acc : 0.847, Sensitive_Loss : 0.12022, Sensitive_Acc : 16.800, Run Time : 8.46 sec
INFO:root:2024-04-26 13:42:39, Train, Epoch : 4, Step : 2050, Loss : 0.31486, Acc : 0.869, Sensitive_Loss : 0.13859, Sensitive_Acc : 15.900, Run Time : 8.66 sec
INFO:root:2024-04-26 13:42:49, Train, Epoch : 4, Step : 2060, Loss : 0.33905, Acc : 0.850, Sensitive_Loss : 0.09942, Sensitive_Acc : 16.800, Run Time : 9.52 sec
INFO:root:2024-04-26 13:42:59, Train, Epoch : 4, Step : 2070, Loss : 0.40410, Acc : 0.825, Sensitive_Loss : 0.13199, Sensitive_Acc : 17.700, Run Time : 9.78 sec
INFO:root:2024-04-26 13:43:07, Train, Epoch : 4, Step : 2080, Loss : 0.38382, Acc : 0.844, Sensitive_Loss : 0.14998, Sensitive_Acc : 16.600, Run Time : 8.19 sec
INFO:root:2024-04-26 13:43:15, Train, Epoch : 4, Step : 2090, Loss : 0.31180, Acc : 0.869, Sensitive_Loss : 0.10080, Sensitive_Acc : 16.700, Run Time : 8.55 sec
INFO:root:2024-04-26 13:43:24, Train, Epoch : 4, Step : 2100, Loss : 0.37974, Acc : 0.831, Sensitive_Loss : 0.12896, Sensitive_Acc : 16.000, Run Time : 8.82 sec
INFO:root:2024-04-26 13:44:59, Dev, Step : 2100, Loss : 0.41422, Acc : 0.825, Auc : 0.905, Sensitive_Loss : 0.15024, Sensitive_Acc : 16.736, Sensitive_Auc : 0.988, Mean auc: 0.905, Run Time : 95.26 sec
INFO:root:2024-04-26 13:45:05, Train, Epoch : 4, Step : 2110, Loss : 0.40270, Acc : 0.822, Sensitive_Loss : 0.11740, Sensitive_Acc : 14.900, Run Time : 100.77 sec
INFO:root:2024-04-26 13:45:13, Train, Epoch : 4, Step : 2120, Loss : 0.34636, Acc : 0.859, Sensitive_Loss : 0.11648, Sensitive_Acc : 16.400, Run Time : 8.55 sec
INFO:root:2024-04-26 13:45:22, Train, Epoch : 4, Step : 2130, Loss : 0.29443, Acc : 0.841, Sensitive_Loss : 0.15566, Sensitive_Acc : 17.200, Run Time : 8.46 sec
INFO:root:2024-04-26 13:45:30, Train, Epoch : 4, Step : 2140, Loss : 0.36656, Acc : 0.834, Sensitive_Loss : 0.12524, Sensitive_Acc : 16.000, Run Time : 8.14 sec
INFO:root:2024-04-26 13:45:38, Train, Epoch : 4, Step : 2150, Loss : 0.39823, Acc : 0.847, Sensitive_Loss : 0.12606, Sensitive_Acc : 17.200, Run Time : 8.45 sec
INFO:root:2024-04-26 13:45:47, Train, Epoch : 4, Step : 2160, Loss : 0.33249, Acc : 0.866, Sensitive_Loss : 0.11483, Sensitive_Acc : 16.400, Run Time : 8.41 sec
INFO:root:2024-04-26 13:45:55, Train, Epoch : 4, Step : 2170, Loss : 0.41682, Acc : 0.850, Sensitive_Loss : 0.09583, Sensitive_Acc : 16.300, Run Time : 8.16 sec
INFO:root:2024-04-26 13:46:04, Train, Epoch : 4, Step : 2180, Loss : 0.35064, Acc : 0.825, Sensitive_Loss : 0.14417, Sensitive_Acc : 17.700, Run Time : 8.65 sec
INFO:root:2024-04-26 13:46:12, Train, Epoch : 4, Step : 2190, Loss : 0.37248, Acc : 0.847, Sensitive_Loss : 0.16110, Sensitive_Acc : 15.600, Run Time : 8.35 sec
INFO:root:2024-04-26 13:46:20, Train, Epoch : 4, Step : 2200, Loss : 0.33091, Acc : 0.863, Sensitive_Loss : 0.12425, Sensitive_Acc : 17.400, Run Time : 8.01 sec
INFO:root:2024-04-26 13:47:53, Dev, Step : 2200, Loss : 0.41867, Acc : 0.822, Auc : 0.904, Sensitive_Loss : 0.16211, Sensitive_Acc : 16.893, Sensitive_Auc : 0.988, Mean auc: 0.904, Run Time : 93.16 sec
INFO:root:2024-04-26 13:48:00, Train, Epoch : 4, Step : 2210, Loss : 0.36591, Acc : 0.816, Sensitive_Loss : 0.12615, Sensitive_Acc : 16.500, Run Time : 99.51 sec
INFO:root:2024-04-26 13:48:08, Train, Epoch : 4, Step : 2220, Loss : 0.32841, Acc : 0.853, Sensitive_Loss : 0.12931, Sensitive_Acc : 17.100, Run Time : 8.42 sec
INFO:root:2024-04-26 13:48:16, Train, Epoch : 4, Step : 2230, Loss : 0.33376, Acc : 0.853, Sensitive_Loss : 0.10783, Sensitive_Acc : 16.100, Run Time : 8.04 sec
INFO:root:2024-04-26 13:48:26, Train, Epoch : 4, Step : 2240, Loss : 0.38640, Acc : 0.841, Sensitive_Loss : 0.14634, Sensitive_Acc : 15.100, Run Time : 9.64 sec
INFO:root:2024-04-26 13:48:35, Train, Epoch : 4, Step : 2250, Loss : 0.24199, Acc : 0.891, Sensitive_Loss : 0.10269, Sensitive_Acc : 15.900, Run Time : 8.86 sec
INFO:root:2024-04-26 13:48:43, Train, Epoch : 4, Step : 2260, Loss : 0.37242, Acc : 0.803, Sensitive_Loss : 0.14482, Sensitive_Acc : 16.200, Run Time : 8.38 sec
INFO:root:2024-04-26 13:48:51, Train, Epoch : 4, Step : 2270, Loss : 0.41065, Acc : 0.841, Sensitive_Loss : 0.12282, Sensitive_Acc : 16.500, Run Time : 8.17 sec
INFO:root:2024-04-26 13:48:59, Train, Epoch : 4, Step : 2280, Loss : 0.31886, Acc : 0.859, Sensitive_Loss : 0.14456, Sensitive_Acc : 16.100, Run Time : 8.18 sec
INFO:root:2024-04-26 13:49:08, Train, Epoch : 4, Step : 2290, Loss : 0.40695, Acc : 0.831, Sensitive_Loss : 0.11470, Sensitive_Acc : 17.300, Run Time : 8.32 sec
INFO:root:2024-04-26 13:49:16, Train, Epoch : 4, Step : 2300, Loss : 0.36575, Acc : 0.800, Sensitive_Loss : 0.13361, Sensitive_Acc : 16.200, Run Time : 8.24 sec
INFO:root:2024-04-26 13:50:49, Dev, Step : 2300, Loss : 0.42479, Acc : 0.820, Auc : 0.904, Sensitive_Loss : 0.15804, Sensitive_Acc : 16.864, Sensitive_Auc : 0.988, Mean auc: 0.904, Run Time : 93.64 sec
INFO:root:2024-04-26 13:50:55, Train, Epoch : 4, Step : 2310, Loss : 0.31274, Acc : 0.853, Sensitive_Loss : 0.14193, Sensitive_Acc : 17.200, Run Time : 98.93 sec
INFO:root:2024-04-26 13:51:03, Train, Epoch : 4, Step : 2320, Loss : 0.34967, Acc : 0.853, Sensitive_Loss : 0.11812, Sensitive_Acc : 16.000, Run Time : 7.80 sec
INFO:root:2024-04-26 13:51:11, Train, Epoch : 4, Step : 2330, Loss : 0.38609, Acc : 0.834, Sensitive_Loss : 0.11184, Sensitive_Acc : 17.500, Run Time : 8.29 sec
INFO:root:2024-04-26 13:51:20, Train, Epoch : 4, Step : 2340, Loss : 0.36350, Acc : 0.847, Sensitive_Loss : 0.16484, Sensitive_Acc : 16.000, Run Time : 8.89 sec
INFO:root:2024-04-26 13:51:28, Train, Epoch : 4, Step : 2350, Loss : 0.33718, Acc : 0.844, Sensitive_Loss : 0.09640, Sensitive_Acc : 16.500, Run Time : 8.22 sec
INFO:root:2024-04-26 13:51:36, Train, Epoch : 4, Step : 2360, Loss : 0.36961, Acc : 0.828, Sensitive_Loss : 0.10012, Sensitive_Acc : 15.300, Run Time : 7.94 sec
INFO:root:2024-04-26 13:51:44, Train, Epoch : 4, Step : 2370, Loss : 0.32852, Acc : 0.847, Sensitive_Loss : 0.09206, Sensitive_Acc : 15.100, Run Time : 8.30 sec
INFO:root:2024-04-26 13:51:53, Train, Epoch : 4, Step : 2380, Loss : 0.35895, Acc : 0.859, Sensitive_Loss : 0.12075, Sensitive_Acc : 17.200, Run Time : 8.36 sec
INFO:root:2024-04-26 13:52:01, Train, Epoch : 4, Step : 2390, Loss : 0.38275, Acc : 0.847, Sensitive_Loss : 0.17214, Sensitive_Acc : 16.600, Run Time : 8.56 sec
INFO:root:2024-04-26 13:52:09, Train, Epoch : 4, Step : 2400, Loss : 0.36587, Acc : 0.844, Sensitive_Loss : 0.08781, Sensitive_Acc : 16.900, Run Time : 8.27 sec
INFO:root:2024-04-26 13:53:43, Dev, Step : 2400, Loss : 0.42916, Acc : 0.821, Auc : 0.906, Sensitive_Loss : 0.14225, Sensitive_Acc : 16.864, Sensitive_Auc : 0.988, Mean auc: 0.906, Run Time : 93.53 sec
INFO:root:2024-04-26 13:53:44, Best, Step : 2400, Loss : 0.42916, Acc : 0.821, Auc : 0.906, Sensitive_Loss : 0.14225, Sensitive_Acc : 16.864, Sensitive_Auc : 0.988, Best Auc : 0.906
INFO:root:2024-04-26 13:53:50, Train, Epoch : 4, Step : 2410, Loss : 0.35498, Acc : 0.847, Sensitive_Loss : 0.09776, Sensitive_Acc : 17.100, Run Time : 100.12 sec
INFO:root:2024-04-26 13:53:58, Train, Epoch : 4, Step : 2420, Loss : 0.34404, Acc : 0.853, Sensitive_Loss : 0.07943, Sensitive_Acc : 15.900, Run Time : 8.70 sec
INFO:root:2024-04-26 13:54:07, Train, Epoch : 4, Step : 2430, Loss : 0.37866, Acc : 0.838, Sensitive_Loss : 0.14726, Sensitive_Acc : 15.200, Run Time : 8.84 sec
INFO:root:2024-04-26 13:54:16, Train, Epoch : 4, Step : 2440, Loss : 0.38348, Acc : 0.812, Sensitive_Loss : 0.17326, Sensitive_Acc : 16.100, Run Time : 8.76 sec
INFO:root:2024-04-26 13:54:24, Train, Epoch : 4, Step : 2450, Loss : 0.37847, Acc : 0.834, Sensitive_Loss : 0.09772, Sensitive_Acc : 16.700, Run Time : 8.34 sec
INFO:root:2024-04-26 13:54:33, Train, Epoch : 4, Step : 2460, Loss : 0.36513, Acc : 0.834, Sensitive_Loss : 0.17550, Sensitive_Acc : 15.700, Run Time : 8.64 sec
INFO:root:2024-04-26 13:54:42, Train, Epoch : 4, Step : 2470, Loss : 0.36176, Acc : 0.869, Sensitive_Loss : 0.09210, Sensitive_Acc : 15.500, Run Time : 8.94 sec
INFO:root:2024-04-26 13:54:51, Train, Epoch : 4, Step : 2480, Loss : 0.35714, Acc : 0.841, Sensitive_Loss : 0.12723, Sensitive_Acc : 17.700, Run Time : 8.94 sec
INFO:root:2024-04-26 13:55:00, Train, Epoch : 4, Step : 2490, Loss : 0.43774, Acc : 0.797, Sensitive_Loss : 0.17166, Sensitive_Acc : 15.000, Run Time : 8.90 sec
INFO:root:2024-04-26 13:55:08, Train, Epoch : 4, Step : 2500, Loss : 0.39162, Acc : 0.831, Sensitive_Loss : 0.17520, Sensitive_Acc : 16.100, Run Time : 8.14 sec
INFO:root:2024-04-26 13:56:41, Dev, Step : 2500, Loss : 0.42817, Acc : 0.822, Auc : 0.905, Sensitive_Loss : 0.14288, Sensitive_Acc : 16.864, Sensitive_Auc : 0.988, Mean auc: 0.905, Run Time : 93.52 sec
INFO:root:2024-04-26 13:58:15
INFO:root:y_pred: [0.15056358 0.91868144 0.08545239 ... 0.7709134  0.00783826 0.79479325]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.6877074e-01 2.3828553e-04 2.6105154e-01 1.1840888e-06 9.9460489e-01
 5.1979760e-07 9.9469888e-01 9.8214507e-01 3.6814213e-03 7.4213439e-01
 9.8596978e-01 9.9704665e-01 9.9359304e-01 9.4797051e-01 1.0784145e-02
 9.3003058e-01 9.9856085e-01 4.2507439e-03 7.5532109e-01 9.9248612e-01
 9.6825123e-01 1.2221946e-01 9.9774253e-01 9.4039905e-01 9.9257141e-01
 7.2290230e-01 4.6925875e-06 9.8748851e-01 9.9008536e-01 6.2161046e-01
 1.4002590e-03 2.0848855e-01 7.7504164e-04 7.6135271e-04 4.5889132e-02
 2.1714151e-02 6.4131252e-02 1.3455240e-03 9.8135734e-01 9.6813476e-01
 2.0779424e-08 3.6078673e-05 9.4021517e-01 5.7428310e-06 9.9805909e-01
 9.8962760e-01 9.8024279e-01 9.9533588e-01 8.1103564e-05 9.8780513e-01
 9.9795121e-01 6.0702711e-03 6.0532796e-01 2.2421962e-05 5.2268801e-06
 2.7333340e-03 2.3861909e-03 8.4834015e-03 2.5737475e-04 4.8694155e-01
 2.8920728e-03 2.2366172e-01 7.0057045e-05 9.7713590e-01 2.0191194e-02
 9.9653339e-01 5.7835568e-06 9.9112296e-01 8.7666458e-01 5.6866252e-01
 9.3805665e-01 6.4720047e-01 5.6543923e-04 2.9353285e-03 1.6703584e-03
 1.3498634e-06 8.5107376e-06 1.8921977e-01 2.1048432e-05 9.8393261e-01
 9.9640757e-01 9.5009258e-05 7.6473880e-01 2.8642887e-04 9.4215226e-01
 3.3735064e-01 1.7978322e-05 2.6923609e-01 9.3464315e-01 9.7553533e-01
 9.8950887e-01 7.7773228e-02 3.0415552e-02 9.9629384e-01 2.3594702e-02
 1.7575722e-02 9.9329752e-01 9.8689538e-01 6.2461049e-06 3.4025166e-02
 9.7301584e-01 9.5624572e-01 9.9501413e-01 9.8882860e-01 1.9148126e-03
 3.4025270e-01 7.2500640e-01 9.1315860e-01 8.7081760e-01 1.7241530e-06
 8.4461099e-01 9.9166965e-01 3.0253085e-03 9.9615932e-01 9.3415177e-01
 9.8350269e-01 9.0902668e-01 9.9410087e-01 6.5804124e-05 5.9604578e-02
 9.9571890e-01 9.8920828e-01 1.2246537e-04 8.5765940e-01 9.8976821e-01
 9.1701202e-02 9.9058646e-01 2.5350438e-02 9.2829643e-05 9.9469358e-01
 9.9574357e-01 5.6496786e-04 1.5942885e-04 3.6160303e-03 9.9153179e-01
 9.9909270e-01 9.7007501e-01 3.7042140e-03 1.5719015e-03 9.7317976e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 13:58:15, Dev, Step : 2504, Loss : 0.43119, Acc : 0.822, Auc : 0.905, Sensitive_Loss : 0.14481, Sensitive_Acc : 16.864, Sensitive_Auc : 0.989, Mean auc: 0.905, Run Time : 92.59 sec
INFO:root:2024-04-26 13:58:22, Train, Epoch : 5, Step : 2510, Loss : 0.21957, Acc : 0.509, Sensitive_Loss : 0.03891, Sensitive_Acc : 9.500, Run Time : 5.71 sec
INFO:root:2024-04-26 13:58:29, Train, Epoch : 5, Step : 2520, Loss : 0.38408, Acc : 0.847, Sensitive_Loss : 0.13475, Sensitive_Acc : 15.600, Run Time : 7.20 sec
INFO:root:2024-04-26 13:58:37, Train, Epoch : 5, Step : 2530, Loss : 0.37336, Acc : 0.863, Sensitive_Loss : 0.11912, Sensitive_Acc : 15.900, Run Time : 7.79 sec
INFO:root:2024-04-26 13:58:44, Train, Epoch : 5, Step : 2540, Loss : 0.34456, Acc : 0.853, Sensitive_Loss : 0.12270, Sensitive_Acc : 16.300, Run Time : 7.32 sec
INFO:root:2024-04-26 13:58:51, Train, Epoch : 5, Step : 2550, Loss : 0.37313, Acc : 0.831, Sensitive_Loss : 0.11980, Sensitive_Acc : 16.200, Run Time : 6.74 sec
INFO:root:2024-04-26 13:58:58, Train, Epoch : 5, Step : 2560, Loss : 0.29684, Acc : 0.834, Sensitive_Loss : 0.16444, Sensitive_Acc : 17.000, Run Time : 7.17 sec
INFO:root:2024-04-26 13:59:05, Train, Epoch : 5, Step : 2570, Loss : 0.32682, Acc : 0.866, Sensitive_Loss : 0.12336, Sensitive_Acc : 17.600, Run Time : 7.20 sec
INFO:root:2024-04-26 13:59:12, Train, Epoch : 5, Step : 2580, Loss : 0.35995, Acc : 0.841, Sensitive_Loss : 0.11376, Sensitive_Acc : 16.000, Run Time : 6.43 sec
INFO:root:2024-04-26 13:59:20, Train, Epoch : 5, Step : 2590, Loss : 0.32384, Acc : 0.897, Sensitive_Loss : 0.13406, Sensitive_Acc : 17.900, Run Time : 8.67 sec
INFO:root:2024-04-26 13:59:29, Train, Epoch : 5, Step : 2600, Loss : 0.31314, Acc : 0.878, Sensitive_Loss : 0.11989, Sensitive_Acc : 15.600, Run Time : 8.99 sec
INFO:root:2024-04-26 14:01:08, Dev, Step : 2600, Loss : 0.42352, Acc : 0.819, Auc : 0.905, Sensitive_Loss : 0.14485, Sensitive_Acc : 16.850, Sensitive_Auc : 0.989, Mean auc: 0.905, Run Time : 99.08 sec
INFO:root:2024-04-26 14:01:19, Train, Epoch : 5, Step : 2610, Loss : 0.40133, Acc : 0.863, Sensitive_Loss : 0.12810, Sensitive_Acc : 16.500, Run Time : 110.05 sec
INFO:root:2024-04-26 14:01:33, Train, Epoch : 5, Step : 2620, Loss : 0.35072, Acc : 0.847, Sensitive_Loss : 0.14745, Sensitive_Acc : 16.000, Run Time : 13.57 sec
INFO:root:2024-04-26 14:01:45, Train, Epoch : 5, Step : 2630, Loss : 0.31132, Acc : 0.844, Sensitive_Loss : 0.12398, Sensitive_Acc : 15.500, Run Time : 11.71 sec
INFO:root:2024-04-26 14:01:55, Train, Epoch : 5, Step : 2640, Loss : 0.36039, Acc : 0.825, Sensitive_Loss : 0.12232, Sensitive_Acc : 15.300, Run Time : 10.13 sec
INFO:root:2024-04-26 14:02:06, Train, Epoch : 5, Step : 2650, Loss : 0.34035, Acc : 0.866, Sensitive_Loss : 0.12354, Sensitive_Acc : 17.400, Run Time : 10.92 sec
INFO:root:2024-04-26 14:02:17, Train, Epoch : 5, Step : 2660, Loss : 0.36100, Acc : 0.844, Sensitive_Loss : 0.09584, Sensitive_Acc : 16.500, Run Time : 11.28 sec
INFO:root:2024-04-26 14:02:26, Train, Epoch : 5, Step : 2670, Loss : 0.29809, Acc : 0.887, Sensitive_Loss : 0.14586, Sensitive_Acc : 16.900, Run Time : 9.62 sec
INFO:root:2024-04-26 14:02:37, Train, Epoch : 5, Step : 2680, Loss : 0.37263, Acc : 0.831, Sensitive_Loss : 0.10544, Sensitive_Acc : 15.800, Run Time : 10.84 sec
INFO:root:2024-04-26 14:02:48, Train, Epoch : 5, Step : 2690, Loss : 0.28610, Acc : 0.881, Sensitive_Loss : 0.09433, Sensitive_Acc : 15.900, Run Time : 10.35 sec
INFO:root:2024-04-26 14:02:57, Train, Epoch : 5, Step : 2700, Loss : 0.32906, Acc : 0.856, Sensitive_Loss : 0.11507, Sensitive_Acc : 15.900, Run Time : 9.23 sec
INFO:root:2024-04-26 14:04:46, Dev, Step : 2700, Loss : 0.44194, Acc : 0.815, Auc : 0.904, Sensitive_Loss : 0.14864, Sensitive_Acc : 16.864, Sensitive_Auc : 0.987, Mean auc: 0.904, Run Time : 108.82 sec
INFO:root:2024-04-26 14:04:52, Train, Epoch : 5, Step : 2710, Loss : 0.34900, Acc : 0.856, Sensitive_Loss : 0.06977, Sensitive_Acc : 16.100, Run Time : 115.36 sec
INFO:root:2024-04-26 14:05:02, Train, Epoch : 5, Step : 2720, Loss : 0.38050, Acc : 0.853, Sensitive_Loss : 0.10199, Sensitive_Acc : 14.600, Run Time : 9.40 sec
INFO:root:2024-04-26 14:05:11, Train, Epoch : 5, Step : 2730, Loss : 0.34033, Acc : 0.834, Sensitive_Loss : 0.09976, Sensitive_Acc : 16.900, Run Time : 9.27 sec
INFO:root:2024-04-26 14:05:20, Train, Epoch : 5, Step : 2740, Loss : 0.31239, Acc : 0.869, Sensitive_Loss : 0.15181, Sensitive_Acc : 16.700, Run Time : 9.18 sec
INFO:root:2024-04-26 14:05:34, Train, Epoch : 5, Step : 2750, Loss : 0.30223, Acc : 0.859, Sensitive_Loss : 0.14744, Sensitive_Acc : 17.700, Run Time : 13.41 sec
INFO:root:2024-04-26 14:05:43, Train, Epoch : 5, Step : 2760, Loss : 0.32817, Acc : 0.850, Sensitive_Loss : 0.08585, Sensitive_Acc : 16.700, Run Time : 9.48 sec
INFO:root:2024-04-26 14:05:51, Train, Epoch : 5, Step : 2770, Loss : 0.32624, Acc : 0.850, Sensitive_Loss : 0.10358, Sensitive_Acc : 15.900, Run Time : 8.43 sec
INFO:root:2024-04-26 14:06:01, Train, Epoch : 5, Step : 2780, Loss : 0.36200, Acc : 0.841, Sensitive_Loss : 0.10450, Sensitive_Acc : 17.100, Run Time : 9.40 sec
INFO:root:2024-04-26 14:06:10, Train, Epoch : 5, Step : 2790, Loss : 0.27993, Acc : 0.872, Sensitive_Loss : 0.11227, Sensitive_Acc : 16.700, Run Time : 9.11 sec
INFO:root:2024-04-26 14:06:18, Train, Epoch : 5, Step : 2800, Loss : 0.28795, Acc : 0.891, Sensitive_Loss : 0.10295, Sensitive_Acc : 16.000, Run Time : 8.12 sec
INFO:root:2024-04-26 14:07:53, Dev, Step : 2800, Loss : 0.42334, Acc : 0.826, Auc : 0.903, Sensitive_Loss : 0.14298, Sensitive_Acc : 16.850, Sensitive_Auc : 0.988, Mean auc: 0.903, Run Time : 95.04 sec
INFO:root:2024-04-26 14:08:00, Train, Epoch : 5, Step : 2810, Loss : 0.35258, Acc : 0.847, Sensitive_Loss : 0.12970, Sensitive_Acc : 17.800, Run Time : 101.64 sec
INFO:root:2024-04-26 14:08:10, Train, Epoch : 5, Step : 2820, Loss : 0.33921, Acc : 0.841, Sensitive_Loss : 0.14673, Sensitive_Acc : 17.300, Run Time : 10.01 sec
INFO:root:2024-04-26 14:08:20, Train, Epoch : 5, Step : 2830, Loss : 0.31653, Acc : 0.863, Sensitive_Loss : 0.15935, Sensitive_Acc : 15.500, Run Time : 9.96 sec
INFO:root:2024-04-26 14:08:29, Train, Epoch : 5, Step : 2840, Loss : 0.33663, Acc : 0.866, Sensitive_Loss : 0.14607, Sensitive_Acc : 15.100, Run Time : 9.76 sec
INFO:root:2024-04-26 14:08:40, Train, Epoch : 5, Step : 2850, Loss : 0.30068, Acc : 0.891, Sensitive_Loss : 0.09881, Sensitive_Acc : 14.900, Run Time : 10.44 sec
INFO:root:2024-04-26 14:08:50, Train, Epoch : 5, Step : 2860, Loss : 0.31842, Acc : 0.875, Sensitive_Loss : 0.12898, Sensitive_Acc : 17.400, Run Time : 10.42 sec
INFO:root:2024-04-26 14:09:01, Train, Epoch : 5, Step : 2870, Loss : 0.33614, Acc : 0.866, Sensitive_Loss : 0.12132, Sensitive_Acc : 16.100, Run Time : 11.01 sec
INFO:root:2024-04-26 14:09:10, Train, Epoch : 5, Step : 2880, Loss : 0.36510, Acc : 0.866, Sensitive_Loss : 0.10747, Sensitive_Acc : 15.700, Run Time : 8.92 sec
INFO:root:2024-04-26 14:09:19, Train, Epoch : 5, Step : 2890, Loss : 0.31593, Acc : 0.856, Sensitive_Loss : 0.10529, Sensitive_Acc : 16.800, Run Time : 9.21 sec
INFO:root:2024-04-26 14:09:31, Train, Epoch : 5, Step : 2900, Loss : 0.33999, Acc : 0.856, Sensitive_Loss : 0.13932, Sensitive_Acc : 16.100, Run Time : 11.32 sec
INFO:root:2024-04-26 14:11:05, Dev, Step : 2900, Loss : 0.44192, Acc : 0.818, Auc : 0.905, Sensitive_Loss : 0.13394, Sensitive_Acc : 16.907, Sensitive_Auc : 0.990, Mean auc: 0.905, Run Time : 93.72 sec
INFO:root:2024-04-26 14:11:11, Train, Epoch : 5, Step : 2910, Loss : 0.32193, Acc : 0.859, Sensitive_Loss : 0.06757, Sensitive_Acc : 17.900, Run Time : 100.64 sec
INFO:root:2024-04-26 14:11:21, Train, Epoch : 5, Step : 2920, Loss : 0.41730, Acc : 0.841, Sensitive_Loss : 0.11770, Sensitive_Acc : 16.000, Run Time : 9.33 sec
INFO:root:2024-04-26 14:11:30, Train, Epoch : 5, Step : 2930, Loss : 0.29052, Acc : 0.900, Sensitive_Loss : 0.10810, Sensitive_Acc : 15.000, Run Time : 9.00 sec
INFO:root:2024-04-26 14:11:41, Train, Epoch : 5, Step : 2940, Loss : 0.26289, Acc : 0.875, Sensitive_Loss : 0.11031, Sensitive_Acc : 16.800, Run Time : 11.59 sec
INFO:root:2024-04-26 14:11:52, Train, Epoch : 5, Step : 2950, Loss : 0.32007, Acc : 0.853, Sensitive_Loss : 0.09803, Sensitive_Acc : 15.600, Run Time : 10.94 sec
INFO:root:2024-04-26 14:12:01, Train, Epoch : 5, Step : 2960, Loss : 0.29840, Acc : 0.859, Sensitive_Loss : 0.11791, Sensitive_Acc : 16.100, Run Time : 8.83 sec
INFO:root:2024-04-26 14:12:10, Train, Epoch : 5, Step : 2970, Loss : 0.38560, Acc : 0.838, Sensitive_Loss : 0.13857, Sensitive_Acc : 15.400, Run Time : 9.10 sec
INFO:root:2024-04-26 14:12:19, Train, Epoch : 5, Step : 2980, Loss : 0.29259, Acc : 0.891, Sensitive_Loss : 0.10277, Sensitive_Acc : 16.400, Run Time : 9.18 sec
INFO:root:2024-04-26 14:12:29, Train, Epoch : 5, Step : 2990, Loss : 0.35057, Acc : 0.841, Sensitive_Loss : 0.09600, Sensitive_Acc : 15.700, Run Time : 9.14 sec
INFO:root:2024-04-26 14:12:37, Train, Epoch : 5, Step : 3000, Loss : 0.36435, Acc : 0.838, Sensitive_Loss : 0.11340, Sensitive_Acc : 15.500, Run Time : 8.87 sec
INFO:root:2024-04-26 14:14:10, Dev, Step : 3000, Loss : 0.42927, Acc : 0.823, Auc : 0.905, Sensitive_Loss : 0.13417, Sensitive_Acc : 16.864, Sensitive_Auc : 0.988, Mean auc: 0.905, Run Time : 92.96 sec
INFO:root:2024-04-26 14:14:17, Train, Epoch : 5, Step : 3010, Loss : 0.39623, Acc : 0.812, Sensitive_Loss : 0.10696, Sensitive_Acc : 16.200, Run Time : 99.17 sec
INFO:root:2024-04-26 14:14:26, Train, Epoch : 5, Step : 3020, Loss : 0.30841, Acc : 0.869, Sensitive_Loss : 0.08543, Sensitive_Acc : 17.300, Run Time : 9.54 sec
INFO:root:2024-04-26 14:14:36, Train, Epoch : 5, Step : 3030, Loss : 0.34477, Acc : 0.844, Sensitive_Loss : 0.07393, Sensitive_Acc : 17.000, Run Time : 10.34 sec
INFO:root:2024-04-26 14:14:51, Train, Epoch : 5, Step : 3040, Loss : 0.36605, Acc : 0.812, Sensitive_Loss : 0.14299, Sensitive_Acc : 16.000, Run Time : 14.86 sec
INFO:root:2024-04-26 14:15:01, Train, Epoch : 5, Step : 3050, Loss : 0.30336, Acc : 0.881, Sensitive_Loss : 0.12514, Sensitive_Acc : 15.900, Run Time : 9.60 sec
INFO:root:2024-04-26 14:15:11, Train, Epoch : 5, Step : 3060, Loss : 0.35522, Acc : 0.872, Sensitive_Loss : 0.09685, Sensitive_Acc : 14.800, Run Time : 9.75 sec
INFO:root:2024-04-26 14:15:20, Train, Epoch : 5, Step : 3070, Loss : 0.32218, Acc : 0.828, Sensitive_Loss : 0.14803, Sensitive_Acc : 17.200, Run Time : 9.82 sec
INFO:root:2024-04-26 14:15:30, Train, Epoch : 5, Step : 3080, Loss : 0.33380, Acc : 0.878, Sensitive_Loss : 0.13391, Sensitive_Acc : 15.000, Run Time : 9.73 sec
INFO:root:2024-04-26 14:15:40, Train, Epoch : 5, Step : 3090, Loss : 0.33580, Acc : 0.859, Sensitive_Loss : 0.09525, Sensitive_Acc : 17.700, Run Time : 9.58 sec
INFO:root:2024-04-26 14:15:49, Train, Epoch : 5, Step : 3100, Loss : 0.31233, Acc : 0.872, Sensitive_Loss : 0.08350, Sensitive_Acc : 16.800, Run Time : 9.64 sec
INFO:root:2024-04-26 14:17:23, Dev, Step : 3100, Loss : 0.41889, Acc : 0.824, Auc : 0.907, Sensitive_Loss : 0.12705, Sensitive_Acc : 16.807, Sensitive_Auc : 0.990, Mean auc: 0.907, Run Time : 94.04 sec
INFO:root:2024-04-26 14:17:24, Best, Step : 3100, Loss : 0.41889, Acc : 0.824, Auc : 0.907, Sensitive_Loss : 0.12705, Sensitive_Acc : 16.807, Sensitive_Auc : 0.990, Best Auc : 0.907
INFO:root:2024-04-26 14:17:30, Train, Epoch : 5, Step : 3110, Loss : 0.34801, Acc : 0.844, Sensitive_Loss : 0.11546, Sensitive_Acc : 15.800, Run Time : 101.06 sec
INFO:root:2024-04-26 14:17:41, Train, Epoch : 5, Step : 3120, Loss : 0.34935, Acc : 0.856, Sensitive_Loss : 0.07323, Sensitive_Acc : 15.800, Run Time : 10.64 sec
INFO:root:2024-04-26 14:17:51, Train, Epoch : 5, Step : 3130, Loss : 0.35158, Acc : 0.834, Sensitive_Loss : 0.15003, Sensitive_Acc : 16.100, Run Time : 10.32 sec
INFO:root:2024-04-26 14:19:24
INFO:root:y_pred: [0.13446362 0.9098737  0.08154177 ... 0.757199   0.00756083 0.8273641 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.70476091e-01 5.65734110e-04 6.60086423e-02 2.65208860e-06
 9.91574347e-01 2.02271025e-07 9.92365539e-01 9.86076832e-01
 3.18605406e-03 8.56230736e-01 9.91788805e-01 9.96482253e-01
 9.92149830e-01 9.41325068e-01 2.18616799e-02 9.11640823e-01
 9.98798013e-01 3.25964415e-03 8.20745647e-01 9.95330989e-01
 9.71298158e-01 5.73094236e-03 9.97421026e-01 9.05161142e-01
 9.90995467e-01 7.63006151e-01 8.33814170e-07 9.82918978e-01
 9.87791300e-01 7.76520789e-01 3.50925326e-03 7.90036917e-02
 8.56815779e-04 7.90093734e-04 2.49091715e-01 3.38950125e-03
 8.04679468e-03 8.58501706e-04 9.84541416e-01 9.63308275e-01
 1.66158536e-08 3.44616070e-04 9.67030227e-01 3.10273790e-05
 9.97954607e-01 9.93272007e-01 9.86147463e-01 9.94455516e-01
 7.66354860e-05 9.88431692e-01 9.98255432e-01 5.10017853e-03
 7.87928760e-01 4.81934621e-05 1.50096207e-06 4.04382311e-03
 1.07936597e-04 4.54613417e-02 3.82644539e-05 1.72035158e-01
 7.13842083e-03 3.16060275e-01 2.50534758e-05 9.91425514e-01
 2.34185648e-03 9.96120751e-01 4.57959366e-04 9.92312372e-01
 8.56422246e-01 6.13272667e-01 9.73398805e-01 6.55452073e-01
 2.46857689e-03 1.74565974e-03 2.12450352e-04 8.92438038e-07
 3.75387230e-04 2.82278419e-01 8.41109704e-06 9.84785438e-01
 9.94213521e-01 3.08225935e-05 6.54008865e-01 4.87844045e-05
 9.61965442e-01 6.36934102e-01 4.52389213e-06 9.53720808e-02
 9.27631855e-01 9.81283188e-01 9.85364854e-01 7.64828771e-02
 6.43907906e-03 9.95224237e-01 7.50154897e-04 4.88447398e-03
 9.91516352e-01 9.80523050e-01 1.17827894e-05 8.24801549e-02
 9.63771582e-01 9.61043239e-01 9.97709870e-01 9.90849972e-01
 6.12328411e-04 3.39282677e-02 7.36234903e-01 9.22683239e-01
 8.10375154e-01 1.48304880e-05 8.90018046e-01 9.93652701e-01
 3.67932511e-03 9.96184170e-01 9.42575157e-01 9.85470891e-01
 8.79682481e-01 9.93548810e-01 1.25232618e-04 2.07478911e-01
 9.95995283e-01 9.84510899e-01 2.55293835e-05 8.69691908e-01
 9.91114378e-01 5.76655157e-02 9.86526787e-01 1.36161037e-02
 2.39047669e-02 9.88893688e-01 9.92155731e-01 3.95544834e-04
 6.69732981e-04 1.40947895e-02 9.88182843e-01 9.98927653e-01
 9.73687708e-01 5.36387553e-03 1.25610107e-03 9.82157350e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 14:19:24, Dev, Step : 3130, Loss : 0.42039, Acc : 0.822, Auc : 0.907, Sensitive_Loss : 0.13101, Sensitive_Acc : 16.850, Sensitive_Auc : 0.989, Mean auc: 0.907, Run Time : 91.90 sec
INFO:root:2024-04-26 14:19:33, Train, Epoch : 6, Step : 3140, Loss : 0.32662, Acc : 0.838, Sensitive_Loss : 0.11164, Sensitive_Acc : 16.600, Run Time : 8.53 sec
INFO:root:2024-04-26 14:19:41, Train, Epoch : 6, Step : 3150, Loss : 0.34136, Acc : 0.841, Sensitive_Loss : 0.14545, Sensitive_Acc : 15.600, Run Time : 7.79 sec
INFO:root:2024-04-26 14:19:49, Train, Epoch : 6, Step : 3160, Loss : 0.36936, Acc : 0.863, Sensitive_Loss : 0.16216, Sensitive_Acc : 15.400, Run Time : 7.55 sec
INFO:root:2024-04-26 14:19:58, Train, Epoch : 6, Step : 3170, Loss : 0.34950, Acc : 0.850, Sensitive_Loss : 0.11576, Sensitive_Acc : 16.000, Run Time : 9.16 sec
INFO:root:2024-04-26 14:20:06, Train, Epoch : 6, Step : 3180, Loss : 0.31530, Acc : 0.859, Sensitive_Loss : 0.10787, Sensitive_Acc : 15.200, Run Time : 8.54 sec
INFO:root:2024-04-26 14:20:14, Train, Epoch : 6, Step : 3190, Loss : 0.36279, Acc : 0.856, Sensitive_Loss : 0.09276, Sensitive_Acc : 16.300, Run Time : 7.93 sec
INFO:root:2024-04-26 14:20:22, Train, Epoch : 6, Step : 3200, Loss : 0.32993, Acc : 0.878, Sensitive_Loss : 0.10050, Sensitive_Acc : 17.600, Run Time : 7.71 sec
INFO:root:2024-04-26 14:21:55, Dev, Step : 3200, Loss : 0.43457, Acc : 0.822, Auc : 0.904, Sensitive_Loss : 0.14071, Sensitive_Acc : 16.850, Sensitive_Auc : 0.988, Mean auc: 0.904, Run Time : 92.99 sec
INFO:root:2024-04-26 14:22:02, Train, Epoch : 6, Step : 3210, Loss : 0.28819, Acc : 0.869, Sensitive_Loss : 0.13492, Sensitive_Acc : 15.100, Run Time : 100.10 sec
INFO:root:2024-04-26 14:22:15, Train, Epoch : 6, Step : 3220, Loss : 0.29064, Acc : 0.897, Sensitive_Loss : 0.12450, Sensitive_Acc : 17.400, Run Time : 12.53 sec
INFO:root:2024-04-26 14:22:27, Train, Epoch : 6, Step : 3230, Loss : 0.35025, Acc : 0.859, Sensitive_Loss : 0.08579, Sensitive_Acc : 16.400, Run Time : 12.15 sec
INFO:root:2024-04-26 14:22:39, Train, Epoch : 6, Step : 3240, Loss : 0.34539, Acc : 0.838, Sensitive_Loss : 0.12567, Sensitive_Acc : 16.400, Run Time : 12.42 sec
INFO:root:2024-04-26 14:22:48, Train, Epoch : 6, Step : 3250, Loss : 0.29203, Acc : 0.887, Sensitive_Loss : 0.16119, Sensitive_Acc : 16.800, Run Time : 9.07 sec
INFO:root:2024-04-26 14:23:00, Train, Epoch : 6, Step : 3260, Loss : 0.31238, Acc : 0.866, Sensitive_Loss : 0.12574, Sensitive_Acc : 15.700, Run Time : 11.52 sec
INFO:root:2024-04-26 14:23:09, Train, Epoch : 6, Step : 3270, Loss : 0.32617, Acc : 0.878, Sensitive_Loss : 0.10075, Sensitive_Acc : 16.300, Run Time : 9.31 sec
INFO:root:2024-04-26 14:23:17, Train, Epoch : 6, Step : 3280, Loss : 0.28749, Acc : 0.891, Sensitive_Loss : 0.11861, Sensitive_Acc : 18.700, Run Time : 7.72 sec
INFO:root:2024-04-26 14:23:25, Train, Epoch : 6, Step : 3290, Loss : 0.32360, Acc : 0.878, Sensitive_Loss : 0.08500, Sensitive_Acc : 17.600, Run Time : 8.25 sec
INFO:root:2024-04-26 14:23:36, Train, Epoch : 6, Step : 3300, Loss : 0.33913, Acc : 0.859, Sensitive_Loss : 0.09780, Sensitive_Acc : 15.900, Run Time : 11.31 sec
INFO:root:2024-04-26 14:25:10, Dev, Step : 3300, Loss : 0.42803, Acc : 0.822, Auc : 0.901, Sensitive_Loss : 0.13713, Sensitive_Acc : 16.864, Sensitive_Auc : 0.990, Mean auc: 0.901, Run Time : 93.33 sec
INFO:root:2024-04-26 14:25:15, Train, Epoch : 6, Step : 3310, Loss : 0.31754, Acc : 0.847, Sensitive_Loss : 0.11658, Sensitive_Acc : 16.300, Run Time : 98.49 sec
INFO:root:2024-04-26 14:25:24, Train, Epoch : 6, Step : 3320, Loss : 0.29572, Acc : 0.878, Sensitive_Loss : 0.11256, Sensitive_Acc : 16.000, Run Time : 9.53 sec
INFO:root:2024-04-26 14:25:33, Train, Epoch : 6, Step : 3330, Loss : 0.31987, Acc : 0.831, Sensitive_Loss : 0.11201, Sensitive_Acc : 15.800, Run Time : 8.35 sec
INFO:root:2024-04-26 14:25:41, Train, Epoch : 6, Step : 3340, Loss : 0.34737, Acc : 0.856, Sensitive_Loss : 0.14597, Sensitive_Acc : 15.700, Run Time : 8.32 sec
INFO:root:2024-04-26 14:25:50, Train, Epoch : 6, Step : 3350, Loss : 0.32091, Acc : 0.866, Sensitive_Loss : 0.14337, Sensitive_Acc : 15.400, Run Time : 8.41 sec
INFO:root:2024-04-26 14:25:58, Train, Epoch : 6, Step : 3360, Loss : 0.30362, Acc : 0.850, Sensitive_Loss : 0.14154, Sensitive_Acc : 15.800, Run Time : 8.34 sec
INFO:root:2024-04-26 14:26:06, Train, Epoch : 6, Step : 3370, Loss : 0.34590, Acc : 0.863, Sensitive_Loss : 0.12583, Sensitive_Acc : 17.100, Run Time : 8.31 sec
INFO:root:2024-04-26 14:26:13, Train, Epoch : 6, Step : 3380, Loss : 0.34094, Acc : 0.872, Sensitive_Loss : 0.11098, Sensitive_Acc : 15.800, Run Time : 7.24 sec
INFO:root:2024-04-26 14:26:22, Train, Epoch : 6, Step : 3390, Loss : 0.30598, Acc : 0.872, Sensitive_Loss : 0.08940, Sensitive_Acc : 16.600, Run Time : 8.09 sec
INFO:root:2024-04-26 14:26:30, Train, Epoch : 6, Step : 3400, Loss : 0.33412, Acc : 0.844, Sensitive_Loss : 0.12611, Sensitive_Acc : 17.000, Run Time : 8.37 sec
INFO:root:2024-04-26 14:32:02, Dev, Step : 3400, Loss : 0.43201, Acc : 0.822, Auc : 0.905, Sensitive_Loss : 0.13211, Sensitive_Acc : 16.864, Sensitive_Auc : 0.990, Mean auc: 0.905, Run Time : 331.63 sec
INFO:root:2024-04-26 14:32:09, Train, Epoch : 6, Step : 3410, Loss : 0.28565, Acc : 0.884, Sensitive_Loss : 0.12633, Sensitive_Acc : 15.100, Run Time : 339.50 sec
INFO:root:2024-04-26 14:32:18, Train, Epoch : 6, Step : 3420, Loss : 0.30582, Acc : 0.847, Sensitive_Loss : 0.12519, Sensitive_Acc : 16.600, Run Time : 9.07 sec
INFO:root:2024-04-26 14:32:32, Train, Epoch : 6, Step : 3430, Loss : 0.29377, Acc : 0.856, Sensitive_Loss : 0.06892, Sensitive_Acc : 17.000, Run Time : 14.01 sec
INFO:root:2024-04-26 14:32:51, Train, Epoch : 6, Step : 3440, Loss : 0.27218, Acc : 0.872, Sensitive_Loss : 0.11851, Sensitive_Acc : 16.100, Run Time : 18.96 sec
INFO:root:2024-04-26 14:33:08, Train, Epoch : 6, Step : 3450, Loss : 0.29788, Acc : 0.859, Sensitive_Loss : 0.09714, Sensitive_Acc : 16.700, Run Time : 16.22 sec
INFO:root:2024-04-26 14:33:23, Train, Epoch : 6, Step : 3460, Loss : 0.31609, Acc : 0.866, Sensitive_Loss : 0.12468, Sensitive_Acc : 17.000, Run Time : 14.90 sec
INFO:root:2024-04-26 14:33:31, Train, Epoch : 6, Step : 3470, Loss : 0.32858, Acc : 0.847, Sensitive_Loss : 0.11059, Sensitive_Acc : 16.700, Run Time : 8.85 sec
INFO:root:2024-04-26 14:33:40, Train, Epoch : 6, Step : 3480, Loss : 0.31516, Acc : 0.869, Sensitive_Loss : 0.14012, Sensitive_Acc : 16.300, Run Time : 8.73 sec
INFO:root:2024-04-26 14:33:50, Train, Epoch : 6, Step : 3490, Loss : 0.26755, Acc : 0.863, Sensitive_Loss : 0.14282, Sensitive_Acc : 16.700, Run Time : 9.66 sec
INFO:root:2024-04-26 14:33:59, Train, Epoch : 6, Step : 3500, Loss : 0.34572, Acc : 0.850, Sensitive_Loss : 0.10418, Sensitive_Acc : 15.400, Run Time : 8.71 sec
INFO:root:2024-04-26 14:35:46, Dev, Step : 3500, Loss : 0.46222, Acc : 0.811, Auc : 0.904, Sensitive_Loss : 0.14140, Sensitive_Acc : 16.850, Sensitive_Auc : 0.990, Mean auc: 0.904, Run Time : 107.33 sec
INFO:root:2024-04-26 14:35:52, Train, Epoch : 6, Step : 3510, Loss : 0.35641, Acc : 0.856, Sensitive_Loss : 0.14897, Sensitive_Acc : 17.100, Run Time : 113.26 sec
INFO:root:2024-04-26 14:36:01, Train, Epoch : 6, Step : 3520, Loss : 0.36081, Acc : 0.859, Sensitive_Loss : 0.10916, Sensitive_Acc : 16.400, Run Time : 9.14 sec
INFO:root:2024-04-26 14:36:10, Train, Epoch : 6, Step : 3530, Loss : 0.32169, Acc : 0.863, Sensitive_Loss : 0.10832, Sensitive_Acc : 17.600, Run Time : 8.76 sec
INFO:root:2024-04-26 14:36:19, Train, Epoch : 6, Step : 3540, Loss : 0.35482, Acc : 0.853, Sensitive_Loss : 0.09405, Sensitive_Acc : 16.700, Run Time : 9.25 sec
INFO:root:2024-04-26 14:36:28, Train, Epoch : 6, Step : 3550, Loss : 0.27371, Acc : 0.866, Sensitive_Loss : 0.11320, Sensitive_Acc : 16.800, Run Time : 8.91 sec
INFO:root:2024-04-26 14:36:37, Train, Epoch : 6, Step : 3560, Loss : 0.34639, Acc : 0.834, Sensitive_Loss : 0.09519, Sensitive_Acc : 17.900, Run Time : 8.70 sec
INFO:root:2024-04-26 14:36:45, Train, Epoch : 6, Step : 3570, Loss : 0.27983, Acc : 0.875, Sensitive_Loss : 0.09587, Sensitive_Acc : 15.000, Run Time : 8.76 sec
INFO:root:2024-04-26 14:36:54, Train, Epoch : 6, Step : 3580, Loss : 0.34304, Acc : 0.869, Sensitive_Loss : 0.10152, Sensitive_Acc : 16.400, Run Time : 8.49 sec
INFO:root:2024-04-26 14:37:04, Train, Epoch : 6, Step : 3590, Loss : 0.32132, Acc : 0.838, Sensitive_Loss : 0.09982, Sensitive_Acc : 15.600, Run Time : 9.89 sec
INFO:root:2024-04-26 14:37:14, Train, Epoch : 6, Step : 3600, Loss : 0.36008, Acc : 0.834, Sensitive_Loss : 0.09051, Sensitive_Acc : 15.600, Run Time : 10.59 sec
INFO:root:2024-04-26 14:38:48, Dev, Step : 3600, Loss : 0.45609, Acc : 0.812, Auc : 0.904, Sensitive_Loss : 0.15438, Sensitive_Acc : 16.850, Sensitive_Auc : 0.989, Mean auc: 0.904, Run Time : 93.26 sec
INFO:root:2024-04-26 14:38:54, Train, Epoch : 6, Step : 3610, Loss : 0.34928, Acc : 0.831, Sensitive_Loss : 0.11697, Sensitive_Acc : 16.200, Run Time : 99.32 sec
INFO:root:2024-04-26 14:39:03, Train, Epoch : 6, Step : 3620, Loss : 0.33338, Acc : 0.850, Sensitive_Loss : 0.13322, Sensitive_Acc : 16.100, Run Time : 8.95 sec
INFO:root:2024-04-26 14:39:12, Train, Epoch : 6, Step : 3630, Loss : 0.35789, Acc : 0.859, Sensitive_Loss : 0.08901, Sensitive_Acc : 16.300, Run Time : 9.04 sec
INFO:root:2024-04-26 14:39:20, Train, Epoch : 6, Step : 3640, Loss : 0.32834, Acc : 0.866, Sensitive_Loss : 0.10689, Sensitive_Acc : 16.100, Run Time : 8.65 sec
INFO:root:2024-04-26 14:39:29, Train, Epoch : 6, Step : 3650, Loss : 0.31425, Acc : 0.856, Sensitive_Loss : 0.08806, Sensitive_Acc : 17.000, Run Time : 9.07 sec
INFO:root:2024-04-26 14:39:39, Train, Epoch : 6, Step : 3660, Loss : 0.30475, Acc : 0.875, Sensitive_Loss : 0.09423, Sensitive_Acc : 15.900, Run Time : 9.26 sec
INFO:root:2024-04-26 14:39:48, Train, Epoch : 6, Step : 3670, Loss : 0.26897, Acc : 0.878, Sensitive_Loss : 0.08793, Sensitive_Acc : 16.800, Run Time : 9.19 sec
INFO:root:2024-04-26 14:39:58, Train, Epoch : 6, Step : 3680, Loss : 0.35061, Acc : 0.850, Sensitive_Loss : 0.08905, Sensitive_Acc : 15.600, Run Time : 10.21 sec
INFO:root:2024-04-26 14:40:07, Train, Epoch : 6, Step : 3690, Loss : 0.35253, Acc : 0.834, Sensitive_Loss : 0.10012, Sensitive_Acc : 17.200, Run Time : 8.59 sec
INFO:root:2024-04-26 14:40:15, Train, Epoch : 6, Step : 3700, Loss : 0.35536, Acc : 0.831, Sensitive_Loss : 0.08774, Sensitive_Acc : 16.400, Run Time : 8.93 sec
INFO:root:2024-04-26 14:41:49, Dev, Step : 3700, Loss : 0.43356, Acc : 0.819, Auc : 0.905, Sensitive_Loss : 0.13020, Sensitive_Acc : 16.793, Sensitive_Auc : 0.991, Mean auc: 0.905, Run Time : 93.55 sec
INFO:root:2024-04-26 14:41:55, Train, Epoch : 6, Step : 3710, Loss : 0.30569, Acc : 0.872, Sensitive_Loss : 0.09309, Sensitive_Acc : 17.000, Run Time : 99.58 sec
INFO:root:2024-04-26 14:42:05, Train, Epoch : 6, Step : 3720, Loss : 0.34370, Acc : 0.863, Sensitive_Loss : 0.08467, Sensitive_Acc : 17.700, Run Time : 9.55 sec
INFO:root:2024-04-26 14:42:14, Train, Epoch : 6, Step : 3730, Loss : 0.27632, Acc : 0.866, Sensitive_Loss : 0.08449, Sensitive_Acc : 16.100, Run Time : 9.37 sec
INFO:root:2024-04-26 14:42:23, Train, Epoch : 6, Step : 3740, Loss : 0.32709, Acc : 0.850, Sensitive_Loss : 0.10105, Sensitive_Acc : 14.900, Run Time : 9.33 sec
INFO:root:2024-04-26 14:42:32, Train, Epoch : 6, Step : 3750, Loss : 0.33458, Acc : 0.863, Sensitive_Loss : 0.11400, Sensitive_Acc : 16.900, Run Time : 9.06 sec
INFO:root:2024-04-26 14:44:11
INFO:root:y_pred: [0.1430699  0.9373073  0.05006215 ... 0.8400483  0.00492126 0.8270069 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.7649604e-01 1.4154665e-04 1.0382886e-01 5.6889138e-07 9.9595225e-01
 1.4717186e-07 9.9459815e-01 9.9091142e-01 5.7711853e-03 9.0850997e-01
 9.9036145e-01 9.9803609e-01 9.9420619e-01 9.4502544e-01 4.1859294e-03
 9.5233947e-01 9.9882168e-01 3.3080555e-03 8.9197099e-01 9.9434292e-01
 9.8229378e-01 2.5462008e-03 9.9819273e-01 9.0890580e-01 9.9612576e-01
 7.8361309e-01 3.8983353e-06 9.9078804e-01 9.9139941e-01 6.3072222e-01
 4.8980024e-03 1.3187003e-01 4.3985422e-04 6.6092601e-03 1.0782883e-01
 3.1881446e-03 4.1509066e-03 6.1614218e-04 9.8562288e-01 9.7347349e-01
 2.2069804e-09 5.1000882e-07 9.6489698e-01 5.7667654e-05 9.9862337e-01
 9.9498343e-01 9.8659050e-01 9.9732435e-01 1.1065186e-03 9.9427670e-01
 9.9830985e-01 1.9140444e-03 2.9319274e-01 6.2215986e-05 2.1431408e-06
 8.4208330e-04 4.4939177e-05 1.9245235e-02 1.5359805e-05 3.0022463e-01
 6.4623723e-04 3.6396322e-01 2.3126195e-05 9.8992050e-01 7.4240742e-03
 9.9791294e-01 4.2496872e-06 9.9310231e-01 9.1813123e-01 7.3719943e-01
 9.7319186e-01 6.8853742e-01 1.7137349e-03 2.0098391e-03 1.3932711e-04
 3.6007295e-06 1.1219274e-05 4.7189498e-01 1.0848780e-05 9.8320824e-01
 9.9683541e-01 3.0830162e-04 7.8797412e-01 1.6934717e-05 9.7679013e-01
 7.1083677e-01 6.1203769e-05 8.3675414e-02 9.4858098e-01 9.9086714e-01
 9.9067605e-01 2.6011260e-02 1.5080537e-02 9.9858248e-01 1.8273422e-04
 9.3769459e-03 9.9375618e-01 9.8299408e-01 1.0366882e-05 1.6152175e-01
 9.7806907e-01 9.6637124e-01 9.9778664e-01 9.9119765e-01 9.4504800e-04
 2.0544953e-03 8.4207422e-01 9.1785711e-01 9.1568106e-01 7.9113977e-07
 9.4892555e-01 9.9540472e-01 3.9843605e-03 9.9642521e-01 9.4965851e-01
 9.8600870e-01 9.3399918e-01 9.9651116e-01 4.0771070e-04 3.3651003e-01
 9.9750251e-01 9.9177152e-01 8.8602692e-06 8.7992072e-01 9.9176109e-01
 2.8774345e-01 9.9261016e-01 1.8003603e-02 1.1360715e-03 9.9443507e-01
 9.9588484e-01 1.0671565e-04 6.9367874e-04 7.6151141e-03 9.9393338e-01
 9.9874866e-01 9.7738326e-01 3.1255854e-03 3.9159656e-03 9.8415840e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 14:44:11, Dev, Step : 3756, Loss : 0.43170, Acc : 0.826, Auc : 0.906, Sensitive_Loss : 0.13753, Sensitive_Acc : 16.793, Sensitive_Auc : 0.990, Mean auc: 0.906, Run Time : 93.15 sec
INFO:root:2024-04-26 14:44:17, Train, Epoch : 7, Step : 3760, Loss : 0.11472, Acc : 0.353, Sensitive_Loss : 0.04684, Sensitive_Acc : 6.900, Run Time : 4.86 sec
INFO:root:2024-04-26 14:44:25, Train, Epoch : 7, Step : 3770, Loss : 0.34919, Acc : 0.866, Sensitive_Loss : 0.09931, Sensitive_Acc : 16.300, Run Time : 7.57 sec
INFO:root:2024-04-26 14:44:32, Train, Epoch : 7, Step : 3780, Loss : 0.27584, Acc : 0.863, Sensitive_Loss : 0.11776, Sensitive_Acc : 16.600, Run Time : 7.57 sec
INFO:root:2024-04-26 14:44:40, Train, Epoch : 7, Step : 3790, Loss : 0.28511, Acc : 0.866, Sensitive_Loss : 0.12106, Sensitive_Acc : 16.900, Run Time : 8.22 sec
INFO:root:2024-04-26 14:44:48, Train, Epoch : 7, Step : 3800, Loss : 0.30816, Acc : 0.859, Sensitive_Loss : 0.12244, Sensitive_Acc : 18.400, Run Time : 7.54 sec
INFO:root:2024-04-26 14:46:21, Dev, Step : 3800, Loss : 0.42691, Acc : 0.828, Auc : 0.904, Sensitive_Loss : 0.14457, Sensitive_Acc : 16.793, Sensitive_Auc : 0.988, Mean auc: 0.904, Run Time : 93.28 sec
INFO:root:2024-04-26 14:46:27, Train, Epoch : 7, Step : 3810, Loss : 0.34418, Acc : 0.863, Sensitive_Loss : 0.12403, Sensitive_Acc : 16.100, Run Time : 99.21 sec
INFO:root:2024-04-26 14:46:35, Train, Epoch : 7, Step : 3820, Loss : 0.33145, Acc : 0.847, Sensitive_Loss : 0.10032, Sensitive_Acc : 15.800, Run Time : 8.25 sec
INFO:root:2024-04-26 14:46:43, Train, Epoch : 7, Step : 3830, Loss : 0.30798, Acc : 0.878, Sensitive_Loss : 0.07701, Sensitive_Acc : 16.200, Run Time : 7.97 sec
INFO:root:2024-04-26 14:46:51, Train, Epoch : 7, Step : 3840, Loss : 0.33900, Acc : 0.841, Sensitive_Loss : 0.10305, Sensitive_Acc : 18.100, Run Time : 7.55 sec
INFO:root:2024-04-26 14:46:59, Train, Epoch : 7, Step : 3850, Loss : 0.29922, Acc : 0.894, Sensitive_Loss : 0.10989, Sensitive_Acc : 16.300, Run Time : 8.33 sec
INFO:root:2024-04-26 14:47:08, Train, Epoch : 7, Step : 3860, Loss : 0.31952, Acc : 0.850, Sensitive_Loss : 0.09969, Sensitive_Acc : 16.600, Run Time : 8.64 sec
INFO:root:2024-04-26 14:47:16, Train, Epoch : 7, Step : 3870, Loss : 0.26819, Acc : 0.881, Sensitive_Loss : 0.12687, Sensitive_Acc : 16.500, Run Time : 7.87 sec
INFO:root:2024-04-26 14:47:24, Train, Epoch : 7, Step : 3880, Loss : 0.25959, Acc : 0.897, Sensitive_Loss : 0.08426, Sensitive_Acc : 15.900, Run Time : 7.95 sec
INFO:root:2024-04-26 14:47:32, Train, Epoch : 7, Step : 3890, Loss : 0.35924, Acc : 0.834, Sensitive_Loss : 0.09594, Sensitive_Acc : 15.200, Run Time : 8.39 sec
INFO:root:2024-04-26 14:47:41, Train, Epoch : 7, Step : 3900, Loss : 0.27359, Acc : 0.875, Sensitive_Loss : 0.12929, Sensitive_Acc : 17.300, Run Time : 8.33 sec
INFO:root:2024-04-26 14:49:14, Dev, Step : 3900, Loss : 0.44662, Acc : 0.821, Auc : 0.901, Sensitive_Loss : 0.14992, Sensitive_Acc : 16.807, Sensitive_Auc : 0.988, Mean auc: 0.901, Run Time : 93.16 sec
INFO:root:2024-04-26 14:49:19, Train, Epoch : 7, Step : 3910, Loss : 0.35909, Acc : 0.875, Sensitive_Loss : 0.09905, Sensitive_Acc : 14.700, Run Time : 98.84 sec
INFO:root:2024-04-26 14:49:29, Train, Epoch : 7, Step : 3920, Loss : 0.32468, Acc : 0.863, Sensitive_Loss : 0.11491, Sensitive_Acc : 16.000, Run Time : 9.62 sec
INFO:root:2024-04-26 14:49:38, Train, Epoch : 7, Step : 3930, Loss : 0.25353, Acc : 0.884, Sensitive_Loss : 0.12632, Sensitive_Acc : 16.200, Run Time : 9.05 sec
INFO:root:2024-04-26 14:49:46, Train, Epoch : 7, Step : 3940, Loss : 0.29656, Acc : 0.884, Sensitive_Loss : 0.11387, Sensitive_Acc : 16.900, Run Time : 7.87 sec
INFO:root:2024-04-26 14:49:56, Train, Epoch : 7, Step : 3950, Loss : 0.29136, Acc : 0.875, Sensitive_Loss : 0.11444, Sensitive_Acc : 14.800, Run Time : 9.66 sec
INFO:root:2024-04-26 14:50:04, Train, Epoch : 7, Step : 3960, Loss : 0.30604, Acc : 0.866, Sensitive_Loss : 0.13996, Sensitive_Acc : 16.300, Run Time : 8.48 sec
INFO:root:2024-04-26 14:50:13, Train, Epoch : 7, Step : 3970, Loss : 0.36250, Acc : 0.850, Sensitive_Loss : 0.11661, Sensitive_Acc : 16.800, Run Time : 9.08 sec
INFO:root:2024-04-26 14:50:21, Train, Epoch : 7, Step : 3980, Loss : 0.30501, Acc : 0.863, Sensitive_Loss : 0.18019, Sensitive_Acc : 16.700, Run Time : 8.07 sec
INFO:root:2024-04-26 14:50:30, Train, Epoch : 7, Step : 3990, Loss : 0.29752, Acc : 0.872, Sensitive_Loss : 0.12879, Sensitive_Acc : 16.100, Run Time : 8.79 sec
INFO:root:2024-04-26 14:50:38, Train, Epoch : 7, Step : 4000, Loss : 0.31966, Acc : 0.884, Sensitive_Loss : 0.10071, Sensitive_Acc : 15.900, Run Time : 7.94 sec
INFO:root:2024-04-26 14:52:11, Dev, Step : 4000, Loss : 0.44545, Acc : 0.816, Auc : 0.903, Sensitive_Loss : 0.11751, Sensitive_Acc : 16.807, Sensitive_Auc : 0.991, Mean auc: 0.903, Run Time : 93.16 sec
INFO:root:2024-04-26 14:52:16, Train, Epoch : 7, Step : 4010, Loss : 0.31322, Acc : 0.878, Sensitive_Loss : 0.10691, Sensitive_Acc : 16.500, Run Time : 98.47 sec
INFO:root:2024-04-26 14:52:25, Train, Epoch : 7, Step : 4020, Loss : 0.28035, Acc : 0.875, Sensitive_Loss : 0.12558, Sensitive_Acc : 18.100, Run Time : 8.84 sec
INFO:root:2024-04-26 14:52:33, Train, Epoch : 7, Step : 4030, Loss : 0.33155, Acc : 0.866, Sensitive_Loss : 0.09733, Sensitive_Acc : 14.200, Run Time : 7.95 sec
INFO:root:2024-04-26 14:52:41, Train, Epoch : 7, Step : 4040, Loss : 0.32213, Acc : 0.859, Sensitive_Loss : 0.11535, Sensitive_Acc : 16.700, Run Time : 7.92 sec
INFO:root:2024-04-26 14:52:49, Train, Epoch : 7, Step : 4050, Loss : 0.30718, Acc : 0.872, Sensitive_Loss : 0.13865, Sensitive_Acc : 17.600, Run Time : 8.17 sec
INFO:root:2024-04-26 14:52:58, Train, Epoch : 7, Step : 4060, Loss : 0.39164, Acc : 0.822, Sensitive_Loss : 0.13333, Sensitive_Acc : 15.700, Run Time : 8.98 sec
INFO:root:2024-04-26 14:53:06, Train, Epoch : 7, Step : 4070, Loss : 0.30986, Acc : 0.853, Sensitive_Loss : 0.09037, Sensitive_Acc : 15.000, Run Time : 7.56 sec
INFO:root:2024-04-26 14:53:14, Train, Epoch : 7, Step : 4080, Loss : 0.31340, Acc : 0.881, Sensitive_Loss : 0.09050, Sensitive_Acc : 16.000, Run Time : 8.53 sec
INFO:root:2024-04-26 14:53:22, Train, Epoch : 7, Step : 4090, Loss : 0.30341, Acc : 0.875, Sensitive_Loss : 0.11036, Sensitive_Acc : 17.700, Run Time : 7.86 sec
INFO:root:2024-04-26 14:53:30, Train, Epoch : 7, Step : 4100, Loss : 0.33263, Acc : 0.841, Sensitive_Loss : 0.07572, Sensitive_Acc : 16.400, Run Time : 8.25 sec
INFO:root:2024-04-26 14:55:04, Dev, Step : 4100, Loss : 0.43553, Acc : 0.820, Auc : 0.904, Sensitive_Loss : 0.11379, Sensitive_Acc : 16.807, Sensitive_Auc : 0.991, Mean auc: 0.904, Run Time : 93.39 sec
INFO:root:2024-04-26 14:55:10, Train, Epoch : 7, Step : 4110, Loss : 0.28391, Acc : 0.872, Sensitive_Loss : 0.10231, Sensitive_Acc : 16.400, Run Time : 99.24 sec
INFO:root:2024-04-26 14:55:18, Train, Epoch : 7, Step : 4120, Loss : 0.31159, Acc : 0.866, Sensitive_Loss : 0.09487, Sensitive_Acc : 16.300, Run Time : 8.37 sec
INFO:root:2024-04-26 14:55:27, Train, Epoch : 7, Step : 4130, Loss : 0.29909, Acc : 0.906, Sensitive_Loss : 0.09455, Sensitive_Acc : 16.800, Run Time : 8.75 sec
INFO:root:2024-04-26 14:55:40, Train, Epoch : 7, Step : 4140, Loss : 0.36438, Acc : 0.850, Sensitive_Loss : 0.10169, Sensitive_Acc : 18.500, Run Time : 13.55 sec
INFO:root:2024-04-26 14:55:50, Train, Epoch : 7, Step : 4150, Loss : 0.30906, Acc : 0.863, Sensitive_Loss : 0.09107, Sensitive_Acc : 17.400, Run Time : 9.13 sec
INFO:root:2024-04-26 14:55:59, Train, Epoch : 7, Step : 4160, Loss : 0.35927, Acc : 0.822, Sensitive_Loss : 0.09594, Sensitive_Acc : 16.200, Run Time : 9.67 sec
INFO:root:2024-04-26 14:56:12, Train, Epoch : 7, Step : 4170, Loss : 0.31837, Acc : 0.859, Sensitive_Loss : 0.06152, Sensitive_Acc : 16.200, Run Time : 13.16 sec
INFO:root:2024-04-26 14:56:23, Train, Epoch : 7, Step : 4180, Loss : 0.27348, Acc : 0.869, Sensitive_Loss : 0.08296, Sensitive_Acc : 16.900, Run Time : 11.05 sec
INFO:root:2024-04-26 14:56:33, Train, Epoch : 7, Step : 4190, Loss : 0.29607, Acc : 0.878, Sensitive_Loss : 0.08401, Sensitive_Acc : 16.800, Run Time : 9.74 sec
INFO:root:2024-04-26 14:56:44, Train, Epoch : 7, Step : 4200, Loss : 0.27239, Acc : 0.903, Sensitive_Loss : 0.10575, Sensitive_Acc : 16.000, Run Time : 10.80 sec
INFO:root:2024-04-26 14:58:17, Dev, Step : 4200, Loss : 0.44395, Acc : 0.819, Auc : 0.905, Sensitive_Loss : 0.11415, Sensitive_Acc : 16.779, Sensitive_Auc : 0.991, Mean auc: 0.905, Run Time : 92.80 sec
INFO:root:2024-04-26 14:58:23, Train, Epoch : 7, Step : 4210, Loss : 0.30028, Acc : 0.875, Sensitive_Loss : 0.08966, Sensitive_Acc : 16.500, Run Time : 99.39 sec
INFO:root:2024-04-26 14:58:34, Train, Epoch : 7, Step : 4220, Loss : 0.27556, Acc : 0.856, Sensitive_Loss : 0.07932, Sensitive_Acc : 17.200, Run Time : 10.52 sec
INFO:root:2024-04-26 14:58:43, Train, Epoch : 7, Step : 4230, Loss : 0.27652, Acc : 0.887, Sensitive_Loss : 0.09010, Sensitive_Acc : 17.000, Run Time : 8.82 sec
INFO:root:2024-04-26 14:58:52, Train, Epoch : 7, Step : 4240, Loss : 0.36411, Acc : 0.859, Sensitive_Loss : 0.07102, Sensitive_Acc : 16.400, Run Time : 9.19 sec
INFO:root:2024-04-26 14:59:01, Train, Epoch : 7, Step : 4250, Loss : 0.28249, Acc : 0.881, Sensitive_Loss : 0.11662, Sensitive_Acc : 16.400, Run Time : 8.92 sec
INFO:root:2024-04-26 14:59:10, Train, Epoch : 7, Step : 4260, Loss : 0.29865, Acc : 0.884, Sensitive_Loss : 0.06446, Sensitive_Acc : 16.000, Run Time : 9.55 sec
INFO:root:2024-04-26 14:59:19, Train, Epoch : 7, Step : 4270, Loss : 0.26951, Acc : 0.894, Sensitive_Loss : 0.08548, Sensitive_Acc : 17.800, Run Time : 8.70 sec
INFO:root:2024-04-26 14:59:27, Train, Epoch : 7, Step : 4280, Loss : 0.31085, Acc : 0.881, Sensitive_Loss : 0.11879, Sensitive_Acc : 17.600, Run Time : 8.06 sec
INFO:root:2024-04-26 14:59:36, Train, Epoch : 7, Step : 4290, Loss : 0.29004, Acc : 0.884, Sensitive_Loss : 0.14376, Sensitive_Acc : 15.400, Run Time : 8.95 sec
INFO:root:2024-04-26 14:59:45, Train, Epoch : 7, Step : 4300, Loss : 0.29794, Acc : 0.872, Sensitive_Loss : 0.06700, Sensitive_Acc : 15.400, Run Time : 8.72 sec
INFO:root:2024-04-26 15:01:34, Dev, Step : 4300, Loss : 0.44520, Acc : 0.820, Auc : 0.905, Sensitive_Loss : 0.12289, Sensitive_Acc : 16.764, Sensitive_Auc : 0.989, Mean auc: 0.905, Run Time : 109.66 sec
INFO:root:2024-04-26 15:01:41, Train, Epoch : 7, Step : 4310, Loss : 0.32411, Acc : 0.856, Sensitive_Loss : 0.08439, Sensitive_Acc : 15.900, Run Time : 115.97 sec
INFO:root:2024-04-26 15:01:52, Train, Epoch : 7, Step : 4320, Loss : 0.31232, Acc : 0.863, Sensitive_Loss : 0.10100, Sensitive_Acc : 15.600, Run Time : 10.97 sec
INFO:root:2024-04-26 15:02:02, Train, Epoch : 7, Step : 4330, Loss : 0.25991, Acc : 0.887, Sensitive_Loss : 0.09115, Sensitive_Acc : 17.600, Run Time : 10.09 sec
INFO:root:2024-04-26 15:02:10, Train, Epoch : 7, Step : 4340, Loss : 0.29972, Acc : 0.872, Sensitive_Loss : 0.07537, Sensitive_Acc : 15.700, Run Time : 8.66 sec
INFO:root:2024-04-26 15:02:19, Train, Epoch : 7, Step : 4350, Loss : 0.29615, Acc : 0.866, Sensitive_Loss : 0.09539, Sensitive_Acc : 15.800, Run Time : 8.51 sec
INFO:root:2024-04-26 15:02:30, Train, Epoch : 7, Step : 4360, Loss : 0.30057, Acc : 0.859, Sensitive_Loss : 0.09081, Sensitive_Acc : 16.100, Run Time : 11.02 sec
INFO:root:2024-04-26 15:02:39, Train, Epoch : 7, Step : 4370, Loss : 0.34325, Acc : 0.841, Sensitive_Loss : 0.08095, Sensitive_Acc : 15.100, Run Time : 9.48 sec
INFO:root:2024-04-26 15:02:48, Train, Epoch : 7, Step : 4380, Loss : 0.26833, Acc : 0.875, Sensitive_Loss : 0.13444, Sensitive_Acc : 15.100, Run Time : 8.21 sec
INFO:root:2024-04-26 15:04:24
INFO:root:y_pred: [0.04667028 0.92063653 0.04663718 ... 0.6137496  0.00278489 0.7849112 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.90972757e-01 3.11005191e-04 7.60379136e-02 1.12288535e-05
 9.96943533e-01 3.12267787e-07 9.96594489e-01 9.95336473e-01
 1.88045693e-03 9.42104816e-01 9.96033847e-01 9.98601615e-01
 9.96512830e-01 9.74567711e-01 1.22972419e-02 9.74560022e-01
 9.99666333e-01 9.72983986e-03 9.60865855e-01 9.97637391e-01
 9.84901726e-01 3.19582922e-03 9.99313951e-01 9.27350640e-01
 9.97782528e-01 8.77156734e-01 1.60256095e-05 9.91636932e-01
 9.92902935e-01 7.30455518e-01 1.48970047e-02 4.66759533e-01
 3.22491303e-03 4.20441525e-03 1.72734573e-01 8.68587010e-03
 1.01695191e-02 1.30923965e-03 9.93094087e-01 9.82736230e-01
 5.07434850e-09 7.68962400e-06 9.90319788e-01 1.68590210e-04
 9.99171734e-01 9.94679511e-01 9.95943606e-01 9.98594224e-01
 2.26641097e-03 9.95602012e-01 9.99232054e-01 2.10382813e-03
 6.60907805e-01 1.69059611e-04 3.64563493e-05 1.06070249e-03
 1.97185623e-03 3.41508277e-02 9.78747776e-05 6.93431437e-01
 9.61514097e-03 5.00161886e-01 2.70285473e-05 9.96078193e-01
 2.02224161e-02 9.98408258e-01 3.06723450e-05 9.96518254e-01
 9.40654039e-01 8.47192883e-01 9.90586817e-01 7.43928552e-01
 2.11008242e-03 5.26189292e-03 2.78953346e-04 1.29954251e-05
 7.00690071e-05 5.33738375e-01 1.20195482e-05 9.88586605e-01
 9.98319089e-01 9.03513515e-04 8.44477832e-01 2.01902163e-04
 9.90440130e-01 8.41046035e-01 6.69190849e-05 8.41546580e-02
 9.51679468e-01 9.93982732e-01 9.96181607e-01 7.08909258e-02
 3.00259348e-02 9.99208510e-01 1.20213919e-03 1.97339468e-02
 9.95650470e-01 9.89714801e-01 4.54161855e-05 6.21338785e-01
 9.75039840e-01 9.80949342e-01 9.98835623e-01 9.94569182e-01
 2.35720584e-03 2.09965091e-02 8.49371195e-01 9.38924193e-01
 9.46668148e-01 1.04400544e-06 9.53720927e-01 9.97548640e-01
 7.84289464e-03 9.97651279e-01 9.78036344e-01 9.93277609e-01
 8.92906249e-01 9.97875571e-01 3.73373245e-04 6.15370095e-01
 9.98531103e-01 9.94262516e-01 6.84921324e-05 9.29385841e-01
 9.95040119e-01 1.79738566e-01 9.95643139e-01 5.12361862e-02
 9.72468220e-03 9.97043550e-01 9.96715188e-01 1.28169009e-03
 1.02757197e-03 2.78716832e-02 9.96910632e-01 9.99401569e-01
 9.83708680e-01 1.12873390e-02 2.64660120e-02 9.92498219e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 15:04:24, Dev, Step : 4382, Loss : 0.49433, Acc : 0.803, Auc : 0.903, Sensitive_Loss : 0.16689, Sensitive_Acc : 16.821, Sensitive_Auc : 0.988, Mean auc: 0.903, Run Time : 94.79 sec
INFO:root:2024-04-26 15:04:32, Train, Epoch : 8, Step : 4390, Loss : 0.25942, Acc : 0.691, Sensitive_Loss : 0.07917, Sensitive_Acc : 13.200, Run Time : 6.97 sec
INFO:root:2024-04-26 15:04:39, Train, Epoch : 8, Step : 4400, Loss : 0.23083, Acc : 0.884, Sensitive_Loss : 0.12115, Sensitive_Acc : 15.500, Run Time : 7.13 sec
INFO:root:2024-04-26 15:06:12, Dev, Step : 4400, Loss : 0.46534, Acc : 0.812, Auc : 0.904, Sensitive_Loss : 0.15107, Sensitive_Acc : 16.807, Sensitive_Auc : 0.989, Mean auc: 0.904, Run Time : 93.01 sec
INFO:root:2024-04-26 15:06:18, Train, Epoch : 8, Step : 4410, Loss : 0.28131, Acc : 0.891, Sensitive_Loss : 0.08575, Sensitive_Acc : 15.100, Run Time : 98.43 sec
INFO:root:2024-04-26 15:06:25, Train, Epoch : 8, Step : 4420, Loss : 0.29151, Acc : 0.872, Sensitive_Loss : 0.12256, Sensitive_Acc : 16.700, Run Time : 7.48 sec
INFO:root:2024-04-26 15:06:32, Train, Epoch : 8, Step : 4430, Loss : 0.27150, Acc : 0.894, Sensitive_Loss : 0.10447, Sensitive_Acc : 16.400, Run Time : 7.09 sec
INFO:root:2024-04-26 15:06:40, Train, Epoch : 8, Step : 4440, Loss : 0.28737, Acc : 0.866, Sensitive_Loss : 0.07891, Sensitive_Acc : 16.100, Run Time : 7.43 sec
INFO:root:2024-04-26 15:06:48, Train, Epoch : 8, Step : 4450, Loss : 0.26991, Acc : 0.887, Sensitive_Loss : 0.12407, Sensitive_Acc : 16.800, Run Time : 8.54 sec
INFO:root:2024-04-26 15:06:56, Train, Epoch : 8, Step : 4460, Loss : 0.30876, Acc : 0.859, Sensitive_Loss : 0.09002, Sensitive_Acc : 16.400, Run Time : 8.29 sec
INFO:root:2024-04-26 15:07:05, Train, Epoch : 8, Step : 4470, Loss : 0.31636, Acc : 0.866, Sensitive_Loss : 0.12371, Sensitive_Acc : 18.200, Run Time : 8.26 sec
INFO:root:2024-04-26 15:07:13, Train, Epoch : 8, Step : 4480, Loss : 0.35622, Acc : 0.831, Sensitive_Loss : 0.13361, Sensitive_Acc : 17.800, Run Time : 7.97 sec
INFO:root:2024-04-26 15:07:21, Train, Epoch : 8, Step : 4490, Loss : 0.25308, Acc : 0.894, Sensitive_Loss : 0.11747, Sensitive_Acc : 17.700, Run Time : 7.86 sec
INFO:root:2024-04-26 15:07:28, Train, Epoch : 8, Step : 4500, Loss : 0.33377, Acc : 0.859, Sensitive_Loss : 0.13422, Sensitive_Acc : 16.600, Run Time : 7.74 sec
INFO:root:2024-04-26 15:09:14, Dev, Step : 4500, Loss : 0.42952, Acc : 0.828, Auc : 0.900, Sensitive_Loss : 0.13364, Sensitive_Acc : 16.807, Sensitive_Auc : 0.989, Mean auc: 0.900, Run Time : 106.17 sec
INFO:root:2024-04-26 15:09:24, Train, Epoch : 8, Step : 4510, Loss : 0.31166, Acc : 0.863, Sensitive_Loss : 0.07037, Sensitive_Acc : 16.200, Run Time : 115.78 sec
INFO:root:2024-04-26 15:09:35, Train, Epoch : 8, Step : 4520, Loss : 0.24339, Acc : 0.891, Sensitive_Loss : 0.06455, Sensitive_Acc : 16.500, Run Time : 10.97 sec
INFO:root:2024-04-26 15:09:50, Train, Epoch : 8, Step : 4530, Loss : 0.29116, Acc : 0.872, Sensitive_Loss : 0.10740, Sensitive_Acc : 16.700, Run Time : 14.70 sec
INFO:root:2024-04-26 15:10:06, Train, Epoch : 8, Step : 4540, Loss : 0.28838, Acc : 0.891, Sensitive_Loss : 0.12567, Sensitive_Acc : 15.400, Run Time : 15.74 sec
INFO:root:2024-04-26 15:10:19, Train, Epoch : 8, Step : 4550, Loss : 0.28475, Acc : 0.894, Sensitive_Loss : 0.11767, Sensitive_Acc : 17.900, Run Time : 13.46 sec
INFO:root:2024-04-26 15:10:34, Train, Epoch : 8, Step : 4560, Loss : 0.27529, Acc : 0.884, Sensitive_Loss : 0.11422, Sensitive_Acc : 16.100, Run Time : 15.35 sec
INFO:root:2024-04-26 15:10:53, Train, Epoch : 8, Step : 4570, Loss : 0.26766, Acc : 0.887, Sensitive_Loss : 0.08591, Sensitive_Acc : 16.800, Run Time : 18.43 sec
INFO:root:2024-04-26 15:11:06, Train, Epoch : 8, Step : 4580, Loss : 0.29969, Acc : 0.853, Sensitive_Loss : 0.08476, Sensitive_Acc : 17.900, Run Time : 13.48 sec
INFO:root:2024-04-26 15:11:18, Train, Epoch : 8, Step : 4590, Loss : 0.30028, Acc : 0.869, Sensitive_Loss : 0.08791, Sensitive_Acc : 15.700, Run Time : 11.96 sec
INFO:root:2024-04-26 15:11:32, Train, Epoch : 8, Step : 4600, Loss : 0.39677, Acc : 0.875, Sensitive_Loss : 0.10999, Sensitive_Acc : 16.700, Run Time : 13.79 sec
INFO:root:2024-04-26 15:13:49, Dev, Step : 4600, Loss : 0.42413, Acc : 0.821, Auc : 0.899, Sensitive_Loss : 0.12060, Sensitive_Acc : 16.864, Sensitive_Auc : 0.985, Mean auc: 0.899, Run Time : 137.40 sec
INFO:root:2024-04-26 15:13:58, Train, Epoch : 8, Step : 4610, Loss : 0.30536, Acc : 0.841, Sensitive_Loss : 0.09439, Sensitive_Acc : 16.400, Run Time : 145.80 sec
INFO:root:2024-04-26 15:14:06, Train, Epoch : 8, Step : 4620, Loss : 0.27732, Acc : 0.878, Sensitive_Loss : 0.13691, Sensitive_Acc : 15.800, Run Time : 7.76 sec
INFO:root:2024-04-26 15:14:14, Train, Epoch : 8, Step : 4630, Loss : 0.25589, Acc : 0.912, Sensitive_Loss : 0.09381, Sensitive_Acc : 15.900, Run Time : 8.60 sec
INFO:root:2024-04-26 15:14:23, Train, Epoch : 8, Step : 4640, Loss : 0.37498, Acc : 0.822, Sensitive_Loss : 0.09982, Sensitive_Acc : 16.100, Run Time : 9.17 sec
INFO:root:2024-04-26 15:14:31, Train, Epoch : 8, Step : 4650, Loss : 0.38298, Acc : 0.841, Sensitive_Loss : 0.12494, Sensitive_Acc : 16.700, Run Time : 7.92 sec
INFO:root:2024-04-26 15:14:39, Train, Epoch : 8, Step : 4660, Loss : 0.26271, Acc : 0.878, Sensitive_Loss : 0.11038, Sensitive_Acc : 16.200, Run Time : 8.16 sec
INFO:root:2024-04-26 15:14:48, Train, Epoch : 8, Step : 4670, Loss : 0.30774, Acc : 0.853, Sensitive_Loss : 0.11767, Sensitive_Acc : 17.700, Run Time : 8.48 sec
INFO:root:2024-04-26 15:14:56, Train, Epoch : 8, Step : 4680, Loss : 0.27346, Acc : 0.884, Sensitive_Loss : 0.08491, Sensitive_Acc : 16.600, Run Time : 7.88 sec
INFO:root:2024-04-26 15:15:04, Train, Epoch : 8, Step : 4690, Loss : 0.29471, Acc : 0.878, Sensitive_Loss : 0.11587, Sensitive_Acc : 16.800, Run Time : 7.83 sec
INFO:root:2024-04-26 15:15:12, Train, Epoch : 8, Step : 4700, Loss : 0.28999, Acc : 0.856, Sensitive_Loss : 0.08828, Sensitive_Acc : 18.000, Run Time : 8.29 sec
INFO:root:2024-04-26 15:16:45, Dev, Step : 4700, Loss : 0.44966, Acc : 0.819, Auc : 0.901, Sensitive_Loss : 0.12673, Sensitive_Acc : 16.807, Sensitive_Auc : 0.987, Mean auc: 0.901, Run Time : 93.33 sec
INFO:root:2024-04-26 15:16:51, Train, Epoch : 8, Step : 4710, Loss : 0.24927, Acc : 0.900, Sensitive_Loss : 0.08528, Sensitive_Acc : 15.900, Run Time : 99.04 sec
INFO:root:2024-04-26 15:16:59, Train, Epoch : 8, Step : 4720, Loss : 0.29682, Acc : 0.878, Sensitive_Loss : 0.07737, Sensitive_Acc : 15.800, Run Time : 8.11 sec
INFO:root:2024-04-26 15:17:06, Train, Epoch : 8, Step : 4730, Loss : 0.26669, Acc : 0.866, Sensitive_Loss : 0.09333, Sensitive_Acc : 16.100, Run Time : 7.41 sec
INFO:root:2024-04-26 15:17:13, Train, Epoch : 8, Step : 4740, Loss : 0.25353, Acc : 0.906, Sensitive_Loss : 0.06763, Sensitive_Acc : 15.700, Run Time : 6.96 sec
INFO:root:2024-04-26 15:17:21, Train, Epoch : 8, Step : 4750, Loss : 0.27894, Acc : 0.866, Sensitive_Loss : 0.13548, Sensitive_Acc : 15.400, Run Time : 7.54 sec
INFO:root:2024-04-26 15:17:29, Train, Epoch : 8, Step : 4760, Loss : 0.26630, Acc : 0.875, Sensitive_Loss : 0.08632, Sensitive_Acc : 15.800, Run Time : 7.86 sec
INFO:root:2024-04-26 15:17:36, Train, Epoch : 8, Step : 4770, Loss : 0.31069, Acc : 0.875, Sensitive_Loss : 0.10001, Sensitive_Acc : 15.700, Run Time : 7.53 sec
INFO:root:2024-04-26 15:17:44, Train, Epoch : 8, Step : 4780, Loss : 0.27281, Acc : 0.878, Sensitive_Loss : 0.07482, Sensitive_Acc : 16.700, Run Time : 7.50 sec
INFO:root:2024-04-26 15:17:51, Train, Epoch : 8, Step : 4790, Loss : 0.32518, Acc : 0.859, Sensitive_Loss : 0.13672, Sensitive_Acc : 17.600, Run Time : 7.14 sec
INFO:root:2024-04-26 15:18:00, Train, Epoch : 8, Step : 4800, Loss : 0.31901, Acc : 0.884, Sensitive_Loss : 0.15098, Sensitive_Acc : 16.400, Run Time : 8.79 sec
INFO:root:2024-04-26 15:19:33, Dev, Step : 4800, Loss : 0.44707, Acc : 0.823, Auc : 0.899, Sensitive_Loss : 0.12035, Sensitive_Acc : 16.793, Sensitive_Auc : 0.989, Mean auc: 0.899, Run Time : 93.64 sec
INFO:root:2024-04-26 15:19:39, Train, Epoch : 8, Step : 4810, Loss : 0.33230, Acc : 0.866, Sensitive_Loss : 0.12149, Sensitive_Acc : 18.100, Run Time : 99.29 sec
INFO:root:2024-04-26 15:19:47, Train, Epoch : 8, Step : 4820, Loss : 0.27740, Acc : 0.884, Sensitive_Loss : 0.14986, Sensitive_Acc : 14.900, Run Time : 7.51 sec
INFO:root:2024-04-26 15:19:54, Train, Epoch : 8, Step : 4830, Loss : 0.21976, Acc : 0.903, Sensitive_Loss : 0.10352, Sensitive_Acc : 16.800, Run Time : 7.88 sec
INFO:root:2024-04-26 15:20:01, Train, Epoch : 8, Step : 4840, Loss : 0.23481, Acc : 0.922, Sensitive_Loss : 0.06862, Sensitive_Acc : 16.000, Run Time : 6.98 sec
INFO:root:2024-04-26 15:20:09, Train, Epoch : 8, Step : 4850, Loss : 0.27434, Acc : 0.856, Sensitive_Loss : 0.12250, Sensitive_Acc : 15.700, Run Time : 7.28 sec
INFO:root:2024-04-26 15:20:16, Train, Epoch : 8, Step : 4860, Loss : 0.31514, Acc : 0.863, Sensitive_Loss : 0.09913, Sensitive_Acc : 16.500, Run Time : 7.06 sec
INFO:root:2024-04-26 15:20:23, Train, Epoch : 8, Step : 4870, Loss : 0.31574, Acc : 0.878, Sensitive_Loss : 0.10090, Sensitive_Acc : 17.300, Run Time : 6.91 sec
INFO:root:2024-04-26 15:20:30, Train, Epoch : 8, Step : 4880, Loss : 0.26022, Acc : 0.903, Sensitive_Loss : 0.12953, Sensitive_Acc : 16.400, Run Time : 7.37 sec
INFO:root:2024-04-26 15:20:38, Train, Epoch : 8, Step : 4890, Loss : 0.25218, Acc : 0.891, Sensitive_Loss : 0.07413, Sensitive_Acc : 16.200, Run Time : 7.67 sec
INFO:root:2024-04-26 15:20:44, Train, Epoch : 8, Step : 4900, Loss : 0.24664, Acc : 0.906, Sensitive_Loss : 0.10412, Sensitive_Acc : 16.800, Run Time : 6.56 sec
INFO:root:2024-04-26 15:22:18, Dev, Step : 4900, Loss : 0.42307, Acc : 0.829, Auc : 0.903, Sensitive_Loss : 0.10949, Sensitive_Acc : 16.807, Sensitive_Auc : 0.991, Mean auc: 0.903, Run Time : 93.52 sec
INFO:root:2024-04-26 15:22:24, Train, Epoch : 8, Step : 4910, Loss : 0.34233, Acc : 0.869, Sensitive_Loss : 0.09929, Sensitive_Acc : 17.300, Run Time : 99.24 sec
INFO:root:2024-04-26 15:22:30, Train, Epoch : 8, Step : 4920, Loss : 0.35120, Acc : 0.869, Sensitive_Loss : 0.08903, Sensitive_Acc : 18.200, Run Time : 6.60 sec
INFO:root:2024-04-26 15:22:37, Train, Epoch : 8, Step : 4930, Loss : 0.27685, Acc : 0.891, Sensitive_Loss : 0.08135, Sensitive_Acc : 15.900, Run Time : 7.21 sec
INFO:root:2024-04-26 15:22:44, Train, Epoch : 8, Step : 4940, Loss : 0.24758, Acc : 0.881, Sensitive_Loss : 0.07437, Sensitive_Acc : 15.800, Run Time : 6.88 sec
INFO:root:2024-04-26 15:22:51, Train, Epoch : 8, Step : 4950, Loss : 0.28541, Acc : 0.875, Sensitive_Loss : 0.08558, Sensitive_Acc : 16.500, Run Time : 7.14 sec
INFO:root:2024-04-26 15:22:58, Train, Epoch : 8, Step : 4960, Loss : 0.26594, Acc : 0.891, Sensitive_Loss : 0.10630, Sensitive_Acc : 16.800, Run Time : 6.89 sec
INFO:root:2024-04-26 15:23:05, Train, Epoch : 8, Step : 4970, Loss : 0.31941, Acc : 0.856, Sensitive_Loss : 0.10158, Sensitive_Acc : 16.100, Run Time : 7.05 sec
INFO:root:2024-04-26 15:23:13, Train, Epoch : 8, Step : 4980, Loss : 0.24836, Acc : 0.884, Sensitive_Loss : 0.09517, Sensitive_Acc : 14.000, Run Time : 7.29 sec
INFO:root:2024-04-26 15:23:20, Train, Epoch : 8, Step : 4990, Loss : 0.30005, Acc : 0.894, Sensitive_Loss : 0.10044, Sensitive_Acc : 16.600, Run Time : 7.26 sec
INFO:root:2024-04-26 15:23:27, Train, Epoch : 8, Step : 5000, Loss : 0.27532, Acc : 0.891, Sensitive_Loss : 0.07124, Sensitive_Acc : 16.200, Run Time : 6.91 sec
INFO:root:2024-04-26 15:24:59, Dev, Step : 5000, Loss : 0.44130, Acc : 0.827, Auc : 0.904, Sensitive_Loss : 0.12378, Sensitive_Acc : 16.807, Sensitive_Auc : 0.989, Mean auc: 0.904, Run Time : 92.05 sec
INFO:root:2024-04-26 15:26:35
INFO:root:y_pred: [0.10010052 0.9610612  0.04305368 ... 0.75370306 0.00481204 0.91047186]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.82581496e-01 9.18918377e-05 9.02882498e-03 1.34534093e-06
 9.95920539e-01 1.59819820e-07 9.95904982e-01 9.95590925e-01
 5.30050835e-04 9.41005528e-01 9.89129007e-01 9.98700380e-01
 9.90783572e-01 9.59739923e-01 2.55458150e-03 9.32223022e-01
 9.99566853e-01 3.07001057e-03 9.05886948e-01 9.93188858e-01
 9.73321676e-01 6.23578439e-03 9.98727620e-01 8.81687880e-01
 9.98660207e-01 7.98160613e-01 2.82811038e-06 9.90057051e-01
 9.86375988e-01 4.31508392e-01 5.86998370e-03 6.61979765e-02
 6.06598478e-05 3.36495996e-03 3.79095860e-02 2.70588003e-04
 2.54255324e-03 6.56933524e-04 9.93039608e-01 9.60595012e-01
 3.91455229e-10 6.80522589e-06 9.82115269e-01 3.17899976e-05
 9.99024034e-01 9.96344507e-01 9.92517829e-01 9.93370175e-01
 3.17378831e-03 9.95925903e-01 9.99223709e-01 1.87442277e-03
 6.41843304e-02 2.25479180e-05 2.82991800e-06 4.56901820e-04
 1.28653087e-02 2.35945694e-02 6.29764036e-05 4.94108468e-01
 1.85377221e-03 1.76691502e-01 3.18795742e-06 9.82167959e-01
 9.01040900e-03 9.98353124e-01 4.89472131e-06 9.95123565e-01
 9.09803331e-01 7.62612879e-01 9.63871121e-01 6.67410553e-01
 2.55379884e-04 2.78025470e-03 1.18397393e-05 5.70212978e-06
 1.70841777e-05 2.98527062e-01 3.34337869e-06 9.84930515e-01
 9.96466637e-01 3.99273013e-05 6.77310705e-01 3.24428431e-04
 9.83080804e-01 2.21792892e-01 2.24016349e-05 2.40397938e-02
 9.26281393e-01 9.91029024e-01 9.92066622e-01 4.07871604e-02
 1.26425140e-02 9.97243404e-01 3.73053990e-06 3.42587195e-03
 9.94624436e-01 9.84591722e-01 4.11390192e-06 1.42141014e-01
 9.74030495e-01 9.71615553e-01 9.98318672e-01 9.86812294e-01
 4.91751183e-04 2.50617944e-04 8.48684907e-01 9.14077878e-01
 9.22136247e-01 1.18903564e-07 9.42235231e-01 9.92233038e-01
 2.47286772e-03 9.96994019e-01 9.69020188e-01 9.89455044e-01
 7.89769530e-01 9.97107089e-01 2.92997429e-04 3.51377547e-01
 9.98273373e-01 9.89837766e-01 3.48073227e-05 8.96090686e-01
 9.94163930e-01 7.00845793e-02 9.94430959e-01 1.50626630e-03
 7.84070289e-04 9.93480325e-01 9.94797349e-01 1.27140942e-04
 1.38785050e-04 9.24043078e-03 9.95411217e-01 9.99395013e-01
 9.76529360e-01 5.14462125e-03 1.10774878e-02 9.90361691e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 15:26:35, Dev, Step : 5008, Loss : 0.43985, Acc : 0.827, Auc : 0.905, Sensitive_Loss : 0.11430, Sensitive_Acc : 16.850, Sensitive_Auc : 0.989, Mean auc: 0.905, Run Time : 91.91 sec
INFO:root:2024-04-26 15:26:39, Train, Epoch : 9, Step : 5010, Loss : 0.03652, Acc : 0.184, Sensitive_Loss : 0.01168, Sensitive_Acc : 3.300, Run Time : 2.92 sec
INFO:root:2024-04-26 15:26:46, Train, Epoch : 9, Step : 5020, Loss : 0.26891, Acc : 0.900, Sensitive_Loss : 0.09753, Sensitive_Acc : 16.800, Run Time : 6.75 sec
INFO:root:2024-04-26 15:26:53, Train, Epoch : 9, Step : 5030, Loss : 0.27071, Acc : 0.887, Sensitive_Loss : 0.09302, Sensitive_Acc : 15.900, Run Time : 7.18 sec
INFO:root:2024-04-26 15:27:01, Train, Epoch : 9, Step : 5040, Loss : 0.32070, Acc : 0.856, Sensitive_Loss : 0.07975, Sensitive_Acc : 16.100, Run Time : 7.20 sec
INFO:root:2024-04-26 15:27:08, Train, Epoch : 9, Step : 5050, Loss : 0.27032, Acc : 0.891, Sensitive_Loss : 0.11016, Sensitive_Acc : 15.400, Run Time : 6.98 sec
INFO:root:2024-04-26 15:27:14, Train, Epoch : 9, Step : 5060, Loss : 0.26727, Acc : 0.884, Sensitive_Loss : 0.08125, Sensitive_Acc : 16.400, Run Time : 6.95 sec
INFO:root:2024-04-26 15:27:22, Train, Epoch : 9, Step : 5070, Loss : 0.22580, Acc : 0.903, Sensitive_Loss : 0.14170, Sensitive_Acc : 17.000, Run Time : 7.12 sec
INFO:root:2024-04-26 15:27:29, Train, Epoch : 9, Step : 5080, Loss : 0.23107, Acc : 0.909, Sensitive_Loss : 0.08778, Sensitive_Acc : 17.400, Run Time : 7.11 sec
INFO:root:2024-04-26 15:27:36, Train, Epoch : 9, Step : 5090, Loss : 0.24620, Acc : 0.906, Sensitive_Loss : 0.08579, Sensitive_Acc : 16.500, Run Time : 7.01 sec
INFO:root:2024-04-26 15:27:43, Train, Epoch : 9, Step : 5100, Loss : 0.26433, Acc : 0.869, Sensitive_Loss : 0.09839, Sensitive_Acc : 15.800, Run Time : 6.90 sec
INFO:root:2024-04-26 15:29:15, Dev, Step : 5100, Loss : 0.42932, Acc : 0.829, Auc : 0.903, Sensitive_Loss : 0.11447, Sensitive_Acc : 16.807, Sensitive_Auc : 0.989, Mean auc: 0.903, Run Time : 92.38 sec
INFO:root:2024-04-26 15:29:21, Train, Epoch : 9, Step : 5110, Loss : 0.21767, Acc : 0.906, Sensitive_Loss : 0.08172, Sensitive_Acc : 15.700, Run Time : 98.61 sec
INFO:root:2024-04-26 15:29:29, Train, Epoch : 9, Step : 5120, Loss : 0.28065, Acc : 0.875, Sensitive_Loss : 0.09310, Sensitive_Acc : 15.000, Run Time : 7.49 sec
INFO:root:2024-04-26 15:29:36, Train, Epoch : 9, Step : 5130, Loss : 0.21596, Acc : 0.897, Sensitive_Loss : 0.12292, Sensitive_Acc : 17.300, Run Time : 7.07 sec
INFO:root:2024-04-26 15:29:43, Train, Epoch : 9, Step : 5140, Loss : 0.28197, Acc : 0.872, Sensitive_Loss : 0.06269, Sensitive_Acc : 16.000, Run Time : 7.00 sec
INFO:root:2024-04-26 15:29:50, Train, Epoch : 9, Step : 5150, Loss : 0.23223, Acc : 0.894, Sensitive_Loss : 0.06715, Sensitive_Acc : 15.600, Run Time : 7.52 sec
INFO:root:2024-04-26 15:29:57, Train, Epoch : 9, Step : 5160, Loss : 0.26180, Acc : 0.894, Sensitive_Loss : 0.11428, Sensitive_Acc : 16.700, Run Time : 6.76 sec
INFO:root:2024-04-26 15:30:04, Train, Epoch : 9, Step : 5170, Loss : 0.29160, Acc : 0.884, Sensitive_Loss : 0.08558, Sensitive_Acc : 16.600, Run Time : 7.23 sec
INFO:root:2024-04-26 15:30:12, Train, Epoch : 9, Step : 5180, Loss : 0.30556, Acc : 0.869, Sensitive_Loss : 0.07985, Sensitive_Acc : 16.900, Run Time : 7.56 sec
INFO:root:2024-04-26 15:30:19, Train, Epoch : 9, Step : 5190, Loss : 0.31355, Acc : 0.847, Sensitive_Loss : 0.11136, Sensitive_Acc : 18.100, Run Time : 7.41 sec
INFO:root:2024-04-26 15:30:26, Train, Epoch : 9, Step : 5200, Loss : 0.23342, Acc : 0.897, Sensitive_Loss : 0.08878, Sensitive_Acc : 16.000, Run Time : 7.10 sec
INFO:root:2024-04-26 15:31:59, Dev, Step : 5200, Loss : 0.45441, Acc : 0.818, Auc : 0.897, Sensitive_Loss : 0.12615, Sensitive_Acc : 16.807, Sensitive_Auc : 0.987, Mean auc: 0.897, Run Time : 92.23 sec
INFO:root:2024-04-26 15:32:04, Train, Epoch : 9, Step : 5210, Loss : 0.34577, Acc : 0.859, Sensitive_Loss : 0.10269, Sensitive_Acc : 16.600, Run Time : 97.86 sec
INFO:root:2024-04-26 15:32:12, Train, Epoch : 9, Step : 5220, Loss : 0.34336, Acc : 0.875, Sensitive_Loss : 0.11663, Sensitive_Acc : 15.300, Run Time : 7.74 sec
INFO:root:2024-04-26 15:32:19, Train, Epoch : 9, Step : 5230, Loss : 0.22656, Acc : 0.919, Sensitive_Loss : 0.10111, Sensitive_Acc : 15.900, Run Time : 6.99 sec
INFO:root:2024-04-26 15:32:26, Train, Epoch : 9, Step : 5240, Loss : 0.30206, Acc : 0.881, Sensitive_Loss : 0.10271, Sensitive_Acc : 15.500, Run Time : 7.28 sec
INFO:root:2024-04-26 15:32:33, Train, Epoch : 9, Step : 5250, Loss : 0.27360, Acc : 0.894, Sensitive_Loss : 0.11807, Sensitive_Acc : 16.900, Run Time : 7.09 sec
INFO:root:2024-04-26 15:32:41, Train, Epoch : 9, Step : 5260, Loss : 0.24138, Acc : 0.872, Sensitive_Loss : 0.07519, Sensitive_Acc : 15.100, Run Time : 7.35 sec
INFO:root:2024-04-26 15:32:47, Train, Epoch : 9, Step : 5270, Loss : 0.26068, Acc : 0.891, Sensitive_Loss : 0.06448, Sensitive_Acc : 16.700, Run Time : 6.47 sec
INFO:root:2024-04-26 15:32:55, Train, Epoch : 9, Step : 5280, Loss : 0.24772, Acc : 0.916, Sensitive_Loss : 0.07636, Sensitive_Acc : 16.900, Run Time : 7.44 sec
INFO:root:2024-04-26 15:33:02, Train, Epoch : 9, Step : 5290, Loss : 0.27389, Acc : 0.881, Sensitive_Loss : 0.07240, Sensitive_Acc : 17.400, Run Time : 7.21 sec
INFO:root:2024-04-26 15:33:09, Train, Epoch : 9, Step : 5300, Loss : 0.27274, Acc : 0.887, Sensitive_Loss : 0.06933, Sensitive_Acc : 16.400, Run Time : 7.25 sec
INFO:root:2024-04-26 15:34:41, Dev, Step : 5300, Loss : 0.47310, Acc : 0.814, Auc : 0.900, Sensitive_Loss : 0.12764, Sensitive_Acc : 16.807, Sensitive_Auc : 0.984, Mean auc: 0.900, Run Time : 91.98 sec
INFO:root:2024-04-26 15:34:47, Train, Epoch : 9, Step : 5310, Loss : 0.25631, Acc : 0.897, Sensitive_Loss : 0.07120, Sensitive_Acc : 15.600, Run Time : 97.68 sec
INFO:root:2024-04-26 15:34:54, Train, Epoch : 9, Step : 5320, Loss : 0.26310, Acc : 0.903, Sensitive_Loss : 0.09026, Sensitive_Acc : 15.900, Run Time : 7.01 sec
INFO:root:2024-04-26 15:35:02, Train, Epoch : 9, Step : 5330, Loss : 0.23538, Acc : 0.891, Sensitive_Loss : 0.08565, Sensitive_Acc : 15.300, Run Time : 7.97 sec
INFO:root:2024-04-26 15:35:08, Train, Epoch : 9, Step : 5340, Loss : 0.26226, Acc : 0.894, Sensitive_Loss : 0.09133, Sensitive_Acc : 17.000, Run Time : 6.52 sec
INFO:root:2024-04-26 15:35:15, Train, Epoch : 9, Step : 5350, Loss : 0.27055, Acc : 0.884, Sensitive_Loss : 0.08844, Sensitive_Acc : 16.300, Run Time : 7.23 sec
INFO:root:2024-04-26 15:35:23, Train, Epoch : 9, Step : 5360, Loss : 0.28128, Acc : 0.887, Sensitive_Loss : 0.08961, Sensitive_Acc : 16.500, Run Time : 7.19 sec
INFO:root:2024-04-26 15:35:30, Train, Epoch : 9, Step : 5370, Loss : 0.30485, Acc : 0.869, Sensitive_Loss : 0.09939, Sensitive_Acc : 18.100, Run Time : 7.03 sec
INFO:root:2024-04-26 15:35:37, Train, Epoch : 9, Step : 5380, Loss : 0.25632, Acc : 0.900, Sensitive_Loss : 0.08816, Sensitive_Acc : 15.400, Run Time : 7.33 sec
INFO:root:2024-04-26 15:35:44, Train, Epoch : 9, Step : 5390, Loss : 0.25660, Acc : 0.887, Sensitive_Loss : 0.07864, Sensitive_Acc : 18.200, Run Time : 6.93 sec
INFO:root:2024-04-26 15:35:52, Train, Epoch : 9, Step : 5400, Loss : 0.30426, Acc : 0.872, Sensitive_Loss : 0.09007, Sensitive_Acc : 16.400, Run Time : 7.58 sec
INFO:root:2024-04-26 15:37:24, Dev, Step : 5400, Loss : 0.47896, Acc : 0.811, Auc : 0.897, Sensitive_Loss : 0.13479, Sensitive_Acc : 16.750, Sensitive_Auc : 0.985, Mean auc: 0.897, Run Time : 92.27 sec
INFO:root:2024-04-26 15:37:30, Train, Epoch : 9, Step : 5410, Loss : 0.28978, Acc : 0.887, Sensitive_Loss : 0.13210, Sensitive_Acc : 16.200, Run Time : 98.06 sec
INFO:root:2024-04-26 15:37:37, Train, Epoch : 9, Step : 5420, Loss : 0.24529, Acc : 0.881, Sensitive_Loss : 0.12135, Sensitive_Acc : 17.100, Run Time : 7.24 sec
INFO:root:2024-04-26 15:37:44, Train, Epoch : 9, Step : 5430, Loss : 0.24707, Acc : 0.900, Sensitive_Loss : 0.13816, Sensitive_Acc : 16.800, Run Time : 7.33 sec
INFO:root:2024-04-26 15:37:51, Train, Epoch : 9, Step : 5440, Loss : 0.23176, Acc : 0.900, Sensitive_Loss : 0.09152, Sensitive_Acc : 17.100, Run Time : 7.20 sec
INFO:root:2024-04-26 15:37:59, Train, Epoch : 9, Step : 5450, Loss : 0.29899, Acc : 0.853, Sensitive_Loss : 0.09456, Sensitive_Acc : 15.100, Run Time : 7.18 sec
INFO:root:2024-04-26 15:38:06, Train, Epoch : 9, Step : 5460, Loss : 0.24037, Acc : 0.897, Sensitive_Loss : 0.08924, Sensitive_Acc : 15.900, Run Time : 7.18 sec
INFO:root:2024-04-26 15:38:13, Train, Epoch : 9, Step : 5470, Loss : 0.26519, Acc : 0.884, Sensitive_Loss : 0.10141, Sensitive_Acc : 16.500, Run Time : 7.24 sec
INFO:root:2024-04-26 15:38:20, Train, Epoch : 9, Step : 5480, Loss : 0.37081, Acc : 0.828, Sensitive_Loss : 0.11266, Sensitive_Acc : 17.300, Run Time : 7.28 sec
INFO:root:2024-04-26 15:38:27, Train, Epoch : 9, Step : 5490, Loss : 0.23292, Acc : 0.912, Sensitive_Loss : 0.08684, Sensitive_Acc : 14.700, Run Time : 6.99 sec
INFO:root:2024-04-26 15:38:35, Train, Epoch : 9, Step : 5500, Loss : 0.27738, Acc : 0.884, Sensitive_Loss : 0.10819, Sensitive_Acc : 17.200, Run Time : 7.47 sec
INFO:root:2024-04-26 15:40:07, Dev, Step : 5500, Loss : 0.44755, Acc : 0.827, Auc : 0.902, Sensitive_Loss : 0.10901, Sensitive_Acc : 16.850, Sensitive_Auc : 0.988, Mean auc: 0.902, Run Time : 92.47 sec
INFO:root:2024-04-26 15:40:13, Train, Epoch : 9, Step : 5510, Loss : 0.35751, Acc : 0.847, Sensitive_Loss : 0.09041, Sensitive_Acc : 16.600, Run Time : 98.47 sec
INFO:root:2024-04-26 15:40:20, Train, Epoch : 9, Step : 5520, Loss : 0.29700, Acc : 0.878, Sensitive_Loss : 0.09354, Sensitive_Acc : 15.100, Run Time : 7.28 sec
INFO:root:2024-04-26 15:40:27, Train, Epoch : 9, Step : 5530, Loss : 0.21914, Acc : 0.916, Sensitive_Loss : 0.09370, Sensitive_Acc : 17.300, Run Time : 7.02 sec
INFO:root:2024-04-26 15:40:35, Train, Epoch : 9, Step : 5540, Loss : 0.23455, Acc : 0.894, Sensitive_Loss : 0.09511, Sensitive_Acc : 15.300, Run Time : 7.21 sec
INFO:root:2024-04-26 15:40:42, Train, Epoch : 9, Step : 5550, Loss : 0.23672, Acc : 0.909, Sensitive_Loss : 0.08155, Sensitive_Acc : 15.600, Run Time : 7.15 sec
INFO:root:2024-04-26 15:40:49, Train, Epoch : 9, Step : 5560, Loss : 0.32741, Acc : 0.869, Sensitive_Loss : 0.12594, Sensitive_Acc : 15.800, Run Time : 7.01 sec
INFO:root:2024-04-26 15:40:56, Train, Epoch : 9, Step : 5570, Loss : 0.26599, Acc : 0.884, Sensitive_Loss : 0.11074, Sensitive_Acc : 16.700, Run Time : 7.44 sec
INFO:root:2024-04-26 15:41:03, Train, Epoch : 9, Step : 5580, Loss : 0.30785, Acc : 0.891, Sensitive_Loss : 0.08018, Sensitive_Acc : 16.500, Run Time : 7.18 sec
INFO:root:2024-04-26 15:41:10, Train, Epoch : 9, Step : 5590, Loss : 0.27824, Acc : 0.863, Sensitive_Loss : 0.12817, Sensitive_Acc : 16.600, Run Time : 6.81 sec
INFO:root:2024-04-26 15:41:18, Train, Epoch : 9, Step : 5600, Loss : 0.33258, Acc : 0.853, Sensitive_Loss : 0.11079, Sensitive_Acc : 17.400, Run Time : 7.67 sec
INFO:root:2024-04-26 15:42:50, Dev, Step : 5600, Loss : 0.43426, Acc : 0.822, Auc : 0.900, Sensitive_Loss : 0.10848, Sensitive_Acc : 16.850, Sensitive_Auc : 0.989, Mean auc: 0.900, Run Time : 92.15 sec
INFO:root:2024-04-26 15:42:57, Train, Epoch : 9, Step : 5610, Loss : 0.25430, Acc : 0.891, Sensitive_Loss : 0.11973, Sensitive_Acc : 16.800, Run Time : 98.72 sec
INFO:root:2024-04-26 15:43:03, Train, Epoch : 9, Step : 5620, Loss : 0.25990, Acc : 0.875, Sensitive_Loss : 0.08920, Sensitive_Acc : 16.800, Run Time : 6.44 sec
INFO:root:2024-04-26 15:43:10, Train, Epoch : 9, Step : 5630, Loss : 0.27432, Acc : 0.869, Sensitive_Loss : 0.09841, Sensitive_Acc : 16.300, Run Time : 6.64 sec
INFO:root:2024-04-26 15:44:43
INFO:root:y_pred: [0.2963881  0.948333   0.10535245 ... 0.83109325 0.00735317 0.90199065]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.5962268e-01 3.2374312e-04 7.4878498e-04 2.2360628e-06 9.9520797e-01
 3.2493426e-09 9.9229324e-01 9.9191332e-01 5.1198145e-05 9.3216491e-01
 9.8724335e-01 9.9810863e-01 9.8755670e-01 9.5392495e-01 1.7984281e-03
 9.2958218e-01 9.9959272e-01 1.1748441e-03 8.2029682e-01 9.9226177e-01
 9.5987666e-01 8.2657095e-03 9.9713588e-01 8.3438170e-01 9.9437970e-01
 7.5628793e-01 1.6045024e-06 9.8160321e-01 9.8081124e-01 3.6508238e-01
 4.7053588e-03 2.1599406e-02 1.2853830e-04 3.6229363e-03 4.1432370e-02
 1.3933454e-04 2.7451587e-03 2.5760086e-04 9.8617017e-01 9.6042091e-01
 5.7129179e-10 2.3557532e-06 9.7517222e-01 5.8271617e-06 9.9732357e-01
 9.9532926e-01 9.8011029e-01 9.8703694e-01 1.6366856e-03 9.9293500e-01
 9.9908566e-01 7.2117435e-04 1.6129943e-02 5.9124263e-06 9.8547989e-06
 4.8174807e-05 3.0851062e-02 7.8115892e-03 2.5706278e-04 3.7979624e-01
 2.9651481e-03 1.7717948e-02 5.1755278e-06 9.7342211e-01 1.8243099e-03
 9.9702102e-01 1.0336449e-06 9.9306113e-01 8.5657269e-01 6.6630733e-01
 8.5619819e-01 6.4585817e-01 2.1697600e-04 1.9375629e-04 4.6560253e-06
 2.6256369e-06 1.4277805e-04 2.6805252e-01 5.7442767e-06 9.7631866e-01
 9.9487066e-01 1.3524286e-05 5.7852447e-01 1.2911209e-04 9.8141104e-01
 6.7046694e-02 2.7741067e-05 7.9611711e-02 9.2871231e-01 9.8676723e-01
 9.8254150e-01 1.7353307e-02 7.4528684e-03 9.9667573e-01 5.9783162e-07
 1.7178663e-03 9.9145973e-01 9.8017621e-01 2.4960661e-06 1.9455614e-03
 9.7164965e-01 9.6408325e-01 9.9759728e-01 9.6983016e-01 6.2426072e-05
 1.6287739e-05 8.8236344e-01 9.3583471e-01 8.8428426e-01 5.0333448e-08
 9.3174142e-01 9.9086273e-01 2.3974628e-04 9.9647480e-01 9.6936363e-01
 9.8281950e-01 7.1923113e-01 9.9317330e-01 7.6505348e-06 1.5883620e-01
 9.9803454e-01 9.8597461e-01 1.5123968e-05 8.8294381e-01 9.9111748e-01
 3.4523576e-02 9.9194109e-01 1.2859849e-04 6.2188022e-03 9.8253781e-01
 9.8652762e-01 1.9132673e-04 1.3235411e-04 2.1685697e-03 9.9009001e-01
 9.9866831e-01 9.5702881e-01 2.9988107e-03 8.1478786e-03 9.8284054e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 15:44:44, Dev, Step : 5634, Loss : 0.43326, Acc : 0.829, Auc : 0.901, Sensitive_Loss : 0.10636, Sensitive_Acc : 16.850, Sensitive_Auc : 0.990, Mean auc: 0.901, Run Time : 91.50 sec
INFO:root:2024-04-26 15:44:51, Train, Epoch : 10, Step : 5640, Loss : 0.15451, Acc : 0.544, Sensitive_Loss : 0.04312, Sensitive_Acc : 10.100, Run Time : 6.83 sec
INFO:root:2024-04-26 15:44:57, Train, Epoch : 10, Step : 5650, Loss : 0.31102, Acc : 0.869, Sensitive_Loss : 0.10590, Sensitive_Acc : 15.900, Run Time : 5.96 sec
INFO:root:2024-04-26 15:45:05, Train, Epoch : 10, Step : 5660, Loss : 0.31513, Acc : 0.863, Sensitive_Loss : 0.10991, Sensitive_Acc : 16.700, Run Time : 7.32 sec
INFO:root:2024-04-26 15:45:12, Train, Epoch : 10, Step : 5670, Loss : 0.22738, Acc : 0.894, Sensitive_Loss : 0.08365, Sensitive_Acc : 16.000, Run Time : 7.13 sec
INFO:root:2024-04-26 15:45:19, Train, Epoch : 10, Step : 5680, Loss : 0.26758, Acc : 0.878, Sensitive_Loss : 0.13508, Sensitive_Acc : 17.800, Run Time : 7.24 sec
INFO:root:2024-04-26 15:45:26, Train, Epoch : 10, Step : 5690, Loss : 0.27335, Acc : 0.894, Sensitive_Loss : 0.09894, Sensitive_Acc : 15.900, Run Time : 7.30 sec
INFO:root:2024-04-26 15:45:33, Train, Epoch : 10, Step : 5700, Loss : 0.22821, Acc : 0.909, Sensitive_Loss : 0.10616, Sensitive_Acc : 18.600, Run Time : 7.03 sec
INFO:root:2024-04-26 15:47:05, Dev, Step : 5700, Loss : 0.46732, Acc : 0.813, Auc : 0.896, Sensitive_Loss : 0.12949, Sensitive_Acc : 16.807, Sensitive_Auc : 0.985, Mean auc: 0.896, Run Time : 91.60 sec
INFO:root:2024-04-26 15:47:11, Train, Epoch : 10, Step : 5710, Loss : 0.24452, Acc : 0.903, Sensitive_Loss : 0.07745, Sensitive_Acc : 16.200, Run Time : 98.02 sec
INFO:root:2024-04-26 15:47:18, Train, Epoch : 10, Step : 5720, Loss : 0.25061, Acc : 0.884, Sensitive_Loss : 0.07059, Sensitive_Acc : 15.500, Run Time : 6.26 sec
INFO:root:2024-04-26 15:47:25, Train, Epoch : 10, Step : 5730, Loss : 0.27081, Acc : 0.887, Sensitive_Loss : 0.07843, Sensitive_Acc : 16.300, Run Time : 7.05 sec
INFO:root:2024-04-26 15:47:32, Train, Epoch : 10, Step : 5740, Loss : 0.26190, Acc : 0.897, Sensitive_Loss : 0.06938, Sensitive_Acc : 15.800, Run Time : 6.87 sec
INFO:root:2024-04-26 15:47:39, Train, Epoch : 10, Step : 5750, Loss : 0.21265, Acc : 0.900, Sensitive_Loss : 0.10340, Sensitive_Acc : 17.100, Run Time : 7.16 sec
INFO:root:2024-04-26 15:47:46, Train, Epoch : 10, Step : 5760, Loss : 0.21512, Acc : 0.912, Sensitive_Loss : 0.07685, Sensitive_Acc : 17.000, Run Time : 7.12 sec
INFO:root:2024-04-26 15:47:53, Train, Epoch : 10, Step : 5770, Loss : 0.26481, Acc : 0.875, Sensitive_Loss : 0.06237, Sensitive_Acc : 15.800, Run Time : 7.12 sec
INFO:root:2024-04-26 15:48:00, Train, Epoch : 10, Step : 5780, Loss : 0.26510, Acc : 0.891, Sensitive_Loss : 0.09943, Sensitive_Acc : 16.200, Run Time : 7.40 sec
INFO:root:2024-04-26 15:48:07, Train, Epoch : 10, Step : 5790, Loss : 0.24477, Acc : 0.909, Sensitive_Loss : 0.10593, Sensitive_Acc : 18.000, Run Time : 6.98 sec
INFO:root:2024-04-26 15:48:14, Train, Epoch : 10, Step : 5800, Loss : 0.25934, Acc : 0.891, Sensitive_Loss : 0.09281, Sensitive_Acc : 16.100, Run Time : 6.66 sec
INFO:root:2024-04-26 15:49:46, Dev, Step : 5800, Loss : 0.44128, Acc : 0.822, Auc : 0.898, Sensitive_Loss : 0.11431, Sensitive_Acc : 16.864, Sensitive_Auc : 0.988, Mean auc: 0.898, Run Time : 91.54 sec
INFO:root:2024-04-26 15:49:51, Train, Epoch : 10, Step : 5810, Loss : 0.24338, Acc : 0.912, Sensitive_Loss : 0.14030, Sensitive_Acc : 15.800, Run Time : 97.13 sec
INFO:root:2024-04-26 15:49:59, Train, Epoch : 10, Step : 5820, Loss : 0.22613, Acc : 0.919, Sensitive_Loss : 0.10660, Sensitive_Acc : 16.200, Run Time : 7.70 sec
INFO:root:2024-04-26 15:50:06, Train, Epoch : 10, Step : 5830, Loss : 0.25398, Acc : 0.881, Sensitive_Loss : 0.08337, Sensitive_Acc : 16.900, Run Time : 7.53 sec
INFO:root:2024-04-26 15:50:13, Train, Epoch : 10, Step : 5840, Loss : 0.23970, Acc : 0.906, Sensitive_Loss : 0.07569, Sensitive_Acc : 16.400, Run Time : 6.88 sec
INFO:root:2024-04-26 15:50:21, Train, Epoch : 10, Step : 5850, Loss : 0.23971, Acc : 0.900, Sensitive_Loss : 0.08026, Sensitive_Acc : 17.600, Run Time : 7.34 sec
INFO:root:2024-04-26 15:50:28, Train, Epoch : 10, Step : 5860, Loss : 0.21759, Acc : 0.897, Sensitive_Loss : 0.09371, Sensitive_Acc : 16.600, Run Time : 7.01 sec
INFO:root:2024-04-26 15:50:35, Train, Epoch : 10, Step : 5870, Loss : 0.24950, Acc : 0.906, Sensitive_Loss : 0.08690, Sensitive_Acc : 15.900, Run Time : 6.96 sec
INFO:root:2024-04-26 15:50:43, Train, Epoch : 10, Step : 5880, Loss : 0.26555, Acc : 0.909, Sensitive_Loss : 0.09117, Sensitive_Acc : 16.700, Run Time : 7.87 sec
INFO:root:2024-04-26 15:50:51, Train, Epoch : 10, Step : 5890, Loss : 0.27007, Acc : 0.878, Sensitive_Loss : 0.08732, Sensitive_Acc : 15.500, Run Time : 8.14 sec
INFO:root:2024-04-26 15:50:58, Train, Epoch : 10, Step : 5900, Loss : 0.26777, Acc : 0.884, Sensitive_Loss : 0.08513, Sensitive_Acc : 15.900, Run Time : 7.11 sec
INFO:root:2024-04-26 15:52:29, Dev, Step : 5900, Loss : 0.45976, Acc : 0.818, Auc : 0.899, Sensitive_Loss : 0.12240, Sensitive_Acc : 16.807, Sensitive_Auc : 0.986, Mean auc: 0.899, Run Time : 91.66 sec
INFO:root:2024-04-26 15:52:35, Train, Epoch : 10, Step : 5910, Loss : 0.21211, Acc : 0.906, Sensitive_Loss : 0.08594, Sensitive_Acc : 16.300, Run Time : 97.60 sec
INFO:root:2024-04-26 15:52:43, Train, Epoch : 10, Step : 5920, Loss : 0.21945, Acc : 0.906, Sensitive_Loss : 0.11864, Sensitive_Acc : 16.800, Run Time : 7.59 sec
INFO:root:2024-04-26 15:52:50, Train, Epoch : 10, Step : 5930, Loss : 0.26011, Acc : 0.903, Sensitive_Loss : 0.07585, Sensitive_Acc : 15.900, Run Time : 7.36 sec
INFO:root:2024-04-26 15:52:58, Train, Epoch : 10, Step : 5940, Loss : 0.21696, Acc : 0.894, Sensitive_Loss : 0.10285, Sensitive_Acc : 16.500, Run Time : 7.28 sec
INFO:root:2024-04-26 15:53:06, Train, Epoch : 10, Step : 5950, Loss : 0.23827, Acc : 0.894, Sensitive_Loss : 0.08171, Sensitive_Acc : 16.900, Run Time : 8.02 sec
INFO:root:2024-04-26 15:53:13, Train, Epoch : 10, Step : 5960, Loss : 0.23288, Acc : 0.900, Sensitive_Loss : 0.08774, Sensitive_Acc : 16.900, Run Time : 7.05 sec
INFO:root:2024-04-26 15:53:20, Train, Epoch : 10, Step : 5970, Loss : 0.31481, Acc : 0.875, Sensitive_Loss : 0.08455, Sensitive_Acc : 15.000, Run Time : 6.95 sec
INFO:root:2024-04-26 15:53:27, Train, Epoch : 10, Step : 5980, Loss : 0.25256, Acc : 0.909, Sensitive_Loss : 0.15916, Sensitive_Acc : 16.500, Run Time : 6.94 sec
INFO:root:2024-04-26 15:53:34, Train, Epoch : 10, Step : 5990, Loss : 0.23978, Acc : 0.878, Sensitive_Loss : 0.10199, Sensitive_Acc : 18.600, Run Time : 7.60 sec
INFO:root:2024-04-26 15:53:41, Train, Epoch : 10, Step : 6000, Loss : 0.29689, Acc : 0.872, Sensitive_Loss : 0.10231, Sensitive_Acc : 15.500, Run Time : 6.90 sec
INFO:root:2024-04-26 15:55:14, Dev, Step : 6000, Loss : 0.46864, Acc : 0.812, Auc : 0.897, Sensitive_Loss : 0.13320, Sensitive_Acc : 16.807, Sensitive_Auc : 0.985, Mean auc: 0.897, Run Time : 93.05 sec
INFO:root:2024-04-26 15:55:20, Train, Epoch : 10, Step : 6010, Loss : 0.22340, Acc : 0.897, Sensitive_Loss : 0.06413, Sensitive_Acc : 16.900, Run Time : 99.35 sec
INFO:root:2024-04-26 15:55:27, Train, Epoch : 10, Step : 6020, Loss : 0.26503, Acc : 0.903, Sensitive_Loss : 0.05761, Sensitive_Acc : 15.900, Run Time : 6.93 sec
INFO:root:2024-04-26 15:55:35, Train, Epoch : 10, Step : 6030, Loss : 0.22478, Acc : 0.897, Sensitive_Loss : 0.09519, Sensitive_Acc : 15.500, Run Time : 7.39 sec
INFO:root:2024-04-26 15:55:42, Train, Epoch : 10, Step : 6040, Loss : 0.27395, Acc : 0.853, Sensitive_Loss : 0.07513, Sensitive_Acc : 16.700, Run Time : 6.82 sec
INFO:root:2024-04-26 15:55:49, Train, Epoch : 10, Step : 6050, Loss : 0.25919, Acc : 0.866, Sensitive_Loss : 0.08439, Sensitive_Acc : 17.600, Run Time : 7.32 sec
INFO:root:2024-04-26 15:55:56, Train, Epoch : 10, Step : 6060, Loss : 0.31260, Acc : 0.859, Sensitive_Loss : 0.10058, Sensitive_Acc : 15.300, Run Time : 7.41 sec
INFO:root:2024-04-26 15:56:03, Train, Epoch : 10, Step : 6070, Loss : 0.26477, Acc : 0.878, Sensitive_Loss : 0.10559, Sensitive_Acc : 15.900, Run Time : 6.98 sec
INFO:root:2024-04-26 15:56:10, Train, Epoch : 10, Step : 6080, Loss : 0.24988, Acc : 0.909, Sensitive_Loss : 0.08029, Sensitive_Acc : 15.900, Run Time : 7.13 sec
INFO:root:2024-04-26 15:56:18, Train, Epoch : 10, Step : 6090, Loss : 0.26361, Acc : 0.900, Sensitive_Loss : 0.08227, Sensitive_Acc : 17.300, Run Time : 7.10 sec
INFO:root:2024-04-26 15:56:26, Train, Epoch : 10, Step : 6100, Loss : 0.25222, Acc : 0.894, Sensitive_Loss : 0.15479, Sensitive_Acc : 15.200, Run Time : 7.96 sec
INFO:root:2024-04-26 15:57:59, Dev, Step : 6100, Loss : 0.46034, Acc : 0.816, Auc : 0.897, Sensitive_Loss : 0.12418, Sensitive_Acc : 16.807, Sensitive_Auc : 0.987, Mean auc: 0.897, Run Time : 93.50 sec
INFO:root:2024-04-26 15:58:04, Train, Epoch : 10, Step : 6110, Loss : 0.29663, Acc : 0.884, Sensitive_Loss : 0.10106, Sensitive_Acc : 17.200, Run Time : 98.91 sec
INFO:root:2024-04-26 15:58:12, Train, Epoch : 10, Step : 6120, Loss : 0.28214, Acc : 0.881, Sensitive_Loss : 0.09710, Sensitive_Acc : 15.300, Run Time : 7.95 sec
INFO:root:2024-04-26 15:58:19, Train, Epoch : 10, Step : 6130, Loss : 0.23088, Acc : 0.925, Sensitive_Loss : 0.08557, Sensitive_Acc : 18.000, Run Time : 7.02 sec
INFO:root:2024-04-26 15:58:27, Train, Epoch : 10, Step : 6140, Loss : 0.27654, Acc : 0.903, Sensitive_Loss : 0.07233, Sensitive_Acc : 17.000, Run Time : 7.20 sec
INFO:root:2024-04-26 15:58:34, Train, Epoch : 10, Step : 6150, Loss : 0.29709, Acc : 0.863, Sensitive_Loss : 0.09136, Sensitive_Acc : 16.000, Run Time : 7.50 sec
INFO:root:2024-04-26 15:58:42, Train, Epoch : 10, Step : 6160, Loss : 0.25390, Acc : 0.906, Sensitive_Loss : 0.11664, Sensitive_Acc : 15.600, Run Time : 8.07 sec
INFO:root:2024-04-26 15:58:49, Train, Epoch : 10, Step : 6170, Loss : 0.31497, Acc : 0.869, Sensitive_Loss : 0.10607, Sensitive_Acc : 16.800, Run Time : 7.08 sec
INFO:root:2024-04-26 15:58:57, Train, Epoch : 10, Step : 6180, Loss : 0.19519, Acc : 0.912, Sensitive_Loss : 0.06629, Sensitive_Acc : 15.600, Run Time : 7.43 sec
INFO:root:2024-04-26 15:59:03, Train, Epoch : 10, Step : 6190, Loss : 0.29122, Acc : 0.859, Sensitive_Loss : 0.11398, Sensitive_Acc : 16.100, Run Time : 6.76 sec
INFO:root:2024-04-26 15:59:11, Train, Epoch : 10, Step : 6200, Loss : 0.28533, Acc : 0.894, Sensitive_Loss : 0.05463, Sensitive_Acc : 16.400, Run Time : 7.25 sec
INFO:root:2024-04-26 16:00:44, Dev, Step : 6200, Loss : 0.47627, Acc : 0.823, Auc : 0.897, Sensitive_Loss : 0.11694, Sensitive_Acc : 16.807, Sensitive_Auc : 0.988, Mean auc: 0.897, Run Time : 93.64 sec
INFO:root:2024-04-26 16:00:50, Train, Epoch : 10, Step : 6210, Loss : 0.26464, Acc : 0.887, Sensitive_Loss : 0.08749, Sensitive_Acc : 16.300, Run Time : 99.18 sec
INFO:root:2024-04-26 16:00:59, Train, Epoch : 10, Step : 6220, Loss : 0.28379, Acc : 0.875, Sensitive_Loss : 0.07900, Sensitive_Acc : 16.700, Run Time : 8.82 sec
INFO:root:2024-04-26 16:01:06, Train, Epoch : 10, Step : 6230, Loss : 0.24058, Acc : 0.887, Sensitive_Loss : 0.12075, Sensitive_Acc : 17.600, Run Time : 7.40 sec
INFO:root:2024-04-26 16:01:14, Train, Epoch : 10, Step : 6240, Loss : 0.17953, Acc : 0.925, Sensitive_Loss : 0.08064, Sensitive_Acc : 15.400, Run Time : 7.48 sec
INFO:root:2024-04-26 16:01:21, Train, Epoch : 10, Step : 6250, Loss : 0.26138, Acc : 0.900, Sensitive_Loss : 0.06311, Sensitive_Acc : 16.500, Run Time : 7.44 sec
INFO:root:2024-04-26 16:01:28, Train, Epoch : 10, Step : 6260, Loss : 0.22825, Acc : 0.906, Sensitive_Loss : 0.11603, Sensitive_Acc : 15.700, Run Time : 6.96 sec
INFO:root:2024-04-26 16:03:01
INFO:root:y_pred: [0.14124452 0.97085977 0.04859312 ... 0.8246218  0.00333451 0.92522246]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.79176283e-01 3.75916097e-05 9.95629234e-04 1.40552174e-06
 9.94663000e-01 5.06540365e-09 9.93526578e-01 9.96042490e-01
 1.98333742e-04 9.50204074e-01 9.96436000e-01 9.98559892e-01
 9.94815052e-01 9.73929644e-01 4.82482166e-04 9.40621138e-01
 9.99755442e-01 3.75174888e-04 8.74291658e-01 9.94221091e-01
 9.71482992e-01 6.24837764e-02 9.98680174e-01 9.11200166e-01
 9.97412741e-01 8.25888574e-01 1.52956900e-07 9.88414168e-01
 9.89004314e-01 5.67564607e-01 1.19720038e-03 3.17935012e-02
 1.10884481e-04 6.44601882e-04 4.37225066e-02 1.96521883e-04
 4.51650849e-04 4.27872765e-05 9.90366936e-01 9.66106355e-01
 3.55021124e-09 8.31007674e-06 9.86407757e-01 4.79024447e-06
 9.98571634e-01 9.92283404e-01 9.91732597e-01 9.94887650e-01
 2.85342947e-04 9.95689690e-01 9.99117792e-01 1.98233538e-04
 5.94782876e-03 8.20157675e-06 3.62723381e-06 6.85068380e-06
 4.63484088e-03 7.30582047e-03 8.46134499e-05 5.59965968e-01
 2.38214992e-03 1.38142947e-02 6.24428509e-08 9.85455453e-01
 7.62391952e-04 9.96757209e-01 4.22585686e-07 9.96290565e-01
 8.57100129e-01 8.13839138e-01 9.55623448e-01 7.79970467e-01
 2.72300473e-04 9.41633130e-04 8.63096034e-07 2.79670576e-06
 1.03816776e-04 4.53689784e-01 1.06424852e-06 9.87988353e-01
 9.96412337e-01 1.66421341e-05 7.07239151e-01 1.71254127e-04
 9.90680158e-01 2.36386761e-01 2.27500823e-06 3.69506259e-03
 9.39790308e-01 9.90645170e-01 9.88963366e-01 2.24813037e-02
 1.65775530e-02 9.97566223e-01 4.27292434e-06 1.33383682e-03
 9.92712677e-01 9.85970438e-01 1.08704842e-06 3.06937575e-01
 9.76201832e-01 9.68662679e-01 9.98424411e-01 9.80403841e-01
 1.06185369e-04 1.17676636e-05 8.94674361e-01 9.32978988e-01
 9.35678244e-01 1.55523594e-08 9.34309781e-01 9.94497240e-01
 6.16774894e-04 9.97707725e-01 9.72332239e-01 9.91105497e-01
 6.86760604e-01 9.98144388e-01 1.13055751e-06 1.98562384e-01
 9.99010921e-01 9.87778485e-01 7.92158899e-05 9.10393596e-01
 9.95280802e-01 1.09463349e-01 9.95575070e-01 8.75682104e-04
 1.74963977e-02 9.90038753e-01 9.93126512e-01 2.72196135e-04
 1.69190425e-05 7.13798543e-03 9.91161942e-01 9.99190390e-01
 9.60767686e-01 2.16134777e-03 1.27453133e-02 9.91410851e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 16:03:01, Dev, Step : 6260, Loss : 0.48098, Acc : 0.812, Auc : 0.897, Sensitive_Loss : 0.11334, Sensitive_Acc : 16.807, Sensitive_Auc : 0.987, Mean auc: 0.897, Run Time : 92.77 sec
