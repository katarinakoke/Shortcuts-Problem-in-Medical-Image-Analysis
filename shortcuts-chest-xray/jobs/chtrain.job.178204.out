Running on desktop25:
stdin: is not a tty
/home/pmen/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
0
Using the specified args:
Namespace(cfg_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_pmen.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_train.csv",
    "dev_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_val.csv",
    "pred_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Biased_Sex_1_pos01.csv",
    "pred_model": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Biased_Sex_1_pos011.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.05,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-26 15:29:23, Train, Epoch : 1, Step : 10, Loss : 0.65289, Acc : 0.588, Sensitive_Loss : 0.70004, Sensitive_Acc : 16.400, Run Time : 20.31 sec
INFO:root:2024-04-26 15:29:36, Train, Epoch : 1, Step : 20, Loss : 0.61554, Acc : 0.706, Sensitive_Loss : 0.61655, Sensitive_Acc : 15.200, Run Time : 13.06 sec
INFO:root:2024-04-26 15:29:53, Train, Epoch : 1, Step : 30, Loss : 0.57541, Acc : 0.703, Sensitive_Loss : 0.63999, Sensitive_Acc : 16.900, Run Time : 17.13 sec
INFO:root:2024-04-26 15:30:09, Train, Epoch : 1, Step : 40, Loss : 0.61547, Acc : 0.706, Sensitive_Loss : 0.50454, Sensitive_Acc : 16.000, Run Time : 16.02 sec
INFO:root:2024-04-26 15:30:24, Train, Epoch : 1, Step : 50, Loss : 0.52910, Acc : 0.753, Sensitive_Loss : 0.45925, Sensitive_Acc : 16.100, Run Time : 14.84 sec
INFO:root:2024-04-26 15:30:35, Train, Epoch : 1, Step : 60, Loss : 0.55025, Acc : 0.738, Sensitive_Loss : 0.45088, Sensitive_Acc : 16.700, Run Time : 11.41 sec
INFO:root:2024-04-26 15:30:47, Train, Epoch : 1, Step : 70, Loss : 0.63423, Acc : 0.716, Sensitive_Loss : 0.49583, Sensitive_Acc : 14.900, Run Time : 12.06 sec
INFO:root:2024-04-26 15:30:59, Train, Epoch : 1, Step : 80, Loss : 0.48810, Acc : 0.778, Sensitive_Loss : 0.40608, Sensitive_Acc : 14.500, Run Time : 11.75 sec
INFO:root:2024-04-26 15:31:11, Train, Epoch : 1, Step : 90, Loss : 0.44783, Acc : 0.762, Sensitive_Loss : 0.38235, Sensitive_Acc : 16.500, Run Time : 12.13 sec
INFO:root:2024-04-26 15:31:23, Train, Epoch : 1, Step : 100, Loss : 0.55604, Acc : 0.750, Sensitive_Loss : 0.35362, Sensitive_Acc : 16.300, Run Time : 12.14 sec
INFO:root:2024-04-26 15:34:12, Dev, Step : 100, Loss : 0.56724, Acc : 0.745, Auc : 0.846, Sensitive_Loss : 0.37986, Sensitive_Acc : 16.421, Sensitive_Auc : 0.939, Mean auc: 0.846, Run Time : 168.49 sec
INFO:root:2024-04-26 15:34:13, Best, Step : 100, Loss : 0.56724, Acc : 0.745, Auc : 0.846, Sensitive_Loss : 0.37986, Sensitive_Acc : 16.421, Sensitive_Auc : 0.939, Best Auc : 0.846
INFO:root:2024-04-26 15:34:21, Train, Epoch : 1, Step : 110, Loss : 0.52011, Acc : 0.759, Sensitive_Loss : 0.35876, Sensitive_Acc : 15.800, Run Time : 177.58 sec
INFO:root:2024-04-26 15:34:33, Train, Epoch : 1, Step : 120, Loss : 0.50965, Acc : 0.750, Sensitive_Loss : 0.38087, Sensitive_Acc : 15.900, Run Time : 11.82 sec
INFO:root:2024-04-26 15:34:44, Train, Epoch : 1, Step : 130, Loss : 0.48393, Acc : 0.744, Sensitive_Loss : 0.37392, Sensitive_Acc : 17.200, Run Time : 11.01 sec
INFO:root:2024-04-26 15:34:56, Train, Epoch : 1, Step : 140, Loss : 0.43470, Acc : 0.775, Sensitive_Loss : 0.35075, Sensitive_Acc : 15.300, Run Time : 11.71 sec
INFO:root:2024-04-26 15:35:08, Train, Epoch : 1, Step : 150, Loss : 0.58689, Acc : 0.738, Sensitive_Loss : 0.32561, Sensitive_Acc : 16.100, Run Time : 12.80 sec
INFO:root:2024-04-26 15:35:20, Train, Epoch : 1, Step : 160, Loss : 0.52930, Acc : 0.725, Sensitive_Loss : 0.30849, Sensitive_Acc : 15.700, Run Time : 11.80 sec
INFO:root:2024-04-26 15:35:32, Train, Epoch : 1, Step : 170, Loss : 0.51517, Acc : 0.747, Sensitive_Loss : 0.31261, Sensitive_Acc : 17.100, Run Time : 12.25 sec
INFO:root:2024-04-26 15:35:43, Train, Epoch : 1, Step : 180, Loss : 0.53513, Acc : 0.787, Sensitive_Loss : 0.28393, Sensitive_Acc : 16.000, Run Time : 11.02 sec
INFO:root:2024-04-26 15:35:55, Train, Epoch : 1, Step : 190, Loss : 0.46496, Acc : 0.812, Sensitive_Loss : 0.35885, Sensitive_Acc : 15.700, Run Time : 11.82 sec
INFO:root:2024-04-26 15:36:07, Train, Epoch : 1, Step : 200, Loss : 0.47960, Acc : 0.769, Sensitive_Loss : 0.38218, Sensitive_Acc : 15.800, Run Time : 11.62 sec
INFO:root:2024-04-26 15:38:52, Dev, Step : 200, Loss : 0.60512, Acc : 0.729, Auc : 0.854, Sensitive_Loss : 0.35863, Sensitive_Acc : 16.836, Sensitive_Auc : 0.950, Mean auc: 0.854, Run Time : 165.35 sec
INFO:root:2024-04-26 15:38:53, Best, Step : 200, Loss : 0.60512, Acc : 0.729, Auc : 0.854, Sensitive_Loss : 0.35863, Sensitive_Acc : 16.836, Sensitive_Auc : 0.950, Best Auc : 0.854
INFO:root:2024-04-26 15:39:02, Train, Epoch : 1, Step : 210, Loss : 0.45656, Acc : 0.778, Sensitive_Loss : 0.31869, Sensitive_Acc : 16.000, Run Time : 174.99 sec
INFO:root:2024-04-26 15:39:14, Train, Epoch : 1, Step : 220, Loss : 0.47446, Acc : 0.781, Sensitive_Loss : 0.35807, Sensitive_Acc : 15.000, Run Time : 12.22 sec
INFO:root:2024-04-26 15:39:26, Train, Epoch : 1, Step : 230, Loss : 0.60909, Acc : 0.738, Sensitive_Loss : 0.28823, Sensitive_Acc : 16.200, Run Time : 11.64 sec
INFO:root:2024-04-26 15:39:37, Train, Epoch : 1, Step : 240, Loss : 0.44369, Acc : 0.791, Sensitive_Loss : 0.31686, Sensitive_Acc : 16.000, Run Time : 11.64 sec
INFO:root:2024-04-26 15:39:48, Train, Epoch : 1, Step : 250, Loss : 0.45136, Acc : 0.784, Sensitive_Loss : 0.27141, Sensitive_Acc : 15.500, Run Time : 11.07 sec
INFO:root:2024-04-26 15:40:01, Train, Epoch : 1, Step : 260, Loss : 0.45843, Acc : 0.803, Sensitive_Loss : 0.24884, Sensitive_Acc : 15.800, Run Time : 12.54 sec
INFO:root:2024-04-26 15:40:12, Train, Epoch : 1, Step : 270, Loss : 0.54510, Acc : 0.744, Sensitive_Loss : 0.28165, Sensitive_Acc : 15.100, Run Time : 11.32 sec
INFO:root:2024-04-26 15:40:24, Train, Epoch : 1, Step : 280, Loss : 0.42515, Acc : 0.769, Sensitive_Loss : 0.26781, Sensitive_Acc : 16.500, Run Time : 11.34 sec
INFO:root:2024-04-26 15:40:35, Train, Epoch : 1, Step : 290, Loss : 0.61344, Acc : 0.722, Sensitive_Loss : 0.31358, Sensitive_Acc : 17.600, Run Time : 11.44 sec
INFO:root:2024-04-26 15:40:46, Train, Epoch : 1, Step : 300, Loss : 0.48044, Acc : 0.809, Sensitive_Loss : 0.30111, Sensitive_Acc : 14.800, Run Time : 11.30 sec
INFO:root:2024-04-26 15:43:22, Dev, Step : 300, Loss : 0.48187, Acc : 0.779, Auc : 0.866, Sensitive_Loss : 0.23929, Sensitive_Acc : 16.650, Sensitive_Auc : 0.973, Mean auc: 0.866, Run Time : 155.51 sec
INFO:root:2024-04-26 15:43:23, Best, Step : 300, Loss : 0.48187, Acc : 0.779, Auc : 0.866, Sensitive_Loss : 0.23929, Sensitive_Acc : 16.650, Sensitive_Auc : 0.973, Best Auc : 0.866
INFO:root:2024-04-26 15:43:31, Train, Epoch : 1, Step : 310, Loss : 0.48884, Acc : 0.775, Sensitive_Loss : 0.24249, Sensitive_Acc : 17.000, Run Time : 164.71 sec
INFO:root:2024-04-26 15:43:43, Train, Epoch : 1, Step : 320, Loss : 0.51048, Acc : 0.784, Sensitive_Loss : 0.26175, Sensitive_Acc : 16.300, Run Time : 12.29 sec
INFO:root:2024-04-26 15:43:58, Train, Epoch : 1, Step : 330, Loss : 0.41465, Acc : 0.825, Sensitive_Loss : 0.26234, Sensitive_Acc : 16.600, Run Time : 14.44 sec
INFO:root:2024-04-26 15:44:10, Train, Epoch : 1, Step : 340, Loss : 0.49450, Acc : 0.775, Sensitive_Loss : 0.21660, Sensitive_Acc : 16.800, Run Time : 12.43 sec
INFO:root:2024-04-26 15:44:23, Train, Epoch : 1, Step : 350, Loss : 0.45200, Acc : 0.781, Sensitive_Loss : 0.25454, Sensitive_Acc : 15.700, Run Time : 12.93 sec
INFO:root:2024-04-26 15:44:35, Train, Epoch : 1, Step : 360, Loss : 0.47051, Acc : 0.784, Sensitive_Loss : 0.28986, Sensitive_Acc : 16.300, Run Time : 12.18 sec
INFO:root:2024-04-26 15:44:48, Train, Epoch : 1, Step : 370, Loss : 0.43376, Acc : 0.819, Sensitive_Loss : 0.18356, Sensitive_Acc : 17.400, Run Time : 12.81 sec
INFO:root:2024-04-26 15:45:01, Train, Epoch : 1, Step : 380, Loss : 0.49577, Acc : 0.787, Sensitive_Loss : 0.23037, Sensitive_Acc : 16.300, Run Time : 12.91 sec
INFO:root:2024-04-26 15:45:14, Train, Epoch : 1, Step : 390, Loss : 0.45726, Acc : 0.769, Sensitive_Loss : 0.24936, Sensitive_Acc : 14.600, Run Time : 13.32 sec
INFO:root:2024-04-26 15:45:26, Train, Epoch : 1, Step : 400, Loss : 0.48793, Acc : 0.775, Sensitive_Loss : 0.23599, Sensitive_Acc : 16.500, Run Time : 11.44 sec
INFO:root:2024-04-26 15:48:01, Dev, Step : 400, Loss : 0.47422, Acc : 0.790, Auc : 0.867, Sensitive_Loss : 0.22488, Sensitive_Acc : 16.750, Sensitive_Auc : 0.971, Mean auc: 0.867, Run Time : 155.25 sec
INFO:root:2024-04-26 15:48:02, Best, Step : 400, Loss : 0.47422, Acc : 0.790, Auc : 0.867, Sensitive_Loss : 0.22488, Sensitive_Acc : 16.750, Sensitive_Auc : 0.971, Best Auc : 0.867
INFO:root:2024-04-26 15:48:11, Train, Epoch : 1, Step : 410, Loss : 0.47057, Acc : 0.812, Sensitive_Loss : 0.22675, Sensitive_Acc : 16.000, Run Time : 165.24 sec
INFO:root:2024-04-26 15:48:24, Train, Epoch : 1, Step : 420, Loss : 0.40802, Acc : 0.828, Sensitive_Loss : 0.15880, Sensitive_Acc : 14.800, Run Time : 12.72 sec
INFO:root:2024-04-26 15:48:35, Train, Epoch : 1, Step : 430, Loss : 0.41603, Acc : 0.809, Sensitive_Loss : 0.21655, Sensitive_Acc : 17.700, Run Time : 11.47 sec
INFO:root:2024-04-26 15:48:47, Train, Epoch : 1, Step : 440, Loss : 0.43300, Acc : 0.803, Sensitive_Loss : 0.18226, Sensitive_Acc : 16.600, Run Time : 12.15 sec
INFO:root:2024-04-26 15:49:00, Train, Epoch : 1, Step : 450, Loss : 0.54219, Acc : 0.753, Sensitive_Loss : 0.24654, Sensitive_Acc : 16.300, Run Time : 12.56 sec
INFO:root:2024-04-26 15:49:12, Train, Epoch : 1, Step : 460, Loss : 0.45630, Acc : 0.809, Sensitive_Loss : 0.22692, Sensitive_Acc : 14.100, Run Time : 12.32 sec
INFO:root:2024-04-26 15:49:25, Train, Epoch : 1, Step : 470, Loss : 0.51294, Acc : 0.797, Sensitive_Loss : 0.23048, Sensitive_Acc : 16.300, Run Time : 13.08 sec
INFO:root:2024-04-26 15:49:38, Train, Epoch : 1, Step : 480, Loss : 0.56239, Acc : 0.756, Sensitive_Loss : 0.21136, Sensitive_Acc : 16.000, Run Time : 13.01 sec
INFO:root:2024-04-26 15:49:51, Train, Epoch : 1, Step : 490, Loss : 0.50416, Acc : 0.750, Sensitive_Loss : 0.19864, Sensitive_Acc : 17.400, Run Time : 12.95 sec
INFO:root:2024-04-26 15:50:06, Train, Epoch : 1, Step : 500, Loss : 0.38647, Acc : 0.819, Sensitive_Loss : 0.18136, Sensitive_Acc : 15.900, Run Time : 14.60 sec
INFO:root:2024-04-26 15:52:42, Dev, Step : 500, Loss : 0.53715, Acc : 0.764, Auc : 0.888, Sensitive_Loss : 0.36033, Sensitive_Acc : 16.736, Sensitive_Auc : 0.979, Mean auc: 0.888, Run Time : 156.34 sec
INFO:root:2024-04-26 15:52:43, Best, Step : 500, Loss : 0.53715, Acc : 0.764, Auc : 0.888, Sensitive_Loss : 0.36033, Sensitive_Acc : 16.736, Sensitive_Auc : 0.979, Best Auc : 0.888
INFO:root:2024-04-26 15:52:51, Train, Epoch : 1, Step : 510, Loss : 0.43802, Acc : 0.791, Sensitive_Loss : 0.21176, Sensitive_Acc : 16.800, Run Time : 165.56 sec
INFO:root:2024-04-26 15:53:05, Train, Epoch : 1, Step : 520, Loss : 0.45425, Acc : 0.812, Sensitive_Loss : 0.19991, Sensitive_Acc : 15.800, Run Time : 13.68 sec
INFO:root:2024-04-26 15:53:17, Train, Epoch : 1, Step : 530, Loss : 0.45316, Acc : 0.778, Sensitive_Loss : 0.17039, Sensitive_Acc : 16.000, Run Time : 12.31 sec
INFO:root:2024-04-26 15:53:30, Train, Epoch : 1, Step : 540, Loss : 0.53293, Acc : 0.756, Sensitive_Loss : 0.21635, Sensitive_Acc : 16.000, Run Time : 12.40 sec
INFO:root:2024-04-26 15:53:42, Train, Epoch : 1, Step : 550, Loss : 0.42187, Acc : 0.794, Sensitive_Loss : 0.22438, Sensitive_Acc : 14.300, Run Time : 12.24 sec
INFO:root:2024-04-26 15:53:55, Train, Epoch : 1, Step : 560, Loss : 0.45655, Acc : 0.794, Sensitive_Loss : 0.24635, Sensitive_Acc : 16.300, Run Time : 12.49 sec
INFO:root:2024-04-26 15:54:07, Train, Epoch : 1, Step : 570, Loss : 0.39931, Acc : 0.838, Sensitive_Loss : 0.21293, Sensitive_Acc : 17.400, Run Time : 12.61 sec
INFO:root:2024-04-26 15:54:20, Train, Epoch : 1, Step : 580, Loss : 0.40854, Acc : 0.831, Sensitive_Loss : 0.23538, Sensitive_Acc : 16.000, Run Time : 13.09 sec
INFO:root:2024-04-26 15:54:33, Train, Epoch : 1, Step : 590, Loss : 0.55764, Acc : 0.766, Sensitive_Loss : 0.19770, Sensitive_Acc : 14.500, Run Time : 12.45 sec
INFO:root:2024-04-26 15:54:45, Train, Epoch : 1, Step : 600, Loss : 0.47323, Acc : 0.778, Sensitive_Loss : 0.21167, Sensitive_Acc : 16.600, Run Time : 12.54 sec
INFO:root:2024-04-26 15:57:22, Dev, Step : 600, Loss : 0.44732, Acc : 0.809, Auc : 0.886, Sensitive_Loss : 0.20162, Sensitive_Acc : 16.907, Sensitive_Auc : 0.982, Mean auc: 0.886, Run Time : 156.51 sec
INFO:root:2024-04-26 15:57:31, Train, Epoch : 1, Step : 610, Loss : 0.43230, Acc : 0.803, Sensitive_Loss : 0.19552, Sensitive_Acc : 16.700, Run Time : 165.46 sec
INFO:root:2024-04-26 15:57:45, Train, Epoch : 1, Step : 620, Loss : 0.42215, Acc : 0.803, Sensitive_Loss : 0.17609, Sensitive_Acc : 14.800, Run Time : 13.87 sec
INFO:root:2024-04-26 16:00:26
INFO:root:y_pred: [0.13574405 0.951434   0.07194753 ... 0.8877305  0.01044025 0.77450424]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.0032500e-01 2.5081628e-03 6.6375852e-02 5.1875500e-04 9.9942219e-01
 3.0775697e-03 9.9993825e-01 9.8035151e-01 1.6244715e-02 6.1919898e-01
 9.7437376e-01 9.9828136e-01 9.6676421e-01 7.8276271e-01 1.4223889e-02
 5.6897670e-01 9.9614489e-01 6.3661938e-03 1.6279039e-01 8.4583497e-01
 9.7969192e-01 1.8450288e-01 9.7793519e-01 8.6320955e-01 9.4475698e-01
 8.9155424e-01 6.0336199e-03 9.8534936e-01 9.8969895e-01 4.2116094e-01
 3.7799880e-02 3.3488601e-01 4.3584656e-02 9.9924728e-03 1.8677492e-01
 1.8569918e-02 9.2096269e-02 2.1045150e-02 9.6765667e-01 9.7092998e-01
 7.2319963e-05 1.1471485e-03 8.8570315e-01 6.0923221e-03 9.9956673e-01
 9.9451089e-01 9.9587053e-01 8.3780289e-01 1.7979175e-01 9.4479734e-01
 9.6958971e-01 1.4364077e-02 4.6439463e-01 1.0880614e-02 2.6723170e-03
 1.0723087e-02 2.1754107e-02 4.1096816e-03 4.5825737e-03 1.7332559e-02
 2.0368660e-03 2.1014104e-02 5.1787939e-02 3.1337988e-01 1.3875346e-01
 9.9921083e-01 1.9272899e-02 9.9184155e-01 9.0572631e-01 6.1114436e-01
 4.7157383e-01 1.0622102e-01 9.0364059e-03 2.9196141e-02 7.7454597e-02
 2.2547408e-03 4.4153556e-02 2.9575555e-02 1.5729541e-03 9.7542274e-01
 9.9837661e-01 5.1274588e-03 3.6544327e-02 9.2341602e-03 7.7662545e-01
 8.7783796e-01 8.3104633e-02 1.5180107e-02 8.5397446e-01 9.9310642e-01
 9.9577361e-01 6.4000329e-03 4.4905585e-03 9.8816699e-01 3.7548062e-01
 1.4258256e-02 9.3885392e-01 9.8419929e-01 4.9835509e-03 2.3657066e-01
 9.5176089e-01 4.7824869e-01 8.9686310e-01 9.8690599e-01 2.1612126e-01
 4.1235983e-01 9.7340292e-01 9.3121892e-01 8.2660556e-01 1.7414072e-03
 8.3127308e-01 9.8483747e-01 6.9810875e-02 9.8886067e-01 9.4845003e-01
 9.6938175e-01 6.6198397e-01 9.8073113e-01 8.7710293e-03 3.0837813e-01
 9.9385190e-01 9.8581046e-01 4.3609426e-03 9.4001770e-01 9.9931288e-01
 3.2492068e-01 9.9588960e-01 1.6576115e-02 1.1011722e-01 9.6648341e-01
 9.7861814e-01 1.9176325e-02 3.1387474e-02 4.7677024e-03 9.9810410e-01
 9.8075807e-01 4.3520248e-01 8.7709129e-03 9.1666207e-03 8.4732687e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 16:00:26, Dev, Step : 626, Loss : 0.44248, Acc : 0.808, Auc : 0.885, Sensitive_Loss : 0.16518, Sensitive_Acc : 16.993, Sensitive_Auc : 0.987, Mean auc: 0.885, Run Time : 153.63 sec
INFO:root:2024-04-26 16:00:34, Train, Epoch : 2, Step : 630, Loss : 0.24520, Acc : 0.312, Sensitive_Loss : 0.08947, Sensitive_Acc : 6.000, Run Time : 6.19 sec
INFO:root:2024-04-26 16:00:45, Train, Epoch : 2, Step : 640, Loss : 0.40205, Acc : 0.816, Sensitive_Loss : 0.16819, Sensitive_Acc : 15.700, Run Time : 11.52 sec
INFO:root:2024-04-26 16:00:57, Train, Epoch : 2, Step : 650, Loss : 0.38496, Acc : 0.800, Sensitive_Loss : 0.16785, Sensitive_Acc : 17.500, Run Time : 12.11 sec
INFO:root:2024-04-26 16:01:09, Train, Epoch : 2, Step : 660, Loss : 0.38160, Acc : 0.819, Sensitive_Loss : 0.19474, Sensitive_Acc : 17.600, Run Time : 12.04 sec
INFO:root:2024-04-26 16:01:21, Train, Epoch : 2, Step : 670, Loss : 0.49355, Acc : 0.769, Sensitive_Loss : 0.21107, Sensitive_Acc : 16.600, Run Time : 11.35 sec
INFO:root:2024-04-26 16:01:34, Train, Epoch : 2, Step : 680, Loss : 0.40523, Acc : 0.831, Sensitive_Loss : 0.12645, Sensitive_Acc : 16.600, Run Time : 12.94 sec
INFO:root:2024-04-26 16:01:46, Train, Epoch : 2, Step : 690, Loss : 0.48792, Acc : 0.778, Sensitive_Loss : 0.18959, Sensitive_Acc : 17.300, Run Time : 11.93 sec
INFO:root:2024-04-26 16:01:57, Train, Epoch : 2, Step : 700, Loss : 0.49087, Acc : 0.772, Sensitive_Loss : 0.18551, Sensitive_Acc : 14.800, Run Time : 11.04 sec
INFO:root:2024-04-26 16:04:33, Dev, Step : 700, Loss : 0.49478, Acc : 0.769, Auc : 0.881, Sensitive_Loss : 0.16768, Sensitive_Acc : 16.807, Sensitive_Auc : 0.987, Mean auc: 0.881, Run Time : 156.08 sec
INFO:root:2024-04-26 16:04:42, Train, Epoch : 2, Step : 710, Loss : 0.43549, Acc : 0.797, Sensitive_Loss : 0.19124, Sensitive_Acc : 16.100, Run Time : 165.00 sec
INFO:root:2024-04-26 16:04:54, Train, Epoch : 2, Step : 720, Loss : 0.37182, Acc : 0.816, Sensitive_Loss : 0.18928, Sensitive_Acc : 16.100, Run Time : 11.90 sec
INFO:root:2024-04-26 16:05:06, Train, Epoch : 2, Step : 730, Loss : 0.50944, Acc : 0.784, Sensitive_Loss : 0.16311, Sensitive_Acc : 15.400, Run Time : 12.18 sec
INFO:root:2024-04-26 16:05:18, Train, Epoch : 2, Step : 740, Loss : 0.51043, Acc : 0.766, Sensitive_Loss : 0.15841, Sensitive_Acc : 14.100, Run Time : 11.95 sec
INFO:root:2024-04-26 16:05:30, Train, Epoch : 2, Step : 750, Loss : 0.42976, Acc : 0.800, Sensitive_Loss : 0.18969, Sensitive_Acc : 15.700, Run Time : 12.18 sec
INFO:root:2024-04-26 16:05:43, Train, Epoch : 2, Step : 760, Loss : 0.41214, Acc : 0.806, Sensitive_Loss : 0.20034, Sensitive_Acc : 14.800, Run Time : 12.81 sec
INFO:root:2024-04-26 16:05:55, Train, Epoch : 2, Step : 770, Loss : 0.46418, Acc : 0.794, Sensitive_Loss : 0.17365, Sensitive_Acc : 16.200, Run Time : 12.69 sec
INFO:root:2024-04-26 16:06:07, Train, Epoch : 2, Step : 780, Loss : 0.41613, Acc : 0.816, Sensitive_Loss : 0.16725, Sensitive_Acc : 16.800, Run Time : 12.07 sec
INFO:root:2024-04-26 16:06:19, Train, Epoch : 2, Step : 790, Loss : 0.41905, Acc : 0.806, Sensitive_Loss : 0.19650, Sensitive_Acc : 17.400, Run Time : 11.98 sec
INFO:root:2024-04-26 16:06:31, Train, Epoch : 2, Step : 800, Loss : 0.40791, Acc : 0.809, Sensitive_Loss : 0.22552, Sensitive_Acc : 17.600, Run Time : 11.70 sec
INFO:root:2024-04-26 16:09:07, Dev, Step : 800, Loss : 0.48130, Acc : 0.788, Auc : 0.886, Sensitive_Loss : 0.21989, Sensitive_Acc : 16.836, Sensitive_Auc : 0.985, Mean auc: 0.886, Run Time : 155.70 sec
INFO:root:2024-04-26 16:09:15, Train, Epoch : 2, Step : 810, Loss : 0.45220, Acc : 0.806, Sensitive_Loss : 0.17075, Sensitive_Acc : 17.000, Run Time : 164.37 sec
INFO:root:2024-04-26 16:09:27, Train, Epoch : 2, Step : 820, Loss : 0.45332, Acc : 0.800, Sensitive_Loss : 0.21142, Sensitive_Acc : 15.500, Run Time : 11.90 sec
INFO:root:2024-04-26 16:09:39, Train, Epoch : 2, Step : 830, Loss : 0.37787, Acc : 0.875, Sensitive_Loss : 0.22616, Sensitive_Acc : 16.300, Run Time : 11.27 sec
INFO:root:2024-04-26 16:09:51, Train, Epoch : 2, Step : 840, Loss : 0.44154, Acc : 0.809, Sensitive_Loss : 0.16769, Sensitive_Acc : 14.900, Run Time : 12.14 sec
INFO:root:2024-04-26 16:10:03, Train, Epoch : 2, Step : 850, Loss : 0.39618, Acc : 0.825, Sensitive_Loss : 0.16927, Sensitive_Acc : 14.100, Run Time : 12.25 sec
INFO:root:2024-04-26 16:10:15, Train, Epoch : 2, Step : 860, Loss : 0.47426, Acc : 0.750, Sensitive_Loss : 0.18719, Sensitive_Acc : 15.500, Run Time : 11.51 sec
INFO:root:2024-04-26 16:10:27, Train, Epoch : 2, Step : 870, Loss : 0.40706, Acc : 0.831, Sensitive_Loss : 0.12033, Sensitive_Acc : 17.600, Run Time : 11.97 sec
INFO:root:2024-04-26 16:10:38, Train, Epoch : 2, Step : 880, Loss : 0.40507, Acc : 0.825, Sensitive_Loss : 0.17833, Sensitive_Acc : 16.600, Run Time : 11.25 sec
INFO:root:2024-04-26 16:10:50, Train, Epoch : 2, Step : 890, Loss : 0.43505, Acc : 0.806, Sensitive_Loss : 0.20256, Sensitive_Acc : 17.100, Run Time : 11.88 sec
INFO:root:2024-04-26 16:11:01, Train, Epoch : 2, Step : 900, Loss : 0.47739, Acc : 0.797, Sensitive_Loss : 0.13613, Sensitive_Acc : 14.800, Run Time : 11.43 sec
INFO:root:2024-04-26 16:13:37, Dev, Step : 900, Loss : 0.61391, Acc : 0.739, Auc : 0.889, Sensitive_Loss : 0.25228, Sensitive_Acc : 16.679, Sensitive_Auc : 0.987, Mean auc: 0.889, Run Time : 156.22 sec
INFO:root:2024-04-26 16:13:38, Best, Step : 900, Loss : 0.61391, Acc : 0.739, Auc : 0.889, Sensitive_Loss : 0.25228, Sensitive_Acc : 16.679, Sensitive_Auc : 0.987, Best Auc : 0.889
INFO:root:2024-04-26 16:13:47, Train, Epoch : 2, Step : 910, Loss : 0.32981, Acc : 0.847, Sensitive_Loss : 0.21210, Sensitive_Acc : 16.700, Run Time : 166.07 sec
INFO:root:2024-04-26 16:13:58, Train, Epoch : 2, Step : 920, Loss : 0.47172, Acc : 0.762, Sensitive_Loss : 0.18371, Sensitive_Acc : 15.700, Run Time : 11.28 sec
INFO:root:2024-04-26 16:14:10, Train, Epoch : 2, Step : 930, Loss : 0.47501, Acc : 0.806, Sensitive_Loss : 0.18201, Sensitive_Acc : 15.500, Run Time : 11.48 sec
INFO:root:2024-04-26 16:14:22, Train, Epoch : 2, Step : 940, Loss : 0.46841, Acc : 0.806, Sensitive_Loss : 0.18526, Sensitive_Acc : 15.300, Run Time : 12.15 sec
INFO:root:2024-04-26 16:14:34, Train, Epoch : 2, Step : 950, Loss : 0.40171, Acc : 0.822, Sensitive_Loss : 0.21300, Sensitive_Acc : 16.100, Run Time : 11.58 sec
INFO:root:2024-04-26 16:14:44, Train, Epoch : 2, Step : 960, Loss : 0.49471, Acc : 0.750, Sensitive_Loss : 0.15740, Sensitive_Acc : 16.700, Run Time : 10.41 sec
INFO:root:2024-04-26 16:14:56, Train, Epoch : 2, Step : 970, Loss : 0.43386, Acc : 0.800, Sensitive_Loss : 0.14756, Sensitive_Acc : 15.600, Run Time : 11.88 sec
INFO:root:2024-04-26 16:15:07, Train, Epoch : 2, Step : 980, Loss : 0.45453, Acc : 0.806, Sensitive_Loss : 0.12948, Sensitive_Acc : 16.400, Run Time : 11.08 sec
INFO:root:2024-04-26 16:15:20, Train, Epoch : 2, Step : 990, Loss : 0.47541, Acc : 0.778, Sensitive_Loss : 0.12954, Sensitive_Acc : 15.000, Run Time : 12.52 sec
INFO:root:2024-04-26 16:15:30, Train, Epoch : 2, Step : 1000, Loss : 0.41479, Acc : 0.800, Sensitive_Loss : 0.22031, Sensitive_Acc : 17.500, Run Time : 10.66 sec
INFO:root:2024-04-26 16:18:06, Dev, Step : 1000, Loss : 0.44195, Acc : 0.805, Auc : 0.886, Sensitive_Loss : 0.15548, Sensitive_Acc : 16.793, Sensitive_Auc : 0.988, Mean auc: 0.886, Run Time : 155.44 sec
INFO:root:2024-04-26 16:18:15, Train, Epoch : 2, Step : 1010, Loss : 0.40252, Acc : 0.831, Sensitive_Loss : 0.15376, Sensitive_Acc : 15.100, Run Time : 164.37 sec
INFO:root:2024-04-26 16:18:26, Train, Epoch : 2, Step : 1020, Loss : 0.42475, Acc : 0.812, Sensitive_Loss : 0.18984, Sensitive_Acc : 16.500, Run Time : 11.18 sec
INFO:root:2024-04-26 16:18:37, Train, Epoch : 2, Step : 1030, Loss : 0.51288, Acc : 0.794, Sensitive_Loss : 0.14486, Sensitive_Acc : 17.500, Run Time : 11.70 sec
INFO:root:2024-04-26 16:18:48, Train, Epoch : 2, Step : 1040, Loss : 0.41291, Acc : 0.831, Sensitive_Loss : 0.21420, Sensitive_Acc : 15.100, Run Time : 11.00 sec
INFO:root:2024-04-26 16:19:00, Train, Epoch : 2, Step : 1050, Loss : 0.42481, Acc : 0.838, Sensitive_Loss : 0.12145, Sensitive_Acc : 15.100, Run Time : 11.51 sec
INFO:root:2024-04-26 16:19:11, Train, Epoch : 2, Step : 1060, Loss : 0.48285, Acc : 0.791, Sensitive_Loss : 0.15053, Sensitive_Acc : 17.000, Run Time : 11.54 sec
INFO:root:2024-04-26 16:19:24, Train, Epoch : 2, Step : 1070, Loss : 0.36574, Acc : 0.850, Sensitive_Loss : 0.18137, Sensitive_Acc : 14.300, Run Time : 12.31 sec
INFO:root:2024-04-26 16:19:35, Train, Epoch : 2, Step : 1080, Loss : 0.35200, Acc : 0.834, Sensitive_Loss : 0.14826, Sensitive_Acc : 15.300, Run Time : 11.08 sec
INFO:root:2024-04-26 16:19:47, Train, Epoch : 2, Step : 1090, Loss : 0.36832, Acc : 0.847, Sensitive_Loss : 0.14634, Sensitive_Acc : 17.000, Run Time : 12.08 sec
INFO:root:2024-04-26 16:19:58, Train, Epoch : 2, Step : 1100, Loss : 0.38295, Acc : 0.828, Sensitive_Loss : 0.14099, Sensitive_Acc : 17.500, Run Time : 11.34 sec
INFO:root:2024-04-26 16:22:33, Dev, Step : 1100, Loss : 0.44974, Acc : 0.799, Auc : 0.897, Sensitive_Loss : 0.13350, Sensitive_Acc : 16.964, Sensitive_Auc : 0.992, Mean auc: 0.897, Run Time : 154.64 sec
INFO:root:2024-04-26 16:22:34, Best, Step : 1100, Loss : 0.44974, Acc : 0.799, Auc : 0.897, Sensitive_Loss : 0.13350, Sensitive_Acc : 16.964, Sensitive_Auc : 0.992, Best Auc : 0.897
INFO:root:2024-04-26 16:22:42, Train, Epoch : 2, Step : 1110, Loss : 0.36866, Acc : 0.828, Sensitive_Loss : 0.16692, Sensitive_Acc : 16.800, Run Time : 163.64 sec
INFO:root:2024-04-26 16:22:53, Train, Epoch : 2, Step : 1120, Loss : 0.35240, Acc : 0.838, Sensitive_Loss : 0.11719, Sensitive_Acc : 16.600, Run Time : 11.31 sec
INFO:root:2024-04-26 16:23:05, Train, Epoch : 2, Step : 1130, Loss : 0.41138, Acc : 0.825, Sensitive_Loss : 0.13953, Sensitive_Acc : 16.100, Run Time : 11.68 sec
INFO:root:2024-04-26 16:23:17, Train, Epoch : 2, Step : 1140, Loss : 0.40402, Acc : 0.812, Sensitive_Loss : 0.18926, Sensitive_Acc : 16.400, Run Time : 11.96 sec
INFO:root:2024-04-26 16:23:28, Train, Epoch : 2, Step : 1150, Loss : 0.41566, Acc : 0.825, Sensitive_Loss : 0.20443, Sensitive_Acc : 14.900, Run Time : 11.26 sec
INFO:root:2024-04-26 16:23:39, Train, Epoch : 2, Step : 1160, Loss : 0.49267, Acc : 0.797, Sensitive_Loss : 0.12304, Sensitive_Acc : 15.500, Run Time : 11.32 sec
INFO:root:2024-04-26 16:23:51, Train, Epoch : 2, Step : 1170, Loss : 0.37011, Acc : 0.850, Sensitive_Loss : 0.17750, Sensitive_Acc : 15.200, Run Time : 12.04 sec
INFO:root:2024-04-26 16:24:03, Train, Epoch : 2, Step : 1180, Loss : 0.35534, Acc : 0.847, Sensitive_Loss : 0.13357, Sensitive_Acc : 16.300, Run Time : 11.81 sec
INFO:root:2024-04-26 16:24:15, Train, Epoch : 2, Step : 1190, Loss : 0.41710, Acc : 0.800, Sensitive_Loss : 0.15348, Sensitive_Acc : 17.200, Run Time : 11.22 sec
INFO:root:2024-04-26 16:24:26, Train, Epoch : 2, Step : 1200, Loss : 0.40606, Acc : 0.822, Sensitive_Loss : 0.16009, Sensitive_Acc : 15.700, Run Time : 11.50 sec
INFO:root:2024-04-26 16:27:13, Dev, Step : 1200, Loss : 0.47931, Acc : 0.789, Auc : 0.893, Sensitive_Loss : 0.32540, Sensitive_Acc : 16.593, Sensitive_Auc : 0.987, Mean auc: 0.893, Run Time : 167.37 sec
INFO:root:2024-04-26 16:27:24, Train, Epoch : 2, Step : 1210, Loss : 0.40977, Acc : 0.825, Sensitive_Loss : 0.18413, Sensitive_Acc : 16.200, Run Time : 177.49 sec
INFO:root:2024-04-26 16:27:43, Train, Epoch : 2, Step : 1220, Loss : 0.37720, Acc : 0.825, Sensitive_Loss : 0.14292, Sensitive_Acc : 14.900, Run Time : 19.34 sec
INFO:root:2024-04-26 16:28:07, Train, Epoch : 2, Step : 1230, Loss : 0.38949, Acc : 0.831, Sensitive_Loss : 0.16711, Sensitive_Acc : 15.500, Run Time : 24.63 sec
INFO:root:2024-04-26 16:28:21, Train, Epoch : 2, Step : 1240, Loss : 0.46141, Acc : 0.828, Sensitive_Loss : 0.15332, Sensitive_Acc : 16.000, Run Time : 13.90 sec
INFO:root:2024-04-26 16:28:33, Train, Epoch : 2, Step : 1250, Loss : 0.40201, Acc : 0.806, Sensitive_Loss : 0.19643, Sensitive_Acc : 15.400, Run Time : 11.90 sec
INFO:root:2024-04-26 16:31:18
INFO:root:y_pred: [0.07546171 0.87423724 0.01836379 ... 0.78291774 0.00534648 0.8618523 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.7107488e-01 3.3747307e-03 3.8386267e-01 1.0578441e-03 9.9999058e-01
 4.7098361e-03 9.9999726e-01 9.9993169e-01 1.5772429e-01 8.8828665e-01
 9.8329562e-01 9.9959296e-01 9.9710125e-01 9.8631048e-01 6.2491223e-02
 8.7266767e-01 9.9960679e-01 1.2512587e-02 7.0584649e-01 7.9224622e-01
 9.9626535e-01 5.1960719e-01 9.9927109e-01 9.8215669e-01 9.9890113e-01
 9.9693239e-01 1.3195903e-02 9.9964833e-01 9.9926966e-01 9.7340649e-01
 4.7157913e-02 6.4797980e-01 6.9224954e-02 2.4793640e-02 2.7113864e-01
 2.8269108e-02 5.2080095e-01 1.6553348e-02 9.9989903e-01 9.9992800e-01
 8.0225211e-05 7.5559183e-03 9.9245739e-01 9.1104079e-03 9.9999392e-01
 9.9651307e-01 9.9995816e-01 9.8852879e-01 1.6527641e-01 9.9906200e-01
 9.9687541e-01 2.4050945e-02 9.2024696e-01 5.1426105e-02 5.4919641e-03
 1.4390036e-01 1.2513709e-01 4.7850958e-01 1.2267593e-02 9.5163323e-02
 4.2291330e-03 7.6884232e-02 4.6939206e-01 8.2021868e-01 8.8158268e-01
 9.9999821e-01 4.3007914e-02 9.9980325e-01 9.8606026e-01 8.8983935e-01
 9.4237018e-01 8.0643129e-01 4.4595055e-02 5.9742761e-01 2.0414522e-02
 2.7203255e-03 6.5212256e-01 3.3886653e-01 8.8563748e-03 9.9953389e-01
 9.9978989e-01 9.2733437e-03 1.2478743e-01 9.3282379e-02 9.5187277e-01
 8.3418453e-01 2.1334748e-01 7.5137191e-02 9.6502197e-01 9.9979788e-01
 9.9999118e-01 5.9026234e-02 2.5356938e-03 9.9911410e-01 2.6439869e-01
 2.9045925e-03 9.9188519e-01 9.9988031e-01 3.2610744e-03 6.7727959e-01
 9.9895573e-01 9.9763572e-01 9.9889165e-01 9.9823475e-01 6.3532643e-02
 2.6180592e-01 9.9907708e-01 9.9634343e-01 9.7372377e-01 5.8739218e-03
 9.8845553e-01 9.9989438e-01 3.4152818e-01 9.9943143e-01 9.9851757e-01
 9.9856156e-01 8.4355611e-01 9.9985850e-01 1.4521864e-01 4.5577312e-01
 9.9970227e-01 9.9925131e-01 2.3966874e-03 9.9594468e-01 9.9999309e-01
 3.6413300e-01 9.9629992e-01 9.3838647e-02 1.5561186e-01 9.7343540e-01
 9.9837554e-01 4.4209410e-02 6.7431086e-01 2.6122817e-01 9.9991381e-01
 9.9766326e-01 9.8837483e-01 4.1974600e-02 1.0852930e-01 9.9760336e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 16:31:18, Dev, Step : 1252, Loss : 0.46934, Acc : 0.792, Auc : 0.897, Sensitive_Loss : 0.21536, Sensitive_Acc : 16.579, Sensitive_Auc : 0.987, Mean auc: 0.897, Run Time : 162.17 sec
INFO:root:2024-04-26 16:31:19, Best, Step : 1252, Loss : 0.46934, Acc : 0.792,Auc : 0.897, Best Auc : 0.897, Sensitive_Loss : 0.21536, Sensitive_Acc : 16.579, Sensitive_Auc : 0.987
INFO:root:2024-04-26 16:31:31, Train, Epoch : 3, Step : 1260, Loss : 0.34531, Acc : 0.662, Sensitive_Loss : 0.11787, Sensitive_Acc : 13.900, Run Time : 10.74 sec
INFO:root:2024-04-26 16:31:43, Train, Epoch : 3, Step : 1270, Loss : 0.35917, Acc : 0.841, Sensitive_Loss : 0.16102, Sensitive_Acc : 17.800, Run Time : 12.01 sec
INFO:root:2024-04-26 16:31:55, Train, Epoch : 3, Step : 1280, Loss : 0.41451, Acc : 0.831, Sensitive_Loss : 0.10916, Sensitive_Acc : 16.400, Run Time : 11.88 sec
INFO:root:2024-04-26 16:32:07, Train, Epoch : 3, Step : 1290, Loss : 0.38358, Acc : 0.834, Sensitive_Loss : 0.13521, Sensitive_Acc : 17.800, Run Time : 12.67 sec
INFO:root:2024-04-26 16:32:18, Train, Epoch : 3, Step : 1300, Loss : 0.36400, Acc : 0.825, Sensitive_Loss : 0.15317, Sensitive_Acc : 17.300, Run Time : 10.75 sec
INFO:root:2024-04-26 16:34:53, Dev, Step : 1300, Loss : 0.41503, Acc : 0.823, Auc : 0.903, Sensitive_Loss : 0.15274, Sensitive_Acc : 16.864, Sensitive_Auc : 0.986, Mean auc: 0.903, Run Time : 154.73 sec
INFO:root:2024-04-26 16:34:54, Best, Step : 1300, Loss : 0.41503, Acc : 0.823, Auc : 0.903, Sensitive_Loss : 0.15274, Sensitive_Acc : 16.864, Sensitive_Auc : 0.986, Best Auc : 0.903
INFO:root:2024-04-26 16:35:03, Train, Epoch : 3, Step : 1310, Loss : 0.35482, Acc : 0.838, Sensitive_Loss : 0.13832, Sensitive_Acc : 16.500, Run Time : 164.49 sec
INFO:root:2024-04-26 16:35:14, Train, Epoch : 3, Step : 1320, Loss : 0.38842, Acc : 0.825, Sensitive_Loss : 0.13419, Sensitive_Acc : 14.900, Run Time : 11.71 sec
INFO:root:2024-04-26 16:35:26, Train, Epoch : 3, Step : 1330, Loss : 0.36314, Acc : 0.841, Sensitive_Loss : 0.10255, Sensitive_Acc : 17.100, Run Time : 11.21 sec
INFO:root:2024-04-26 16:35:37, Train, Epoch : 3, Step : 1340, Loss : 0.36261, Acc : 0.834, Sensitive_Loss : 0.12942, Sensitive_Acc : 15.100, Run Time : 11.16 sec
INFO:root:2024-04-26 16:35:49, Train, Epoch : 3, Step : 1350, Loss : 0.31238, Acc : 0.859, Sensitive_Loss : 0.13932, Sensitive_Acc : 17.100, Run Time : 11.78 sec
INFO:root:2024-04-26 16:36:00, Train, Epoch : 3, Step : 1360, Loss : 0.38282, Acc : 0.828, Sensitive_Loss : 0.13009, Sensitive_Acc : 15.100, Run Time : 11.54 sec
INFO:root:2024-04-26 16:36:12, Train, Epoch : 3, Step : 1370, Loss : 0.39082, Acc : 0.816, Sensitive_Loss : 0.14026, Sensitive_Acc : 16.100, Run Time : 12.31 sec
INFO:root:2024-04-26 16:36:24, Train, Epoch : 3, Step : 1380, Loss : 0.34994, Acc : 0.844, Sensitive_Loss : 0.11873, Sensitive_Acc : 15.100, Run Time : 11.98 sec
INFO:root:2024-04-26 16:36:37, Train, Epoch : 3, Step : 1390, Loss : 0.37359, Acc : 0.803, Sensitive_Loss : 0.16438, Sensitive_Acc : 15.600, Run Time : 12.65 sec
INFO:root:2024-04-26 16:36:51, Train, Epoch : 3, Step : 1400, Loss : 0.39593, Acc : 0.834, Sensitive_Loss : 0.12353, Sensitive_Acc : 16.000, Run Time : 13.51 sec
INFO:root:2024-04-26 16:39:28, Dev, Step : 1400, Loss : 0.41134, Acc : 0.826, Auc : 0.907, Sensitive_Loss : 0.13891, Sensitive_Acc : 16.893, Sensitive_Auc : 0.985, Mean auc: 0.907, Run Time : 157.19 sec
INFO:root:2024-04-26 16:39:28, Best, Step : 1400, Loss : 0.41134, Acc : 0.826, Auc : 0.907, Sensitive_Loss : 0.13891, Sensitive_Acc : 16.893, Sensitive_Auc : 0.985, Best Auc : 0.907
INFO:root:2024-04-26 16:39:37, Train, Epoch : 3, Step : 1410, Loss : 0.36541, Acc : 0.838, Sensitive_Loss : 0.12627, Sensitive_Acc : 16.300, Run Time : 166.62 sec
INFO:root:2024-04-26 16:39:49, Train, Epoch : 3, Step : 1420, Loss : 0.34281, Acc : 0.850, Sensitive_Loss : 0.13628, Sensitive_Acc : 15.100, Run Time : 12.29 sec
INFO:root:2024-04-26 16:40:01, Train, Epoch : 3, Step : 1430, Loss : 0.35801, Acc : 0.825, Sensitive_Loss : 0.14087, Sensitive_Acc : 16.200, Run Time : 11.72 sec
INFO:root:2024-04-26 16:40:13, Train, Epoch : 3, Step : 1440, Loss : 0.29014, Acc : 0.869, Sensitive_Loss : 0.17175, Sensitive_Acc : 16.400, Run Time : 11.37 sec
INFO:root:2024-04-26 16:40:25, Train, Epoch : 3, Step : 1450, Loss : 0.34546, Acc : 0.841, Sensitive_Loss : 0.12725, Sensitive_Acc : 16.700, Run Time : 12.18 sec
INFO:root:2024-04-26 16:40:36, Train, Epoch : 3, Step : 1460, Loss : 0.38049, Acc : 0.838, Sensitive_Loss : 0.12915, Sensitive_Acc : 17.700, Run Time : 11.69 sec
INFO:root:2024-04-26 16:40:48, Train, Epoch : 3, Step : 1470, Loss : 0.36784, Acc : 0.850, Sensitive_Loss : 0.10667, Sensitive_Acc : 16.900, Run Time : 11.54 sec
INFO:root:2024-04-26 16:40:59, Train, Epoch : 3, Step : 1480, Loss : 0.32373, Acc : 0.859, Sensitive_Loss : 0.16883, Sensitive_Acc : 16.200, Run Time : 11.22 sec
INFO:root:2024-04-26 16:41:12, Train, Epoch : 3, Step : 1490, Loss : 0.34286, Acc : 0.869, Sensitive_Loss : 0.12970, Sensitive_Acc : 16.500, Run Time : 12.82 sec
INFO:root:2024-04-26 16:41:23, Train, Epoch : 3, Step : 1500, Loss : 0.42627, Acc : 0.841, Sensitive_Loss : 0.11173, Sensitive_Acc : 15.000, Run Time : 10.93 sec
INFO:root:2024-04-26 16:44:11, Dev, Step : 1500, Loss : 0.42135, Acc : 0.824, Auc : 0.908, Sensitive_Loss : 0.14521, Sensitive_Acc : 16.864, Sensitive_Auc : 0.986, Mean auc: 0.908, Run Time : 168.41 sec
INFO:root:2024-04-26 16:44:12, Best, Step : 1500, Loss : 0.42135, Acc : 0.824, Auc : 0.908, Sensitive_Loss : 0.14521, Sensitive_Acc : 16.864, Sensitive_Auc : 0.986, Best Auc : 0.908
INFO:root:2024-04-26 16:44:20, Train, Epoch : 3, Step : 1510, Loss : 0.36427, Acc : 0.844, Sensitive_Loss : 0.11856, Sensitive_Acc : 15.400, Run Time : 177.35 sec
INFO:root:2024-04-26 16:44:32, Train, Epoch : 3, Step : 1520, Loss : 0.38460, Acc : 0.816, Sensitive_Loss : 0.14189, Sensitive_Acc : 15.600, Run Time : 11.37 sec
INFO:root:2024-04-26 16:44:44, Train, Epoch : 3, Step : 1530, Loss : 0.36496, Acc : 0.859, Sensitive_Loss : 0.12303, Sensitive_Acc : 16.900, Run Time : 12.17 sec
INFO:root:2024-04-26 16:44:56, Train, Epoch : 3, Step : 1540, Loss : 0.36840, Acc : 0.844, Sensitive_Loss : 0.11447, Sensitive_Acc : 14.700, Run Time : 11.68 sec
INFO:root:2024-04-26 16:45:07, Train, Epoch : 3, Step : 1550, Loss : 0.43369, Acc : 0.809, Sensitive_Loss : 0.12166, Sensitive_Acc : 15.600, Run Time : 11.06 sec
INFO:root:2024-04-26 16:45:19, Train, Epoch : 3, Step : 1560, Loss : 0.42719, Acc : 0.800, Sensitive_Loss : 0.12372, Sensitive_Acc : 15.900, Run Time : 12.60 sec
INFO:root:2024-04-26 16:45:30, Train, Epoch : 3, Step : 1570, Loss : 0.25599, Acc : 0.875, Sensitive_Loss : 0.13044, Sensitive_Acc : 15.700, Run Time : 11.02 sec
INFO:root:2024-04-26 16:45:42, Train, Epoch : 3, Step : 1580, Loss : 0.37499, Acc : 0.834, Sensitive_Loss : 0.11790, Sensitive_Acc : 15.300, Run Time : 11.34 sec
INFO:root:2024-04-26 16:45:53, Train, Epoch : 3, Step : 1590, Loss : 0.35611, Acc : 0.869, Sensitive_Loss : 0.15261, Sensitive_Acc : 16.100, Run Time : 11.49 sec
INFO:root:2024-04-26 16:46:05, Train, Epoch : 3, Step : 1600, Loss : 0.39147, Acc : 0.853, Sensitive_Loss : 0.14126, Sensitive_Acc : 17.600, Run Time : 11.93 sec
INFO:root:2024-04-26 16:48:40, Dev, Step : 1600, Loss : 0.41300, Acc : 0.824, Auc : 0.909, Sensitive_Loss : 0.13505, Sensitive_Acc : 16.864, Sensitive_Auc : 0.987, Mean auc: 0.909, Run Time : 154.61 sec
INFO:root:2024-04-26 16:48:40, Best, Step : 1600, Loss : 0.41300, Acc : 0.824, Auc : 0.909, Sensitive_Loss : 0.13505, Sensitive_Acc : 16.864, Sensitive_Auc : 0.987, Best Auc : 0.909
INFO:root:2024-04-26 16:48:48, Train, Epoch : 3, Step : 1610, Loss : 0.33640, Acc : 0.850, Sensitive_Loss : 0.09059, Sensitive_Acc : 14.900, Run Time : 163.40 sec
INFO:root:2024-04-26 16:49:01, Train, Epoch : 3, Step : 1620, Loss : 0.30748, Acc : 0.847, Sensitive_Loss : 0.14822, Sensitive_Acc : 16.500, Run Time : 12.14 sec
INFO:root:2024-04-26 16:49:12, Train, Epoch : 3, Step : 1630, Loss : 0.27212, Acc : 0.859, Sensitive_Loss : 0.14184, Sensitive_Acc : 17.400, Run Time : 11.22 sec
INFO:root:2024-04-26 16:49:23, Train, Epoch : 3, Step : 1640, Loss : 0.34011, Acc : 0.866, Sensitive_Loss : 0.11246, Sensitive_Acc : 17.700, Run Time : 11.18 sec
INFO:root:2024-04-26 16:49:35, Train, Epoch : 3, Step : 1650, Loss : 0.34581, Acc : 0.841, Sensitive_Loss : 0.15070, Sensitive_Acc : 16.600, Run Time : 12.13 sec
INFO:root:2024-04-26 16:49:47, Train, Epoch : 3, Step : 1660, Loss : 0.34304, Acc : 0.863, Sensitive_Loss : 0.09016, Sensitive_Acc : 15.700, Run Time : 12.35 sec
INFO:root:2024-04-26 16:49:59, Train, Epoch : 3, Step : 1670, Loss : 0.39780, Acc : 0.841, Sensitive_Loss : 0.14382, Sensitive_Acc : 16.900, Run Time : 11.84 sec
INFO:root:2024-04-26 16:50:11, Train, Epoch : 3, Step : 1680, Loss : 0.41238, Acc : 0.816, Sensitive_Loss : 0.15088, Sensitive_Acc : 15.500, Run Time : 12.10 sec
INFO:root:2024-04-26 16:50:23, Train, Epoch : 3, Step : 1690, Loss : 0.33129, Acc : 0.847, Sensitive_Loss : 0.14923, Sensitive_Acc : 15.200, Run Time : 12.03 sec
INFO:root:2024-04-26 16:50:35, Train, Epoch : 3, Step : 1700, Loss : 0.32775, Acc : 0.841, Sensitive_Loss : 0.09278, Sensitive_Acc : 15.700, Run Time : 11.52 sec
INFO:root:2024-04-26 16:53:16, Dev, Step : 1700, Loss : 0.41345, Acc : 0.827, Auc : 0.910, Sensitive_Loss : 0.15097, Sensitive_Acc : 16.821, Sensitive_Auc : 0.984, Mean auc: 0.910, Run Time : 161.38 sec
INFO:root:2024-04-26 16:53:17, Best, Step : 1700, Loss : 0.41345, Acc : 0.827, Auc : 0.910, Sensitive_Loss : 0.15097, Sensitive_Acc : 16.821, Sensitive_Auc : 0.984, Best Auc : 0.910
INFO:root:2024-04-26 16:53:26, Train, Epoch : 3, Step : 1710, Loss : 0.34598, Acc : 0.856, Sensitive_Loss : 0.11823, Sensitive_Acc : 15.400, Run Time : 171.09 sec
INFO:root:2024-04-26 16:53:39, Train, Epoch : 3, Step : 1720, Loss : 0.31597, Acc : 0.875, Sensitive_Loss : 0.16170, Sensitive_Acc : 15.400, Run Time : 13.27 sec
INFO:root:2024-04-26 16:53:53, Train, Epoch : 3, Step : 1730, Loss : 0.32396, Acc : 0.850, Sensitive_Loss : 0.08810, Sensitive_Acc : 16.400, Run Time : 14.11 sec
INFO:root:2024-04-26 16:54:07, Train, Epoch : 3, Step : 1740, Loss : 0.40684, Acc : 0.847, Sensitive_Loss : 0.13053, Sensitive_Acc : 15.800, Run Time : 14.04 sec
INFO:root:2024-04-26 16:54:21, Train, Epoch : 3, Step : 1750, Loss : 0.37849, Acc : 0.841, Sensitive_Loss : 0.13611, Sensitive_Acc : 16.700, Run Time : 13.25 sec
INFO:root:2024-04-26 16:54:33, Train, Epoch : 3, Step : 1760, Loss : 0.34427, Acc : 0.856, Sensitive_Loss : 0.10226, Sensitive_Acc : 15.700, Run Time : 12.47 sec
INFO:root:2024-04-26 16:54:46, Train, Epoch : 3, Step : 1770, Loss : 0.35203, Acc : 0.847, Sensitive_Loss : 0.09532, Sensitive_Acc : 15.800, Run Time : 12.56 sec
INFO:root:2024-04-26 16:54:57, Train, Epoch : 3, Step : 1780, Loss : 0.30349, Acc : 0.872, Sensitive_Loss : 0.12067, Sensitive_Acc : 17.000, Run Time : 11.21 sec
INFO:root:2024-04-26 16:55:08, Train, Epoch : 3, Step : 1790, Loss : 0.35488, Acc : 0.866, Sensitive_Loss : 0.12719, Sensitive_Acc : 16.200, Run Time : 11.46 sec
INFO:root:2024-04-26 16:55:20, Train, Epoch : 3, Step : 1800, Loss : 0.32331, Acc : 0.872, Sensitive_Loss : 0.12665, Sensitive_Acc : 14.000, Run Time : 11.96 sec
INFO:root:2024-04-26 16:57:56, Dev, Step : 1800, Loss : 0.40175, Acc : 0.829, Auc : 0.911, Sensitive_Loss : 0.13259, Sensitive_Acc : 16.821, Sensitive_Auc : 0.984, Mean auc: 0.911, Run Time : 155.98 sec
INFO:root:2024-04-26 16:57:57, Best, Step : 1800, Loss : 0.40175, Acc : 0.829, Auc : 0.911, Sensitive_Loss : 0.13259, Sensitive_Acc : 16.821, Sensitive_Auc : 0.984, Best Auc : 0.911
INFO:root:2024-04-26 16:58:06, Train, Epoch : 3, Step : 1810, Loss : 0.34348, Acc : 0.856, Sensitive_Loss : 0.09089, Sensitive_Acc : 16.600, Run Time : 165.45 sec
INFO:root:2024-04-26 16:58:17, Train, Epoch : 3, Step : 1820, Loss : 0.35042, Acc : 0.863, Sensitive_Loss : 0.10174, Sensitive_Acc : 15.700, Run Time : 11.42 sec
INFO:root:2024-04-26 16:58:29, Train, Epoch : 3, Step : 1830, Loss : 0.36055, Acc : 0.850, Sensitive_Loss : 0.10947, Sensitive_Acc : 16.900, Run Time : 11.84 sec
INFO:root:2024-04-26 16:58:41, Train, Epoch : 3, Step : 1840, Loss : 0.37511, Acc : 0.806, Sensitive_Loss : 0.08691, Sensitive_Acc : 17.500, Run Time : 12.46 sec
INFO:root:2024-04-26 16:58:52, Train, Epoch : 3, Step : 1850, Loss : 0.32712, Acc : 0.853, Sensitive_Loss : 0.09335, Sensitive_Acc : 15.900, Run Time : 10.66 sec
INFO:root:2024-04-26 16:59:03, Train, Epoch : 3, Step : 1860, Loss : 0.35673, Acc : 0.850, Sensitive_Loss : 0.09919, Sensitive_Acc : 16.400, Run Time : 11.34 sec
INFO:root:2024-04-26 16:59:15, Train, Epoch : 3, Step : 1870, Loss : 0.37655, Acc : 0.834, Sensitive_Loss : 0.10485, Sensitive_Acc : 15.300, Run Time : 11.85 sec
INFO:root:2024-04-26 17:02:14
INFO:root:y_pred: [0.07258092 0.94238544 0.04344743 ... 0.82619554 0.0014455  0.86675227]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.5721179e-01 2.8399145e-03 3.2478729e-01 7.3352632e-05 9.9981982e-01
 1.1783178e-03 9.9999738e-01 9.9988186e-01 2.6959922e-02 8.8125867e-01
 9.9086434e-01 9.9984825e-01 9.9716198e-01 9.7867006e-01 3.2495666e-02
 9.5770162e-01 9.9967575e-01 2.1135986e-02 5.6693465e-01 9.1575444e-01
 9.8718852e-01 7.4859172e-02 9.9850732e-01 9.6843231e-01 9.9879467e-01
 9.9594826e-01 1.8792338e-03 9.9912912e-01 9.9885345e-01 7.3264837e-01
 2.7205894e-02 3.7244678e-01 3.9549328e-02 2.4342466e-02 5.0893292e-02
 1.6300512e-02 3.0767101e-01 1.1858394e-02 9.9961096e-01 9.9970871e-01
 3.0403375e-05 2.1348514e-03 9.8728406e-01 4.2620515e-03 9.9998057e-01
 9.9652928e-01 9.9994767e-01 9.9393189e-01 7.3584996e-02 9.9953783e-01
 9.9738961e-01 1.1492623e-02 6.5065569e-01 1.7032355e-02 7.1214829e-03
 3.6570653e-02 2.5925614e-02 8.2431193e-03 6.9543514e-03 1.4557609e-01
 5.8376053e-03 3.0201364e-02 1.2319040e-01 6.4494991e-01 7.5849921e-01
 9.9999785e-01 3.9303601e-03 9.9979562e-01 9.9105155e-01 8.7639415e-01
 8.4784114e-01 5.5359352e-01 1.0258929e-02 2.7824306e-01 6.6030263e-03
 9.8470808e-04 9.0757810e-02 1.0059470e-02 2.8435872e-03 9.9902678e-01
 9.9966359e-01 2.1522564e-03 9.2566624e-02 3.0979969e-02 9.7249401e-01
 7.9398811e-01 5.4512605e-02 2.4267452e-02 9.6581548e-01 9.9986637e-01
 9.9996924e-01 3.8052986e-03 1.7337541e-03 9.9924493e-01 2.1784233e-01
 4.4322163e-03 9.9615401e-01 9.9889320e-01 4.3746200e-04 5.4897767e-01
 9.9861145e-01 9.9690104e-01 9.9878591e-01 9.9622822e-01 2.5325822e-02
 4.1526651e-01 9.9884963e-01 9.9563688e-01 9.7125638e-01 9.0988883e-04
 9.9358290e-01 9.9720109e-01 1.5787919e-01 9.9874675e-01 9.9766147e-01
 9.9847645e-01 7.8288734e-01 9.9932241e-01 4.6562228e-02 2.0434988e-01
 9.9976474e-01 9.9936384e-01 1.5551997e-03 9.8844844e-01 9.9999452e-01
 1.8166569e-01 9.9669915e-01 1.9237695e-02 6.0933508e-02 9.8770231e-01
 9.9692672e-01 3.1793583e-03 6.0688153e-02 7.0668578e-02 9.9977952e-01
 9.9690044e-01 9.8705858e-01 3.0408478e-03 2.1816552e-02 9.9824584e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 17:02:14, Dev, Step : 1878, Loss : 0.41993, Acc : 0.820, Auc : 0.911, Sensitive_Loss : 0.14754, Sensitive_Acc : 16.864, Sensitive_Auc : 0.985, Mean auc: 0.911, Run Time : 169.22 sec
INFO:root:2024-04-26 17:02:15, Best, Step : 1878, Loss : 0.41993, Acc : 0.820,Auc : 0.911, Best Auc : 0.911, Sensitive_Loss : 0.14754, Sensitive_Acc : 16.864, Sensitive_Auc : 0.985
INFO:root:2024-04-26 17:02:20, Train, Epoch : 4, Step : 1880, Loss : 0.08303, Acc : 0.163, Sensitive_Loss : 0.01166, Sensitive_Acc : 2.800, Run Time : 3.74 sec
INFO:root:2024-04-26 17:02:32, Train, Epoch : 4, Step : 1890, Loss : 0.29824, Acc : 0.878, Sensitive_Loss : 0.13388, Sensitive_Acc : 15.400, Run Time : 11.51 sec
INFO:root:2024-04-26 17:02:44, Train, Epoch : 4, Step : 1900, Loss : 0.29091, Acc : 0.872, Sensitive_Loss : 0.14138, Sensitive_Acc : 17.800, Run Time : 11.91 sec
INFO:root:2024-04-26 17:05:20, Dev, Step : 1900, Loss : 0.41345, Acc : 0.826, Auc : 0.912, Sensitive_Loss : 0.14164, Sensitive_Acc : 16.864, Sensitive_Auc : 0.985, Mean auc: 0.912, Run Time : 155.79 sec
INFO:root:2024-04-26 17:05:20, Best, Step : 1900, Loss : 0.41345, Acc : 0.826, Auc : 0.912, Sensitive_Loss : 0.14164, Sensitive_Acc : 16.864, Sensitive_Auc : 0.985, Best Auc : 0.912
INFO:root:2024-04-26 17:05:29, Train, Epoch : 4, Step : 1910, Loss : 0.34121, Acc : 0.831, Sensitive_Loss : 0.09050, Sensitive_Acc : 16.100, Run Time : 165.36 sec
INFO:root:2024-04-26 17:05:41, Train, Epoch : 4, Step : 1920, Loss : 0.36960, Acc : 0.844, Sensitive_Loss : 0.10462, Sensitive_Acc : 16.800, Run Time : 11.45 sec
INFO:root:2024-04-26 17:05:52, Train, Epoch : 4, Step : 1930, Loss : 0.26967, Acc : 0.872, Sensitive_Loss : 0.12203, Sensitive_Acc : 15.300, Run Time : 10.97 sec
INFO:root:2024-04-26 17:06:04, Train, Epoch : 4, Step : 1940, Loss : 0.25806, Acc : 0.894, Sensitive_Loss : 0.11025, Sensitive_Acc : 16.100, Run Time : 12.13 sec
INFO:root:2024-04-26 17:06:15, Train, Epoch : 4, Step : 1950, Loss : 0.32652, Acc : 0.850, Sensitive_Loss : 0.09163, Sensitive_Acc : 17.200, Run Time : 11.37 sec
INFO:root:2024-04-26 17:06:26, Train, Epoch : 4, Step : 1960, Loss : 0.32899, Acc : 0.847, Sensitive_Loss : 0.12369, Sensitive_Acc : 15.900, Run Time : 10.77 sec
INFO:root:2024-04-26 17:06:37, Train, Epoch : 4, Step : 1970, Loss : 0.26751, Acc : 0.897, Sensitive_Loss : 0.10711, Sensitive_Acc : 17.600, Run Time : 11.63 sec
INFO:root:2024-04-26 17:06:49, Train, Epoch : 4, Step : 1980, Loss : 0.26589, Acc : 0.909, Sensitive_Loss : 0.17445, Sensitive_Acc : 15.600, Run Time : 11.49 sec
INFO:root:2024-04-26 17:07:01, Train, Epoch : 4, Step : 1990, Loss : 0.35734, Acc : 0.856, Sensitive_Loss : 0.11831, Sensitive_Acc : 17.300, Run Time : 12.31 sec
INFO:root:2024-04-26 17:07:12, Train, Epoch : 4, Step : 2000, Loss : 0.31167, Acc : 0.878, Sensitive_Loss : 0.09973, Sensitive_Acc : 17.200, Run Time : 11.09 sec
INFO:root:2024-04-26 17:09:48, Dev, Step : 2000, Loss : 0.40869, Acc : 0.831, Auc : 0.912, Sensitive_Loss : 0.12596, Sensitive_Acc : 16.893, Sensitive_Auc : 0.983, Mean auc: 0.912, Run Time : 155.18 sec
INFO:root:2024-04-26 17:09:56, Train, Epoch : 4, Step : 2010, Loss : 0.30906, Acc : 0.856, Sensitive_Loss : 0.10851, Sensitive_Acc : 16.300, Run Time : 163.91 sec
INFO:root:2024-04-26 17:10:08, Train, Epoch : 4, Step : 2020, Loss : 0.27277, Acc : 0.881, Sensitive_Loss : 0.14113, Sensitive_Acc : 17.300, Run Time : 11.68 sec
INFO:root:2024-04-26 17:10:20, Train, Epoch : 4, Step : 2030, Loss : 0.38794, Acc : 0.822, Sensitive_Loss : 0.11664, Sensitive_Acc : 15.600, Run Time : 11.70 sec
INFO:root:2024-04-26 17:10:32, Train, Epoch : 4, Step : 2040, Loss : 0.40315, Acc : 0.825, Sensitive_Loss : 0.10144, Sensitive_Acc : 17.000, Run Time : 11.92 sec
INFO:root:2024-04-26 17:10:43, Train, Epoch : 4, Step : 2050, Loss : 0.30793, Acc : 0.875, Sensitive_Loss : 0.15272, Sensitive_Acc : 15.900, Run Time : 11.15 sec
INFO:root:2024-04-26 17:10:54, Train, Epoch : 4, Step : 2060, Loss : 0.33433, Acc : 0.853, Sensitive_Loss : 0.08117, Sensitive_Acc : 16.800, Run Time : 11.42 sec
INFO:root:2024-04-26 17:11:06, Train, Epoch : 4, Step : 2070, Loss : 0.36491, Acc : 0.831, Sensitive_Loss : 0.10815, Sensitive_Acc : 17.700, Run Time : 11.37 sec
INFO:root:2024-04-26 17:11:17, Train, Epoch : 4, Step : 2080, Loss : 0.39945, Acc : 0.831, Sensitive_Loss : 0.11805, Sensitive_Acc : 16.000, Run Time : 11.92 sec
INFO:root:2024-04-26 17:11:29, Train, Epoch : 4, Step : 2090, Loss : 0.30106, Acc : 0.878, Sensitive_Loss : 0.10055, Sensitive_Acc : 16.700, Run Time : 11.61 sec
INFO:root:2024-04-26 17:11:41, Train, Epoch : 4, Step : 2100, Loss : 0.33119, Acc : 0.878, Sensitive_Loss : 0.10645, Sensitive_Acc : 16.600, Run Time : 12.07 sec
INFO:root:2024-04-26 17:14:17, Dev, Step : 2100, Loss : 0.39843, Acc : 0.834, Auc : 0.912, Sensitive_Loss : 0.12596, Sensitive_Acc : 16.764, Sensitive_Auc : 0.985, Mean auc: 0.912, Run Time : 155.68 sec
INFO:root:2024-04-26 17:14:25, Train, Epoch : 4, Step : 2110, Loss : 0.37136, Acc : 0.828, Sensitive_Loss : 0.11691, Sensitive_Acc : 14.900, Run Time : 163.80 sec
INFO:root:2024-04-26 17:14:38, Train, Epoch : 4, Step : 2120, Loss : 0.33192, Acc : 0.875, Sensitive_Loss : 0.11272, Sensitive_Acc : 17.200, Run Time : 13.48 sec
INFO:root:2024-04-26 17:14:50, Train, Epoch : 4, Step : 2130, Loss : 0.27792, Acc : 0.847, Sensitive_Loss : 0.14237, Sensitive_Acc : 17.200, Run Time : 11.57 sec
INFO:root:2024-04-26 17:15:02, Train, Epoch : 4, Step : 2140, Loss : 0.32703, Acc : 0.866, Sensitive_Loss : 0.11120, Sensitive_Acc : 16.000, Run Time : 11.79 sec
INFO:root:2024-04-26 17:15:13, Train, Epoch : 4, Step : 2150, Loss : 0.39276, Acc : 0.850, Sensitive_Loss : 0.11078, Sensitive_Acc : 17.200, Run Time : 10.87 sec
INFO:root:2024-04-26 17:15:24, Train, Epoch : 4, Step : 2160, Loss : 0.33939, Acc : 0.869, Sensitive_Loss : 0.09449, Sensitive_Acc : 16.400, Run Time : 11.81 sec
INFO:root:2024-04-26 17:15:36, Train, Epoch : 4, Step : 2170, Loss : 0.40792, Acc : 0.841, Sensitive_Loss : 0.10249, Sensitive_Acc : 16.300, Run Time : 11.47 sec
INFO:root:2024-04-26 17:15:48, Train, Epoch : 4, Step : 2180, Loss : 0.31256, Acc : 0.863, Sensitive_Loss : 0.12504, Sensitive_Acc : 17.700, Run Time : 12.05 sec
INFO:root:2024-04-26 17:15:59, Train, Epoch : 4, Step : 2190, Loss : 0.34477, Acc : 0.856, Sensitive_Loss : 0.15458, Sensitive_Acc : 15.600, Run Time : 11.29 sec
INFO:root:2024-04-26 17:16:11, Train, Epoch : 4, Step : 2200, Loss : 0.30100, Acc : 0.875, Sensitive_Loss : 0.11102, Sensitive_Acc : 17.000, Run Time : 11.79 sec
INFO:root:2024-04-26 17:18:46, Dev, Step : 2200, Loss : 0.41189, Acc : 0.826, Auc : 0.912, Sensitive_Loss : 0.15995, Sensitive_Acc : 16.821, Sensitive_Auc : 0.986, Mean auc: 0.912, Run Time : 155.33 sec
INFO:root:2024-04-26 17:18:47, Best, Step : 2200, Loss : 0.41189, Acc : 0.826, Auc : 0.912, Sensitive_Loss : 0.15995, Sensitive_Acc : 16.821, Sensitive_Auc : 0.986, Best Auc : 0.912
INFO:root:2024-04-26 17:18:56, Train, Epoch : 4, Step : 2210, Loss : 0.34714, Acc : 0.844, Sensitive_Loss : 0.10894, Sensitive_Acc : 15.900, Run Time : 164.87 sec
INFO:root:2024-04-26 17:19:08, Train, Epoch : 4, Step : 2220, Loss : 0.33623, Acc : 0.850, Sensitive_Loss : 0.10233, Sensitive_Acc : 17.100, Run Time : 11.88 sec
INFO:root:2024-04-26 17:19:20, Train, Epoch : 4, Step : 2230, Loss : 0.33014, Acc : 0.859, Sensitive_Loss : 0.10505, Sensitive_Acc : 16.100, Run Time : 12.51 sec
INFO:root:2024-04-26 17:19:32, Train, Epoch : 4, Step : 2240, Loss : 0.35443, Acc : 0.844, Sensitive_Loss : 0.14782, Sensitive_Acc : 15.100, Run Time : 11.48 sec
INFO:root:2024-04-26 17:19:44, Train, Epoch : 4, Step : 2250, Loss : 0.21796, Acc : 0.925, Sensitive_Loss : 0.11114, Sensitive_Acc : 15.900, Run Time : 11.99 sec
INFO:root:2024-04-26 17:19:56, Train, Epoch : 4, Step : 2260, Loss : 0.33865, Acc : 0.816, Sensitive_Loss : 0.10012, Sensitive_Acc : 16.200, Run Time : 12.32 sec
INFO:root:2024-04-26 17:20:08, Train, Epoch : 4, Step : 2270, Loss : 0.39630, Acc : 0.850, Sensitive_Loss : 0.11524, Sensitive_Acc : 16.500, Run Time : 11.86 sec
INFO:root:2024-04-26 17:20:20, Train, Epoch : 4, Step : 2280, Loss : 0.31162, Acc : 0.881, Sensitive_Loss : 0.15688, Sensitive_Acc : 16.100, Run Time : 11.92 sec
INFO:root:2024-04-26 17:20:31, Train, Epoch : 4, Step : 2290, Loss : 0.40781, Acc : 0.831, Sensitive_Loss : 0.09351, Sensitive_Acc : 16.500, Run Time : 11.38 sec
INFO:root:2024-04-26 17:20:43, Train, Epoch : 4, Step : 2300, Loss : 0.36178, Acc : 0.822, Sensitive_Loss : 0.08614, Sensitive_Acc : 16.800, Run Time : 11.53 sec
INFO:root:2024-04-26 17:23:18, Dev, Step : 2300, Loss : 0.41190, Acc : 0.824, Auc : 0.912, Sensitive_Loss : 0.15688, Sensitive_Acc : 16.864, Sensitive_Auc : 0.987, Mean auc: 0.912, Run Time : 155.69 sec
INFO:root:2024-04-26 17:23:19, Best, Step : 2300, Loss : 0.41190, Acc : 0.824, Auc : 0.912, Sensitive_Loss : 0.15688, Sensitive_Acc : 16.864, Sensitive_Auc : 0.987, Best Auc : 0.912
INFO:root:2024-04-26 17:23:28, Train, Epoch : 4, Step : 2310, Loss : 0.29878, Acc : 0.866, Sensitive_Loss : 0.13382, Sensitive_Acc : 17.200, Run Time : 164.86 sec
INFO:root:2024-04-26 17:23:39, Train, Epoch : 4, Step : 2320, Loss : 0.33241, Acc : 0.872, Sensitive_Loss : 0.11725, Sensitive_Acc : 16.000, Run Time : 11.85 sec
INFO:root:2024-04-26 17:23:51, Train, Epoch : 4, Step : 2330, Loss : 0.36210, Acc : 0.847, Sensitive_Loss : 0.09573, Sensitive_Acc : 17.500, Run Time : 11.21 sec
INFO:root:2024-04-26 17:24:02, Train, Epoch : 4, Step : 2340, Loss : 0.34968, Acc : 0.844, Sensitive_Loss : 0.12984, Sensitive_Acc : 16.000, Run Time : 11.52 sec
INFO:root:2024-04-26 17:24:14, Train, Epoch : 4, Step : 2350, Loss : 0.33560, Acc : 0.859, Sensitive_Loss : 0.09123, Sensitive_Acc : 16.300, Run Time : 11.69 sec
INFO:root:2024-04-26 17:24:26, Train, Epoch : 4, Step : 2360, Loss : 0.32848, Acc : 0.838, Sensitive_Loss : 0.09297, Sensitive_Acc : 15.300, Run Time : 12.23 sec
INFO:root:2024-04-26 17:24:38, Train, Epoch : 4, Step : 2370, Loss : 0.30812, Acc : 0.872, Sensitive_Loss : 0.11327, Sensitive_Acc : 15.700, Run Time : 12.37 sec
INFO:root:2024-04-26 17:24:50, Train, Epoch : 4, Step : 2380, Loss : 0.32972, Acc : 0.875, Sensitive_Loss : 0.12778, Sensitive_Acc : 15.000, Run Time : 11.46 sec
INFO:root:2024-04-26 17:25:01, Train, Epoch : 4, Step : 2390, Loss : 0.35996, Acc : 0.856, Sensitive_Loss : 0.18726, Sensitive_Acc : 16.600, Run Time : 11.53 sec
INFO:root:2024-04-26 17:25:13, Train, Epoch : 4, Step : 2400, Loss : 0.31890, Acc : 0.869, Sensitive_Loss : 0.06756, Sensitive_Acc : 16.900, Run Time : 11.47 sec
INFO:root:2024-04-26 17:27:49, Dev, Step : 2400, Loss : 0.41246, Acc : 0.827, Auc : 0.914, Sensitive_Loss : 0.14013, Sensitive_Acc : 16.864, Sensitive_Auc : 0.986, Mean auc: 0.914, Run Time : 155.66 sec
INFO:root:2024-04-26 17:27:49, Best, Step : 2400, Loss : 0.41246, Acc : 0.827, Auc : 0.914, Sensitive_Loss : 0.14013, Sensitive_Acc : 16.864, Sensitive_Auc : 0.986, Best Auc : 0.914
INFO:root:2024-04-26 17:27:58, Train, Epoch : 4, Step : 2410, Loss : 0.32621, Acc : 0.866, Sensitive_Loss : 0.09412, Sensitive_Acc : 17.100, Run Time : 164.88 sec
INFO:root:2024-04-26 17:28:09, Train, Epoch : 4, Step : 2420, Loss : 0.34298, Acc : 0.866, Sensitive_Loss : 0.07427, Sensitive_Acc : 15.900, Run Time : 11.55 sec
INFO:root:2024-04-26 17:28:20, Train, Epoch : 4, Step : 2430, Loss : 0.38054, Acc : 0.850, Sensitive_Loss : 0.13967, Sensitive_Acc : 15.400, Run Time : 11.00 sec
INFO:root:2024-04-26 17:28:32, Train, Epoch : 4, Step : 2440, Loss : 0.33793, Acc : 0.828, Sensitive_Loss : 0.15665, Sensitive_Acc : 17.100, Run Time : 11.65 sec
INFO:root:2024-04-26 17:28:43, Train, Epoch : 4, Step : 2450, Loss : 0.36252, Acc : 0.844, Sensitive_Loss : 0.11113, Sensitive_Acc : 16.700, Run Time : 11.34 sec
INFO:root:2024-04-26 17:28:56, Train, Epoch : 4, Step : 2460, Loss : 0.32696, Acc : 0.866, Sensitive_Loss : 0.15411, Sensitive_Acc : 15.700, Run Time : 12.68 sec
INFO:root:2024-04-26 17:29:08, Train, Epoch : 4, Step : 2470, Loss : 0.32916, Acc : 0.863, Sensitive_Loss : 0.09017, Sensitive_Acc : 17.100, Run Time : 11.98 sec
INFO:root:2024-04-26 17:29:18, Train, Epoch : 4, Step : 2480, Loss : 0.32165, Acc : 0.853, Sensitive_Loss : 0.11834, Sensitive_Acc : 17.700, Run Time : 10.12 sec
INFO:root:2024-04-26 17:29:30, Train, Epoch : 4, Step : 2490, Loss : 0.40638, Acc : 0.812, Sensitive_Loss : 0.11863, Sensitive_Acc : 15.000, Run Time : 12.13 sec
INFO:root:2024-04-26 17:29:42, Train, Epoch : 4, Step : 2500, Loss : 0.36822, Acc : 0.816, Sensitive_Loss : 0.15783, Sensitive_Acc : 16.100, Run Time : 11.91 sec
INFO:root:2024-04-26 17:32:18, Dev, Step : 2500, Loss : 0.41398, Acc : 0.827, Auc : 0.913, Sensitive_Loss : 0.14483, Sensitive_Acc : 16.879, Sensitive_Auc : 0.988, Mean auc: 0.913, Run Time : 155.53 sec
INFO:root:2024-04-26 17:34:54
INFO:root:y_pred: [8.4078550e-02 9.4259775e-01 2.8269108e-02 ... 7.8195804e-01 8.5405423e-04
 8.2290131e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.5794594e-01 2.4826331e-03 5.5925542e-01 3.5559271e-05 9.9990427e-01
 1.3754547e-03 9.9999619e-01 9.9989855e-01 2.4157047e-02 7.9388720e-01
 9.9183851e-01 9.9991477e-01 9.9899834e-01 9.8284447e-01 5.3503163e-02
 9.8530179e-01 9.9983835e-01 2.1260774e-02 7.1251470e-01 8.4553885e-01
 9.8553026e-01 6.1345633e-02 9.9971920e-01 9.7527790e-01 9.9958128e-01
 9.9691927e-01 2.2098119e-03 9.9894804e-01 9.9941552e-01 7.5811756e-01
 2.6795290e-02 3.1004128e-01 1.3750039e-02 5.9724126e-02 6.4721487e-02
 7.8805834e-03 2.6000011e-01 5.9137261e-03 9.9981266e-01 9.9971670e-01
 5.9978406e-06 1.6856928e-03 9.8004407e-01 2.4911121e-03 9.9997902e-01
 9.9422425e-01 9.9993706e-01 9.9549955e-01 4.5639664e-02 9.9987233e-01
 9.9739885e-01 1.2848564e-02 7.4566823e-01 1.6471228e-02 7.3483442e-03
 1.9451112e-02 9.3140723e-03 1.8192848e-02 8.0250753e-03 3.0075231e-01
 7.7080228e-03 3.2079451e-02 4.1089144e-02 5.2052146e-01 5.9184778e-01
 9.9999905e-01 8.5229557e-03 9.9968410e-01 9.9430895e-01 8.8747567e-01
 7.8199762e-01 6.3508523e-01 6.2363250e-03 3.4907982e-01 4.8346310e-03
 1.1896420e-03 6.8283148e-02 3.8952414e-02 2.7874412e-03 9.9950016e-01
 9.9985349e-01 1.8948223e-03 6.0373411e-02 3.0515477e-02 9.5842379e-01
 5.0212550e-01 3.2574248e-02 5.4616358e-02 9.8126936e-01 9.9974018e-01
 9.9998844e-01 1.0019453e-02 1.7606156e-03 9.9944240e-01 2.2647347e-01
 5.3459378e-03 9.9434412e-01 9.9904829e-01 3.4645086e-04 2.3800662e-01
 9.9920696e-01 9.9804866e-01 9.9854928e-01 9.9815995e-01 1.3366141e-02
 1.6416921e-01 9.9923980e-01 9.9873966e-01 9.8500788e-01 6.3627155e-04
 9.9387920e-01 9.9806029e-01 1.8806945e-01 9.9846244e-01 9.9872190e-01
 9.9860281e-01 7.1605915e-01 9.9874556e-01 4.7016371e-02 2.1220191e-01
 9.9971014e-01 9.9958223e-01 1.7796381e-03 9.8273879e-01 9.9999797e-01
 2.1220496e-01 9.9473792e-01 1.7018693e-02 1.1797745e-02 9.9225593e-01
 9.9867541e-01 4.1081184e-03 1.1755529e-01 6.5744504e-02 9.9976438e-01
 9.9804807e-01 9.9272424e-01 3.5218734e-03 2.6272172e-02 9.9801695e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 17:34:54, Dev, Step : 2504, Loss : 0.42427, Acc : 0.826, Auc : 0.913, Sensitive_Loss : 0.15076, Sensitive_Acc : 16.879, Sensitive_Auc : 0.988, Mean auc: 0.913, Run Time : 153.87 sec
INFO:root:2024-04-26 17:35:03, Train, Epoch : 5, Step : 2510, Loss : 0.21093, Acc : 0.525, Sensitive_Loss : 0.04119, Sensitive_Acc : 9.500, Run Time : 8.12 sec
INFO:root:2024-04-26 17:35:15, Train, Epoch : 5, Step : 2520, Loss : 0.35857, Acc : 0.853, Sensitive_Loss : 0.12722, Sensitive_Acc : 15.000, Run Time : 12.30 sec
INFO:root:2024-04-26 17:35:26, Train, Epoch : 5, Step : 2530, Loss : 0.38108, Acc : 0.847, Sensitive_Loss : 0.09327, Sensitive_Acc : 15.900, Run Time : 11.14 sec
INFO:root:2024-04-26 17:35:38, Train, Epoch : 5, Step : 2540, Loss : 0.33294, Acc : 0.853, Sensitive_Loss : 0.12478, Sensitive_Acc : 17.100, Run Time : 11.77 sec
INFO:root:2024-04-26 17:35:49, Train, Epoch : 5, Step : 2550, Loss : 0.33358, Acc : 0.847, Sensitive_Loss : 0.09099, Sensitive_Acc : 16.200, Run Time : 11.30 sec
INFO:root:2024-04-26 17:36:01, Train, Epoch : 5, Step : 2560, Loss : 0.27115, Acc : 0.847, Sensitive_Loss : 0.17168, Sensitive_Acc : 16.200, Run Time : 11.70 sec
INFO:root:2024-04-26 17:36:12, Train, Epoch : 5, Step : 2570, Loss : 0.30783, Acc : 0.869, Sensitive_Loss : 0.10722, Sensitive_Acc : 17.600, Run Time : 11.40 sec
INFO:root:2024-04-26 17:36:25, Train, Epoch : 5, Step : 2580, Loss : 0.36198, Acc : 0.838, Sensitive_Loss : 0.13008, Sensitive_Acc : 16.000, Run Time : 12.00 sec
INFO:root:2024-04-26 17:36:37, Train, Epoch : 5, Step : 2590, Loss : 0.29078, Acc : 0.891, Sensitive_Loss : 0.11257, Sensitive_Acc : 17.300, Run Time : 12.30 sec
INFO:root:2024-04-26 17:36:49, Train, Epoch : 5, Step : 2600, Loss : 0.28857, Acc : 0.884, Sensitive_Loss : 0.10374, Sensitive_Acc : 17.000, Run Time : 11.94 sec
INFO:root:2024-04-26 17:39:23, Dev, Step : 2600, Loss : 0.39450, Acc : 0.832, Auc : 0.914, Sensitive_Loss : 0.12711, Sensitive_Acc : 16.964, Sensitive_Auc : 0.989, Mean auc: 0.914, Run Time : 154.45 sec
INFO:root:2024-04-26 17:39:24, Best, Step : 2600, Loss : 0.39450, Acc : 0.832, Auc : 0.914, Sensitive_Loss : 0.12711, Sensitive_Acc : 16.964, Sensitive_Auc : 0.989, Best Auc : 0.914
INFO:root:2024-04-26 17:39:33, Train, Epoch : 5, Step : 2610, Loss : 0.37606, Acc : 0.850, Sensitive_Loss : 0.12339, Sensitive_Acc : 15.900, Run Time : 163.80 sec
INFO:root:2024-04-26 17:39:45, Train, Epoch : 5, Step : 2620, Loss : 0.32785, Acc : 0.881, Sensitive_Loss : 0.11948, Sensitive_Acc : 17.000, Run Time : 12.04 sec
INFO:root:2024-04-26 17:39:57, Train, Epoch : 5, Step : 2630, Loss : 0.29419, Acc : 0.850, Sensitive_Loss : 0.11417, Sensitive_Acc : 15.500, Run Time : 12.10 sec
INFO:root:2024-04-26 17:40:08, Train, Epoch : 5, Step : 2640, Loss : 0.33915, Acc : 0.859, Sensitive_Loss : 0.08824, Sensitive_Acc : 15.300, Run Time : 11.37 sec
INFO:root:2024-04-26 17:40:19, Train, Epoch : 5, Step : 2650, Loss : 0.32114, Acc : 0.859, Sensitive_Loss : 0.13171, Sensitive_Acc : 16.400, Run Time : 10.77 sec
INFO:root:2024-04-26 17:40:31, Train, Epoch : 5, Step : 2660, Loss : 0.34465, Acc : 0.866, Sensitive_Loss : 0.06651, Sensitive_Acc : 16.500, Run Time : 11.96 sec
INFO:root:2024-04-26 17:40:43, Train, Epoch : 5, Step : 2670, Loss : 0.29239, Acc : 0.878, Sensitive_Loss : 0.12568, Sensitive_Acc : 16.900, Run Time : 12.24 sec
INFO:root:2024-04-26 17:40:54, Train, Epoch : 5, Step : 2680, Loss : 0.34071, Acc : 0.875, Sensitive_Loss : 0.10780, Sensitive_Acc : 15.800, Run Time : 11.43 sec
INFO:root:2024-04-26 17:41:06, Train, Epoch : 5, Step : 2690, Loss : 0.26037, Acc : 0.869, Sensitive_Loss : 0.07697, Sensitive_Acc : 15.900, Run Time : 11.93 sec
INFO:root:2024-04-26 17:41:19, Train, Epoch : 5, Step : 2700, Loss : 0.30028, Acc : 0.872, Sensitive_Loss : 0.10429, Sensitive_Acc : 14.900, Run Time : 12.12 sec
INFO:root:2024-04-26 17:43:54, Dev, Step : 2700, Loss : 0.41243, Acc : 0.830, Auc : 0.913, Sensitive_Loss : 0.13086, Sensitive_Acc : 16.964, Sensitive_Auc : 0.988, Mean auc: 0.913, Run Time : 155.89 sec
INFO:root:2024-04-26 17:44:03, Train, Epoch : 5, Step : 2710, Loss : 0.31859, Acc : 0.869, Sensitive_Loss : 0.08145, Sensitive_Acc : 16.100, Run Time : 164.01 sec
INFO:root:2024-04-26 17:44:15, Train, Epoch : 5, Step : 2720, Loss : 0.35224, Acc : 0.859, Sensitive_Loss : 0.09049, Sensitive_Acc : 16.000, Run Time : 12.38 sec
INFO:root:2024-04-26 17:44:26, Train, Epoch : 5, Step : 2730, Loss : 0.31705, Acc : 0.847, Sensitive_Loss : 0.08536, Sensitive_Acc : 16.900, Run Time : 11.40 sec
INFO:root:2024-04-26 17:44:38, Train, Epoch : 5, Step : 2740, Loss : 0.26897, Acc : 0.891, Sensitive_Loss : 0.11650, Sensitive_Acc : 16.700, Run Time : 11.38 sec
INFO:root:2024-04-26 17:44:50, Train, Epoch : 5, Step : 2750, Loss : 0.30799, Acc : 0.881, Sensitive_Loss : 0.12169, Sensitive_Acc : 17.700, Run Time : 12.07 sec
INFO:root:2024-04-26 17:45:02, Train, Epoch : 5, Step : 2760, Loss : 0.30443, Acc : 0.856, Sensitive_Loss : 0.08631, Sensitive_Acc : 16.700, Run Time : 11.85 sec
INFO:root:2024-04-26 17:45:12, Train, Epoch : 5, Step : 2770, Loss : 0.30056, Acc : 0.872, Sensitive_Loss : 0.10709, Sensitive_Acc : 15.900, Run Time : 10.70 sec
INFO:root:2024-04-26 17:45:24, Train, Epoch : 5, Step : 2780, Loss : 0.32151, Acc : 0.878, Sensitive_Loss : 0.09005, Sensitive_Acc : 17.100, Run Time : 11.87 sec
INFO:root:2024-04-26 17:45:36, Train, Epoch : 5, Step : 2790, Loss : 0.27672, Acc : 0.878, Sensitive_Loss : 0.11010, Sensitive_Acc : 16.700, Run Time : 12.06 sec
INFO:root:2024-04-26 17:45:48, Train, Epoch : 5, Step : 2800, Loss : 0.27528, Acc : 0.884, Sensitive_Loss : 0.12364, Sensitive_Acc : 16.000, Run Time : 11.59 sec
INFO:root:2024-04-26 17:48:23, Dev, Step : 2800, Loss : 0.43539, Acc : 0.817, Auc : 0.913, Sensitive_Loss : 0.16972, Sensitive_Acc : 16.836, Sensitive_Auc : 0.987, Mean auc: 0.913, Run Time : 155.16 sec
INFO:root:2024-04-26 17:48:32, Train, Epoch : 5, Step : 2810, Loss : 0.33488, Acc : 0.866, Sensitive_Loss : 0.11239, Sensitive_Acc : 17.000, Run Time : 164.08 sec
INFO:root:2024-04-26 17:48:44, Train, Epoch : 5, Step : 2820, Loss : 0.29898, Acc : 0.872, Sensitive_Loss : 0.13350, Sensitive_Acc : 16.500, Run Time : 12.18 sec
INFO:root:2024-04-26 17:48:56, Train, Epoch : 5, Step : 2830, Loss : 0.31748, Acc : 0.853, Sensitive_Loss : 0.15785, Sensitive_Acc : 16.900, Run Time : 11.89 sec
INFO:root:2024-04-26 17:49:07, Train, Epoch : 5, Step : 2840, Loss : 0.26185, Acc : 0.894, Sensitive_Loss : 0.15580, Sensitive_Acc : 15.100, Run Time : 11.10 sec
INFO:root:2024-04-26 17:49:18, Train, Epoch : 5, Step : 2850, Loss : 0.29206, Acc : 0.875, Sensitive_Loss : 0.11070, Sensitive_Acc : 14.900, Run Time : 11.38 sec
INFO:root:2024-04-26 17:49:31, Train, Epoch : 5, Step : 2860, Loss : 0.29586, Acc : 0.872, Sensitive_Loss : 0.09976, Sensitive_Acc : 17.400, Run Time : 12.17 sec
INFO:root:2024-04-26 17:49:42, Train, Epoch : 5, Step : 2870, Loss : 0.30091, Acc : 0.884, Sensitive_Loss : 0.13025, Sensitive_Acc : 16.100, Run Time : 11.71 sec
INFO:root:2024-04-26 17:49:52, Train, Epoch : 5, Step : 2880, Loss : 0.33164, Acc : 0.875, Sensitive_Loss : 0.09270, Sensitive_Acc : 15.500, Run Time : 9.99 sec
INFO:root:2024-04-26 17:50:04, Train, Epoch : 5, Step : 2890, Loss : 0.28098, Acc : 0.872, Sensitive_Loss : 0.09070, Sensitive_Acc : 16.800, Run Time : 11.56 sec
INFO:root:2024-04-26 17:50:16, Train, Epoch : 5, Step : 2900, Loss : 0.33137, Acc : 0.872, Sensitive_Loss : 0.13456, Sensitive_Acc : 16.100, Run Time : 11.68 sec
INFO:root:2024-04-26 17:52:53, Dev, Step : 2900, Loss : 0.42914, Acc : 0.825, Auc : 0.914, Sensitive_Loss : 0.15723, Sensitive_Acc : 16.836, Sensitive_Auc : 0.988, Mean auc: 0.914, Run Time : 157.53 sec
INFO:root:2024-04-26 17:52:54, Best, Step : 2900, Loss : 0.42914, Acc : 0.825, Auc : 0.914, Sensitive_Loss : 0.15723, Sensitive_Acc : 16.836, Sensitive_Auc : 0.988, Best Auc : 0.914
INFO:root:2024-04-26 17:53:03, Train, Epoch : 5, Step : 2910, Loss : 0.31577, Acc : 0.850, Sensitive_Loss : 0.07591, Sensitive_Acc : 17.900, Run Time : 167.37 sec
INFO:root:2024-04-26 17:53:14, Train, Epoch : 5, Step : 2920, Loss : 0.38548, Acc : 0.859, Sensitive_Loss : 0.10552, Sensitive_Acc : 16.000, Run Time : 11.38 sec
INFO:root:2024-04-26 17:53:26, Train, Epoch : 5, Step : 2930, Loss : 0.26653, Acc : 0.891, Sensitive_Loss : 0.10990, Sensitive_Acc : 15.000, Run Time : 11.27 sec
INFO:root:2024-04-26 17:53:37, Train, Epoch : 5, Step : 2940, Loss : 0.24993, Acc : 0.866, Sensitive_Loss : 0.10033, Sensitive_Acc : 17.000, Run Time : 11.71 sec
INFO:root:2024-04-26 17:53:48, Train, Epoch : 5, Step : 2950, Loss : 0.28731, Acc : 0.872, Sensitive_Loss : 0.09284, Sensitive_Acc : 16.600, Run Time : 11.15 sec
INFO:root:2024-04-26 17:54:00, Train, Epoch : 5, Step : 2960, Loss : 0.28430, Acc : 0.866, Sensitive_Loss : 0.10118, Sensitive_Acc : 16.100, Run Time : 12.00 sec
INFO:root:2024-04-26 17:54:12, Train, Epoch : 5, Step : 2970, Loss : 0.35546, Acc : 0.856, Sensitive_Loss : 0.11374, Sensitive_Acc : 15.400, Run Time : 11.57 sec
INFO:root:2024-04-26 17:54:23, Train, Epoch : 5, Step : 2980, Loss : 0.25462, Acc : 0.881, Sensitive_Loss : 0.12082, Sensitive_Acc : 16.000, Run Time : 11.30 sec
INFO:root:2024-04-26 17:54:34, Train, Epoch : 5, Step : 2990, Loss : 0.34019, Acc : 0.841, Sensitive_Loss : 0.10189, Sensitive_Acc : 15.700, Run Time : 10.96 sec
INFO:root:2024-04-26 17:54:46, Train, Epoch : 5, Step : 3000, Loss : 0.34239, Acc : 0.841, Sensitive_Loss : 0.10104, Sensitive_Acc : 16.300, Run Time : 11.72 sec
INFO:root:2024-04-26 17:57:22, Dev, Step : 3000, Loss : 0.41176, Acc : 0.827, Auc : 0.913, Sensitive_Loss : 0.12909, Sensitive_Acc : 16.936, Sensitive_Auc : 0.988, Mean auc: 0.913, Run Time : 155.79 sec
INFO:root:2024-04-26 17:57:30, Train, Epoch : 5, Step : 3010, Loss : 0.35334, Acc : 0.853, Sensitive_Loss : 0.12137, Sensitive_Acc : 16.200, Run Time : 164.26 sec
INFO:root:2024-04-26 17:57:42, Train, Epoch : 5, Step : 3020, Loss : 0.30718, Acc : 0.866, Sensitive_Loss : 0.08738, Sensitive_Acc : 17.300, Run Time : 11.52 sec
INFO:root:2024-04-26 17:57:53, Train, Epoch : 5, Step : 3030, Loss : 0.31918, Acc : 0.872, Sensitive_Loss : 0.08921, Sensitive_Acc : 17.000, Run Time : 11.71 sec
INFO:root:2024-04-26 17:58:05, Train, Epoch : 5, Step : 3040, Loss : 0.35147, Acc : 0.819, Sensitive_Loss : 0.11375, Sensitive_Acc : 16.000, Run Time : 11.89 sec
INFO:root:2024-04-26 17:58:17, Train, Epoch : 5, Step : 3050, Loss : 0.28680, Acc : 0.894, Sensitive_Loss : 0.11357, Sensitive_Acc : 15.900, Run Time : 11.47 sec
INFO:root:2024-04-26 17:58:28, Train, Epoch : 5, Step : 3060, Loss : 0.33913, Acc : 0.856, Sensitive_Loss : 0.08979, Sensitive_Acc : 14.800, Run Time : 11.58 sec
INFO:root:2024-04-26 17:58:39, Train, Epoch : 5, Step : 3070, Loss : 0.30887, Acc : 0.856, Sensitive_Loss : 0.13093, Sensitive_Acc : 17.000, Run Time : 11.01 sec
INFO:root:2024-04-26 17:58:51, Train, Epoch : 5, Step : 3080, Loss : 0.29152, Acc : 0.887, Sensitive_Loss : 0.09964, Sensitive_Acc : 14.200, Run Time : 12.00 sec
INFO:root:2024-04-26 17:59:03, Train, Epoch : 5, Step : 3090, Loss : 0.32604, Acc : 0.853, Sensitive_Loss : 0.09593, Sensitive_Acc : 17.100, Run Time : 11.60 sec
INFO:root:2024-04-26 17:59:14, Train, Epoch : 5, Step : 3100, Loss : 0.29683, Acc : 0.906, Sensitive_Loss : 0.07581, Sensitive_Acc : 16.800, Run Time : 11.34 sec
INFO:root:2024-04-26 18:01:50, Dev, Step : 3100, Loss : 0.40913, Acc : 0.829, Auc : 0.914, Sensitive_Loss : 0.14263, Sensitive_Acc : 16.921, Sensitive_Auc : 0.987, Mean auc: 0.914, Run Time : 156.05 sec
INFO:root:2024-04-26 18:01:59, Train, Epoch : 5, Step : 3110, Loss : 0.30291, Acc : 0.863, Sensitive_Loss : 0.13716, Sensitive_Acc : 15.800, Run Time : 164.40 sec
INFO:root:2024-04-26 18:02:11, Train, Epoch : 5, Step : 3120, Loss : 0.31711, Acc : 0.856, Sensitive_Loss : 0.06822, Sensitive_Acc : 15.800, Run Time : 12.11 sec
INFO:root:2024-04-26 18:02:21, Train, Epoch : 5, Step : 3130, Loss : 0.31736, Acc : 0.859, Sensitive_Loss : 0.11644, Sensitive_Acc : 16.100, Run Time : 10.59 sec
INFO:root:2024-04-26 18:04:56
INFO:root:y_pred: [1.6154082e-01 9.5706916e-01 2.8028430e-02 ... 9.4057727e-01 7.3451723e-04
 9.0743506e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.2372417e-01 1.9055579e-03 4.6261847e-01 1.8450086e-05 9.9982518e-01
 6.4172980e-04 9.9999261e-01 9.9991941e-01 1.1787237e-02 8.6071444e-01
 9.9744487e-01 9.9986672e-01 9.9800366e-01 9.7584504e-01 3.0546734e-02
 9.6876580e-01 9.9986911e-01 1.9486345e-02 5.2152526e-01 8.7227374e-01
 9.8339069e-01 4.9349096e-02 9.9912196e-01 9.5920408e-01 9.9905854e-01
 9.9516630e-01 1.3921387e-03 9.9884701e-01 9.9891472e-01 5.9525216e-01
 2.4736308e-02 3.9804748e-01 9.9710515e-03 5.3698987e-02 3.1729531e-02
 8.2939388e-03 7.2572298e-02 3.7740814e-03 9.9934775e-01 9.9921739e-01
 5.5420060e-06 1.2754520e-03 9.8676378e-01 3.8486489e-03 9.9996936e-01
 9.9669790e-01 9.9988484e-01 9.9616760e-01 3.3652436e-02 9.9974257e-01
 9.9537694e-01 7.8590000e-03 6.3224900e-01 1.1418005e-02 5.3318865e-03
 2.3158701e-02 1.4420366e-02 8.7605175e-03 2.9904053e-03 1.9434492e-01
 2.1271775e-02 2.9600104e-02 2.8779354e-02 7.8576356e-01 4.9164039e-01
 9.9999619e-01 1.8349146e-03 9.9963057e-01 9.9284536e-01 7.3548543e-01
 7.7742070e-01 5.5083025e-01 5.7889284e-03 3.0723321e-01 2.7106572e-03
 7.1572920e-04 3.8345058e-02 8.8751465e-03 1.0755899e-03 9.9942917e-01
 9.9959689e-01 6.7991839e-04 3.2083850e-02 1.6727393e-02 9.6230280e-01
 5.8607697e-01 8.0352928e-03 1.7607598e-02 9.7583187e-01 9.9982399e-01
 9.9995995e-01 1.7054204e-03 1.5842130e-03 9.9901927e-01 1.6378839e-01
 4.0827673e-03 9.9200070e-01 9.9882764e-01 3.0677111e-04 3.0242187e-01
 9.9912673e-01 9.9619943e-01 9.9941456e-01 9.9485296e-01 1.5095786e-02
 6.7633703e-02 9.9765861e-01 9.9764341e-01 9.7086841e-01 3.6791016e-04
 9.9329984e-01 9.9626476e-01 7.5560533e-02 9.9867225e-01 9.9746633e-01
 9.9794453e-01 6.9901240e-01 9.9866688e-01 1.9930407e-02 1.5751328e-01
 9.9963355e-01 9.9931669e-01 1.5357909e-03 9.8717099e-01 9.9999344e-01
 1.2273174e-01 9.8639166e-01 1.2223846e-02 5.2508149e-02 9.8927665e-01
 9.9867475e-01 3.2305601e-03 5.3319316e-02 2.8767221e-02 9.9949515e-01
 9.9706668e-01 9.8025954e-01 1.9772225e-03 1.3927195e-02 9.9802893e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 18:04:56, Dev, Step : 3130, Loss : 0.40799, Acc : 0.831, Auc : 0.914, Sensitive_Loss : 0.12866, Sensitive_Acc : 16.907, Sensitive_Auc : 0.987, Mean auc: 0.914, Run Time : 154.39 sec
INFO:root:2024-04-26 18:05:10, Train, Epoch : 6, Step : 3140, Loss : 0.31367, Acc : 0.847, Sensitive_Loss : 0.09834, Sensitive_Acc : 17.000, Run Time : 12.84 sec
INFO:root:2024-04-26 18:05:21, Train, Epoch : 6, Step : 3150, Loss : 0.31769, Acc : 0.866, Sensitive_Loss : 0.15337, Sensitive_Acc : 15.600, Run Time : 11.09 sec
INFO:root:2024-04-26 18:05:33, Train, Epoch : 6, Step : 3160, Loss : 0.32480, Acc : 0.866, Sensitive_Loss : 0.12216, Sensitive_Acc : 16.800, Run Time : 12.37 sec
INFO:root:2024-04-26 18:05:45, Train, Epoch : 6, Step : 3170, Loss : 0.32487, Acc : 0.853, Sensitive_Loss : 0.08686, Sensitive_Acc : 16.000, Run Time : 11.77 sec
INFO:root:2024-04-26 18:05:57, Train, Epoch : 6, Step : 3180, Loss : 0.29763, Acc : 0.863, Sensitive_Loss : 0.10535, Sensitive_Acc : 15.200, Run Time : 12.09 sec
INFO:root:2024-04-26 18:06:08, Train, Epoch : 6, Step : 3190, Loss : 0.32462, Acc : 0.850, Sensitive_Loss : 0.09785, Sensitive_Acc : 16.500, Run Time : 11.13 sec
INFO:root:2024-04-26 18:06:20, Train, Epoch : 6, Step : 3200, Loss : 0.30889, Acc : 0.872, Sensitive_Loss : 0.09793, Sensitive_Acc : 17.600, Run Time : 11.85 sec
INFO:root:2024-04-26 18:08:54, Dev, Step : 3200, Loss : 0.41910, Acc : 0.824, Auc : 0.913, Sensitive_Loss : 0.14720, Sensitive_Acc : 16.879, Sensitive_Auc : 0.986, Mean auc: 0.913, Run Time : 154.38 sec
INFO:root:2024-04-26 18:09:03, Train, Epoch : 6, Step : 3210, Loss : 0.29342, Acc : 0.875, Sensitive_Loss : 0.11968, Sensitive_Acc : 15.100, Run Time : 163.28 sec
INFO:root:2024-04-26 18:09:14, Train, Epoch : 6, Step : 3220, Loss : 0.26475, Acc : 0.884, Sensitive_Loss : 0.11023, Sensitive_Acc : 17.400, Run Time : 10.76 sec
INFO:root:2024-04-26 18:09:26, Train, Epoch : 6, Step : 3230, Loss : 0.29733, Acc : 0.894, Sensitive_Loss : 0.08777, Sensitive_Acc : 16.400, Run Time : 11.84 sec
INFO:root:2024-04-26 18:09:38, Train, Epoch : 6, Step : 3240, Loss : 0.29232, Acc : 0.869, Sensitive_Loss : 0.10493, Sensitive_Acc : 16.400, Run Time : 12.17 sec
INFO:root:2024-04-26 18:09:50, Train, Epoch : 6, Step : 3250, Loss : 0.28800, Acc : 0.884, Sensitive_Loss : 0.16008, Sensitive_Acc : 16.000, Run Time : 11.46 sec
INFO:root:2024-04-26 18:10:01, Train, Epoch : 6, Step : 3260, Loss : 0.30424, Acc : 0.856, Sensitive_Loss : 0.12204, Sensitive_Acc : 15.700, Run Time : 10.97 sec
INFO:root:2024-04-26 18:10:13, Train, Epoch : 6, Step : 3270, Loss : 0.32528, Acc : 0.881, Sensitive_Loss : 0.10002, Sensitive_Acc : 16.300, Run Time : 12.08 sec
INFO:root:2024-04-26 18:10:24, Train, Epoch : 6, Step : 3280, Loss : 0.26234, Acc : 0.922, Sensitive_Loss : 0.09293, Sensitive_Acc : 18.700, Run Time : 11.34 sec
INFO:root:2024-04-26 18:10:36, Train, Epoch : 6, Step : 3290, Loss : 0.32408, Acc : 0.894, Sensitive_Loss : 0.07280, Sensitive_Acc : 17.600, Run Time : 11.63 sec
INFO:root:2024-04-26 18:10:47, Train, Epoch : 6, Step : 3300, Loss : 0.33672, Acc : 0.859, Sensitive_Loss : 0.06561, Sensitive_Acc : 15.900, Run Time : 11.75 sec
INFO:root:2024-04-26 18:13:22, Dev, Step : 3300, Loss : 0.41595, Acc : 0.825, Auc : 0.911, Sensitive_Loss : 0.14776, Sensitive_Acc : 16.907, Sensitive_Auc : 0.987, Mean auc: 0.911, Run Time : 154.22 sec
INFO:root:2024-04-26 18:13:30, Train, Epoch : 6, Step : 3310, Loss : 0.28853, Acc : 0.872, Sensitive_Loss : 0.07824, Sensitive_Acc : 16.300, Run Time : 162.77 sec
INFO:root:2024-04-26 18:13:42, Train, Epoch : 6, Step : 3320, Loss : 0.28067, Acc : 0.884, Sensitive_Loss : 0.11631, Sensitive_Acc : 16.000, Run Time : 11.91 sec
INFO:root:2024-04-26 18:13:53, Train, Epoch : 6, Step : 3330, Loss : 0.30095, Acc : 0.859, Sensitive_Loss : 0.07392, Sensitive_Acc : 15.800, Run Time : 11.12 sec
INFO:root:2024-04-26 18:14:05, Train, Epoch : 6, Step : 3340, Loss : 0.30392, Acc : 0.866, Sensitive_Loss : 0.12844, Sensitive_Acc : 15.700, Run Time : 12.21 sec
INFO:root:2024-04-26 18:14:17, Train, Epoch : 6, Step : 3350, Loss : 0.27992, Acc : 0.863, Sensitive_Loss : 0.12815, Sensitive_Acc : 15.400, Run Time : 11.48 sec
INFO:root:2024-04-26 18:14:29, Train, Epoch : 6, Step : 3360, Loss : 0.27401, Acc : 0.878, Sensitive_Loss : 0.14133, Sensitive_Acc : 15.800, Run Time : 11.68 sec
INFO:root:2024-04-26 18:14:40, Train, Epoch : 6, Step : 3370, Loss : 0.30990, Acc : 0.897, Sensitive_Loss : 0.13074, Sensitive_Acc : 16.500, Run Time : 11.61 sec
INFO:root:2024-04-26 18:14:51, Train, Epoch : 6, Step : 3380, Loss : 0.31925, Acc : 0.884, Sensitive_Loss : 0.11061, Sensitive_Acc : 17.200, Run Time : 10.57 sec
INFO:root:2024-04-26 18:15:02, Train, Epoch : 6, Step : 3390, Loss : 0.28229, Acc : 0.878, Sensitive_Loss : 0.11644, Sensitive_Acc : 16.600, Run Time : 11.46 sec
INFO:root:2024-04-26 18:15:14, Train, Epoch : 6, Step : 3400, Loss : 0.27506, Acc : 0.866, Sensitive_Loss : 0.12152, Sensitive_Acc : 17.000, Run Time : 11.72 sec
INFO:root:2024-04-26 18:17:49, Dev, Step : 3400, Loss : 0.43511, Acc : 0.822, Auc : 0.912, Sensitive_Loss : 0.14298, Sensitive_Acc : 16.921, Sensitive_Auc : 0.988, Mean auc: 0.912, Run Time : 154.91 sec
INFO:root:2024-04-26 18:17:58, Train, Epoch : 6, Step : 3410, Loss : 0.28582, Acc : 0.884, Sensitive_Loss : 0.14135, Sensitive_Acc : 15.100, Run Time : 163.99 sec
INFO:root:2024-04-26 18:18:10, Train, Epoch : 6, Step : 3420, Loss : 0.30210, Acc : 0.856, Sensitive_Loss : 0.11859, Sensitive_Acc : 16.600, Run Time : 11.91 sec
INFO:root:2024-04-26 18:18:20, Train, Epoch : 6, Step : 3430, Loss : 0.25577, Acc : 0.863, Sensitive_Loss : 0.08516, Sensitive_Acc : 17.000, Run Time : 10.54 sec
INFO:root:2024-04-26 18:18:32, Train, Epoch : 6, Step : 3440, Loss : 0.23391, Acc : 0.875, Sensitive_Loss : 0.09689, Sensitive_Acc : 16.100, Run Time : 12.03 sec
INFO:root:2024-04-26 18:18:45, Train, Epoch : 6, Step : 3450, Loss : 0.26463, Acc : 0.897, Sensitive_Loss : 0.08127, Sensitive_Acc : 15.500, Run Time : 12.29 sec
INFO:root:2024-04-26 18:18:56, Train, Epoch : 6, Step : 3460, Loss : 0.31777, Acc : 0.887, Sensitive_Loss : 0.16014, Sensitive_Acc : 17.000, Run Time : 11.08 sec
INFO:root:2024-04-26 18:19:08, Train, Epoch : 6, Step : 3470, Loss : 0.31041, Acc : 0.875, Sensitive_Loss : 0.09802, Sensitive_Acc : 16.500, Run Time : 12.33 sec
INFO:root:2024-04-26 18:19:19, Train, Epoch : 6, Step : 3480, Loss : 0.30480, Acc : 0.869, Sensitive_Loss : 0.13660, Sensitive_Acc : 16.300, Run Time : 11.04 sec
INFO:root:2024-04-26 18:19:31, Train, Epoch : 6, Step : 3490, Loss : 0.25723, Acc : 0.872, Sensitive_Loss : 0.13997, Sensitive_Acc : 16.700, Run Time : 11.61 sec
INFO:root:2024-04-26 18:19:42, Train, Epoch : 6, Step : 3500, Loss : 0.29668, Acc : 0.884, Sensitive_Loss : 0.10133, Sensitive_Acc : 16.400, Run Time : 11.56 sec
INFO:root:2024-04-26 18:22:19, Dev, Step : 3500, Loss : 0.45827, Acc : 0.816, Auc : 0.911, Sensitive_Loss : 0.14329, Sensitive_Acc : 16.864, Sensitive_Auc : 0.987, Mean auc: 0.911, Run Time : 156.53 sec
INFO:root:2024-04-26 18:22:27, Train, Epoch : 6, Step : 3510, Loss : 0.34264, Acc : 0.863, Sensitive_Loss : 0.14237, Sensitive_Acc : 17.100, Run Time : 165.19 sec
INFO:root:2024-04-26 18:22:39, Train, Epoch : 6, Step : 3520, Loss : 0.31969, Acc : 0.869, Sensitive_Loss : 0.10427, Sensitive_Acc : 16.400, Run Time : 11.45 sec
INFO:root:2024-04-26 18:22:50, Train, Epoch : 6, Step : 3530, Loss : 0.31818, Acc : 0.859, Sensitive_Loss : 0.09483, Sensitive_Acc : 17.000, Run Time : 11.06 sec
INFO:root:2024-04-26 18:23:02, Train, Epoch : 6, Step : 3540, Loss : 0.33198, Acc : 0.859, Sensitive_Loss : 0.08748, Sensitive_Acc : 16.700, Run Time : 12.12 sec
INFO:root:2024-04-26 18:23:14, Train, Epoch : 6, Step : 3550, Loss : 0.25399, Acc : 0.878, Sensitive_Loss : 0.08874, Sensitive_Acc : 16.800, Run Time : 11.83 sec
INFO:root:2024-04-26 18:23:26, Train, Epoch : 6, Step : 3560, Loss : 0.34315, Acc : 0.847, Sensitive_Loss : 0.11573, Sensitive_Acc : 17.900, Run Time : 11.98 sec
INFO:root:2024-04-26 18:23:39, Train, Epoch : 6, Step : 3570, Loss : 0.25525, Acc : 0.894, Sensitive_Loss : 0.08577, Sensitive_Acc : 15.800, Run Time : 12.78 sec
INFO:root:2024-04-26 18:23:51, Train, Epoch : 6, Step : 3580, Loss : 0.31034, Acc : 0.859, Sensitive_Loss : 0.06580, Sensitive_Acc : 16.400, Run Time : 12.28 sec
INFO:root:2024-04-26 18:24:02, Train, Epoch : 6, Step : 3590, Loss : 0.27597, Acc : 0.850, Sensitive_Loss : 0.08620, Sensitive_Acc : 15.400, Run Time : 11.44 sec
INFO:root:2024-04-26 18:24:14, Train, Epoch : 6, Step : 3600, Loss : 0.29710, Acc : 0.875, Sensitive_Loss : 0.08688, Sensitive_Acc : 16.200, Run Time : 11.81 sec
INFO:root:2024-04-26 18:26:50, Dev, Step : 3600, Loss : 0.43758, Acc : 0.823, Auc : 0.913, Sensitive_Loss : 0.15122, Sensitive_Acc : 16.893, Sensitive_Auc : 0.985, Mean auc: 0.913, Run Time : 156.13 sec
INFO:root:2024-04-26 18:26:59, Train, Epoch : 6, Step : 3610, Loss : 0.32479, Acc : 0.875, Sensitive_Loss : 0.09345, Sensitive_Acc : 16.200, Run Time : 164.58 sec
INFO:root:2024-04-26 18:27:23, Train, Epoch : 6, Step : 3620, Loss : 0.31452, Acc : 0.859, Sensitive_Loss : 0.12701, Sensitive_Acc : 16.100, Run Time : 24.12 sec
INFO:root:2024-04-26 18:27:41, Train, Epoch : 6, Step : 3630, Loss : 0.29586, Acc : 0.875, Sensitive_Loss : 0.08408, Sensitive_Acc : 16.300, Run Time : 18.27 sec
INFO:root:2024-04-26 18:28:04, Train, Epoch : 6, Step : 3640, Loss : 0.30294, Acc : 0.878, Sensitive_Loss : 0.11187, Sensitive_Acc : 16.100, Run Time : 22.88 sec
INFO:root:2024-04-26 18:28:29, Train, Epoch : 6, Step : 3650, Loss : 0.28180, Acc : 0.875, Sensitive_Loss : 0.08648, Sensitive_Acc : 16.000, Run Time : 25.17 sec
INFO:root:2024-04-26 18:28:41, Train, Epoch : 6, Step : 3660, Loss : 0.27618, Acc : 0.891, Sensitive_Loss : 0.11069, Sensitive_Acc : 15.900, Run Time : 12.06 sec
INFO:root:2024-04-26 18:28:54, Train, Epoch : 6, Step : 3670, Loss : 0.27926, Acc : 0.863, Sensitive_Loss : 0.07406, Sensitive_Acc : 15.600, Run Time : 12.30 sec
INFO:root:2024-04-26 18:29:06, Train, Epoch : 6, Step : 3680, Loss : 0.30741, Acc : 0.881, Sensitive_Loss : 0.08932, Sensitive_Acc : 15.600, Run Time : 12.18 sec
INFO:root:2024-04-26 18:29:20, Train, Epoch : 6, Step : 3690, Loss : 0.32289, Acc : 0.856, Sensitive_Loss : 0.08429, Sensitive_Acc : 17.200, Run Time : 13.88 sec
INFO:root:2024-04-26 18:29:34, Train, Epoch : 6, Step : 3700, Loss : 0.32413, Acc : 0.850, Sensitive_Loss : 0.11499, Sensitive_Acc : 16.400, Run Time : 14.75 sec
INFO:root:2024-04-26 18:32:14, Dev, Step : 3700, Loss : 0.41548, Acc : 0.831, Auc : 0.914, Sensitive_Loss : 0.13898, Sensitive_Acc : 16.893, Sensitive_Auc : 0.989, Mean auc: 0.914, Run Time : 159.13 sec
INFO:root:2024-04-26 18:32:23, Train, Epoch : 6, Step : 3710, Loss : 0.25540, Acc : 0.906, Sensitive_Loss : 0.09829, Sensitive_Acc : 16.800, Run Time : 168.24 sec
INFO:root:2024-04-26 18:32:36, Train, Epoch : 6, Step : 3720, Loss : 0.30666, Acc : 0.884, Sensitive_Loss : 0.07664, Sensitive_Acc : 17.700, Run Time : 13.64 sec
INFO:root:2024-04-26 18:32:48, Train, Epoch : 6, Step : 3730, Loss : 0.24430, Acc : 0.869, Sensitive_Loss : 0.06973, Sensitive_Acc : 15.900, Run Time : 11.38 sec
INFO:root:2024-04-26 18:32:59, Train, Epoch : 6, Step : 3740, Loss : 0.30510, Acc : 0.866, Sensitive_Loss : 0.09851, Sensitive_Acc : 14.900, Run Time : 11.78 sec
INFO:root:2024-04-26 18:33:11, Train, Epoch : 6, Step : 3750, Loss : 0.32432, Acc : 0.878, Sensitive_Loss : 0.11334, Sensitive_Acc : 17.300, Run Time : 11.30 sec
INFO:root:2024-04-26 18:35:51
INFO:root:y_pred: [2.33000323e-01 9.62386668e-01 1.48867145e-02 ... 9.29169357e-01
 1.94259643e-04 8.75731170e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.53664899e-01 1.03175128e-03 3.98193777e-01 1.18152857e-05
 9.99935269e-01 1.28665520e-03 9.99996424e-01 9.99971867e-01
 7.88697600e-03 9.14249420e-01 9.97644365e-01 9.99950647e-01
 9.98542309e-01 9.87273097e-01 3.46333981e-02 9.85411465e-01
 9.99936104e-01 2.56850999e-02 6.31821036e-01 8.83166909e-01
 9.87901330e-01 7.51026571e-02 9.99498129e-01 9.43477333e-01
 9.99737680e-01 9.96892631e-01 1.86215888e-03 9.99328494e-01
 9.99517202e-01 8.23444605e-01 3.89688909e-02 1.85928196e-01
 1.18540367e-02 9.96210948e-02 4.91788164e-02 9.42664873e-03
 1.04563326e-01 1.54536346e-03 9.99615073e-01 9.99468267e-01
 4.08287633e-06 1.06347643e-03 9.88514960e-01 9.64507274e-03
 9.99979973e-01 9.97866213e-01 9.99912739e-01 9.96695280e-01
 3.79653312e-02 9.99851704e-01 9.97557640e-01 8.07533320e-03
 6.56318724e-01 1.40543012e-02 2.90023861e-03 1.06354281e-02
 1.31671168e-02 6.63865032e-03 3.43499309e-03 2.18647525e-01
 5.80994785e-02 3.53151187e-02 5.90148978e-02 7.78846323e-01
 6.00322604e-01 9.99997973e-01 3.16078705e-03 9.99776542e-01
 9.97405589e-01 6.42350793e-01 7.71593153e-01 6.46229744e-01
 3.20913154e-03 3.89320791e-01 5.09702694e-03 3.20700667e-04
 6.89604133e-02 1.19545460e-02 6.10566174e-04 9.99555290e-01
 9.99758899e-01 4.67919104e-04 2.45566629e-02 4.61586975e-02
 9.48618174e-01 4.87274051e-01 1.14485091e-02 2.05609873e-02
 9.81075346e-01 9.99926925e-01 9.99988675e-01 2.97722127e-03
 1.79714803e-03 9.99225616e-01 1.23659074e-01 2.39248876e-03
 9.98618722e-01 9.99398232e-01 1.51449494e-04 1.91887334e-01
 9.99478400e-01 9.98725116e-01 9.99246240e-01 9.97246742e-01
 1.86960343e-02 3.22392210e-02 9.98799443e-01 9.97789502e-01
 9.84715462e-01 3.06314614e-04 9.96860743e-01 9.98574376e-01
 1.74946114e-01 9.98995960e-01 9.98452902e-01 9.98934567e-01
 8.99482310e-01 9.99126017e-01 4.26313765e-02 2.56337047e-01
 9.99792874e-01 9.99476492e-01 1.32069713e-03 9.90829706e-01
 9.99995589e-01 2.41484299e-01 9.91355717e-01 9.97582451e-03
 2.60149762e-02 9.92548645e-01 9.99386907e-01 3.23705375e-03
 9.45942104e-02 7.73467775e-03 9.99783576e-01 9.96406138e-01
 9.85872209e-01 3.45526007e-03 1.67866796e-02 9.98807430e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 18:35:51, Dev, Step : 3756, Loss : 0.43502, Acc : 0.824, Auc : 0.913, Sensitive_Loss : 0.13894, Sensitive_Acc : 16.879, Sensitive_Auc : 0.988, Mean auc: 0.913, Run Time : 153.85 sec
INFO:root:2024-04-26 18:35:59, Train, Epoch : 7, Step : 3760, Loss : 0.10210, Acc : 0.359, Sensitive_Loss : 0.06033, Sensitive_Acc : 6.900, Run Time : 6.22 sec
INFO:root:2024-04-26 18:36:10, Train, Epoch : 7, Step : 3770, Loss : 0.31522, Acc : 0.878, Sensitive_Loss : 0.08486, Sensitive_Acc : 17.300, Run Time : 11.44 sec
INFO:root:2024-04-26 18:36:22, Train, Epoch : 7, Step : 3780, Loss : 0.24045, Acc : 0.900, Sensitive_Loss : 0.09499, Sensitive_Acc : 16.600, Run Time : 12.07 sec
INFO:root:2024-04-26 18:36:34, Train, Epoch : 7, Step : 3790, Loss : 0.24762, Acc : 0.884, Sensitive_Loss : 0.10890, Sensitive_Acc : 15.900, Run Time : 11.41 sec
INFO:root:2024-04-26 18:36:45, Train, Epoch : 7, Step : 3800, Loss : 0.28656, Acc : 0.884, Sensitive_Loss : 0.11666, Sensitive_Acc : 18.400, Run Time : 11.44 sec
INFO:root:2024-04-26 18:39:20, Dev, Step : 3800, Loss : 0.41203, Acc : 0.832, Auc : 0.915, Sensitive_Loss : 0.13566, Sensitive_Acc : 16.921, Sensitive_Auc : 0.989, Mean auc: 0.915, Run Time : 154.79 sec
INFO:root:2024-04-26 18:39:21, Best, Step : 3800, Loss : 0.41203, Acc : 0.832, Auc : 0.915, Sensitive_Loss : 0.13566, Sensitive_Acc : 16.921, Sensitive_Auc : 0.989, Best Auc : 0.915
INFO:root:2024-04-26 18:39:29, Train, Epoch : 7, Step : 3810, Loss : 0.31900, Acc : 0.859, Sensitive_Loss : 0.10928, Sensitive_Acc : 16.100, Run Time : 163.44 sec
INFO:root:2024-04-26 18:39:41, Train, Epoch : 7, Step : 3820, Loss : 0.29825, Acc : 0.872, Sensitive_Loss : 0.10276, Sensitive_Acc : 15.800, Run Time : 12.23 sec
INFO:root:2024-04-26 18:39:52, Train, Epoch : 7, Step : 3830, Loss : 0.28012, Acc : 0.887, Sensitive_Loss : 0.08619, Sensitive_Acc : 16.200, Run Time : 11.02 sec
INFO:root:2024-04-26 18:40:03, Train, Epoch : 7, Step : 3840, Loss : 0.30114, Acc : 0.875, Sensitive_Loss : 0.09044, Sensitive_Acc : 18.100, Run Time : 10.84 sec
INFO:root:2024-04-26 18:40:15, Train, Epoch : 7, Step : 3850, Loss : 0.22227, Acc : 0.931, Sensitive_Loss : 0.10203, Sensitive_Acc : 16.300, Run Time : 11.92 sec
INFO:root:2024-04-26 18:40:26, Train, Epoch : 7, Step : 3860, Loss : 0.29083, Acc : 0.887, Sensitive_Loss : 0.08701, Sensitive_Acc : 16.600, Run Time : 11.88 sec
INFO:root:2024-04-26 18:40:38, Train, Epoch : 7, Step : 3870, Loss : 0.23013, Acc : 0.900, Sensitive_Loss : 0.09936, Sensitive_Acc : 15.700, Run Time : 11.81 sec
INFO:root:2024-04-26 18:40:49, Train, Epoch : 7, Step : 3880, Loss : 0.25860, Acc : 0.881, Sensitive_Loss : 0.09063, Sensitive_Acc : 15.500, Run Time : 10.80 sec
INFO:root:2024-04-26 18:41:01, Train, Epoch : 7, Step : 3890, Loss : 0.31435, Acc : 0.859, Sensitive_Loss : 0.10002, Sensitive_Acc : 15.200, Run Time : 11.71 sec
INFO:root:2024-04-26 18:41:12, Train, Epoch : 7, Step : 3900, Loss : 0.22558, Acc : 0.906, Sensitive_Loss : 0.12137, Sensitive_Acc : 17.300, Run Time : 11.19 sec
INFO:root:2024-04-26 18:43:48, Dev, Step : 3900, Loss : 0.44541, Acc : 0.820, Auc : 0.912, Sensitive_Loss : 0.16556, Sensitive_Acc : 16.850, Sensitive_Auc : 0.986, Mean auc: 0.912, Run Time : 155.65 sec
INFO:root:2024-04-26 18:43:56, Train, Epoch : 7, Step : 3910, Loss : 0.27869, Acc : 0.894, Sensitive_Loss : 0.10263, Sensitive_Acc : 14.700, Run Time : 163.73 sec
INFO:root:2024-04-26 18:44:07, Train, Epoch : 7, Step : 3920, Loss : 0.31447, Acc : 0.844, Sensitive_Loss : 0.09534, Sensitive_Acc : 16.000, Run Time : 11.64 sec
INFO:root:2024-04-26 18:44:19, Train, Epoch : 7, Step : 3930, Loss : 0.23527, Acc : 0.891, Sensitive_Loss : 0.12308, Sensitive_Acc : 16.000, Run Time : 11.56 sec
INFO:root:2024-04-26 18:44:31, Train, Epoch : 7, Step : 3940, Loss : 0.26802, Acc : 0.878, Sensitive_Loss : 0.10291, Sensitive_Acc : 16.900, Run Time : 12.53 sec
INFO:root:2024-04-26 18:44:43, Train, Epoch : 7, Step : 3950, Loss : 0.26233, Acc : 0.891, Sensitive_Loss : 0.11336, Sensitive_Acc : 14.800, Run Time : 11.30 sec
INFO:root:2024-04-26 18:44:54, Train, Epoch : 7, Step : 3960, Loss : 0.25237, Acc : 0.897, Sensitive_Loss : 0.10619, Sensitive_Acc : 16.300, Run Time : 11.26 sec
INFO:root:2024-04-26 18:45:06, Train, Epoch : 7, Step : 3970, Loss : 0.33634, Acc : 0.863, Sensitive_Loss : 0.12964, Sensitive_Acc : 17.200, Run Time : 12.08 sec
INFO:root:2024-04-26 18:45:18, Train, Epoch : 7, Step : 3980, Loss : 0.28301, Acc : 0.878, Sensitive_Loss : 0.16618, Sensitive_Acc : 16.700, Run Time : 11.99 sec
INFO:root:2024-04-26 18:45:30, Train, Epoch : 7, Step : 3990, Loss : 0.24151, Acc : 0.881, Sensitive_Loss : 0.10198, Sensitive_Acc : 16.300, Run Time : 11.56 sec
INFO:root:2024-04-26 18:45:41, Train, Epoch : 7, Step : 4000, Loss : 0.30110, Acc : 0.881, Sensitive_Loss : 0.10879, Sensitive_Acc : 15.900, Run Time : 10.88 sec
INFO:root:2024-04-26 18:48:16, Dev, Step : 4000, Loss : 0.42644, Acc : 0.826, Auc : 0.912, Sensitive_Loss : 0.13555, Sensitive_Acc : 16.921, Sensitive_Auc : 0.987, Mean auc: 0.912, Run Time : 155.93 sec
INFO:root:2024-04-26 18:48:25, Train, Epoch : 7, Step : 4010, Loss : 0.28665, Acc : 0.891, Sensitive_Loss : 0.08838, Sensitive_Acc : 16.500, Run Time : 164.82 sec
INFO:root:2024-04-26 18:48:37, Train, Epoch : 7, Step : 4020, Loss : 0.24971, Acc : 0.875, Sensitive_Loss : 0.10783, Sensitive_Acc : 18.100, Run Time : 11.91 sec
INFO:root:2024-04-26 18:48:49, Train, Epoch : 7, Step : 4030, Loss : 0.29451, Acc : 0.897, Sensitive_Loss : 0.10436, Sensitive_Acc : 15.000, Run Time : 11.53 sec
INFO:root:2024-04-26 18:49:00, Train, Epoch : 7, Step : 4040, Loss : 0.28540, Acc : 0.869, Sensitive_Loss : 0.10597, Sensitive_Acc : 15.900, Run Time : 11.45 sec
INFO:root:2024-04-26 18:49:12, Train, Epoch : 7, Step : 4050, Loss : 0.27871, Acc : 0.887, Sensitive_Loss : 0.11802, Sensitive_Acc : 17.600, Run Time : 11.65 sec
INFO:root:2024-04-26 18:49:25, Train, Epoch : 7, Step : 4060, Loss : 0.33338, Acc : 0.856, Sensitive_Loss : 0.10637, Sensitive_Acc : 15.700, Run Time : 12.88 sec
INFO:root:2024-04-26 18:49:36, Train, Epoch : 7, Step : 4070, Loss : 0.25619, Acc : 0.875, Sensitive_Loss : 0.11770, Sensitive_Acc : 15.400, Run Time : 11.55 sec
INFO:root:2024-04-26 18:49:48, Train, Epoch : 7, Step : 4080, Loss : 0.30555, Acc : 0.863, Sensitive_Loss : 0.12341, Sensitive_Acc : 16.600, Run Time : 11.94 sec
INFO:root:2024-04-26 18:50:00, Train, Epoch : 7, Step : 4090, Loss : 0.30332, Acc : 0.887, Sensitive_Loss : 0.08773, Sensitive_Acc : 16.500, Run Time : 11.86 sec
INFO:root:2024-04-26 18:50:12, Train, Epoch : 7, Step : 4100, Loss : 0.29941, Acc : 0.875, Sensitive_Loss : 0.08007, Sensitive_Acc : 16.400, Run Time : 11.68 sec
INFO:root:2024-04-26 18:52:47, Dev, Step : 4100, Loss : 0.41586, Acc : 0.830, Auc : 0.909, Sensitive_Loss : 0.12448, Sensitive_Acc : 16.864, Sensitive_Auc : 0.988, Mean auc: 0.909, Run Time : 155.37 sec
INFO:root:2024-04-26 18:52:55, Train, Epoch : 7, Step : 4110, Loss : 0.28066, Acc : 0.869, Sensitive_Loss : 0.10508, Sensitive_Acc : 16.400, Run Time : 163.60 sec
INFO:root:2024-04-26 18:53:07, Train, Epoch : 7, Step : 4120, Loss : 0.26602, Acc : 0.900, Sensitive_Loss : 0.09465, Sensitive_Acc : 16.300, Run Time : 11.69 sec
INFO:root:2024-04-26 18:53:19, Train, Epoch : 7, Step : 4130, Loss : 0.28247, Acc : 0.897, Sensitive_Loss : 0.10028, Sensitive_Acc : 15.800, Run Time : 11.64 sec
INFO:root:2024-04-26 18:53:30, Train, Epoch : 7, Step : 4140, Loss : 0.27928, Acc : 0.875, Sensitive_Loss : 0.10017, Sensitive_Acc : 18.500, Run Time : 11.69 sec
INFO:root:2024-04-26 18:53:42, Train, Epoch : 7, Step : 4150, Loss : 0.30477, Acc : 0.881, Sensitive_Loss : 0.09421, Sensitive_Acc : 17.200, Run Time : 12.03 sec
INFO:root:2024-04-26 18:53:54, Train, Epoch : 7, Step : 4160, Loss : 0.34327, Acc : 0.841, Sensitive_Loss : 0.07922, Sensitive_Acc : 16.200, Run Time : 11.76 sec
INFO:root:2024-04-26 18:54:06, Train, Epoch : 7, Step : 4170, Loss : 0.28518, Acc : 0.881, Sensitive_Loss : 0.06965, Sensitive_Acc : 16.000, Run Time : 11.87 sec
INFO:root:2024-04-26 18:54:18, Train, Epoch : 7, Step : 4180, Loss : 0.24501, Acc : 0.900, Sensitive_Loss : 0.08708, Sensitive_Acc : 16.900, Run Time : 11.81 sec
INFO:root:2024-04-26 18:54:29, Train, Epoch : 7, Step : 4190, Loss : 0.28581, Acc : 0.875, Sensitive_Loss : 0.09434, Sensitive_Acc : 16.800, Run Time : 10.74 sec
INFO:root:2024-04-26 18:54:40, Train, Epoch : 7, Step : 4200, Loss : 0.22730, Acc : 0.919, Sensitive_Loss : 0.14659, Sensitive_Acc : 16.000, Run Time : 11.83 sec
INFO:root:2024-04-26 18:57:15, Dev, Step : 4200, Loss : 0.43796, Acc : 0.821, Auc : 0.912, Sensitive_Loss : 0.13448, Sensitive_Acc : 16.807, Sensitive_Auc : 0.991, Mean auc: 0.912, Run Time : 154.57 sec
INFO:root:2024-04-26 18:57:23, Train, Epoch : 7, Step : 4210, Loss : 0.26183, Acc : 0.869, Sensitive_Loss : 0.10565, Sensitive_Acc : 16.500, Run Time : 162.91 sec
INFO:root:2024-04-26 18:57:34, Train, Epoch : 7, Step : 4220, Loss : 0.24169, Acc : 0.887, Sensitive_Loss : 0.07876, Sensitive_Acc : 17.200, Run Time : 11.08 sec
INFO:root:2024-04-26 18:57:46, Train, Epoch : 7, Step : 4230, Loss : 0.24603, Acc : 0.887, Sensitive_Loss : 0.09120, Sensitive_Acc : 17.000, Run Time : 11.92 sec
INFO:root:2024-04-26 18:57:57, Train, Epoch : 7, Step : 4240, Loss : 0.34063, Acc : 0.872, Sensitive_Loss : 0.07184, Sensitive_Acc : 16.400, Run Time : 10.87 sec
INFO:root:2024-04-26 18:58:09, Train, Epoch : 7, Step : 4250, Loss : 0.30518, Acc : 0.872, Sensitive_Loss : 0.12280, Sensitive_Acc : 16.400, Run Time : 11.80 sec
INFO:root:2024-04-26 18:58:20, Train, Epoch : 7, Step : 4260, Loss : 0.31853, Acc : 0.859, Sensitive_Loss : 0.07775, Sensitive_Acc : 17.000, Run Time : 11.39 sec
INFO:root:2024-04-26 18:58:32, Train, Epoch : 7, Step : 4270, Loss : 0.27448, Acc : 0.897, Sensitive_Loss : 0.09308, Sensitive_Acc : 17.400, Run Time : 11.47 sec
INFO:root:2024-04-26 18:58:43, Train, Epoch : 7, Step : 4280, Loss : 0.28310, Acc : 0.884, Sensitive_Loss : 0.13498, Sensitive_Acc : 17.600, Run Time : 11.32 sec
INFO:root:2024-04-26 18:58:55, Train, Epoch : 7, Step : 4290, Loss : 0.25800, Acc : 0.903, Sensitive_Loss : 0.12227, Sensitive_Acc : 15.400, Run Time : 12.17 sec
INFO:root:2024-04-26 18:59:06, Train, Epoch : 7, Step : 4300, Loss : 0.30058, Acc : 0.875, Sensitive_Loss : 0.08485, Sensitive_Acc : 15.400, Run Time : 10.54 sec
INFO:root:2024-04-26 19:01:42, Dev, Step : 4300, Loss : 0.45390, Acc : 0.817, Auc : 0.912, Sensitive_Loss : 0.14471, Sensitive_Acc : 16.864, Sensitive_Auc : 0.989, Mean auc: 0.912, Run Time : 155.76 sec
INFO:root:2024-04-26 19:01:50, Train, Epoch : 7, Step : 4310, Loss : 0.29422, Acc : 0.875, Sensitive_Loss : 0.06278, Sensitive_Acc : 15.900, Run Time : 164.21 sec
INFO:root:2024-04-26 19:02:02, Train, Epoch : 7, Step : 4320, Loss : 0.29916, Acc : 0.872, Sensitive_Loss : 0.11534, Sensitive_Acc : 15.600, Run Time : 11.41 sec
INFO:root:2024-04-26 19:02:13, Train, Epoch : 7, Step : 4330, Loss : 0.22716, Acc : 0.887, Sensitive_Loss : 0.08764, Sensitive_Acc : 17.600, Run Time : 11.72 sec
INFO:root:2024-04-26 19:02:25, Train, Epoch : 7, Step : 4340, Loss : 0.25812, Acc : 0.881, Sensitive_Loss : 0.09774, Sensitive_Acc : 15.700, Run Time : 11.84 sec
INFO:root:2024-04-26 19:02:37, Train, Epoch : 7, Step : 4350, Loss : 0.25906, Acc : 0.881, Sensitive_Loss : 0.08524, Sensitive_Acc : 15.800, Run Time : 11.56 sec
INFO:root:2024-04-26 19:02:49, Train, Epoch : 7, Step : 4360, Loss : 0.24922, Acc : 0.887, Sensitive_Loss : 0.09239, Sensitive_Acc : 16.100, Run Time : 12.42 sec
INFO:root:2024-04-26 19:03:01, Train, Epoch : 7, Step : 4370, Loss : 0.30029, Acc : 0.859, Sensitive_Loss : 0.06674, Sensitive_Acc : 15.100, Run Time : 11.63 sec
INFO:root:2024-04-26 19:03:12, Train, Epoch : 7, Step : 4380, Loss : 0.24362, Acc : 0.894, Sensitive_Loss : 0.12960, Sensitive_Acc : 14.700, Run Time : 10.87 sec
INFO:root:2024-04-26 19:05:47
INFO:root:y_pred: [1.0078724e-01 9.7706473e-01 7.9748034e-03 ... 8.8920182e-01 6.0594110e-05
 7.8739184e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.8750883e-01 2.3819983e-03 6.2459320e-01 6.5281172e-05 9.9988902e-01
 1.5675119e-03 9.9999690e-01 9.9997962e-01 9.5440205e-03 9.6203625e-01
 9.9878627e-01 9.9997699e-01 9.9898630e-01 9.9237096e-01 4.0145457e-02
 9.9329209e-01 9.9997580e-01 4.3652397e-02 8.1252170e-01 9.3776751e-01
 9.9167246e-01 1.1328535e-01 9.9935406e-01 9.5971614e-01 9.9986517e-01
 9.9820065e-01 3.8555283e-03 9.9904126e-01 9.9934047e-01 7.8370637e-01
 5.4013241e-02 2.6900390e-01 2.5832554e-02 1.9299385e-01 8.5112259e-02
 1.0233324e-02 2.4881363e-01 1.8970277e-03 9.9988520e-01 9.9967670e-01
 4.8846396e-06 1.3603813e-03 9.9693513e-01 1.8404905e-02 9.9999857e-01
 9.9890590e-01 9.9995518e-01 9.9901986e-01 4.3382794e-02 9.9985039e-01
 9.9820888e-01 1.1235923e-02 6.0257775e-01 1.3067685e-02 1.2195006e-02
 4.5139238e-02 1.2055682e-02 5.3008082e-03 1.0051045e-02 3.7507904e-01
 1.0466136e-01 3.7072860e-02 9.1594361e-02 9.3171638e-01 5.0826389e-01
 9.9999857e-01 5.9059463e-03 9.9986935e-01 9.9785668e-01 6.4914286e-01
 8.8535297e-01 7.1912497e-01 9.1048395e-03 4.4881797e-01 7.0948526e-03
 1.0424834e-03 1.3396497e-01 1.9259524e-02 1.4648744e-03 9.9979383e-01
 9.9983358e-01 8.2829490e-04 4.9011745e-02 6.1450653e-02 9.7999328e-01
 5.9860873e-01 1.1556346e-02 5.2640971e-02 9.8145103e-01 9.9995792e-01
 9.9999607e-01 2.8339659e-03 6.5608392e-03 9.9948478e-01 3.2381657e-01
 4.0034442e-03 9.9929190e-01 9.9964392e-01 3.8530710e-04 4.4737062e-01
 9.9956053e-01 9.9934119e-01 9.9980205e-01 9.9815863e-01 4.3664753e-02
 9.9588595e-02 9.9804640e-01 9.9827135e-01 9.9211007e-01 4.9567461e-04
 9.9791783e-01 9.9947160e-01 2.1459180e-01 9.9957818e-01 9.9892920e-01
 9.9951637e-01 8.9676952e-01 9.9942857e-01 6.6250168e-02 2.8068751e-01
 9.9989986e-01 9.9971229e-01 1.1886000e-03 9.9494272e-01 9.9999678e-01
 3.5658428e-01 9.9231601e-01 3.2976899e-02 6.9527656e-02 9.9790764e-01
 9.9945337e-01 3.7654941e-03 1.4148520e-01 2.9020486e-02 9.9982989e-01
 9.9758589e-01 9.9322975e-01 3.9157593e-03 3.9464295e-02 9.9931645e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 19:05:47, Dev, Step : 4382, Loss : 0.50650, Acc : 0.799, Auc : 0.910, Sensitive_Loss : 0.17243, Sensitive_Acc : 16.879, Sensitive_Auc : 0.989, Mean auc: 0.910, Run Time : 153.64 sec
INFO:root:2024-04-26 19:05:59, Train, Epoch : 8, Step : 4390, Loss : 0.22841, Acc : 0.700, Sensitive_Loss : 0.07134, Sensitive_Acc : 13.200, Run Time : 10.51 sec
INFO:root:2024-04-26 19:06:12, Train, Epoch : 8, Step : 4400, Loss : 0.21290, Acc : 0.919, Sensitive_Loss : 0.12145, Sensitive_Acc : 15.500, Run Time : 12.90 sec
INFO:root:2024-04-26 19:08:46, Dev, Step : 4400, Loss : 0.44927, Acc : 0.819, Auc : 0.911, Sensitive_Loss : 0.14102, Sensitive_Acc : 16.821, Sensitive_Auc : 0.989, Mean auc: 0.911, Run Time : 154.60 sec
INFO:root:2024-04-26 19:08:55, Train, Epoch : 8, Step : 4410, Loss : 0.24538, Acc : 0.922, Sensitive_Loss : 0.10357, Sensitive_Acc : 15.100, Run Time : 163.37 sec
INFO:root:2024-04-26 19:09:06, Train, Epoch : 8, Step : 4420, Loss : 0.26229, Acc : 0.869, Sensitive_Loss : 0.13836, Sensitive_Acc : 17.100, Run Time : 10.79 sec
INFO:root:2024-04-26 19:09:18, Train, Epoch : 8, Step : 4430, Loss : 0.22589, Acc : 0.916, Sensitive_Loss : 0.11436, Sensitive_Acc : 15.400, Run Time : 11.85 sec
INFO:root:2024-04-26 19:09:29, Train, Epoch : 8, Step : 4440, Loss : 0.25423, Acc : 0.878, Sensitive_Loss : 0.09473, Sensitive_Acc : 16.700, Run Time : 11.22 sec
INFO:root:2024-04-26 19:09:41, Train, Epoch : 8, Step : 4450, Loss : 0.26648, Acc : 0.891, Sensitive_Loss : 0.08201, Sensitive_Acc : 16.800, Run Time : 11.64 sec
INFO:root:2024-04-26 19:09:53, Train, Epoch : 8, Step : 4460, Loss : 0.27176, Acc : 0.884, Sensitive_Loss : 0.09672, Sensitive_Acc : 16.400, Run Time : 11.93 sec
INFO:root:2024-04-26 19:10:04, Train, Epoch : 8, Step : 4470, Loss : 0.28239, Acc : 0.891, Sensitive_Loss : 0.12208, Sensitive_Acc : 16.600, Run Time : 11.12 sec
INFO:root:2024-04-26 19:10:15, Train, Epoch : 8, Step : 4480, Loss : 0.25277, Acc : 0.891, Sensitive_Loss : 0.11203, Sensitive_Acc : 17.800, Run Time : 11.30 sec
INFO:root:2024-04-26 19:10:28, Train, Epoch : 8, Step : 4490, Loss : 0.21508, Acc : 0.919, Sensitive_Loss : 0.08801, Sensitive_Acc : 17.700, Run Time : 12.69 sec
INFO:root:2024-04-26 19:10:39, Train, Epoch : 8, Step : 4500, Loss : 0.28551, Acc : 0.897, Sensitive_Loss : 0.10818, Sensitive_Acc : 16.600, Run Time : 11.00 sec
INFO:root:2024-04-26 19:13:15, Dev, Step : 4500, Loss : 0.44986, Acc : 0.822, Auc : 0.909, Sensitive_Loss : 0.15160, Sensitive_Acc : 16.879, Sensitive_Auc : 0.987, Mean auc: 0.909, Run Time : 156.07 sec
INFO:root:2024-04-26 19:13:23, Train, Epoch : 8, Step : 4510, Loss : 0.30280, Acc : 0.859, Sensitive_Loss : 0.07291, Sensitive_Acc : 16.000, Run Time : 164.74 sec
INFO:root:2024-04-26 19:13:36, Train, Epoch : 8, Step : 4520, Loss : 0.21157, Acc : 0.909, Sensitive_Loss : 0.06099, Sensitive_Acc : 16.500, Run Time : 12.36 sec
INFO:root:2024-04-26 19:13:47, Train, Epoch : 8, Step : 4530, Loss : 0.23520, Acc : 0.897, Sensitive_Loss : 0.09932, Sensitive_Acc : 15.700, Run Time : 11.23 sec
INFO:root:2024-04-26 19:13:59, Train, Epoch : 8, Step : 4540, Loss : 0.28665, Acc : 0.872, Sensitive_Loss : 0.11580, Sensitive_Acc : 15.400, Run Time : 11.99 sec
INFO:root:2024-04-26 19:14:10, Train, Epoch : 8, Step : 4550, Loss : 0.23565, Acc : 0.894, Sensitive_Loss : 0.10426, Sensitive_Acc : 17.900, Run Time : 10.95 sec
INFO:root:2024-04-26 19:14:22, Train, Epoch : 8, Step : 4560, Loss : 0.23518, Acc : 0.925, Sensitive_Loss : 0.08866, Sensitive_Acc : 16.900, Run Time : 12.29 sec
INFO:root:2024-04-26 19:14:34, Train, Epoch : 8, Step : 4570, Loss : 0.25503, Acc : 0.900, Sensitive_Loss : 0.08421, Sensitive_Acc : 16.800, Run Time : 11.51 sec
INFO:root:2024-04-26 19:14:45, Train, Epoch : 8, Step : 4580, Loss : 0.26975, Acc : 0.894, Sensitive_Loss : 0.10399, Sensitive_Acc : 17.900, Run Time : 11.51 sec
INFO:root:2024-04-26 19:14:56, Train, Epoch : 8, Step : 4590, Loss : 0.26514, Acc : 0.844, Sensitive_Loss : 0.09555, Sensitive_Acc : 15.700, Run Time : 11.21 sec
INFO:root:2024-04-26 19:15:08, Train, Epoch : 8, Step : 4600, Loss : 0.30612, Acc : 0.897, Sensitive_Loss : 0.12336, Sensitive_Acc : 16.700, Run Time : 11.69 sec
INFO:root:2024-04-26 19:17:45, Dev, Step : 4600, Loss : 0.42197, Acc : 0.833, Auc : 0.908, Sensitive_Loss : 0.12641, Sensitive_Acc : 16.836, Sensitive_Auc : 0.989, Mean auc: 0.908, Run Time : 156.86 sec
INFO:root:2024-04-26 19:17:54, Train, Epoch : 8, Step : 4610, Loss : 0.25788, Acc : 0.859, Sensitive_Loss : 0.07408, Sensitive_Acc : 16.400, Run Time : 165.61 sec
INFO:root:2024-04-26 19:18:06, Train, Epoch : 8, Step : 4620, Loss : 0.23831, Acc : 0.897, Sensitive_Loss : 0.12446, Sensitive_Acc : 15.800, Run Time : 12.28 sec
INFO:root:2024-04-26 19:18:18, Train, Epoch : 8, Step : 4630, Loss : 0.23551, Acc : 0.922, Sensitive_Loss : 0.09722, Sensitive_Acc : 15.300, Run Time : 11.56 sec
INFO:root:2024-04-26 19:18:29, Train, Epoch : 8, Step : 4640, Loss : 0.33250, Acc : 0.881, Sensitive_Loss : 0.06848, Sensitive_Acc : 16.100, Run Time : 11.41 sec
INFO:root:2024-04-26 19:18:41, Train, Epoch : 8, Step : 4650, Loss : 0.32724, Acc : 0.881, Sensitive_Loss : 0.12535, Sensitive_Acc : 16.700, Run Time : 11.72 sec
INFO:root:2024-04-26 19:18:51, Train, Epoch : 8, Step : 4660, Loss : 0.24663, Acc : 0.894, Sensitive_Loss : 0.11792, Sensitive_Acc : 16.200, Run Time : 10.34 sec
INFO:root:2024-04-26 19:19:03, Train, Epoch : 8, Step : 4670, Loss : 0.26269, Acc : 0.900, Sensitive_Loss : 0.08890, Sensitive_Acc : 17.700, Run Time : 11.73 sec
INFO:root:2024-04-26 19:19:15, Train, Epoch : 8, Step : 4680, Loss : 0.24436, Acc : 0.909, Sensitive_Loss : 0.10964, Sensitive_Acc : 16.600, Run Time : 12.28 sec
INFO:root:2024-04-26 19:19:26, Train, Epoch : 8, Step : 4690, Loss : 0.23979, Acc : 0.912, Sensitive_Loss : 0.10906, Sensitive_Acc : 16.800, Run Time : 10.94 sec
INFO:root:2024-04-26 19:19:38, Train, Epoch : 8, Step : 4700, Loss : 0.26310, Acc : 0.884, Sensitive_Loss : 0.11171, Sensitive_Acc : 17.400, Run Time : 11.90 sec
INFO:root:2024-04-26 19:22:14, Dev, Step : 4700, Loss : 0.43481, Acc : 0.826, Auc : 0.909, Sensitive_Loss : 0.13303, Sensitive_Acc : 16.879, Sensitive_Auc : 0.990, Mean auc: 0.909, Run Time : 156.01 sec
INFO:root:2024-04-26 19:22:22, Train, Epoch : 8, Step : 4710, Loss : 0.22867, Acc : 0.909, Sensitive_Loss : 0.07487, Sensitive_Acc : 15.900, Run Time : 164.35 sec
INFO:root:2024-04-26 19:22:34, Train, Epoch : 8, Step : 4720, Loss : 0.25244, Acc : 0.912, Sensitive_Loss : 0.10113, Sensitive_Acc : 15.800, Run Time : 11.76 sec
INFO:root:2024-04-26 19:22:46, Train, Epoch : 8, Step : 4730, Loss : 0.24244, Acc : 0.866, Sensitive_Loss : 0.12313, Sensitive_Acc : 16.100, Run Time : 11.76 sec
INFO:root:2024-04-26 19:22:58, Train, Epoch : 8, Step : 4740, Loss : 0.22759, Acc : 0.909, Sensitive_Loss : 0.08556, Sensitive_Acc : 15.900, Run Time : 11.85 sec
INFO:root:2024-04-26 19:23:09, Train, Epoch : 8, Step : 4750, Loss : 0.23915, Acc : 0.878, Sensitive_Loss : 0.11975, Sensitive_Acc : 15.400, Run Time : 10.83 sec
INFO:root:2024-04-26 19:23:20, Train, Epoch : 8, Step : 4760, Loss : 0.22490, Acc : 0.909, Sensitive_Loss : 0.08314, Sensitive_Acc : 15.800, Run Time : 11.86 sec
INFO:root:2024-04-26 19:23:31, Train, Epoch : 8, Step : 4770, Loss : 0.25359, Acc : 0.900, Sensitive_Loss : 0.10301, Sensitive_Acc : 15.700, Run Time : 10.57 sec
INFO:root:2024-04-26 19:23:43, Train, Epoch : 8, Step : 4780, Loss : 0.25323, Acc : 0.887, Sensitive_Loss : 0.08727, Sensitive_Acc : 16.700, Run Time : 12.32 sec
INFO:root:2024-04-26 19:23:55, Train, Epoch : 8, Step : 4790, Loss : 0.28840, Acc : 0.863, Sensitive_Loss : 0.12354, Sensitive_Acc : 17.600, Run Time : 11.55 sec
INFO:root:2024-04-26 19:24:06, Train, Epoch : 8, Step : 4800, Loss : 0.27452, Acc : 0.872, Sensitive_Loss : 0.09882, Sensitive_Acc : 16.400, Run Time : 11.54 sec
INFO:root:2024-04-26 19:26:42, Dev, Step : 4800, Loss : 0.44565, Acc : 0.827, Auc : 0.908, Sensitive_Loss : 0.12991, Sensitive_Acc : 16.807, Sensitive_Auc : 0.989, Mean auc: 0.908, Run Time : 155.21 sec
INFO:root:2024-04-26 19:26:50, Train, Epoch : 8, Step : 4810, Loss : 0.27752, Acc : 0.881, Sensitive_Loss : 0.12225, Sensitive_Acc : 17.700, Run Time : 163.99 sec
INFO:root:2024-04-26 19:27:02, Train, Epoch : 8, Step : 4820, Loss : 0.26279, Acc : 0.894, Sensitive_Loss : 0.13033, Sensitive_Acc : 16.900, Run Time : 11.35 sec
INFO:root:2024-04-26 19:27:13, Train, Epoch : 8, Step : 4830, Loss : 0.18425, Acc : 0.931, Sensitive_Loss : 0.11114, Sensitive_Acc : 16.400, Run Time : 11.12 sec
INFO:root:2024-04-26 19:27:25, Train, Epoch : 8, Step : 4840, Loss : 0.25152, Acc : 0.903, Sensitive_Loss : 0.10196, Sensitive_Acc : 16.000, Run Time : 11.83 sec
INFO:root:2024-04-26 19:27:36, Train, Epoch : 8, Step : 4850, Loss : 0.24483, Acc : 0.859, Sensitive_Loss : 0.11328, Sensitive_Acc : 15.700, Run Time : 11.10 sec
INFO:root:2024-04-26 19:27:47, Train, Epoch : 8, Step : 4860, Loss : 0.26155, Acc : 0.875, Sensitive_Loss : 0.07471, Sensitive_Acc : 16.500, Run Time : 11.43 sec
INFO:root:2024-04-26 19:27:59, Train, Epoch : 8, Step : 4870, Loss : 0.29976, Acc : 0.891, Sensitive_Loss : 0.10580, Sensitive_Acc : 16.700, Run Time : 11.67 sec
INFO:root:2024-04-26 19:28:10, Train, Epoch : 8, Step : 4880, Loss : 0.24090, Acc : 0.906, Sensitive_Loss : 0.11476, Sensitive_Acc : 16.200, Run Time : 11.48 sec
INFO:root:2024-04-26 19:28:22, Train, Epoch : 8, Step : 4890, Loss : 0.21325, Acc : 0.919, Sensitive_Loss : 0.10355, Sensitive_Acc : 16.200, Run Time : 11.71 sec
INFO:root:2024-04-26 19:28:33, Train, Epoch : 8, Step : 4900, Loss : 0.23454, Acc : 0.909, Sensitive_Loss : 0.11948, Sensitive_Acc : 16.800, Run Time : 10.67 sec
INFO:root:2024-04-26 19:31:09, Dev, Step : 4900, Loss : 0.42162, Acc : 0.830, Auc : 0.909, Sensitive_Loss : 0.12378, Sensitive_Acc : 16.864, Sensitive_Auc : 0.992, Mean auc: 0.909, Run Time : 156.62 sec
INFO:root:2024-04-26 19:31:18, Train, Epoch : 8, Step : 4910, Loss : 0.29261, Acc : 0.887, Sensitive_Loss : 0.07571, Sensitive_Acc : 17.300, Run Time : 165.51 sec
INFO:root:2024-04-26 19:31:30, Train, Epoch : 8, Step : 4920, Loss : 0.30080, Acc : 0.875, Sensitive_Loss : 0.09384, Sensitive_Acc : 18.200, Run Time : 11.43 sec
INFO:root:2024-04-26 19:31:41, Train, Epoch : 8, Step : 4930, Loss : 0.24372, Acc : 0.906, Sensitive_Loss : 0.08541, Sensitive_Acc : 15.900, Run Time : 11.55 sec
INFO:root:2024-04-26 19:31:52, Train, Epoch : 8, Step : 4940, Loss : 0.25395, Acc : 0.903, Sensitive_Loss : 0.08008, Sensitive_Acc : 15.800, Run Time : 11.27 sec
INFO:root:2024-04-26 19:32:04, Train, Epoch : 8, Step : 4950, Loss : 0.26758, Acc : 0.897, Sensitive_Loss : 0.09379, Sensitive_Acc : 16.500, Run Time : 11.97 sec
INFO:root:2024-04-26 19:32:16, Train, Epoch : 8, Step : 4960, Loss : 0.23868, Acc : 0.900, Sensitive_Loss : 0.09616, Sensitive_Acc : 16.800, Run Time : 11.21 sec
INFO:root:2024-04-26 19:32:27, Train, Epoch : 8, Step : 4970, Loss : 0.26460, Acc : 0.859, Sensitive_Loss : 0.10802, Sensitive_Acc : 16.100, Run Time : 11.74 sec
INFO:root:2024-04-26 19:32:39, Train, Epoch : 8, Step : 4980, Loss : 0.19917, Acc : 0.912, Sensitive_Loss : 0.09135, Sensitive_Acc : 14.000, Run Time : 11.48 sec
INFO:root:2024-04-26 19:32:51, Train, Epoch : 8, Step : 4990, Loss : 0.24081, Acc : 0.900, Sensitive_Loss : 0.08454, Sensitive_Acc : 16.600, Run Time : 11.75 sec
INFO:root:2024-04-26 19:33:02, Train, Epoch : 8, Step : 5000, Loss : 0.25051, Acc : 0.887, Sensitive_Loss : 0.07555, Sensitive_Acc : 16.200, Run Time : 11.60 sec
INFO:root:2024-04-26 19:35:39, Dev, Step : 5000, Loss : 0.43571, Acc : 0.831, Auc : 0.911, Sensitive_Loss : 0.12984, Sensitive_Acc : 16.836, Sensitive_Auc : 0.989, Mean auc: 0.911, Run Time : 156.93 sec
INFO:root:2024-04-26 19:38:18
INFO:root:y_pred: [3.1839249e-01 9.9086189e-01 5.4228855e-03 ... 9.3180621e-01 1.0069460e-04
 9.4094181e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.36641514e-01 1.58933317e-03 2.81618059e-01 2.49430523e-05
 9.99817193e-01 2.70289672e-03 9.99994397e-01 9.99950528e-01
 5.78089850e-03 9.46950555e-01 9.97829497e-01 9.99965072e-01
 9.98623729e-01 9.81859386e-01 1.60236806e-02 9.91140783e-01
 9.99945521e-01 2.18294021e-02 4.85336363e-01 8.86336148e-01
 9.86980200e-01 6.82193413e-02 9.98556316e-01 9.37801778e-01
 9.99798357e-01 9.98789012e-01 2.60087010e-03 9.98651922e-01
 9.99097824e-01 7.10972846e-01 1.24933934e-02 1.87677234e-01
 1.03668999e-02 1.17516778e-01 5.71988076e-02 6.55786274e-03
 1.07553914e-01 3.41106183e-03 9.99527574e-01 9.99498010e-01
 7.42718566e-06 1.31253887e-03 9.92785692e-01 1.42777190e-02
 9.99996185e-01 9.98256266e-01 9.99881029e-01 9.94103253e-01
 1.90022178e-02 9.99866724e-01 9.95955110e-01 6.75005512e-03
 3.89436424e-01 7.77904550e-03 5.31002367e-03 6.99794712e-03
 5.47265774e-03 2.62135500e-03 2.46014772e-03 1.91627324e-01
 7.40895793e-02 2.03946698e-02 3.62240523e-02 8.90169144e-01
 1.61529571e-01 9.99994040e-01 1.61312427e-03 9.99610960e-01
 9.96996522e-01 2.85401165e-01 6.87020957e-01 5.31970322e-01
 1.00095011e-02 2.82999307e-01 4.37870109e-03 4.72367916e-04
 6.95489049e-02 1.85949989e-02 3.00341315e-04 9.99318480e-01
 9.99779165e-01 4.44306410e-04 1.96094252e-02 3.36602069e-02
 9.67304051e-01 4.02693331e-01 5.06126788e-03 2.25886535e-02
 9.74891722e-01 9.99925613e-01 9.99973655e-01 1.15191354e-03
 5.06247859e-03 9.98764634e-01 9.62303281e-02 1.75336201e-03
 9.96393621e-01 9.98738587e-01 2.18660105e-04 1.39960125e-01
 9.99394417e-01 9.98897910e-01 9.99565065e-01 9.96795952e-01
 2.40912717e-02 3.78011726e-02 9.98042941e-01 9.98246670e-01
 9.90949690e-01 2.76308012e-04 9.97495174e-01 9.98156011e-01
 1.61320508e-01 9.98804212e-01 9.97994184e-01 9.99125779e-01
 7.87484169e-01 9.98098433e-01 1.42082665e-02 1.10513970e-01
 9.99691248e-01 9.99753892e-01 8.60940025e-04 9.91591692e-01
 9.99996662e-01 2.36020997e-01 9.86013472e-01 1.65850557e-02
 4.74267229e-02 9.94774163e-01 9.99329805e-01 3.80953401e-03
 6.70257434e-02 1.05443485e-02 9.99785483e-01 9.95132744e-01
 9.86545384e-01 1.99214928e-03 1.68047901e-02 9.97976005e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 19:38:18, Dev, Step : 5008, Loss : 0.42913, Acc : 0.832, Auc : 0.911, Sensitive_Loss : 0.12921, Sensitive_Acc : 16.836, Sensitive_Auc : 0.990, Mean auc: 0.911, Run Time : 153.35 sec
INFO:root:2024-04-26 19:38:23, Train, Epoch : 9, Step : 5010, Loss : 0.03423, Acc : 0.184, Sensitive_Loss : 0.01389, Sensitive_Acc : 3.300, Run Time : 4.13 sec
INFO:root:2024-04-26 19:38:35, Train, Epoch : 9, Step : 5020, Loss : 0.19459, Acc : 0.944, Sensitive_Loss : 0.10282, Sensitive_Acc : 17.000, Run Time : 11.44 sec
INFO:root:2024-04-26 19:38:46, Train, Epoch : 9, Step : 5030, Loss : 0.22158, Acc : 0.881, Sensitive_Loss : 0.10098, Sensitive_Acc : 15.900, Run Time : 11.02 sec
INFO:root:2024-04-26 19:38:57, Train, Epoch : 9, Step : 5040, Loss : 0.26258, Acc : 0.894, Sensitive_Loss : 0.08625, Sensitive_Acc : 16.100, Run Time : 11.37 sec
INFO:root:2024-04-26 19:39:09, Train, Epoch : 9, Step : 5050, Loss : 0.21797, Acc : 0.916, Sensitive_Loss : 0.09051, Sensitive_Acc : 16.600, Run Time : 11.95 sec
INFO:root:2024-04-26 19:39:20, Train, Epoch : 9, Step : 5060, Loss : 0.21345, Acc : 0.894, Sensitive_Loss : 0.07682, Sensitive_Acc : 16.400, Run Time : 10.86 sec
INFO:root:2024-04-26 19:39:32, Train, Epoch : 9, Step : 5070, Loss : 0.21296, Acc : 0.891, Sensitive_Loss : 0.11988, Sensitive_Acc : 17.000, Run Time : 11.91 sec
INFO:root:2024-04-26 19:39:43, Train, Epoch : 9, Step : 5080, Loss : 0.21577, Acc : 0.906, Sensitive_Loss : 0.07796, Sensitive_Acc : 17.400, Run Time : 11.33 sec
INFO:root:2024-04-26 19:39:55, Train, Epoch : 9, Step : 5090, Loss : 0.21376, Acc : 0.916, Sensitive_Loss : 0.08883, Sensitive_Acc : 16.900, Run Time : 11.50 sec
INFO:root:2024-04-26 19:40:06, Train, Epoch : 9, Step : 5100, Loss : 0.19553, Acc : 0.909, Sensitive_Loss : 0.10191, Sensitive_Acc : 17.200, Run Time : 11.30 sec
INFO:root:2024-04-26 19:42:41, Dev, Step : 5100, Loss : 0.44952, Acc : 0.828, Auc : 0.910, Sensitive_Loss : 0.12744, Sensitive_Acc : 16.836, Sensitive_Auc : 0.991, Mean auc: 0.910, Run Time : 155.08 sec
INFO:root:2024-04-26 19:42:50, Train, Epoch : 9, Step : 5110, Loss : 0.18913, Acc : 0.922, Sensitive_Loss : 0.10948, Sensitive_Acc : 15.700, Run Time : 164.23 sec
INFO:root:2024-04-26 19:43:01, Train, Epoch : 9, Step : 5120, Loss : 0.29039, Acc : 0.884, Sensitive_Loss : 0.10461, Sensitive_Acc : 14.600, Run Time : 11.21 sec
INFO:root:2024-04-26 19:43:14, Train, Epoch : 9, Step : 5130, Loss : 0.17107, Acc : 0.928, Sensitive_Loss : 0.13507, Sensitive_Acc : 17.700, Run Time : 12.25 sec
INFO:root:2024-04-26 19:43:25, Train, Epoch : 9, Step : 5140, Loss : 0.22925, Acc : 0.900, Sensitive_Loss : 0.07507, Sensitive_Acc : 16.800, Run Time : 11.20 sec
INFO:root:2024-04-26 19:43:35, Train, Epoch : 9, Step : 5150, Loss : 0.20578, Acc : 0.894, Sensitive_Loss : 0.05410, Sensitive_Acc : 15.600, Run Time : 10.58 sec
INFO:root:2024-04-26 19:43:48, Train, Epoch : 9, Step : 5160, Loss : 0.23951, Acc : 0.884, Sensitive_Loss : 0.13875, Sensitive_Acc : 16.700, Run Time : 12.11 sec
INFO:root:2024-04-26 19:43:58, Train, Epoch : 9, Step : 5170, Loss : 0.23926, Acc : 0.900, Sensitive_Loss : 0.07101, Sensitive_Acc : 16.600, Run Time : 10.78 sec
INFO:root:2024-04-26 19:44:09, Train, Epoch : 9, Step : 5180, Loss : 0.26424, Acc : 0.878, Sensitive_Loss : 0.09034, Sensitive_Acc : 16.900, Run Time : 11.14 sec
INFO:root:2024-04-26 19:44:21, Train, Epoch : 9, Step : 5190, Loss : 0.27395, Acc : 0.872, Sensitive_Loss : 0.10651, Sensitive_Acc : 18.100, Run Time : 11.94 sec
INFO:root:2024-04-26 19:44:32, Train, Epoch : 9, Step : 5200, Loss : 0.20520, Acc : 0.928, Sensitive_Loss : 0.08908, Sensitive_Acc : 16.000, Run Time : 10.95 sec
INFO:root:2024-04-26 19:47:07, Dev, Step : 5200, Loss : 0.44700, Acc : 0.826, Auc : 0.908, Sensitive_Loss : 0.13248, Sensitive_Acc : 16.864, Sensitive_Auc : 0.992, Mean auc: 0.908, Run Time : 154.59 sec
INFO:root:2024-04-26 19:47:16, Train, Epoch : 9, Step : 5210, Loss : 0.29950, Acc : 0.881, Sensitive_Loss : 0.10894, Sensitive_Acc : 16.200, Run Time : 163.57 sec
INFO:root:2024-04-26 19:47:27, Train, Epoch : 9, Step : 5220, Loss : 0.28647, Acc : 0.891, Sensitive_Loss : 0.09700, Sensitive_Acc : 15.300, Run Time : 11.21 sec
INFO:root:2024-04-26 19:47:38, Train, Epoch : 9, Step : 5230, Loss : 0.19337, Acc : 0.934, Sensitive_Loss : 0.12345, Sensitive_Acc : 15.900, Run Time : 11.24 sec
INFO:root:2024-04-26 19:47:50, Train, Epoch : 9, Step : 5240, Loss : 0.25375, Acc : 0.897, Sensitive_Loss : 0.08162, Sensitive_Acc : 15.500, Run Time : 11.73 sec
INFO:root:2024-04-26 19:48:02, Train, Epoch : 9, Step : 5250, Loss : 0.24207, Acc : 0.900, Sensitive_Loss : 0.11590, Sensitive_Acc : 16.900, Run Time : 11.40 sec
INFO:root:2024-04-26 19:48:12, Train, Epoch : 9, Step : 5260, Loss : 0.21032, Acc : 0.897, Sensitive_Loss : 0.07788, Sensitive_Acc : 16.100, Run Time : 10.90 sec
INFO:root:2024-04-26 19:48:24, Train, Epoch : 9, Step : 5270, Loss : 0.23202, Acc : 0.912, Sensitive_Loss : 0.08208, Sensitive_Acc : 16.700, Run Time : 11.34 sec
INFO:root:2024-04-26 19:48:35, Train, Epoch : 9, Step : 5280, Loss : 0.22248, Acc : 0.912, Sensitive_Loss : 0.08914, Sensitive_Acc : 16.900, Run Time : 11.69 sec
INFO:root:2024-04-26 19:48:47, Train, Epoch : 9, Step : 5290, Loss : 0.25311, Acc : 0.897, Sensitive_Loss : 0.08767, Sensitive_Acc : 17.400, Run Time : 11.97 sec
INFO:root:2024-04-26 19:48:58, Train, Epoch : 9, Step : 5300, Loss : 0.22294, Acc : 0.922, Sensitive_Loss : 0.05656, Sensitive_Acc : 18.000, Run Time : 10.54 sec
INFO:root:2024-04-26 19:51:34, Dev, Step : 5300, Loss : 0.46812, Acc : 0.823, Auc : 0.909, Sensitive_Loss : 0.14083, Sensitive_Acc : 16.879, Sensitive_Auc : 0.991, Mean auc: 0.909, Run Time : 155.56 sec
INFO:root:2024-04-26 19:51:42, Train, Epoch : 9, Step : 5310, Loss : 0.23173, Acc : 0.916, Sensitive_Loss : 0.06926, Sensitive_Acc : 15.600, Run Time : 164.13 sec
INFO:root:2024-04-26 19:51:54, Train, Epoch : 9, Step : 5320, Loss : 0.23267, Acc : 0.916, Sensitive_Loss : 0.10886, Sensitive_Acc : 16.100, Run Time : 11.50 sec
INFO:root:2024-04-26 19:52:06, Train, Epoch : 9, Step : 5330, Loss : 0.20793, Acc : 0.891, Sensitive_Loss : 0.12273, Sensitive_Acc : 15.300, Run Time : 12.34 sec
INFO:root:2024-04-26 19:52:17, Train, Epoch : 9, Step : 5340, Loss : 0.25637, Acc : 0.903, Sensitive_Loss : 0.08770, Sensitive_Acc : 17.000, Run Time : 11.30 sec
INFO:root:2024-04-26 19:52:28, Train, Epoch : 9, Step : 5350, Loss : 0.24557, Acc : 0.916, Sensitive_Loss : 0.11064, Sensitive_Acc : 16.300, Run Time : 10.94 sec
INFO:root:2024-04-26 19:52:40, Train, Epoch : 9, Step : 5360, Loss : 0.21657, Acc : 0.919, Sensitive_Loss : 0.08017, Sensitive_Acc : 16.500, Run Time : 11.76 sec
INFO:root:2024-04-26 19:52:51, Train, Epoch : 9, Step : 5370, Loss : 0.28165, Acc : 0.894, Sensitive_Loss : 0.10467, Sensitive_Acc : 18.100, Run Time : 10.87 sec
INFO:root:2024-04-26 19:53:02, Train, Epoch : 9, Step : 5380, Loss : 0.23307, Acc : 0.894, Sensitive_Loss : 0.10008, Sensitive_Acc : 15.400, Run Time : 10.94 sec
INFO:root:2024-04-26 19:53:14, Train, Epoch : 9, Step : 5390, Loss : 0.18297, Acc : 0.925, Sensitive_Loss : 0.07661, Sensitive_Acc : 18.200, Run Time : 11.95 sec
INFO:root:2024-04-26 19:53:25, Train, Epoch : 9, Step : 5400, Loss : 0.26338, Acc : 0.903, Sensitive_Loss : 0.10482, Sensitive_Acc : 16.000, Run Time : 11.08 sec
INFO:root:2024-04-26 19:56:00, Dev, Step : 5400, Loss : 0.50120, Acc : 0.809, Auc : 0.903, Sensitive_Loss : 0.16278, Sensitive_Acc : 16.879, Sensitive_Auc : 0.992, Mean auc: 0.903, Run Time : 155.70 sec
INFO:root:2024-04-26 19:56:09, Train, Epoch : 9, Step : 5410, Loss : 0.26148, Acc : 0.897, Sensitive_Loss : 0.10830, Sensitive_Acc : 16.000, Run Time : 164.56 sec
INFO:root:2024-04-26 19:56:20, Train, Epoch : 9, Step : 5420, Loss : 0.20340, Acc : 0.906, Sensitive_Loss : 0.11545, Sensitive_Acc : 17.100, Run Time : 11.08 sec
INFO:root:2024-04-26 19:56:32, Train, Epoch : 9, Step : 5430, Loss : 0.22674, Acc : 0.903, Sensitive_Loss : 0.15281, Sensitive_Acc : 16.600, Run Time : 11.59 sec
INFO:root:2024-04-26 19:56:43, Train, Epoch : 9, Step : 5440, Loss : 0.20379, Acc : 0.922, Sensitive_Loss : 0.09319, Sensitive_Acc : 16.900, Run Time : 11.05 sec
INFO:root:2024-04-26 19:56:55, Train, Epoch : 9, Step : 5450, Loss : 0.25493, Acc : 0.884, Sensitive_Loss : 0.10632, Sensitive_Acc : 16.300, Run Time : 11.99 sec
INFO:root:2024-04-26 19:57:07, Train, Epoch : 9, Step : 5460, Loss : 0.20586, Acc : 0.919, Sensitive_Loss : 0.08748, Sensitive_Acc : 15.900, Run Time : 12.39 sec
INFO:root:2024-04-26 19:57:19, Train, Epoch : 9, Step : 5470, Loss : 0.21330, Acc : 0.919, Sensitive_Loss : 0.08600, Sensitive_Acc : 16.500, Run Time : 11.76 sec
INFO:root:2024-04-26 19:57:30, Train, Epoch : 9, Step : 5480, Loss : 0.33055, Acc : 0.853, Sensitive_Loss : 0.12330, Sensitive_Acc : 16.700, Run Time : 10.33 sec
INFO:root:2024-04-26 19:57:41, Train, Epoch : 9, Step : 5490, Loss : 0.19396, Acc : 0.934, Sensitive_Loss : 0.09067, Sensitive_Acc : 14.700, Run Time : 11.92 sec
INFO:root:2024-04-26 19:57:53, Train, Epoch : 9, Step : 5500, Loss : 0.27054, Acc : 0.900, Sensitive_Loss : 0.11945, Sensitive_Acc : 17.200, Run Time : 11.75 sec
INFO:root:2024-04-26 20:00:28, Dev, Step : 5500, Loss : 0.43653, Acc : 0.832, Auc : 0.909, Sensitive_Loss : 0.12782, Sensitive_Acc : 16.879, Sensitive_Auc : 0.991, Mean auc: 0.909, Run Time : 155.26 sec
INFO:root:2024-04-26 20:00:37, Train, Epoch : 9, Step : 5510, Loss : 0.27726, Acc : 0.891, Sensitive_Loss : 0.09627, Sensitive_Acc : 16.600, Run Time : 164.05 sec
INFO:root:2024-04-26 20:00:49, Train, Epoch : 9, Step : 5520, Loss : 0.30088, Acc : 0.869, Sensitive_Loss : 0.10287, Sensitive_Acc : 15.100, Run Time : 11.38 sec
INFO:root:2024-04-26 20:01:00, Train, Epoch : 9, Step : 5530, Loss : 0.18787, Acc : 0.912, Sensitive_Loss : 0.08401, Sensitive_Acc : 17.700, Run Time : 11.31 sec
INFO:root:2024-04-26 20:01:12, Train, Epoch : 9, Step : 5540, Loss : 0.18754, Acc : 0.919, Sensitive_Loss : 0.09873, Sensitive_Acc : 15.300, Run Time : 11.75 sec
INFO:root:2024-04-26 20:01:23, Train, Epoch : 9, Step : 5550, Loss : 0.21930, Acc : 0.900, Sensitive_Loss : 0.07926, Sensitive_Acc : 15.600, Run Time : 11.50 sec
INFO:root:2024-04-26 20:01:34, Train, Epoch : 9, Step : 5560, Loss : 0.25883, Acc : 0.900, Sensitive_Loss : 0.10348, Sensitive_Acc : 14.600, Run Time : 11.00 sec
INFO:root:2024-04-26 20:01:46, Train, Epoch : 9, Step : 5570, Loss : 0.23266, Acc : 0.891, Sensitive_Loss : 0.10889, Sensitive_Acc : 16.700, Run Time : 11.75 sec
INFO:root:2024-04-26 20:01:58, Train, Epoch : 9, Step : 5580, Loss : 0.27512, Acc : 0.897, Sensitive_Loss : 0.07944, Sensitive_Acc : 15.700, Run Time : 11.99 sec
INFO:root:2024-04-26 20:02:09, Train, Epoch : 9, Step : 5590, Loss : 0.27652, Acc : 0.866, Sensitive_Loss : 0.09749, Sensitive_Acc : 16.600, Run Time : 11.48 sec
INFO:root:2024-04-26 20:02:21, Train, Epoch : 9, Step : 5600, Loss : 0.29467, Acc : 0.875, Sensitive_Loss : 0.10333, Sensitive_Acc : 17.400, Run Time : 11.78 sec
INFO:root:2024-04-26 20:04:57, Dev, Step : 5600, Loss : 0.43520, Acc : 0.829, Auc : 0.907, Sensitive_Loss : 0.12438, Sensitive_Acc : 16.821, Sensitive_Auc : 0.991, Mean auc: 0.907, Run Time : 155.55 sec
INFO:root:2024-04-26 20:05:05, Train, Epoch : 9, Step : 5610, Loss : 0.23723, Acc : 0.900, Sensitive_Loss : 0.12210, Sensitive_Acc : 16.800, Run Time : 163.97 sec
INFO:root:2024-04-26 20:05:17, Train, Epoch : 9, Step : 5620, Loss : 0.23691, Acc : 0.903, Sensitive_Loss : 0.09443, Sensitive_Acc : 16.800, Run Time : 11.58 sec
INFO:root:2024-04-26 20:05:28, Train, Epoch : 9, Step : 5630, Loss : 0.24070, Acc : 0.884, Sensitive_Loss : 0.10362, Sensitive_Acc : 16.300, Run Time : 11.43 sec
INFO:root:2024-04-26 20:08:07
INFO:root:y_pred: [4.7145721e-01 9.9269682e-01 2.3407133e-02 ... 9.0747648e-01 1.5661441e-04
 9.7451144e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.31951761e-01 7.50126957e-04 1.51576221e-01 8.01395799e-06
 9.99536157e-01 6.87188061e-04 9.99991059e-01 9.99944091e-01
 1.65362179e-03 9.64646578e-01 9.98176575e-01 9.99919176e-01
 9.97060835e-01 9.68676865e-01 9.55963414e-03 9.83822644e-01
 9.99925137e-01 2.33701840e-02 3.95796269e-01 8.10426354e-01
 9.81318176e-01 2.41974015e-02 9.98276949e-01 8.45009506e-01
 9.99588072e-01 9.96736467e-01 1.41836610e-03 9.97009754e-01
 9.98995006e-01 5.22029757e-01 7.36139202e-03 2.29262531e-01
 2.93424306e-03 5.24144657e-02 2.21485253e-02 4.65249037e-03
 5.43113239e-02 1.29213545e-03 9.99263823e-01 9.99273121e-01
 3.35251048e-06 1.95675995e-04 9.85117435e-01 6.01674803e-03
 9.99989390e-01 9.97298896e-01 9.99814093e-01 9.90633726e-01
 2.32543722e-02 9.98803139e-01 9.95658398e-01 1.32695842e-03
 2.72661924e-01 1.69803097e-03 2.79425178e-03 5.17669413e-03
 3.94182326e-03 9.71811765e-04 2.49795942e-03 9.44663063e-02
 6.24004416e-02 8.33889656e-03 6.86430791e-03 7.40199387e-01
 9.61494520e-02 9.99992251e-01 8.15596781e-04 9.99769151e-01
 9.96510208e-01 2.54603505e-01 3.48737627e-01 4.15679604e-01
 4.48592892e-03 1.22465365e-01 1.60910527e-03 1.37228169e-04
 3.79374586e-02 6.50018873e-03 1.16973883e-04 9.98433769e-01
 9.99715626e-01 3.44036525e-04 2.35542059e-02 2.49205139e-02
 9.24489498e-01 4.84519035e-01 3.78634431e-03 2.38880459e-02
 9.71722007e-01 9.99876738e-01 9.99963284e-01 7.29034538e-04
 2.04535807e-03 9.97877836e-01 6.15613759e-02 9.70675319e-04
 9.96464968e-01 9.98427153e-01 1.03929713e-04 6.80357814e-02
 9.99160528e-01 9.98554170e-01 9.99775589e-01 9.98056293e-01
 5.66563569e-03 3.74063887e-02 9.93377626e-01 9.96462047e-01
 9.78857696e-01 6.84626793e-05 9.94814277e-01 9.97590899e-01
 3.57391238e-02 9.98918891e-01 9.96083975e-01 9.98622775e-01
 6.38310313e-01 9.98049259e-01 1.30580887e-02 5.24225980e-02
 9.99800503e-01 9.99423862e-01 3.55814263e-04 9.89813864e-01
 9.99992132e-01 1.27212733e-01 9.82478082e-01 8.33878201e-03
 7.63997901e-03 9.91653621e-01 9.98243332e-01 1.19466905e-03
 2.66354959e-02 3.79175832e-03 9.99323368e-01 9.91106093e-01
 9.77867246e-01 6.91634836e-04 5.70793916e-03 9.98343468e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 20:08:07, Dev, Step : 5634, Loss : 0.43446, Acc : 0.832, Auc : 0.907, Sensitive_Loss : 0.11083, Sensitive_Acc : 16.921, Sensitive_Auc : 0.991, Mean auc: 0.907, Run Time : 154.30 sec
INFO:root:2024-04-26 20:08:16, Train, Epoch : 10, Step : 5640, Loss : 0.13236, Acc : 0.553, Sensitive_Loss : 0.04963, Sensitive_Acc : 9.500, Run Time : 8.18 sec
INFO:root:2024-04-26 20:08:29, Train, Epoch : 10, Step : 5650, Loss : 0.22756, Acc : 0.906, Sensitive_Loss : 0.10075, Sensitive_Acc : 16.100, Run Time : 12.30 sec
INFO:root:2024-04-26 20:08:40, Train, Epoch : 10, Step : 5660, Loss : 0.24508, Acc : 0.887, Sensitive_Loss : 0.15863, Sensitive_Acc : 16.700, Run Time : 11.37 sec
INFO:root:2024-04-26 20:08:52, Train, Epoch : 10, Step : 5670, Loss : 0.19015, Acc : 0.900, Sensitive_Loss : 0.07833, Sensitive_Acc : 15.200, Run Time : 11.51 sec
INFO:root:2024-04-26 20:09:03, Train, Epoch : 10, Step : 5680, Loss : 0.19137, Acc : 0.906, Sensitive_Loss : 0.13160, Sensitive_Acc : 17.800, Run Time : 11.41 sec
INFO:root:2024-04-26 20:09:15, Train, Epoch : 10, Step : 5690, Loss : 0.22814, Acc : 0.909, Sensitive_Loss : 0.09789, Sensitive_Acc : 15.900, Run Time : 11.57 sec
INFO:root:2024-04-26 20:09:26, Train, Epoch : 10, Step : 5700, Loss : 0.18417, Acc : 0.934, Sensitive_Loss : 0.13137, Sensitive_Acc : 18.600, Run Time : 11.58 sec
INFO:root:2024-04-26 20:12:03, Dev, Step : 5700, Loss : 0.48377, Acc : 0.814, Auc : 0.902, Sensitive_Loss : 0.13003, Sensitive_Acc : 16.864, Sensitive_Auc : 0.991, Mean auc: 0.902, Run Time : 157.25 sec
INFO:root:2024-04-26 20:12:12, Train, Epoch : 10, Step : 5710, Loss : 0.18428, Acc : 0.925, Sensitive_Loss : 0.08582, Sensitive_Acc : 16.200, Run Time : 166.13 sec
INFO:root:2024-04-26 20:12:23, Train, Epoch : 10, Step : 5720, Loss : 0.22427, Acc : 0.916, Sensitive_Loss : 0.09554, Sensitive_Acc : 16.700, Run Time : 11.20 sec
INFO:root:2024-04-26 20:12:35, Train, Epoch : 10, Step : 5730, Loss : 0.24581, Acc : 0.884, Sensitive_Loss : 0.06410, Sensitive_Acc : 16.300, Run Time : 11.78 sec
INFO:root:2024-04-26 20:12:47, Train, Epoch : 10, Step : 5740, Loss : 0.21632, Acc : 0.928, Sensitive_Loss : 0.07292, Sensitive_Acc : 16.400, Run Time : 12.01 sec
INFO:root:2024-04-26 20:12:59, Train, Epoch : 10, Step : 5750, Loss : 0.21660, Acc : 0.912, Sensitive_Loss : 0.09364, Sensitive_Acc : 16.700, Run Time : 11.76 sec
INFO:root:2024-04-26 20:13:11, Train, Epoch : 10, Step : 5760, Loss : 0.17542, Acc : 0.925, Sensitive_Loss : 0.08269, Sensitive_Acc : 17.000, Run Time : 12.09 sec
INFO:root:2024-04-26 20:13:22, Train, Epoch : 10, Step : 5770, Loss : 0.20252, Acc : 0.922, Sensitive_Loss : 0.08287, Sensitive_Acc : 15.800, Run Time : 10.42 sec
INFO:root:2024-04-26 20:13:33, Train, Epoch : 10, Step : 5780, Loss : 0.20982, Acc : 0.912, Sensitive_Loss : 0.10532, Sensitive_Acc : 15.200, Run Time : 11.97 sec
INFO:root:2024-04-26 20:13:46, Train, Epoch : 10, Step : 5790, Loss : 0.20254, Acc : 0.919, Sensitive_Loss : 0.10097, Sensitive_Acc : 18.000, Run Time : 12.32 sec
INFO:root:2024-04-26 20:13:57, Train, Epoch : 10, Step : 5800, Loss : 0.22901, Acc : 0.912, Sensitive_Loss : 0.11745, Sensitive_Acc : 16.100, Run Time : 10.95 sec
INFO:root:2024-04-26 20:16:33, Dev, Step : 5800, Loss : 0.47372, Acc : 0.820, Auc : 0.904, Sensitive_Loss : 0.15153, Sensitive_Acc : 16.850, Sensitive_Auc : 0.991, Mean auc: 0.904, Run Time : 155.93 sec
INFO:root:2024-04-26 20:16:41, Train, Epoch : 10, Step : 5810, Loss : 0.21198, Acc : 0.891, Sensitive_Loss : 0.08644, Sensitive_Acc : 16.800, Run Time : 164.39 sec
INFO:root:2024-04-26 20:16:52, Train, Epoch : 10, Step : 5820, Loss : 0.18256, Acc : 0.931, Sensitive_Loss : 0.13078, Sensitive_Acc : 16.200, Run Time : 10.75 sec
INFO:root:2024-04-26 20:17:04, Train, Epoch : 10, Step : 5830, Loss : 0.21435, Acc : 0.903, Sensitive_Loss : 0.11632, Sensitive_Acc : 16.900, Run Time : 12.29 sec
INFO:root:2024-04-26 20:17:15, Train, Epoch : 10, Step : 5840, Loss : 0.18502, Acc : 0.934, Sensitive_Loss : 0.09494, Sensitive_Acc : 16.400, Run Time : 10.98 sec
INFO:root:2024-04-26 20:17:27, Train, Epoch : 10, Step : 5850, Loss : 0.19127, Acc : 0.916, Sensitive_Loss : 0.06427, Sensitive_Acc : 17.400, Run Time : 12.04 sec
INFO:root:2024-04-26 20:17:39, Train, Epoch : 10, Step : 5860, Loss : 0.18385, Acc : 0.922, Sensitive_Loss : 0.10200, Sensitive_Acc : 16.600, Run Time : 11.72 sec
INFO:root:2024-04-26 20:17:51, Train, Epoch : 10, Step : 5870, Loss : 0.24897, Acc : 0.891, Sensitive_Loss : 0.09205, Sensitive_Acc : 15.100, Run Time : 12.31 sec
INFO:root:2024-04-26 20:18:02, Train, Epoch : 10, Step : 5880, Loss : 0.20945, Acc : 0.919, Sensitive_Loss : 0.10208, Sensitive_Acc : 16.700, Run Time : 11.09 sec
INFO:root:2024-04-26 20:18:14, Train, Epoch : 10, Step : 5890, Loss : 0.21219, Acc : 0.919, Sensitive_Loss : 0.10106, Sensitive_Acc : 16.100, Run Time : 11.80 sec
INFO:root:2024-04-26 20:18:26, Train, Epoch : 10, Step : 5900, Loss : 0.21158, Acc : 0.900, Sensitive_Loss : 0.08759, Sensitive_Acc : 15.900, Run Time : 11.52 sec
INFO:root:2024-04-26 20:21:00, Dev, Step : 5900, Loss : 0.47356, Acc : 0.821, Auc : 0.906, Sensitive_Loss : 0.12629, Sensitive_Acc : 16.821, Sensitive_Auc : 0.991, Mean auc: 0.906, Run Time : 154.60 sec
INFO:root:2024-04-26 20:21:09, Train, Epoch : 10, Step : 5910, Loss : 0.21341, Acc : 0.875, Sensitive_Loss : 0.10993, Sensitive_Acc : 16.300, Run Time : 163.16 sec
INFO:root:2024-04-26 20:21:19, Train, Epoch : 10, Step : 5920, Loss : 0.17221, Acc : 0.912, Sensitive_Loss : 0.08752, Sensitive_Acc : 17.800, Run Time : 10.55 sec
INFO:root:2024-04-26 20:21:31, Train, Epoch : 10, Step : 5930, Loss : 0.22493, Acc : 0.909, Sensitive_Loss : 0.10187, Sensitive_Acc : 17.300, Run Time : 11.80 sec
INFO:root:2024-04-26 20:21:42, Train, Epoch : 10, Step : 5940, Loss : 0.18102, Acc : 0.922, Sensitive_Loss : 0.13333, Sensitive_Acc : 16.500, Run Time : 10.75 sec
INFO:root:2024-04-26 20:21:54, Train, Epoch : 10, Step : 5950, Loss : 0.17612, Acc : 0.928, Sensitive_Loss : 0.06401, Sensitive_Acc : 16.900, Run Time : 12.33 sec
INFO:root:2024-04-26 20:22:06, Train, Epoch : 10, Step : 5960, Loss : 0.21144, Acc : 0.912, Sensitive_Loss : 0.10835, Sensitive_Acc : 16.900, Run Time : 11.44 sec
INFO:root:2024-04-26 20:22:17, Train, Epoch : 10, Step : 5970, Loss : 0.27504, Acc : 0.884, Sensitive_Loss : 0.08862, Sensitive_Acc : 15.000, Run Time : 11.40 sec
INFO:root:2024-04-26 20:22:29, Train, Epoch : 10, Step : 5980, Loss : 0.23223, Acc : 0.906, Sensitive_Loss : 0.15317, Sensitive_Acc : 16.900, Run Time : 11.62 sec
INFO:root:2024-04-26 20:22:40, Train, Epoch : 10, Step : 5990, Loss : 0.20788, Acc : 0.884, Sensitive_Loss : 0.10742, Sensitive_Acc : 18.600, Run Time : 11.61 sec
INFO:root:2024-04-26 20:22:52, Train, Epoch : 10, Step : 6000, Loss : 0.25012, Acc : 0.884, Sensitive_Loss : 0.12200, Sensitive_Acc : 15.500, Run Time : 11.65 sec
INFO:root:2024-04-26 20:25:27, Dev, Step : 6000, Loss : 0.47483, Acc : 0.821, Auc : 0.906, Sensitive_Loss : 0.14988, Sensitive_Acc : 16.779, Sensitive_Auc : 0.991, Mean auc: 0.906, Run Time : 155.44 sec
INFO:root:2024-04-26 20:25:37, Train, Epoch : 10, Step : 6010, Loss : 0.19898, Acc : 0.925, Sensitive_Loss : 0.05582, Sensitive_Acc : 17.100, Run Time : 164.59 sec
INFO:root:2024-04-26 20:25:48, Train, Epoch : 10, Step : 6020, Loss : 0.23437, Acc : 0.906, Sensitive_Loss : 0.05923, Sensitive_Acc : 17.300, Run Time : 10.96 sec
INFO:root:2024-04-26 20:25:59, Train, Epoch : 10, Step : 6030, Loss : 0.16138, Acc : 0.938, Sensitive_Loss : 0.09880, Sensitive_Acc : 15.500, Run Time : 11.39 sec
INFO:root:2024-04-26 20:26:11, Train, Epoch : 10, Step : 6040, Loss : 0.20703, Acc : 0.891, Sensitive_Loss : 0.08019, Sensitive_Acc : 17.100, Run Time : 11.67 sec
INFO:root:2024-04-26 20:26:33, Train, Epoch : 10, Step : 6050, Loss : 0.23119, Acc : 0.887, Sensitive_Loss : 0.12633, Sensitive_Acc : 17.800, Run Time : 22.81 sec
INFO:root:2024-04-26 20:26:51, Train, Epoch : 10, Step : 6060, Loss : 0.24677, Acc : 0.887, Sensitive_Loss : 0.08676, Sensitive_Acc : 15.300, Run Time : 18.10 sec
INFO:root:2024-04-26 20:27:13, Train, Epoch : 10, Step : 6070, Loss : 0.21321, Acc : 0.906, Sensitive_Loss : 0.09439, Sensitive_Acc : 15.900, Run Time : 21.20 sec
INFO:root:2024-04-26 20:27:36, Train, Epoch : 10, Step : 6080, Loss : 0.17463, Acc : 0.922, Sensitive_Loss : 0.09590, Sensitive_Acc : 16.500, Run Time : 22.87 sec
INFO:root:2024-04-26 20:27:47, Train, Epoch : 10, Step : 6090, Loss : 0.22298, Acc : 0.884, Sensitive_Loss : 0.08038, Sensitive_Acc : 17.300, Run Time : 11.77 sec
INFO:root:2024-04-26 20:27:59, Train, Epoch : 10, Step : 6100, Loss : 0.18597, Acc : 0.909, Sensitive_Loss : 0.15943, Sensitive_Acc : 15.200, Run Time : 11.75 sec
INFO:root:2024-04-26 20:30:42, Dev, Step : 6100, Loss : 0.47120, Acc : 0.817, Auc : 0.903, Sensitive_Loss : 0.14526, Sensitive_Acc : 16.750, Sensitive_Auc : 0.990, Mean auc: 0.903, Run Time : 163.11 sec
INFO:root:2024-04-26 20:30:51, Train, Epoch : 10, Step : 6110, Loss : 0.27460, Acc : 0.894, Sensitive_Loss : 0.07778, Sensitive_Acc : 17.200, Run Time : 171.82 sec
INFO:root:2024-04-26 20:31:03, Train, Epoch : 10, Step : 6120, Loss : 0.19834, Acc : 0.909, Sensitive_Loss : 0.09438, Sensitive_Acc : 15.300, Run Time : 11.80 sec
INFO:root:2024-04-26 20:31:15, Train, Epoch : 10, Step : 6130, Loss : 0.18138, Acc : 0.944, Sensitive_Loss : 0.08074, Sensitive_Acc : 18.000, Run Time : 12.31 sec
INFO:root:2024-04-26 20:31:27, Train, Epoch : 10, Step : 6140, Loss : 0.22955, Acc : 0.916, Sensitive_Loss : 0.06821, Sensitive_Acc : 17.000, Run Time : 11.76 sec
INFO:root:2024-04-26 20:31:39, Train, Epoch : 10, Step : 6150, Loss : 0.26338, Acc : 0.866, Sensitive_Loss : 0.09722, Sensitive_Acc : 16.000, Run Time : 12.03 sec
INFO:root:2024-04-26 20:31:50, Train, Epoch : 10, Step : 6160, Loss : 0.19414, Acc : 0.922, Sensitive_Loss : 0.13816, Sensitive_Acc : 15.400, Run Time : 11.40 sec
INFO:root:2024-04-26 20:32:02, Train, Epoch : 10, Step : 6170, Loss : 0.25701, Acc : 0.891, Sensitive_Loss : 0.12218, Sensitive_Acc : 15.400, Run Time : 11.47 sec
INFO:root:2024-04-26 20:32:14, Train, Epoch : 10, Step : 6180, Loss : 0.17703, Acc : 0.919, Sensitive_Loss : 0.08146, Sensitive_Acc : 15.600, Run Time : 12.30 sec
INFO:root:2024-04-26 20:32:27, Train, Epoch : 10, Step : 6190, Loss : 0.26193, Acc : 0.884, Sensitive_Loss : 0.09469, Sensitive_Acc : 16.100, Run Time : 12.62 sec
INFO:root:2024-04-26 20:32:38, Train, Epoch : 10, Step : 6200, Loss : 0.19877, Acc : 0.922, Sensitive_Loss : 0.06884, Sensitive_Acc : 16.400, Run Time : 11.64 sec
INFO:root:2024-04-26 20:35:14, Dev, Step : 6200, Loss : 0.48797, Acc : 0.818, Auc : 0.906, Sensitive_Loss : 0.12226, Sensitive_Acc : 16.836, Sensitive_Auc : 0.991, Mean auc: 0.906, Run Time : 156.15 sec
INFO:root:2024-04-26 20:35:23, Train, Epoch : 10, Step : 6210, Loss : 0.25514, Acc : 0.891, Sensitive_Loss : 0.07681, Sensitive_Acc : 16.300, Run Time : 165.00 sec
INFO:root:2024-04-26 20:35:35, Train, Epoch : 10, Step : 6220, Loss : 0.22756, Acc : 0.897, Sensitive_Loss : 0.07410, Sensitive_Acc : 16.700, Run Time : 11.46 sec
INFO:root:2024-04-26 20:35:47, Train, Epoch : 10, Step : 6230, Loss : 0.18750, Acc : 0.925, Sensitive_Loss : 0.15201, Sensitive_Acc : 18.000, Run Time : 12.08 sec
INFO:root:2024-04-26 20:35:59, Train, Epoch : 10, Step : 6240, Loss : 0.16865, Acc : 0.928, Sensitive_Loss : 0.08016, Sensitive_Acc : 15.600, Run Time : 12.03 sec
INFO:root:2024-04-26 20:36:10, Train, Epoch : 10, Step : 6250, Loss : 0.20476, Acc : 0.928, Sensitive_Loss : 0.07478, Sensitive_Acc : 16.500, Run Time : 10.99 sec
INFO:root:2024-04-26 20:36:20, Train, Epoch : 10, Step : 6260, Loss : 0.20435, Acc : 0.903, Sensitive_Loss : 0.12318, Sensitive_Acc : 15.700, Run Time : 10.35 sec
INFO:root:2024-04-26 20:38:54
INFO:root:y_pred: [3.9261660e-01 9.9342573e-01 4.8255385e-03 ... 8.5250193e-01 2.9348095e-05
 9.7846174e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.55349505e-01 2.15813844e-03 1.52584538e-01 1.70842250e-05
 9.99172747e-01 1.16134365e-03 9.99987245e-01 9.99955773e-01
 1.42622471e-03 9.77450132e-01 9.99000728e-01 9.99959826e-01
 9.98384833e-01 9.85760152e-01 1.32497139e-02 9.84426022e-01
 9.99922991e-01 1.11632561e-02 3.42837363e-01 9.04499292e-01
 9.82050061e-01 2.50775199e-02 9.97338355e-01 9.27677393e-01
 9.99602973e-01 9.97540951e-01 1.77065539e-03 9.97422576e-01
 9.99525547e-01 4.87080514e-01 1.78541392e-02 1.43959239e-01
 1.61499530e-03 7.97197893e-02 3.67622301e-02 3.15877120e-03
 8.99665058e-02 1.38869917e-03 9.99223113e-01 9.99389291e-01
 7.01293402e-06 4.66983591e-04 9.82799649e-01 1.13958949e-02
 9.99989152e-01 9.98581886e-01 9.99847651e-01 9.98404205e-01
 4.60691564e-02 9.99184430e-01 9.97088253e-01 2.60951207e-03
 1.66800246e-01 5.79358730e-03 4.05485230e-03 3.25415819e-03
 4.43088682e-03 2.75975501e-04 2.89564487e-03 1.51636302e-01
 9.95393395e-02 3.32396128e-03 1.49735138e-02 7.84834087e-01
 1.16233565e-01 9.99990225e-01 1.44438783e-03 9.99662280e-01
 9.97048318e-01 3.02470356e-01 5.30564487e-01 4.22951609e-01
 3.90773080e-03 1.48899689e-01 2.59689242e-03 2.54518935e-04
 2.90392581e-02 1.25157451e-02 3.86393513e-04 9.99361217e-01
 9.99717772e-01 5.37324522e-04 2.94843465e-02 4.74227592e-02
 8.42156768e-01 6.40037596e-01 2.65015033e-03 3.63303535e-02
 9.86623883e-01 9.99864936e-01 9.99944210e-01 8.49993201e-04
 2.54624919e-03 9.98619676e-01 7.36700669e-02 1.04295963e-03
 9.95128155e-01 9.98737514e-01 1.23417514e-04 7.99374431e-02
 9.99419451e-01 9.98819053e-01 9.99708235e-01 9.98325646e-01
 1.14647700e-02 5.43187745e-02 9.93278682e-01 9.97186124e-01
 9.89359140e-01 1.71974127e-04 9.96420026e-01 9.95284021e-01
 3.81542146e-02 9.98619795e-01 9.95153069e-01 9.99500155e-01
 7.62450457e-01 9.98166800e-01 1.09491311e-02 5.54475188e-02
 9.99794662e-01 9.99592483e-01 6.99902535e-04 9.89991546e-01
 9.99988556e-01 2.47316182e-01 9.90954936e-01 1.56844109e-02
 3.69407833e-02 9.97846007e-01 9.99169827e-01 2.29875930e-03
 3.03157233e-02 5.39790932e-03 9.98833239e-01 9.88492608e-01
 9.83199716e-01 5.13095700e-04 1.81126148e-02 9.97970521e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 20:38:54, Dev, Step : 6260, Loss : 0.45559, Acc : 0.825, Auc : 0.906, Sensitive_Loss : 0.12081, Sensitive_Acc : 16.893, Sensitive_Auc : 0.992, Mean auc: 0.906, Run Time : 153.93 sec
