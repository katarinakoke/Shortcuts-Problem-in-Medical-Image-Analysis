Running on desktop25:
stdin: is not a tty
Activating chexpert environment...
2
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_val.csv",
    "pred_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Biased_Sex_1_pos01.csv",
    "pred_model": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Biased_Sex_1_pos011.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": -0.05,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-27 14:10:43, Train, Epoch : 1, Step : 10, Loss : 0.72163, Acc : 0.600, Sensitive_Loss : 0.66908, Sensitive_Acc : 17.800, Run Time : 13.74 sec
INFO:root:2024-04-27 14:10:55, Train, Epoch : 1, Step : 20, Loss : 0.64664, Acc : 0.619, Sensitive_Loss : 0.71179, Sensitive_Acc : 14.900, Run Time : 12.19 sec
INFO:root:2024-04-27 14:11:07, Train, Epoch : 1, Step : 30, Loss : 0.61420, Acc : 0.722, Sensitive_Loss : 0.73885, Sensitive_Acc : 16.800, Run Time : 11.12 sec
INFO:root:2024-04-27 14:11:19, Train, Epoch : 1, Step : 40, Loss : 0.66497, Acc : 0.681, Sensitive_Loss : 0.76023, Sensitive_Acc : 16.000, Run Time : 12.50 sec
INFO:root:2024-04-27 14:11:30, Train, Epoch : 1, Step : 50, Loss : 0.54546, Acc : 0.750, Sensitive_Loss : 0.93254, Sensitive_Acc : 17.500, Run Time : 11.23 sec
INFO:root:2024-04-27 14:11:41, Train, Epoch : 1, Step : 60, Loss : 0.63235, Acc : 0.697, Sensitive_Loss : 1.02854, Sensitive_Acc : 16.500, Run Time : 11.06 sec
INFO:root:2024-04-27 14:11:54, Train, Epoch : 1, Step : 70, Loss : 0.52691, Acc : 0.756, Sensitive_Loss : 1.24305, Sensitive_Acc : 16.900, Run Time : 12.38 sec
INFO:root:2024-04-27 14:12:05, Train, Epoch : 1, Step : 80, Loss : 0.63360, Acc : 0.741, Sensitive_Loss : 1.22130, Sensitive_Acc : 16.200, Run Time : 11.07 sec
INFO:root:2024-04-27 14:12:17, Train, Epoch : 1, Step : 90, Loss : 0.52556, Acc : 0.759, Sensitive_Loss : 1.45184, Sensitive_Acc : 15.600, Run Time : 12.30 sec
INFO:root:2024-04-27 14:12:29, Train, Epoch : 1, Step : 100, Loss : 0.49147, Acc : 0.769, Sensitive_Loss : 1.58518, Sensitive_Acc : 15.300, Run Time : 12.32 sec
INFO:root:2024-04-27 14:15:22, Dev, Step : 100, Loss : 0.55044, Acc : 0.736, Auc : 0.819, Sensitive_Loss : 1.80256, Sensitive_Acc : 15.114, Sensitive_Auc : 0.043, Mean auc: 0.819, Run Time : 172.20 sec
INFO:root:2024-04-27 14:15:22, Best, Step : 100, Loss : 0.55044, Acc : 0.736, Auc : 0.819, Sensitive_Loss : 1.80256, Sensitive_Acc : 15.114, Sensitive_Auc : 0.043, Best Auc : 0.819
INFO:root:2024-04-27 14:15:31, Train, Epoch : 1, Step : 110, Loss : 0.55848, Acc : 0.750, Sensitive_Loss : 1.72874, Sensitive_Acc : 16.100, Run Time : 181.42 sec
INFO:root:2024-04-27 14:15:43, Train, Epoch : 1, Step : 120, Loss : 0.53849, Acc : 0.784, Sensitive_Loss : 1.64500, Sensitive_Acc : 15.200, Run Time : 12.48 sec
INFO:root:2024-04-27 14:15:54, Train, Epoch : 1, Step : 130, Loss : 0.59127, Acc : 0.738, Sensitive_Loss : 1.87643, Sensitive_Acc : 16.600, Run Time : 11.18 sec
INFO:root:2024-04-27 14:16:07, Train, Epoch : 1, Step : 140, Loss : 0.55618, Acc : 0.741, Sensitive_Loss : 1.89872, Sensitive_Acc : 17.200, Run Time : 12.09 sec
INFO:root:2024-04-27 14:16:19, Train, Epoch : 1, Step : 150, Loss : 0.60061, Acc : 0.731, Sensitive_Loss : 1.39703, Sensitive_Acc : 14.900, Run Time : 12.11 sec
INFO:root:2024-04-27 14:16:31, Train, Epoch : 1, Step : 160, Loss : 0.57220, Acc : 0.725, Sensitive_Loss : 1.21985, Sensitive_Acc : 15.600, Run Time : 12.00 sec
INFO:root:2024-04-27 14:16:43, Train, Epoch : 1, Step : 170, Loss : 0.58647, Acc : 0.756, Sensitive_Loss : 1.00002, Sensitive_Acc : 14.500, Run Time : 12.18 sec
INFO:root:2024-04-27 14:16:55, Train, Epoch : 1, Step : 180, Loss : 0.55744, Acc : 0.750, Sensitive_Loss : 0.98808, Sensitive_Acc : 15.200, Run Time : 11.79 sec
INFO:root:2024-04-27 14:17:06, Train, Epoch : 1, Step : 190, Loss : 0.57129, Acc : 0.728, Sensitive_Loss : 0.87701, Sensitive_Acc : 15.400, Run Time : 11.45 sec
INFO:root:2024-04-27 14:17:19, Train, Epoch : 1, Step : 200, Loss : 0.52377, Acc : 0.734, Sensitive_Loss : 0.93861, Sensitive_Acc : 16.100, Run Time : 12.65 sec
INFO:root:2024-04-27 14:19:54, Dev, Step : 200, Loss : 0.56064, Acc : 0.761, Auc : 0.831, Sensitive_Loss : 0.72400, Sensitive_Acc : 15.357, Sensitive_Auc : 0.551, Mean auc: 0.831, Run Time : 155.08 sec
INFO:root:2024-04-27 14:19:54, Best, Step : 200, Loss : 0.56064, Acc : 0.761, Auc : 0.831, Sensitive_Loss : 0.72400, Sensitive_Acc : 15.357, Sensitive_Auc : 0.551, Best Auc : 0.831
INFO:root:2024-04-27 14:20:03, Train, Epoch : 1, Step : 210, Loss : 0.50402, Acc : 0.769, Sensitive_Loss : 0.78252, Sensitive_Acc : 14.600, Run Time : 164.10 sec
INFO:root:2024-04-27 14:20:15, Train, Epoch : 1, Step : 220, Loss : 0.63093, Acc : 0.725, Sensitive_Loss : 0.76292, Sensitive_Acc : 16.600, Run Time : 12.07 sec
INFO:root:2024-04-27 14:20:26, Train, Epoch : 1, Step : 230, Loss : 0.49288, Acc : 0.741, Sensitive_Loss : 0.82548, Sensitive_Acc : 14.500, Run Time : 11.43 sec
INFO:root:2024-04-27 14:20:38, Train, Epoch : 1, Step : 240, Loss : 0.66404, Acc : 0.697, Sensitive_Loss : 1.06413, Sensitive_Acc : 15.000, Run Time : 11.50 sec
INFO:root:2024-04-27 14:20:50, Train, Epoch : 1, Step : 250, Loss : 0.47788, Acc : 0.809, Sensitive_Loss : 0.69619, Sensitive_Acc : 15.400, Run Time : 11.96 sec
INFO:root:2024-04-27 14:21:02, Train, Epoch : 1, Step : 260, Loss : 0.59740, Acc : 0.716, Sensitive_Loss : 0.72119, Sensitive_Acc : 17.800, Run Time : 12.41 sec
INFO:root:2024-04-27 14:21:14, Train, Epoch : 1, Step : 270, Loss : 0.53320, Acc : 0.741, Sensitive_Loss : 0.60442, Sensitive_Acc : 15.900, Run Time : 11.96 sec
INFO:root:2024-04-27 14:21:26, Train, Epoch : 1, Step : 280, Loss : 0.47507, Acc : 0.769, Sensitive_Loss : 0.56559, Sensitive_Acc : 15.200, Run Time : 11.78 sec
INFO:root:2024-04-27 14:21:38, Train, Epoch : 1, Step : 290, Loss : 0.51990, Acc : 0.781, Sensitive_Loss : 0.45797, Sensitive_Acc : 16.500, Run Time : 12.15 sec
INFO:root:2024-04-27 14:21:50, Train, Epoch : 1, Step : 300, Loss : 0.50729, Acc : 0.753, Sensitive_Loss : 0.45302, Sensitive_Acc : 15.400, Run Time : 11.78 sec
INFO:root:2024-04-27 14:24:25, Dev, Step : 300, Loss : 0.62055, Acc : 0.735, Auc : 0.862, Sensitive_Loss : 0.57030, Sensitive_Acc : 16.457, Sensitive_Auc : 0.675, Mean auc: 0.862, Run Time : 154.73 sec
INFO:root:2024-04-27 14:24:25, Best, Step : 300, Loss : 0.62055, Acc : 0.735, Auc : 0.862, Sensitive_Loss : 0.57030, Sensitive_Acc : 16.457, Sensitive_Auc : 0.675, Best Auc : 0.862
INFO:root:2024-04-27 14:24:34, Train, Epoch : 1, Step : 310, Loss : 0.40936, Acc : 0.803, Sensitive_Loss : 0.50407, Sensitive_Acc : 16.000, Run Time : 163.95 sec
INFO:root:2024-04-27 14:24:46, Train, Epoch : 1, Step : 320, Loss : 0.51639, Acc : 0.759, Sensitive_Loss : 0.45950, Sensitive_Acc : 16.800, Run Time : 12.17 sec
INFO:root:2024-04-27 14:24:58, Train, Epoch : 1, Step : 330, Loss : 0.47202, Acc : 0.778, Sensitive_Loss : 0.49351, Sensitive_Acc : 14.900, Run Time : 11.70 sec
INFO:root:2024-04-27 14:25:10, Train, Epoch : 1, Step : 340, Loss : 0.60770, Acc : 0.728, Sensitive_Loss : 0.39944, Sensitive_Acc : 18.600, Run Time : 12.10 sec
INFO:root:2024-04-27 14:25:22, Train, Epoch : 1, Step : 350, Loss : 0.50664, Acc : 0.738, Sensitive_Loss : 0.37058, Sensitive_Acc : 15.500, Run Time : 12.24 sec
INFO:root:2024-04-27 14:25:34, Train, Epoch : 1, Step : 360, Loss : 0.48724, Acc : 0.778, Sensitive_Loss : 0.40167, Sensitive_Acc : 16.000, Run Time : 11.95 sec
INFO:root:2024-04-27 14:25:46, Train, Epoch : 1, Step : 370, Loss : 0.49643, Acc : 0.772, Sensitive_Loss : 0.31000, Sensitive_Acc : 16.000, Run Time : 11.57 sec
INFO:root:2024-04-27 14:25:57, Train, Epoch : 1, Step : 380, Loss : 0.56276, Acc : 0.703, Sensitive_Loss : 0.33483, Sensitive_Acc : 17.300, Run Time : 11.78 sec
INFO:root:2024-04-27 14:26:09, Train, Epoch : 1, Step : 390, Loss : 0.48511, Acc : 0.787, Sensitive_Loss : 0.34440, Sensitive_Acc : 16.400, Run Time : 12.03 sec
INFO:root:2024-04-27 14:26:21, Train, Epoch : 1, Step : 400, Loss : 0.45770, Acc : 0.791, Sensitive_Loss : 0.33092, Sensitive_Acc : 16.300, Run Time : 11.86 sec
INFO:root:2024-04-27 14:29:32, Dev, Step : 400, Loss : 0.51977, Acc : 0.761, Auc : 0.834, Sensitive_Loss : 0.36497, Sensitive_Acc : 16.793, Sensitive_Auc : 0.839, Mean auc: 0.834, Run Time : 190.56 sec
INFO:root:2024-04-27 14:29:40, Train, Epoch : 1, Step : 410, Loss : 0.46410, Acc : 0.762, Sensitive_Loss : 0.29362, Sensitive_Acc : 16.700, Run Time : 199.05 sec
INFO:root:2024-04-27 14:29:52, Train, Epoch : 1, Step : 420, Loss : 0.51075, Acc : 0.781, Sensitive_Loss : 0.23763, Sensitive_Acc : 17.700, Run Time : 11.62 sec
INFO:root:2024-04-27 14:30:04, Train, Epoch : 1, Step : 430, Loss : 0.46934, Acc : 0.744, Sensitive_Loss : 0.30963, Sensitive_Acc : 16.100, Run Time : 12.24 sec
INFO:root:2024-04-27 14:30:16, Train, Epoch : 1, Step : 440, Loss : 0.53632, Acc : 0.744, Sensitive_Loss : 0.26192, Sensitive_Acc : 16.700, Run Time : 11.71 sec
INFO:root:2024-04-27 14:30:28, Train, Epoch : 1, Step : 450, Loss : 0.43868, Acc : 0.812, Sensitive_Loss : 0.26550, Sensitive_Acc : 16.400, Run Time : 12.22 sec
INFO:root:2024-04-27 14:30:40, Train, Epoch : 1, Step : 460, Loss : 0.43368, Acc : 0.797, Sensitive_Loss : 0.29498, Sensitive_Acc : 16.400, Run Time : 11.90 sec
INFO:root:2024-04-27 14:30:52, Train, Epoch : 1, Step : 470, Loss : 0.49992, Acc : 0.769, Sensitive_Loss : 0.27578, Sensitive_Acc : 14.200, Run Time : 11.82 sec
INFO:root:2024-04-27 14:31:04, Train, Epoch : 1, Step : 480, Loss : 0.40514, Acc : 0.806, Sensitive_Loss : 0.23012, Sensitive_Acc : 14.400, Run Time : 11.88 sec
INFO:root:2024-04-27 14:31:15, Train, Epoch : 1, Step : 490, Loss : 0.46909, Acc : 0.800, Sensitive_Loss : 0.24900, Sensitive_Acc : 16.400, Run Time : 11.59 sec
INFO:root:2024-04-27 14:31:27, Train, Epoch : 1, Step : 500, Loss : 0.45755, Acc : 0.781, Sensitive_Loss : 0.17588, Sensitive_Acc : 16.900, Run Time : 11.54 sec
INFO:root:2024-04-27 14:34:01, Dev, Step : 500, Loss : 0.61994, Acc : 0.695, Auc : 0.849, Sensitive_Loss : 0.38214, Sensitive_Acc : 16.636, Sensitive_Auc : 0.928, Mean auc: 0.849, Run Time : 154.19 sec
INFO:root:2024-04-27 14:34:09, Train, Epoch : 1, Step : 510, Loss : 0.44475, Acc : 0.803, Sensitive_Loss : 0.18562, Sensitive_Acc : 17.200, Run Time : 162.67 sec
INFO:root:2024-04-27 14:34:21, Train, Epoch : 1, Step : 520, Loss : 0.49557, Acc : 0.794, Sensitive_Loss : 0.20625, Sensitive_Acc : 16.200, Run Time : 11.46 sec
INFO:root:2024-04-27 14:34:33, Train, Epoch : 1, Step : 530, Loss : 0.49250, Acc : 0.800, Sensitive_Loss : 0.21454, Sensitive_Acc : 17.300, Run Time : 12.23 sec
INFO:root:2024-04-27 14:34:46, Train, Epoch : 1, Step : 540, Loss : 0.44661, Acc : 0.831, Sensitive_Loss : 0.21464, Sensitive_Acc : 17.400, Run Time : 12.40 sec
INFO:root:2024-04-27 14:34:57, Train, Epoch : 1, Step : 550, Loss : 0.50957, Acc : 0.787, Sensitive_Loss : 0.20872, Sensitive_Acc : 17.300, Run Time : 11.29 sec
INFO:root:2024-04-27 14:35:08, Train, Epoch : 1, Step : 560, Loss : 0.47850, Acc : 0.806, Sensitive_Loss : 0.18891, Sensitive_Acc : 16.900, Run Time : 11.56 sec
INFO:root:2024-04-27 14:35:20, Train, Epoch : 1, Step : 570, Loss : 0.51286, Acc : 0.787, Sensitive_Loss : 0.19283, Sensitive_Acc : 16.300, Run Time : 11.71 sec
INFO:root:2024-04-27 14:35:33, Train, Epoch : 1, Step : 580, Loss : 0.52016, Acc : 0.759, Sensitive_Loss : 0.23237, Sensitive_Acc : 16.000, Run Time : 12.68 sec
INFO:root:2024-04-27 14:35:45, Train, Epoch : 1, Step : 590, Loss : 0.46305, Acc : 0.759, Sensitive_Loss : 0.19341, Sensitive_Acc : 17.000, Run Time : 12.30 sec
INFO:root:2024-04-27 14:35:57, Train, Epoch : 1, Step : 600, Loss : 0.44713, Acc : 0.781, Sensitive_Loss : 0.19257, Sensitive_Acc : 15.000, Run Time : 11.99 sec
INFO:root:2024-04-27 14:38:32, Dev, Step : 600, Loss : 0.50199, Acc : 0.779, Auc : 0.861, Sensitive_Loss : 0.23673, Sensitive_Acc : 16.864, Sensitive_Auc : 0.947, Mean auc: 0.861, Run Time : 154.94 sec
INFO:root:2024-04-27 14:38:41, Train, Epoch : 1, Step : 610, Loss : 0.42492, Acc : 0.791, Sensitive_Loss : 0.17818, Sensitive_Acc : 16.200, Run Time : 163.96 sec
INFO:root:2024-04-27 14:38:54, Train, Epoch : 1, Step : 620, Loss : 0.46988, Acc : 0.794, Sensitive_Loss : 0.21475, Sensitive_Acc : 16.200, Run Time : 12.52 sec
INFO:root:2024-04-27 14:41:33
INFO:root:y_pred: [0.03135308 0.9341817  0.03051769 ... 0.9140091  0.02428938 0.66275114]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.81249869e-01 1.65579899e-03 6.18172437e-02 2.85293099e-05
 9.64447498e-01 3.94164563e-05 9.88705337e-01 9.37846482e-01
 1.82959321e-03 9.72559392e-01 9.85536635e-01 9.86907840e-01
 9.92519617e-01 8.52795660e-01 6.45396709e-02 8.78561139e-01
 9.85850871e-01 3.67785548e-03 9.26846504e-01 8.80365014e-01
 9.26608086e-01 4.16272096e-02 9.78158236e-01 8.37649226e-01
 9.76298213e-01 9.13664639e-01 7.38500617e-04 9.63910997e-01
 9.28634524e-01 5.22779047e-01 6.92079542e-04 3.91017139e-01
 2.22763605e-02 2.60687843e-02 3.73913556e-01 2.77748011e-04
 9.18334648e-02 8.34684633e-03 9.72031772e-01 9.35618937e-01
 2.31785612e-04 3.17966077e-03 9.66579616e-01 2.36819941e-03
 9.83320296e-01 9.83463585e-01 9.14765060e-01 9.71022069e-01
 1.07766144e-01 9.68474209e-01 9.83359218e-01 8.72579589e-02
 4.15899813e-01 4.90636239e-03 2.94288093e-05 1.24776606e-02
 1.32669685e-02 3.43072802e-01 6.22741319e-03 6.93647742e-01
 3.34170051e-02 1.29926413e-01 7.72768282e-04 8.48220408e-01
 4.08482403e-02 9.67761636e-01 2.48555886e-03 9.71717179e-01
 9.26202118e-01 6.32470310e-01 9.33002114e-01 6.95784688e-01
 3.81908263e-03 7.25660771e-02 6.08149800e-04 5.11939405e-04
 9.94480923e-02 1.63123652e-01 1.11405095e-02 9.46079314e-01
 9.83964026e-01 2.58956221e-04 3.04406378e-02 2.70913774e-03
 9.25230503e-01 8.64838660e-01 2.03692121e-03 1.02399429e-02
 9.48767900e-01 9.70231831e-01 9.75198328e-01 7.95641690e-02
 2.03111973e-02 9.73161697e-01 8.88567805e-01 7.57258967e-05
 9.75544870e-01 9.63229954e-01 1.41049488e-04 8.71167285e-04
 9.48616266e-01 8.85586083e-01 9.77552414e-01 9.74268556e-01
 1.05293060e-03 1.66090745e-02 9.17249978e-01 9.43553746e-01
 9.78648722e-01 1.26219238e-03 8.25502992e-01 9.75106716e-01
 6.23764217e-01 9.79564250e-01 9.36901748e-01 9.57862377e-01
 9.45644140e-01 9.66198862e-01 3.43074292e-01 3.83535922e-01
 9.69562232e-01 9.66824234e-01 5.21171140e-04 9.80674148e-01
 9.66649234e-01 8.50742698e-01 9.83370066e-01 5.75902313e-03
 2.42237300e-01 9.48854923e-01 9.88102317e-01 7.81676080e-03
 2.99957581e-04 1.30341621e-02 9.78983819e-01 9.93585646e-01
 9.15850818e-01 5.27029624e-04 3.42631526e-02 9.26849604e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 14:41:33, Dev, Step : 626, Loss : 0.53203, Acc : 0.789, Auc : 0.867, Sensitive_Loss : 0.17555, Sensitive_Acc : 16.850, Sensitive_Auc : 0.989, Mean auc: 0.867, Run Time : 152.74 sec
INFO:root:2024-04-27 14:41:34, Best, Step : 626, Loss : 0.53203, Acc : 0.789,Auc : 0.867, Best Auc : 0.867, Sensitive_Loss : 0.17555, Sensitive_Acc : 16.850, Sensitive_Auc : 0.989
INFO:root:2024-04-27 14:41:41, Train, Epoch : 2, Step : 630, Loss : 0.17577, Acc : 0.312, Sensitive_Loss : 0.04455, Sensitive_Acc : 6.500, Run Time : 5.78 sec
INFO:root:2024-04-27 14:41:53, Train, Epoch : 2, Step : 640, Loss : 0.41620, Acc : 0.847, Sensitive_Loss : 0.17265, Sensitive_Acc : 14.800, Run Time : 11.96 sec
INFO:root:2024-04-27 14:42:04, Train, Epoch : 2, Step : 650, Loss : 0.37461, Acc : 0.791, Sensitive_Loss : 0.22337, Sensitive_Acc : 18.000, Run Time : 10.79 sec
INFO:root:2024-04-27 14:42:16, Train, Epoch : 2, Step : 660, Loss : 0.41347, Acc : 0.791, Sensitive_Loss : 0.17049, Sensitive_Acc : 15.100, Run Time : 12.04 sec
INFO:root:2024-04-27 14:42:28, Train, Epoch : 2, Step : 670, Loss : 0.45567, Acc : 0.797, Sensitive_Loss : 0.16392, Sensitive_Acc : 16.700, Run Time : 11.96 sec
INFO:root:2024-04-27 14:42:38, Train, Epoch : 2, Step : 680, Loss : 0.47029, Acc : 0.803, Sensitive_Loss : 0.16541, Sensitive_Acc : 16.000, Run Time : 10.58 sec
INFO:root:2024-04-27 14:42:49, Train, Epoch : 2, Step : 690, Loss : 0.49010, Acc : 0.784, Sensitive_Loss : 0.19616, Sensitive_Acc : 14.800, Run Time : 11.02 sec
INFO:root:2024-04-27 14:43:00, Train, Epoch : 2, Step : 700, Loss : 0.43386, Acc : 0.819, Sensitive_Loss : 0.17066, Sensitive_Acc : 14.800, Run Time : 11.35 sec
INFO:root:2024-04-27 14:45:38, Dev, Step : 700, Loss : 0.46692, Acc : 0.803, Auc : 0.872, Sensitive_Loss : 0.17490, Sensitive_Acc : 16.793, Sensitive_Auc : 0.979, Mean auc: 0.872, Run Time : 157.10 sec
INFO:root:2024-04-27 14:45:38, Best, Step : 700, Loss : 0.46692, Acc : 0.803, Auc : 0.872, Sensitive_Loss : 0.17490, Sensitive_Acc : 16.793, Sensitive_Auc : 0.979, Best Auc : 0.872
INFO:root:2024-04-27 14:45:47, Train, Epoch : 2, Step : 710, Loss : 0.45943, Acc : 0.809, Sensitive_Loss : 0.18135, Sensitive_Acc : 16.500, Run Time : 166.72 sec
INFO:root:2024-04-27 14:45:59, Train, Epoch : 2, Step : 720, Loss : 0.43697, Acc : 0.825, Sensitive_Loss : 0.18250, Sensitive_Acc : 16.500, Run Time : 11.78 sec
INFO:root:2024-04-27 14:46:10, Train, Epoch : 2, Step : 730, Loss : 0.44207, Acc : 0.781, Sensitive_Loss : 0.21406, Sensitive_Acc : 15.900, Run Time : 10.77 sec
INFO:root:2024-04-27 14:46:22, Train, Epoch : 2, Step : 740, Loss : 0.41578, Acc : 0.769, Sensitive_Loss : 0.18441, Sensitive_Acc : 16.800, Run Time : 11.74 sec
INFO:root:2024-04-27 14:46:33, Train, Epoch : 2, Step : 750, Loss : 0.47795, Acc : 0.797, Sensitive_Loss : 0.13887, Sensitive_Acc : 16.800, Run Time : 11.83 sec
INFO:root:2024-04-27 14:46:45, Train, Epoch : 2, Step : 760, Loss : 0.42609, Acc : 0.806, Sensitive_Loss : 0.15399, Sensitive_Acc : 14.600, Run Time : 11.61 sec
INFO:root:2024-04-27 14:46:56, Train, Epoch : 2, Step : 770, Loss : 0.45522, Acc : 0.800, Sensitive_Loss : 0.14659, Sensitive_Acc : 16.600, Run Time : 11.40 sec
INFO:root:2024-04-27 14:47:07, Train, Epoch : 2, Step : 780, Loss : 0.39401, Acc : 0.831, Sensitive_Loss : 0.13100, Sensitive_Acc : 16.100, Run Time : 10.54 sec
INFO:root:2024-04-27 14:47:18, Train, Epoch : 2, Step : 790, Loss : 0.41679, Acc : 0.812, Sensitive_Loss : 0.19260, Sensitive_Acc : 16.800, Run Time : 11.15 sec
INFO:root:2024-04-27 14:47:29, Train, Epoch : 2, Step : 800, Loss : 0.41938, Acc : 0.831, Sensitive_Loss : 0.20510, Sensitive_Acc : 16.900, Run Time : 11.22 sec
INFO:root:2024-04-27 14:50:04, Dev, Step : 800, Loss : 0.46075, Acc : 0.793, Auc : 0.878, Sensitive_Loss : 0.17636, Sensitive_Acc : 16.807, Sensitive_Auc : 0.953, Mean auc: 0.878, Run Time : 154.96 sec
INFO:root:2024-04-27 14:50:05, Best, Step : 800, Loss : 0.46075, Acc : 0.793, Auc : 0.878, Sensitive_Loss : 0.17636, Sensitive_Acc : 16.807, Sensitive_Auc : 0.953, Best Auc : 0.878
INFO:root:2024-04-27 14:50:14, Train, Epoch : 2, Step : 810, Loss : 0.42039, Acc : 0.809, Sensitive_Loss : 0.20233, Sensitive_Acc : 15.800, Run Time : 164.66 sec
INFO:root:2024-04-27 14:50:25, Train, Epoch : 2, Step : 820, Loss : 0.43919, Acc : 0.819, Sensitive_Loss : 0.16870, Sensitive_Acc : 17.400, Run Time : 11.40 sec
INFO:root:2024-04-27 14:50:37, Train, Epoch : 2, Step : 830, Loss : 0.41911, Acc : 0.822, Sensitive_Loss : 0.13816, Sensitive_Acc : 16.200, Run Time : 11.25 sec
INFO:root:2024-04-27 14:50:48, Train, Epoch : 2, Step : 840, Loss : 0.55242, Acc : 0.784, Sensitive_Loss : 0.16181, Sensitive_Acc : 17.400, Run Time : 11.59 sec
INFO:root:2024-04-27 14:51:00, Train, Epoch : 2, Step : 850, Loss : 0.50757, Acc : 0.778, Sensitive_Loss : 0.15575, Sensitive_Acc : 16.700, Run Time : 11.95 sec
INFO:root:2024-04-27 14:51:12, Train, Epoch : 2, Step : 860, Loss : 0.40039, Acc : 0.828, Sensitive_Loss : 0.14920, Sensitive_Acc : 16.100, Run Time : 11.96 sec
INFO:root:2024-04-27 14:51:23, Train, Epoch : 2, Step : 870, Loss : 0.40757, Acc : 0.819, Sensitive_Loss : 0.13636, Sensitive_Acc : 17.000, Run Time : 11.24 sec
INFO:root:2024-04-27 14:51:35, Train, Epoch : 2, Step : 880, Loss : 0.42602, Acc : 0.803, Sensitive_Loss : 0.16322, Sensitive_Acc : 16.200, Run Time : 11.84 sec
INFO:root:2024-04-27 14:51:48, Train, Epoch : 2, Step : 890, Loss : 0.41663, Acc : 0.819, Sensitive_Loss : 0.15430, Sensitive_Acc : 17.500, Run Time : 12.48 sec
INFO:root:2024-04-27 14:51:57, Train, Epoch : 2, Step : 900, Loss : 0.41631, Acc : 0.831, Sensitive_Loss : 0.20677, Sensitive_Acc : 16.400, Run Time : 9.83 sec
INFO:root:2024-04-27 14:54:36, Dev, Step : 900, Loss : 0.46831, Acc : 0.805, Auc : 0.886, Sensitive_Loss : 0.15711, Sensitive_Acc : 16.921, Sensitive_Auc : 0.983, Mean auc: 0.886, Run Time : 158.59 sec
INFO:root:2024-04-27 14:54:37, Best, Step : 900, Loss : 0.46831, Acc : 0.805, Auc : 0.886, Sensitive_Loss : 0.15711, Sensitive_Acc : 16.921, Sensitive_Auc : 0.983, Best Auc : 0.886
INFO:root:2024-04-27 14:54:45, Train, Epoch : 2, Step : 910, Loss : 0.44103, Acc : 0.816, Sensitive_Loss : 0.19994, Sensitive_Acc : 16.900, Run Time : 167.49 sec
INFO:root:2024-04-27 14:54:58, Train, Epoch : 2, Step : 920, Loss : 0.47409, Acc : 0.744, Sensitive_Loss : 0.12171, Sensitive_Acc : 15.800, Run Time : 13.37 sec
INFO:root:2024-04-27 14:55:12, Train, Epoch : 2, Step : 930, Loss : 0.38758, Acc : 0.847, Sensitive_Loss : 0.15907, Sensitive_Acc : 17.400, Run Time : 13.32 sec
INFO:root:2024-04-27 14:55:23, Train, Epoch : 2, Step : 940, Loss : 0.45612, Acc : 0.781, Sensitive_Loss : 0.15611, Sensitive_Acc : 15.700, Run Time : 10.86 sec
INFO:root:2024-04-27 14:55:34, Train, Epoch : 2, Step : 950, Loss : 0.48409, Acc : 0.753, Sensitive_Loss : 0.14155, Sensitive_Acc : 15.700, Run Time : 11.25 sec
INFO:root:2024-04-27 14:55:47, Train, Epoch : 2, Step : 960, Loss : 0.46384, Acc : 0.825, Sensitive_Loss : 0.12490, Sensitive_Acc : 15.800, Run Time : 12.79 sec
INFO:root:2024-04-27 14:55:58, Train, Epoch : 2, Step : 970, Loss : 0.40145, Acc : 0.853, Sensitive_Loss : 0.13701, Sensitive_Acc : 15.900, Run Time : 11.66 sec
INFO:root:2024-04-27 14:56:10, Train, Epoch : 2, Step : 980, Loss : 0.34559, Acc : 0.831, Sensitive_Loss : 0.12466, Sensitive_Acc : 16.300, Run Time : 11.79 sec
INFO:root:2024-04-27 14:56:22, Train, Epoch : 2, Step : 990, Loss : 0.52173, Acc : 0.787, Sensitive_Loss : 0.12905, Sensitive_Acc : 15.300, Run Time : 12.00 sec
INFO:root:2024-04-27 14:56:33, Train, Epoch : 2, Step : 1000, Loss : 0.39854, Acc : 0.822, Sensitive_Loss : 0.20514, Sensitive_Acc : 16.600, Run Time : 10.79 sec
INFO:root:2024-04-27 14:59:14, Dev, Step : 1000, Loss : 0.50404, Acc : 0.779, Auc : 0.885, Sensitive_Loss : 0.16622, Sensitive_Acc : 16.821, Sensitive_Auc : 0.979, Mean auc: 0.885, Run Time : 161.63 sec
INFO:root:2024-04-27 14:59:23, Train, Epoch : 2, Step : 1010, Loss : 0.35957, Acc : 0.819, Sensitive_Loss : 0.15491, Sensitive_Acc : 17.100, Run Time : 170.53 sec
INFO:root:2024-04-27 14:59:34, Train, Epoch : 2, Step : 1020, Loss : 0.48439, Acc : 0.769, Sensitive_Loss : 0.15746, Sensitive_Acc : 17.000, Run Time : 10.84 sec
INFO:root:2024-04-27 14:59:48, Train, Epoch : 2, Step : 1030, Loss : 0.43092, Acc : 0.784, Sensitive_Loss : 0.13701, Sensitive_Acc : 15.800, Run Time : 13.37 sec
INFO:root:2024-04-27 15:00:01, Train, Epoch : 2, Step : 1040, Loss : 0.50182, Acc : 0.784, Sensitive_Loss : 0.13931, Sensitive_Acc : 15.000, Run Time : 13.53 sec
INFO:root:2024-04-27 15:00:11, Train, Epoch : 2, Step : 1050, Loss : 0.50285, Acc : 0.787, Sensitive_Loss : 0.14357, Sensitive_Acc : 18.200, Run Time : 10.20 sec
INFO:root:2024-04-27 15:00:22, Train, Epoch : 2, Step : 1060, Loss : 0.41696, Acc : 0.794, Sensitive_Loss : 0.14780, Sensitive_Acc : 16.300, Run Time : 11.04 sec
INFO:root:2024-04-27 15:00:36, Train, Epoch : 2, Step : 1070, Loss : 0.40449, Acc : 0.828, Sensitive_Loss : 0.15259, Sensitive_Acc : 17.300, Run Time : 13.31 sec
INFO:root:2024-04-27 15:00:48, Train, Epoch : 2, Step : 1080, Loss : 0.44366, Acc : 0.812, Sensitive_Loss : 0.16452, Sensitive_Acc : 16.200, Run Time : 12.27 sec
INFO:root:2024-04-27 15:01:00, Train, Epoch : 2, Step : 1090, Loss : 0.44637, Acc : 0.806, Sensitive_Loss : 0.16695, Sensitive_Acc : 17.500, Run Time : 11.85 sec
INFO:root:2024-04-27 15:01:11, Train, Epoch : 2, Step : 1100, Loss : 0.45770, Acc : 0.806, Sensitive_Loss : 0.14741, Sensitive_Acc : 17.100, Run Time : 11.68 sec
INFO:root:2024-04-27 15:03:54, Dev, Step : 1100, Loss : 0.47794, Acc : 0.791, Auc : 0.884, Sensitive_Loss : 0.24225, Sensitive_Acc : 16.821, Sensitive_Auc : 0.964, Mean auc: 0.884, Run Time : 162.43 sec
INFO:root:2024-04-27 15:04:05, Train, Epoch : 2, Step : 1110, Loss : 0.42696, Acc : 0.809, Sensitive_Loss : 0.21695, Sensitive_Acc : 16.200, Run Time : 173.17 sec
INFO:root:2024-04-27 15:04:16, Train, Epoch : 2, Step : 1120, Loss : 0.34145, Acc : 0.844, Sensitive_Loss : 0.10084, Sensitive_Acc : 17.400, Run Time : 11.91 sec
INFO:root:2024-04-27 15:04:28, Train, Epoch : 2, Step : 1130, Loss : 0.46753, Acc : 0.800, Sensitive_Loss : 0.15143, Sensitive_Acc : 16.300, Run Time : 11.83 sec
INFO:root:2024-04-27 15:04:42, Train, Epoch : 2, Step : 1140, Loss : 0.45275, Acc : 0.812, Sensitive_Loss : 0.14231, Sensitive_Acc : 16.500, Run Time : 13.40 sec
INFO:root:2024-04-27 15:04:54, Train, Epoch : 2, Step : 1150, Loss : 0.47787, Acc : 0.775, Sensitive_Loss : 0.24373, Sensitive_Acc : 17.400, Run Time : 12.13 sec
INFO:root:2024-04-27 15:05:07, Train, Epoch : 2, Step : 1160, Loss : 0.48062, Acc : 0.797, Sensitive_Loss : 0.18321, Sensitive_Acc : 14.200, Run Time : 13.06 sec
INFO:root:2024-04-27 15:05:19, Train, Epoch : 2, Step : 1170, Loss : 0.39100, Acc : 0.825, Sensitive_Loss : 0.14773, Sensitive_Acc : 17.100, Run Time : 11.69 sec
INFO:root:2024-04-27 15:05:30, Train, Epoch : 2, Step : 1180, Loss : 0.48352, Acc : 0.800, Sensitive_Loss : 0.10444, Sensitive_Acc : 16.100, Run Time : 11.38 sec
INFO:root:2024-04-27 15:05:42, Train, Epoch : 2, Step : 1190, Loss : 0.45769, Acc : 0.778, Sensitive_Loss : 0.08417, Sensitive_Acc : 15.200, Run Time : 11.62 sec
INFO:root:2024-04-27 15:05:53, Train, Epoch : 2, Step : 1200, Loss : 0.47733, Acc : 0.772, Sensitive_Loss : 0.14093, Sensitive_Acc : 15.300, Run Time : 11.07 sec
INFO:root:2024-04-27 15:08:37, Dev, Step : 1200, Loss : 0.46907, Acc : 0.799, Auc : 0.882, Sensitive_Loss : 0.20822, Sensitive_Acc : 16.750, Sensitive_Auc : 0.978, Mean auc: 0.882, Run Time : 164.06 sec
INFO:root:2024-04-27 15:08:46, Train, Epoch : 2, Step : 1210, Loss : 0.48527, Acc : 0.778, Sensitive_Loss : 0.09455, Sensitive_Acc : 16.200, Run Time : 172.88 sec
INFO:root:2024-04-27 15:08:59, Train, Epoch : 2, Step : 1220, Loss : 0.46091, Acc : 0.812, Sensitive_Loss : 0.13074, Sensitive_Acc : 15.800, Run Time : 13.88 sec
INFO:root:2024-04-27 15:09:12, Train, Epoch : 2, Step : 1230, Loss : 0.45298, Acc : 0.787, Sensitive_Loss : 0.15108, Sensitive_Acc : 15.000, Run Time : 12.18 sec
INFO:root:2024-04-27 15:09:22, Train, Epoch : 2, Step : 1240, Loss : 0.49945, Acc : 0.797, Sensitive_Loss : 0.10738, Sensitive_Acc : 16.000, Run Time : 10.74 sec
INFO:root:2024-04-27 15:09:33, Train, Epoch : 2, Step : 1250, Loss : 0.39124, Acc : 0.828, Sensitive_Loss : 0.09940, Sensitive_Acc : 16.100, Run Time : 11.13 sec
INFO:root:2024-04-27 15:12:08
INFO:root:y_pred: [0.0379472  0.72876    0.05145824 ... 0.595281   0.02602688 0.7554545 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.9592751e-01 1.2012682e-06 9.4196945e-02 3.6700005e-06 9.9581760e-01
 7.1919675e-07 9.9848419e-01 9.8672765e-01 1.6747731e-04 7.6562899e-01
 9.9836105e-01 9.9850732e-01 9.9760854e-01 8.7706989e-01 1.1713097e-01
 9.4582522e-01 9.9649256e-01 2.5170724e-04 9.4634175e-01 7.3757970e-01
 9.9445701e-01 1.5366774e-03 9.9941981e-01 9.7175115e-01 9.6085006e-01
 8.5392970e-01 1.6454885e-03 9.9450725e-01 9.8656321e-01 9.7211683e-01
 7.6751262e-03 4.1559783e-01 2.9666177e-03 5.8569367e-05 3.0370164e-01
 7.3589021e-05 1.1263615e-01 1.2921944e-02 9.8811054e-01 9.7985774e-01
 4.0229716e-06 6.6597614e-04 9.8073304e-01 1.1519553e-05 9.9875915e-01
 9.9160075e-01 9.8133987e-01 9.8462808e-01 3.3767710e-03 9.8206699e-01
 9.9774361e-01 1.9665733e-03 3.4412113e-01 2.4347793e-05 2.6371234e-04
 4.2711034e-02 1.4731039e-03 9.0480391e-03 5.0183566e-04 6.2382179e-01
 4.4135042e-03 2.7792713e-02 6.8427989e-04 9.4538158e-01 2.4136972e-01
 9.9797636e-01 3.6177345e-02 9.9732971e-01 8.3840370e-01 9.6089834e-01
 9.8490566e-01 1.0995086e-01 1.9746814e-02 2.4199754e-02 1.9077897e-04
 2.7252923e-05 3.4113925e-02 7.6641142e-01 1.1993503e-04 9.9157917e-01
 9.9754661e-01 2.1805117e-05 1.5928690e-01 5.9948093e-03 1.2645666e-01
 7.9526061e-01 3.7535839e-03 4.5713659e-02 9.8718899e-01 9.8608392e-01
 9.9769348e-01 9.1869764e-02 3.5734503e-03 9.9082243e-01 7.6163566e-01
 4.5492998e-05 9.9172783e-01 9.9510914e-01 1.6445067e-05 1.0703257e-03
 9.8876232e-01 9.5436740e-01 9.9399298e-01 9.9213362e-01 4.6541107e-05
 1.0664862e-01 8.0545443e-01 9.4124979e-01 9.5485234e-01 8.9719206e-06
 6.3499105e-01 9.9069864e-01 6.0469002e-01 9.9722123e-01 9.7868472e-01
 9.9194747e-01 8.9612335e-01 9.9564362e-01 6.0413921e-01 8.5695875e-01
 9.9785727e-01 9.8912209e-01 7.8320127e-06 9.3858278e-01 9.9662906e-01
 1.4967431e-02 9.9592400e-01 7.7658035e-02 3.5661478e-02 9.7450089e-01
 9.9479324e-01 1.0861161e-02 8.4944506e-04 1.8004682e-02 9.9322695e-01
 9.9888045e-01 9.4786167e-01 1.1271227e-02 7.9753427e-03 9.6397448e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 15:12:08, Dev, Step : 1252, Loss : 0.46228, Acc : 0.801, Auc : 0.892, Sensitive_Loss : 0.16727, Sensitive_Acc : 16.793, Sensitive_Auc : 0.984, Mean auc: 0.892, Run Time : 152.98 sec
INFO:root:2024-04-27 15:12:08, Best, Step : 1252, Loss : 0.46228, Acc : 0.801,Auc : 0.892, Best Auc : 0.892, Sensitive_Loss : 0.16727, Sensitive_Acc : 16.793, Sensitive_Auc : 0.984
INFO:root:2024-04-27 15:12:21, Train, Epoch : 3, Step : 1260, Loss : 0.32810, Acc : 0.650, Sensitive_Loss : 0.10757, Sensitive_Acc : 13.700, Run Time : 11.05 sec
INFO:root:2024-04-27 15:12:33, Train, Epoch : 3, Step : 1270, Loss : 0.42408, Acc : 0.819, Sensitive_Loss : 0.10347, Sensitive_Acc : 16.100, Run Time : 12.45 sec
INFO:root:2024-04-27 15:12:44, Train, Epoch : 3, Step : 1280, Loss : 0.41897, Acc : 0.816, Sensitive_Loss : 0.14903, Sensitive_Acc : 15.200, Run Time : 10.84 sec
INFO:root:2024-04-27 15:12:56, Train, Epoch : 3, Step : 1290, Loss : 0.43623, Acc : 0.822, Sensitive_Loss : 0.16670, Sensitive_Acc : 15.500, Run Time : 11.84 sec
INFO:root:2024-04-27 15:13:06, Train, Epoch : 3, Step : 1300, Loss : 0.41113, Acc : 0.819, Sensitive_Loss : 0.08994, Sensitive_Acc : 15.900, Run Time : 10.44 sec
INFO:root:2024-04-27 15:15:41, Dev, Step : 1300, Loss : 0.43391, Acc : 0.815, Auc : 0.897, Sensitive_Loss : 0.13106, Sensitive_Acc : 16.821, Sensitive_Auc : 0.989, Mean auc: 0.897, Run Time : 154.61 sec
INFO:root:2024-04-27 15:15:41, Best, Step : 1300, Loss : 0.43391, Acc : 0.815, Auc : 0.897, Sensitive_Loss : 0.13106, Sensitive_Acc : 16.821, Sensitive_Auc : 0.989, Best Auc : 0.897
INFO:root:2024-04-27 15:15:50, Train, Epoch : 3, Step : 1310, Loss : 0.37934, Acc : 0.819, Sensitive_Loss : 0.12002, Sensitive_Acc : 15.900, Run Time : 164.30 sec
INFO:root:2024-04-27 15:16:02, Train, Epoch : 3, Step : 1320, Loss : 0.47261, Acc : 0.794, Sensitive_Loss : 0.10538, Sensitive_Acc : 16.700, Run Time : 11.98 sec
INFO:root:2024-04-27 15:16:13, Train, Epoch : 3, Step : 1330, Loss : 0.42616, Acc : 0.819, Sensitive_Loss : 0.10206, Sensitive_Acc : 15.500, Run Time : 10.76 sec
INFO:root:2024-04-27 15:16:25, Train, Epoch : 3, Step : 1340, Loss : 0.47385, Acc : 0.784, Sensitive_Loss : 0.08082, Sensitive_Acc : 17.600, Run Time : 11.39 sec
INFO:root:2024-04-27 15:16:37, Train, Epoch : 3, Step : 1350, Loss : 0.35307, Acc : 0.847, Sensitive_Loss : 0.11085, Sensitive_Acc : 17.100, Run Time : 12.12 sec
INFO:root:2024-04-27 15:16:47, Train, Epoch : 3, Step : 1360, Loss : 0.42795, Acc : 0.797, Sensitive_Loss : 0.07898, Sensitive_Acc : 16.200, Run Time : 10.57 sec
INFO:root:2024-04-27 15:16:59, Train, Epoch : 3, Step : 1370, Loss : 0.37259, Acc : 0.847, Sensitive_Loss : 0.14109, Sensitive_Acc : 17.500, Run Time : 11.89 sec
INFO:root:2024-04-27 15:17:11, Train, Epoch : 3, Step : 1380, Loss : 0.37341, Acc : 0.834, Sensitive_Loss : 0.09628, Sensitive_Acc : 15.100, Run Time : 11.38 sec
INFO:root:2024-04-27 15:17:22, Train, Epoch : 3, Step : 1390, Loss : 0.40454, Acc : 0.822, Sensitive_Loss : 0.10047, Sensitive_Acc : 16.800, Run Time : 11.80 sec
INFO:root:2024-04-27 15:17:34, Train, Epoch : 3, Step : 1400, Loss : 0.36440, Acc : 0.844, Sensitive_Loss : 0.05786, Sensitive_Acc : 15.400, Run Time : 11.36 sec
INFO:root:2024-04-27 15:20:08, Dev, Step : 1400, Loss : 0.43073, Acc : 0.815, Auc : 0.899, Sensitive_Loss : 0.13178, Sensitive_Acc : 16.821, Sensitive_Auc : 0.987, Mean auc: 0.899, Run Time : 154.43 sec
INFO:root:2024-04-27 15:20:09, Best, Step : 1400, Loss : 0.43073, Acc : 0.815, Auc : 0.899, Sensitive_Loss : 0.13178, Sensitive_Acc : 16.821, Sensitive_Auc : 0.987, Best Auc : 0.899
INFO:root:2024-04-27 15:20:18, Train, Epoch : 3, Step : 1410, Loss : 0.35229, Acc : 0.841, Sensitive_Loss : 0.09578, Sensitive_Acc : 14.900, Run Time : 164.17 sec
INFO:root:2024-04-27 15:20:30, Train, Epoch : 3, Step : 1420, Loss : 0.36831, Acc : 0.863, Sensitive_Loss : 0.08179, Sensitive_Acc : 16.600, Run Time : 11.70 sec
INFO:root:2024-04-27 15:20:41, Train, Epoch : 3, Step : 1430, Loss : 0.40587, Acc : 0.825, Sensitive_Loss : 0.08848, Sensitive_Acc : 16.400, Run Time : 10.97 sec
INFO:root:2024-04-27 15:20:52, Train, Epoch : 3, Step : 1440, Loss : 0.32037, Acc : 0.866, Sensitive_Loss : 0.08644, Sensitive_Acc : 16.200, Run Time : 11.42 sec
INFO:root:2024-04-27 15:21:03, Train, Epoch : 3, Step : 1450, Loss : 0.39812, Acc : 0.803, Sensitive_Loss : 0.13468, Sensitive_Acc : 15.400, Run Time : 11.24 sec
INFO:root:2024-04-27 15:21:16, Train, Epoch : 3, Step : 1460, Loss : 0.37826, Acc : 0.844, Sensitive_Loss : 0.07345, Sensitive_Acc : 15.300, Run Time : 12.83 sec
INFO:root:2024-04-27 15:21:28, Train, Epoch : 3, Step : 1470, Loss : 0.33991, Acc : 0.875, Sensitive_Loss : 0.10589, Sensitive_Acc : 16.000, Run Time : 11.55 sec
INFO:root:2024-04-27 15:21:39, Train, Epoch : 3, Step : 1480, Loss : 0.40917, Acc : 0.828, Sensitive_Loss : 0.09757, Sensitive_Acc : 17.300, Run Time : 11.23 sec
INFO:root:2024-04-27 15:21:50, Train, Epoch : 3, Step : 1490, Loss : 0.32933, Acc : 0.853, Sensitive_Loss : 0.08734, Sensitive_Acc : 17.100, Run Time : 11.42 sec
INFO:root:2024-04-27 15:22:01, Train, Epoch : 3, Step : 1500, Loss : 0.41095, Acc : 0.809, Sensitive_Loss : 0.12766, Sensitive_Acc : 16.800, Run Time : 10.72 sec
INFO:root:2024-04-27 15:24:36, Dev, Step : 1500, Loss : 0.44116, Acc : 0.810, Auc : 0.902, Sensitive_Loss : 0.12132, Sensitive_Acc : 16.879, Sensitive_Auc : 0.989, Mean auc: 0.902, Run Time : 154.86 sec
INFO:root:2024-04-27 15:24:36, Best, Step : 1500, Loss : 0.44116, Acc : 0.810, Auc : 0.902, Sensitive_Loss : 0.12132, Sensitive_Acc : 16.879, Sensitive_Auc : 0.989, Best Auc : 0.902
INFO:root:2024-04-27 15:24:45, Train, Epoch : 3, Step : 1510, Loss : 0.36755, Acc : 0.812, Sensitive_Loss : 0.11979, Sensitive_Acc : 16.900, Run Time : 164.43 sec
INFO:root:2024-04-27 15:24:57, Train, Epoch : 3, Step : 1520, Loss : 0.41090, Acc : 0.825, Sensitive_Loss : 0.07074, Sensitive_Acc : 16.000, Run Time : 11.95 sec
INFO:root:2024-04-27 15:25:08, Train, Epoch : 3, Step : 1530, Loss : 0.33906, Acc : 0.838, Sensitive_Loss : 0.10667, Sensitive_Acc : 17.000, Run Time : 11.20 sec
INFO:root:2024-04-27 15:25:19, Train, Epoch : 3, Step : 1540, Loss : 0.35989, Acc : 0.834, Sensitive_Loss : 0.09842, Sensitive_Acc : 16.600, Run Time : 10.34 sec
INFO:root:2024-04-27 15:25:30, Train, Epoch : 3, Step : 1550, Loss : 0.39867, Acc : 0.834, Sensitive_Loss : 0.11698, Sensitive_Acc : 17.200, Run Time : 11.07 sec
INFO:root:2024-04-27 15:25:41, Train, Epoch : 3, Step : 1560, Loss : 0.45042, Acc : 0.803, Sensitive_Loss : 0.07659, Sensitive_Acc : 16.500, Run Time : 11.44 sec
INFO:root:2024-04-27 15:25:53, Train, Epoch : 3, Step : 1570, Loss : 0.39822, Acc : 0.816, Sensitive_Loss : 0.07476, Sensitive_Acc : 16.100, Run Time : 12.10 sec
INFO:root:2024-04-27 15:26:05, Train, Epoch : 3, Step : 1580, Loss : 0.40374, Acc : 0.781, Sensitive_Loss : 0.10147, Sensitive_Acc : 16.300, Run Time : 11.15 sec
INFO:root:2024-04-27 15:26:16, Train, Epoch : 3, Step : 1590, Loss : 0.41686, Acc : 0.831, Sensitive_Loss : 0.08790, Sensitive_Acc : 17.200, Run Time : 11.55 sec
INFO:root:2024-04-27 15:26:27, Train, Epoch : 3, Step : 1600, Loss : 0.33062, Acc : 0.822, Sensitive_Loss : 0.10628, Sensitive_Acc : 17.700, Run Time : 10.80 sec
INFO:root:2024-04-27 15:29:02, Dev, Step : 1600, Loss : 0.43385, Acc : 0.813, Auc : 0.900, Sensitive_Loss : 0.12884, Sensitive_Acc : 16.921, Sensitive_Auc : 0.987, Mean auc: 0.900, Run Time : 154.82 sec
INFO:root:2024-04-27 15:29:11, Train, Epoch : 3, Step : 1610, Loss : 0.34729, Acc : 0.850, Sensitive_Loss : 0.10902, Sensitive_Acc : 19.000, Run Time : 163.62 sec
INFO:root:2024-04-27 15:29:22, Train, Epoch : 3, Step : 1620, Loss : 0.34764, Acc : 0.838, Sensitive_Loss : 0.10026, Sensitive_Acc : 16.000, Run Time : 11.57 sec
INFO:root:2024-04-27 15:29:35, Train, Epoch : 3, Step : 1630, Loss : 0.36991, Acc : 0.803, Sensitive_Loss : 0.10034, Sensitive_Acc : 16.300, Run Time : 12.51 sec
INFO:root:2024-04-27 15:29:46, Train, Epoch : 3, Step : 1640, Loss : 0.45549, Acc : 0.794, Sensitive_Loss : 0.08537, Sensitive_Acc : 15.900, Run Time : 11.49 sec
INFO:root:2024-04-27 15:29:57, Train, Epoch : 3, Step : 1650, Loss : 0.35620, Acc : 0.881, Sensitive_Loss : 0.09067, Sensitive_Acc : 17.800, Run Time : 10.57 sec
INFO:root:2024-04-27 15:30:08, Train, Epoch : 3, Step : 1660, Loss : 0.35773, Acc : 0.856, Sensitive_Loss : 0.09984, Sensitive_Acc : 16.600, Run Time : 11.51 sec
INFO:root:2024-04-27 15:30:20, Train, Epoch : 3, Step : 1670, Loss : 0.39577, Acc : 0.838, Sensitive_Loss : 0.10404, Sensitive_Acc : 17.300, Run Time : 11.88 sec
INFO:root:2024-04-27 15:30:31, Train, Epoch : 3, Step : 1680, Loss : 0.39295, Acc : 0.838, Sensitive_Loss : 0.07343, Sensitive_Acc : 16.100, Run Time : 10.85 sec
INFO:root:2024-04-27 15:30:42, Train, Epoch : 3, Step : 1690, Loss : 0.42102, Acc : 0.787, Sensitive_Loss : 0.12088, Sensitive_Acc : 17.000, Run Time : 10.70 sec
INFO:root:2024-04-27 15:30:53, Train, Epoch : 3, Step : 1700, Loss : 0.36042, Acc : 0.844, Sensitive_Loss : 0.07464, Sensitive_Acc : 16.800, Run Time : 11.80 sec
INFO:root:2024-04-27 15:33:28, Dev, Step : 1700, Loss : 0.41335, Acc : 0.819, Auc : 0.904, Sensitive_Loss : 0.10843, Sensitive_Acc : 16.921, Sensitive_Auc : 0.989, Mean auc: 0.904, Run Time : 154.41 sec
INFO:root:2024-04-27 15:33:28, Best, Step : 1700, Loss : 0.41335, Acc : 0.819, Auc : 0.904, Sensitive_Loss : 0.10843, Sensitive_Acc : 16.921, Sensitive_Auc : 0.989, Best Auc : 0.904
INFO:root:2024-04-27 15:33:38, Train, Epoch : 3, Step : 1710, Loss : 0.41651, Acc : 0.863, Sensitive_Loss : 0.09034, Sensitive_Acc : 15.900, Run Time : 164.12 sec
INFO:root:2024-04-27 15:33:49, Train, Epoch : 3, Step : 1720, Loss : 0.31665, Acc : 0.869, Sensitive_Loss : 0.06955, Sensitive_Acc : 16.200, Run Time : 11.39 sec
INFO:root:2024-04-27 15:34:00, Train, Epoch : 3, Step : 1730, Loss : 0.35610, Acc : 0.806, Sensitive_Loss : 0.07176, Sensitive_Acc : 17.800, Run Time : 11.47 sec
INFO:root:2024-04-27 15:34:11, Train, Epoch : 3, Step : 1740, Loss : 0.38655, Acc : 0.838, Sensitive_Loss : 0.10928, Sensitive_Acc : 16.600, Run Time : 11.00 sec
INFO:root:2024-04-27 15:34:23, Train, Epoch : 3, Step : 1750, Loss : 0.35137, Acc : 0.869, Sensitive_Loss : 0.08656, Sensitive_Acc : 17.000, Run Time : 11.52 sec
INFO:root:2024-04-27 15:34:35, Train, Epoch : 3, Step : 1760, Loss : 0.35795, Acc : 0.847, Sensitive_Loss : 0.11579, Sensitive_Acc : 15.800, Run Time : 11.96 sec
INFO:root:2024-04-27 15:34:45, Train, Epoch : 3, Step : 1770, Loss : 0.34950, Acc : 0.841, Sensitive_Loss : 0.06953, Sensitive_Acc : 15.600, Run Time : 10.26 sec
INFO:root:2024-04-27 15:34:57, Train, Epoch : 3, Step : 1780, Loss : 0.39218, Acc : 0.825, Sensitive_Loss : 0.09545, Sensitive_Acc : 15.800, Run Time : 11.95 sec
INFO:root:2024-04-27 15:35:08, Train, Epoch : 3, Step : 1790, Loss : 0.35472, Acc : 0.838, Sensitive_Loss : 0.09148, Sensitive_Acc : 16.700, Run Time : 11.03 sec
INFO:root:2024-04-27 15:35:20, Train, Epoch : 3, Step : 1800, Loss : 0.35942, Acc : 0.841, Sensitive_Loss : 0.08147, Sensitive_Acc : 15.800, Run Time : 12.24 sec
INFO:root:2024-04-27 15:37:55, Dev, Step : 1800, Loss : 0.42783, Acc : 0.812, Auc : 0.903, Sensitive_Loss : 0.10988, Sensitive_Acc : 16.893, Sensitive_Auc : 0.991, Mean auc: 0.903, Run Time : 154.18 sec
INFO:root:2024-04-27 15:38:03, Train, Epoch : 3, Step : 1810, Loss : 0.43541, Acc : 0.794, Sensitive_Loss : 0.06053, Sensitive_Acc : 15.500, Run Time : 162.78 sec
INFO:root:2024-04-27 15:38:15, Train, Epoch : 3, Step : 1820, Loss : 0.32033, Acc : 0.872, Sensitive_Loss : 0.10190, Sensitive_Acc : 16.500, Run Time : 11.43 sec
INFO:root:2024-04-27 15:38:27, Train, Epoch : 3, Step : 1830, Loss : 0.40152, Acc : 0.841, Sensitive_Loss : 0.11766, Sensitive_Acc : 16.100, Run Time : 12.21 sec
INFO:root:2024-04-27 15:38:38, Train, Epoch : 3, Step : 1840, Loss : 0.40175, Acc : 0.806, Sensitive_Loss : 0.08193, Sensitive_Acc : 15.300, Run Time : 11.25 sec
INFO:root:2024-04-27 15:38:50, Train, Epoch : 3, Step : 1850, Loss : 0.38200, Acc : 0.831, Sensitive_Loss : 0.07092, Sensitive_Acc : 16.400, Run Time : 11.46 sec
INFO:root:2024-04-27 15:39:00, Train, Epoch : 3, Step : 1860, Loss : 0.29818, Acc : 0.878, Sensitive_Loss : 0.07824, Sensitive_Acc : 15.600, Run Time : 10.79 sec
INFO:root:2024-04-27 15:39:12, Train, Epoch : 3, Step : 1870, Loss : 0.35100, Acc : 0.866, Sensitive_Loss : 0.09619, Sensitive_Acc : 16.100, Run Time : 11.30 sec
INFO:root:2024-04-27 15:42:02
INFO:root:y_pred: [0.06109224 0.8888876  0.02462903 ... 0.8521045  0.01415266 0.81897163]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.8668265e-01 1.6235694e-05 3.7013488e-03 2.8864401e-06 9.9452895e-01
 5.9854751e-06 9.9890029e-01 9.7141808e-01 5.3721371e-05 9.7239017e-01
 9.9847931e-01 9.9852741e-01 9.9743712e-01 7.6480699e-01 1.5044730e-03
 9.6378660e-01 9.9820185e-01 3.5671244e-04 3.2870898e-01 8.3838618e-01
 9.8256874e-01 2.3312210e-03 9.9905330e-01 8.9775306e-01 9.7113842e-01
 8.4497178e-01 6.6284556e-04 9.8687232e-01 9.7702438e-01 4.2001665e-01
 2.9478170e-04 5.3986140e-02 7.0303218e-03 1.7555560e-03 9.5868848e-02
 4.8623039e-04 7.3915338e-03 1.9356203e-03 9.7934514e-01 9.7150248e-01
 1.5409000e-04 2.8072248e-04 9.8495007e-01 4.0615061e-05 9.9918824e-01
 9.9525732e-01 9.7766751e-01 9.9265397e-01 1.1759796e-02 9.8088008e-01
 9.9685448e-01 1.8760323e-04 1.2678642e-02 7.2071845e-05 2.6116890e-04
 4.2395634e-03 2.1941680e-03 5.1626187e-02 3.8314986e-04 2.3682103e-01
 9.3122327e-04 4.4303030e-02 4.0589925e-04 9.8078513e-01 1.1977801e-02
 9.9482900e-01 5.3734677e-03 9.9551880e-01 8.4411025e-01 6.2407124e-01
 8.5602564e-01 1.6787522e-01 2.0059427e-04 2.6226980e-03 6.3925261e-05
 1.0876688e-04 8.2234956e-02 1.0268340e-01 2.6681913e-05 9.7788268e-01
 9.9746692e-01 1.6446906e-04 7.3460519e-02 1.3573612e-03 4.0011758e-01
 7.9044634e-01 1.7592089e-02 6.7802593e-03 9.7484404e-01 9.9513310e-01
 9.9411494e-01 2.0345449e-02 1.5356733e-03 9.9674785e-01 6.1945301e-01
 9.8351178e-05 9.9391621e-01 9.9380010e-01 8.5304837e-06 5.5974978e-03
 9.7609597e-01 9.3409330e-01 9.9746168e-01 9.9188411e-01 3.0451803e-04
 2.2353079e-02 7.5333202e-01 8.3548629e-01 9.5894068e-01 8.0433718e-05
 6.6364968e-01 9.9857104e-01 4.9601786e-02 9.9827862e-01 9.1140705e-01
 9.6072072e-01 5.6514257e-01 9.8367727e-01 1.6534418e-01 9.4319218e-01
 9.9602771e-01 9.9002469e-01 8.9499672e-06 8.8568372e-01 9.9474585e-01
 6.5415718e-02 9.8580450e-01 3.0117664e-03 1.4149393e-02 9.8383242e-01
 9.9235278e-01 1.2620511e-03 6.5355981e-04 3.5061254e-04 9.9217504e-01
 9.9778301e-01 8.8990438e-01 7.3357671e-04 8.3132228e-04 9.0119016e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 15:42:02, Dev, Step : 1878, Loss : 0.41096, Acc : 0.824, Auc : 0.905, Sensitive_Loss : 0.10452, Sensitive_Acc : 16.921, Sensitive_Auc : 0.992, Mean auc: 0.905, Run Time : 161.81 sec
INFO:root:2024-04-27 15:42:02, Best, Step : 1878, Loss : 0.41096, Acc : 0.824,Auc : 0.905, Best Auc : 0.905, Sensitive_Loss : 0.10452, Sensitive_Acc : 16.921, Sensitive_Auc : 0.992
INFO:root:2024-04-27 15:42:08, Train, Epoch : 4, Step : 1880, Loss : 0.11019, Acc : 0.150, Sensitive_Loss : 0.00750, Sensitive_Acc : 2.700, Run Time : 5.06 sec
INFO:root:2024-04-27 15:42:19, Train, Epoch : 4, Step : 1890, Loss : 0.34420, Acc : 0.859, Sensitive_Loss : 0.08452, Sensitive_Acc : 15.300, Run Time : 10.73 sec
INFO:root:2024-04-27 15:42:30, Train, Epoch : 4, Step : 1900, Loss : 0.33784, Acc : 0.847, Sensitive_Loss : 0.09823, Sensitive_Acc : 16.700, Run Time : 10.95 sec
INFO:root:2024-04-27 15:45:07, Dev, Step : 1900, Loss : 0.40338, Acc : 0.831, Auc : 0.905, Sensitive_Loss : 0.10154, Sensitive_Acc : 16.921, Sensitive_Auc : 0.992, Mean auc: 0.905, Run Time : 157.37 sec
INFO:root:2024-04-27 15:45:16, Train, Epoch : 4, Step : 1910, Loss : 0.34786, Acc : 0.859, Sensitive_Loss : 0.08991, Sensitive_Acc : 15.900, Run Time : 166.45 sec
INFO:root:2024-04-27 15:45:30, Train, Epoch : 4, Step : 1920, Loss : 0.34057, Acc : 0.819, Sensitive_Loss : 0.08554, Sensitive_Acc : 15.300, Run Time : 13.22 sec
INFO:root:2024-04-27 15:45:41, Train, Epoch : 4, Step : 1930, Loss : 0.36471, Acc : 0.844, Sensitive_Loss : 0.07552, Sensitive_Acc : 17.000, Run Time : 11.51 sec
INFO:root:2024-04-27 15:45:52, Train, Epoch : 4, Step : 1940, Loss : 0.33100, Acc : 0.838, Sensitive_Loss : 0.07801, Sensitive_Acc : 15.700, Run Time : 10.99 sec
INFO:root:2024-04-27 15:46:04, Train, Epoch : 4, Step : 1950, Loss : 0.30440, Acc : 0.884, Sensitive_Loss : 0.07260, Sensitive_Acc : 17.500, Run Time : 12.02 sec
INFO:root:2024-04-27 15:46:15, Train, Epoch : 4, Step : 1960, Loss : 0.37095, Acc : 0.859, Sensitive_Loss : 0.08669, Sensitive_Acc : 14.800, Run Time : 11.11 sec
INFO:root:2024-04-27 15:46:26, Train, Epoch : 4, Step : 1970, Loss : 0.34783, Acc : 0.841, Sensitive_Loss : 0.08461, Sensitive_Acc : 16.500, Run Time : 10.66 sec
INFO:root:2024-04-27 15:46:38, Train, Epoch : 4, Step : 1980, Loss : 0.36787, Acc : 0.834, Sensitive_Loss : 0.08079, Sensitive_Acc : 15.600, Run Time : 11.53 sec
INFO:root:2024-04-27 15:46:49, Train, Epoch : 4, Step : 1990, Loss : 0.35007, Acc : 0.859, Sensitive_Loss : 0.07621, Sensitive_Acc : 16.400, Run Time : 11.26 sec
INFO:root:2024-04-27 15:47:00, Train, Epoch : 4, Step : 2000, Loss : 0.39976, Acc : 0.828, Sensitive_Loss : 0.07240, Sensitive_Acc : 14.400, Run Time : 11.64 sec
INFO:root:2024-04-27 15:49:47, Dev, Step : 2000, Loss : 0.41474, Acc : 0.824, Auc : 0.906, Sensitive_Loss : 0.12253, Sensitive_Acc : 16.879, Sensitive_Auc : 0.988, Mean auc: 0.906, Run Time : 166.55 sec
INFO:root:2024-04-27 15:49:48, Best, Step : 2000, Loss : 0.41474, Acc : 0.824, Auc : 0.906, Sensitive_Loss : 0.12253, Sensitive_Acc : 16.879, Sensitive_Auc : 0.988, Best Auc : 0.906
INFO:root:2024-04-27 15:49:59, Train, Epoch : 4, Step : 2010, Loss : 0.37438, Acc : 0.847, Sensitive_Loss : 0.08470, Sensitive_Acc : 17.700, Run Time : 178.07 sec
INFO:root:2024-04-27 15:50:10, Train, Epoch : 4, Step : 2020, Loss : 0.33896, Acc : 0.875, Sensitive_Loss : 0.11516, Sensitive_Acc : 15.900, Run Time : 11.46 sec
INFO:root:2024-04-27 15:50:24, Train, Epoch : 4, Step : 2030, Loss : 0.32682, Acc : 0.853, Sensitive_Loss : 0.10309, Sensitive_Acc : 16.700, Run Time : 13.78 sec
INFO:root:2024-04-27 15:50:36, Train, Epoch : 4, Step : 2040, Loss : 0.32135, Acc : 0.847, Sensitive_Loss : 0.08193, Sensitive_Acc : 15.300, Run Time : 12.39 sec
INFO:root:2024-04-27 15:50:49, Train, Epoch : 4, Step : 2050, Loss : 0.36247, Acc : 0.834, Sensitive_Loss : 0.11787, Sensitive_Acc : 16.100, Run Time : 13.33 sec
INFO:root:2024-04-27 15:51:01, Train, Epoch : 4, Step : 2060, Loss : 0.33937, Acc : 0.847, Sensitive_Loss : 0.10445, Sensitive_Acc : 18.000, Run Time : 11.85 sec
INFO:root:2024-04-27 15:51:12, Train, Epoch : 4, Step : 2070, Loss : 0.31543, Acc : 0.841, Sensitive_Loss : 0.09532, Sensitive_Acc : 16.500, Run Time : 10.69 sec
INFO:root:2024-04-27 15:51:24, Train, Epoch : 4, Step : 2080, Loss : 0.39336, Acc : 0.831, Sensitive_Loss : 0.07025, Sensitive_Acc : 16.400, Run Time : 11.72 sec
INFO:root:2024-04-27 15:51:36, Train, Epoch : 4, Step : 2090, Loss : 0.37392, Acc : 0.828, Sensitive_Loss : 0.08891, Sensitive_Acc : 16.300, Run Time : 11.86 sec
INFO:root:2024-04-27 15:51:47, Train, Epoch : 4, Step : 2100, Loss : 0.40210, Acc : 0.822, Sensitive_Loss : 0.14141, Sensitive_Acc : 17.200, Run Time : 11.29 sec
INFO:root:2024-04-27 15:54:29, Dev, Step : 2100, Loss : 0.41025, Acc : 0.820, Auc : 0.905, Sensitive_Loss : 0.11235, Sensitive_Acc : 16.879, Sensitive_Auc : 0.989, Mean auc: 0.905, Run Time : 161.98 sec
INFO:root:2024-04-27 15:54:39, Train, Epoch : 4, Step : 2110, Loss : 0.38579, Acc : 0.834, Sensitive_Loss : 0.12859, Sensitive_Acc : 16.000, Run Time : 172.45 sec
INFO:root:2024-04-27 15:54:52, Train, Epoch : 4, Step : 2120, Loss : 0.33707, Acc : 0.847, Sensitive_Loss : 0.08473, Sensitive_Acc : 16.100, Run Time : 13.06 sec
INFO:root:2024-04-27 15:55:05, Train, Epoch : 4, Step : 2130, Loss : 0.36662, Acc : 0.825, Sensitive_Loss : 0.10407, Sensitive_Acc : 16.400, Run Time : 12.92 sec
INFO:root:2024-04-27 15:55:20, Train, Epoch : 4, Step : 2140, Loss : 0.34610, Acc : 0.847, Sensitive_Loss : 0.12933, Sensitive_Acc : 16.000, Run Time : 14.36 sec
INFO:root:2024-04-27 15:55:31, Train, Epoch : 4, Step : 2150, Loss : 0.39833, Acc : 0.831, Sensitive_Loss : 0.07844, Sensitive_Acc : 16.800, Run Time : 11.28 sec
INFO:root:2024-04-27 15:55:43, Train, Epoch : 4, Step : 2160, Loss : 0.33519, Acc : 0.859, Sensitive_Loss : 0.10448, Sensitive_Acc : 14.800, Run Time : 12.24 sec
INFO:root:2024-04-27 15:55:56, Train, Epoch : 4, Step : 2170, Loss : 0.33585, Acc : 0.838, Sensitive_Loss : 0.08325, Sensitive_Acc : 17.900, Run Time : 12.69 sec
INFO:root:2024-04-27 15:56:07, Train, Epoch : 4, Step : 2180, Loss : 0.35986, Acc : 0.822, Sensitive_Loss : 0.12421, Sensitive_Acc : 16.100, Run Time : 10.97 sec
INFO:root:2024-04-27 15:56:23, Train, Epoch : 4, Step : 2190, Loss : 0.35095, Acc : 0.847, Sensitive_Loss : 0.05458, Sensitive_Acc : 16.800, Run Time : 15.88 sec
INFO:root:2024-04-27 15:56:38, Train, Epoch : 4, Step : 2200, Loss : 0.34245, Acc : 0.841, Sensitive_Loss : 0.08614, Sensitive_Acc : 15.400, Run Time : 15.13 sec
INFO:root:2024-04-27 15:59:47, Dev, Step : 2200, Loss : 0.41825, Acc : 0.823, Auc : 0.903, Sensitive_Loss : 0.12155, Sensitive_Acc : 16.779, Sensitive_Auc : 0.984, Mean auc: 0.903, Run Time : 189.18 sec
INFO:root:2024-04-27 15:59:56, Train, Epoch : 4, Step : 2210, Loss : 0.32135, Acc : 0.869, Sensitive_Loss : 0.10781, Sensitive_Acc : 16.500, Run Time : 198.17 sec
INFO:root:2024-04-27 16:00:07, Train, Epoch : 4, Step : 2220, Loss : 0.40803, Acc : 0.828, Sensitive_Loss : 0.06016, Sensitive_Acc : 16.400, Run Time : 11.39 sec
INFO:root:2024-04-27 16:00:19, Train, Epoch : 4, Step : 2230, Loss : 0.43836, Acc : 0.800, Sensitive_Loss : 0.08269, Sensitive_Acc : 16.300, Run Time : 11.15 sec
INFO:root:2024-04-27 16:00:30, Train, Epoch : 4, Step : 2240, Loss : 0.33841, Acc : 0.878, Sensitive_Loss : 0.09392, Sensitive_Acc : 16.800, Run Time : 11.27 sec
INFO:root:2024-04-27 16:00:42, Train, Epoch : 4, Step : 2250, Loss : 0.33801, Acc : 0.869, Sensitive_Loss : 0.06905, Sensitive_Acc : 15.800, Run Time : 11.68 sec
INFO:root:2024-04-27 16:00:52, Train, Epoch : 4, Step : 2260, Loss : 0.33205, Acc : 0.850, Sensitive_Loss : 0.07527, Sensitive_Acc : 15.300, Run Time : 10.82 sec
INFO:root:2024-04-27 16:01:04, Train, Epoch : 4, Step : 2270, Loss : 0.33886, Acc : 0.825, Sensitive_Loss : 0.09511, Sensitive_Acc : 15.600, Run Time : 12.01 sec
INFO:root:2024-04-27 16:01:17, Train, Epoch : 4, Step : 2280, Loss : 0.41509, Acc : 0.809, Sensitive_Loss : 0.06864, Sensitive_Acc : 16.200, Run Time : 12.41 sec
INFO:root:2024-04-27 16:01:27, Train, Epoch : 4, Step : 2290, Loss : 0.33013, Acc : 0.841, Sensitive_Loss : 0.08393, Sensitive_Acc : 15.300, Run Time : 10.13 sec
INFO:root:2024-04-27 16:01:38, Train, Epoch : 4, Step : 2300, Loss : 0.35860, Acc : 0.825, Sensitive_Loss : 0.08617, Sensitive_Acc : 14.900, Run Time : 11.50 sec
INFO:root:2024-04-27 16:04:14, Dev, Step : 2300, Loss : 0.42210, Acc : 0.817, Auc : 0.902, Sensitive_Loss : 0.11989, Sensitive_Acc : 16.850, Sensitive_Auc : 0.987, Mean auc: 0.902, Run Time : 155.79 sec
INFO:root:2024-04-27 16:04:23, Train, Epoch : 4, Step : 2310, Loss : 0.34289, Acc : 0.863, Sensitive_Loss : 0.07615, Sensitive_Acc : 16.000, Run Time : 164.60 sec
INFO:root:2024-04-27 16:04:34, Train, Epoch : 4, Step : 2320, Loss : 0.36297, Acc : 0.828, Sensitive_Loss : 0.12396, Sensitive_Acc : 16.500, Run Time : 11.07 sec
INFO:root:2024-04-27 16:04:46, Train, Epoch : 4, Step : 2330, Loss : 0.38751, Acc : 0.838, Sensitive_Loss : 0.11498, Sensitive_Acc : 16.800, Run Time : 11.47 sec
INFO:root:2024-04-27 16:04:57, Train, Epoch : 4, Step : 2340, Loss : 0.34915, Acc : 0.859, Sensitive_Loss : 0.08551, Sensitive_Acc : 15.600, Run Time : 11.45 sec
INFO:root:2024-04-27 16:05:08, Train, Epoch : 4, Step : 2350, Loss : 0.37119, Acc : 0.850, Sensitive_Loss : 0.07154, Sensitive_Acc : 16.700, Run Time : 11.29 sec
INFO:root:2024-04-27 16:05:20, Train, Epoch : 4, Step : 2360, Loss : 0.33375, Acc : 0.841, Sensitive_Loss : 0.10735, Sensitive_Acc : 16.300, Run Time : 11.60 sec
INFO:root:2024-04-27 16:05:32, Train, Epoch : 4, Step : 2370, Loss : 0.36584, Acc : 0.825, Sensitive_Loss : 0.08993, Sensitive_Acc : 15.900, Run Time : 11.63 sec
INFO:root:2024-04-27 16:05:43, Train, Epoch : 4, Step : 2380, Loss : 0.34902, Acc : 0.863, Sensitive_Loss : 0.07892, Sensitive_Acc : 16.100, Run Time : 11.33 sec
INFO:root:2024-04-27 16:05:54, Train, Epoch : 4, Step : 2390, Loss : 0.34651, Acc : 0.850, Sensitive_Loss : 0.12095, Sensitive_Acc : 17.200, Run Time : 10.65 sec
INFO:root:2024-04-27 16:06:06, Train, Epoch : 4, Step : 2400, Loss : 0.30096, Acc : 0.866, Sensitive_Loss : 0.07576, Sensitive_Acc : 16.600, Run Time : 12.11 sec
INFO:root:2024-04-27 16:08:40, Dev, Step : 2400, Loss : 0.42476, Acc : 0.822, Auc : 0.902, Sensitive_Loss : 0.12783, Sensitive_Acc : 16.907, Sensitive_Auc : 0.985, Mean auc: 0.902, Run Time : 154.41 sec
INFO:root:2024-04-27 16:08:49, Train, Epoch : 4, Step : 2410, Loss : 0.42646, Acc : 0.809, Sensitive_Loss : 0.06249, Sensitive_Acc : 18.100, Run Time : 163.30 sec
INFO:root:2024-04-27 16:09:01, Train, Epoch : 4, Step : 2420, Loss : 0.32387, Acc : 0.859, Sensitive_Loss : 0.09222, Sensitive_Acc : 15.500, Run Time : 11.63 sec
INFO:root:2024-04-27 16:09:11, Train, Epoch : 4, Step : 2430, Loss : 0.35177, Acc : 0.844, Sensitive_Loss : 0.07787, Sensitive_Acc : 17.100, Run Time : 10.91 sec
INFO:root:2024-04-27 16:09:23, Train, Epoch : 4, Step : 2440, Loss : 0.36305, Acc : 0.838, Sensitive_Loss : 0.07722, Sensitive_Acc : 17.400, Run Time : 11.33 sec
INFO:root:2024-04-27 16:09:35, Train, Epoch : 4, Step : 2450, Loss : 0.30701, Acc : 0.841, Sensitive_Loss : 0.07788, Sensitive_Acc : 16.700, Run Time : 11.89 sec
INFO:root:2024-04-27 16:09:47, Train, Epoch : 4, Step : 2460, Loss : 0.38621, Acc : 0.831, Sensitive_Loss : 0.08317, Sensitive_Acc : 16.000, Run Time : 12.25 sec
INFO:root:2024-04-27 16:09:58, Train, Epoch : 4, Step : 2470, Loss : 0.27417, Acc : 0.875, Sensitive_Loss : 0.06320, Sensitive_Acc : 15.300, Run Time : 10.56 sec
INFO:root:2024-04-27 16:10:09, Train, Epoch : 4, Step : 2480, Loss : 0.39256, Acc : 0.816, Sensitive_Loss : 0.09538, Sensitive_Acc : 15.900, Run Time : 11.42 sec
INFO:root:2024-04-27 16:10:21, Train, Epoch : 4, Step : 2490, Loss : 0.37804, Acc : 0.816, Sensitive_Loss : 0.06526, Sensitive_Acc : 17.200, Run Time : 11.61 sec
INFO:root:2024-04-27 16:10:32, Train, Epoch : 4, Step : 2500, Loss : 0.42573, Acc : 0.834, Sensitive_Loss : 0.11395, Sensitive_Acc : 17.600, Run Time : 11.73 sec
INFO:root:2024-04-27 16:13:07, Dev, Step : 2500, Loss : 0.43230, Acc : 0.813, Auc : 0.904, Sensitive_Loss : 0.12241, Sensitive_Acc : 16.850, Sensitive_Auc : 0.985, Mean auc: 0.904, Run Time : 155.07 sec
INFO:root:2024-04-27 16:15:42
INFO:root:y_pred: [0.11191382 0.8629398  0.02432956 ... 0.80797917 0.02239962 0.8712289 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.83202934e-01 1.04798737e-05 5.69457537e-04 5.98944439e-07
 9.92455363e-01 1.55682585e-06 9.98869836e-01 9.71816599e-01
 2.62596186e-05 9.84258413e-01 9.98528481e-01 9.98866677e-01
 9.98213649e-01 8.15219104e-01 7.09490501e-04 9.72163439e-01
 9.99175370e-01 2.07490404e-04 3.14692050e-01 8.71342719e-01
 9.89291489e-01 4.84915599e-02 9.99348581e-01 9.09654677e-01
 9.80234206e-01 8.21271241e-01 1.11872941e-04 9.91501331e-01
 9.80783880e-01 4.98945564e-01 6.20031133e-05 8.57535824e-02
 6.59940904e-03 9.05565103e-04 1.79374397e-01 1.05244748e-04
 2.92813336e-03 1.44354664e-02 9.73702550e-01 9.78174925e-01
 1.88484373e-05 1.78431146e-04 9.83288467e-01 3.49849288e-05
 9.99293566e-01 9.94922936e-01 9.75681961e-01 9.98582125e-01
 1.13678435e-02 9.88530695e-01 9.96925175e-01 1.51960558e-04
 1.78810079e-02 2.51866968e-05 1.82406424e-04 9.91872605e-03
 1.24132726e-03 2.85133690e-01 4.67087753e-04 1.15272820e-01
 8.64781206e-04 1.66932642e-02 4.77191352e-04 9.94448543e-01
 3.27325128e-02 9.95181382e-01 4.51744272e-04 9.94720101e-01
 8.14724743e-01 9.22635555e-01 9.40911472e-01 8.13302696e-02
 1.68196973e-04 9.99971176e-04 3.42202911e-05 5.48276948e-05
 6.70403361e-01 8.73173699e-02 9.04476383e-06 9.75650311e-01
 9.97772634e-01 4.74073422e-05 1.60018876e-01 2.07252539e-04
 4.10987794e-01 9.09241676e-01 1.29680913e-02 2.02735839e-03
 9.71770406e-01 9.94888723e-01 9.93839920e-01 4.16820534e-02
 3.11994430e-04 9.97386515e-01 6.84640586e-01 2.34058025e-05
 9.95318115e-01 9.94525790e-01 9.90694502e-07 7.04705622e-03
 9.73183274e-01 9.52496588e-01 9.98935997e-01 9.90849733e-01
 1.13872797e-04 6.34332327e-03 7.17089176e-01 8.31184983e-01
 9.67314005e-01 8.65288621e-06 6.46115780e-01 9.98828351e-01
 1.02213025e-01 9.98195708e-01 9.44085896e-01 9.61467862e-01
 5.85469306e-01 9.87364233e-01 1.43910572e-01 9.67154860e-01
 9.96479452e-01 9.91048396e-01 1.49208915e-06 8.45349371e-01
 9.94440496e-01 4.73292684e-03 9.90063965e-01 1.14242884e-03
 1.17114671e-02 9.89319384e-01 9.93864298e-01 4.99251590e-04
 1.60832028e-03 7.99539790e-04 9.90981042e-01 9.98588622e-01
 9.37865138e-01 4.70185943e-04 6.00739615e-04 9.36025083e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 16:15:42, Dev, Step : 2504, Loss : 0.41858, Acc : 0.819, Auc : 0.905, Sensitive_Loss : 0.11238, Sensitive_Acc : 16.864, Sensitive_Auc : 0.987, Mean auc: 0.905, Run Time : 153.05 sec
INFO:root:2024-04-27 16:15:52, Train, Epoch : 5, Step : 2510, Loss : 0.23366, Acc : 0.512, Sensitive_Loss : 0.07039, Sensitive_Acc : 9.400, Run Time : 8.82 sec
INFO:root:2024-04-27 16:16:05, Train, Epoch : 5, Step : 2520, Loss : 0.32962, Acc : 0.872, Sensitive_Loss : 0.09033, Sensitive_Acc : 16.400, Run Time : 12.56 sec
INFO:root:2024-04-27 16:16:16, Train, Epoch : 5, Step : 2530, Loss : 0.33524, Acc : 0.853, Sensitive_Loss : 0.08980, Sensitive_Acc : 18.100, Run Time : 11.27 sec
INFO:root:2024-04-27 16:16:26, Train, Epoch : 5, Step : 2540, Loss : 0.33824, Acc : 0.825, Sensitive_Loss : 0.08468, Sensitive_Acc : 16.700, Run Time : 10.23 sec
INFO:root:2024-04-27 16:16:39, Train, Epoch : 5, Step : 2550, Loss : 0.36410, Acc : 0.841, Sensitive_Loss : 0.06804, Sensitive_Acc : 17.600, Run Time : 12.40 sec
INFO:root:2024-04-27 16:16:50, Train, Epoch : 5, Step : 2560, Loss : 0.29440, Acc : 0.891, Sensitive_Loss : 0.09732, Sensitive_Acc : 15.800, Run Time : 11.02 sec
INFO:root:2024-04-27 16:17:02, Train, Epoch : 5, Step : 2570, Loss : 0.35217, Acc : 0.859, Sensitive_Loss : 0.08362, Sensitive_Acc : 15.000, Run Time : 12.07 sec
INFO:root:2024-04-27 16:17:13, Train, Epoch : 5, Step : 2580, Loss : 0.37573, Acc : 0.847, Sensitive_Loss : 0.09592, Sensitive_Acc : 14.500, Run Time : 11.59 sec
INFO:root:2024-04-27 16:17:24, Train, Epoch : 5, Step : 2590, Loss : 0.32082, Acc : 0.844, Sensitive_Loss : 0.06502, Sensitive_Acc : 15.100, Run Time : 10.78 sec
INFO:root:2024-04-27 16:17:36, Train, Epoch : 5, Step : 2600, Loss : 0.34372, Acc : 0.856, Sensitive_Loss : 0.08084, Sensitive_Acc : 16.800, Run Time : 12.01 sec
INFO:root:2024-04-27 16:20:11, Dev, Step : 2600, Loss : 0.43510, Acc : 0.812, Auc : 0.903, Sensitive_Loss : 0.12972, Sensitive_Acc : 16.864, Sensitive_Auc : 0.986, Mean auc: 0.903, Run Time : 154.56 sec
INFO:root:2024-04-27 16:20:20, Train, Epoch : 5, Step : 2610, Loss : 0.31539, Acc : 0.859, Sensitive_Loss : 0.09867, Sensitive_Acc : 16.100, Run Time : 163.32 sec
INFO:root:2024-04-27 16:20:31, Train, Epoch : 5, Step : 2620, Loss : 0.35005, Acc : 0.834, Sensitive_Loss : 0.17378, Sensitive_Acc : 15.600, Run Time : 11.89 sec
INFO:root:2024-04-27 16:20:43, Train, Epoch : 5, Step : 2630, Loss : 0.33933, Acc : 0.850, Sensitive_Loss : 0.11770, Sensitive_Acc : 16.200, Run Time : 12.00 sec
INFO:root:2024-04-27 16:20:54, Train, Epoch : 5, Step : 2640, Loss : 0.34537, Acc : 0.844, Sensitive_Loss : 0.05687, Sensitive_Acc : 15.600, Run Time : 10.86 sec
INFO:root:2024-04-27 16:21:06, Train, Epoch : 5, Step : 2650, Loss : 0.33321, Acc : 0.838, Sensitive_Loss : 0.07545, Sensitive_Acc : 15.300, Run Time : 11.49 sec
INFO:root:2024-04-27 16:21:18, Train, Epoch : 5, Step : 2660, Loss : 0.34541, Acc : 0.853, Sensitive_Loss : 0.05977, Sensitive_Acc : 15.800, Run Time : 11.94 sec
INFO:root:2024-04-27 16:21:29, Train, Epoch : 5, Step : 2670, Loss : 0.33609, Acc : 0.844, Sensitive_Loss : 0.09154, Sensitive_Acc : 17.300, Run Time : 11.06 sec
INFO:root:2024-04-27 16:21:39, Train, Epoch : 5, Step : 2680, Loss : 0.30783, Acc : 0.878, Sensitive_Loss : 0.07831, Sensitive_Acc : 15.900, Run Time : 10.40 sec
INFO:root:2024-04-27 16:21:51, Train, Epoch : 5, Step : 2690, Loss : 0.35096, Acc : 0.847, Sensitive_Loss : 0.08971, Sensitive_Acc : 16.700, Run Time : 11.81 sec
INFO:root:2024-04-27 16:22:02, Train, Epoch : 5, Step : 2700, Loss : 0.30359, Acc : 0.866, Sensitive_Loss : 0.08021, Sensitive_Acc : 16.400, Run Time : 10.72 sec
INFO:root:2024-04-27 16:24:37, Dev, Step : 2700, Loss : 0.41429, Acc : 0.824, Auc : 0.902, Sensitive_Loss : 0.11700, Sensitive_Acc : 16.921, Sensitive_Auc : 0.988, Mean auc: 0.902, Run Time : 155.13 sec
INFO:root:2024-04-27 16:24:46, Train, Epoch : 5, Step : 2710, Loss : 0.38734, Acc : 0.850, Sensitive_Loss : 0.09006, Sensitive_Acc : 17.600, Run Time : 164.01 sec
INFO:root:2024-04-27 16:24:57, Train, Epoch : 5, Step : 2720, Loss : 0.40150, Acc : 0.822, Sensitive_Loss : 0.14101, Sensitive_Acc : 16.700, Run Time : 10.86 sec
INFO:root:2024-04-27 16:25:08, Train, Epoch : 5, Step : 2730, Loss : 0.35745, Acc : 0.841, Sensitive_Loss : 0.11383, Sensitive_Acc : 16.300, Run Time : 11.46 sec
INFO:root:2024-04-27 16:25:20, Train, Epoch : 5, Step : 2740, Loss : 0.28666, Acc : 0.891, Sensitive_Loss : 0.06827, Sensitive_Acc : 15.300, Run Time : 11.53 sec
INFO:root:2024-04-27 16:25:31, Train, Epoch : 5, Step : 2750, Loss : 0.27437, Acc : 0.875, Sensitive_Loss : 0.07210, Sensitive_Acc : 16.000, Run Time : 11.64 sec
INFO:root:2024-04-27 16:25:42, Train, Epoch : 5, Step : 2760, Loss : 0.30729, Acc : 0.866, Sensitive_Loss : 0.05548, Sensitive_Acc : 15.400, Run Time : 10.76 sec
INFO:root:2024-04-27 16:25:54, Train, Epoch : 5, Step : 2770, Loss : 0.34721, Acc : 0.850, Sensitive_Loss : 0.07277, Sensitive_Acc : 16.600, Run Time : 12.26 sec
INFO:root:2024-04-27 16:26:05, Train, Epoch : 5, Step : 2780, Loss : 0.36307, Acc : 0.847, Sensitive_Loss : 0.08428, Sensitive_Acc : 16.500, Run Time : 10.91 sec
INFO:root:2024-04-27 16:26:16, Train, Epoch : 5, Step : 2790, Loss : 0.31087, Acc : 0.869, Sensitive_Loss : 0.08212, Sensitive_Acc : 16.200, Run Time : 11.28 sec
INFO:root:2024-04-27 16:26:31, Train, Epoch : 5, Step : 2800, Loss : 0.38701, Acc : 0.834, Sensitive_Loss : 0.08849, Sensitive_Acc : 17.500, Run Time : 14.77 sec
INFO:root:2024-04-27 16:29:55, Dev, Step : 2800, Loss : 0.40146, Acc : 0.831, Auc : 0.906, Sensitive_Loss : 0.09564, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992, Mean auc: 0.906, Run Time : 203.83 sec
INFO:root:2024-04-27 16:29:56, Best, Step : 2800, Loss : 0.40146, Acc : 0.831, Auc : 0.906, Sensitive_Loss : 0.09564, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992, Best Auc : 0.906
INFO:root:2024-04-27 16:30:04, Train, Epoch : 5, Step : 2810, Loss : 0.35542, Acc : 0.838, Sensitive_Loss : 0.08414, Sensitive_Acc : 17.700, Run Time : 212.59 sec
INFO:root:2024-04-27 16:30:15, Train, Epoch : 5, Step : 2820, Loss : 0.36061, Acc : 0.847, Sensitive_Loss : 0.07438, Sensitive_Acc : 16.600, Run Time : 11.46 sec
INFO:root:2024-04-27 16:30:27, Train, Epoch : 5, Step : 2830, Loss : 0.32846, Acc : 0.878, Sensitive_Loss : 0.08801, Sensitive_Acc : 16.400, Run Time : 11.59 sec
INFO:root:2024-04-27 16:30:39, Train, Epoch : 5, Step : 2840, Loss : 0.39895, Acc : 0.856, Sensitive_Loss : 0.07405, Sensitive_Acc : 15.400, Run Time : 11.94 sec
INFO:root:2024-04-27 16:30:51, Train, Epoch : 5, Step : 2850, Loss : 0.33732, Acc : 0.828, Sensitive_Loss : 0.07611, Sensitive_Acc : 17.300, Run Time : 12.35 sec
INFO:root:2024-04-27 16:31:02, Train, Epoch : 5, Step : 2860, Loss : 0.34812, Acc : 0.853, Sensitive_Loss : 0.08047, Sensitive_Acc : 15.500, Run Time : 10.86 sec
INFO:root:2024-04-27 16:31:14, Train, Epoch : 5, Step : 2870, Loss : 0.37903, Acc : 0.859, Sensitive_Loss : 0.06752, Sensitive_Acc : 15.800, Run Time : 12.13 sec
INFO:root:2024-04-27 16:31:26, Train, Epoch : 5, Step : 2880, Loss : 0.35100, Acc : 0.853, Sensitive_Loss : 0.06774, Sensitive_Acc : 15.000, Run Time : 12.25 sec
INFO:root:2024-04-27 16:31:38, Train, Epoch : 5, Step : 2890, Loss : 0.32267, Acc : 0.863, Sensitive_Loss : 0.08446, Sensitive_Acc : 17.200, Run Time : 11.21 sec
INFO:root:2024-04-27 16:31:49, Train, Epoch : 5, Step : 2900, Loss : 0.27643, Acc : 0.900, Sensitive_Loss : 0.05290, Sensitive_Acc : 16.400, Run Time : 11.32 sec
INFO:root:2024-04-27 16:34:25, Dev, Step : 2900, Loss : 0.40978, Acc : 0.831, Auc : 0.906, Sensitive_Loss : 0.10942, Sensitive_Acc : 16.836, Sensitive_Auc : 0.989, Mean auc: 0.906, Run Time : 155.70 sec
INFO:root:2024-04-27 16:34:33, Train, Epoch : 5, Step : 2910, Loss : 0.29410, Acc : 0.866, Sensitive_Loss : 0.06257, Sensitive_Acc : 15.000, Run Time : 164.21 sec
INFO:root:2024-04-27 16:34:46, Train, Epoch : 5, Step : 2920, Loss : 0.25563, Acc : 0.863, Sensitive_Loss : 0.10186, Sensitive_Acc : 16.400, Run Time : 12.29 sec
INFO:root:2024-04-27 16:34:56, Train, Epoch : 5, Step : 2930, Loss : 0.37651, Acc : 0.822, Sensitive_Loss : 0.07636, Sensitive_Acc : 14.600, Run Time : 10.77 sec
INFO:root:2024-04-27 16:35:07, Train, Epoch : 5, Step : 2940, Loss : 0.36697, Acc : 0.819, Sensitive_Loss : 0.07918, Sensitive_Acc : 17.000, Run Time : 10.96 sec
INFO:root:2024-04-27 16:35:19, Train, Epoch : 5, Step : 2950, Loss : 0.34856, Acc : 0.863, Sensitive_Loss : 0.06864, Sensitive_Acc : 15.100, Run Time : 11.72 sec
INFO:root:2024-04-27 16:35:31, Train, Epoch : 5, Step : 2960, Loss : 0.34650, Acc : 0.850, Sensitive_Loss : 0.07506, Sensitive_Acc : 15.700, Run Time : 12.26 sec
INFO:root:2024-04-27 16:35:42, Train, Epoch : 5, Step : 2970, Loss : 0.34840, Acc : 0.831, Sensitive_Loss : 0.07319, Sensitive_Acc : 16.100, Run Time : 10.72 sec
INFO:root:2024-04-27 16:35:54, Train, Epoch : 5, Step : 2980, Loss : 0.35490, Acc : 0.834, Sensitive_Loss : 0.07454, Sensitive_Acc : 15.800, Run Time : 11.83 sec
INFO:root:2024-04-27 16:36:06, Train, Epoch : 5, Step : 2990, Loss : 0.38432, Acc : 0.828, Sensitive_Loss : 0.06988, Sensitive_Acc : 16.300, Run Time : 12.00 sec
INFO:root:2024-04-27 16:36:17, Train, Epoch : 5, Step : 3000, Loss : 0.31347, Acc : 0.856, Sensitive_Loss : 0.09636, Sensitive_Acc : 15.300, Run Time : 11.42 sec
INFO:root:2024-04-27 16:38:52, Dev, Step : 3000, Loss : 0.40894, Acc : 0.830, Auc : 0.903, Sensitive_Loss : 0.12740, Sensitive_Acc : 16.850, Sensitive_Auc : 0.988, Mean auc: 0.903, Run Time : 154.41 sec
INFO:root:2024-04-27 16:39:00, Train, Epoch : 5, Step : 3010, Loss : 0.36448, Acc : 0.838, Sensitive_Loss : 0.06550, Sensitive_Acc : 17.200, Run Time : 163.09 sec
INFO:root:2024-04-27 16:39:12, Train, Epoch : 5, Step : 3020, Loss : 0.38386, Acc : 0.853, Sensitive_Loss : 0.06598, Sensitive_Acc : 16.400, Run Time : 11.54 sec
INFO:root:2024-04-27 16:39:23, Train, Epoch : 5, Step : 3030, Loss : 0.31698, Acc : 0.834, Sensitive_Loss : 0.08449, Sensitive_Acc : 16.400, Run Time : 11.30 sec
INFO:root:2024-04-27 16:39:35, Train, Epoch : 5, Step : 3040, Loss : 0.36866, Acc : 0.794, Sensitive_Loss : 0.08929, Sensitive_Acc : 15.100, Run Time : 11.39 sec
INFO:root:2024-04-27 16:39:46, Train, Epoch : 5, Step : 3050, Loss : 0.37589, Acc : 0.791, Sensitive_Loss : 0.09512, Sensitive_Acc : 15.900, Run Time : 11.94 sec
INFO:root:2024-04-27 16:39:58, Train, Epoch : 5, Step : 3060, Loss : 0.39159, Acc : 0.812, Sensitive_Loss : 0.05519, Sensitive_Acc : 16.300, Run Time : 11.53 sec
INFO:root:2024-04-27 16:40:09, Train, Epoch : 5, Step : 3070, Loss : 0.32155, Acc : 0.869, Sensitive_Loss : 0.10175, Sensitive_Acc : 16.200, Run Time : 11.30 sec
INFO:root:2024-04-27 16:40:21, Train, Epoch : 5, Step : 3080, Loss : 0.35974, Acc : 0.863, Sensitive_Loss : 0.06758, Sensitive_Acc : 15.900, Run Time : 12.09 sec
INFO:root:2024-04-27 16:40:32, Train, Epoch : 5, Step : 3090, Loss : 0.30768, Acc : 0.875, Sensitive_Loss : 0.07873, Sensitive_Acc : 15.100, Run Time : 10.55 sec
INFO:root:2024-04-27 16:40:44, Train, Epoch : 5, Step : 3100, Loss : 0.35402, Acc : 0.878, Sensitive_Loss : 0.07178, Sensitive_Acc : 15.600, Run Time : 11.77 sec
INFO:root:2024-04-27 16:43:20, Dev, Step : 3100, Loss : 0.40899, Acc : 0.831, Auc : 0.908, Sensitive_Loss : 0.10278, Sensitive_Acc : 16.893, Sensitive_Auc : 0.990, Mean auc: 0.908, Run Time : 156.20 sec
INFO:root:2024-04-27 16:43:21, Best, Step : 3100, Loss : 0.40899, Acc : 0.831, Auc : 0.908, Sensitive_Loss : 0.10278, Sensitive_Acc : 16.893, Sensitive_Auc : 0.990, Best Auc : 0.908
INFO:root:2024-04-27 16:43:29, Train, Epoch : 5, Step : 3110, Loss : 0.32966, Acc : 0.850, Sensitive_Loss : 0.08297, Sensitive_Acc : 16.600, Run Time : 165.44 sec
INFO:root:2024-04-27 16:43:41, Train, Epoch : 5, Step : 3120, Loss : 0.36133, Acc : 0.866, Sensitive_Loss : 0.05360, Sensitive_Acc : 16.300, Run Time : 11.50 sec
INFO:root:2024-04-27 16:43:51, Train, Epoch : 5, Step : 3130, Loss : 0.33742, Acc : 0.869, Sensitive_Loss : 0.09556, Sensitive_Acc : 15.200, Run Time : 10.64 sec
INFO:root:2024-04-27 16:46:24
INFO:root:y_pred: [0.15495108 0.8593764  0.03712161 ... 0.8939103  0.04225894 0.9077401 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.74602997e-01 8.98880080e-06 5.57211279e-05 2.24964182e-07
 9.94638264e-01 8.29350142e-07 9.99210358e-01 9.78842735e-01
 1.50775268e-05 9.76376712e-01 9.98793244e-01 9.99180377e-01
 9.97643888e-01 8.05298626e-01 2.33646540e-04 9.63427663e-01
 9.99507189e-01 1.18096992e-04 3.67802292e-01 6.00486875e-01
 9.82009053e-01 3.63659323e-03 9.99396443e-01 8.27887177e-01
 9.78247464e-01 8.62975717e-01 5.62713467e-05 9.89993274e-01
 9.83481109e-01 2.84431607e-01 2.86214199e-05 7.74536096e-03
 5.47809992e-03 2.53233593e-04 1.33702785e-01 4.01010366e-05
 1.07944850e-03 1.01057393e-02 9.74551380e-01 9.73133683e-01
 3.31497586e-06 1.35146285e-04 9.69644427e-01 3.11594486e-05
 9.99268830e-01 9.94531751e-01 9.70004737e-01 9.96680915e-01
 8.49359948e-03 9.87935543e-01 9.97086704e-01 9.27340880e-05
 5.08465152e-03 2.28009521e-05 4.02868245e-05 1.60590326e-03
 3.85287480e-04 1.90542012e-01 1.06878084e-04 3.25886197e-02
 1.56373717e-04 7.77387246e-03 4.16376570e-05 9.90939498e-01
 8.04540142e-03 9.95848656e-01 7.63649878e-05 9.90968764e-01
 8.29495609e-01 6.82043254e-01 8.81419539e-01 2.10337918e-02
 2.97705574e-05 5.97651757e-04 7.17077592e-06 1.68779970e-05
 2.51654625e-01 6.11095093e-02 7.34983360e-06 9.68402565e-01
 9.95255411e-01 3.31574438e-06 2.44072005e-02 2.99952917e-05
 7.16731846e-02 8.98557961e-01 6.33502146e-03 1.31578941e-03
 9.61175919e-01 9.93933499e-01 9.94364142e-01 2.01628767e-02
 1.80537507e-04 9.95976150e-01 3.73969615e-01 1.02811955e-05
 9.91416395e-01 9.94941890e-01 2.34702910e-07 3.85765248e-04
 9.69663680e-01 9.44605947e-01 9.98235464e-01 9.90749896e-01
 1.60934560e-05 1.58883841e-03 7.29239941e-01 8.46388996e-01
 9.64291692e-01 4.54096562e-06 6.27614260e-01 9.98902082e-01
 1.13100059e-01 9.98044610e-01 9.54898119e-01 9.62869406e-01
 4.97913629e-01 9.91912246e-01 4.86834273e-02 9.49240804e-01
 9.96184051e-01 9.91513669e-01 9.41225323e-07 7.91880012e-01
 9.95688856e-01 2.74664490e-03 9.90414619e-01 1.05334504e-04
 1.35829428e-03 9.79000032e-01 9.91532147e-01 1.19903700e-04
 6.42977422e-04 2.88816751e-04 9.85993147e-01 9.98406351e-01
 8.89667869e-01 2.83289090e-04 2.97004939e-04 9.00996685e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 16:46:24, Dev, Step : 3130, Loss : 0.39745, Acc : 0.831, Auc : 0.908, Sensitive_Loss : 0.09532, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992, Mean auc: 0.908, Run Time : 152.87 sec
INFO:root:2024-04-27 16:46:25, Best, Step : 3130, Loss : 0.39745, Acc : 0.831,Auc : 0.908, Best Auc : 0.908, Sensitive_Loss : 0.09532, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992
INFO:root:2024-04-27 16:46:39, Train, Epoch : 6, Step : 3140, Loss : 0.30897, Acc : 0.863, Sensitive_Loss : 0.07680, Sensitive_Acc : 16.300, Run Time : 13.37 sec
INFO:root:2024-04-27 16:46:51, Train, Epoch : 6, Step : 3150, Loss : 0.30120, Acc : 0.856, Sensitive_Loss : 0.07578, Sensitive_Acc : 16.700, Run Time : 11.25 sec
INFO:root:2024-04-27 16:47:03, Train, Epoch : 6, Step : 3160, Loss : 0.34559, Acc : 0.847, Sensitive_Loss : 0.08504, Sensitive_Acc : 16.200, Run Time : 11.91 sec
INFO:root:2024-04-27 16:47:14, Train, Epoch : 6, Step : 3170, Loss : 0.32619, Acc : 0.844, Sensitive_Loss : 0.08016, Sensitive_Acc : 17.700, Run Time : 11.66 sec
INFO:root:2024-04-27 16:47:26, Train, Epoch : 6, Step : 3180, Loss : 0.35276, Acc : 0.856, Sensitive_Loss : 0.10474, Sensitive_Acc : 15.200, Run Time : 11.72 sec
INFO:root:2024-04-27 16:47:37, Train, Epoch : 6, Step : 3190, Loss : 0.38547, Acc : 0.847, Sensitive_Loss : 0.07230, Sensitive_Acc : 15.200, Run Time : 10.82 sec
INFO:root:2024-04-27 16:47:49, Train, Epoch : 6, Step : 3200, Loss : 0.35359, Acc : 0.828, Sensitive_Loss : 0.08294, Sensitive_Acc : 16.800, Run Time : 11.91 sec
INFO:root:2024-04-27 16:50:24, Dev, Step : 3200, Loss : 0.40893, Acc : 0.829, Auc : 0.905, Sensitive_Loss : 0.10967, Sensitive_Acc : 16.893, Sensitive_Auc : 0.989, Mean auc: 0.905, Run Time : 155.07 sec
INFO:root:2024-04-27 16:50:32, Train, Epoch : 6, Step : 3210, Loss : 0.29524, Acc : 0.866, Sensitive_Loss : 0.06916, Sensitive_Acc : 16.900, Run Time : 163.31 sec
INFO:root:2024-04-27 16:50:44, Train, Epoch : 6, Step : 3220, Loss : 0.36630, Acc : 0.819, Sensitive_Loss : 0.10676, Sensitive_Acc : 16.100, Run Time : 11.76 sec
INFO:root:2024-04-27 16:50:57, Train, Epoch : 6, Step : 3230, Loss : 0.36430, Acc : 0.841, Sensitive_Loss : 0.08804, Sensitive_Acc : 17.500, Run Time : 12.96 sec
INFO:root:2024-04-27 16:51:08, Train, Epoch : 6, Step : 3240, Loss : 0.38071, Acc : 0.850, Sensitive_Loss : 0.07150, Sensitive_Acc : 15.400, Run Time : 11.05 sec
INFO:root:2024-04-27 16:51:19, Train, Epoch : 6, Step : 3250, Loss : 0.32063, Acc : 0.841, Sensitive_Loss : 0.06384, Sensitive_Acc : 16.700, Run Time : 10.96 sec
INFO:root:2024-04-27 16:51:31, Train, Epoch : 6, Step : 3260, Loss : 0.33043, Acc : 0.881, Sensitive_Loss : 0.05975, Sensitive_Acc : 17.100, Run Time : 12.03 sec
INFO:root:2024-04-27 16:51:42, Train, Epoch : 6, Step : 3270, Loss : 0.28803, Acc : 0.869, Sensitive_Loss : 0.04888, Sensitive_Acc : 16.800, Run Time : 11.48 sec
INFO:root:2024-04-27 16:51:53, Train, Epoch : 6, Step : 3280, Loss : 0.38859, Acc : 0.838, Sensitive_Loss : 0.06866, Sensitive_Acc : 15.500, Run Time : 11.06 sec
INFO:root:2024-04-27 16:52:06, Train, Epoch : 6, Step : 3290, Loss : 0.33140, Acc : 0.859, Sensitive_Loss : 0.04425, Sensitive_Acc : 17.200, Run Time : 12.65 sec
INFO:root:2024-04-27 16:52:17, Train, Epoch : 6, Step : 3300, Loss : 0.34490, Acc : 0.853, Sensitive_Loss : 0.07021, Sensitive_Acc : 16.300, Run Time : 11.52 sec
INFO:root:2024-04-27 16:54:54, Dev, Step : 3300, Loss : 0.40284, Acc : 0.829, Auc : 0.907, Sensitive_Loss : 0.10630, Sensitive_Acc : 16.893, Sensitive_Auc : 0.990, Mean auc: 0.907, Run Time : 156.78 sec
INFO:root:2024-04-27 16:55:03, Train, Epoch : 6, Step : 3310, Loss : 0.31481, Acc : 0.872, Sensitive_Loss : 0.07762, Sensitive_Acc : 15.800, Run Time : 165.13 sec
INFO:root:2024-04-27 16:55:14, Train, Epoch : 6, Step : 3320, Loss : 0.36812, Acc : 0.822, Sensitive_Loss : 0.06883, Sensitive_Acc : 15.300, Run Time : 11.56 sec
INFO:root:2024-04-27 16:55:26, Train, Epoch : 6, Step : 3330, Loss : 0.29673, Acc : 0.891, Sensitive_Loss : 0.07904, Sensitive_Acc : 18.000, Run Time : 11.65 sec
INFO:root:2024-04-27 16:55:37, Train, Epoch : 6, Step : 3340, Loss : 0.28277, Acc : 0.912, Sensitive_Loss : 0.05149, Sensitive_Acc : 15.700, Run Time : 11.31 sec
INFO:root:2024-04-27 16:55:51, Train, Epoch : 6, Step : 3350, Loss : 0.37249, Acc : 0.809, Sensitive_Loss : 0.07879, Sensitive_Acc : 15.900, Run Time : 13.93 sec
INFO:root:2024-04-27 16:56:03, Train, Epoch : 6, Step : 3360, Loss : 0.35868, Acc : 0.844, Sensitive_Loss : 0.09496, Sensitive_Acc : 15.900, Run Time : 12.34 sec
INFO:root:2024-04-27 16:56:14, Train, Epoch : 6, Step : 3370, Loss : 0.37521, Acc : 0.841, Sensitive_Loss : 0.09363, Sensitive_Acc : 16.800, Run Time : 10.68 sec
INFO:root:2024-04-27 16:56:29, Train, Epoch : 6, Step : 3380, Loss : 0.29933, Acc : 0.853, Sensitive_Loss : 0.09433, Sensitive_Acc : 16.800, Run Time : 14.90 sec
INFO:root:2024-04-27 16:56:40, Train, Epoch : 6, Step : 3390, Loss : 0.36441, Acc : 0.850, Sensitive_Loss : 0.06056, Sensitive_Acc : 16.700, Run Time : 11.25 sec
INFO:root:2024-04-27 16:56:51, Train, Epoch : 6, Step : 3400, Loss : 0.34099, Acc : 0.866, Sensitive_Loss : 0.09628, Sensitive_Acc : 16.100, Run Time : 10.94 sec
INFO:root:2024-04-27 16:59:35, Dev, Step : 3400, Loss : 0.43233, Acc : 0.818, Auc : 0.903, Sensitive_Loss : 0.11827, Sensitive_Acc : 16.836, Sensitive_Auc : 0.989, Mean auc: 0.903, Run Time : 164.40 sec
INFO:root:2024-04-27 16:59:44, Train, Epoch : 6, Step : 3410, Loss : 0.31296, Acc : 0.866, Sensitive_Loss : 0.07165, Sensitive_Acc : 17.300, Run Time : 173.01 sec
INFO:root:2024-04-27 16:59:56, Train, Epoch : 6, Step : 3420, Loss : 0.32725, Acc : 0.856, Sensitive_Loss : 0.07057, Sensitive_Acc : 15.900, Run Time : 11.81 sec
INFO:root:2024-04-27 17:00:07, Train, Epoch : 6, Step : 3430, Loss : 0.31592, Acc : 0.863, Sensitive_Loss : 0.06594, Sensitive_Acc : 17.000, Run Time : 11.42 sec
INFO:root:2024-04-27 17:00:20, Train, Epoch : 6, Step : 3440, Loss : 0.28661, Acc : 0.834, Sensitive_Loss : 0.10121, Sensitive_Acc : 15.800, Run Time : 12.40 sec
INFO:root:2024-04-27 17:00:31, Train, Epoch : 6, Step : 3450, Loss : 0.28004, Acc : 0.891, Sensitive_Loss : 0.07111, Sensitive_Acc : 16.000, Run Time : 11.27 sec
INFO:root:2024-04-27 17:00:43, Train, Epoch : 6, Step : 3460, Loss : 0.27885, Acc : 0.869, Sensitive_Loss : 0.10764, Sensitive_Acc : 17.800, Run Time : 11.55 sec
INFO:root:2024-04-27 17:00:54, Train, Epoch : 6, Step : 3470, Loss : 0.33493, Acc : 0.859, Sensitive_Loss : 0.05209, Sensitive_Acc : 16.100, Run Time : 10.99 sec
INFO:root:2024-04-27 17:01:06, Train, Epoch : 6, Step : 3480, Loss : 0.33037, Acc : 0.866, Sensitive_Loss : 0.09123, Sensitive_Acc : 16.000, Run Time : 12.45 sec
INFO:root:2024-04-27 17:01:20, Train, Epoch : 6, Step : 3490, Loss : 0.34319, Acc : 0.847, Sensitive_Loss : 0.05692, Sensitive_Acc : 15.800, Run Time : 13.62 sec
INFO:root:2024-04-27 17:01:31, Train, Epoch : 6, Step : 3500, Loss : 0.35101, Acc : 0.863, Sensitive_Loss : 0.07390, Sensitive_Acc : 15.900, Run Time : 11.46 sec
INFO:root:2024-04-27 17:04:09, Dev, Step : 3500, Loss : 0.42049, Acc : 0.826, Auc : 0.903, Sensitive_Loss : 0.11266, Sensitive_Acc : 16.893, Sensitive_Auc : 0.990, Mean auc: 0.903, Run Time : 158.22 sec
INFO:root:2024-04-27 17:04:18, Train, Epoch : 6, Step : 3510, Loss : 0.35065, Acc : 0.841, Sensitive_Loss : 0.05587, Sensitive_Acc : 16.500, Run Time : 166.67 sec
INFO:root:2024-04-27 17:04:29, Train, Epoch : 6, Step : 3520, Loss : 0.31315, Acc : 0.866, Sensitive_Loss : 0.07209, Sensitive_Acc : 17.300, Run Time : 11.74 sec
INFO:root:2024-04-27 17:04:40, Train, Epoch : 6, Step : 3530, Loss : 0.30045, Acc : 0.866, Sensitive_Loss : 0.06213, Sensitive_Acc : 17.300, Run Time : 10.85 sec
INFO:root:2024-04-27 17:04:52, Train, Epoch : 6, Step : 3540, Loss : 0.38324, Acc : 0.866, Sensitive_Loss : 0.08365, Sensitive_Acc : 15.700, Run Time : 11.81 sec
INFO:root:2024-04-27 17:05:04, Train, Epoch : 6, Step : 3550, Loss : 0.31658, Acc : 0.878, Sensitive_Loss : 0.08983, Sensitive_Acc : 15.400, Run Time : 11.48 sec
INFO:root:2024-04-27 17:05:15, Train, Epoch : 6, Step : 3560, Loss : 0.28162, Acc : 0.891, Sensitive_Loss : 0.10835, Sensitive_Acc : 16.500, Run Time : 11.32 sec
INFO:root:2024-04-27 17:05:27, Train, Epoch : 6, Step : 3570, Loss : 0.40194, Acc : 0.812, Sensitive_Loss : 0.05796, Sensitive_Acc : 16.100, Run Time : 11.73 sec
INFO:root:2024-04-27 17:05:38, Train, Epoch : 6, Step : 3580, Loss : 0.31614, Acc : 0.869, Sensitive_Loss : 0.09729, Sensitive_Acc : 16.000, Run Time : 11.31 sec
INFO:root:2024-04-27 17:05:50, Train, Epoch : 6, Step : 3590, Loss : 0.31608, Acc : 0.856, Sensitive_Loss : 0.08756, Sensitive_Acc : 16.100, Run Time : 12.37 sec
INFO:root:2024-04-27 17:06:00, Train, Epoch : 6, Step : 3600, Loss : 0.31821, Acc : 0.866, Sensitive_Loss : 0.08779, Sensitive_Acc : 15.900, Run Time : 10.13 sec
INFO:root:2024-04-27 17:08:36, Dev, Step : 3600, Loss : 0.40934, Acc : 0.827, Auc : 0.904, Sensitive_Loss : 0.10363, Sensitive_Acc : 16.893, Sensitive_Auc : 0.990, Mean auc: 0.904, Run Time : 155.88 sec
INFO:root:2024-04-27 17:08:45, Train, Epoch : 6, Step : 3610, Loss : 0.35839, Acc : 0.834, Sensitive_Loss : 0.06699, Sensitive_Acc : 16.400, Run Time : 164.78 sec
INFO:root:2024-04-27 17:08:57, Train, Epoch : 6, Step : 3620, Loss : 0.30273, Acc : 0.878, Sensitive_Loss : 0.05165, Sensitive_Acc : 17.000, Run Time : 12.05 sec
INFO:root:2024-04-27 17:09:09, Train, Epoch : 6, Step : 3630, Loss : 0.32360, Acc : 0.878, Sensitive_Loss : 0.07172, Sensitive_Acc : 15.500, Run Time : 11.73 sec
INFO:root:2024-04-27 17:09:21, Train, Epoch : 6, Step : 3640, Loss : 0.35297, Acc : 0.850, Sensitive_Loss : 0.07026, Sensitive_Acc : 17.000, Run Time : 11.52 sec
INFO:root:2024-04-27 17:09:31, Train, Epoch : 6, Step : 3650, Loss : 0.30620, Acc : 0.838, Sensitive_Loss : 0.08347, Sensitive_Acc : 18.100, Run Time : 10.34 sec
INFO:root:2024-04-27 17:09:43, Train, Epoch : 6, Step : 3660, Loss : 0.31143, Acc : 0.847, Sensitive_Loss : 0.10748, Sensitive_Acc : 15.700, Run Time : 12.28 sec
INFO:root:2024-04-27 17:09:54, Train, Epoch : 6, Step : 3670, Loss : 0.23237, Acc : 0.897, Sensitive_Loss : 0.06475, Sensitive_Acc : 16.200, Run Time : 10.49 sec
INFO:root:2024-04-27 17:10:05, Train, Epoch : 6, Step : 3680, Loss : 0.32487, Acc : 0.872, Sensitive_Loss : 0.08212, Sensitive_Acc : 15.600, Run Time : 10.83 sec
INFO:root:2024-04-27 17:10:16, Train, Epoch : 6, Step : 3690, Loss : 0.33213, Acc : 0.850, Sensitive_Loss : 0.06450, Sensitive_Acc : 16.900, Run Time : 11.69 sec
INFO:root:2024-04-27 17:10:28, Train, Epoch : 6, Step : 3700, Loss : 0.33979, Acc : 0.831, Sensitive_Loss : 0.06946, Sensitive_Acc : 17.600, Run Time : 11.52 sec
INFO:root:2024-04-27 17:13:03, Dev, Step : 3700, Loss : 0.43384, Acc : 0.824, Auc : 0.903, Sensitive_Loss : 0.10752, Sensitive_Acc : 16.850, Sensitive_Auc : 0.992, Mean auc: 0.903, Run Time : 155.69 sec
INFO:root:2024-04-27 17:13:14, Train, Epoch : 6, Step : 3710, Loss : 0.35234, Acc : 0.847, Sensitive_Loss : 0.05913, Sensitive_Acc : 16.300, Run Time : 166.51 sec
INFO:root:2024-04-27 17:13:25, Train, Epoch : 6, Step : 3720, Loss : 0.31679, Acc : 0.853, Sensitive_Loss : 0.06885, Sensitive_Acc : 16.300, Run Time : 11.02 sec
INFO:root:2024-04-27 17:13:37, Train, Epoch : 6, Step : 3730, Loss : 0.31502, Acc : 0.859, Sensitive_Loss : 0.07675, Sensitive_Acc : 14.900, Run Time : 11.38 sec
INFO:root:2024-04-27 17:13:51, Train, Epoch : 6, Step : 3740, Loss : 0.36865, Acc : 0.834, Sensitive_Loss : 0.06363, Sensitive_Acc : 15.100, Run Time : 14.37 sec
INFO:root:2024-04-27 17:14:04, Train, Epoch : 6, Step : 3750, Loss : 0.33904, Acc : 0.834, Sensitive_Loss : 0.06689, Sensitive_Acc : 17.000, Run Time : 12.73 sec
INFO:root:2024-04-27 17:16:51
INFO:root:y_pred: [0.14336674 0.86161375 0.01836715 ... 0.86986965 0.04034022 0.7608722 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.8351902e-01 1.0328772e-04 4.4036817e-04 1.4433742e-06 9.9804461e-01
 4.1977582e-06 9.9969077e-01 9.9057555e-01 2.5414800e-05 9.8562092e-01
 9.9847871e-01 9.9984515e-01 9.9893051e-01 8.8558495e-01 1.5490419e-03
 9.8102987e-01 9.9983108e-01 5.0973269e-04 4.8462629e-01 9.5681590e-01
 9.8729891e-01 3.6902666e-02 9.9981624e-01 9.3456024e-01 9.8871583e-01
 9.3491918e-01 6.6961475e-05 9.9621660e-01 9.9213493e-01 3.9623025e-01
 2.2145618e-05 1.5568044e-02 4.0808232e-03 1.2594699e-03 3.1511027e-01
 3.0959290e-04 3.5266308e-03 1.9822776e-02 9.8750699e-01 9.8829788e-01
 6.8717447e-05 1.7277985e-04 9.9188280e-01 2.3972771e-05 9.9962258e-01
 9.9717653e-01 9.8652864e-01 9.9798620e-01 2.0272370e-02 9.9449086e-01
 9.9811268e-01 2.3276672e-04 3.6437390e-03 7.5320575e-05 9.9020261e-05
 2.8595768e-03 7.4296175e-03 7.0518303e-01 3.2327708e-04 7.1170621e-02
 1.3350339e-03 1.0129504e-02 1.7284640e-04 9.9506760e-01 1.2149287e-02
 9.9786061e-01 2.6794558e-04 9.9143082e-01 9.3312085e-01 9.6366560e-01
 9.3891168e-01 8.3131008e-02 1.0851451e-04 4.2616375e-04 2.8388751e-05
 3.9677616e-05 5.9644997e-01 2.3215686e-01 1.7521479e-05 9.8260844e-01
 9.9819678e-01 3.4467270e-05 4.5678180e-01 1.2016467e-04 3.9283758e-01
 9.5796275e-01 7.0934944e-02 1.5893484e-02 9.6555585e-01 9.9821728e-01
 9.9704933e-01 1.8990017e-02 1.2257461e-03 9.9910694e-01 6.1185074e-01
 8.5266205e-05 9.9576879e-01 9.9839717e-01 9.2989239e-07 4.0660391e-04
 9.8493862e-01 9.8124826e-01 9.9934119e-01 9.9547017e-01 7.9681893e-05
 5.2026953e-03 8.0998617e-01 9.0843689e-01 9.7782636e-01 9.8336859e-06
 7.6794600e-01 9.9943370e-01 6.6053569e-02 9.9873024e-01 9.8700815e-01
 9.7576773e-01 9.0763366e-01 9.9770492e-01 3.6535773e-02 9.7787094e-01
 9.9814260e-01 9.9570835e-01 6.7659462e-06 8.4678203e-01 9.9798226e-01
 1.0215783e-02 9.9405718e-01 1.2972770e-03 1.3237780e-02 9.9750012e-01
 9.9497002e-01 3.5227250e-04 7.9017278e-04 1.5520164e-03 9.9212015e-01
 9.9958247e-01 9.5158619e-01 4.6081777e-04 6.8685412e-04 9.5571905e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 17:16:51, Dev, Step : 3756, Loss : 0.43493, Acc : 0.821, Auc : 0.902, Sensitive_Loss : 0.11297, Sensitive_Acc : 16.921, Sensitive_Auc : 0.989, Mean auc: 0.902, Run Time : 160.93 sec
INFO:root:2024-04-27 17:17:01, Train, Epoch : 7, Step : 3760, Loss : 0.12459, Acc : 0.353, Sensitive_Loss : 0.01681, Sensitive_Acc : 5.700, Run Time : 7.04 sec
INFO:root:2024-04-27 17:17:13, Train, Epoch : 7, Step : 3770, Loss : 0.27922, Acc : 0.853, Sensitive_Loss : 0.11226, Sensitive_Acc : 18.300, Run Time : 11.73 sec
INFO:root:2024-04-27 17:17:24, Train, Epoch : 7, Step : 3780, Loss : 0.29641, Acc : 0.884, Sensitive_Loss : 0.06320, Sensitive_Acc : 16.500, Run Time : 11.28 sec
INFO:root:2024-04-27 17:17:35, Train, Epoch : 7, Step : 3790, Loss : 0.29280, Acc : 0.869, Sensitive_Loss : 0.06601, Sensitive_Acc : 15.700, Run Time : 11.41 sec
INFO:root:2024-04-27 17:17:46, Train, Epoch : 7, Step : 3800, Loss : 0.31127, Acc : 0.850, Sensitive_Loss : 0.08675, Sensitive_Acc : 17.000, Run Time : 10.30 sec
INFO:root:2024-04-27 17:20:25, Dev, Step : 3800, Loss : 0.41698, Acc : 0.833, Auc : 0.906, Sensitive_Loss : 0.09830, Sensitive_Acc : 16.921, Sensitive_Auc : 0.992, Mean auc: 0.906, Run Time : 159.24 sec
INFO:root:2024-04-27 17:20:35, Train, Epoch : 7, Step : 3810, Loss : 0.30501, Acc : 0.869, Sensitive_Loss : 0.08032, Sensitive_Acc : 16.000, Run Time : 168.83 sec
INFO:root:2024-04-27 17:20:46, Train, Epoch : 7, Step : 3820, Loss : 0.29190, Acc : 0.863, Sensitive_Loss : 0.04634, Sensitive_Acc : 16.700, Run Time : 11.55 sec
INFO:root:2024-04-27 17:20:57, Train, Epoch : 7, Step : 3830, Loss : 0.29419, Acc : 0.856, Sensitive_Loss : 0.05319, Sensitive_Acc : 16.500, Run Time : 10.50 sec
INFO:root:2024-04-27 17:21:08, Train, Epoch : 7, Step : 3840, Loss : 0.29854, Acc : 0.844, Sensitive_Loss : 0.06787, Sensitive_Acc : 15.600, Run Time : 11.35 sec
INFO:root:2024-04-27 17:21:20, Train, Epoch : 7, Step : 3850, Loss : 0.31510, Acc : 0.859, Sensitive_Loss : 0.11084, Sensitive_Acc : 15.900, Run Time : 12.45 sec
INFO:root:2024-04-27 17:21:31, Train, Epoch : 7, Step : 3860, Loss : 0.32214, Acc : 0.859, Sensitive_Loss : 0.07508, Sensitive_Acc : 15.900, Run Time : 10.66 sec
INFO:root:2024-04-27 17:21:43, Train, Epoch : 7, Step : 3870, Loss : 0.32461, Acc : 0.863, Sensitive_Loss : 0.06774, Sensitive_Acc : 15.800, Run Time : 11.43 sec
INFO:root:2024-04-27 17:21:53, Train, Epoch : 7, Step : 3880, Loss : 0.24754, Acc : 0.900, Sensitive_Loss : 0.05541, Sensitive_Acc : 16.400, Run Time : 10.72 sec
INFO:root:2024-04-27 17:22:05, Train, Epoch : 7, Step : 3890, Loss : 0.36318, Acc : 0.853, Sensitive_Loss : 0.06155, Sensitive_Acc : 15.700, Run Time : 11.80 sec
INFO:root:2024-04-27 17:22:17, Train, Epoch : 7, Step : 3900, Loss : 0.35095, Acc : 0.853, Sensitive_Loss : 0.05298, Sensitive_Acc : 16.000, Run Time : 11.64 sec
INFO:root:2024-04-27 17:24:52, Dev, Step : 3900, Loss : 0.41973, Acc : 0.827, Auc : 0.904, Sensitive_Loss : 0.11151, Sensitive_Acc : 16.893, Sensitive_Auc : 0.992, Mean auc: 0.904, Run Time : 155.76 sec
INFO:root:2024-04-27 17:25:01, Train, Epoch : 7, Step : 3910, Loss : 0.29911, Acc : 0.869, Sensitive_Loss : 0.06506, Sensitive_Acc : 16.900, Run Time : 164.16 sec
INFO:root:2024-04-27 17:25:13, Train, Epoch : 7, Step : 3920, Loss : 0.30729, Acc : 0.866, Sensitive_Loss : 0.06992, Sensitive_Acc : 15.800, Run Time : 11.88 sec
INFO:root:2024-04-27 17:25:24, Train, Epoch : 7, Step : 3930, Loss : 0.28728, Acc : 0.894, Sensitive_Loss : 0.07283, Sensitive_Acc : 15.600, Run Time : 11.50 sec
INFO:root:2024-04-27 17:25:35, Train, Epoch : 7, Step : 3940, Loss : 0.30697, Acc : 0.887, Sensitive_Loss : 0.06523, Sensitive_Acc : 17.600, Run Time : 11.23 sec
INFO:root:2024-04-27 17:25:47, Train, Epoch : 7, Step : 3950, Loss : 0.33866, Acc : 0.881, Sensitive_Loss : 0.04870, Sensitive_Acc : 14.900, Run Time : 11.35 sec
INFO:root:2024-04-27 17:25:58, Train, Epoch : 7, Step : 3960, Loss : 0.29682, Acc : 0.856, Sensitive_Loss : 0.08442, Sensitive_Acc : 16.000, Run Time : 11.42 sec
INFO:root:2024-04-27 17:26:10, Train, Epoch : 7, Step : 3970, Loss : 0.28072, Acc : 0.856, Sensitive_Loss : 0.07611, Sensitive_Acc : 16.800, Run Time : 12.23 sec
INFO:root:2024-04-27 17:26:21, Train, Epoch : 7, Step : 3980, Loss : 0.26003, Acc : 0.891, Sensitive_Loss : 0.07880, Sensitive_Acc : 17.800, Run Time : 10.79 sec
INFO:root:2024-04-27 17:26:33, Train, Epoch : 7, Step : 3990, Loss : 0.31145, Acc : 0.891, Sensitive_Loss : 0.08038, Sensitive_Acc : 15.700, Run Time : 12.09 sec
INFO:root:2024-04-27 17:26:45, Train, Epoch : 7, Step : 4000, Loss : 0.31304, Acc : 0.856, Sensitive_Loss : 0.07486, Sensitive_Acc : 15.500, Run Time : 11.28 sec
INFO:root:2024-04-27 17:29:19, Dev, Step : 4000, Loss : 0.42799, Acc : 0.825, Auc : 0.903, Sensitive_Loss : 0.12273, Sensitive_Acc : 16.836, Sensitive_Auc : 0.991, Mean auc: 0.903, Run Time : 154.32 sec
INFO:root:2024-04-27 17:29:28, Train, Epoch : 7, Step : 4010, Loss : 0.37814, Acc : 0.831, Sensitive_Loss : 0.08201, Sensitive_Acc : 17.100, Run Time : 163.15 sec
INFO:root:2024-04-27 17:29:40, Train, Epoch : 7, Step : 4020, Loss : 0.41944, Acc : 0.828, Sensitive_Loss : 0.03810, Sensitive_Acc : 16.500, Run Time : 11.71 sec
INFO:root:2024-04-27 17:29:51, Train, Epoch : 7, Step : 4030, Loss : 0.32012, Acc : 0.850, Sensitive_Loss : 0.06948, Sensitive_Acc : 16.800, Run Time : 11.63 sec
INFO:root:2024-04-27 17:30:02, Train, Epoch : 7, Step : 4040, Loss : 0.38086, Acc : 0.856, Sensitive_Loss : 0.08645, Sensitive_Acc : 15.900, Run Time : 10.58 sec
INFO:root:2024-04-27 17:30:14, Train, Epoch : 7, Step : 4050, Loss : 0.31187, Acc : 0.866, Sensitive_Loss : 0.04886, Sensitive_Acc : 16.100, Run Time : 12.05 sec
INFO:root:2024-04-27 17:30:24, Train, Epoch : 7, Step : 4060, Loss : 0.28865, Acc : 0.872, Sensitive_Loss : 0.07238, Sensitive_Acc : 17.300, Run Time : 10.26 sec
INFO:root:2024-04-27 17:30:36, Train, Epoch : 7, Step : 4070, Loss : 0.34572, Acc : 0.847, Sensitive_Loss : 0.07747, Sensitive_Acc : 15.900, Run Time : 11.59 sec
INFO:root:2024-04-27 17:30:47, Train, Epoch : 7, Step : 4080, Loss : 0.29790, Acc : 0.866, Sensitive_Loss : 0.07669, Sensitive_Acc : 16.900, Run Time : 11.66 sec
INFO:root:2024-04-27 17:30:58, Train, Epoch : 7, Step : 4090, Loss : 0.32104, Acc : 0.872, Sensitive_Loss : 0.04915, Sensitive_Acc : 17.000, Run Time : 10.63 sec
INFO:root:2024-04-27 17:31:10, Train, Epoch : 7, Step : 4100, Loss : 0.31654, Acc : 0.859, Sensitive_Loss : 0.06243, Sensitive_Acc : 17.200, Run Time : 11.99 sec
INFO:root:2024-04-27 17:33:45, Dev, Step : 4100, Loss : 0.41763, Acc : 0.828, Auc : 0.906, Sensitive_Loss : 0.11305, Sensitive_Acc : 16.750, Sensitive_Auc : 0.992, Mean auc: 0.906, Run Time : 155.43 sec
INFO:root:2024-04-27 17:33:55, Train, Epoch : 7, Step : 4110, Loss : 0.26334, Acc : 0.881, Sensitive_Loss : 0.03988, Sensitive_Acc : 17.000, Run Time : 165.17 sec
INFO:root:2024-04-27 17:34:06, Train, Epoch : 7, Step : 4120, Loss : 0.33078, Acc : 0.872, Sensitive_Loss : 0.06683, Sensitive_Acc : 15.600, Run Time : 11.07 sec
INFO:root:2024-04-27 17:34:18, Train, Epoch : 7, Step : 4130, Loss : 0.30921, Acc : 0.859, Sensitive_Loss : 0.04892, Sensitive_Acc : 16.200, Run Time : 11.76 sec
INFO:root:2024-04-27 17:34:29, Train, Epoch : 7, Step : 4140, Loss : 0.29415, Acc : 0.863, Sensitive_Loss : 0.07345, Sensitive_Acc : 16.900, Run Time : 11.37 sec
INFO:root:2024-04-27 17:34:40, Train, Epoch : 7, Step : 4150, Loss : 0.31464, Acc : 0.872, Sensitive_Loss : 0.07084, Sensitive_Acc : 18.200, Run Time : 11.01 sec
INFO:root:2024-04-27 17:34:51, Train, Epoch : 7, Step : 4160, Loss : 0.37655, Acc : 0.834, Sensitive_Loss : 0.06705, Sensitive_Acc : 17.000, Run Time : 11.13 sec
INFO:root:2024-04-27 17:35:03, Train, Epoch : 7, Step : 4170, Loss : 0.29881, Acc : 0.853, Sensitive_Loss : 0.05996, Sensitive_Acc : 17.200, Run Time : 11.37 sec
INFO:root:2024-04-27 17:35:15, Train, Epoch : 7, Step : 4180, Loss : 0.30802, Acc : 0.875, Sensitive_Loss : 0.05448, Sensitive_Acc : 15.600, Run Time : 11.94 sec
INFO:root:2024-04-27 17:35:25, Train, Epoch : 7, Step : 4190, Loss : 0.36448, Acc : 0.872, Sensitive_Loss : 0.10798, Sensitive_Acc : 15.500, Run Time : 10.27 sec
INFO:root:2024-04-27 17:35:37, Train, Epoch : 7, Step : 4200, Loss : 0.29020, Acc : 0.881, Sensitive_Loss : 0.11255, Sensitive_Acc : 17.400, Run Time : 12.46 sec
INFO:root:2024-04-27 17:38:12, Dev, Step : 4200, Loss : 0.42053, Acc : 0.826, Auc : 0.901, Sensitive_Loss : 0.13341, Sensitive_Acc : 16.850, Sensitive_Auc : 0.984, Mean auc: 0.901, Run Time : 154.12 sec
INFO:root:2024-04-27 17:38:21, Train, Epoch : 7, Step : 4210, Loss : 0.30423, Acc : 0.872, Sensitive_Loss : 0.07558, Sensitive_Acc : 16.300, Run Time : 163.26 sec
INFO:root:2024-04-27 17:38:32, Train, Epoch : 7, Step : 4220, Loss : 0.32946, Acc : 0.872, Sensitive_Loss : 0.06901, Sensitive_Acc : 14.800, Run Time : 11.22 sec
INFO:root:2024-04-27 17:38:44, Train, Epoch : 7, Step : 4230, Loss : 0.38066, Acc : 0.828, Sensitive_Loss : 0.08480, Sensitive_Acc : 15.600, Run Time : 12.04 sec
INFO:root:2024-04-27 17:38:55, Train, Epoch : 7, Step : 4240, Loss : 0.34741, Acc : 0.863, Sensitive_Loss : 0.05355, Sensitive_Acc : 16.300, Run Time : 11.50 sec
INFO:root:2024-04-27 17:39:06, Train, Epoch : 7, Step : 4250, Loss : 0.26460, Acc : 0.894, Sensitive_Loss : 0.04568, Sensitive_Acc : 17.500, Run Time : 10.72 sec
INFO:root:2024-04-27 17:39:18, Train, Epoch : 7, Step : 4260, Loss : 0.34065, Acc : 0.863, Sensitive_Loss : 0.09736, Sensitive_Acc : 18.100, Run Time : 11.56 sec
INFO:root:2024-04-27 17:39:28, Train, Epoch : 7, Step : 4270, Loss : 0.27137, Acc : 0.887, Sensitive_Loss : 0.09349, Sensitive_Acc : 15.400, Run Time : 10.37 sec
INFO:root:2024-04-27 17:39:40, Train, Epoch : 7, Step : 4280, Loss : 0.30472, Acc : 0.866, Sensitive_Loss : 0.06059, Sensitive_Acc : 16.300, Run Time : 12.20 sec
INFO:root:2024-04-27 17:39:52, Train, Epoch : 7, Step : 4290, Loss : 0.31398, Acc : 0.859, Sensitive_Loss : 0.07014, Sensitive_Acc : 15.700, Run Time : 11.21 sec
INFO:root:2024-04-27 17:40:03, Train, Epoch : 7, Step : 4300, Loss : 0.32874, Acc : 0.834, Sensitive_Loss : 0.06811, Sensitive_Acc : 18.600, Run Time : 11.30 sec
INFO:root:2024-04-27 17:42:38, Dev, Step : 4300, Loss : 0.43719, Acc : 0.824, Auc : 0.903, Sensitive_Loss : 0.11239, Sensitive_Acc : 16.850, Sensitive_Auc : 0.989, Mean auc: 0.903, Run Time : 155.00 sec
INFO:root:2024-04-27 17:42:46, Train, Epoch : 7, Step : 4310, Loss : 0.34396, Acc : 0.847, Sensitive_Loss : 0.07807, Sensitive_Acc : 18.200, Run Time : 163.63 sec
INFO:root:2024-04-27 17:42:59, Train, Epoch : 7, Step : 4320, Loss : 0.29837, Acc : 0.859, Sensitive_Loss : 0.05615, Sensitive_Acc : 17.200, Run Time : 12.24 sec
INFO:root:2024-04-27 17:43:10, Train, Epoch : 7, Step : 4330, Loss : 0.28567, Acc : 0.875, Sensitive_Loss : 0.04599, Sensitive_Acc : 16.400, Run Time : 11.31 sec
INFO:root:2024-04-27 17:43:21, Train, Epoch : 7, Step : 4340, Loss : 0.33106, Acc : 0.859, Sensitive_Loss : 0.05857, Sensitive_Acc : 18.400, Run Time : 11.07 sec
INFO:root:2024-04-27 17:43:33, Train, Epoch : 7, Step : 4350, Loss : 0.29302, Acc : 0.866, Sensitive_Loss : 0.10628, Sensitive_Acc : 14.700, Run Time : 12.17 sec
INFO:root:2024-04-27 17:43:44, Train, Epoch : 7, Step : 4360, Loss : 0.34429, Acc : 0.844, Sensitive_Loss : 0.09677, Sensitive_Acc : 17.600, Run Time : 10.45 sec
INFO:root:2024-04-27 17:43:55, Train, Epoch : 7, Step : 4370, Loss : 0.35344, Acc : 0.850, Sensitive_Loss : 0.09680, Sensitive_Acc : 16.600, Run Time : 11.21 sec
INFO:root:2024-04-27 17:44:07, Train, Epoch : 7, Step : 4380, Loss : 0.31524, Acc : 0.884, Sensitive_Loss : 0.06456, Sensitive_Acc : 18.100, Run Time : 11.73 sec
INFO:root:2024-04-27 17:46:41
INFO:root:y_pred: [0.16761193 0.9458936  0.02023978 ... 0.81092006 0.03098873 0.8878014 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.78638113e-01 8.08132099e-05 1.26130501e-04 7.55093822e-07
 9.95911717e-01 1.68878887e-06 9.99699712e-01 9.84202623e-01
 1.69852283e-05 9.84821856e-01 9.99392867e-01 9.99657750e-01
 9.98618007e-01 7.73887694e-01 2.12760715e-04 9.75401819e-01
 9.99827504e-01 4.62220574e-04 2.33499721e-01 8.30116749e-01
 9.86354530e-01 7.23492876e-02 9.99835134e-01 8.26352656e-01
 9.82044101e-01 9.02552485e-01 2.01176426e-05 9.89833832e-01
 9.78903651e-01 2.67153740e-01 1.75417426e-05 6.57668337e-03
 6.84482837e-03 1.30046927e-03 1.61678880e-01 2.49162586e-05
 1.18363544e-03 2.10507121e-03 9.80798304e-01 9.88358736e-01
 7.65336608e-06 1.22337806e-04 9.93038595e-01 9.01098392e-05
 9.99688506e-01 9.96703446e-01 9.83879983e-01 9.98976946e-01
 2.02666000e-02 9.78796244e-01 9.98749137e-01 1.89361381e-04
 7.27516133e-03 7.45357611e-05 9.44872081e-05 2.17082654e-03
 3.23499902e-03 4.54787940e-01 3.09404626e-04 2.47128382e-02
 2.95379228e-04 9.93347727e-03 2.59297431e-05 9.96666610e-01
 3.40061937e-03 9.97056127e-01 1.45914033e-04 9.96602893e-01
 8.82142603e-01 7.45138645e-01 9.42557514e-01 2.05283780e-02
 3.24658613e-05 8.59569875e-04 9.00466421e-06 4.78116417e-05
 5.85381627e-01 2.26445243e-01 8.18569788e-06 9.67504919e-01
 9.97453034e-01 1.27563781e-05 1.95704959e-02 5.72388781e-05
 1.23715125e-01 9.29166615e-01 6.25418946e-02 1.16016821e-03
 9.46796775e-01 9.97463584e-01 9.96678710e-01 2.55592149e-02
 9.89570748e-04 9.98122990e-01 6.46682143e-01 3.97531039e-05
 9.96478975e-01 9.97919977e-01 4.58901894e-07 4.58744529e-04
 9.72286820e-01 9.81615007e-01 9.98955727e-01 9.94910896e-01
 3.05926842e-05 8.66160088e-04 6.91608369e-01 8.33746672e-01
 9.58133340e-01 3.71771534e-06 6.64334118e-01 9.99538302e-01
 5.17831966e-02 9.98879969e-01 9.80256617e-01 9.52926695e-01
 7.33758688e-01 9.91515040e-01 7.14153722e-02 9.51089799e-01
 9.98701453e-01 9.91639972e-01 4.05750643e-06 8.54934931e-01
 9.97038126e-01 7.49397231e-03 9.92263079e-01 8.81954795e-04
 5.14853047e-04 9.93445396e-01 9.87868607e-01 2.87737144e-04
 1.38836598e-03 2.14685127e-03 9.90354776e-01 9.99636292e-01
 9.64361548e-01 4.08967258e-04 7.79597438e-04 9.60484028e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 17:46:41, Dev, Step : 4382, Loss : 0.41908, Acc : 0.824, Auc : 0.902, Sensitive_Loss : 0.09950, Sensitive_Acc : 16.864, Sensitive_Auc : 0.992, Mean auc: 0.902, Run Time : 152.71 sec
INFO:root:2024-04-27 17:46:53, Train, Epoch : 8, Step : 4390, Loss : 0.23346, Acc : 0.697, Sensitive_Loss : 0.04406, Sensitive_Acc : 13.400, Run Time : 11.28 sec
INFO:root:2024-04-27 17:47:04, Train, Epoch : 8, Step : 4400, Loss : 0.31928, Acc : 0.869, Sensitive_Loss : 0.03032, Sensitive_Acc : 17.200, Run Time : 10.98 sec
INFO:root:2024-04-27 17:49:39, Dev, Step : 4400, Loss : 0.43106, Acc : 0.828, Auc : 0.903, Sensitive_Loss : 0.11268, Sensitive_Acc : 16.921, Sensitive_Auc : 0.991, Mean auc: 0.903, Run Time : 154.90 sec
INFO:root:2024-04-27 17:49:48, Train, Epoch : 8, Step : 4410, Loss : 0.28524, Acc : 0.863, Sensitive_Loss : 0.04483, Sensitive_Acc : 15.200, Run Time : 163.97 sec
INFO:root:2024-04-27 17:50:01, Train, Epoch : 8, Step : 4420, Loss : 0.27309, Acc : 0.891, Sensitive_Loss : 0.06790, Sensitive_Acc : 18.000, Run Time : 12.48 sec
INFO:root:2024-04-27 17:50:11, Train, Epoch : 8, Step : 4430, Loss : 0.32733, Acc : 0.856, Sensitive_Loss : 0.06926, Sensitive_Acc : 15.800, Run Time : 10.36 sec
INFO:root:2024-04-27 17:50:22, Train, Epoch : 8, Step : 4440, Loss : 0.27524, Acc : 0.881, Sensitive_Loss : 0.09311, Sensitive_Acc : 16.800, Run Time : 11.05 sec
INFO:root:2024-04-27 17:50:34, Train, Epoch : 8, Step : 4450, Loss : 0.31276, Acc : 0.878, Sensitive_Loss : 0.07186, Sensitive_Acc : 15.800, Run Time : 11.97 sec
INFO:root:2024-04-27 17:50:47, Train, Epoch : 8, Step : 4460, Loss : 0.29895, Acc : 0.891, Sensitive_Loss : 0.05133, Sensitive_Acc : 15.400, Run Time : 12.25 sec
INFO:root:2024-04-27 17:50:56, Train, Epoch : 8, Step : 4470, Loss : 0.32626, Acc : 0.850, Sensitive_Loss : 0.07841, Sensitive_Acc : 14.600, Run Time : 9.57 sec
INFO:root:2024-04-27 17:51:08, Train, Epoch : 8, Step : 4480, Loss : 0.28518, Acc : 0.884, Sensitive_Loss : 0.05814, Sensitive_Acc : 17.300, Run Time : 12.30 sec
INFO:root:2024-04-27 17:51:19, Train, Epoch : 8, Step : 4490, Loss : 0.30884, Acc : 0.894, Sensitive_Loss : 0.08332, Sensitive_Acc : 17.400, Run Time : 11.02 sec
INFO:root:2024-04-27 17:51:31, Train, Epoch : 8, Step : 4500, Loss : 0.33043, Acc : 0.869, Sensitive_Loss : 0.08551, Sensitive_Acc : 16.800, Run Time : 11.52 sec
INFO:root:2024-04-27 17:54:06, Dev, Step : 4500, Loss : 0.42612, Acc : 0.826, Auc : 0.903, Sensitive_Loss : 0.11986, Sensitive_Acc : 16.850, Sensitive_Auc : 0.989, Mean auc: 0.903, Run Time : 155.14 sec
INFO:root:2024-04-27 17:54:15, Train, Epoch : 8, Step : 4510, Loss : 0.29611, Acc : 0.875, Sensitive_Loss : 0.08056, Sensitive_Acc : 16.000, Run Time : 164.26 sec
INFO:root:2024-04-27 17:54:27, Train, Epoch : 8, Step : 4520, Loss : 0.33640, Acc : 0.869, Sensitive_Loss : 0.05015, Sensitive_Acc : 16.500, Run Time : 11.54 sec
INFO:root:2024-04-27 17:54:38, Train, Epoch : 8, Step : 4530, Loss : 0.27061, Acc : 0.909, Sensitive_Loss : 0.06629, Sensitive_Acc : 17.700, Run Time : 11.52 sec
INFO:root:2024-04-27 17:54:50, Train, Epoch : 8, Step : 4540, Loss : 0.40131, Acc : 0.838, Sensitive_Loss : 0.08366, Sensitive_Acc : 16.800, Run Time : 11.63 sec
INFO:root:2024-04-27 17:55:01, Train, Epoch : 8, Step : 4550, Loss : 0.33019, Acc : 0.866, Sensitive_Loss : 0.08086, Sensitive_Acc : 15.500, Run Time : 11.30 sec
INFO:root:2024-04-27 17:55:13, Train, Epoch : 8, Step : 4560, Loss : 0.28427, Acc : 0.891, Sensitive_Loss : 0.06235, Sensitive_Acc : 17.100, Run Time : 11.57 sec
INFO:root:2024-04-27 17:55:23, Train, Epoch : 8, Step : 4570, Loss : 0.24570, Acc : 0.897, Sensitive_Loss : 0.05403, Sensitive_Acc : 17.500, Run Time : 10.45 sec
INFO:root:2024-04-27 17:55:35, Train, Epoch : 8, Step : 4580, Loss : 0.35334, Acc : 0.844, Sensitive_Loss : 0.04170, Sensitive_Acc : 15.400, Run Time : 11.39 sec
INFO:root:2024-04-27 17:55:46, Train, Epoch : 8, Step : 4590, Loss : 0.31197, Acc : 0.884, Sensitive_Loss : 0.05881, Sensitive_Acc : 15.900, Run Time : 11.64 sec
INFO:root:2024-04-27 17:55:58, Train, Epoch : 8, Step : 4600, Loss : 0.24719, Acc : 0.897, Sensitive_Loss : 0.07319, Sensitive_Acc : 16.900, Run Time : 11.71 sec
INFO:root:2024-04-27 17:58:32, Dev, Step : 4600, Loss : 0.43609, Acc : 0.820, Auc : 0.902, Sensitive_Loss : 0.10916, Sensitive_Acc : 16.893, Sensitive_Auc : 0.991, Mean auc: 0.902, Run Time : 154.26 sec
INFO:root:2024-04-27 17:58:41, Train, Epoch : 8, Step : 4610, Loss : 0.30710, Acc : 0.878, Sensitive_Loss : 0.11841, Sensitive_Acc : 16.600, Run Time : 163.23 sec
INFO:root:2024-04-27 17:58:53, Train, Epoch : 8, Step : 4620, Loss : 0.26793, Acc : 0.900, Sensitive_Loss : 0.05876, Sensitive_Acc : 17.700, Run Time : 11.69 sec
INFO:root:2024-04-27 17:59:04, Train, Epoch : 8, Step : 4630, Loss : 0.35223, Acc : 0.850, Sensitive_Loss : 0.05859, Sensitive_Acc : 15.000, Run Time : 11.56 sec
INFO:root:2024-04-27 17:59:15, Train, Epoch : 8, Step : 4640, Loss : 0.27905, Acc : 0.900, Sensitive_Loss : 0.05843, Sensitive_Acc : 16.000, Run Time : 10.39 sec
INFO:root:2024-04-27 17:59:26, Train, Epoch : 8, Step : 4650, Loss : 0.25312, Acc : 0.906, Sensitive_Loss : 0.06093, Sensitive_Acc : 16.600, Run Time : 11.65 sec
INFO:root:2024-04-27 17:59:37, Train, Epoch : 8, Step : 4660, Loss : 0.30977, Acc : 0.894, Sensitive_Loss : 0.03962, Sensitive_Acc : 15.500, Run Time : 10.85 sec
INFO:root:2024-04-27 17:59:49, Train, Epoch : 8, Step : 4670, Loss : 0.27318, Acc : 0.894, Sensitive_Loss : 0.05246, Sensitive_Acc : 17.000, Run Time : 11.63 sec
INFO:root:2024-04-27 18:00:00, Train, Epoch : 8, Step : 4680, Loss : 0.27456, Acc : 0.859, Sensitive_Loss : 0.04852, Sensitive_Acc : 16.800, Run Time : 11.25 sec
INFO:root:2024-04-27 18:00:12, Train, Epoch : 8, Step : 4690, Loss : 0.22973, Acc : 0.887, Sensitive_Loss : 0.07485, Sensitive_Acc : 15.400, Run Time : 11.41 sec
INFO:root:2024-04-27 18:00:23, Train, Epoch : 8, Step : 4700, Loss : 0.22660, Acc : 0.906, Sensitive_Loss : 0.06203, Sensitive_Acc : 15.100, Run Time : 11.85 sec
INFO:root:2024-04-27 18:02:57, Dev, Step : 4700, Loss : 0.44092, Acc : 0.820, Auc : 0.903, Sensitive_Loss : 0.11946, Sensitive_Acc : 16.879, Sensitive_Auc : 0.990, Mean auc: 0.903, Run Time : 153.58 sec
INFO:root:2024-04-27 18:03:06, Train, Epoch : 8, Step : 4710, Loss : 0.37644, Acc : 0.853, Sensitive_Loss : 0.05504, Sensitive_Acc : 17.800, Run Time : 162.38 sec
INFO:root:2024-04-27 18:03:17, Train, Epoch : 8, Step : 4720, Loss : 0.26491, Acc : 0.887, Sensitive_Loss : 0.09147, Sensitive_Acc : 17.300, Run Time : 11.47 sec
INFO:root:2024-04-27 18:03:28, Train, Epoch : 8, Step : 4730, Loss : 0.30237, Acc : 0.844, Sensitive_Loss : 0.07619, Sensitive_Acc : 16.300, Run Time : 10.74 sec
INFO:root:2024-04-27 18:03:39, Train, Epoch : 8, Step : 4740, Loss : 0.23900, Acc : 0.884, Sensitive_Loss : 0.05298, Sensitive_Acc : 15.600, Run Time : 11.27 sec
INFO:root:2024-04-27 18:03:51, Train, Epoch : 8, Step : 4750, Loss : 0.34215, Acc : 0.869, Sensitive_Loss : 0.07437, Sensitive_Acc : 16.300, Run Time : 11.80 sec
INFO:root:2024-04-27 18:04:03, Train, Epoch : 8, Step : 4760, Loss : 0.30955, Acc : 0.838, Sensitive_Loss : 0.05202, Sensitive_Acc : 15.900, Run Time : 11.63 sec
INFO:root:2024-04-27 18:04:14, Train, Epoch : 8, Step : 4770, Loss : 0.28791, Acc : 0.875, Sensitive_Loss : 0.07634, Sensitive_Acc : 17.000, Run Time : 11.65 sec
INFO:root:2024-04-27 18:04:26, Train, Epoch : 8, Step : 4780, Loss : 0.24003, Acc : 0.891, Sensitive_Loss : 0.08814, Sensitive_Acc : 16.800, Run Time : 11.30 sec
INFO:root:2024-04-27 18:04:37, Train, Epoch : 8, Step : 4790, Loss : 0.28931, Acc : 0.859, Sensitive_Loss : 0.07461, Sensitive_Acc : 15.900, Run Time : 11.29 sec
INFO:root:2024-04-27 18:04:48, Train, Epoch : 8, Step : 4800, Loss : 0.33861, Acc : 0.859, Sensitive_Loss : 0.05949, Sensitive_Acc : 16.400, Run Time : 11.13 sec
INFO:root:2024-04-27 18:07:23, Dev, Step : 4800, Loss : 0.42784, Acc : 0.820, Auc : 0.903, Sensitive_Loss : 0.11548, Sensitive_Acc : 16.893, Sensitive_Auc : 0.991, Mean auc: 0.903, Run Time : 155.20 sec
INFO:root:2024-04-27 18:07:32, Train, Epoch : 8, Step : 4810, Loss : 0.35867, Acc : 0.850, Sensitive_Loss : 0.09270, Sensitive_Acc : 17.400, Run Time : 163.32 sec
INFO:root:2024-04-27 18:07:44, Train, Epoch : 8, Step : 4820, Loss : 0.26025, Acc : 0.903, Sensitive_Loss : 0.06368, Sensitive_Acc : 16.800, Run Time : 12.20 sec
INFO:root:2024-04-27 18:07:54, Train, Epoch : 8, Step : 4830, Loss : 0.29076, Acc : 0.878, Sensitive_Loss : 0.07413, Sensitive_Acc : 17.000, Run Time : 10.75 sec
INFO:root:2024-04-27 18:08:06, Train, Epoch : 8, Step : 4840, Loss : 0.28127, Acc : 0.903, Sensitive_Loss : 0.05498, Sensitive_Acc : 16.300, Run Time : 11.79 sec
INFO:root:2024-04-27 18:08:18, Train, Epoch : 8, Step : 4850, Loss : 0.29176, Acc : 0.878, Sensitive_Loss : 0.05796, Sensitive_Acc : 17.600, Run Time : 11.36 sec
INFO:root:2024-04-27 18:08:29, Train, Epoch : 8, Step : 4860, Loss : 0.25537, Acc : 0.897, Sensitive_Loss : 0.09104, Sensitive_Acc : 16.600, Run Time : 11.44 sec
INFO:root:2024-04-27 18:08:39, Train, Epoch : 8, Step : 4870, Loss : 0.32915, Acc : 0.850, Sensitive_Loss : 0.03687, Sensitive_Acc : 16.200, Run Time : 10.40 sec
INFO:root:2024-04-27 18:08:50, Train, Epoch : 8, Step : 4880, Loss : 0.28523, Acc : 0.875, Sensitive_Loss : 0.07957, Sensitive_Acc : 15.600, Run Time : 10.92 sec
INFO:root:2024-04-27 18:09:03, Train, Epoch : 8, Step : 4890, Loss : 0.29806, Acc : 0.866, Sensitive_Loss : 0.09249, Sensitive_Acc : 16.300, Run Time : 12.23 sec
INFO:root:2024-04-27 18:09:13, Train, Epoch : 8, Step : 4900, Loss : 0.30315, Acc : 0.872, Sensitive_Loss : 0.06415, Sensitive_Acc : 17.600, Run Time : 10.73 sec
INFO:root:2024-04-27 18:11:50, Dev, Step : 4900, Loss : 0.45651, Acc : 0.815, Auc : 0.902, Sensitive_Loss : 0.11297, Sensitive_Acc : 16.893, Sensitive_Auc : 0.991, Mean auc: 0.902, Run Time : 156.28 sec
INFO:root:2024-04-27 18:11:58, Train, Epoch : 8, Step : 4910, Loss : 0.25630, Acc : 0.875, Sensitive_Loss : 0.07619, Sensitive_Acc : 16.200, Run Time : 164.89 sec
INFO:root:2024-04-27 18:12:10, Train, Epoch : 8, Step : 4920, Loss : 0.27771, Acc : 0.875, Sensitive_Loss : 0.04788, Sensitive_Acc : 16.900, Run Time : 11.67 sec
INFO:root:2024-04-27 18:12:21, Train, Epoch : 8, Step : 4930, Loss : 0.24082, Acc : 0.909, Sensitive_Loss : 0.07144, Sensitive_Acc : 15.100, Run Time : 11.59 sec
INFO:root:2024-04-27 18:12:33, Train, Epoch : 8, Step : 4940, Loss : 0.29090, Acc : 0.881, Sensitive_Loss : 0.07071, Sensitive_Acc : 16.000, Run Time : 11.27 sec
INFO:root:2024-04-27 18:12:44, Train, Epoch : 8, Step : 4950, Loss : 0.30522, Acc : 0.844, Sensitive_Loss : 0.05668, Sensitive_Acc : 16.100, Run Time : 11.54 sec
INFO:root:2024-04-27 18:12:55, Train, Epoch : 8, Step : 4960, Loss : 0.34078, Acc : 0.875, Sensitive_Loss : 0.07667, Sensitive_Acc : 16.300, Run Time : 10.42 sec
INFO:root:2024-04-27 18:13:06, Train, Epoch : 8, Step : 4970, Loss : 0.25731, Acc : 0.887, Sensitive_Loss : 0.07333, Sensitive_Acc : 17.400, Run Time : 11.57 sec
INFO:root:2024-04-27 18:13:18, Train, Epoch : 8, Step : 4980, Loss : 0.34961, Acc : 0.872, Sensitive_Loss : 0.08333, Sensitive_Acc : 17.400, Run Time : 12.16 sec
INFO:root:2024-04-27 18:13:30, Train, Epoch : 8, Step : 4990, Loss : 0.28042, Acc : 0.891, Sensitive_Loss : 0.06511, Sensitive_Acc : 15.400, Run Time : 11.63 sec
INFO:root:2024-04-27 18:13:42, Train, Epoch : 8, Step : 5000, Loss : 0.26697, Acc : 0.884, Sensitive_Loss : 0.05908, Sensitive_Acc : 15.600, Run Time : 11.73 sec
INFO:root:2024-04-27 18:16:16, Dev, Step : 5000, Loss : 0.44747, Acc : 0.818, Auc : 0.898, Sensitive_Loss : 0.11167, Sensitive_Acc : 16.850, Sensitive_Auc : 0.991, Mean auc: 0.898, Run Time : 153.95 sec
INFO:root:2024-04-27 18:18:55
INFO:root:y_pred: [0.1419827  0.9505609  0.02118989 ... 0.75774264 0.12815724 0.8985002 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.8754710e-01 3.6231795e-05 3.2652821e-04 1.5846393e-07 9.9773538e-01
 1.5953751e-07 9.9957603e-01 9.7009695e-01 7.0974343e-06 9.8428118e-01
 9.9848562e-01 9.9990475e-01 9.9939001e-01 8.1393147e-01 1.3643112e-04
 9.7929013e-01 9.9991572e-01 1.3677526e-04 7.4830109e-01 9.5914584e-01
 9.8675621e-01 3.1370424e-02 9.9991536e-01 8.6682224e-01 9.8029411e-01
 8.4479177e-01 7.5254888e-05 9.9434018e-01 9.8245174e-01 5.5394483e-01
 2.8999912e-05 6.0647607e-02 7.4131438e-03 2.1649676e-03 2.7465010e-01
 1.5974585e-05 5.4530641e-03 1.0909863e-02 9.7034079e-01 9.9209315e-01
 2.1740377e-06 3.2984342e-05 9.9211651e-01 1.0474105e-04 9.9961215e-01
 9.9524492e-01 9.6802217e-01 9.9875307e-01 8.4863417e-03 9.8833072e-01
 9.9903488e-01 3.6323891e-05 1.4738747e-02 1.6979333e-05 1.1796900e-05
 2.4990707e-03 8.9788586e-03 6.1125863e-01 2.4485792e-04 4.0366545e-02
 2.6254728e-04 2.2947493e-03 2.8080884e-05 9.9351054e-01 1.0679419e-03
 9.9737275e-01 2.2548089e-05 9.9497235e-01 8.6474115e-01 9.3516678e-01
 8.9123106e-01 1.0232148e-01 1.6126103e-05 6.4252719e-04 1.0582918e-05
 2.6287831e-05 7.9557675e-01 1.5986907e-01 3.4155075e-06 9.5138943e-01
 9.9835950e-01 3.2249486e-06 1.8385810e-01 1.6350410e-05 3.2858685e-01
 9.6930683e-01 5.8147162e-02 2.4477199e-03 9.6604472e-01 9.9696153e-01
 9.9606556e-01 1.0189595e-01 5.1027734e-04 9.9933976e-01 6.5794241e-01
 2.9803708e-05 9.9327612e-01 9.9703908e-01 2.4422147e-07 1.8134526e-03
 9.8140025e-01 9.4864535e-01 9.9895823e-01 9.9658173e-01 8.1288090e-06
 1.4340695e-03 6.8686682e-01 8.5035765e-01 9.6875727e-01 4.9781551e-07
 6.5378886e-01 9.9961382e-01 2.3325875e-02 9.9764001e-01 9.8269039e-01
 9.5642376e-01 9.1704631e-01 9.9632972e-01 2.9287934e-02 9.6834522e-01
 9.9885988e-01 9.9114579e-01 1.8331457e-07 8.2386124e-01 9.9702877e-01
 4.7266013e-03 9.8989290e-01 5.7404028e-04 1.0006197e-03 9.9632072e-01
 9.9218339e-01 2.3104557e-04 6.4504158e-04 2.1321664e-03 9.7927940e-01
 9.9970478e-01 9.5464808e-01 7.7123084e-05 9.8953594e-04 9.4673306e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 18:18:55, Dev, Step : 5008, Loss : 0.43282, Acc : 0.820, Auc : 0.898, Sensitive_Loss : 0.11722, Sensitive_Acc : 16.850, Sensitive_Auc : 0.990, Mean auc: 0.898, Run Time : 153.02 sec
INFO:root:2024-04-27 18:19:00, Train, Epoch : 9, Step : 5010, Loss : 0.06508, Acc : 0.169, Sensitive_Loss : 0.00817, Sensitive_Acc : 3.200, Run Time : 4.17 sec
INFO:root:2024-04-27 18:19:11, Train, Epoch : 9, Step : 5020, Loss : 0.32775, Acc : 0.844, Sensitive_Loss : 0.08032, Sensitive_Acc : 15.900, Run Time : 11.88 sec
INFO:root:2024-04-27 18:19:22, Train, Epoch : 9, Step : 5030, Loss : 0.22196, Acc : 0.891, Sensitive_Loss : 0.09109, Sensitive_Acc : 16.200, Run Time : 10.67 sec
INFO:root:2024-04-27 18:19:34, Train, Epoch : 9, Step : 5040, Loss : 0.26932, Acc : 0.897, Sensitive_Loss : 0.06318, Sensitive_Acc : 17.400, Run Time : 12.30 sec
INFO:root:2024-04-27 18:19:45, Train, Epoch : 9, Step : 5050, Loss : 0.31000, Acc : 0.881, Sensitive_Loss : 0.05054, Sensitive_Acc : 16.200, Run Time : 10.89 sec
INFO:root:2024-04-27 18:19:57, Train, Epoch : 9, Step : 5060, Loss : 0.31066, Acc : 0.853, Sensitive_Loss : 0.05411, Sensitive_Acc : 15.900, Run Time : 11.99 sec
INFO:root:2024-04-27 18:20:09, Train, Epoch : 9, Step : 5070, Loss : 0.24114, Acc : 0.909, Sensitive_Loss : 0.06926, Sensitive_Acc : 16.700, Run Time : 11.19 sec
INFO:root:2024-04-27 18:20:19, Train, Epoch : 9, Step : 5080, Loss : 0.25141, Acc : 0.887, Sensitive_Loss : 0.05401, Sensitive_Acc : 16.300, Run Time : 10.34 sec
INFO:root:2024-04-27 18:20:30, Train, Epoch : 9, Step : 5090, Loss : 0.35522, Acc : 0.856, Sensitive_Loss : 0.03245, Sensitive_Acc : 15.600, Run Time : 11.32 sec
INFO:root:2024-04-27 18:20:42, Train, Epoch : 9, Step : 5100, Loss : 0.29322, Acc : 0.866, Sensitive_Loss : 0.06988, Sensitive_Acc : 16.000, Run Time : 11.42 sec
INFO:root:2024-04-27 18:23:16, Dev, Step : 5100, Loss : 0.42382, Acc : 0.828, Auc : 0.903, Sensitive_Loss : 0.10516, Sensitive_Acc : 16.864, Sensitive_Auc : 0.991, Mean auc: 0.903, Run Time : 154.87 sec
INFO:root:2024-04-27 18:23:25, Train, Epoch : 9, Step : 5110, Loss : 0.32018, Acc : 0.875, Sensitive_Loss : 0.04578, Sensitive_Acc : 15.200, Run Time : 163.47 sec
INFO:root:2024-04-27 18:23:37, Train, Epoch : 9, Step : 5120, Loss : 0.29557, Acc : 0.881, Sensitive_Loss : 0.05352, Sensitive_Acc : 17.000, Run Time : 11.84 sec
INFO:root:2024-04-27 18:23:50, Train, Epoch : 9, Step : 5130, Loss : 0.30325, Acc : 0.866, Sensitive_Loss : 0.05572, Sensitive_Acc : 17.000, Run Time : 12.62 sec
INFO:root:2024-04-27 18:24:00, Train, Epoch : 9, Step : 5140, Loss : 0.22580, Acc : 0.887, Sensitive_Loss : 0.05079, Sensitive_Acc : 15.600, Run Time : 10.08 sec
INFO:root:2024-04-27 18:24:11, Train, Epoch : 9, Step : 5150, Loss : 0.24501, Acc : 0.912, Sensitive_Loss : 0.07208, Sensitive_Acc : 16.900, Run Time : 11.51 sec
INFO:root:2024-04-27 18:24:22, Train, Epoch : 9, Step : 5160, Loss : 0.25547, Acc : 0.875, Sensitive_Loss : 0.05324, Sensitive_Acc : 16.600, Run Time : 11.28 sec
INFO:root:2024-04-27 18:24:35, Train, Epoch : 9, Step : 5170, Loss : 0.26790, Acc : 0.869, Sensitive_Loss : 0.07778, Sensitive_Acc : 16.400, Run Time : 12.77 sec
INFO:root:2024-04-27 18:24:46, Train, Epoch : 9, Step : 5180, Loss : 0.28154, Acc : 0.881, Sensitive_Loss : 0.06977, Sensitive_Acc : 15.800, Run Time : 10.54 sec
INFO:root:2024-04-27 18:24:57, Train, Epoch : 9, Step : 5190, Loss : 0.22782, Acc : 0.906, Sensitive_Loss : 0.04597, Sensitive_Acc : 17.000, Run Time : 11.63 sec
INFO:root:2024-04-27 18:25:09, Train, Epoch : 9, Step : 5200, Loss : 0.28979, Acc : 0.875, Sensitive_Loss : 0.03455, Sensitive_Acc : 16.400, Run Time : 11.57 sec
INFO:root:2024-04-27 18:28:37, Dev, Step : 5200, Loss : 0.45154, Acc : 0.819, Auc : 0.901, Sensitive_Loss : 0.12860, Sensitive_Acc : 16.850, Sensitive_Auc : 0.988, Mean auc: 0.901, Run Time : 208.02 sec
INFO:root:2024-04-27 18:28:46, Train, Epoch : 9, Step : 5210, Loss : 0.28495, Acc : 0.866, Sensitive_Loss : 0.04316, Sensitive_Acc : 16.700, Run Time : 216.56 sec
INFO:root:2024-04-27 18:28:57, Train, Epoch : 9, Step : 5220, Loss : 0.22126, Acc : 0.903, Sensitive_Loss : 0.05517, Sensitive_Acc : 16.400, Run Time : 11.53 sec
INFO:root:2024-04-27 18:29:08, Train, Epoch : 9, Step : 5230, Loss : 0.29565, Acc : 0.850, Sensitive_Loss : 0.07098, Sensitive_Acc : 16.200, Run Time : 11.28 sec
INFO:root:2024-04-27 18:29:20, Train, Epoch : 9, Step : 5240, Loss : 0.24381, Acc : 0.912, Sensitive_Loss : 0.05625, Sensitive_Acc : 17.100, Run Time : 12.04 sec
INFO:root:2024-04-27 18:29:32, Train, Epoch : 9, Step : 5250, Loss : 0.31378, Acc : 0.856, Sensitive_Loss : 0.05233, Sensitive_Acc : 16.800, Run Time : 11.80 sec
INFO:root:2024-04-27 18:29:44, Train, Epoch : 9, Step : 5260, Loss : 0.28700, Acc : 0.853, Sensitive_Loss : 0.09923, Sensitive_Acc : 16.500, Run Time : 11.59 sec
INFO:root:2024-04-27 18:29:55, Train, Epoch : 9, Step : 5270, Loss : 0.27585, Acc : 0.887, Sensitive_Loss : 0.07751, Sensitive_Acc : 16.400, Run Time : 11.22 sec
INFO:root:2024-04-27 18:30:07, Train, Epoch : 9, Step : 5280, Loss : 0.34923, Acc : 0.853, Sensitive_Loss : 0.06619, Sensitive_Acc : 16.300, Run Time : 11.44 sec
INFO:root:2024-04-27 18:30:18, Train, Epoch : 9, Step : 5290, Loss : 0.35423, Acc : 0.850, Sensitive_Loss : 0.07193, Sensitive_Acc : 17.100, Run Time : 11.43 sec
INFO:root:2024-04-27 18:30:30, Train, Epoch : 9, Step : 5300, Loss : 0.36415, Acc : 0.872, Sensitive_Loss : 0.06699, Sensitive_Acc : 15.500, Run Time : 11.65 sec
INFO:root:2024-04-27 18:33:10, Dev, Step : 5300, Loss : 0.43074, Acc : 0.828, Auc : 0.900, Sensitive_Loss : 0.10984, Sensitive_Acc : 16.864, Sensitive_Auc : 0.989, Mean auc: 0.900, Run Time : 160.37 sec
INFO:root:2024-04-27 18:33:18, Train, Epoch : 9, Step : 5310, Loss : 0.30783, Acc : 0.866, Sensitive_Loss : 0.08735, Sensitive_Acc : 17.400, Run Time : 168.66 sec
INFO:root:2024-04-27 18:33:30, Train, Epoch : 9, Step : 5320, Loss : 0.33098, Acc : 0.856, Sensitive_Loss : 0.07112, Sensitive_Acc : 16.000, Run Time : 11.62 sec
INFO:root:2024-04-27 18:33:42, Train, Epoch : 9, Step : 5330, Loss : 0.21271, Acc : 0.912, Sensitive_Loss : 0.06786, Sensitive_Acc : 17.100, Run Time : 11.63 sec
INFO:root:2024-04-27 18:33:53, Train, Epoch : 9, Step : 5340, Loss : 0.24919, Acc : 0.887, Sensitive_Loss : 0.05924, Sensitive_Acc : 17.100, Run Time : 11.05 sec
INFO:root:2024-04-27 18:34:03, Train, Epoch : 9, Step : 5350, Loss : 0.32358, Acc : 0.866, Sensitive_Loss : 0.09726, Sensitive_Acc : 16.400, Run Time : 10.94 sec
INFO:root:2024-04-27 18:34:16, Train, Epoch : 9, Step : 5360, Loss : 0.28433, Acc : 0.863, Sensitive_Loss : 0.05333, Sensitive_Acc : 17.100, Run Time : 12.03 sec
INFO:root:2024-04-27 18:34:27, Train, Epoch : 9, Step : 5370, Loss : 0.22889, Acc : 0.903, Sensitive_Loss : 0.06431, Sensitive_Acc : 16.200, Run Time : 11.66 sec
INFO:root:2024-04-27 18:34:39, Train, Epoch : 9, Step : 5380, Loss : 0.31584, Acc : 0.887, Sensitive_Loss : 0.07081, Sensitive_Acc : 16.900, Run Time : 11.37 sec
INFO:root:2024-04-27 18:34:50, Train, Epoch : 9, Step : 5390, Loss : 0.24829, Acc : 0.906, Sensitive_Loss : 0.08102, Sensitive_Acc : 16.600, Run Time : 11.15 sec
INFO:root:2024-04-27 18:35:02, Train, Epoch : 9, Step : 5400, Loss : 0.26307, Acc : 0.881, Sensitive_Loss : 0.06192, Sensitive_Acc : 15.900, Run Time : 12.16 sec
INFO:root:2024-04-27 18:37:37, Dev, Step : 5400, Loss : 0.44843, Acc : 0.818, Auc : 0.899, Sensitive_Loss : 0.12526, Sensitive_Acc : 16.850, Sensitive_Auc : 0.987, Mean auc: 0.899, Run Time : 155.13 sec
INFO:root:2024-04-27 18:37:45, Train, Epoch : 9, Step : 5410, Loss : 0.26325, Acc : 0.906, Sensitive_Loss : 0.07845, Sensitive_Acc : 17.400, Run Time : 163.32 sec
INFO:root:2024-04-27 18:37:57, Train, Epoch : 9, Step : 5420, Loss : 0.27352, Acc : 0.897, Sensitive_Loss : 0.03182, Sensitive_Acc : 17.500, Run Time : 11.60 sec
INFO:root:2024-04-27 18:38:08, Train, Epoch : 9, Step : 5430, Loss : 0.24941, Acc : 0.884, Sensitive_Loss : 0.05543, Sensitive_Acc : 17.500, Run Time : 11.61 sec
INFO:root:2024-04-27 18:38:20, Train, Epoch : 9, Step : 5440, Loss : 0.32175, Acc : 0.850, Sensitive_Loss : 0.07514, Sensitive_Acc : 16.000, Run Time : 11.35 sec
INFO:root:2024-04-27 18:38:31, Train, Epoch : 9, Step : 5450, Loss : 0.25952, Acc : 0.891, Sensitive_Loss : 0.06254, Sensitive_Acc : 18.800, Run Time : 11.22 sec
INFO:root:2024-04-27 18:38:43, Train, Epoch : 9, Step : 5460, Loss : 0.24902, Acc : 0.900, Sensitive_Loss : 0.04842, Sensitive_Acc : 15.900, Run Time : 11.69 sec
INFO:root:2024-04-27 18:38:55, Train, Epoch : 9, Step : 5470, Loss : 0.31877, Acc : 0.847, Sensitive_Loss : 0.05378, Sensitive_Acc : 16.400, Run Time : 12.02 sec
INFO:root:2024-04-27 18:39:06, Train, Epoch : 9, Step : 5480, Loss : 0.30378, Acc : 0.881, Sensitive_Loss : 0.06281, Sensitive_Acc : 15.000, Run Time : 11.28 sec
INFO:root:2024-04-27 18:39:18, Train, Epoch : 9, Step : 5490, Loss : 0.23113, Acc : 0.881, Sensitive_Loss : 0.03778, Sensitive_Acc : 15.200, Run Time : 11.56 sec
INFO:root:2024-04-27 18:39:28, Train, Epoch : 9, Step : 5500, Loss : 0.32887, Acc : 0.853, Sensitive_Loss : 0.04659, Sensitive_Acc : 16.500, Run Time : 10.97 sec
INFO:root:2024-04-27 18:42:04, Dev, Step : 5500, Loss : 0.45903, Acc : 0.816, Auc : 0.903, Sensitive_Loss : 0.12484, Sensitive_Acc : 16.836, Sensitive_Auc : 0.990, Mean auc: 0.903, Run Time : 155.77 sec
INFO:root:2024-04-27 18:42:14, Train, Epoch : 9, Step : 5510, Loss : 0.33643, Acc : 0.872, Sensitive_Loss : 0.05681, Sensitive_Acc : 17.000, Run Time : 165.09 sec
INFO:root:2024-04-27 18:42:25, Train, Epoch : 9, Step : 5520, Loss : 0.28567, Acc : 0.866, Sensitive_Loss : 0.08675, Sensitive_Acc : 15.300, Run Time : 11.29 sec
INFO:root:2024-04-27 18:42:37, Train, Epoch : 9, Step : 5530, Loss : 0.30009, Acc : 0.863, Sensitive_Loss : 0.07883, Sensitive_Acc : 17.000, Run Time : 11.87 sec
INFO:root:2024-04-27 18:42:48, Train, Epoch : 9, Step : 5540, Loss : 0.25934, Acc : 0.859, Sensitive_Loss : 0.08442, Sensitive_Acc : 16.900, Run Time : 11.48 sec
INFO:root:2024-04-27 18:43:00, Train, Epoch : 9, Step : 5550, Loss : 0.33667, Acc : 0.856, Sensitive_Loss : 0.10359, Sensitive_Acc : 14.200, Run Time : 11.60 sec
INFO:root:2024-04-27 18:43:11, Train, Epoch : 9, Step : 5560, Loss : 0.25312, Acc : 0.894, Sensitive_Loss : 0.06577, Sensitive_Acc : 15.300, Run Time : 10.80 sec
INFO:root:2024-04-27 18:43:22, Train, Epoch : 9, Step : 5570, Loss : 0.32119, Acc : 0.869, Sensitive_Loss : 0.10922, Sensitive_Acc : 15.600, Run Time : 11.72 sec
INFO:root:2024-04-27 18:43:33, Train, Epoch : 9, Step : 5580, Loss : 0.29075, Acc : 0.875, Sensitive_Loss : 0.08618, Sensitive_Acc : 17.400, Run Time : 10.75 sec
INFO:root:2024-04-27 18:43:44, Train, Epoch : 9, Step : 5590, Loss : 0.35723, Acc : 0.863, Sensitive_Loss : 0.05822, Sensitive_Acc : 14.100, Run Time : 11.28 sec
INFO:root:2024-04-27 18:43:56, Train, Epoch : 9, Step : 5600, Loss : 0.26304, Acc : 0.859, Sensitive_Loss : 0.08495, Sensitive_Acc : 16.800, Run Time : 11.64 sec
INFO:root:2024-04-27 18:46:32, Dev, Step : 5600, Loss : 0.45024, Acc : 0.824, Auc : 0.904, Sensitive_Loss : 0.11388, Sensitive_Acc : 16.793, Sensitive_Auc : 0.991, Mean auc: 0.904, Run Time : 156.30 sec
INFO:root:2024-04-27 18:46:41, Train, Epoch : 9, Step : 5610, Loss : 0.26165, Acc : 0.875, Sensitive_Loss : 0.04215, Sensitive_Acc : 14.800, Run Time : 164.66 sec
INFO:root:2024-04-27 18:46:52, Train, Epoch : 9, Step : 5620, Loss : 0.33922, Acc : 0.853, Sensitive_Loss : 0.06208, Sensitive_Acc : 15.700, Run Time : 11.52 sec
INFO:root:2024-04-27 18:47:04, Train, Epoch : 9, Step : 5630, Loss : 0.29092, Acc : 0.875, Sensitive_Loss : 0.04924, Sensitive_Acc : 16.600, Run Time : 12.19 sec
INFO:root:2024-04-27 18:49:41
INFO:root:y_pred: [0.12001422 0.9549389  0.00522876 ... 0.6799608  0.02119219 0.86339074]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.88918006e-01 2.19283811e-05 1.16668976e-04 1.16374459e-07
 9.98781383e-01 2.01387152e-07 9.99892235e-01 9.94844317e-01
 2.78427888e-05 9.92117465e-01 9.99280632e-01 9.99946594e-01
 9.99770224e-01 9.08889234e-01 1.10603578e-04 9.96819258e-01
 9.99966264e-01 6.02558721e-05 8.02978218e-01 9.85497952e-01
 9.94017363e-01 4.25514020e-02 9.99909878e-01 9.16524231e-01
 9.92323279e-01 9.37031925e-01 2.33821083e-05 9.98615265e-01
 9.95710135e-01 4.75695968e-01 7.87300542e-06 2.37334650e-02
 5.31070866e-03 2.53406237e-03 4.51129556e-01 1.01856822e-05
 1.51100627e-03 1.43356407e-02 9.87989962e-01 9.98218596e-01
 3.18346702e-06 4.54768197e-05 9.95769501e-01 1.93715150e-05
 9.99848485e-01 9.97655988e-01 9.81407404e-01 9.99616265e-01
 3.31186526e-03 9.92599964e-01 9.99701917e-01 5.89907213e-05
 1.79139208e-02 3.61020793e-05 2.67684900e-05 1.25967909e-03
 6.97952276e-03 7.57986665e-01 6.08613700e-05 6.01415448e-02
 9.92211135e-05 9.50032729e-04 4.54557885e-05 9.97221947e-01
 5.24232222e-04 9.99131858e-01 1.05779211e-04 9.97371078e-01
 9.38607454e-01 9.49625731e-01 9.19065714e-01 7.45270401e-02
 1.66147947e-05 3.24579683e-04 4.06730351e-06 9.25307268e-06
 9.02267933e-01 4.45165515e-01 4.66700499e-07 9.82880056e-01
 9.99288380e-01 2.05183665e-06 1.89337824e-02 1.19041842e-05
 1.63165897e-01 9.85693574e-01 3.13222669e-02 9.57812066e-04
 9.84053254e-01 9.99342859e-01 9.99236465e-01 5.25960922e-02
 7.71596504e-04 9.99790967e-01 6.16476059e-01 1.92158368e-05
 9.97168958e-01 9.99633551e-01 1.56436997e-07 4.67026082e-04
 9.89589870e-01 9.88422573e-01 9.99402404e-01 9.98611927e-01
 7.00422606e-06 2.10295152e-03 8.27811182e-01 9.36504185e-01
 9.84315932e-01 1.52684805e-07 7.69042909e-01 9.99741852e-01
 8.02255645e-02 9.99183357e-01 9.97496545e-01 9.82204139e-01
 9.35488820e-01 9.98987496e-01 1.51631525e-02 9.56235766e-01
 9.99710739e-01 9.97348547e-01 6.48868877e-07 8.88212919e-01
 9.98810053e-01 6.68554101e-03 9.96819019e-01 1.38041459e-03
 3.16959864e-04 9.97919261e-01 9.91765082e-01 1.19843229e-04
 3.81149119e-03 2.04883210e-04 9.91038442e-01 9.99609888e-01
 9.70160127e-01 1.27570849e-04 1.55916833e-03 9.86300826e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 18:49:41, Dev, Step : 5634, Loss : 0.45247, Acc : 0.822, Auc : 0.905, Sensitive_Loss : 0.10856, Sensitive_Acc : 16.893, Sensitive_Auc : 0.992, Mean auc: 0.905, Run Time : 153.40 sec
INFO:root:2024-04-27 18:49:51, Train, Epoch : 10, Step : 5640, Loss : 0.18517, Acc : 0.509, Sensitive_Loss : 0.03218, Sensitive_Acc : 9.300, Run Time : 8.63 sec
INFO:root:2024-04-27 18:50:02, Train, Epoch : 10, Step : 5650, Loss : 0.26962, Acc : 0.891, Sensitive_Loss : 0.05175, Sensitive_Acc : 16.100, Run Time : 11.13 sec
INFO:root:2024-04-27 18:50:14, Train, Epoch : 10, Step : 5660, Loss : 0.22522, Acc : 0.900, Sensitive_Loss : 0.04885, Sensitive_Acc : 15.400, Run Time : 11.67 sec
INFO:root:2024-04-27 18:50:26, Train, Epoch : 10, Step : 5670, Loss : 0.23793, Acc : 0.887, Sensitive_Loss : 0.07283, Sensitive_Acc : 16.200, Run Time : 12.34 sec
INFO:root:2024-04-27 18:50:37, Train, Epoch : 10, Step : 5680, Loss : 0.26251, Acc : 0.881, Sensitive_Loss : 0.04211, Sensitive_Acc : 17.100, Run Time : 11.23 sec
INFO:root:2024-04-27 18:50:49, Train, Epoch : 10, Step : 5690, Loss : 0.29476, Acc : 0.863, Sensitive_Loss : 0.06595, Sensitive_Acc : 18.100, Run Time : 11.72 sec
INFO:root:2024-04-27 18:51:00, Train, Epoch : 10, Step : 5700, Loss : 0.30603, Acc : 0.875, Sensitive_Loss : 0.06611, Sensitive_Acc : 17.100, Run Time : 11.39 sec
INFO:root:2024-04-27 18:53:35, Dev, Step : 5700, Loss : 0.44580, Acc : 0.818, Auc : 0.903, Sensitive_Loss : 0.11133, Sensitive_Acc : 16.893, Sensitive_Auc : 0.990, Mean auc: 0.903, Run Time : 154.69 sec
INFO:root:2024-04-27 18:53:43, Train, Epoch : 10, Step : 5710, Loss : 0.28076, Acc : 0.878, Sensitive_Loss : 0.06149, Sensitive_Acc : 15.200, Run Time : 162.83 sec
INFO:root:2024-04-27 18:53:55, Train, Epoch : 10, Step : 5720, Loss : 0.24978, Acc : 0.906, Sensitive_Loss : 0.03568, Sensitive_Acc : 17.000, Run Time : 11.69 sec
INFO:root:2024-04-27 18:54:07, Train, Epoch : 10, Step : 5730, Loss : 0.31992, Acc : 0.869, Sensitive_Loss : 0.06137, Sensitive_Acc : 16.200, Run Time : 11.81 sec
INFO:root:2024-04-27 18:54:19, Train, Epoch : 10, Step : 5740, Loss : 0.28505, Acc : 0.887, Sensitive_Loss : 0.04886, Sensitive_Acc : 16.600, Run Time : 11.75 sec
INFO:root:2024-04-27 18:54:30, Train, Epoch : 10, Step : 5750, Loss : 0.24589, Acc : 0.909, Sensitive_Loss : 0.05289, Sensitive_Acc : 16.400, Run Time : 11.24 sec
INFO:root:2024-04-27 18:54:41, Train, Epoch : 10, Step : 5760, Loss : 0.24744, Acc : 0.891, Sensitive_Loss : 0.05378, Sensitive_Acc : 15.500, Run Time : 11.16 sec
INFO:root:2024-04-27 18:54:52, Train, Epoch : 10, Step : 5770, Loss : 0.27043, Acc : 0.887, Sensitive_Loss : 0.05127, Sensitive_Acc : 15.600, Run Time : 10.88 sec
INFO:root:2024-04-27 18:55:03, Train, Epoch : 10, Step : 5780, Loss : 0.21792, Acc : 0.916, Sensitive_Loss : 0.05333, Sensitive_Acc : 16.200, Run Time : 11.54 sec
INFO:root:2024-04-27 18:55:15, Train, Epoch : 10, Step : 5790, Loss : 0.27464, Acc : 0.894, Sensitive_Loss : 0.06274, Sensitive_Acc : 14.400, Run Time : 11.32 sec
INFO:root:2024-04-27 18:55:26, Train, Epoch : 10, Step : 5800, Loss : 0.21363, Acc : 0.897, Sensitive_Loss : 0.06688, Sensitive_Acc : 15.700, Run Time : 11.51 sec
INFO:root:2024-04-27 18:58:03, Dev, Step : 5800, Loss : 0.45479, Acc : 0.820, Auc : 0.901, Sensitive_Loss : 0.12567, Sensitive_Acc : 16.893, Sensitive_Auc : 0.990, Mean auc: 0.901, Run Time : 156.66 sec
INFO:root:2024-04-27 18:58:11, Train, Epoch : 10, Step : 5810, Loss : 0.24725, Acc : 0.897, Sensitive_Loss : 0.04393, Sensitive_Acc : 15.900, Run Time : 164.79 sec
INFO:root:2024-04-27 18:58:23, Train, Epoch : 10, Step : 5820, Loss : 0.26426, Acc : 0.881, Sensitive_Loss : 0.05201, Sensitive_Acc : 16.200, Run Time : 12.32 sec
INFO:root:2024-04-27 18:58:36, Train, Epoch : 10, Step : 5830, Loss : 0.27179, Acc : 0.891, Sensitive_Loss : 0.03973, Sensitive_Acc : 16.400, Run Time : 12.20 sec
INFO:root:2024-04-27 18:58:48, Train, Epoch : 10, Step : 5840, Loss : 0.29425, Acc : 0.872, Sensitive_Loss : 0.06876, Sensitive_Acc : 14.900, Run Time : 12.17 sec
INFO:root:2024-04-27 18:58:59, Train, Epoch : 10, Step : 5850, Loss : 0.22490, Acc : 0.900, Sensitive_Loss : 0.06385, Sensitive_Acc : 16.600, Run Time : 11.28 sec
INFO:root:2024-04-27 18:59:10, Train, Epoch : 10, Step : 5860, Loss : 0.25198, Acc : 0.891, Sensitive_Loss : 0.08730, Sensitive_Acc : 17.600, Run Time : 10.99 sec
INFO:root:2024-04-27 18:59:21, Train, Epoch : 10, Step : 5870, Loss : 0.28468, Acc : 0.875, Sensitive_Loss : 0.09455, Sensitive_Acc : 16.400, Run Time : 11.13 sec
INFO:root:2024-04-27 18:59:32, Train, Epoch : 10, Step : 5880, Loss : 0.24619, Acc : 0.903, Sensitive_Loss : 0.08793, Sensitive_Acc : 18.300, Run Time : 11.17 sec
INFO:root:2024-04-27 18:59:44, Train, Epoch : 10, Step : 5890, Loss : 0.27966, Acc : 0.881, Sensitive_Loss : 0.07464, Sensitive_Acc : 15.400, Run Time : 12.02 sec
INFO:root:2024-04-27 18:59:57, Train, Epoch : 10, Step : 5900, Loss : 0.25649, Acc : 0.878, Sensitive_Loss : 0.08429, Sensitive_Acc : 14.900, Run Time : 12.27 sec
INFO:root:2024-04-27 19:02:32, Dev, Step : 5900, Loss : 0.44859, Acc : 0.825, Auc : 0.901, Sensitive_Loss : 0.10556, Sensitive_Acc : 16.950, Sensitive_Auc : 0.992, Mean auc: 0.901, Run Time : 155.21 sec
INFO:root:2024-04-27 19:02:40, Train, Epoch : 10, Step : 5910, Loss : 0.26010, Acc : 0.891, Sensitive_Loss : 0.06370, Sensitive_Acc : 14.900, Run Time : 163.29 sec
INFO:root:2024-04-27 19:02:51, Train, Epoch : 10, Step : 5920, Loss : 0.29359, Acc : 0.881, Sensitive_Loss : 0.07312, Sensitive_Acc : 15.300, Run Time : 11.64 sec
INFO:root:2024-04-27 19:03:04, Train, Epoch : 10, Step : 5930, Loss : 0.27033, Acc : 0.866, Sensitive_Loss : 0.06953, Sensitive_Acc : 16.400, Run Time : 12.73 sec
INFO:root:2024-04-27 19:03:15, Train, Epoch : 10, Step : 5940, Loss : 0.30630, Acc : 0.894, Sensitive_Loss : 0.06964, Sensitive_Acc : 17.300, Run Time : 10.91 sec
INFO:root:2024-04-27 19:03:27, Train, Epoch : 10, Step : 5950, Loss : 0.27512, Acc : 0.887, Sensitive_Loss : 0.05584, Sensitive_Acc : 15.700, Run Time : 11.65 sec
INFO:root:2024-04-27 19:03:39, Train, Epoch : 10, Step : 5960, Loss : 0.25053, Acc : 0.900, Sensitive_Loss : 0.05851, Sensitive_Acc : 16.600, Run Time : 12.51 sec
INFO:root:2024-04-27 19:03:50, Train, Epoch : 10, Step : 5970, Loss : 0.20175, Acc : 0.897, Sensitive_Loss : 0.05631, Sensitive_Acc : 15.900, Run Time : 11.20 sec
INFO:root:2024-04-27 19:04:02, Train, Epoch : 10, Step : 5980, Loss : 0.19689, Acc : 0.897, Sensitive_Loss : 0.07168, Sensitive_Acc : 17.800, Run Time : 11.16 sec
INFO:root:2024-04-27 19:04:13, Train, Epoch : 10, Step : 5990, Loss : 0.28283, Acc : 0.856, Sensitive_Loss : 0.09316, Sensitive_Acc : 14.900, Run Time : 11.84 sec
INFO:root:2024-04-27 19:04:25, Train, Epoch : 10, Step : 6000, Loss : 0.24508, Acc : 0.897, Sensitive_Loss : 0.05333, Sensitive_Acc : 17.500, Run Time : 11.29 sec
INFO:root:2024-04-27 19:07:00, Dev, Step : 6000, Loss : 0.45212, Acc : 0.819, Auc : 0.899, Sensitive_Loss : 0.13108, Sensitive_Acc : 16.836, Sensitive_Auc : 0.988, Mean auc: 0.899, Run Time : 155.35 sec
INFO:root:2024-04-27 19:07:09, Train, Epoch : 10, Step : 6010, Loss : 0.31439, Acc : 0.875, Sensitive_Loss : 0.06636, Sensitive_Acc : 17.200, Run Time : 163.90 sec
INFO:root:2024-04-27 19:07:20, Train, Epoch : 10, Step : 6020, Loss : 0.25247, Acc : 0.891, Sensitive_Loss : 0.04504, Sensitive_Acc : 16.900, Run Time : 11.74 sec
INFO:root:2024-04-27 19:07:32, Train, Epoch : 10, Step : 6030, Loss : 0.25020, Acc : 0.909, Sensitive_Loss : 0.03860, Sensitive_Acc : 16.500, Run Time : 12.04 sec
INFO:root:2024-04-27 19:07:44, Train, Epoch : 10, Step : 6040, Loss : 0.29328, Acc : 0.894, Sensitive_Loss : 0.04950, Sensitive_Acc : 14.900, Run Time : 11.98 sec
INFO:root:2024-04-27 19:07:57, Train, Epoch : 10, Step : 6050, Loss : 0.24340, Acc : 0.894, Sensitive_Loss : 0.05344, Sensitive_Acc : 17.300, Run Time : 12.53 sec
INFO:root:2024-04-27 19:08:09, Train, Epoch : 10, Step : 6060, Loss : 0.27627, Acc : 0.881, Sensitive_Loss : 0.06329, Sensitive_Acc : 17.200, Run Time : 12.03 sec
INFO:root:2024-04-27 19:08:21, Train, Epoch : 10, Step : 6070, Loss : 0.22108, Acc : 0.909, Sensitive_Loss : 0.04693, Sensitive_Acc : 17.600, Run Time : 11.91 sec
INFO:root:2024-04-27 19:08:32, Train, Epoch : 10, Step : 6080, Loss : 0.23542, Acc : 0.872, Sensitive_Loss : 0.08983, Sensitive_Acc : 18.400, Run Time : 11.20 sec
INFO:root:2024-04-27 19:08:44, Train, Epoch : 10, Step : 6090, Loss : 0.29077, Acc : 0.903, Sensitive_Loss : 0.07651, Sensitive_Acc : 17.800, Run Time : 11.75 sec
INFO:root:2024-04-27 19:08:56, Train, Epoch : 10, Step : 6100, Loss : 0.23755, Acc : 0.894, Sensitive_Loss : 0.06604, Sensitive_Acc : 17.000, Run Time : 11.68 sec
INFO:root:2024-04-27 19:11:31, Dev, Step : 6100, Loss : 0.44289, Acc : 0.823, Auc : 0.902, Sensitive_Loss : 0.10064, Sensitive_Acc : 16.864, Sensitive_Auc : 0.995, Mean auc: 0.902, Run Time : 155.64 sec
INFO:root:2024-04-27 19:11:40, Train, Epoch : 10, Step : 6110, Loss : 0.27988, Acc : 0.881, Sensitive_Loss : 0.05802, Sensitive_Acc : 16.200, Run Time : 164.85 sec
INFO:root:2024-04-27 19:11:52, Train, Epoch : 10, Step : 6120, Loss : 0.30758, Acc : 0.866, Sensitive_Loss : 0.04322, Sensitive_Acc : 16.200, Run Time : 11.19 sec
INFO:root:2024-04-27 19:12:03, Train, Epoch : 10, Step : 6130, Loss : 0.22283, Acc : 0.897, Sensitive_Loss : 0.07428, Sensitive_Acc : 17.700, Run Time : 11.63 sec
INFO:root:2024-04-27 19:12:16, Train, Epoch : 10, Step : 6140, Loss : 0.23812, Acc : 0.897, Sensitive_Loss : 0.04894, Sensitive_Acc : 16.700, Run Time : 12.75 sec
INFO:root:2024-04-27 19:12:28, Train, Epoch : 10, Step : 6150, Loss : 0.27103, Acc : 0.887, Sensitive_Loss : 0.05874, Sensitive_Acc : 17.700, Run Time : 12.14 sec
INFO:root:2024-04-27 19:12:40, Train, Epoch : 10, Step : 6160, Loss : 0.27964, Acc : 0.884, Sensitive_Loss : 0.07478, Sensitive_Acc : 17.100, Run Time : 11.44 sec
INFO:root:2024-04-27 19:12:52, Train, Epoch : 10, Step : 6170, Loss : 0.32683, Acc : 0.866, Sensitive_Loss : 0.04304, Sensitive_Acc : 17.500, Run Time : 12.09 sec
INFO:root:2024-04-27 19:13:03, Train, Epoch : 10, Step : 6180, Loss : 0.26036, Acc : 0.891, Sensitive_Loss : 0.03175, Sensitive_Acc : 16.100, Run Time : 11.77 sec
INFO:root:2024-04-27 19:13:15, Train, Epoch : 10, Step : 6190, Loss : 0.29156, Acc : 0.872, Sensitive_Loss : 0.06142, Sensitive_Acc : 16.000, Run Time : 11.88 sec
INFO:root:2024-04-27 19:13:27, Train, Epoch : 10, Step : 6200, Loss : 0.29963, Acc : 0.856, Sensitive_Loss : 0.06700, Sensitive_Acc : 17.600, Run Time : 11.75 sec
INFO:root:2024-04-27 19:16:03, Dev, Step : 6200, Loss : 0.45277, Acc : 0.820, Auc : 0.901, Sensitive_Loss : 0.12112, Sensitive_Acc : 16.893, Sensitive_Auc : 0.992, Mean auc: 0.901, Run Time : 156.21 sec
INFO:root:2024-04-27 19:16:11, Train, Epoch : 10, Step : 6210, Loss : 0.27274, Acc : 0.875, Sensitive_Loss : 0.04736, Sensitive_Acc : 16.100, Run Time : 164.40 sec
INFO:root:2024-04-27 19:16:24, Train, Epoch : 10, Step : 6220, Loss : 0.27028, Acc : 0.887, Sensitive_Loss : 0.05027, Sensitive_Acc : 16.800, Run Time : 12.14 sec
INFO:root:2024-04-27 19:16:36, Train, Epoch : 10, Step : 6230, Loss : 0.33948, Acc : 0.863, Sensitive_Loss : 0.08418, Sensitive_Acc : 17.400, Run Time : 12.92 sec
INFO:root:2024-04-27 19:16:49, Train, Epoch : 10, Step : 6240, Loss : 0.23547, Acc : 0.903, Sensitive_Loss : 0.08378, Sensitive_Acc : 16.300, Run Time : 12.69 sec
INFO:root:2024-04-27 19:17:01, Train, Epoch : 10, Step : 6250, Loss : 0.25753, Acc : 0.878, Sensitive_Loss : 0.05865, Sensitive_Acc : 17.500, Run Time : 12.13 sec
INFO:root:2024-04-27 19:17:13, Train, Epoch : 10, Step : 6260, Loss : 0.21837, Acc : 0.916, Sensitive_Loss : 0.08112, Sensitive_Acc : 16.700, Run Time : 11.40 sec
INFO:root:2024-04-27 19:19:46
INFO:root:y_pred: [0.30312467 0.9560868  0.00597999 ... 0.7220409  0.02541891 0.8767188 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.7101730e-01 6.7011424e-05 5.9980334e-04 9.0154913e-07 9.9600899e-01
 1.6893284e-06 9.9987638e-01 9.9668294e-01 2.8432401e-05 9.9398887e-01
 9.9958497e-01 9.9991322e-01 9.9972624e-01 8.8423979e-01 2.2598167e-04
 9.9547946e-01 9.9996984e-01 7.5902586e-05 9.3454713e-01 9.9877650e-01
 9.8727697e-01 7.9719909e-02 9.9994981e-01 8.9311010e-01 9.8996359e-01
 9.4258279e-01 2.4350116e-05 9.9631989e-01 9.8841697e-01 3.8455823e-01
 3.8879505e-05 4.0669233e-02 9.1968998e-03 4.0056640e-03 3.7290493e-01
 3.1215812e-05 4.6118253e-04 9.6461074e-03 9.8968047e-01 9.9551636e-01
 1.2055925e-06 4.4674423e-05 9.9658751e-01 2.5317157e-04 9.9982905e-01
 9.9570376e-01 9.8373574e-01 9.9976903e-01 1.1047257e-02 9.8983383e-01
 9.9966252e-01 2.8181446e-04 7.3610701e-02 9.0228838e-05 3.9859620e-05
 1.0657718e-02 1.3498828e-02 7.3113048e-01 2.0353349e-04 2.2561079e-01
 6.6601363e-04 1.8644037e-02 1.3200547e-05 9.9678397e-01 9.2410599e-04
 9.9835891e-01 2.1628529e-04 9.9827766e-01 9.4229966e-01 9.7425622e-01
 9.6085882e-01 5.2631792e-02 6.4039479e-05 4.4627706e-04 6.8451591e-06
 3.6599275e-05 8.7623209e-01 2.7236515e-01 8.4472907e-07 9.8523349e-01
 9.9927992e-01 3.9373986e-06 5.6723631e-03 4.1845005e-06 5.1894855e-02
 9.9150813e-01 7.0725106e-02 3.9103171e-03 9.7109747e-01 9.9932539e-01
 9.9802220e-01 7.4289948e-02 2.5127288e-03 9.9960011e-01 7.9992628e-01
 1.0463713e-04 9.9587989e-01 9.9834311e-01 3.4736794e-07 1.3969704e-03
 9.8208123e-01 9.8983568e-01 9.9955112e-01 9.9895906e-01 1.9097783e-05
 1.9853562e-04 7.5795335e-01 9.2119420e-01 9.8319775e-01 1.4366778e-07
 7.5769889e-01 9.9975306e-01 2.0313564e-01 9.9838722e-01 9.9543959e-01
 9.7509921e-01 9.0919650e-01 9.9855047e-01 3.2712415e-02 9.2795885e-01
 9.9935514e-01 9.9571997e-01 1.2704533e-06 8.6194444e-01 9.9814785e-01
 1.5399075e-02 9.9515212e-01 4.7647217e-03 1.9847204e-04 9.9890423e-01
 9.7522485e-01 1.0621068e-04 6.2594824e-03 1.3991232e-03 9.8388207e-01
 9.9971312e-01 9.7805047e-01 3.0160847e-04 1.7928122e-02 9.8960364e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-27 19:19:46, Dev, Step : 6260, Loss : 0.45653, Acc : 0.820, Auc : 0.901, Sensitive_Loss : 0.12094, Sensitive_Acc : 16.893, Sensitive_Auc : 0.991, Mean auc: 0.901, Run Time : 152.92 sec
