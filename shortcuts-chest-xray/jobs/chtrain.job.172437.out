Running on desktop22:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/random_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/random_dataset_val.csv",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.1,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-01 04:21:34, Train, Epoch : 1, Step : 10, Loss : 1.22787, Acc : 0.525, Sensitive_Loss : 0.78723, Sensitive_Acc : 14.200, Run Time : 8.68 sec
INFO:root:2024-04-01 04:21:41, Train, Epoch : 1, Step : 20, Loss : 0.99058, Acc : 0.531, Sensitive_Loss : 0.71011, Sensitive_Acc : 14.900, Run Time : 7.10 sec
INFO:root:2024-04-01 04:21:48, Train, Epoch : 1, Step : 30, Loss : 1.12405, Acc : 0.528, Sensitive_Loss : 0.79225, Sensitive_Acc : 15.800, Run Time : 7.09 sec
INFO:root:2024-04-01 04:21:55, Train, Epoch : 1, Step : 40, Loss : 1.45268, Acc : 0.481, Sensitive_Loss : 0.74437, Sensitive_Acc : 17.700, Run Time : 7.28 sec
INFO:root:2024-04-01 04:22:03, Train, Epoch : 1, Step : 50, Loss : 1.19836, Acc : 0.534, Sensitive_Loss : 0.69060, Sensitive_Acc : 17.800, Run Time : 7.10 sec
INFO:root:2024-04-01 04:22:10, Train, Epoch : 1, Step : 60, Loss : 1.04033, Acc : 0.503, Sensitive_Loss : 0.76230, Sensitive_Acc : 16.300, Run Time : 7.30 sec
INFO:root:2024-04-01 04:22:17, Train, Epoch : 1, Step : 70, Loss : 1.37665, Acc : 0.519, Sensitive_Loss : 0.73817, Sensitive_Acc : 16.300, Run Time : 7.33 sec
INFO:root:2024-04-01 04:22:24, Train, Epoch : 1, Step : 80, Loss : 0.81281, Acc : 0.525, Sensitive_Loss : 0.68338, Sensitive_Acc : 18.000, Run Time : 6.85 sec
INFO:root:2024-04-01 04:22:31, Train, Epoch : 1, Step : 90, Loss : 1.22114, Acc : 0.512, Sensitive_Loss : 0.67631, Sensitive_Acc : 16.700, Run Time : 7.38 sec
INFO:root:2024-04-01 04:22:39, Train, Epoch : 1, Step : 100, Loss : 1.24939, Acc : 0.503, Sensitive_Loss : 0.62454, Sensitive_Acc : 17.300, Run Time : 7.41 sec
INFO:root:2024-04-01 04:24:14, Dev, Step : 100, Loss : 1.22853, Acc : 0.226, Auc : 0.652, Sensitive_Loss : 0.68769, Sensitive_Acc : 15.433, Sensitive_Auc : 0.724, Mean auc: 0.652, Run Time : 95.11 sec
INFO:root:2024-04-01 04:24:15, Best, Step : 100, Loss : 1.22853, Acc : 0.226, Auc : 0.652, Sensitive_Loss : 0.68769, Sensitive_Acc : 15.433, Sensitive_Auc : 0.724, Best Auc : 0.652
INFO:root:2024-04-01 04:24:20, Train, Epoch : 1, Step : 110, Loss : 0.86197, Acc : 0.525, Sensitive_Loss : 0.74043, Sensitive_Acc : 15.600, Run Time : 101.12 sec
INFO:root:2024-04-01 04:24:27, Train, Epoch : 1, Step : 120, Loss : 1.28132, Acc : 0.550, Sensitive_Loss : 0.72741, Sensitive_Acc : 16.700, Run Time : 7.09 sec
INFO:root:2024-04-01 04:24:35, Train, Epoch : 1, Step : 130, Loss : 1.22195, Acc : 0.578, Sensitive_Loss : 0.62488, Sensitive_Acc : 17.800, Run Time : 7.51 sec
INFO:root:2024-04-01 04:24:42, Train, Epoch : 1, Step : 140, Loss : 1.01607, Acc : 0.578, Sensitive_Loss : 0.59322, Sensitive_Acc : 18.000, Run Time : 7.21 sec
INFO:root:2024-04-01 04:24:49, Train, Epoch : 1, Step : 150, Loss : 1.27688, Acc : 0.541, Sensitive_Loss : 0.60571, Sensitive_Acc : 16.400, Run Time : 7.19 sec
INFO:root:2024-04-01 04:24:56, Train, Epoch : 1, Step : 160, Loss : 1.09294, Acc : 0.575, Sensitive_Loss : 0.54140, Sensitive_Acc : 16.100, Run Time : 7.10 sec
INFO:root:2024-04-01 04:25:03, Train, Epoch : 1, Step : 170, Loss : 0.89375, Acc : 0.575, Sensitive_Loss : 0.58980, Sensitive_Acc : 18.000, Run Time : 7.12 sec
INFO:root:2024-04-01 04:25:11, Train, Epoch : 1, Step : 180, Loss : 1.22791, Acc : 0.572, Sensitive_Loss : 0.51994, Sensitive_Acc : 15.300, Run Time : 7.35 sec
INFO:root:2024-04-01 04:25:18, Train, Epoch : 1, Step : 190, Loss : 0.96557, Acc : 0.581, Sensitive_Loss : 0.58200, Sensitive_Acc : 14.900, Run Time : 7.11 sec
INFO:root:2024-04-01 04:25:25, Train, Epoch : 1, Step : 200, Loss : 1.09769, Acc : 0.562, Sensitive_Loss : 0.56547, Sensitive_Acc : 16.000, Run Time : 7.29 sec
INFO:root:2024-04-01 04:26:59, Dev, Step : 200, Loss : 1.14773, Acc : 0.451, Auc : 0.665, Sensitive_Loss : 0.58606, Sensitive_Acc : 15.943, Sensitive_Auc : 0.866, Mean auc: 0.665, Run Time : 93.74 sec
INFO:root:2024-04-01 04:26:59, Best, Step : 200, Loss : 1.14773, Acc : 0.451, Auc : 0.665, Sensitive_Loss : 0.58606, Sensitive_Acc : 15.943, Sensitive_Auc : 0.866, Best Auc : 0.665
INFO:root:2024-04-01 04:27:05, Train, Epoch : 1, Step : 210, Loss : 1.17079, Acc : 0.562, Sensitive_Loss : 0.62548, Sensitive_Acc : 18.300, Run Time : 99.66 sec
INFO:root:2024-04-01 04:27:12, Train, Epoch : 1, Step : 220, Loss : 1.07014, Acc : 0.512, Sensitive_Loss : 0.50174, Sensitive_Acc : 16.000, Run Time : 7.84 sec
INFO:root:2024-04-01 04:27:19, Train, Epoch : 1, Step : 230, Loss : 1.33175, Acc : 0.566, Sensitive_Loss : 0.56788, Sensitive_Acc : 16.200, Run Time : 6.99 sec
INFO:root:2024-04-01 04:27:27, Train, Epoch : 1, Step : 240, Loss : 1.05010, Acc : 0.603, Sensitive_Loss : 0.54436, Sensitive_Acc : 15.100, Run Time : 7.33 sec
INFO:root:2024-04-01 04:27:34, Train, Epoch : 1, Step : 250, Loss : 0.88250, Acc : 0.547, Sensitive_Loss : 0.55248, Sensitive_Acc : 17.600, Run Time : 7.36 sec
INFO:root:2024-04-01 04:27:41, Train, Epoch : 1, Step : 260, Loss : 1.22060, Acc : 0.550, Sensitive_Loss : 0.58672, Sensitive_Acc : 17.000, Run Time : 7.00 sec
INFO:root:2024-04-01 04:27:48, Train, Epoch : 1, Step : 270, Loss : 0.98803, Acc : 0.584, Sensitive_Loss : 0.46318, Sensitive_Acc : 16.600, Run Time : 7.17 sec
INFO:root:2024-04-01 04:27:56, Train, Epoch : 1, Step : 280, Loss : 1.30256, Acc : 0.553, Sensitive_Loss : 0.47541, Sensitive_Acc : 16.500, Run Time : 7.64 sec
INFO:root:2024-04-01 04:28:03, Train, Epoch : 1, Step : 290, Loss : 1.20880, Acc : 0.572, Sensitive_Loss : 0.45331, Sensitive_Acc : 15.100, Run Time : 6.70 sec
INFO:root:2024-04-01 04:28:10, Train, Epoch : 1, Step : 300, Loss : 1.12414, Acc : 0.581, Sensitive_Loss : 0.47328, Sensitive_Acc : 16.400, Run Time : 7.61 sec
INFO:root:2024-04-01 04:29:44, Dev, Step : 300, Loss : 1.28024, Acc : 0.299, Auc : 0.706, Sensitive_Loss : 0.46507, Sensitive_Acc : 16.809, Sensitive_Auc : 0.943, Mean auc: 0.706, Run Time : 94.03 sec
INFO:root:2024-04-01 04:29:45, Best, Step : 300, Loss : 1.28024, Acc : 0.299, Auc : 0.706, Sensitive_Loss : 0.46507, Sensitive_Acc : 16.809, Sensitive_Auc : 0.943, Best Auc : 0.706
INFO:root:2024-04-01 04:29:50, Train, Epoch : 1, Step : 310, Loss : 0.78563, Acc : 0.584, Sensitive_Loss : 0.42810, Sensitive_Acc : 15.900, Run Time : 100.12 sec
INFO:root:2024-04-01 04:29:58, Train, Epoch : 1, Step : 320, Loss : 1.00325, Acc : 0.606, Sensitive_Loss : 0.44913, Sensitive_Acc : 17.500, Run Time : 7.39 sec
INFO:root:2024-04-01 04:30:05, Train, Epoch : 1, Step : 330, Loss : 1.29353, Acc : 0.613, Sensitive_Loss : 0.40711, Sensitive_Acc : 16.000, Run Time : 7.41 sec
INFO:root:2024-04-01 04:30:12, Train, Epoch : 1, Step : 340, Loss : 1.16866, Acc : 0.591, Sensitive_Loss : 0.41813, Sensitive_Acc : 15.200, Run Time : 6.85 sec
INFO:root:2024-04-01 04:30:19, Train, Epoch : 1, Step : 350, Loss : 1.02357, Acc : 0.606, Sensitive_Loss : 0.51491, Sensitive_Acc : 18.200, Run Time : 7.14 sec
INFO:root:2024-04-01 04:30:26, Train, Epoch : 1, Step : 360, Loss : 1.25393, Acc : 0.578, Sensitive_Loss : 0.51359, Sensitive_Acc : 17.600, Run Time : 7.03 sec
INFO:root:2024-04-01 04:30:34, Train, Epoch : 1, Step : 370, Loss : 1.16887, Acc : 0.628, Sensitive_Loss : 0.49171, Sensitive_Acc : 18.100, Run Time : 7.58 sec
INFO:root:2024-04-01 04:30:41, Train, Epoch : 1, Step : 380, Loss : 1.09469, Acc : 0.594, Sensitive_Loss : 0.42392, Sensitive_Acc : 15.400, Run Time : 7.13 sec
INFO:root:2024-04-01 04:30:48, Train, Epoch : 1, Step : 390, Loss : 1.15647, Acc : 0.569, Sensitive_Loss : 0.41133, Sensitive_Acc : 14.600, Run Time : 7.17 sec
INFO:root:2024-04-01 04:30:55, Train, Epoch : 1, Step : 400, Loss : 1.07810, Acc : 0.616, Sensitive_Loss : 0.46966, Sensitive_Acc : 17.400, Run Time : 7.12 sec
INFO:root:2024-04-01 04:32:29, Dev, Step : 400, Loss : 1.30705, Acc : 0.368, Auc : 0.664, Sensitive_Loss : 0.54154, Sensitive_Acc : 15.348, Sensitive_Auc : 0.949, Mean auc: 0.664, Run Time : 93.95 sec
INFO:root:2024-04-01 04:32:35, Train, Epoch : 1, Step : 410, Loss : 1.33935, Acc : 0.569, Sensitive_Loss : 0.43925, Sensitive_Acc : 15.900, Run Time : 99.45 sec
INFO:root:2024-04-01 04:32:42, Train, Epoch : 1, Step : 420, Loss : 0.90980, Acc : 0.584, Sensitive_Loss : 0.47397, Sensitive_Acc : 15.600, Run Time : 7.24 sec
INFO:root:2024-04-01 04:32:49, Train, Epoch : 1, Step : 430, Loss : 1.25534, Acc : 0.597, Sensitive_Loss : 0.44323, Sensitive_Acc : 18.500, Run Time : 7.18 sec
INFO:root:2024-04-01 04:32:56, Train, Epoch : 1, Step : 440, Loss : 1.10029, Acc : 0.609, Sensitive_Loss : 0.38163, Sensitive_Acc : 16.800, Run Time : 7.09 sec
INFO:root:2024-04-01 04:33:03, Train, Epoch : 1, Step : 450, Loss : 1.11777, Acc : 0.594, Sensitive_Loss : 0.38261, Sensitive_Acc : 16.600, Run Time : 7.08 sec
INFO:root:2024-04-01 04:33:11, Train, Epoch : 1, Step : 460, Loss : 0.88476, Acc : 0.569, Sensitive_Loss : 0.40564, Sensitive_Acc : 14.200, Run Time : 7.64 sec
INFO:root:2024-04-01 04:33:18, Train, Epoch : 1, Step : 470, Loss : 1.01675, Acc : 0.584, Sensitive_Loss : 0.49821, Sensitive_Acc : 16.300, Run Time : 7.19 sec
INFO:root:2024-04-01 04:33:25, Train, Epoch : 1, Step : 480, Loss : 1.28963, Acc : 0.588, Sensitive_Loss : 0.39472, Sensitive_Acc : 14.800, Run Time : 6.88 sec
INFO:root:2024-04-01 04:33:32, Train, Epoch : 1, Step : 490, Loss : 1.00215, Acc : 0.619, Sensitive_Loss : 0.41727, Sensitive_Acc : 16.100, Run Time : 7.26 sec
INFO:root:2024-04-01 04:35:06
INFO:root:y_pred: [0.29877824 0.02548143 0.67293644 ... 0.07144335 0.33461142 0.29545227]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [1.02440715e-02 2.40502551e-01 7.31980920e-01 4.40846175e-01
 9.83503044e-01 1.58250146e-02 2.64740080e-01 2.26122394e-01
 4.42428067e-02 1.43550947e-01 1.19504340e-01 1.28749004e-02
 9.01578784e-01 1.78576335e-02 9.74061072e-01 8.46527100e-01
 2.03355169e-03 2.99936444e-01 5.70980191e-01 7.25320339e-01
 5.87440617e-02 1.43473357e-01 9.96198475e-01 6.46263838e-01
 1.15135036e-01 4.48474497e-01 1.13803349e-01 9.60732818e-01
 4.66359332e-02 5.17228663e-01 9.93341029e-01 8.66155088e-01
 3.42459627e-03 2.44250055e-02 4.67766166e-01 3.84660438e-02
 1.49556175e-01 1.64061040e-01 8.59860599e-01 9.63050365e-01
 2.99099414e-03 1.01794831e-01 4.24786866e-01 9.84536588e-01
 2.27003440e-01 6.86024055e-02 2.50392873e-03 1.20127045e-01
 9.91388500e-01 8.12379122e-01 7.44098067e-01 8.45090270e-01
 8.15946341e-01 6.26279950e-01 8.51932943e-01 5.72631776e-01
 6.29079878e-01 1.96591929e-01 7.82556176e-01 1.45454347e-01
 4.91474697e-04 8.41894984e-01 7.85768151e-01 9.16365921e-01
 8.72766554e-01 9.37868774e-01 4.44030762e-01 8.36354077e-01
 8.08500946e-01 6.42105460e-01 9.47125912e-01 7.19564334e-02
 1.65465832e-01 9.35008883e-01 5.23343205e-01 9.97451600e-03
 9.19278562e-01 5.36836162e-02 9.68611121e-01 6.36259258e-01
 5.79706371e-01 2.12597940e-02 1.63219318e-01 5.52618146e-01
 7.48261988e-01 6.47190034e-01 6.24189079e-01 7.42788553e-01
 1.85773522e-01 2.24551231e-01 4.21732038e-01 1.96248889e-01
 2.03327432e-01 5.81475273e-02 6.10645950e-01 6.03195786e-01
 5.98584890e-01 2.17306629e-01 8.38882804e-01 8.70406210e-01
 9.54354405e-01 1.56870927e-03 9.92746890e-01 2.19968230e-01
 1.93992063e-01 9.27527189e-01 8.39303613e-01 1.41693160e-01
 5.09424470e-02 5.03103174e-02 4.51087393e-02 8.58130872e-01
 2.86192298e-01 8.85918289e-02 8.13179240e-02 2.04997975e-03
 9.22014415e-01 8.41944385e-03 7.34935701e-01 5.15242247e-03
 6.06235743e-01 1.67774633e-02 9.81163800e-01 7.26639852e-02
 2.87558250e-02 9.79633212e-01 6.99822828e-02 8.19493234e-01
 7.48267353e-01 9.53345835e-01 6.35439996e-03 9.62224424e-01
 8.07117373e-02 9.96799827e-01 9.74464476e-01 8.83549988e-01
 6.75685406e-01 8.92232239e-01 5.20167649e-02 9.18150842e-01
 9.02660847e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-04-01 04:35:06, Dev, Step : 492, Loss : 1.08156, Acc : 0.798, Auc : 0.713, Sensitive_Loss : 0.36186, Sensitive_Acc : 16.496, Sensitive_Auc : 0.943, Mean auc: 0.713, Run Time : 92.25 sec
INFO:root:2024-04-01 04:35:06, Best, Step : 492, Loss : 1.08156, Acc : 0.798,Auc : 0.713, Best Auc : 0.713, Sensitive_Loss : 0.36186, Sensitive_Acc : 16.496, Sensitive_Auc : 0.943
INFO:root:2024-04-01 04:35:14, Train, Epoch : 2, Step : 500, Loss : 0.79549, Acc : 0.484, Sensitive_Loss : 0.28331, Sensitive_Acc : 10.700, Run Time : 6.75 sec
INFO:root:2024-04-01 04:36:47, Dev, Step : 500, Loss : 1.03813, Acc : 0.707, Auc : 0.719, Sensitive_Loss : 0.35121, Sensitive_Acc : 16.482, Sensitive_Auc : 0.957, Mean auc: 0.719, Run Time : 93.17 sec
INFO:root:2024-04-01 04:36:48, Best, Step : 500, Loss : 1.03813, Acc : 0.707, Auc : 0.719, Sensitive_Loss : 0.35121, Sensitive_Acc : 16.482, Sensitive_Auc : 0.957, Best Auc : 0.719
INFO:root:2024-04-01 04:36:53, Train, Epoch : 2, Step : 510, Loss : 0.90453, Acc : 0.631, Sensitive_Loss : 0.33029, Sensitive_Acc : 15.800, Run Time : 99.14 sec
INFO:root:2024-04-01 04:37:01, Train, Epoch : 2, Step : 520, Loss : 0.98088, Acc : 0.619, Sensitive_Loss : 0.32748, Sensitive_Acc : 14.700, Run Time : 7.72 sec
INFO:root:2024-04-01 04:37:07, Train, Epoch : 2, Step : 530, Loss : 1.04262, Acc : 0.603, Sensitive_Loss : 0.35288, Sensitive_Acc : 16.200, Run Time : 6.65 sec
INFO:root:2024-04-01 04:37:15, Train, Epoch : 2, Step : 540, Loss : 1.12249, Acc : 0.613, Sensitive_Loss : 0.45070, Sensitive_Acc : 15.200, Run Time : 7.23 sec
INFO:root:2024-04-01 04:37:22, Train, Epoch : 2, Step : 550, Loss : 0.92650, Acc : 0.637, Sensitive_Loss : 0.37719, Sensitive_Acc : 13.800, Run Time : 7.48 sec
INFO:root:2024-04-01 04:37:29, Train, Epoch : 2, Step : 560, Loss : 1.09385, Acc : 0.647, Sensitive_Loss : 0.35761, Sensitive_Acc : 18.200, Run Time : 6.83 sec
INFO:root:2024-04-01 04:37:36, Train, Epoch : 2, Step : 570, Loss : 1.05188, Acc : 0.591, Sensitive_Loss : 0.33102, Sensitive_Acc : 15.200, Run Time : 6.81 sec
INFO:root:2024-04-01 04:37:43, Train, Epoch : 2, Step : 580, Loss : 0.92535, Acc : 0.572, Sensitive_Loss : 0.35723, Sensitive_Acc : 16.300, Run Time : 7.14 sec
INFO:root:2024-04-01 04:37:50, Train, Epoch : 2, Step : 590, Loss : 1.37605, Acc : 0.597, Sensitive_Loss : 0.31612, Sensitive_Acc : 15.500, Run Time : 6.96 sec
INFO:root:2024-04-01 04:37:57, Train, Epoch : 2, Step : 600, Loss : 1.02082, Acc : 0.581, Sensitive_Loss : 0.32882, Sensitive_Acc : 16.000, Run Time : 7.20 sec
INFO:root:2024-04-01 04:39:31, Dev, Step : 600, Loss : 1.00000, Acc : 0.734, Auc : 0.741, Sensitive_Loss : 0.38113, Sensitive_Acc : 15.901, Sensitive_Auc : 0.975, Mean auc: 0.741, Run Time : 93.83 sec
INFO:root:2024-04-01 04:39:31, Best, Step : 600, Loss : 1.00000, Acc : 0.734, Auc : 0.741, Sensitive_Loss : 0.38113, Sensitive_Acc : 15.901, Sensitive_Auc : 0.975, Best Auc : 0.741
INFO:root:2024-04-01 04:39:37, Train, Epoch : 2, Step : 610, Loss : 0.92797, Acc : 0.634, Sensitive_Loss : 0.31915, Sensitive_Acc : 16.500, Run Time : 100.07 sec
INFO:root:2024-04-01 04:39:44, Train, Epoch : 2, Step : 620, Loss : 0.87124, Acc : 0.662, Sensitive_Loss : 0.31476, Sensitive_Acc : 17.200, Run Time : 7.43 sec
INFO:root:2024-04-01 04:39:51, Train, Epoch : 2, Step : 630, Loss : 0.90820, Acc : 0.641, Sensitive_Loss : 0.27742, Sensitive_Acc : 16.400, Run Time : 6.47 sec
INFO:root:2024-04-01 04:39:58, Train, Epoch : 2, Step : 640, Loss : 1.04719, Acc : 0.647, Sensitive_Loss : 0.35784, Sensitive_Acc : 15.600, Run Time : 7.28 sec
INFO:root:2024-04-01 04:40:05, Train, Epoch : 2, Step : 650, Loss : 1.04274, Acc : 0.650, Sensitive_Loss : 0.32339, Sensitive_Acc : 18.100, Run Time : 7.13 sec
INFO:root:2024-04-01 04:40:12, Train, Epoch : 2, Step : 660, Loss : 0.94063, Acc : 0.628, Sensitive_Loss : 0.31687, Sensitive_Acc : 17.100, Run Time : 7.07 sec
INFO:root:2024-04-01 04:40:20, Train, Epoch : 2, Step : 670, Loss : 1.04994, Acc : 0.641, Sensitive_Loss : 0.32985, Sensitive_Acc : 17.400, Run Time : 7.36 sec
INFO:root:2024-04-01 04:40:27, Train, Epoch : 2, Step : 680, Loss : 1.06938, Acc : 0.644, Sensitive_Loss : 0.26482, Sensitive_Acc : 15.900, Run Time : 7.30 sec
INFO:root:2024-04-01 04:40:34, Train, Epoch : 2, Step : 690, Loss : 0.94324, Acc : 0.616, Sensitive_Loss : 0.31886, Sensitive_Acc : 17.000, Run Time : 7.03 sec
INFO:root:2024-04-01 04:40:41, Train, Epoch : 2, Step : 700, Loss : 1.08427, Acc : 0.628, Sensitive_Loss : 0.36714, Sensitive_Acc : 15.300, Run Time : 7.28 sec
INFO:root:2024-04-01 04:42:14, Dev, Step : 700, Loss : 1.02577, Acc : 0.647, Auc : 0.724, Sensitive_Loss : 0.33231, Sensitive_Acc : 17.248, Sensitive_Auc : 0.977, Mean auc: 0.724, Run Time : 92.89 sec
INFO:root:2024-04-01 04:42:20, Train, Epoch : 2, Step : 710, Loss : 0.70766, Acc : 0.666, Sensitive_Loss : 0.31936, Sensitive_Acc : 15.000, Run Time : 98.57 sec
INFO:root:2024-04-01 04:42:27, Train, Epoch : 2, Step : 720, Loss : 0.87146, Acc : 0.628, Sensitive_Loss : 0.30625, Sensitive_Acc : 15.600, Run Time : 7.16 sec
INFO:root:2024-04-01 04:42:34, Train, Epoch : 2, Step : 730, Loss : 1.03176, Acc : 0.625, Sensitive_Loss : 0.35437, Sensitive_Acc : 16.700, Run Time : 6.85 sec
INFO:root:2024-04-01 04:42:42, Train, Epoch : 2, Step : 740, Loss : 1.19745, Acc : 0.634, Sensitive_Loss : 0.30677, Sensitive_Acc : 16.500, Run Time : 7.94 sec
INFO:root:2024-04-01 04:42:49, Train, Epoch : 2, Step : 750, Loss : 1.18538, Acc : 0.628, Sensitive_Loss : 0.34203, Sensitive_Acc : 18.700, Run Time : 6.89 sec
INFO:root:2024-04-01 04:42:56, Train, Epoch : 2, Step : 760, Loss : 0.90001, Acc : 0.694, Sensitive_Loss : 0.27689, Sensitive_Acc : 18.500, Run Time : 6.89 sec
INFO:root:2024-04-01 04:43:02, Train, Epoch : 2, Step : 770, Loss : 1.14201, Acc : 0.656, Sensitive_Loss : 0.35291, Sensitive_Acc : 16.000, Run Time : 6.68 sec
INFO:root:2024-04-01 04:43:10, Train, Epoch : 2, Step : 780, Loss : 1.06005, Acc : 0.653, Sensitive_Loss : 0.37634, Sensitive_Acc : 16.000, Run Time : 7.32 sec
INFO:root:2024-04-01 04:43:17, Train, Epoch : 2, Step : 790, Loss : 0.96676, Acc : 0.637, Sensitive_Loss : 0.30056, Sensitive_Acc : 16.500, Run Time : 7.38 sec
INFO:root:2024-04-01 04:43:24, Train, Epoch : 2, Step : 800, Loss : 1.07207, Acc : 0.684, Sensitive_Loss : 0.28485, Sensitive_Acc : 17.900, Run Time : 6.47 sec
INFO:root:2024-04-01 04:44:58, Dev, Step : 800, Loss : 0.99712, Acc : 0.734, Auc : 0.742, Sensitive_Loss : 0.28434, Sensitive_Acc : 17.021, Sensitive_Auc : 0.985, Mean auc: 0.742, Run Time : 93.97 sec
INFO:root:2024-04-01 04:44:58, Best, Step : 800, Loss : 0.99712, Acc : 0.734, Auc : 0.742, Sensitive_Loss : 0.28434, Sensitive_Acc : 17.021, Sensitive_Auc : 0.985, Best Auc : 0.742
INFO:root:2024-04-01 04:45:04, Train, Epoch : 2, Step : 810, Loss : 1.27364, Acc : 0.606, Sensitive_Loss : 0.29574, Sensitive_Acc : 15.800, Run Time : 100.60 sec
INFO:root:2024-04-01 04:45:11, Train, Epoch : 2, Step : 820, Loss : 0.97838, Acc : 0.656, Sensitive_Loss : 0.27587, Sensitive_Acc : 15.600, Run Time : 6.82 sec
INFO:root:2024-04-01 04:45:18, Train, Epoch : 2, Step : 830, Loss : 1.22509, Acc : 0.637, Sensitive_Loss : 0.30045, Sensitive_Acc : 16.900, Run Time : 7.31 sec
INFO:root:2024-04-01 04:45:25, Train, Epoch : 2, Step : 840, Loss : 1.33420, Acc : 0.681, Sensitive_Loss : 0.33192, Sensitive_Acc : 14.900, Run Time : 7.19 sec
INFO:root:2024-04-01 04:45:33, Train, Epoch : 2, Step : 850, Loss : 1.00208, Acc : 0.613, Sensitive_Loss : 0.32542, Sensitive_Acc : 17.900, Run Time : 7.14 sec
INFO:root:2024-04-01 04:45:39, Train, Epoch : 2, Step : 860, Loss : 1.03970, Acc : 0.694, Sensitive_Loss : 0.34523, Sensitive_Acc : 17.300, Run Time : 6.69 sec
INFO:root:2024-04-01 04:45:46, Train, Epoch : 2, Step : 870, Loss : 1.12362, Acc : 0.672, Sensitive_Loss : 0.25742, Sensitive_Acc : 16.500, Run Time : 7.03 sec
INFO:root:2024-04-01 04:45:53, Train, Epoch : 2, Step : 880, Loss : 1.20362, Acc : 0.637, Sensitive_Loss : 0.34254, Sensitive_Acc : 16.000, Run Time : 6.89 sec
INFO:root:2024-04-01 04:46:01, Train, Epoch : 2, Step : 890, Loss : 1.15578, Acc : 0.637, Sensitive_Loss : 0.26117, Sensitive_Acc : 17.900, Run Time : 7.36 sec
INFO:root:2024-04-01 04:46:08, Train, Epoch : 2, Step : 900, Loss : 0.77766, Acc : 0.694, Sensitive_Loss : 0.28358, Sensitive_Acc : 17.000, Run Time : 7.26 sec
INFO:root:2024-04-01 04:47:41, Dev, Step : 900, Loss : 0.97589, Acc : 0.647, Auc : 0.763, Sensitive_Loss : 0.25373, Sensitive_Acc : 17.277, Sensitive_Auc : 0.985, Mean auc: 0.763, Run Time : 93.57 sec
INFO:root:2024-04-01 04:47:42, Best, Step : 900, Loss : 0.97589, Acc : 0.647, Auc : 0.763, Sensitive_Loss : 0.25373, Sensitive_Acc : 17.277, Sensitive_Auc : 0.985, Best Auc : 0.763
INFO:root:2024-04-01 04:47:48, Train, Epoch : 2, Step : 910, Loss : 1.08431, Acc : 0.622, Sensitive_Loss : 0.26746, Sensitive_Acc : 16.400, Run Time : 99.66 sec
INFO:root:2024-04-01 04:47:55, Train, Epoch : 2, Step : 920, Loss : 0.88142, Acc : 0.644, Sensitive_Loss : 0.30190, Sensitive_Acc : 15.700, Run Time : 7.06 sec
INFO:root:2024-04-01 04:48:02, Train, Epoch : 2, Step : 930, Loss : 0.96512, Acc : 0.662, Sensitive_Loss : 0.24604, Sensitive_Acc : 17.600, Run Time : 7.37 sec
INFO:root:2024-04-01 04:48:09, Train, Epoch : 2, Step : 940, Loss : 1.01159, Acc : 0.666, Sensitive_Loss : 0.31864, Sensitive_Acc : 14.700, Run Time : 7.44 sec
INFO:root:2024-04-01 04:48:17, Train, Epoch : 2, Step : 950, Loss : 0.97624, Acc : 0.647, Sensitive_Loss : 0.26105, Sensitive_Acc : 17.300, Run Time : 7.22 sec
INFO:root:2024-04-01 04:48:24, Train, Epoch : 2, Step : 960, Loss : 0.95226, Acc : 0.694, Sensitive_Loss : 0.26283, Sensitive_Acc : 17.700, Run Time : 7.19 sec
INFO:root:2024-04-01 04:48:31, Train, Epoch : 2, Step : 970, Loss : 0.89106, Acc : 0.684, Sensitive_Loss : 0.26706, Sensitive_Acc : 18.700, Run Time : 6.79 sec
INFO:root:2024-04-01 04:48:38, Train, Epoch : 2, Step : 980, Loss : 0.95887, Acc : 0.628, Sensitive_Loss : 0.25712, Sensitive_Acc : 15.600, Run Time : 7.28 sec
INFO:root:2024-04-01 04:50:12
INFO:root:y_pred: [0.6092288  0.3823418  0.4311781  ... 0.6737762  0.66813356 0.10281216]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [1.1171528e-04 4.5094413e-01 9.1335374e-01 2.4256155e-02 9.9758840e-01
 7.4175361e-04 4.6980351e-02 4.7422708e-03 2.1186471e-03 1.3533083e-01
 1.0328832e-01 8.7381013e-02 9.4646841e-01 1.7639762e-02 9.7982782e-01
 8.3764470e-01 7.4153871e-04 5.5728011e-02 9.2249095e-01 8.2918990e-01
 4.3269205e-03 2.2332687e-02 9.9978167e-01 6.7467839e-01 2.2368927e-02
 6.9775873e-01 1.0035665e-02 9.7454768e-01 3.8662564e-03 8.7427378e-01
 9.9675083e-01 9.5775080e-01 1.0091235e-02 3.5440494e-04 1.6502874e-01
 2.3410725e-04 3.0295772e-03 1.4281860e-01 9.5378125e-01 8.2233137e-01
 2.0186575e-03 6.8934215e-04 9.3171960e-01 9.9683595e-01 1.2361671e-01
 1.1238360e-02 6.1564142e-04 2.0185360e-01 9.9645340e-01 4.2446172e-01
 9.7097707e-01 4.7163057e-01 9.8996663e-01 9.1876495e-01 3.7972388e-01
 9.2850232e-01 8.5480052e-01 3.3326209e-02 9.9150556e-01 2.4562601e-02
 1.0495671e-03 2.7300360e-02 7.3677915e-01 9.2634624e-01 9.7941065e-01
 9.8256129e-01 5.6059003e-02 8.1344163e-01 8.9919353e-01 5.0619441e-01
 9.9331892e-01 3.5338923e-03 9.9888882e-03 3.7269020e-01 9.6106571e-01
 1.1373514e-03 9.2306620e-01 2.2063514e-03 9.9934834e-01 9.4145708e-02
 2.9483277e-01 1.6597084e-03 5.2047458e-02 3.9533924e-02 9.6570110e-01
 5.5249993e-02 6.8071157e-01 3.8054708e-01 9.2134975e-02 8.8010013e-02
 7.1844319e-03 1.8388458e-02 1.3343598e-02 7.3001441e-03 9.3157154e-01
 3.8148889e-01 6.1483458e-02 3.1270284e-02 7.1443951e-01 4.0493095e-01
 8.6791044e-01 2.0291419e-03 9.9909484e-01 5.3081275e-03 3.5754874e-01
 9.7787982e-01 4.5325305e-02 9.3799546e-02 2.0680051e-02 2.6915693e-03
 2.1769975e-03 7.8292298e-01 2.0766299e-02 3.5260597e-03 1.4324379e-01
 1.6128934e-05 9.0096688e-01 3.7804474e-03 2.1245128e-01 7.3435728e-04
 2.3471628e-01 1.6649585e-04 9.4880277e-01 5.5256528e-03 5.1833852e-03
 9.9095541e-01 9.1543412e-03 8.8142329e-01 4.9342489e-01 9.9338430e-01
 1.8816792e-04 9.4332236e-01 3.8920748e-03 9.9876618e-01 9.9787879e-01
 1.7081152e-01 2.2441794e-01 5.3894109e-01 3.5307072e-02 9.6424693e-01
 8.7177771e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-04-01 04:50:12, Dev, Step : 984, Loss : 0.97429, Acc : 0.730, Auc : 0.761, Sensitive_Loss : 0.25069, Sensitive_Acc : 17.291, Sensitive_Auc : 0.991, Mean auc: 0.761, Run Time : 92.55 sec
INFO:root:2024-04-01 04:50:19, Train, Epoch : 3, Step : 990, Loss : 0.58961, Acc : 0.400, Sensitive_Loss : 0.15177, Sensitive_Acc : 8.700, Run Time : 5.48 sec
INFO:root:2024-04-01 04:50:26, Train, Epoch : 3, Step : 1000, Loss : 0.80516, Acc : 0.684, Sensitive_Loss : 0.27160, Sensitive_Acc : 19.600, Run Time : 6.82 sec
INFO:root:2024-04-01 04:51:59, Dev, Step : 1000, Loss : 0.96408, Acc : 0.712, Auc : 0.766, Sensitive_Loss : 0.25534, Sensitive_Acc : 17.092, Sensitive_Auc : 0.992, Mean auc: 0.766, Run Time : 93.29 sec
INFO:root:2024-04-01 04:52:00, Best, Step : 1000, Loss : 0.96408, Acc : 0.712, Auc : 0.766, Sensitive_Loss : 0.25534, Sensitive_Acc : 17.092, Sensitive_Auc : 0.992, Best Auc : 0.766
INFO:root:2024-04-01 04:52:05, Train, Epoch : 3, Step : 1010, Loss : 0.71965, Acc : 0.719, Sensitive_Loss : 0.28377, Sensitive_Acc : 15.900, Run Time : 99.40 sec
INFO:root:2024-04-01 04:52:12, Train, Epoch : 3, Step : 1020, Loss : 0.91958, Acc : 0.669, Sensitive_Loss : 0.25123, Sensitive_Acc : 18.100, Run Time : 7.27 sec
INFO:root:2024-04-01 04:52:20, Train, Epoch : 3, Step : 1030, Loss : 1.14473, Acc : 0.650, Sensitive_Loss : 0.29396, Sensitive_Acc : 17.700, Run Time : 7.31 sec
INFO:root:2024-04-01 04:52:27, Train, Epoch : 3, Step : 1040, Loss : 0.94992, Acc : 0.681, Sensitive_Loss : 0.27474, Sensitive_Acc : 16.200, Run Time : 7.24 sec
INFO:root:2024-04-01 04:52:34, Train, Epoch : 3, Step : 1050, Loss : 0.89876, Acc : 0.666, Sensitive_Loss : 0.27167, Sensitive_Acc : 16.600, Run Time : 6.84 sec
INFO:root:2024-04-01 04:52:41, Train, Epoch : 3, Step : 1060, Loss : 0.87913, Acc : 0.678, Sensitive_Loss : 0.23651, Sensitive_Acc : 15.800, Run Time : 7.09 sec
INFO:root:2024-04-01 04:52:48, Train, Epoch : 3, Step : 1070, Loss : 0.84340, Acc : 0.653, Sensitive_Loss : 0.23419, Sensitive_Acc : 16.000, Run Time : 7.28 sec
INFO:root:2024-04-01 04:52:55, Train, Epoch : 3, Step : 1080, Loss : 1.06586, Acc : 0.691, Sensitive_Loss : 0.23006, Sensitive_Acc : 16.800, Run Time : 6.76 sec
INFO:root:2024-04-01 04:53:02, Train, Epoch : 3, Step : 1090, Loss : 1.00634, Acc : 0.666, Sensitive_Loss : 0.26266, Sensitive_Acc : 16.800, Run Time : 7.47 sec
INFO:root:2024-04-01 04:53:10, Train, Epoch : 3, Step : 1100, Loss : 1.03933, Acc : 0.672, Sensitive_Loss : 0.21566, Sensitive_Acc : 18.100, Run Time : 7.67 sec
INFO:root:2024-04-01 04:54:43, Dev, Step : 1100, Loss : 0.92797, Acc : 0.722, Auc : 0.786, Sensitive_Loss : 0.27050, Sensitive_Acc : 16.965, Sensitive_Auc : 0.993, Mean auc: 0.786, Run Time : 93.12 sec
INFO:root:2024-04-01 04:54:44, Best, Step : 1100, Loss : 0.92797, Acc : 0.722, Auc : 0.786, Sensitive_Loss : 0.27050, Sensitive_Acc : 16.965, Sensitive_Auc : 0.993, Best Auc : 0.786
INFO:root:2024-04-01 04:54:49, Train, Epoch : 3, Step : 1110, Loss : 1.00387, Acc : 0.675, Sensitive_Loss : 0.23007, Sensitive_Acc : 18.300, Run Time : 99.29 sec
INFO:root:2024-04-01 04:54:56, Train, Epoch : 3, Step : 1120, Loss : 0.75905, Acc : 0.766, Sensitive_Loss : 0.29716, Sensitive_Acc : 15.200, Run Time : 7.07 sec
INFO:root:2024-04-01 04:55:04, Train, Epoch : 3, Step : 1130, Loss : 0.77892, Acc : 0.662, Sensitive_Loss : 0.23869, Sensitive_Acc : 16.000, Run Time : 7.48 sec
INFO:root:2024-04-01 04:55:11, Train, Epoch : 3, Step : 1140, Loss : 0.67148, Acc : 0.672, Sensitive_Loss : 0.19228, Sensitive_Acc : 14.400, Run Time : 6.82 sec
INFO:root:2024-04-01 04:55:18, Train, Epoch : 3, Step : 1150, Loss : 0.95709, Acc : 0.709, Sensitive_Loss : 0.24290, Sensitive_Acc : 16.700, Run Time : 7.15 sec
INFO:root:2024-04-01 04:55:25, Train, Epoch : 3, Step : 1160, Loss : 0.83744, Acc : 0.731, Sensitive_Loss : 0.21724, Sensitive_Acc : 16.600, Run Time : 6.79 sec
INFO:root:2024-04-01 04:55:32, Train, Epoch : 3, Step : 1170, Loss : 0.86809, Acc : 0.697, Sensitive_Loss : 0.21851, Sensitive_Acc : 15.900, Run Time : 7.85 sec
INFO:root:2024-04-01 04:55:39, Train, Epoch : 3, Step : 1180, Loss : 0.82162, Acc : 0.697, Sensitive_Loss : 0.27814, Sensitive_Acc : 13.000, Run Time : 6.80 sec
INFO:root:2024-04-01 04:55:46, Train, Epoch : 3, Step : 1190, Loss : 0.97433, Acc : 0.684, Sensitive_Loss : 0.21541, Sensitive_Acc : 17.600, Run Time : 6.68 sec
INFO:root:2024-04-01 04:55:54, Train, Epoch : 3, Step : 1200, Loss : 0.85131, Acc : 0.694, Sensitive_Loss : 0.20538, Sensitive_Acc : 16.100, Run Time : 7.60 sec
INFO:root:2024-04-01 04:57:27, Dev, Step : 1200, Loss : 0.92734, Acc : 0.750, Auc : 0.785, Sensitive_Loss : 0.26842, Sensitive_Acc : 16.908, Sensitive_Auc : 0.992, Mean auc: 0.785, Run Time : 93.74 sec
INFO:root:2024-04-01 04:57:33, Train, Epoch : 3, Step : 1210, Loss : 1.01376, Acc : 0.675, Sensitive_Loss : 0.29113, Sensitive_Acc : 15.600, Run Time : 99.30 sec
INFO:root:2024-04-01 04:57:40, Train, Epoch : 3, Step : 1220, Loss : 0.80895, Acc : 0.675, Sensitive_Loss : 0.27187, Sensitive_Acc : 16.500, Run Time : 7.36 sec
INFO:root:2024-04-01 04:57:47, Train, Epoch : 3, Step : 1230, Loss : 0.90320, Acc : 0.691, Sensitive_Loss : 0.21286, Sensitive_Acc : 17.500, Run Time : 7.01 sec
INFO:root:2024-04-01 04:57:54, Train, Epoch : 3, Step : 1240, Loss : 0.94552, Acc : 0.688, Sensitive_Loss : 0.22762, Sensitive_Acc : 16.500, Run Time : 7.08 sec
INFO:root:2024-04-01 04:58:02, Train, Epoch : 3, Step : 1250, Loss : 0.79257, Acc : 0.662, Sensitive_Loss : 0.22463, Sensitive_Acc : 16.400, Run Time : 7.29 sec
INFO:root:2024-04-01 04:58:09, Train, Epoch : 3, Step : 1260, Loss : 0.73266, Acc : 0.716, Sensitive_Loss : 0.19124, Sensitive_Acc : 17.200, Run Time : 7.08 sec
INFO:root:2024-04-01 04:58:16, Train, Epoch : 3, Step : 1270, Loss : 0.98175, Acc : 0.688, Sensitive_Loss : 0.27981, Sensitive_Acc : 15.400, Run Time : 7.03 sec
INFO:root:2024-04-01 04:58:23, Train, Epoch : 3, Step : 1280, Loss : 0.95628, Acc : 0.672, Sensitive_Loss : 0.21171, Sensitive_Acc : 19.900, Run Time : 7.57 sec
INFO:root:2024-04-01 04:58:30, Train, Epoch : 3, Step : 1290, Loss : 0.90299, Acc : 0.697, Sensitive_Loss : 0.23138, Sensitive_Acc : 17.700, Run Time : 7.00 sec
INFO:root:2024-04-01 04:58:37, Train, Epoch : 3, Step : 1300, Loss : 0.82394, Acc : 0.688, Sensitive_Loss : 0.21028, Sensitive_Acc : 16.200, Run Time : 7.10 sec
INFO:root:2024-04-01 05:00:11, Dev, Step : 1300, Loss : 0.92118, Acc : 0.688, Auc : 0.791, Sensitive_Loss : 0.22330, Sensitive_Acc : 17.234, Sensitive_Auc : 0.992, Mean auc: 0.791, Run Time : 93.51 sec
INFO:root:2024-04-01 05:00:11, Best, Step : 1300, Loss : 0.92118, Acc : 0.688, Auc : 0.791, Sensitive_Loss : 0.22330, Sensitive_Acc : 17.234, Sensitive_Auc : 0.992, Best Auc : 0.791
INFO:root:2024-04-01 05:00:17, Train, Epoch : 3, Step : 1310, Loss : 0.79062, Acc : 0.731, Sensitive_Loss : 0.20208, Sensitive_Acc : 17.000, Run Time : 99.89 sec
INFO:root:2024-04-01 05:00:24, Train, Epoch : 3, Step : 1320, Loss : 0.75319, Acc : 0.688, Sensitive_Loss : 0.20565, Sensitive_Acc : 16.900, Run Time : 6.93 sec
INFO:root:2024-04-01 05:00:32, Train, Epoch : 3, Step : 1330, Loss : 0.92516, Acc : 0.694, Sensitive_Loss : 0.24163, Sensitive_Acc : 17.100, Run Time : 7.45 sec
INFO:root:2024-04-01 05:00:39, Train, Epoch : 3, Step : 1340, Loss : 0.84534, Acc : 0.719, Sensitive_Loss : 0.25091, Sensitive_Acc : 18.600, Run Time : 7.21 sec
INFO:root:2024-04-01 05:00:46, Train, Epoch : 3, Step : 1350, Loss : 0.74064, Acc : 0.706, Sensitive_Loss : 0.20466, Sensitive_Acc : 17.300, Run Time : 6.88 sec
INFO:root:2024-04-01 05:00:52, Train, Epoch : 3, Step : 1360, Loss : 0.81757, Acc : 0.706, Sensitive_Loss : 0.22139, Sensitive_Acc : 17.300, Run Time : 6.78 sec
INFO:root:2024-04-01 05:01:00, Train, Epoch : 3, Step : 1370, Loss : 0.77429, Acc : 0.656, Sensitive_Loss : 0.20373, Sensitive_Acc : 16.100, Run Time : 7.37 sec
INFO:root:2024-04-01 05:01:08, Train, Epoch : 3, Step : 1380, Loss : 1.01074, Acc : 0.678, Sensitive_Loss : 0.23743, Sensitive_Acc : 17.900, Run Time : 7.78 sec
INFO:root:2024-04-01 05:01:14, Train, Epoch : 3, Step : 1390, Loss : 0.93286, Acc : 0.703, Sensitive_Loss : 0.15791, Sensitive_Acc : 17.600, Run Time : 6.51 sec
INFO:root:2024-04-01 05:01:22, Train, Epoch : 3, Step : 1400, Loss : 0.87516, Acc : 0.709, Sensitive_Loss : 0.21810, Sensitive_Acc : 14.500, Run Time : 7.45 sec
INFO:root:2024-04-01 05:02:55, Dev, Step : 1400, Loss : 0.91075, Acc : 0.717, Auc : 0.795, Sensitive_Loss : 0.23200, Sensitive_Acc : 17.078, Sensitive_Auc : 0.994, Mean auc: 0.795, Run Time : 93.89 sec
INFO:root:2024-04-01 05:02:56, Best, Step : 1400, Loss : 0.91075, Acc : 0.717, Auc : 0.795, Sensitive_Loss : 0.23200, Sensitive_Acc : 17.078, Sensitive_Auc : 0.994, Best Auc : 0.795
INFO:root:2024-04-01 05:03:02, Train, Epoch : 3, Step : 1410, Loss : 0.96939, Acc : 0.688, Sensitive_Loss : 0.27416, Sensitive_Acc : 17.100, Run Time : 100.79 sec
INFO:root:2024-04-01 05:03:09, Train, Epoch : 3, Step : 1420, Loss : 0.89034, Acc : 0.706, Sensitive_Loss : 0.17642, Sensitive_Acc : 17.400, Run Time : 6.67 sec
INFO:root:2024-04-01 05:03:16, Train, Epoch : 3, Step : 1430, Loss : 0.77353, Acc : 0.669, Sensitive_Loss : 0.27863, Sensitive_Acc : 16.800, Run Time : 7.03 sec
INFO:root:2024-04-01 05:03:24, Train, Epoch : 3, Step : 1440, Loss : 0.74389, Acc : 0.672, Sensitive_Loss : 0.20879, Sensitive_Acc : 17.600, Run Time : 7.49 sec
INFO:root:2024-04-01 05:03:31, Train, Epoch : 3, Step : 1450, Loss : 0.95039, Acc : 0.681, Sensitive_Loss : 0.22800, Sensitive_Acc : 15.500, Run Time : 7.08 sec
INFO:root:2024-04-01 05:03:37, Train, Epoch : 3, Step : 1460, Loss : 0.93799, Acc : 0.653, Sensitive_Loss : 0.20166, Sensitive_Acc : 15.500, Run Time : 6.85 sec
INFO:root:2024-04-01 05:03:45, Train, Epoch : 3, Step : 1470, Loss : 0.79704, Acc : 0.731, Sensitive_Loss : 0.20968, Sensitive_Acc : 16.700, Run Time : 7.27 sec
INFO:root:2024-04-01 05:05:21
INFO:root:y_pred: [0.27923056 0.14496806 0.25976923 ... 0.5188038  0.4603716  0.09617966]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [3.80974525e-04 2.44016156e-01 9.66323078e-01 2.61784166e-01
 9.99358833e-01 7.42905249e-04 1.09820612e-01 4.07813005e-02
 2.70011574e-02 1.52966455e-01 2.09305257e-01 1.55940458e-01
 9.87720072e-01 2.80714594e-02 9.92165685e-01 8.21380198e-01
 6.01033214e-03 9.41200107e-02 9.81959939e-01 8.76318693e-01
 9.57078412e-02 7.42861852e-02 9.99852777e-01 9.76710498e-01
 2.86881030e-02 8.35672379e-01 3.64041962e-02 9.92567778e-01
 1.30190244e-02 9.80495393e-01 9.98851776e-01 9.85637009e-01
 4.03546430e-02 1.03020866e-03 3.08007628e-01 5.47484867e-03
 1.47827677e-02 6.69788718e-02 9.86346781e-01 9.17404950e-01
 4.09294572e-03 1.78078841e-02 9.53711927e-01 9.98995483e-01
 9.88400504e-02 3.35274078e-02 3.50686279e-03 5.26798368e-01
 9.99190629e-01 6.03507936e-01 9.95577455e-01 8.93348157e-01
 9.97123659e-01 9.42865372e-01 8.36261034e-01 9.85985875e-01
 9.07151341e-01 6.53831810e-02 9.98095214e-01 1.48423627e-01
 1.21293380e-03 1.45238519e-01 8.59435678e-01 9.74511623e-01
 9.85186756e-01 9.91753042e-01 3.78561050e-01 8.83874118e-01
 9.78828609e-01 7.71721363e-01 9.93797481e-01 1.00954035e-02
 3.60962600e-02 3.47988456e-01 9.71282542e-01 3.88578488e-03
 9.64471459e-01 5.79713238e-03 9.99276340e-01 3.03747445e-01
 3.66450340e-01 1.64093189e-02 6.68775365e-02 2.26797000e-01
 9.83390510e-01 8.56165588e-02 9.00528371e-01 3.78847033e-01
 2.05071956e-01 3.03129256e-01 1.09707035e-01 7.07974881e-02
 1.63413472e-02 8.19463730e-02 9.84797537e-01 8.83175731e-01
 8.67457017e-02 1.33934766e-01 9.50792491e-01 9.11979675e-01
 9.50352848e-01 2.19102893e-02 9.99320149e-01 1.88984610e-02
 7.21707866e-02 9.91692960e-01 1.78300574e-01 6.92582950e-02
 1.95765402e-02 6.23084605e-03 2.97856936e-03 9.51896846e-01
 7.51634166e-02 8.60079192e-03 2.31164441e-01 5.28620731e-04
 9.83477652e-01 3.22230696e-03 3.20366263e-01 9.18908569e-04
 5.72888017e-01 7.38320081e-03 9.81376290e-01 1.01482244e-02
 1.64376032e-02 9.96439159e-01 5.66565990e-03 8.77911925e-01
 7.41019309e-01 9.97539043e-01 7.95031898e-04 9.89686906e-01
 6.69811852e-03 9.99236822e-01 9.98785198e-01 5.69317818e-01
 3.71690601e-01 6.66998744e-01 8.89945701e-02 9.85687315e-01
 9.81974661e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-04-01 05:05:22, Dev, Step : 1476, Loss : 0.91811, Acc : 0.769, Auc : 0.792, Sensitive_Loss : 0.24434, Sensitive_Acc : 16.979, Sensitive_Auc : 0.995, Mean auc: 0.792, Run Time : 92.62 sec
INFO:root:2024-04-01 05:05:26, Train, Epoch : 4, Step : 1480, Loss : 0.23978, Acc : 0.287, Sensitive_Loss : 0.08183, Sensitive_Acc : 7.100, Run Time : 3.79 sec
INFO:root:2024-04-01 05:05:33, Train, Epoch : 4, Step : 1490, Loss : 0.84247, Acc : 0.728, Sensitive_Loss : 0.18352, Sensitive_Acc : 18.000, Run Time : 6.99 sec
INFO:root:2024-04-01 05:05:40, Train, Epoch : 4, Step : 1500, Loss : 0.72251, Acc : 0.722, Sensitive_Loss : 0.22942, Sensitive_Acc : 17.400, Run Time : 7.29 sec
INFO:root:2024-04-01 05:07:14, Dev, Step : 1500, Loss : 0.91395, Acc : 0.753, Auc : 0.792, Sensitive_Loss : 0.25276, Sensitive_Acc : 16.979, Sensitive_Auc : 0.994, Mean auc: 0.792, Run Time : 93.72 sec
INFO:root:2024-04-01 05:07:20, Train, Epoch : 4, Step : 1510, Loss : 1.00006, Acc : 0.694, Sensitive_Loss : 0.21236, Sensitive_Acc : 18.600, Run Time : 99.30 sec
INFO:root:2024-04-01 05:07:27, Train, Epoch : 4, Step : 1520, Loss : 0.80388, Acc : 0.744, Sensitive_Loss : 0.26266, Sensitive_Acc : 17.600, Run Time : 7.00 sec
INFO:root:2024-04-01 05:07:34, Train, Epoch : 4, Step : 1530, Loss : 0.82948, Acc : 0.709, Sensitive_Loss : 0.20500, Sensitive_Acc : 17.700, Run Time : 6.85 sec
INFO:root:2024-04-01 05:07:41, Train, Epoch : 4, Step : 1540, Loss : 0.73632, Acc : 0.716, Sensitive_Loss : 0.20303, Sensitive_Acc : 15.100, Run Time : 7.80 sec
INFO:root:2024-04-01 05:07:48, Train, Epoch : 4, Step : 1550, Loss : 0.79410, Acc : 0.706, Sensitive_Loss : 0.25750, Sensitive_Acc : 18.500, Run Time : 6.94 sec
INFO:root:2024-04-01 05:07:55, Train, Epoch : 4, Step : 1560, Loss : 0.79040, Acc : 0.681, Sensitive_Loss : 0.20495, Sensitive_Acc : 18.100, Run Time : 6.98 sec
INFO:root:2024-04-01 05:08:03, Train, Epoch : 4, Step : 1570, Loss : 0.87737, Acc : 0.675, Sensitive_Loss : 0.22354, Sensitive_Acc : 16.700, Run Time : 7.28 sec
INFO:root:2024-04-01 05:08:10, Train, Epoch : 4, Step : 1580, Loss : 0.94547, Acc : 0.731, Sensitive_Loss : 0.19142, Sensitive_Acc : 17.000, Run Time : 7.36 sec
INFO:root:2024-04-01 05:08:17, Train, Epoch : 4, Step : 1590, Loss : 0.87860, Acc : 0.716, Sensitive_Loss : 0.16406, Sensitive_Acc : 15.900, Run Time : 7.01 sec
INFO:root:2024-04-01 05:08:24, Train, Epoch : 4, Step : 1600, Loss : 0.87859, Acc : 0.700, Sensitive_Loss : 0.20363, Sensitive_Acc : 14.500, Run Time : 7.13 sec
INFO:root:2024-04-01 05:09:58, Dev, Step : 1600, Loss : 0.91651, Acc : 0.770, Auc : 0.791, Sensitive_Loss : 0.23246, Sensitive_Acc : 17.078, Sensitive_Auc : 0.995, Mean auc: 0.791, Run Time : 93.53 sec
INFO:root:2024-04-01 05:10:03, Train, Epoch : 4, Step : 1610, Loss : 0.94505, Acc : 0.697, Sensitive_Loss : 0.25606, Sensitive_Acc : 16.000, Run Time : 98.90 sec
INFO:root:2024-04-01 05:10:10, Train, Epoch : 4, Step : 1620, Loss : 0.90100, Acc : 0.734, Sensitive_Loss : 0.18178, Sensitive_Acc : 16.100, Run Time : 6.86 sec
INFO:root:2024-04-01 05:10:17, Train, Epoch : 4, Step : 1630, Loss : 0.99855, Acc : 0.694, Sensitive_Loss : 0.23779, Sensitive_Acc : 16.400, Run Time : 7.39 sec
INFO:root:2024-04-01 05:10:25, Train, Epoch : 4, Step : 1640, Loss : 0.74574, Acc : 0.728, Sensitive_Loss : 0.24883, Sensitive_Acc : 13.600, Run Time : 7.31 sec
INFO:root:2024-04-01 05:10:32, Train, Epoch : 4, Step : 1650, Loss : 1.08227, Acc : 0.700, Sensitive_Loss : 0.21857, Sensitive_Acc : 16.600, Run Time : 6.99 sec
INFO:root:2024-04-01 05:10:39, Train, Epoch : 4, Step : 1660, Loss : 0.80186, Acc : 0.703, Sensitive_Loss : 0.22916, Sensitive_Acc : 14.500, Run Time : 7.27 sec
INFO:root:2024-04-01 05:10:46, Train, Epoch : 4, Step : 1670, Loss : 0.72273, Acc : 0.731, Sensitive_Loss : 0.26376, Sensitive_Acc : 17.000, Run Time : 6.80 sec
INFO:root:2024-04-01 05:10:53, Train, Epoch : 4, Step : 1680, Loss : 0.85068, Acc : 0.703, Sensitive_Loss : 0.15812, Sensitive_Acc : 17.300, Run Time : 7.22 sec
INFO:root:2024-04-01 05:11:01, Train, Epoch : 4, Step : 1690, Loss : 0.74814, Acc : 0.725, Sensitive_Loss : 0.23128, Sensitive_Acc : 15.700, Run Time : 7.71 sec
INFO:root:2024-04-01 05:11:07, Train, Epoch : 4, Step : 1700, Loss : 0.78633, Acc : 0.728, Sensitive_Loss : 0.16401, Sensitive_Acc : 17.200, Run Time : 6.74 sec
INFO:root:2024-04-01 05:12:42, Dev, Step : 1700, Loss : 0.91146, Acc : 0.762, Auc : 0.795, Sensitive_Loss : 0.21416, Sensitive_Acc : 17.177, Sensitive_Auc : 0.995, Mean auc: 0.795, Run Time : 94.35 sec
INFO:root:2024-04-01 05:12:47, Train, Epoch : 4, Step : 1710, Loss : 0.68726, Acc : 0.697, Sensitive_Loss : 0.18626, Sensitive_Acc : 18.500, Run Time : 99.80 sec
INFO:root:2024-04-01 05:12:54, Train, Epoch : 4, Step : 1720, Loss : 0.87026, Acc : 0.719, Sensitive_Loss : 0.21402, Sensitive_Acc : 15.400, Run Time : 6.99 sec
INFO:root:2024-04-01 05:13:02, Train, Epoch : 4, Step : 1730, Loss : 0.73231, Acc : 0.694, Sensitive_Loss : 0.24727, Sensitive_Acc : 16.700, Run Time : 7.62 sec
INFO:root:2024-04-01 05:13:09, Train, Epoch : 4, Step : 1740, Loss : 0.87670, Acc : 0.750, Sensitive_Loss : 0.19663, Sensitive_Acc : 15.700, Run Time : 7.10 sec
INFO:root:2024-04-01 05:13:16, Train, Epoch : 4, Step : 1750, Loss : 0.89726, Acc : 0.684, Sensitive_Loss : 0.21564, Sensitive_Acc : 17.100, Run Time : 7.09 sec
INFO:root:2024-04-01 05:13:23, Train, Epoch : 4, Step : 1760, Loss : 0.93829, Acc : 0.688, Sensitive_Loss : 0.27461, Sensitive_Acc : 13.500, Run Time : 7.61 sec
INFO:root:2024-04-01 05:13:30, Train, Epoch : 4, Step : 1770, Loss : 0.86863, Acc : 0.697, Sensitive_Loss : 0.21285, Sensitive_Acc : 17.900, Run Time : 6.39 sec
INFO:root:2024-04-01 05:13:37, Train, Epoch : 4, Step : 1780, Loss : 0.85280, Acc : 0.753, Sensitive_Loss : 0.21185, Sensitive_Acc : 17.000, Run Time : 7.34 sec
INFO:root:2024-04-01 05:13:45, Train, Epoch : 4, Step : 1790, Loss : 0.89860, Acc : 0.713, Sensitive_Loss : 0.23000, Sensitive_Acc : 18.900, Run Time : 7.75 sec
INFO:root:2024-04-01 05:13:51, Train, Epoch : 4, Step : 1800, Loss : 0.82304, Acc : 0.669, Sensitive_Loss : 0.16574, Sensitive_Acc : 15.900, Run Time : 6.20 sec
INFO:root:2024-04-01 05:15:25, Dev, Step : 1800, Loss : 0.90331, Acc : 0.751, Auc : 0.798, Sensitive_Loss : 0.22354, Sensitive_Acc : 17.163, Sensitive_Auc : 0.996, Mean auc: 0.798, Run Time : 93.96 sec
INFO:root:2024-04-01 05:15:26, Best, Step : 1800, Loss : 0.90331, Acc : 0.751, Auc : 0.798, Sensitive_Loss : 0.22354, Sensitive_Acc : 17.163, Sensitive_Auc : 0.996, Best Auc : 0.798
INFO:root:2024-04-01 05:15:31, Train, Epoch : 4, Step : 1810, Loss : 0.73893, Acc : 0.688, Sensitive_Loss : 0.20854, Sensitive_Acc : 16.300, Run Time : 100.21 sec
INFO:root:2024-04-01 05:15:39, Train, Epoch : 4, Step : 1820, Loss : 0.72241, Acc : 0.703, Sensitive_Loss : 0.18612, Sensitive_Acc : 15.100, Run Time : 7.23 sec
INFO:root:2024-04-01 05:15:46, Train, Epoch : 4, Step : 1830, Loss : 0.85810, Acc : 0.725, Sensitive_Loss : 0.24634, Sensitive_Acc : 16.500, Run Time : 7.56 sec
INFO:root:2024-04-01 05:15:53, Train, Epoch : 4, Step : 1840, Loss : 0.90284, Acc : 0.678, Sensitive_Loss : 0.22323, Sensitive_Acc : 16.200, Run Time : 6.82 sec
INFO:root:2024-04-01 05:16:00, Train, Epoch : 4, Step : 1850, Loss : 0.76698, Acc : 0.706, Sensitive_Loss : 0.23414, Sensitive_Acc : 16.900, Run Time : 6.87 sec
INFO:root:2024-04-01 05:16:07, Train, Epoch : 4, Step : 1860, Loss : 0.71110, Acc : 0.706, Sensitive_Loss : 0.19411, Sensitive_Acc : 18.200, Run Time : 6.99 sec
INFO:root:2024-04-01 05:16:14, Train, Epoch : 4, Step : 1870, Loss : 0.74409, Acc : 0.694, Sensitive_Loss : 0.17603, Sensitive_Acc : 15.300, Run Time : 6.88 sec
INFO:root:2024-04-01 05:16:21, Train, Epoch : 4, Step : 1880, Loss : 0.88985, Acc : 0.706, Sensitive_Loss : 0.21947, Sensitive_Acc : 17.700, Run Time : 7.14 sec
INFO:root:2024-04-01 05:16:28, Train, Epoch : 4, Step : 1890, Loss : 0.79369, Acc : 0.691, Sensitive_Loss : 0.19892, Sensitive_Acc : 17.700, Run Time : 7.46 sec
INFO:root:2024-04-01 05:16:35, Train, Epoch : 4, Step : 1900, Loss : 0.80101, Acc : 0.713, Sensitive_Loss : 0.23274, Sensitive_Acc : 15.400, Run Time : 6.97 sec
INFO:root:2024-04-01 05:18:09, Dev, Step : 1900, Loss : 0.90865, Acc : 0.736, Auc : 0.795, Sensitive_Loss : 0.24496, Sensitive_Acc : 17.007, Sensitive_Auc : 0.997, Mean auc: 0.795, Run Time : 93.54 sec
INFO:root:2024-04-01 05:18:15, Train, Epoch : 4, Step : 1910, Loss : 0.71993, Acc : 0.691, Sensitive_Loss : 0.23908, Sensitive_Acc : 16.300, Run Time : 99.38 sec
INFO:root:2024-04-01 05:18:22, Train, Epoch : 4, Step : 1920, Loss : 1.14025, Acc : 0.644, Sensitive_Loss : 0.19294, Sensitive_Acc : 16.500, Run Time : 6.86 sec
INFO:root:2024-04-01 05:18:29, Train, Epoch : 4, Step : 1930, Loss : 0.77721, Acc : 0.728, Sensitive_Loss : 0.23806, Sensitive_Acc : 17.300, Run Time : 7.11 sec
INFO:root:2024-04-01 05:18:36, Train, Epoch : 4, Step : 1940, Loss : 0.84136, Acc : 0.719, Sensitive_Loss : 0.23701, Sensitive_Acc : 18.100, Run Time : 7.20 sec
INFO:root:2024-04-01 05:18:43, Train, Epoch : 4, Step : 1950, Loss : 0.78645, Acc : 0.734, Sensitive_Loss : 0.21403, Sensitive_Acc : 16.000, Run Time : 7.23 sec
INFO:root:2024-04-01 05:18:50, Train, Epoch : 4, Step : 1960, Loss : 0.91173, Acc : 0.713, Sensitive_Loss : 0.25554, Sensitive_Acc : 17.000, Run Time : 7.41 sec
INFO:root:2024-04-01 05:20:28
INFO:root:y_pred: [0.36875972 0.10615622 0.3155277  ... 0.43287686 0.43972135 0.12396107]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [3.58745339e-04 1.09889969e-01 9.58820105e-01 2.24387065e-01
 9.99550164e-01 3.85001447e-04 1.24407977e-01 3.13820653e-02
 4.08222303e-02 1.70745522e-01 2.14459598e-01 1.91907138e-01
 9.89269197e-01 8.53652135e-03 9.94542480e-01 8.11990201e-01
 9.69710480e-03 2.54405569e-02 9.79690671e-01 8.05478573e-01
 1.59078270e-01 8.39992911e-02 9.99840856e-01 9.67588127e-01
 2.72121355e-02 7.34043777e-01 2.05697268e-02 9.90244210e-01
 1.57275181e-02 9.80950713e-01 9.99502063e-01 9.90840256e-01
 4.83218804e-02 1.80783123e-03 2.06664413e-01 4.08285670e-03
 6.49076607e-03 7.08716810e-02 9.86342013e-01 9.55813468e-01
 3.53426184e-03 1.65267661e-02 9.73882258e-01 9.99398470e-01
 3.10878940e-02 2.16810238e-02 1.37078424e-03 4.53026772e-01
 9.99701917e-01 6.20855093e-01 9.98826325e-01 8.46237361e-01
 9.97525632e-01 9.47746754e-01 8.87290239e-01 9.69830632e-01
 9.16636586e-01 5.71212396e-02 9.98988450e-01 1.43881142e-01
 1.42916478e-03 1.66996062e-01 8.30482900e-01 9.85177159e-01
 9.94605601e-01 9.97123659e-01 5.64894617e-01 9.22896504e-01
 9.84461010e-01 6.29470646e-01 9.93741333e-01 3.84457083e-03
 3.99360508e-02 1.63324982e-01 9.85482693e-01 7.59431766e-03
 9.58016276e-01 4.17680200e-03 9.99389648e-01 4.96737689e-01
 4.17954028e-01 7.44357286e-03 6.63341135e-02 1.14254296e-01
 9.93245065e-01 7.61569664e-02 8.78773808e-01 2.60554284e-01
 1.17574587e-01 2.91750401e-01 1.66719392e-01 8.41643587e-02
 3.90987098e-02 5.91407269e-02 9.88935888e-01 9.20419335e-01
 3.25347371e-02 1.25754118e-01 9.57423449e-01 9.06004250e-01
 9.72568870e-01 1.96773466e-02 9.99234438e-01 1.23477075e-02
 3.91307622e-02 9.94025946e-01 2.62886494e-01 3.88767682e-02
 1.81892924e-02 1.02531733e-02 1.89688662e-03 9.52291071e-01
 7.82814994e-02 3.74712027e-03 1.81335077e-01 1.03537086e-03
 9.83134985e-01 2.07620719e-03 4.58327651e-01 1.27656688e-03
 6.69009447e-01 4.39318130e-03 9.90860224e-01 6.96092425e-03
 9.69086960e-03 9.98320520e-01 1.14189843e-02 8.46904457e-01
 6.29542470e-01 9.97449458e-01 7.90487509e-04 9.88077939e-01
 4.22589295e-03 9.99635935e-01 9.99204338e-01 5.64071715e-01
 4.04742062e-01 4.72051114e-01 8.20537955e-02 9.91308928e-01
 9.86951470e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-04-01 05:20:28, Dev, Step : 1968, Loss : 0.90620, Acc : 0.692, Auc : 0.803, Sensitive_Loss : 0.23831, Sensitive_Acc : 17.035, Sensitive_Auc : 0.996, Mean auc: 0.803, Run Time : 93.15 sec
INFO:root:2024-04-01 05:20:29, Best, Step : 1968, Loss : 0.90620, Acc : 0.692,Auc : 0.803, Best Auc : 0.803, Sensitive_Loss : 0.23831, Sensitive_Acc : 17.035, Sensitive_Auc : 0.996
INFO:root:2024-04-01 05:20:33, Train, Epoch : 5, Step : 1970, Loss : 0.14751, Acc : 0.153, Sensitive_Loss : 0.03558, Sensitive_Acc : 3.500, Run Time : 2.82 sec
INFO:root:2024-04-01 05:20:40, Train, Epoch : 5, Step : 1980, Loss : 0.72416, Acc : 0.738, Sensitive_Loss : 0.24683, Sensitive_Acc : 16.900, Run Time : 6.78 sec
INFO:root:2024-04-01 05:20:47, Train, Epoch : 5, Step : 1990, Loss : 0.83559, Acc : 0.725, Sensitive_Loss : 0.22569, Sensitive_Acc : 15.200, Run Time : 7.57 sec
INFO:root:2024-04-01 05:20:54, Train, Epoch : 5, Step : 2000, Loss : 0.78728, Acc : 0.725, Sensitive_Loss : 0.19631, Sensitive_Acc : 17.300, Run Time : 6.87 sec
INFO:root:2024-04-01 05:22:28, Dev, Step : 2000, Loss : 0.89691, Acc : 0.729, Auc : 0.802, Sensitive_Loss : 0.21798, Sensitive_Acc : 17.092, Sensitive_Auc : 0.997, Mean auc: 0.802, Run Time : 93.66 sec
INFO:root:2024-04-01 05:22:33, Train, Epoch : 5, Step : 2010, Loss : 0.84140, Acc : 0.684, Sensitive_Loss : 0.18358, Sensitive_Acc : 17.600, Run Time : 99.28 sec
INFO:root:2024-04-01 05:22:40, Train, Epoch : 5, Step : 2020, Loss : 0.76211, Acc : 0.719, Sensitive_Loss : 0.16046, Sensitive_Acc : 15.700, Run Time : 6.95 sec
INFO:root:2024-04-01 05:22:48, Train, Epoch : 5, Step : 2030, Loss : 0.97872, Acc : 0.700, Sensitive_Loss : 0.20479, Sensitive_Acc : 15.700, Run Time : 7.10 sec
INFO:root:2024-04-01 05:22:55, Train, Epoch : 5, Step : 2040, Loss : 0.74566, Acc : 0.759, Sensitive_Loss : 0.20795, Sensitive_Acc : 16.100, Run Time : 7.31 sec
INFO:root:2024-04-01 05:23:02, Train, Epoch : 5, Step : 2050, Loss : 0.75439, Acc : 0.672, Sensitive_Loss : 0.23018, Sensitive_Acc : 15.800, Run Time : 6.84 sec
INFO:root:2024-04-01 05:23:09, Train, Epoch : 5, Step : 2060, Loss : 0.88149, Acc : 0.734, Sensitive_Loss : 0.18017, Sensitive_Acc : 17.400, Run Time : 7.17 sec
INFO:root:2024-04-01 05:23:16, Train, Epoch : 5, Step : 2070, Loss : 0.70017, Acc : 0.709, Sensitive_Loss : 0.18843, Sensitive_Acc : 15.100, Run Time : 7.00 sec
INFO:root:2024-04-01 05:23:23, Train, Epoch : 5, Step : 2080, Loss : 0.83231, Acc : 0.728, Sensitive_Loss : 0.18701, Sensitive_Acc : 15.900, Run Time : 7.47 sec
INFO:root:2024-04-01 05:23:31, Train, Epoch : 5, Step : 2090, Loss : 0.63753, Acc : 0.716, Sensitive_Loss : 0.22134, Sensitive_Acc : 16.600, Run Time : 7.25 sec
INFO:root:2024-04-01 05:23:38, Train, Epoch : 5, Step : 2100, Loss : 0.66892, Acc : 0.719, Sensitive_Loss : 0.14473, Sensitive_Acc : 16.000, Run Time : 7.19 sec
INFO:root:2024-04-01 05:25:12, Dev, Step : 2100, Loss : 0.89984, Acc : 0.745, Auc : 0.801, Sensitive_Loss : 0.21580, Sensitive_Acc : 17.206, Sensitive_Auc : 0.997, Mean auc: 0.801, Run Time : 93.74 sec
INFO:root:2024-04-01 05:25:17, Train, Epoch : 5, Step : 2110, Loss : 0.86611, Acc : 0.716, Sensitive_Loss : 0.17636, Sensitive_Acc : 15.600, Run Time : 99.20 sec
INFO:root:2024-04-01 05:25:24, Train, Epoch : 5, Step : 2120, Loss : 0.80918, Acc : 0.731, Sensitive_Loss : 0.17999, Sensitive_Acc : 17.300, Run Time : 7.41 sec
INFO:root:2024-04-01 05:25:32, Train, Epoch : 5, Step : 2130, Loss : 0.58286, Acc : 0.750, Sensitive_Loss : 0.21523, Sensitive_Acc : 16.400, Run Time : 7.15 sec
INFO:root:2024-04-01 05:25:38, Train, Epoch : 5, Step : 2140, Loss : 0.76855, Acc : 0.716, Sensitive_Loss : 0.15586, Sensitive_Acc : 14.800, Run Time : 6.78 sec
INFO:root:2024-04-01 05:25:46, Train, Epoch : 5, Step : 2150, Loss : 0.79768, Acc : 0.719, Sensitive_Loss : 0.17624, Sensitive_Acc : 17.000, Run Time : 7.70 sec
INFO:root:2024-04-01 05:25:53, Train, Epoch : 5, Step : 2160, Loss : 0.69102, Acc : 0.691, Sensitive_Loss : 0.17759, Sensitive_Acc : 16.100, Run Time : 7.19 sec
INFO:root:2024-04-01 05:26:00, Train, Epoch : 5, Step : 2170, Loss : 0.82199, Acc : 0.741, Sensitive_Loss : 0.25404, Sensitive_Acc : 18.000, Run Time : 6.52 sec
INFO:root:2024-04-01 05:26:07, Train, Epoch : 5, Step : 2180, Loss : 0.91322, Acc : 0.719, Sensitive_Loss : 0.19175, Sensitive_Acc : 18.100, Run Time : 7.69 sec
INFO:root:2024-04-01 05:26:15, Train, Epoch : 5, Step : 2190, Loss : 0.95493, Acc : 0.709, Sensitive_Loss : 0.23677, Sensitive_Acc : 17.700, Run Time : 7.46 sec
INFO:root:2024-04-01 05:26:21, Train, Epoch : 5, Step : 2200, Loss : 0.78816, Acc : 0.722, Sensitive_Loss : 0.22541, Sensitive_Acc : 14.700, Run Time : 6.03 sec
INFO:root:2024-04-01 05:27:55, Dev, Step : 2200, Loss : 0.89693, Acc : 0.733, Auc : 0.801, Sensitive_Loss : 0.22943, Sensitive_Acc : 17.007, Sensitive_Auc : 0.998, Mean auc: 0.801, Run Time : 94.10 sec
INFO:root:2024-04-01 05:28:01, Train, Epoch : 5, Step : 2210, Loss : 1.04164, Acc : 0.688, Sensitive_Loss : 0.17631, Sensitive_Acc : 15.800, Run Time : 99.66 sec
INFO:root:2024-04-01 05:28:08, Train, Epoch : 5, Step : 2220, Loss : 0.73036, Acc : 0.675, Sensitive_Loss : 0.25132, Sensitive_Acc : 15.900, Run Time : 7.23 sec
INFO:root:2024-04-01 05:28:15, Train, Epoch : 5, Step : 2230, Loss : 0.90336, Acc : 0.728, Sensitive_Loss : 0.23882, Sensitive_Acc : 17.600, Run Time : 7.45 sec
INFO:root:2024-04-01 05:28:22, Train, Epoch : 5, Step : 2240, Loss : 0.75871, Acc : 0.709, Sensitive_Loss : 0.25319, Sensitive_Acc : 17.700, Run Time : 6.94 sec
INFO:root:2024-04-01 05:28:29, Train, Epoch : 5, Step : 2250, Loss : 0.85598, Acc : 0.734, Sensitive_Loss : 0.18613, Sensitive_Acc : 16.300, Run Time : 6.93 sec
INFO:root:2024-04-01 05:28:37, Train, Epoch : 5, Step : 2260, Loss : 0.70450, Acc : 0.713, Sensitive_Loss : 0.19252, Sensitive_Acc : 14.700, Run Time : 7.51 sec
INFO:root:2024-04-01 05:28:44, Train, Epoch : 5, Step : 2270, Loss : 0.81088, Acc : 0.725, Sensitive_Loss : 0.19649, Sensitive_Acc : 17.600, Run Time : 7.17 sec
INFO:root:2024-04-01 05:28:51, Train, Epoch : 5, Step : 2280, Loss : 0.97448, Acc : 0.703, Sensitive_Loss : 0.22005, Sensitive_Acc : 15.300, Run Time : 7.35 sec
INFO:root:2024-04-01 05:28:59, Train, Epoch : 5, Step : 2290, Loss : 0.81440, Acc : 0.756, Sensitive_Loss : 0.21692, Sensitive_Acc : 15.100, Run Time : 7.36 sec
INFO:root:2024-04-01 05:29:06, Train, Epoch : 5, Step : 2300, Loss : 0.78560, Acc : 0.703, Sensitive_Loss : 0.21142, Sensitive_Acc : 16.400, Run Time : 7.04 sec
INFO:root:2024-04-01 05:30:39, Dev, Step : 2300, Loss : 0.90370, Acc : 0.721, Auc : 0.798, Sensitive_Loss : 0.23308, Sensitive_Acc : 17.035, Sensitive_Auc : 0.997, Mean auc: 0.798, Run Time : 93.52 sec
INFO:root:2024-04-01 05:30:45, Train, Epoch : 5, Step : 2310, Loss : 0.77215, Acc : 0.716, Sensitive_Loss : 0.19774, Sensitive_Acc : 17.300, Run Time : 99.06 sec
INFO:root:2024-04-01 05:30:52, Train, Epoch : 5, Step : 2320, Loss : 0.76156, Acc : 0.703, Sensitive_Loss : 0.20501, Sensitive_Acc : 16.400, Run Time : 7.15 sec
INFO:root:2024-04-01 05:30:59, Train, Epoch : 5, Step : 2330, Loss : 0.68222, Acc : 0.731, Sensitive_Loss : 0.19615, Sensitive_Acc : 16.500, Run Time : 7.14 sec
INFO:root:2024-04-01 05:31:06, Train, Epoch : 5, Step : 2340, Loss : 0.86072, Acc : 0.722, Sensitive_Loss : 0.19885, Sensitive_Acc : 14.300, Run Time : 7.27 sec
INFO:root:2024-04-01 05:31:13, Train, Epoch : 5, Step : 2350, Loss : 0.80293, Acc : 0.744, Sensitive_Loss : 0.17187, Sensitive_Acc : 16.300, Run Time : 6.78 sec
INFO:root:2024-04-01 05:31:20, Train, Epoch : 5, Step : 2360, Loss : 0.86127, Acc : 0.747, Sensitive_Loss : 0.21380, Sensitive_Acc : 17.600, Run Time : 7.11 sec
INFO:root:2024-04-01 05:31:28, Train, Epoch : 5, Step : 2370, Loss : 0.80018, Acc : 0.719, Sensitive_Loss : 0.20583, Sensitive_Acc : 16.600, Run Time : 7.47 sec
INFO:root:2024-04-01 05:31:35, Train, Epoch : 5, Step : 2380, Loss : 0.79749, Acc : 0.744, Sensitive_Loss : 0.17533, Sensitive_Acc : 17.200, Run Time : 7.44 sec
INFO:root:2024-04-01 05:31:42, Train, Epoch : 5, Step : 2390, Loss : 0.78574, Acc : 0.753, Sensitive_Loss : 0.26348, Sensitive_Acc : 14.100, Run Time : 7.28 sec
INFO:root:2024-04-01 05:31:49, Train, Epoch : 5, Step : 2400, Loss : 0.90128, Acc : 0.731, Sensitive_Loss : 0.23706, Sensitive_Acc : 17.100, Run Time : 6.53 sec
INFO:root:2024-04-01 05:33:23, Dev, Step : 2400, Loss : 0.89025, Acc : 0.730, Auc : 0.807, Sensitive_Loss : 0.23413, Sensitive_Acc : 16.809, Sensitive_Auc : 0.998, Mean auc: 0.807, Run Time : 93.97 sec
INFO:root:2024-04-01 05:33:24, Best, Step : 2400, Loss : 0.89025, Acc : 0.730, Auc : 0.807, Sensitive_Loss : 0.23413, Sensitive_Acc : 16.809, Sensitive_Auc : 0.998, Best Auc : 0.807
INFO:root:2024-04-01 05:33:29, Train, Epoch : 5, Step : 2410, Loss : 0.90625, Acc : 0.725, Sensitive_Loss : 0.14427, Sensitive_Acc : 14.800, Run Time : 100.28 sec
INFO:root:2024-04-01 05:33:36, Train, Epoch : 5, Step : 2420, Loss : 0.86946, Acc : 0.744, Sensitive_Loss : 0.16379, Sensitive_Acc : 16.500, Run Time : 7.09 sec
INFO:root:2024-04-01 05:33:43, Train, Epoch : 5, Step : 2430, Loss : 0.71548, Acc : 0.762, Sensitive_Loss : 0.17539, Sensitive_Acc : 14.700, Run Time : 7.16 sec
INFO:root:2024-04-01 05:33:50, Train, Epoch : 5, Step : 2440, Loss : 0.79754, Acc : 0.706, Sensitive_Loss : 0.19190, Sensitive_Acc : 16.000, Run Time : 6.99 sec
INFO:root:2024-04-01 05:33:58, Train, Epoch : 5, Step : 2450, Loss : 0.83526, Acc : 0.722, Sensitive_Loss : 0.21104, Sensitive_Acc : 15.100, Run Time : 7.39 sec
INFO:root:2024-04-01 05:34:05, Train, Epoch : 5, Step : 2460, Loss : 0.88680, Acc : 0.759, Sensitive_Loss : 0.26305, Sensitive_Acc : 16.800, Run Time : 6.79 sec
INFO:root:2024-04-01 05:35:37
INFO:root:y_pred: [0.20698994 0.11527307 0.1930185  ... 0.40561703 0.27732468 0.06699093]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [4.73686785e-04 1.70889199e-01 9.56927896e-01 2.01726586e-01
 9.99493837e-01 6.78945740e-04 1.28304884e-01 4.32487726e-02
 2.10725609e-02 1.11667864e-01 3.13152432e-01 1.68269157e-01
 9.86778378e-01 1.08749652e-02 9.95817721e-01 8.70857298e-01
 6.07453892e-03 5.17744347e-02 9.78783727e-01 7.98330843e-01
 9.13128480e-02 2.44539510e-02 9.99838948e-01 9.76617157e-01
 1.37476334e-02 5.94596446e-01 1.44934952e-02 9.94947553e-01
 6.28375355e-03 9.87608910e-01 9.99630094e-01 9.89282608e-01
 2.55386550e-02 8.64644884e-04 2.97357947e-01 3.17703187e-03
 7.51461321e-03 5.77254258e-02 9.83983397e-01 9.62680578e-01
 2.21735681e-03 1.47086214e-02 9.57220972e-01 9.99204457e-01
 3.30897234e-02 4.12395857e-02 6.91451540e-04 5.08429050e-01
 9.99852538e-01 7.47399330e-01 9.98581886e-01 9.03627574e-01
 9.96798873e-01 9.68462288e-01 9.09103036e-01 9.75438476e-01
 9.49970901e-01 3.84011231e-02 9.99309301e-01 1.72135934e-01
 1.25597790e-03 1.14095353e-01 8.99632692e-01 9.83576477e-01
 9.97099638e-01 9.96308506e-01 5.81771672e-01 9.58788812e-01
 9.87418413e-01 7.18371391e-01 9.96201813e-01 2.62592291e-03
 7.23304898e-02 1.64557323e-01 9.87902582e-01 1.97294587e-03
 9.70651627e-01 1.70777319e-03 9.99011755e-01 2.28612840e-01
 3.03977638e-01 6.90484839e-03 3.72990444e-02 1.43319532e-01
 9.93523121e-01 6.02453016e-02 9.34041679e-01 3.88872951e-01
 1.66340292e-01 3.56767535e-01 6.88328743e-02 7.18880668e-02
 4.82664406e-02 5.72165102e-02 9.89926219e-01 9.14007902e-01
 3.37363109e-02 7.44833574e-02 9.68029737e-01 9.46866214e-01
 9.71692085e-01 8.85759667e-03 9.99330401e-01 1.36073763e-02
 2.09470317e-02 9.96191740e-01 1.78923592e-01 3.70112248e-02
 1.10431938e-02 6.36955863e-03 1.09297375e-03 9.83739793e-01
 4.44156937e-02 3.89415980e-03 5.34107201e-02 1.96261494e-03
 9.85784352e-01 1.32246304e-03 4.98089850e-01 3.26486188e-04
 5.82038581e-01 4.59413370e-03 9.89857197e-01 5.68689127e-03
 8.99427384e-03 9.97926831e-01 7.83279072e-03 9.14650857e-01
 6.90651178e-01 9.98785555e-01 4.98935929e-04 9.92299914e-01
 7.26065645e-03 9.99548137e-01 9.98664141e-01 6.67350292e-01
 2.51752049e-01 5.58588624e-01 3.40404026e-02 9.94058251e-01
 9.87774193e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-04-01 05:35:37, Dev, Step : 2460, Loss : 0.89850, Acc : 0.788, Auc : 0.805, Sensitive_Loss : 0.23248, Sensitive_Acc : 16.908, Sensitive_Auc : 0.997, Mean auc: 0.805, Run Time : 92.67 sec
INFO:root:2024-04-01 05:35:47, Train, Epoch : 6, Step : 2470, Loss : 0.74754, Acc : 0.738, Sensitive_Loss : 0.15468, Sensitive_Acc : 17.600, Run Time : 8.43 sec
INFO:root:2024-04-01 05:35:53, Train, Epoch : 6, Step : 2480, Loss : 0.84523, Acc : 0.766, Sensitive_Loss : 0.15149, Sensitive_Acc : 17.600, Run Time : 6.82 sec
INFO:root:2024-04-01 05:36:01, Train, Epoch : 6, Step : 2490, Loss : 0.60595, Acc : 0.756, Sensitive_Loss : 0.25262, Sensitive_Acc : 18.200, Run Time : 7.47 sec
INFO:root:2024-04-01 05:36:08, Train, Epoch : 6, Step : 2500, Loss : 0.83695, Acc : 0.731, Sensitive_Loss : 0.24195, Sensitive_Acc : 18.000, Run Time : 7.07 sec
INFO:root:2024-04-01 05:37:41, Dev, Step : 2500, Loss : 0.89386, Acc : 0.764, Auc : 0.805, Sensitive_Loss : 0.22197, Sensitive_Acc : 16.993, Sensitive_Auc : 0.998, Mean auc: 0.805, Run Time : 93.54 sec
INFO:root:2024-04-01 05:37:47, Train, Epoch : 6, Step : 2510, Loss : 0.94804, Acc : 0.731, Sensitive_Loss : 0.19669, Sensitive_Acc : 16.200, Run Time : 98.95 sec
INFO:root:2024-04-01 05:37:54, Train, Epoch : 6, Step : 2520, Loss : 0.88091, Acc : 0.750, Sensitive_Loss : 0.20475, Sensitive_Acc : 13.800, Run Time : 7.39 sec
INFO:root:2024-04-01 05:38:01, Train, Epoch : 6, Step : 2530, Loss : 0.65489, Acc : 0.700, Sensitive_Loss : 0.18326, Sensitive_Acc : 17.800, Run Time : 6.98 sec
INFO:root:2024-04-01 05:38:08, Train, Epoch : 6, Step : 2540, Loss : 0.63893, Acc : 0.697, Sensitive_Loss : 0.20627, Sensitive_Acc : 18.300, Run Time : 7.13 sec
INFO:root:2024-04-01 05:38:15, Train, Epoch : 6, Step : 2550, Loss : 0.73116, Acc : 0.719, Sensitive_Loss : 0.21802, Sensitive_Acc : 15.300, Run Time : 7.11 sec
INFO:root:2024-04-01 05:38:23, Train, Epoch : 6, Step : 2560, Loss : 0.66394, Acc : 0.741, Sensitive_Loss : 0.22672, Sensitive_Acc : 16.400, Run Time : 7.25 sec
INFO:root:2024-04-01 05:38:30, Train, Epoch : 6, Step : 2570, Loss : 0.81035, Acc : 0.759, Sensitive_Loss : 0.18565, Sensitive_Acc : 18.200, Run Time : 7.60 sec
INFO:root:2024-04-01 05:38:37, Train, Epoch : 6, Step : 2580, Loss : 0.69462, Acc : 0.722, Sensitive_Loss : 0.22598, Sensitive_Acc : 13.900, Run Time : 6.65 sec
INFO:root:2024-04-01 05:38:44, Train, Epoch : 6, Step : 2590, Loss : 0.67644, Acc : 0.700, Sensitive_Loss : 0.18321, Sensitive_Acc : 15.900, Run Time : 7.16 sec
INFO:root:2024-04-01 05:38:52, Train, Epoch : 6, Step : 2600, Loss : 0.70081, Acc : 0.725, Sensitive_Loss : 0.19419, Sensitive_Acc : 17.100, Run Time : 7.60 sec
INFO:root:2024-04-01 05:40:25, Dev, Step : 2600, Loss : 0.89484, Acc : 0.725, Auc : 0.804, Sensitive_Loss : 0.21416, Sensitive_Acc : 17.135, Sensitive_Auc : 0.998, Mean auc: 0.804, Run Time : 93.31 sec
INFO:root:2024-04-01 05:40:30, Train, Epoch : 6, Step : 2610, Loss : 0.64200, Acc : 0.747, Sensitive_Loss : 0.24820, Sensitive_Acc : 15.600, Run Time : 98.56 sec
INFO:root:2024-04-01 05:40:38, Train, Epoch : 6, Step : 2620, Loss : 0.77784, Acc : 0.719, Sensitive_Loss : 0.16399, Sensitive_Acc : 16.800, Run Time : 7.44 sec
INFO:root:2024-04-01 05:40:45, Train, Epoch : 6, Step : 2630, Loss : 0.73475, Acc : 0.728, Sensitive_Loss : 0.25048, Sensitive_Acc : 17.300, Run Time : 6.90 sec
INFO:root:2024-04-01 05:40:52, Train, Epoch : 6, Step : 2640, Loss : 0.69825, Acc : 0.725, Sensitive_Loss : 0.19437, Sensitive_Acc : 17.000, Run Time : 7.48 sec
INFO:root:2024-04-01 05:40:59, Train, Epoch : 6, Step : 2650, Loss : 0.77446, Acc : 0.769, Sensitive_Loss : 0.24642, Sensitive_Acc : 16.300, Run Time : 7.26 sec
INFO:root:2024-04-01 05:41:06, Train, Epoch : 6, Step : 2660, Loss : 0.81875, Acc : 0.703, Sensitive_Loss : 0.21902, Sensitive_Acc : 15.600, Run Time : 7.02 sec
INFO:root:2024-04-01 05:41:14, Train, Epoch : 6, Step : 2670, Loss : 1.07823, Acc : 0.703, Sensitive_Loss : 0.16042, Sensitive_Acc : 16.100, Run Time : 7.38 sec
INFO:root:2024-04-01 05:41:21, Train, Epoch : 6, Step : 2680, Loss : 0.69862, Acc : 0.734, Sensitive_Loss : 0.19483, Sensitive_Acc : 19.000, Run Time : 6.98 sec
INFO:root:2024-04-01 05:41:28, Train, Epoch : 6, Step : 2690, Loss : 0.73207, Acc : 0.769, Sensitive_Loss : 0.23780, Sensitive_Acc : 17.200, Run Time : 6.96 sec
INFO:root:2024-04-01 05:41:35, Train, Epoch : 6, Step : 2700, Loss : 0.75723, Acc : 0.753, Sensitive_Loss : 0.21998, Sensitive_Acc : 17.200, Run Time : 7.39 sec
INFO:root:2024-04-01 05:43:09, Dev, Step : 2700, Loss : 0.90475, Acc : 0.775, Auc : 0.798, Sensitive_Loss : 0.22834, Sensitive_Acc : 16.979, Sensitive_Auc : 0.998, Mean auc: 0.798, Run Time : 93.50 sec
INFO:root:2024-04-01 05:43:14, Train, Epoch : 6, Step : 2710, Loss : 0.77247, Acc : 0.741, Sensitive_Loss : 0.19100, Sensitive_Acc : 14.700, Run Time : 98.99 sec
INFO:root:2024-04-01 05:43:21, Train, Epoch : 6, Step : 2720, Loss : 0.81097, Acc : 0.722, Sensitive_Loss : 0.19837, Sensitive_Acc : 16.600, Run Time : 7.14 sec
INFO:root:2024-04-01 05:43:29, Train, Epoch : 6, Step : 2730, Loss : 0.79367, Acc : 0.731, Sensitive_Loss : 0.14509, Sensitive_Acc : 18.000, Run Time : 7.33 sec
INFO:root:2024-04-01 05:43:36, Train, Epoch : 6, Step : 2740, Loss : 0.83150, Acc : 0.728, Sensitive_Loss : 0.16026, Sensitive_Acc : 15.900, Run Time : 7.04 sec
INFO:root:2024-04-01 05:43:43, Train, Epoch : 6, Step : 2750, Loss : 0.61375, Acc : 0.750, Sensitive_Loss : 0.18534, Sensitive_Acc : 16.800, Run Time : 7.03 sec
INFO:root:2024-04-01 05:43:50, Train, Epoch : 6, Step : 2760, Loss : 0.79782, Acc : 0.722, Sensitive_Loss : 0.19074, Sensitive_Acc : 16.000, Run Time : 7.35 sec
INFO:root:2024-04-01 05:43:57, Train, Epoch : 6, Step : 2770, Loss : 0.91152, Acc : 0.728, Sensitive_Loss : 0.15984, Sensitive_Acc : 15.400, Run Time : 7.07 sec
INFO:root:2024-04-01 05:44:04, Train, Epoch : 6, Step : 2780, Loss : 0.57981, Acc : 0.697, Sensitive_Loss : 0.22545, Sensitive_Acc : 19.900, Run Time : 6.99 sec
INFO:root:2024-04-01 05:44:11, Train, Epoch : 6, Step : 2790, Loss : 0.62558, Acc : 0.722, Sensitive_Loss : 0.15743, Sensitive_Acc : 16.800, Run Time : 7.09 sec
INFO:root:2024-04-01 05:44:18, Train, Epoch : 6, Step : 2800, Loss : 0.86281, Acc : 0.741, Sensitive_Loss : 0.21982, Sensitive_Acc : 17.900, Run Time : 7.24 sec
INFO:root:2024-04-01 05:45:52, Dev, Step : 2800, Loss : 0.89064, Acc : 0.771, Auc : 0.808, Sensitive_Loss : 0.19840, Sensitive_Acc : 17.220, Sensitive_Auc : 0.998, Mean auc: 0.808, Run Time : 94.04 sec
INFO:root:2024-04-01 05:45:53, Best, Step : 2800, Loss : 0.89064, Acc : 0.771, Auc : 0.808, Sensitive_Loss : 0.19840, Sensitive_Acc : 17.220, Sensitive_Auc : 0.998, Best Auc : 0.808
INFO:root:2024-04-01 05:45:59, Train, Epoch : 6, Step : 2810, Loss : 0.66957, Acc : 0.709, Sensitive_Loss : 0.16182, Sensitive_Acc : 17.400, Run Time : 100.39 sec
INFO:root:2024-04-01 05:46:06, Train, Epoch : 6, Step : 2820, Loss : 0.70245, Acc : 0.762, Sensitive_Loss : 0.17265, Sensitive_Acc : 15.400, Run Time : 6.77 sec
INFO:root:2024-04-01 05:46:13, Train, Epoch : 6, Step : 2830, Loss : 0.72978, Acc : 0.731, Sensitive_Loss : 0.17871, Sensitive_Acc : 14.100, Run Time : 7.22 sec
INFO:root:2024-04-01 05:46:20, Train, Epoch : 6, Step : 2840, Loss : 0.87916, Acc : 0.734, Sensitive_Loss : 0.20530, Sensitive_Acc : 14.300, Run Time : 7.05 sec
INFO:root:2024-04-01 05:46:27, Train, Epoch : 6, Step : 2850, Loss : 0.62045, Acc : 0.744, Sensitive_Loss : 0.16298, Sensitive_Acc : 16.700, Run Time : 7.28 sec
INFO:root:2024-04-01 05:46:34, Train, Epoch : 6, Step : 2860, Loss : 0.56790, Acc : 0.756, Sensitive_Loss : 0.16740, Sensitive_Acc : 16.800, Run Time : 7.40 sec
INFO:root:2024-04-01 05:46:42, Train, Epoch : 6, Step : 2870, Loss : 0.76360, Acc : 0.762, Sensitive_Loss : 0.18259, Sensitive_Acc : 15.000, Run Time : 7.16 sec
INFO:root:2024-04-01 05:46:49, Train, Epoch : 6, Step : 2880, Loss : 0.76396, Acc : 0.734, Sensitive_Loss : 0.21409, Sensitive_Acc : 18.300, Run Time : 6.88 sec
INFO:root:2024-04-01 05:46:56, Train, Epoch : 6, Step : 2890, Loss : 0.88936, Acc : 0.697, Sensitive_Loss : 0.17906, Sensitive_Acc : 14.400, Run Time : 7.58 sec
INFO:root:2024-04-01 05:47:03, Train, Epoch : 6, Step : 2900, Loss : 0.77902, Acc : 0.709, Sensitive_Loss : 0.21666, Sensitive_Acc : 16.100, Run Time : 6.80 sec
INFO:root:2024-04-01 05:48:37, Dev, Step : 2900, Loss : 0.91517, Acc : 0.705, Auc : 0.796, Sensitive_Loss : 0.20678, Sensitive_Acc : 17.206, Sensitive_Auc : 0.997, Mean auc: 0.796, Run Time : 93.64 sec
INFO:root:2024-04-01 05:48:42, Train, Epoch : 6, Step : 2910, Loss : 0.78653, Acc : 0.762, Sensitive_Loss : 0.21496, Sensitive_Acc : 19.300, Run Time : 99.46 sec
INFO:root:2024-04-01 05:48:49, Train, Epoch : 6, Step : 2920, Loss : 0.83621, Acc : 0.731, Sensitive_Loss : 0.19272, Sensitive_Acc : 14.800, Run Time : 7.05 sec
INFO:root:2024-04-01 05:48:56, Train, Epoch : 6, Step : 2930, Loss : 0.80286, Acc : 0.709, Sensitive_Loss : 0.14668, Sensitive_Acc : 17.000, Run Time : 6.84 sec
INFO:root:2024-04-01 05:49:03, Train, Epoch : 6, Step : 2940, Loss : 0.65223, Acc : 0.750, Sensitive_Loss : 0.18534, Sensitive_Acc : 16.300, Run Time : 6.76 sec
INFO:root:2024-04-01 05:49:11, Train, Epoch : 6, Step : 2950, Loss : 0.73996, Acc : 0.769, Sensitive_Loss : 0.17768, Sensitive_Acc : 15.500, Run Time : 7.95 sec
INFO:root:2024-04-01 05:50:44
INFO:root:y_pred: [0.26054558 0.08340241 0.19574066 ... 0.38025418 0.18529418 0.10540833]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [6.53528026e-04 1.73829526e-01 9.45620060e-01 1.70274302e-01
 9.99597728e-01 1.16632343e-03 1.27823010e-01 4.38299887e-02
 1.90882273e-02 8.29248577e-02 8.32161829e-02 2.03621402e-01
 9.79170799e-01 3.14658182e-03 9.91791606e-01 7.42393732e-01
 8.08328949e-03 3.38218771e-02 9.51770782e-01 6.95462167e-01
 7.72652999e-02 1.75437052e-02 9.99810159e-01 9.78322566e-01
 6.41644886e-03 6.59399569e-01 1.18920105e-02 9.94998693e-01
 6.16181362e-03 9.88887608e-01 9.99622703e-01 9.90400016e-01
 3.38735469e-02 6.04280154e-04 2.90928334e-01 2.88125058e-03
 3.57914739e-03 1.59516428e-02 9.74464774e-01 9.81255352e-01
 9.07598936e-04 2.74728108e-02 9.52082455e-01 9.98840392e-01
 2.35271268e-02 5.42199053e-02 1.32976775e-03 4.65712130e-01
 9.99757826e-01 5.94137967e-01 9.98816252e-01 7.42174327e-01
 9.96872842e-01 9.74559784e-01 8.73437643e-01 9.64063108e-01
 9.31481421e-01 2.53440011e-02 9.99328375e-01 3.34436059e-01
 9.04857123e-04 9.81442407e-02 7.64883161e-01 9.66587424e-01
 9.93068695e-01 9.94690955e-01 4.27365899e-01 9.63043332e-01
 9.72864747e-01 5.83778918e-01 9.92166519e-01 7.74816726e-04
 5.72775453e-02 9.30036828e-02 9.88043427e-01 7.94451963e-03
 9.78409350e-01 1.81723374e-03 9.98580933e-01 2.70090669e-01
 2.15568796e-01 1.35102244e-02 2.43579987e-02 6.94050565e-02
 9.91687357e-01 6.13050349e-02 8.99911404e-01 2.82639533e-01
 5.17612770e-02 2.19933435e-01 8.10906589e-02 1.40135095e-01
 4.15243432e-02 3.31313089e-02 9.93402660e-01 8.89512479e-01
 2.65076403e-02 1.01348989e-01 9.61565852e-01 9.36967075e-01
 9.65844512e-01 4.15784074e-03 9.99525666e-01 6.40417356e-03
 2.15597134e-02 9.96533275e-01 1.30467921e-01 1.83728468e-02
 1.67366974e-02 7.65176630e-03 9.33569856e-04 9.78777111e-01
 5.39696887e-02 1.24788005e-03 1.12319142e-01 1.79265416e-03
 9.85393345e-01 9.84138227e-04 3.53305340e-01 8.78302206e-04
 4.45621639e-01 2.36623990e-03 9.90913749e-01 7.99222011e-03
 3.68786161e-03 9.96830761e-01 6.50462415e-03 8.07683110e-01
 5.56618989e-01 9.99128282e-01 6.33384043e-04 9.93659914e-01
 1.14976792e-02 9.99665260e-01 9.98699784e-01 5.88262916e-01
 1.22744404e-01 6.08307958e-01 3.77778076e-02 9.95495796e-01
 9.79328215e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-04-01 05:50:44, Dev, Step : 2952, Loss : 0.90074, Acc : 0.748, Auc : 0.800, Sensitive_Loss : 0.22004, Sensitive_Acc : 17.092, Sensitive_Auc : 0.997, Mean auc: 0.800, Run Time : 92.81 sec
INFO:root:2024-04-01 05:50:52, Train, Epoch : 7, Step : 2960, Loss : 0.57774, Acc : 0.575, Sensitive_Loss : 0.15742, Sensitive_Acc : 13.200, Run Time : 6.57 sec
INFO:root:2024-04-01 05:50:59, Train, Epoch : 7, Step : 2970, Loss : 0.59044, Acc : 0.797, Sensitive_Loss : 0.17273, Sensitive_Acc : 17.500, Run Time : 7.30 sec
INFO:root:2024-04-01 05:51:06, Train, Epoch : 7, Step : 2980, Loss : 0.88104, Acc : 0.713, Sensitive_Loss : 0.24741, Sensitive_Acc : 19.100, Run Time : 6.91 sec
INFO:root:2024-04-01 05:51:14, Train, Epoch : 7, Step : 2990, Loss : 0.69851, Acc : 0.722, Sensitive_Loss : 0.22495, Sensitive_Acc : 15.500, Run Time : 7.58 sec
INFO:root:2024-04-01 05:51:20, Train, Epoch : 7, Step : 3000, Loss : 0.71243, Acc : 0.787, Sensitive_Loss : 0.15825, Sensitive_Acc : 17.300, Run Time : 6.65 sec
INFO:root:2024-04-01 05:52:54, Dev, Step : 3000, Loss : 0.90052, Acc : 0.768, Auc : 0.802, Sensitive_Loss : 0.20614, Sensitive_Acc : 17.149, Sensitive_Auc : 0.998, Mean auc: 0.802, Run Time : 93.86 sec
INFO:root:2024-04-01 05:53:00, Train, Epoch : 7, Step : 3010, Loss : 0.82243, Acc : 0.716, Sensitive_Loss : 0.20789, Sensitive_Acc : 15.900, Run Time : 99.45 sec
INFO:root:2024-04-01 05:53:07, Train, Epoch : 7, Step : 3020, Loss : 0.83167, Acc : 0.725, Sensitive_Loss : 0.21513, Sensitive_Acc : 15.900, Run Time : 7.26 sec
INFO:root:2024-04-01 05:53:14, Train, Epoch : 7, Step : 3030, Loss : 0.83133, Acc : 0.747, Sensitive_Loss : 0.18606, Sensitive_Acc : 17.200, Run Time : 7.33 sec
INFO:root:2024-04-01 05:53:22, Train, Epoch : 7, Step : 3040, Loss : 0.62561, Acc : 0.741, Sensitive_Loss : 0.21677, Sensitive_Acc : 18.500, Run Time : 7.22 sec
INFO:root:2024-04-01 05:53:29, Train, Epoch : 7, Step : 3050, Loss : 0.91684, Acc : 0.731, Sensitive_Loss : 0.15151, Sensitive_Acc : 17.300, Run Time : 7.08 sec
INFO:root:2024-04-01 05:53:36, Train, Epoch : 7, Step : 3060, Loss : 0.64800, Acc : 0.778, Sensitive_Loss : 0.24994, Sensitive_Acc : 18.600, Run Time : 6.95 sec
INFO:root:2024-04-01 05:53:43, Train, Epoch : 7, Step : 3070, Loss : 0.74116, Acc : 0.762, Sensitive_Loss : 0.15581, Sensitive_Acc : 16.900, Run Time : 6.97 sec
INFO:root:2024-04-01 05:53:50, Train, Epoch : 7, Step : 3080, Loss : 0.62803, Acc : 0.731, Sensitive_Loss : 0.15763, Sensitive_Acc : 16.000, Run Time : 7.05 sec
INFO:root:2024-04-01 05:53:57, Train, Epoch : 7, Step : 3090, Loss : 0.70987, Acc : 0.759, Sensitive_Loss : 0.24391, Sensitive_Acc : 17.800, Run Time : 7.29 sec
INFO:root:2024-04-01 05:54:04, Train, Epoch : 7, Step : 3100, Loss : 0.74849, Acc : 0.747, Sensitive_Loss : 0.27101, Sensitive_Acc : 15.800, Run Time : 7.04 sec
INFO:root:2024-04-01 05:55:38, Dev, Step : 3100, Loss : 0.89710, Acc : 0.761, Auc : 0.803, Sensitive_Loss : 0.22184, Sensitive_Acc : 17.007, Sensitive_Auc : 0.997, Mean auc: 0.803, Run Time : 93.49 sec
INFO:root:2024-04-01 05:55:43, Train, Epoch : 7, Step : 3110, Loss : 0.66589, Acc : 0.772, Sensitive_Loss : 0.17674, Sensitive_Acc : 16.500, Run Time : 98.71 sec
INFO:root:2024-04-01 05:55:50, Train, Epoch : 7, Step : 3120, Loss : 0.71852, Acc : 0.738, Sensitive_Loss : 0.18051, Sensitive_Acc : 16.800, Run Time : 7.16 sec
INFO:root:2024-04-01 05:55:57, Train, Epoch : 7, Step : 3130, Loss : 0.91325, Acc : 0.753, Sensitive_Loss : 0.18414, Sensitive_Acc : 16.900, Run Time : 7.15 sec
INFO:root:2024-04-01 05:56:04, Train, Epoch : 7, Step : 3140, Loss : 0.69316, Acc : 0.747, Sensitive_Loss : 0.18942, Sensitive_Acc : 17.600, Run Time : 7.41 sec
INFO:root:2024-04-01 05:56:12, Train, Epoch : 7, Step : 3150, Loss : 0.78400, Acc : 0.750, Sensitive_Loss : 0.22282, Sensitive_Acc : 16.300, Run Time : 7.40 sec
INFO:root:2024-04-01 05:56:19, Train, Epoch : 7, Step : 3160, Loss : 0.81445, Acc : 0.716, Sensitive_Loss : 0.20354, Sensitive_Acc : 15.200, Run Time : 7.28 sec
INFO:root:2024-04-01 05:56:25, Train, Epoch : 7, Step : 3170, Loss : 0.75799, Acc : 0.738, Sensitive_Loss : 0.18673, Sensitive_Acc : 16.200, Run Time : 6.31 sec
INFO:root:2024-04-01 05:56:33, Train, Epoch : 7, Step : 3180, Loss : 0.68353, Acc : 0.759, Sensitive_Loss : 0.20150, Sensitive_Acc : 19.100, Run Time : 7.32 sec
INFO:root:2024-04-01 05:56:40, Train, Epoch : 7, Step : 3190, Loss : 0.78606, Acc : 0.772, Sensitive_Loss : 0.19727, Sensitive_Acc : 16.800, Run Time : 7.48 sec
INFO:root:2024-04-01 05:56:47, Train, Epoch : 7, Step : 3200, Loss : 0.61309, Acc : 0.716, Sensitive_Loss : 0.19533, Sensitive_Acc : 16.600, Run Time : 6.98 sec
INFO:root:2024-04-01 05:58:21, Dev, Step : 3200, Loss : 0.91255, Acc : 0.757, Auc : 0.795, Sensitive_Loss : 0.22529, Sensitive_Acc : 17.050, Sensitive_Auc : 0.998, Mean auc: 0.795, Run Time : 93.59 sec
INFO:root:2024-04-01 05:58:26, Train, Epoch : 7, Step : 3210, Loss : 0.76793, Acc : 0.719, Sensitive_Loss : 0.15356, Sensitive_Acc : 16.400, Run Time : 99.19 sec
INFO:root:2024-04-01 05:58:33, Train, Epoch : 7, Step : 3220, Loss : 0.66482, Acc : 0.756, Sensitive_Loss : 0.19004, Sensitive_Acc : 18.000, Run Time : 6.83 sec
INFO:root:2024-04-01 05:58:41, Train, Epoch : 7, Step : 3230, Loss : 0.76591, Acc : 0.738, Sensitive_Loss : 0.16156, Sensitive_Acc : 17.400, Run Time : 7.84 sec
INFO:root:2024-04-01 05:58:48, Train, Epoch : 7, Step : 3240, Loss : 0.69980, Acc : 0.762, Sensitive_Loss : 0.14026, Sensitive_Acc : 17.100, Run Time : 6.53 sec
INFO:root:2024-04-01 05:58:55, Train, Epoch : 7, Step : 3250, Loss : 0.70851, Acc : 0.766, Sensitive_Loss : 0.19336, Sensitive_Acc : 17.000, Run Time : 7.05 sec
INFO:root:2024-04-01 05:59:02, Train, Epoch : 7, Step : 3260, Loss : 0.71939, Acc : 0.731, Sensitive_Loss : 0.16504, Sensitive_Acc : 15.900, Run Time : 6.90 sec
INFO:root:2024-04-01 05:59:09, Train, Epoch : 7, Step : 3270, Loss : 0.80300, Acc : 0.734, Sensitive_Loss : 0.17761, Sensitive_Acc : 15.600, Run Time : 7.72 sec
INFO:root:2024-04-01 05:59:16, Train, Epoch : 7, Step : 3280, Loss : 0.70041, Acc : 0.756, Sensitive_Loss : 0.15428, Sensitive_Acc : 17.100, Run Time : 7.01 sec
INFO:root:2024-04-01 05:59:23, Train, Epoch : 7, Step : 3290, Loss : 0.71124, Acc : 0.791, Sensitive_Loss : 0.15718, Sensitive_Acc : 16.800, Run Time : 7.08 sec
INFO:root:2024-04-01 05:59:31, Train, Epoch : 7, Step : 3300, Loss : 0.80829, Acc : 0.741, Sensitive_Loss : 0.15115, Sensitive_Acc : 18.300, Run Time : 7.32 sec
INFO:root:2024-04-01 06:01:05, Dev, Step : 3300, Loss : 0.91565, Acc : 0.716, Auc : 0.798, Sensitive_Loss : 0.20049, Sensitive_Acc : 17.191, Sensitive_Auc : 0.998, Mean auc: 0.798, Run Time : 93.93 sec
INFO:root:2024-04-01 06:01:11, Train, Epoch : 7, Step : 3310, Loss : 0.77509, Acc : 0.747, Sensitive_Loss : 0.17698, Sensitive_Acc : 14.400, Run Time : 99.85 sec
INFO:root:2024-04-01 06:01:17, Train, Epoch : 7, Step : 3320, Loss : 0.68574, Acc : 0.803, Sensitive_Loss : 0.18463, Sensitive_Acc : 17.100, Run Time : 6.73 sec
INFO:root:2024-04-01 06:01:24, Train, Epoch : 7, Step : 3330, Loss : 0.64002, Acc : 0.716, Sensitive_Loss : 0.19876, Sensitive_Acc : 16.900, Run Time : 7.14 sec
INFO:root:2024-04-01 06:01:32, Train, Epoch : 7, Step : 3340, Loss : 0.74067, Acc : 0.725, Sensitive_Loss : 0.18505, Sensitive_Acc : 16.900, Run Time : 7.61 sec
INFO:root:2024-04-01 06:01:39, Train, Epoch : 7, Step : 3350, Loss : 0.70352, Acc : 0.769, Sensitive_Loss : 0.16584, Sensitive_Acc : 18.500, Run Time : 7.27 sec
INFO:root:2024-04-01 06:01:46, Train, Epoch : 7, Step : 3360, Loss : 0.57128, Acc : 0.703, Sensitive_Loss : 0.21409, Sensitive_Acc : 17.900, Run Time : 7.15 sec
INFO:root:2024-04-01 06:01:53, Train, Epoch : 7, Step : 3370, Loss : 0.82401, Acc : 0.766, Sensitive_Loss : 0.15812, Sensitive_Acc : 14.800, Run Time : 6.90 sec
INFO:root:2024-04-01 06:02:00, Train, Epoch : 7, Step : 3380, Loss : 0.65471, Acc : 0.738, Sensitive_Loss : 0.14752, Sensitive_Acc : 16.900, Run Time : 7.03 sec
INFO:root:2024-04-01 06:02:08, Train, Epoch : 7, Step : 3390, Loss : 0.77682, Acc : 0.772, Sensitive_Loss : 0.19189, Sensitive_Acc : 16.100, Run Time : 7.28 sec
INFO:root:2024-04-01 06:02:15, Train, Epoch : 7, Step : 3400, Loss : 0.68735, Acc : 0.775, Sensitive_Loss : 0.19621, Sensitive_Acc : 17.000, Run Time : 7.18 sec
INFO:root:2024-04-01 06:03:48, Dev, Step : 3400, Loss : 0.90737, Acc : 0.751, Auc : 0.800, Sensitive_Loss : 0.22476, Sensitive_Acc : 17.135, Sensitive_Auc : 0.997, Mean auc: 0.800, Run Time : 93.55 sec
INFO:root:2024-04-01 06:03:54, Train, Epoch : 7, Step : 3410, Loss : 0.71115, Acc : 0.728, Sensitive_Loss : 0.08741, Sensitive_Acc : 18.000, Run Time : 99.05 sec
INFO:root:2024-04-01 06:04:01, Train, Epoch : 7, Step : 3420, Loss : 0.57336, Acc : 0.728, Sensitive_Loss : 0.18463, Sensitive_Acc : 15.900, Run Time : 7.26 sec
INFO:root:2024-04-01 06:04:09, Train, Epoch : 7, Step : 3430, Loss : 0.93874, Acc : 0.744, Sensitive_Loss : 0.16160, Sensitive_Acc : 16.400, Run Time : 7.44 sec
INFO:root:2024-04-01 06:04:16, Train, Epoch : 7, Step : 3440, Loss : 0.63305, Acc : 0.747, Sensitive_Loss : 0.19263, Sensitive_Acc : 18.700, Run Time : 7.55 sec
INFO:root:2024-04-01 06:05:51
INFO:root:y_pred: [0.16255248 0.11938383 0.28394726 ... 0.46060458 0.33111677 0.11106645]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [1.01631950e-03 1.91884995e-01 9.57468987e-01 1.30245015e-01
 9.99430478e-01 1.11685705e-03 1.78602323e-01 2.68903375e-02
 8.06207210e-02 1.45359457e-01 7.50507861e-02 3.42231750e-01
 9.87283587e-01 2.59176083e-03 9.93889809e-01 7.53557682e-01
 7.32260849e-03 5.44743091e-02 9.51195478e-01 8.08258057e-01
 1.47456214e-01 1.09212557e-02 9.99841213e-01 9.84920263e-01
 5.51849045e-03 5.43035865e-01 1.19862966e-02 9.95302081e-01
 1.00654308e-02 9.94389594e-01 9.99695182e-01 9.93454754e-01
 6.05698489e-02 4.99484595e-04 4.94026631e-01 6.81702001e-03
 5.40236011e-03 1.98195577e-02 9.87326086e-01 9.90567505e-01
 3.73954815e-03 3.55229490e-02 9.60279047e-01 9.98901725e-01
 1.24961948e-02 7.84857497e-02 2.93193385e-03 4.40139502e-01
 9.99760449e-01 7.45383859e-01 9.99391675e-01 6.97337568e-01
 9.98208642e-01 9.70761597e-01 9.13799405e-01 9.77159560e-01
 9.51333344e-01 3.50203998e-02 9.99465406e-01 3.69092017e-01
 3.38720041e-03 7.86760747e-02 8.46676826e-01 9.69337702e-01
 9.96978045e-01 9.96713996e-01 4.20610517e-01 9.82191384e-01
 9.74485755e-01 5.66742599e-01 9.92224455e-01 1.19136856e-03
 6.84378967e-02 2.03878522e-01 9.93745565e-01 1.39810611e-02
 9.89782214e-01 3.34082800e-03 9.98849034e-01 3.71293724e-01
 2.69602627e-01 1.58441942e-02 2.36833505e-02 1.20023943e-01
 9.95116353e-01 5.57234623e-02 8.96847665e-01 2.97745734e-01
 6.08344823e-02 2.25986063e-01 1.05188385e-01 1.49160251e-01
 5.05792201e-02 4.07288112e-02 9.95569885e-01 9.16175783e-01
 4.06027064e-02 5.79303727e-02 9.68789518e-01 9.33602929e-01
 9.39046979e-01 4.21405304e-03 9.99582231e-01 1.14514558e-02
 2.52192039e-02 9.97529447e-01 1.75587773e-01 2.55258400e-02
 2.51022764e-02 7.58760003e-03 3.80139565e-04 9.83183503e-01
 4.01719660e-02 1.10323634e-03 1.60896301e-01 3.03776585e-03
 9.83608663e-01 3.75576015e-03 4.14435655e-01 2.48944666e-03
 4.23186690e-01 3.72709800e-03 9.88338590e-01 6.80689607e-03
 4.77407780e-03 9.97173429e-01 9.02163889e-03 9.03986931e-01
 6.98086023e-01 9.99616027e-01 1.51489244e-03 9.91596282e-01
 1.00498972e-02 9.99860883e-01 9.98911738e-01 6.50388181e-01
 1.25509128e-01 8.29702556e-01 4.81665395e-02 9.98330534e-01
 9.84404027e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-04-01 06:05:51, Dev, Step : 3444, Loss : 0.91659, Acc : 0.697, Auc : 0.801, Sensitive_Loss : 0.23669, Sensitive_Acc : 17.092, Sensitive_Auc : 0.997, Mean auc: 0.801, Run Time : 92.92 sec
INFO:root:2024-04-01 06:05:57, Train, Epoch : 8, Step : 3450, Loss : 0.49044, Acc : 0.463, Sensitive_Loss : 0.14838, Sensitive_Acc : 8.400, Run Time : 5.45 sec
INFO:root:2024-04-01 06:06:04, Train, Epoch : 8, Step : 3460, Loss : 0.71038, Acc : 0.738, Sensitive_Loss : 0.16166, Sensitive_Acc : 16.700, Run Time : 6.95 sec
INFO:root:2024-04-01 06:06:12, Train, Epoch : 8, Step : 3470, Loss : 0.89746, Acc : 0.753, Sensitive_Loss : 0.19943, Sensitive_Acc : 16.700, Run Time : 7.24 sec
INFO:root:2024-04-01 06:06:19, Train, Epoch : 8, Step : 3480, Loss : 0.71784, Acc : 0.756, Sensitive_Loss : 0.17886, Sensitive_Acc : 16.600, Run Time : 7.30 sec
INFO:root:2024-04-01 06:06:26, Train, Epoch : 8, Step : 3490, Loss : 0.60958, Acc : 0.762, Sensitive_Loss : 0.14076, Sensitive_Acc : 17.800, Run Time : 7.26 sec
INFO:root:2024-04-01 06:06:33, Train, Epoch : 8, Step : 3500, Loss : 0.55322, Acc : 0.738, Sensitive_Loss : 0.19045, Sensitive_Acc : 19.400, Run Time : 7.19 sec
INFO:root:2024-04-01 06:08:07, Dev, Step : 3500, Loss : 0.89801, Acc : 0.736, Auc : 0.806, Sensitive_Loss : 0.20939, Sensitive_Acc : 17.163, Sensitive_Auc : 0.997, Mean auc: 0.806, Run Time : 93.98 sec
INFO:root:2024-04-01 06:08:13, Train, Epoch : 8, Step : 3510, Loss : 0.61828, Acc : 0.756, Sensitive_Loss : 0.19727, Sensitive_Acc : 17.600, Run Time : 99.37 sec
INFO:root:2024-04-01 06:08:20, Train, Epoch : 8, Step : 3520, Loss : 0.72798, Acc : 0.738, Sensitive_Loss : 0.15410, Sensitive_Acc : 17.700, Run Time : 7.17 sec
INFO:root:2024-04-01 06:08:27, Train, Epoch : 8, Step : 3530, Loss : 0.75927, Acc : 0.769, Sensitive_Loss : 0.21747, Sensitive_Acc : 18.600, Run Time : 7.33 sec
INFO:root:2024-04-01 06:08:35, Train, Epoch : 8, Step : 3540, Loss : 0.76572, Acc : 0.775, Sensitive_Loss : 0.18790, Sensitive_Acc : 15.100, Run Time : 7.55 sec
INFO:root:2024-04-01 06:08:42, Train, Epoch : 8, Step : 3550, Loss : 0.69591, Acc : 0.803, Sensitive_Loss : 0.17660, Sensitive_Acc : 15.600, Run Time : 7.27 sec
INFO:root:2024-04-01 06:08:49, Train, Epoch : 8, Step : 3560, Loss : 0.70571, Acc : 0.766, Sensitive_Loss : 0.19334, Sensitive_Acc : 16.500, Run Time : 6.59 sec
INFO:root:2024-04-01 06:08:56, Train, Epoch : 8, Step : 3570, Loss : 0.63877, Acc : 0.744, Sensitive_Loss : 0.19906, Sensitive_Acc : 16.800, Run Time : 7.42 sec
INFO:root:2024-04-01 06:09:03, Train, Epoch : 8, Step : 3580, Loss : 0.74056, Acc : 0.787, Sensitive_Loss : 0.17936, Sensitive_Acc : 14.000, Run Time : 7.34 sec
INFO:root:2024-04-01 06:09:10, Train, Epoch : 8, Step : 3590, Loss : 0.66003, Acc : 0.759, Sensitive_Loss : 0.20908, Sensitive_Acc : 19.300, Run Time : 6.60 sec
INFO:root:2024-04-01 06:09:18, Train, Epoch : 8, Step : 3600, Loss : 0.76834, Acc : 0.747, Sensitive_Loss : 0.14578, Sensitive_Acc : 16.300, Run Time : 8.14 sec
INFO:root:2024-04-01 06:10:51, Dev, Step : 3600, Loss : 0.90473, Acc : 0.729, Auc : 0.804, Sensitive_Loss : 0.21019, Sensitive_Acc : 17.135, Sensitive_Auc : 0.998, Mean auc: 0.804, Run Time : 92.75 sec
INFO:root:2024-04-01 06:10:56, Train, Epoch : 8, Step : 3610, Loss : 0.62016, Acc : 0.731, Sensitive_Loss : 0.13740, Sensitive_Acc : 15.900, Run Time : 97.97 sec
INFO:root:2024-04-01 06:11:04, Train, Epoch : 8, Step : 3620, Loss : 0.61315, Acc : 0.747, Sensitive_Loss : 0.20523, Sensitive_Acc : 17.600, Run Time : 7.62 sec
INFO:root:2024-04-01 06:11:11, Train, Epoch : 8, Step : 3630, Loss : 0.70073, Acc : 0.766, Sensitive_Loss : 0.22136, Sensitive_Acc : 16.500, Run Time : 6.83 sec
INFO:root:2024-04-01 06:11:18, Train, Epoch : 8, Step : 3640, Loss : 0.55702, Acc : 0.709, Sensitive_Loss : 0.19082, Sensitive_Acc : 17.600, Run Time : 7.27 sec
INFO:root:2024-04-01 06:11:25, Train, Epoch : 8, Step : 3650, Loss : 0.67904, Acc : 0.750, Sensitive_Loss : 0.17857, Sensitive_Acc : 17.400, Run Time : 7.39 sec
INFO:root:2024-04-01 06:11:32, Train, Epoch : 8, Step : 3660, Loss : 0.63322, Acc : 0.794, Sensitive_Loss : 0.18912, Sensitive_Acc : 17.400, Run Time : 6.95 sec
INFO:root:2024-04-01 06:11:40, Train, Epoch : 8, Step : 3670, Loss : 0.81061, Acc : 0.744, Sensitive_Loss : 0.20618, Sensitive_Acc : 16.100, Run Time : 7.33 sec
INFO:root:2024-04-01 06:11:47, Train, Epoch : 8, Step : 3680, Loss : 0.66231, Acc : 0.759, Sensitive_Loss : 0.15175, Sensitive_Acc : 16.600, Run Time : 7.41 sec
INFO:root:2024-04-01 06:11:54, Train, Epoch : 8, Step : 3690, Loss : 0.63074, Acc : 0.744, Sensitive_Loss : 0.16850, Sensitive_Acc : 17.300, Run Time : 7.17 sec
INFO:root:2024-04-01 06:12:01, Train, Epoch : 8, Step : 3700, Loss : 0.70371, Acc : 0.747, Sensitive_Loss : 0.20043, Sensitive_Acc : 15.900, Run Time : 6.95 sec
INFO:root:2024-04-01 06:13:35, Dev, Step : 3700, Loss : 0.90416, Acc : 0.768, Auc : 0.801, Sensitive_Loss : 0.20381, Sensitive_Acc : 17.135, Sensitive_Auc : 0.999, Mean auc: 0.801, Run Time : 93.84 sec
INFO:root:2024-04-01 06:13:41, Train, Epoch : 8, Step : 3710, Loss : 0.80604, Acc : 0.766, Sensitive_Loss : 0.21440, Sensitive_Acc : 16.200, Run Time : 99.66 sec
INFO:root:2024-04-01 06:13:48, Train, Epoch : 8, Step : 3720, Loss : 0.52341, Acc : 0.800, Sensitive_Loss : 0.18953, Sensitive_Acc : 16.500, Run Time : 7.15 sec
INFO:root:2024-04-01 06:13:55, Train, Epoch : 8, Step : 3730, Loss : 0.74831, Acc : 0.769, Sensitive_Loss : 0.15298, Sensitive_Acc : 17.400, Run Time : 6.88 sec
INFO:root:2024-04-01 06:14:02, Train, Epoch : 8, Step : 3740, Loss : 0.71714, Acc : 0.759, Sensitive_Loss : 0.19083, Sensitive_Acc : 16.800, Run Time : 7.07 sec
INFO:root:2024-04-01 06:14:09, Train, Epoch : 8, Step : 3750, Loss : 0.82606, Acc : 0.766, Sensitive_Loss : 0.18971, Sensitive_Acc : 17.100, Run Time : 7.27 sec
INFO:root:2024-04-01 06:14:16, Train, Epoch : 8, Step : 3760, Loss : 0.73228, Acc : 0.766, Sensitive_Loss : 0.18690, Sensitive_Acc : 15.300, Run Time : 6.96 sec
INFO:root:2024-04-01 06:14:23, Train, Epoch : 8, Step : 3770, Loss : 0.76519, Acc : 0.784, Sensitive_Loss : 0.21293, Sensitive_Acc : 18.100, Run Time : 7.11 sec
INFO:root:2024-04-01 06:14:31, Train, Epoch : 8, Step : 3780, Loss : 0.74196, Acc : 0.766, Sensitive_Loss : 0.14277, Sensitive_Acc : 16.900, Run Time : 7.49 sec
INFO:root:2024-04-01 06:14:38, Train, Epoch : 8, Step : 3790, Loss : 0.63790, Acc : 0.769, Sensitive_Loss : 0.17965, Sensitive_Acc : 16.800, Run Time : 7.08 sec
INFO:root:2024-04-01 06:14:45, Train, Epoch : 8, Step : 3800, Loss : 0.60837, Acc : 0.747, Sensitive_Loss : 0.12349, Sensitive_Acc : 15.400, Run Time : 7.15 sec
INFO:root:2024-04-01 06:16:18, Dev, Step : 3800, Loss : 0.90634, Acc : 0.752, Auc : 0.799, Sensitive_Loss : 0.22059, Sensitive_Acc : 16.993, Sensitive_Auc : 0.998, Mean auc: 0.799, Run Time : 93.40 sec
INFO:root:2024-04-01 06:16:24, Train, Epoch : 8, Step : 3810, Loss : 0.66676, Acc : 0.734, Sensitive_Loss : 0.20489, Sensitive_Acc : 18.000, Run Time : 98.85 sec
INFO:root:2024-04-01 06:16:31, Train, Epoch : 8, Step : 3820, Loss : 0.74091, Acc : 0.738, Sensitive_Loss : 0.17536, Sensitive_Acc : 17.200, Run Time : 7.41 sec
INFO:root:2024-04-01 06:16:39, Train, Epoch : 8, Step : 3830, Loss : 0.73355, Acc : 0.759, Sensitive_Loss : 0.16572, Sensitive_Acc : 14.900, Run Time : 7.44 sec
INFO:root:2024-04-01 06:16:45, Train, Epoch : 8, Step : 3840, Loss : 0.63031, Acc : 0.725, Sensitive_Loss : 0.17227, Sensitive_Acc : 16.700, Run Time : 6.84 sec
INFO:root:2024-04-01 06:16:53, Train, Epoch : 8, Step : 3850, Loss : 0.67908, Acc : 0.772, Sensitive_Loss : 0.17400, Sensitive_Acc : 17.700, Run Time : 7.78 sec
INFO:root:2024-04-01 06:17:00, Train, Epoch : 8, Step : 3860, Loss : 0.87601, Acc : 0.769, Sensitive_Loss : 0.15523, Sensitive_Acc : 17.400, Run Time : 6.86 sec
INFO:root:2024-04-01 06:17:07, Train, Epoch : 8, Step : 3870, Loss : 0.63319, Acc : 0.716, Sensitive_Loss : 0.19758, Sensitive_Acc : 15.900, Run Time : 6.72 sec
INFO:root:2024-04-01 06:17:14, Train, Epoch : 8, Step : 3880, Loss : 0.78354, Acc : 0.738, Sensitive_Loss : 0.15611, Sensitive_Acc : 15.800, Run Time : 7.13 sec
INFO:root:2024-04-01 06:17:22, Train, Epoch : 8, Step : 3890, Loss : 0.75479, Acc : 0.762, Sensitive_Loss : 0.21694, Sensitive_Acc : 15.600, Run Time : 7.90 sec
INFO:root:2024-04-01 06:17:29, Train, Epoch : 8, Step : 3900, Loss : 0.62931, Acc : 0.800, Sensitive_Loss : 0.17137, Sensitive_Acc : 15.900, Run Time : 6.82 sec
INFO:root:2024-04-01 06:19:02, Dev, Step : 3900, Loss : 0.89744, Acc : 0.762, Auc : 0.804, Sensitive_Loss : 0.21669, Sensitive_Acc : 16.922, Sensitive_Auc : 0.998, Mean auc: 0.804, Run Time : 93.36 sec
INFO:root:2024-04-01 06:19:08, Train, Epoch : 8, Step : 3910, Loss : 0.75353, Acc : 0.744, Sensitive_Loss : 0.16309, Sensitive_Acc : 16.900, Run Time : 99.04 sec
INFO:root:2024-04-01 06:19:15, Train, Epoch : 8, Step : 3920, Loss : 0.66094, Acc : 0.800, Sensitive_Loss : 0.15343, Sensitive_Acc : 18.900, Run Time : 6.89 sec
INFO:root:2024-04-01 06:19:22, Train, Epoch : 8, Step : 3930, Loss : 0.82924, Acc : 0.762, Sensitive_Loss : 0.20125, Sensitive_Acc : 16.600, Run Time : 7.51 sec
INFO:root:2024-04-01 06:20:59
INFO:root:y_pred: [0.20555936 0.04475839 0.13931611 ... 0.2211936  0.11885145 0.11114531]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [4.77516674e-04 1.89013854e-01 9.57081616e-01 8.42254832e-02
 9.99229074e-01 6.48338348e-04 1.37996480e-01 1.86173599e-02
 2.06733644e-02 5.46217263e-02 7.47239739e-02 2.58557200e-01
 9.81909513e-01 1.60026096e-03 9.97179866e-01 7.88843393e-01
 2.84976722e-03 2.00161822e-02 9.62916613e-01 7.81941533e-01
 9.86997336e-02 7.27470359e-03 9.99855161e-01 9.81293857e-01
 3.94614553e-03 3.84530425e-01 4.12495947e-03 9.94292974e-01
 3.40371532e-03 9.95037258e-01 9.99758542e-01 9.93975461e-01
 1.70387998e-02 2.39545217e-04 3.52652133e-01 1.04587351e-03
 7.52634369e-03 1.05971033e-02 9.86325324e-01 9.83027160e-01
 1.87418354e-03 2.78984215e-02 9.48889554e-01 9.98899341e-01
 4.16185660e-03 4.99029681e-02 6.07892522e-04 4.43408102e-01
 9.99864697e-01 7.72770941e-01 9.99459565e-01 5.15335739e-01
 9.97225285e-01 9.74854827e-01 9.14144635e-01 9.59126949e-01
 9.40320849e-01 1.64661687e-02 9.99286354e-01 2.96829313e-01
 4.40590922e-03 7.07426518e-02 8.23915005e-01 9.64092255e-01
 9.96899724e-01 9.97663260e-01 5.15312850e-01 9.78719413e-01
 9.77281511e-01 6.58790886e-01 9.94131863e-01 7.81447627e-04
 2.52540857e-02 1.72146648e-01 9.93291140e-01 9.38746787e-04
 9.86447453e-01 7.96678301e-04 9.98599112e-01 9.84178334e-02
 2.21746787e-01 1.38341207e-02 6.58000819e-03 6.25814348e-02
 9.96571898e-01 2.93174405e-02 8.32864523e-01 2.26638451e-01
 3.17357853e-02 1.51526764e-01 5.31049520e-02 5.86943179e-02
 1.00913923e-02 3.95023972e-02 9.92335260e-01 9.24251556e-01
 4.55545038e-02 3.82503308e-02 9.57699656e-01 9.29080069e-01
 9.40110803e-01 1.59973686e-03 9.99465048e-01 1.09581202e-02
 5.69327688e-03 9.96862650e-01 2.21580967e-01 1.57414470e-02
 6.48416765e-03 5.00608794e-03 1.16159463e-04 9.82564926e-01
 2.58412547e-02 9.87874344e-04 1.18443415e-01 2.35503702e-03
 9.82625484e-01 1.37307204e-03 3.19798291e-01 4.49786399e-04
 2.74023086e-01 2.69820308e-03 9.90735412e-01 6.38852362e-03
 2.06506136e-03 9.98079300e-01 7.62012508e-03 8.89385760e-01
 6.95186198e-01 9.99457181e-01 2.37571163e-04 9.95035410e-01
 4.74522682e-03 9.99706805e-01 9.96902525e-01 6.43624187e-01
 6.16762526e-02 7.27510452e-01 1.15738325e-02 9.97154355e-01
 9.88965929e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-04-01 06:20:59, Dev, Step : 3936, Loss : 0.91013, Acc : 0.784, Auc : 0.798, Sensitive_Loss : 0.20801, Sensitive_Acc : 17.064, Sensitive_Auc : 0.998, Mean auc: 0.798, Run Time : 93.13 sec
INFO:root:2024-04-01 06:21:04, Train, Epoch : 9, Step : 3940, Loss : 0.27397, Acc : 0.316, Sensitive_Loss : 0.07264, Sensitive_Acc : 5.400, Run Time : 4.04 sec
INFO:root:2024-04-01 06:21:11, Train, Epoch : 9, Step : 3950, Loss : 0.61467, Acc : 0.769, Sensitive_Loss : 0.21701, Sensitive_Acc : 16.700, Run Time : 6.89 sec
INFO:root:2024-04-01 06:21:18, Train, Epoch : 9, Step : 3960, Loss : 0.67718, Acc : 0.762, Sensitive_Loss : 0.15504, Sensitive_Acc : 18.900, Run Time : 7.17 sec
INFO:root:2024-04-01 06:21:25, Train, Epoch : 9, Step : 3970, Loss : 0.61772, Acc : 0.819, Sensitive_Loss : 0.15423, Sensitive_Acc : 14.900, Run Time : 7.35 sec
INFO:root:2024-04-01 06:21:32, Train, Epoch : 9, Step : 3980, Loss : 0.52544, Acc : 0.753, Sensitive_Loss : 0.15491, Sensitive_Acc : 17.300, Run Time : 7.02 sec
INFO:root:2024-04-01 06:21:39, Train, Epoch : 9, Step : 3990, Loss : 0.75325, Acc : 0.762, Sensitive_Loss : 0.14564, Sensitive_Acc : 16.900, Run Time : 6.78 sec
INFO:root:2024-04-01 06:21:47, Train, Epoch : 9, Step : 4000, Loss : 0.76809, Acc : 0.741, Sensitive_Loss : 0.27427, Sensitive_Acc : 17.400, Run Time : 7.49 sec
INFO:root:2024-04-01 06:23:20, Dev, Step : 4000, Loss : 0.92046, Acc : 0.727, Auc : 0.799, Sensitive_Loss : 0.21685, Sensitive_Acc : 16.965, Sensitive_Auc : 0.997, Mean auc: 0.799, Run Time : 93.81 sec
INFO:root:2024-04-01 06:23:26, Train, Epoch : 9, Step : 4010, Loss : 0.64046, Acc : 0.791, Sensitive_Loss : 0.18564, Sensitive_Acc : 17.800, Run Time : 99.45 sec
INFO:root:2024-04-01 06:23:33, Train, Epoch : 9, Step : 4020, Loss : 0.59646, Acc : 0.766, Sensitive_Loss : 0.15641, Sensitive_Acc : 16.500, Run Time : 7.02 sec
INFO:root:2024-04-01 06:23:40, Train, Epoch : 9, Step : 4030, Loss : 0.56663, Acc : 0.722, Sensitive_Loss : 0.11347, Sensitive_Acc : 14.700, Run Time : 7.36 sec
INFO:root:2024-04-01 06:23:48, Train, Epoch : 9, Step : 4040, Loss : 0.70904, Acc : 0.781, Sensitive_Loss : 0.19362, Sensitive_Acc : 18.100, Run Time : 7.26 sec
INFO:root:2024-04-01 06:23:54, Train, Epoch : 9, Step : 4050, Loss : 0.68702, Acc : 0.747, Sensitive_Loss : 0.18315, Sensitive_Acc : 19.000, Run Time : 6.79 sec
INFO:root:2024-04-01 06:24:02, Train, Epoch : 9, Step : 4060, Loss : 0.63337, Acc : 0.738, Sensitive_Loss : 0.18466, Sensitive_Acc : 17.500, Run Time : 7.30 sec
INFO:root:2024-04-01 06:24:09, Train, Epoch : 9, Step : 4070, Loss : 0.62434, Acc : 0.784, Sensitive_Loss : 0.13542, Sensitive_Acc : 16.400, Run Time : 7.19 sec
INFO:root:2024-04-01 06:24:16, Train, Epoch : 9, Step : 4080, Loss : 0.58071, Acc : 0.787, Sensitive_Loss : 0.16123, Sensitive_Acc : 16.900, Run Time : 7.18 sec
INFO:root:2024-04-01 06:24:23, Train, Epoch : 9, Step : 4090, Loss : 0.72722, Acc : 0.791, Sensitive_Loss : 0.12784, Sensitive_Acc : 15.100, Run Time : 7.11 sec
INFO:root:2024-04-01 06:24:30, Train, Epoch : 9, Step : 4100, Loss : 0.62953, Acc : 0.750, Sensitive_Loss : 0.15511, Sensitive_Acc : 16.200, Run Time : 7.03 sec
INFO:root:2024-04-01 06:26:04, Dev, Step : 4100, Loss : 0.91841, Acc : 0.766, Auc : 0.798, Sensitive_Loss : 0.19960, Sensitive_Acc : 17.064, Sensitive_Auc : 0.997, Mean auc: 0.798, Run Time : 93.83 sec
INFO:root:2024-04-01 06:26:09, Train, Epoch : 9, Step : 4110, Loss : 0.75798, Acc : 0.759, Sensitive_Loss : 0.15527, Sensitive_Acc : 17.700, Run Time : 99.10 sec
INFO:root:2024-04-01 06:26:17, Train, Epoch : 9, Step : 4120, Loss : 0.56394, Acc : 0.784, Sensitive_Loss : 0.16426, Sensitive_Acc : 16.800, Run Time : 7.51 sec
INFO:root:2024-04-01 06:26:24, Train, Epoch : 9, Step : 4130, Loss : 0.74289, Acc : 0.762, Sensitive_Loss : 0.19250, Sensitive_Acc : 18.800, Run Time : 7.12 sec
INFO:root:2024-04-01 06:26:31, Train, Epoch : 9, Step : 4140, Loss : 0.63153, Acc : 0.819, Sensitive_Loss : 0.17842, Sensitive_Acc : 16.200, Run Time : 7.20 sec
INFO:root:2024-04-01 06:26:38, Train, Epoch : 9, Step : 4150, Loss : 0.62991, Acc : 0.775, Sensitive_Loss : 0.21108, Sensitive_Acc : 17.200, Run Time : 6.96 sec
INFO:root:2024-04-01 06:26:45, Train, Epoch : 9, Step : 4160, Loss : 0.53630, Acc : 0.766, Sensitive_Loss : 0.18286, Sensitive_Acc : 16.800, Run Time : 7.16 sec
INFO:root:2024-04-01 06:26:53, Train, Epoch : 9, Step : 4170, Loss : 0.62242, Acc : 0.766, Sensitive_Loss : 0.14299, Sensitive_Acc : 15.000, Run Time : 7.40 sec
INFO:root:2024-04-01 06:27:00, Train, Epoch : 9, Step : 4180, Loss : 0.65668, Acc : 0.819, Sensitive_Loss : 0.23590, Sensitive_Acc : 17.800, Run Time : 6.86 sec
INFO:root:2024-04-01 06:27:07, Train, Epoch : 9, Step : 4190, Loss : 0.71350, Acc : 0.756, Sensitive_Loss : 0.15878, Sensitive_Acc : 17.600, Run Time : 7.48 sec
INFO:root:2024-04-01 06:27:14, Train, Epoch : 9, Step : 4200, Loss : 0.74245, Acc : 0.762, Sensitive_Loss : 0.17951, Sensitive_Acc : 14.500, Run Time : 6.89 sec
INFO:root:2024-04-01 06:28:48, Dev, Step : 4200, Loss : 0.93312, Acc : 0.787, Auc : 0.795, Sensitive_Loss : 0.19688, Sensitive_Acc : 17.092, Sensitive_Auc : 0.997, Mean auc: 0.795, Run Time : 93.90 sec
INFO:root:2024-04-01 06:28:53, Train, Epoch : 9, Step : 4210, Loss : 0.59223, Acc : 0.803, Sensitive_Loss : 0.15021, Sensitive_Acc : 17.500, Run Time : 99.39 sec
INFO:root:2024-04-01 06:29:01, Train, Epoch : 9, Step : 4220, Loss : 0.65952, Acc : 0.772, Sensitive_Loss : 0.16423, Sensitive_Acc : 14.400, Run Time : 7.26 sec
INFO:root:2024-04-01 06:29:08, Train, Epoch : 9, Step : 4230, Loss : 0.73161, Acc : 0.791, Sensitive_Loss : 0.16766, Sensitive_Acc : 15.800, Run Time : 7.47 sec
INFO:root:2024-04-01 06:29:15, Train, Epoch : 9, Step : 4240, Loss : 0.53868, Acc : 0.778, Sensitive_Loss : 0.19858, Sensitive_Acc : 12.700, Run Time : 7.24 sec
INFO:root:2024-04-01 06:29:23, Train, Epoch : 9, Step : 4250, Loss : 0.66887, Acc : 0.747, Sensitive_Loss : 0.19391, Sensitive_Acc : 15.200, Run Time : 7.32 sec
INFO:root:2024-04-01 06:29:30, Train, Epoch : 9, Step : 4260, Loss : 0.65035, Acc : 0.756, Sensitive_Loss : 0.18893, Sensitive_Acc : 13.500, Run Time : 7.09 sec
INFO:root:2024-04-01 06:29:37, Train, Epoch : 9, Step : 4270, Loss : 0.73332, Acc : 0.769, Sensitive_Loss : 0.16840, Sensitive_Acc : 16.500, Run Time : 6.84 sec
INFO:root:2024-04-01 06:29:44, Train, Epoch : 9, Step : 4280, Loss : 0.69586, Acc : 0.800, Sensitive_Loss : 0.15747, Sensitive_Acc : 16.300, Run Time : 7.36 sec
INFO:root:2024-04-01 06:29:51, Train, Epoch : 9, Step : 4290, Loss : 0.65823, Acc : 0.787, Sensitive_Loss : 0.17020, Sensitive_Acc : 16.500, Run Time : 7.10 sec
INFO:root:2024-04-01 06:29:58, Train, Epoch : 9, Step : 4300, Loss : 0.71473, Acc : 0.775, Sensitive_Loss : 0.15006, Sensitive_Acc : 17.400, Run Time : 6.97 sec
INFO:root:2024-04-01 06:31:32, Dev, Step : 4300, Loss : 0.91769, Acc : 0.770, Auc : 0.799, Sensitive_Loss : 0.19451, Sensitive_Acc : 17.135, Sensitive_Auc : 0.997, Mean auc: 0.799, Run Time : 93.63 sec
INFO:root:2024-04-01 06:31:37, Train, Epoch : 9, Step : 4310, Loss : 0.65145, Acc : 0.791, Sensitive_Loss : 0.16102, Sensitive_Acc : 16.900, Run Time : 99.08 sec
INFO:root:2024-04-01 06:31:44, Train, Epoch : 9, Step : 4320, Loss : 0.64541, Acc : 0.772, Sensitive_Loss : 0.12953, Sensitive_Acc : 16.300, Run Time : 7.27 sec
INFO:root:2024-04-01 06:31:52, Train, Epoch : 9, Step : 4330, Loss : 0.59774, Acc : 0.803, Sensitive_Loss : 0.15241, Sensitive_Acc : 17.400, Run Time : 7.59 sec
INFO:root:2024-04-01 06:31:58, Train, Epoch : 9, Step : 4340, Loss : 0.64241, Acc : 0.803, Sensitive_Loss : 0.16683, Sensitive_Acc : 18.100, Run Time : 6.52 sec
INFO:root:2024-04-01 06:32:06, Train, Epoch : 9, Step : 4350, Loss : 0.59799, Acc : 0.762, Sensitive_Loss : 0.18733, Sensitive_Acc : 16.300, Run Time : 7.15 sec
INFO:root:2024-04-01 06:32:13, Train, Epoch : 9, Step : 4360, Loss : 0.61810, Acc : 0.781, Sensitive_Loss : 0.22777, Sensitive_Acc : 17.600, Run Time : 7.61 sec
INFO:root:2024-04-01 06:32:21, Train, Epoch : 9, Step : 4370, Loss : 0.59337, Acc : 0.812, Sensitive_Loss : 0.15972, Sensitive_Acc : 16.600, Run Time : 7.29 sec
INFO:root:2024-04-01 06:32:28, Train, Epoch : 9, Step : 4380, Loss : 0.68097, Acc : 0.800, Sensitive_Loss : 0.14614, Sensitive_Acc : 15.900, Run Time : 7.10 sec
INFO:root:2024-04-01 06:32:35, Train, Epoch : 9, Step : 4390, Loss : 0.57376, Acc : 0.800, Sensitive_Loss : 0.15647, Sensitive_Acc : 16.700, Run Time : 7.40 sec
INFO:root:2024-04-01 06:32:42, Train, Epoch : 9, Step : 4400, Loss : 0.65986, Acc : 0.803, Sensitive_Loss : 0.17544, Sensitive_Acc : 18.700, Run Time : 7.08 sec
INFO:root:2024-04-01 06:34:16, Dev, Step : 4400, Loss : 0.93036, Acc : 0.786, Auc : 0.796, Sensitive_Loss : 0.19616, Sensitive_Acc : 17.035, Sensitive_Auc : 0.998, Mean auc: 0.796, Run Time : 93.58 sec
INFO:root:2024-04-01 06:34:21, Train, Epoch : 9, Step : 4410, Loss : 0.63438, Acc : 0.791, Sensitive_Loss : 0.18574, Sensitive_Acc : 14.100, Run Time : 99.35 sec
INFO:root:2024-04-01 06:34:28, Train, Epoch : 9, Step : 4420, Loss : 0.75347, Acc : 0.791, Sensitive_Loss : 0.13230, Sensitive_Acc : 17.100, Run Time : 6.87 sec
INFO:root:2024-04-01 06:36:06
INFO:root:y_pred: [0.17828287 0.11272148 0.16100481 ... 0.27139997 0.21007156 0.07963333]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [2.6043164e-04 1.2012412e-01 9.2054355e-01 4.6119131e-02 9.9936384e-01
 2.9609908e-04 1.0130296e-01 1.0920622e-02 1.6612448e-02 3.8024291e-02
 2.8151164e-02 1.1923996e-01 9.6144450e-01 6.4382859e-04 9.9032307e-01
 7.1501821e-01 2.7587032e-03 1.7445542e-02 9.1817486e-01 6.6058916e-01
 6.1622348e-02 2.5018821e-03 9.9975842e-01 9.7381592e-01 1.3144541e-03
 2.6342154e-01 2.0419932e-03 9.9034673e-01 1.7510892e-03 9.8951626e-01
 9.9952960e-01 9.9162501e-01 6.2889694e-03 1.0183715e-04 2.8932640e-01
 1.2396537e-03 2.2204588e-03 3.9773537e-03 9.7482705e-01 9.7717446e-01
 4.5967445e-04 2.8951855e-02 8.4600806e-01 9.9807149e-01 1.8022757e-03
 2.1656860e-02 2.6258308e-04 2.6440296e-01 9.9982870e-01 6.1919928e-01
 9.9930966e-01 3.0423060e-01 9.9730778e-01 9.4336307e-01 8.8576311e-01
 9.5996851e-01 9.3083119e-01 1.2844567e-02 9.9935514e-01 1.7499058e-01
 3.3381886e-03 4.4202033e-02 6.6537970e-01 9.6053714e-01 9.8932576e-01
 9.9594879e-01 4.0950719e-01 9.5129460e-01 9.7261852e-01 3.3470157e-01
 9.9003029e-01 1.8271418e-04 1.6024899e-02 6.4855807e-02 9.8934388e-01
 3.8556380e-03 9.7493601e-01 4.6280446e-04 9.9861538e-01 2.3997949e-02
 4.1699342e-02 3.1174438e-03 2.6523566e-03 1.8944517e-02 9.9386901e-01
 2.0004941e-02 6.5824932e-01 9.5561169e-02 1.8721357e-02 5.0089229e-02
 5.3337008e-02 6.2439784e-02 1.0893867e-02 2.6622834e-02 9.9039477e-01
 7.6611084e-01 2.1774950e-02 3.6364086e-02 9.3424714e-01 9.0061837e-01
 8.2953048e-01 2.7015065e-03 9.9950004e-01 4.7871615e-03 4.0837759e-03
 9.9300224e-01 5.2055914e-02 5.9825615e-03 8.5475957e-03 4.6254303e-03
 1.7279021e-04 9.5253736e-01 2.6176305e-02 4.9700332e-04 6.2567309e-02
 1.1370238e-03 9.8027188e-01 3.1965857e-04 1.2615061e-01 2.0643251e-04
 1.5361777e-01 1.6619327e-03 9.8387074e-01 2.2677665e-03 6.7413884e-04
 9.9692458e-01 4.1542253e-03 8.0292094e-01 5.1374614e-01 9.9914527e-01
 9.0262052e-05 9.9168980e-01 2.9613404e-03 9.9951041e-01 9.9455750e-01
 5.6118697e-01 1.7367704e-02 6.2941766e-01 6.6356980e-03 9.9787951e-01
 9.7489899e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-04-01 06:36:06, Dev, Step : 4428, Loss : 0.92453, Acc : 0.757, Auc : 0.797, Sensitive_Loss : 0.18061, Sensitive_Acc : 17.106, Sensitive_Auc : 0.998, Mean auc: 0.797, Run Time : 92.76 sec
INFO:root:2024-04-01 06:36:10, Train, Epoch : 10, Step : 4430, Loss : 0.10418, Acc : 0.156, Sensitive_Loss : 0.02740, Sensitive_Acc : 2.700, Run Time : 2.88 sec
INFO:root:2024-04-01 06:36:17, Train, Epoch : 10, Step : 4440, Loss : 0.69394, Acc : 0.747, Sensitive_Loss : 0.15097, Sensitive_Acc : 16.600, Run Time : 6.56 sec
INFO:root:2024-04-01 06:36:24, Train, Epoch : 10, Step : 4450, Loss : 0.67560, Acc : 0.741, Sensitive_Loss : 0.16272, Sensitive_Acc : 16.800, Run Time : 7.25 sec
INFO:root:2024-04-01 06:36:31, Train, Epoch : 10, Step : 4460, Loss : 0.67187, Acc : 0.794, Sensitive_Loss : 0.16920, Sensitive_Acc : 16.100, Run Time : 7.13 sec
INFO:root:2024-04-01 06:36:38, Train, Epoch : 10, Step : 4470, Loss : 0.57466, Acc : 0.778, Sensitive_Loss : 0.16405, Sensitive_Acc : 17.000, Run Time : 7.08 sec
INFO:root:2024-04-01 06:36:46, Train, Epoch : 10, Step : 4480, Loss : 0.50276, Acc : 0.797, Sensitive_Loss : 0.16951, Sensitive_Acc : 16.300, Run Time : 7.64 sec
INFO:root:2024-04-01 06:36:53, Train, Epoch : 10, Step : 4490, Loss : 0.54660, Acc : 0.816, Sensitive_Loss : 0.18948, Sensitive_Acc : 14.200, Run Time : 6.91 sec
INFO:root:2024-04-01 06:37:00, Train, Epoch : 10, Step : 4500, Loss : 0.62301, Acc : 0.787, Sensitive_Loss : 0.18150, Sensitive_Acc : 14.900, Run Time : 6.94 sec
INFO:root:2024-04-01 06:38:34, Dev, Step : 4500, Loss : 0.92010, Acc : 0.800, Auc : 0.805, Sensitive_Loss : 0.20107, Sensitive_Acc : 17.035, Sensitive_Auc : 0.998, Mean auc: 0.805, Run Time : 94.01 sec
INFO:root:2024-04-01 06:38:39, Train, Epoch : 10, Step : 4510, Loss : 0.63918, Acc : 0.784, Sensitive_Loss : 0.13220, Sensitive_Acc : 17.300, Run Time : 99.58 sec
INFO:root:2024-04-01 06:38:46, Train, Epoch : 10, Step : 4520, Loss : 0.59052, Acc : 0.762, Sensitive_Loss : 0.17037, Sensitive_Acc : 14.700, Run Time : 7.27 sec
INFO:root:2024-04-01 06:38:54, Train, Epoch : 10, Step : 4530, Loss : 0.43388, Acc : 0.800, Sensitive_Loss : 0.15247, Sensitive_Acc : 17.200, Run Time : 7.04 sec
INFO:root:2024-04-01 06:39:01, Train, Epoch : 10, Step : 4540, Loss : 0.57223, Acc : 0.803, Sensitive_Loss : 0.15660, Sensitive_Acc : 15.000, Run Time : 7.14 sec
INFO:root:2024-04-01 06:39:08, Train, Epoch : 10, Step : 4550, Loss : 0.65326, Acc : 0.750, Sensitive_Loss : 0.19330, Sensitive_Acc : 16.600, Run Time : 6.92 sec
INFO:root:2024-04-01 06:39:15, Train, Epoch : 10, Step : 4560, Loss : 0.68237, Acc : 0.784, Sensitive_Loss : 0.15785, Sensitive_Acc : 17.300, Run Time : 7.08 sec
INFO:root:2024-04-01 06:39:22, Train, Epoch : 10, Step : 4570, Loss : 0.66144, Acc : 0.781, Sensitive_Loss : 0.15816, Sensitive_Acc : 17.300, Run Time : 7.14 sec
INFO:root:2024-04-01 06:39:29, Train, Epoch : 10, Step : 4580, Loss : 0.51304, Acc : 0.791, Sensitive_Loss : 0.13139, Sensitive_Acc : 15.400, Run Time : 7.01 sec
INFO:root:2024-04-01 06:39:36, Train, Epoch : 10, Step : 4590, Loss : 0.61393, Acc : 0.794, Sensitive_Loss : 0.15417, Sensitive_Acc : 15.100, Run Time : 7.31 sec
INFO:root:2024-04-01 06:39:43, Train, Epoch : 10, Step : 4600, Loss : 0.62005, Acc : 0.787, Sensitive_Loss : 0.15555, Sensitive_Acc : 16.300, Run Time : 7.31 sec
INFO:root:2024-04-01 06:41:17, Dev, Step : 4600, Loss : 0.94643, Acc : 0.802, Auc : 0.794, Sensitive_Loss : 0.19333, Sensitive_Acc : 17.135, Sensitive_Auc : 0.998, Mean auc: 0.794, Run Time : 93.18 sec
INFO:root:2024-04-01 06:41:22, Train, Epoch : 10, Step : 4610, Loss : 0.44934, Acc : 0.787, Sensitive_Loss : 0.16764, Sensitive_Acc : 17.000, Run Time : 98.74 sec
INFO:root:2024-04-01 06:41:29, Train, Epoch : 10, Step : 4620, Loss : 0.58760, Acc : 0.787, Sensitive_Loss : 0.28093, Sensitive_Acc : 15.800, Run Time : 7.06 sec
INFO:root:2024-04-01 06:41:36, Train, Epoch : 10, Step : 4630, Loss : 0.63040, Acc : 0.772, Sensitive_Loss : 0.12794, Sensitive_Acc : 17.200, Run Time : 7.20 sec
INFO:root:2024-04-01 06:41:43, Train, Epoch : 10, Step : 4640, Loss : 0.48096, Acc : 0.762, Sensitive_Loss : 0.23593, Sensitive_Acc : 17.100, Run Time : 6.93 sec
INFO:root:2024-04-01 06:41:50, Train, Epoch : 10, Step : 4650, Loss : 0.58057, Acc : 0.822, Sensitive_Loss : 0.16527, Sensitive_Acc : 17.900, Run Time : 7.10 sec
INFO:root:2024-04-01 06:41:58, Train, Epoch : 10, Step : 4660, Loss : 0.56349, Acc : 0.803, Sensitive_Loss : 0.15461, Sensitive_Acc : 17.400, Run Time : 7.17 sec
INFO:root:2024-04-01 06:42:05, Train, Epoch : 10, Step : 4670, Loss : 0.68770, Acc : 0.791, Sensitive_Loss : 0.13233, Sensitive_Acc : 17.100, Run Time : 7.39 sec
INFO:root:2024-04-01 06:42:12, Train, Epoch : 10, Step : 4680, Loss : 0.61853, Acc : 0.831, Sensitive_Loss : 0.20761, Sensitive_Acc : 16.400, Run Time : 7.10 sec
INFO:root:2024-04-01 06:42:20, Train, Epoch : 10, Step : 4690, Loss : 0.70475, Acc : 0.797, Sensitive_Loss : 0.13414, Sensitive_Acc : 15.100, Run Time : 7.88 sec
INFO:root:2024-04-01 06:42:27, Train, Epoch : 10, Step : 4700, Loss : 0.61742, Acc : 0.809, Sensitive_Loss : 0.19439, Sensitive_Acc : 16.700, Run Time : 6.54 sec
INFO:root:2024-04-01 06:44:00, Dev, Step : 4700, Loss : 0.91590, Acc : 0.773, Auc : 0.802, Sensitive_Loss : 0.21402, Sensitive_Acc : 17.064, Sensitive_Auc : 0.999, Mean auc: 0.802, Run Time : 93.85 sec
INFO:root:2024-04-01 06:44:06, Train, Epoch : 10, Step : 4710, Loss : 0.60847, Acc : 0.794, Sensitive_Loss : 0.12545, Sensitive_Acc : 14.900, Run Time : 99.19 sec
INFO:root:2024-04-01 06:44:13, Train, Epoch : 10, Step : 4720, Loss : 0.57476, Acc : 0.797, Sensitive_Loss : 0.14462, Sensitive_Acc : 17.200, Run Time : 7.34 sec
INFO:root:2024-04-01 06:44:20, Train, Epoch : 10, Step : 4730, Loss : 0.71455, Acc : 0.803, Sensitive_Loss : 0.13801, Sensitive_Acc : 14.700, Run Time : 7.20 sec
INFO:root:2024-04-01 06:44:27, Train, Epoch : 10, Step : 4740, Loss : 0.47535, Acc : 0.769, Sensitive_Loss : 0.16809, Sensitive_Acc : 17.200, Run Time : 6.90 sec
INFO:root:2024-04-01 06:44:35, Train, Epoch : 10, Step : 4750, Loss : 0.66143, Acc : 0.806, Sensitive_Loss : 0.18578, Sensitive_Acc : 17.000, Run Time : 7.52 sec
INFO:root:2024-04-01 06:44:42, Train, Epoch : 10, Step : 4760, Loss : 0.54263, Acc : 0.794, Sensitive_Loss : 0.16621, Sensitive_Acc : 17.500, Run Time : 7.06 sec
INFO:root:2024-04-01 06:44:49, Train, Epoch : 10, Step : 4770, Loss : 0.61498, Acc : 0.819, Sensitive_Loss : 0.15199, Sensitive_Acc : 14.800, Run Time : 7.30 sec
INFO:root:2024-04-01 06:44:56, Train, Epoch : 10, Step : 4780, Loss : 0.54377, Acc : 0.781, Sensitive_Loss : 0.12273, Sensitive_Acc : 14.900, Run Time : 6.87 sec
INFO:root:2024-04-01 06:45:03, Train, Epoch : 10, Step : 4790, Loss : 0.58816, Acc : 0.812, Sensitive_Loss : 0.16340, Sensitive_Acc : 15.300, Run Time : 7.04 sec
INFO:root:2024-04-01 06:45:10, Train, Epoch : 10, Step : 4800, Loss : 0.64272, Acc : 0.772, Sensitive_Loss : 0.19912, Sensitive_Acc : 19.800, Run Time : 7.24 sec
INFO:root:2024-04-01 06:46:44, Dev, Step : 4800, Loss : 0.97642, Acc : 0.825, Auc : 0.790, Sensitive_Loss : 0.19210, Sensitive_Acc : 17.092, Sensitive_Auc : 0.999, Mean auc: 0.790, Run Time : 93.58 sec
INFO:root:2024-04-01 06:46:50, Train, Epoch : 10, Step : 4810, Loss : 0.54156, Acc : 0.800, Sensitive_Loss : 0.19754, Sensitive_Acc : 17.000, Run Time : 99.50 sec
INFO:root:2024-04-01 06:46:56, Train, Epoch : 10, Step : 4820, Loss : 0.71012, Acc : 0.791, Sensitive_Loss : 0.18141, Sensitive_Acc : 17.600, Run Time : 6.70 sec
INFO:root:2024-04-01 06:47:04, Train, Epoch : 10, Step : 4830, Loss : 0.58590, Acc : 0.812, Sensitive_Loss : 0.12810, Sensitive_Acc : 15.500, Run Time : 7.25 sec
INFO:root:2024-04-01 06:47:11, Train, Epoch : 10, Step : 4840, Loss : 0.70537, Acc : 0.812, Sensitive_Loss : 0.25958, Sensitive_Acc : 16.300, Run Time : 7.44 sec
INFO:root:2024-04-01 06:47:18, Train, Epoch : 10, Step : 4850, Loss : 0.63577, Acc : 0.769, Sensitive_Loss : 0.15682, Sensitive_Acc : 15.000, Run Time : 6.97 sec
INFO:root:2024-04-01 06:47:25, Train, Epoch : 10, Step : 4860, Loss : 0.55062, Acc : 0.847, Sensitive_Loss : 0.18822, Sensitive_Acc : 15.400, Run Time : 7.30 sec
INFO:root:2024-04-01 06:47:33, Train, Epoch : 10, Step : 4870, Loss : 0.68292, Acc : 0.819, Sensitive_Loss : 0.15509, Sensitive_Acc : 16.800, Run Time : 7.30 sec
INFO:root:2024-04-01 06:47:40, Train, Epoch : 10, Step : 4880, Loss : 0.45578, Acc : 0.778, Sensitive_Loss : 0.19742, Sensitive_Acc : 16.200, Run Time : 7.05 sec
INFO:root:2024-04-01 06:47:47, Train, Epoch : 10, Step : 4890, Loss : 0.60484, Acc : 0.787, Sensitive_Loss : 0.14268, Sensitive_Acc : 17.500, Run Time : 6.96 sec
INFO:root:2024-04-01 06:47:54, Train, Epoch : 10, Step : 4900, Loss : 0.70902, Acc : 0.759, Sensitive_Loss : 0.16708, Sensitive_Acc : 15.900, Run Time : 7.38 sec
INFO:root:2024-04-01 06:49:28, Dev, Step : 4900, Loss : 0.91586, Acc : 0.742, Auc : 0.802, Sensitive_Loss : 0.21072, Sensitive_Acc : 17.021, Sensitive_Auc : 0.999, Mean auc: 0.802, Run Time : 93.86 sec
INFO:root:2024-04-01 06:49:33, Train, Epoch : 10, Step : 4910, Loss : 0.51956, Acc : 0.803, Sensitive_Loss : 0.14481, Sensitive_Acc : 17.900, Run Time : 99.41 sec
INFO:root:2024-04-01 06:49:40, Train, Epoch : 10, Step : 4920, Loss : 0.60339, Acc : 0.800, Sensitive_Loss : 0.11708, Sensitive_Acc : 17.000, Run Time : 6.57 sec
INFO:root:2024-04-01 06:51:13
INFO:root:y_pred: [0.1520682  0.06421    0.19717945 ... 0.17612483 0.34679914 0.19641307]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [6.54484495e-04 3.07656586e-01 9.61152911e-01 9.99616757e-02
 9.99340117e-01 4.27229912e-04 6.24387115e-02 2.00347938e-02
 3.54669280e-02 1.04477212e-01 9.27052274e-02 3.50214452e-01
 9.82797742e-01 1.09678123e-03 9.98217881e-01 7.78115630e-01
 8.40417109e-03 2.35074013e-02 9.61604953e-01 5.18199623e-01
 9.21060815e-02 5.12848329e-03 9.99895692e-01 9.79106188e-01
 6.11028401e-04 5.73264778e-01 3.23594012e-03 9.96104360e-01
 3.36200162e-03 9.96890247e-01 9.99907136e-01 9.95124757e-01
 5.46992011e-03 2.94883153e-04 4.26029503e-01 1.71701761e-03
 5.74395526e-03 8.54980107e-03 9.88722801e-01 9.94538724e-01
 1.26960629e-03 4.47238348e-02 9.14031267e-01 9.99086976e-01
 3.78674688e-03 6.41063973e-02 2.30675517e-03 2.76826888e-01
 9.99944568e-01 6.51548862e-01 9.99415040e-01 4.73892927e-01
 9.98810768e-01 9.76617932e-01 9.64109659e-01 9.79077876e-01
 9.62254107e-01 1.65241696e-02 9.99731362e-01 2.29726344e-01
 6.34646462e-03 8.14639702e-02 8.48154902e-01 9.87936020e-01
 9.96508777e-01 9.97838318e-01 5.75633287e-01 9.81207311e-01
 9.90567148e-01 6.32680953e-01 9.96294200e-01 2.45931384e-04
 2.96071321e-02 1.05270125e-01 9.93271172e-01 5.31357341e-03
 9.85038459e-01 7.31826527e-04 9.99524236e-01 1.56214297e-01
 1.14588574e-01 7.97294360e-03 4.53562336e-03 6.66829497e-02
 9.97538090e-01 1.75685044e-02 8.50062072e-01 2.12981507e-01
 1.70416255e-02 1.07012935e-01 7.30668977e-02 1.07041918e-01
 1.72916912e-02 3.00645158e-02 9.94863212e-01 9.11158204e-01
 3.10868472e-02 2.96710432e-02 9.81271923e-01 9.22340751e-01
 9.43796098e-01 4.93039051e-03 9.99743760e-01 1.37334829e-02
 1.88021492e-02 9.98210311e-01 6.54719323e-02 1.34438844e-02
 1.16289491e-02 4.51066811e-03 3.48355592e-04 9.80762661e-01
 3.23871188e-02 3.72372946e-04 1.20623030e-01 2.21889326e-03
 9.88452375e-01 1.95133942e-03 2.62274593e-01 1.09934772e-03
 2.36006781e-01 1.26183825e-03 9.93465960e-01 4.55731200e-03
 2.26823054e-03 9.99194443e-01 4.31318441e-03 9.13026810e-01
 5.57340682e-01 9.99843478e-01 2.68555275e-04 9.95597184e-01
 7.46019837e-03 9.99755800e-01 9.98077989e-01 5.18724561e-01
 3.39976624e-02 7.36268044e-01 3.74599136e-02 9.98465061e-01
 9.93998766e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-04-01 06:51:13, Dev, Step : 4920, Loss : 0.92283, Acc : 0.780, Auc : 0.802, Sensitive_Loss : 0.20241, Sensitive_Acc : 16.993, Sensitive_Auc : 0.999, Mean auc: 0.802, Run Time : 92.71 sec
