Running on desktop22:
stdin: is not a tty
/home/pmen/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
0
Using the specified args:
Namespace(cfg_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_pmen.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_train.csv",
    "dev_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_val.csv",
    "pred_csv": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Biased_Sex_1_pos01.csv",
    "pred_model": "/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Biased_Sex_1_pos011.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-26 15:04:32, Train, Epoch : 1, Step : 10, Loss : 0.64138, Acc : 0.606, Sensitive_Loss : 0.71996, Sensitive_Acc : 16.400, Run Time : 9.44 sec
INFO:root:2024-04-26 15:04:40, Train, Epoch : 1, Step : 20, Loss : 0.62898, Acc : 0.675, Sensitive_Loss : 0.67909, Sensitive_Acc : 15.200, Run Time : 7.99 sec
INFO:root:2024-04-26 15:04:48, Train, Epoch : 1, Step : 30, Loss : 0.58423, Acc : 0.697, Sensitive_Loss : 0.74823, Sensitive_Acc : 15.500, Run Time : 8.17 sec
INFO:root:2024-04-26 15:04:56, Train, Epoch : 1, Step : 40, Loss : 0.65817, Acc : 0.703, Sensitive_Loss : 0.60582, Sensitive_Acc : 18.000, Run Time : 7.91 sec
INFO:root:2024-04-26 15:05:04, Train, Epoch : 1, Step : 50, Loss : 0.54229, Acc : 0.741, Sensitive_Loss : 0.58479, Sensitive_Acc : 15.100, Run Time : 8.07 sec
INFO:root:2024-04-26 15:05:12, Train, Epoch : 1, Step : 60, Loss : 0.55632, Acc : 0.734, Sensitive_Loss : 0.54936, Sensitive_Acc : 17.300, Run Time : 8.17 sec
INFO:root:2024-04-26 15:05:20, Train, Epoch : 1, Step : 70, Loss : 0.62921, Acc : 0.703, Sensitive_Loss : 0.59580, Sensitive_Acc : 15.700, Run Time : 8.10 sec
INFO:root:2024-04-26 15:05:29, Train, Epoch : 1, Step : 80, Loss : 0.46067, Acc : 0.809, Sensitive_Loss : 0.53032, Sensitive_Acc : 15.300, Run Time : 8.30 sec
INFO:root:2024-04-26 15:05:37, Train, Epoch : 1, Step : 90, Loss : 0.43779, Acc : 0.791, Sensitive_Loss : 0.56377, Sensitive_Acc : 16.500, Run Time : 8.05 sec
INFO:root:2024-04-26 15:05:44, Train, Epoch : 1, Step : 100, Loss : 0.59145, Acc : 0.719, Sensitive_Loss : 0.51910, Sensitive_Acc : 14.700, Run Time : 7.75 sec
INFO:root:2024-04-26 15:07:19, Dev, Step : 100, Loss : 0.84584, Acc : 0.596, Auc : 0.843, Sensitive_Loss : 0.61550, Sensitive_Acc : 15.814, Sensitive_Auc : 0.802, Mean auc: 0.843, Run Time : 94.77 sec
INFO:root:2024-04-26 15:07:20, Best, Step : 100, Loss : 0.84584, Acc : 0.596, Auc : 0.843, Sensitive_Loss : 0.61550, Sensitive_Acc : 15.814, Sensitive_Auc : 0.802, Best Auc : 0.843
INFO:root:2024-04-26 15:07:26, Train, Epoch : 1, Step : 110, Loss : 0.50196, Acc : 0.787, Sensitive_Loss : 0.50873, Sensitive_Acc : 15.400, Run Time : 101.34 sec
INFO:root:2024-04-26 15:07:34, Train, Epoch : 1, Step : 120, Loss : 0.52001, Acc : 0.738, Sensitive_Loss : 0.49231, Sensitive_Acc : 15.900, Run Time : 8.34 sec
INFO:root:2024-04-26 15:07:42, Train, Epoch : 1, Step : 130, Loss : 0.48744, Acc : 0.778, Sensitive_Loss : 0.47732, Sensitive_Acc : 17.200, Run Time : 7.78 sec
INFO:root:2024-04-26 15:07:51, Train, Epoch : 1, Step : 140, Loss : 0.45809, Acc : 0.769, Sensitive_Loss : 0.45341, Sensitive_Acc : 15.300, Run Time : 8.77 sec
INFO:root:2024-04-26 15:07:59, Train, Epoch : 1, Step : 150, Loss : 0.58886, Acc : 0.734, Sensitive_Loss : 0.41390, Sensitive_Acc : 15.900, Run Time : 8.48 sec
INFO:root:2024-04-26 15:08:07, Train, Epoch : 1, Step : 160, Loss : 0.55194, Acc : 0.719, Sensitive_Loss : 0.41803, Sensitive_Acc : 14.900, Run Time : 7.91 sec
INFO:root:2024-04-26 15:08:15, Train, Epoch : 1, Step : 170, Loss : 0.55353, Acc : 0.725, Sensitive_Loss : 0.41075, Sensitive_Acc : 18.100, Run Time : 7.79 sec
INFO:root:2024-04-26 15:08:23, Train, Epoch : 1, Step : 180, Loss : 0.53979, Acc : 0.747, Sensitive_Loss : 0.39965, Sensitive_Acc : 16.000, Run Time : 8.55 sec
INFO:root:2024-04-26 15:08:32, Train, Epoch : 1, Step : 190, Loss : 0.48638, Acc : 0.784, Sensitive_Loss : 0.47004, Sensitive_Acc : 16.500, Run Time : 8.58 sec
INFO:root:2024-04-26 15:08:41, Train, Epoch : 1, Step : 200, Loss : 0.46912, Acc : 0.762, Sensitive_Loss : 0.45244, Sensitive_Acc : 15.800, Run Time : 8.62 sec
INFO:root:2024-04-26 15:11:13, Dev, Step : 200, Loss : 0.51044, Acc : 0.771, Auc : 0.854, Sensitive_Loss : 0.37919, Sensitive_Acc : 16.864, Sensitive_Auc : 0.911, Mean auc: 0.854, Run Time : 152.28 sec
INFO:root:2024-04-26 15:11:14, Best, Step : 200, Loss : 0.51044, Acc : 0.771, Auc : 0.854, Sensitive_Loss : 0.37919, Sensitive_Acc : 16.864, Sensitive_Auc : 0.911, Best Auc : 0.854
INFO:root:2024-04-26 15:11:20, Train, Epoch : 1, Step : 210, Loss : 0.46989, Acc : 0.784, Sensitive_Loss : 0.39194, Sensitive_Acc : 16.600, Run Time : 159.32 sec
INFO:root:2024-04-26 15:11:34, Train, Epoch : 1, Step : 220, Loss : 0.48630, Acc : 0.762, Sensitive_Loss : 0.39852, Sensitive_Acc : 14.800, Run Time : 14.18 sec
INFO:root:2024-04-26 15:11:54, Train, Epoch : 1, Step : 230, Loss : 0.58671, Acc : 0.750, Sensitive_Loss : 0.35167, Sensitive_Acc : 16.800, Run Time : 19.39 sec
INFO:root:2024-04-26 15:12:11, Train, Epoch : 1, Step : 240, Loss : 0.42934, Acc : 0.750, Sensitive_Loss : 0.38985, Sensitive_Acc : 15.600, Run Time : 17.14 sec
INFO:root:2024-04-26 15:12:27, Train, Epoch : 1, Step : 250, Loss : 0.43079, Acc : 0.791, Sensitive_Loss : 0.34345, Sensitive_Acc : 16.100, Run Time : 16.79 sec
INFO:root:2024-04-26 15:12:43, Train, Epoch : 1, Step : 260, Loss : 0.46184, Acc : 0.778, Sensitive_Loss : 0.36397, Sensitive_Acc : 16.000, Run Time : 15.48 sec
INFO:root:2024-04-26 15:12:55, Train, Epoch : 1, Step : 270, Loss : 0.56208, Acc : 0.734, Sensitive_Loss : 0.37162, Sensitive_Acc : 16.300, Run Time : 12.18 sec
INFO:root:2024-04-26 15:13:08, Train, Epoch : 1, Step : 280, Loss : 0.42152, Acc : 0.778, Sensitive_Loss : 0.35950, Sensitive_Acc : 16.700, Run Time : 13.17 sec
INFO:root:2024-04-26 15:13:23, Train, Epoch : 1, Step : 290, Loss : 0.63202, Acc : 0.731, Sensitive_Loss : 0.38265, Sensitive_Acc : 17.600, Run Time : 14.57 sec
INFO:root:2024-04-26 15:13:39, Train, Epoch : 1, Step : 300, Loss : 0.50402, Acc : 0.747, Sensitive_Loss : 0.42206, Sensitive_Acc : 15.000, Run Time : 16.61 sec
INFO:root:2024-04-26 15:15:19, Dev, Step : 300, Loss : 0.48818, Acc : 0.776, Auc : 0.859, Sensitive_Loss : 0.37408, Sensitive_Acc : 16.393, Sensitive_Auc : 0.931, Mean auc: 0.859, Run Time : 99.11 sec
INFO:root:2024-04-26 15:15:19, Best, Step : 300, Loss : 0.48818, Acc : 0.776, Auc : 0.859, Sensitive_Loss : 0.37408, Sensitive_Acc : 16.393, Sensitive_Auc : 0.931, Best Auc : 0.859
INFO:root:2024-04-26 15:15:25, Train, Epoch : 1, Step : 310, Loss : 0.51850, Acc : 0.753, Sensitive_Loss : 0.36315, Sensitive_Acc : 17.000, Run Time : 105.84 sec
INFO:root:2024-04-26 15:15:34, Train, Epoch : 1, Step : 320, Loss : 0.52929, Acc : 0.756, Sensitive_Loss : 0.31664, Sensitive_Acc : 15.300, Run Time : 8.51 sec
INFO:root:2024-04-26 15:15:43, Train, Epoch : 1, Step : 330, Loss : 0.42188, Acc : 0.828, Sensitive_Loss : 0.34744, Sensitive_Acc : 16.600, Run Time : 9.18 sec
INFO:root:2024-04-26 15:15:51, Train, Epoch : 1, Step : 340, Loss : 0.48451, Acc : 0.784, Sensitive_Loss : 0.29595, Sensitive_Acc : 16.800, Run Time : 8.49 sec
INFO:root:2024-04-26 15:16:00, Train, Epoch : 1, Step : 350, Loss : 0.45917, Acc : 0.787, Sensitive_Loss : 0.34481, Sensitive_Acc : 15.700, Run Time : 8.58 sec
INFO:root:2024-04-26 15:16:08, Train, Epoch : 1, Step : 360, Loss : 0.45135, Acc : 0.791, Sensitive_Loss : 0.40124, Sensitive_Acc : 18.300, Run Time : 8.27 sec
INFO:root:2024-04-26 15:16:17, Train, Epoch : 1, Step : 370, Loss : 0.43571, Acc : 0.819, Sensitive_Loss : 0.24725, Sensitive_Acc : 16.600, Run Time : 8.88 sec
INFO:root:2024-04-26 15:16:26, Train, Epoch : 1, Step : 380, Loss : 0.51481, Acc : 0.766, Sensitive_Loss : 0.29347, Sensitive_Acc : 16.300, Run Time : 8.80 sec
INFO:root:2024-04-26 15:16:35, Train, Epoch : 1, Step : 390, Loss : 0.45963, Acc : 0.769, Sensitive_Loss : 0.32049, Sensitive_Acc : 14.600, Run Time : 8.64 sec
INFO:root:2024-04-26 15:16:43, Train, Epoch : 1, Step : 400, Loss : 0.50990, Acc : 0.753, Sensitive_Loss : 0.32673, Sensitive_Acc : 16.500, Run Time : 8.46 sec
INFO:root:2024-04-26 15:18:18, Dev, Step : 400, Loss : 0.64324, Acc : 0.709, Auc : 0.861, Sensitive_Loss : 0.36573, Sensitive_Acc : 16.693, Sensitive_Auc : 0.961, Mean auc: 0.861, Run Time : 94.78 sec
INFO:root:2024-04-26 15:18:19, Best, Step : 400, Loss : 0.64324, Acc : 0.709, Auc : 0.861, Sensitive_Loss : 0.36573, Sensitive_Acc : 16.693, Sensitive_Auc : 0.961, Best Auc : 0.861
INFO:root:2024-04-26 15:18:25, Train, Epoch : 1, Step : 410, Loss : 0.51082, Acc : 0.772, Sensitive_Loss : 0.28860, Sensitive_Acc : 16.800, Run Time : 101.43 sec
INFO:root:2024-04-26 15:18:32, Train, Epoch : 1, Step : 420, Loss : 0.44679, Acc : 0.816, Sensitive_Loss : 0.23500, Sensitive_Acc : 14.800, Run Time : 7.94 sec
INFO:root:2024-04-26 15:18:41, Train, Epoch : 1, Step : 430, Loss : 0.38767, Acc : 0.834, Sensitive_Loss : 0.29462, Sensitive_Acc : 17.100, Run Time : 8.60 sec
INFO:root:2024-04-26 15:18:49, Train, Epoch : 1, Step : 440, Loss : 0.42998, Acc : 0.806, Sensitive_Loss : 0.25570, Sensitive_Acc : 16.600, Run Time : 8.03 sec
INFO:root:2024-04-26 15:18:57, Train, Epoch : 1, Step : 450, Loss : 0.55716, Acc : 0.750, Sensitive_Loss : 0.34571, Sensitive_Acc : 15.500, Run Time : 7.78 sec
INFO:root:2024-04-26 15:19:05, Train, Epoch : 1, Step : 460, Loss : 0.47845, Acc : 0.781, Sensitive_Loss : 0.33672, Sensitive_Acc : 14.100, Run Time : 8.14 sec
INFO:root:2024-04-26 15:19:14, Train, Epoch : 1, Step : 470, Loss : 0.52837, Acc : 0.781, Sensitive_Loss : 0.27041, Sensitive_Acc : 16.300, Run Time : 8.72 sec
INFO:root:2024-04-26 15:19:23, Train, Epoch : 1, Step : 480, Loss : 0.55950, Acc : 0.738, Sensitive_Loss : 0.29951, Sensitive_Acc : 16.400, Run Time : 9.10 sec
INFO:root:2024-04-26 15:19:31, Train, Epoch : 1, Step : 490, Loss : 0.52373, Acc : 0.725, Sensitive_Loss : 0.29838, Sensitive_Acc : 17.400, Run Time : 7.89 sec
INFO:root:2024-04-26 15:19:38, Train, Epoch : 1, Step : 500, Loss : 0.40151, Acc : 0.819, Sensitive_Loss : 0.24319, Sensitive_Acc : 17.100, Run Time : 7.71 sec
INFO:root:2024-04-26 15:21:13, Dev, Step : 500, Loss : 0.44784, Acc : 0.802, Auc : 0.882, Sensitive_Loss : 0.26742, Sensitive_Acc : 16.807, Sensitive_Auc : 0.963, Mean auc: 0.882, Run Time : 94.88 sec
INFO:root:2024-04-26 15:21:14, Best, Step : 500, Loss : 0.44784, Acc : 0.802, Auc : 0.882, Sensitive_Loss : 0.26742, Sensitive_Acc : 16.807, Sensitive_Auc : 0.963, Best Auc : 0.882
INFO:root:2024-04-26 15:21:20, Train, Epoch : 1, Step : 510, Loss : 0.46221, Acc : 0.781, Sensitive_Loss : 0.29323, Sensitive_Acc : 17.400, Run Time : 101.65 sec
INFO:root:2024-04-26 15:21:28, Train, Epoch : 1, Step : 520, Loss : 0.49136, Acc : 0.794, Sensitive_Loss : 0.30808, Sensitive_Acc : 15.800, Run Time : 7.53 sec
INFO:root:2024-04-26 15:21:35, Train, Epoch : 1, Step : 530, Loss : 0.46634, Acc : 0.781, Sensitive_Loss : 0.23629, Sensitive_Acc : 16.000, Run Time : 7.72 sec
INFO:root:2024-04-26 15:21:43, Train, Epoch : 1, Step : 540, Loss : 0.52877, Acc : 0.759, Sensitive_Loss : 0.27482, Sensitive_Acc : 15.400, Run Time : 7.65 sec
INFO:root:2024-04-26 15:21:50, Train, Epoch : 1, Step : 550, Loss : 0.46260, Acc : 0.794, Sensitive_Loss : 0.29346, Sensitive_Acc : 15.100, Run Time : 7.35 sec
INFO:root:2024-04-26 15:21:58, Train, Epoch : 1, Step : 560, Loss : 0.44950, Acc : 0.772, Sensitive_Loss : 0.27151, Sensitive_Acc : 15.300, Run Time : 7.97 sec
INFO:root:2024-04-26 15:22:06, Train, Epoch : 1, Step : 570, Loss : 0.42593, Acc : 0.819, Sensitive_Loss : 0.23501, Sensitive_Acc : 17.400, Run Time : 7.70 sec
INFO:root:2024-04-26 15:22:14, Train, Epoch : 1, Step : 580, Loss : 0.41889, Acc : 0.803, Sensitive_Loss : 0.27861, Sensitive_Acc : 17.000, Run Time : 8.32 sec
INFO:root:2024-04-26 15:22:22, Train, Epoch : 1, Step : 590, Loss : 0.54512, Acc : 0.753, Sensitive_Loss : 0.20560, Sensitive_Acc : 14.300, Run Time : 7.20 sec
INFO:root:2024-04-26 15:22:29, Train, Epoch : 1, Step : 600, Loss : 0.49626, Acc : 0.775, Sensitive_Loss : 0.27826, Sensitive_Acc : 15.600, Run Time : 7.50 sec
INFO:root:2024-04-26 15:24:03, Dev, Step : 600, Loss : 0.51422, Acc : 0.761, Auc : 0.870, Sensitive_Loss : 0.26498, Sensitive_Acc : 16.707, Sensitive_Auc : 0.978, Mean auc: 0.870, Run Time : 94.33 sec
INFO:root:2024-04-26 15:24:09, Train, Epoch : 1, Step : 610, Loss : 0.46466, Acc : 0.800, Sensitive_Loss : 0.25398, Sensitive_Acc : 16.700, Run Time : 100.08 sec
INFO:root:2024-04-26 15:24:17, Train, Epoch : 1, Step : 620, Loss : 0.40632, Acc : 0.834, Sensitive_Loss : 0.22444, Sensitive_Acc : 14.600, Run Time : 7.88 sec
INFO:root:2024-04-26 15:25:54
INFO:root:y_pred: [0.15835285 0.9799731  0.005134   ... 0.85975164 0.00288781 0.7415443 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [8.85740280e-01 1.07115554e-02 1.20387092e-01 7.17761926e-03
 9.99342263e-01 6.83438852e-02 9.99079466e-01 8.90308022e-01
 2.31348462e-02 6.64950490e-01 9.92430866e-01 9.93455350e-01
 9.00078475e-01 4.90189105e-01 3.54947478e-01 6.33393705e-01
 9.90538478e-01 2.48693563e-02 4.71726209e-01 9.78102744e-01
 8.85348380e-01 9.05595183e-01 9.96679425e-01 8.57587934e-01
 9.09581900e-01 8.40163291e-01 3.61999460e-02 9.84662533e-01
 9.09152150e-01 8.46177042e-01 6.36208579e-02 7.02583730e-01
 5.52022278e-01 7.47334585e-02 2.96529979e-01 1.31134633e-02
 4.61943895e-02 5.54450974e-02 9.88484979e-01 9.83409941e-01
 6.04123436e-03 2.69972626e-03 8.73400629e-01 9.70454421e-03
 9.99396443e-01 9.86715853e-01 9.72199082e-01 9.69775438e-01
 3.70308757e-02 3.30417573e-01 9.84481812e-01 2.14905273e-02
 7.17590928e-01 9.34167951e-02 1.16017263e-03 4.18962330e-01
 1.18769147e-01 2.76382267e-02 5.44554256e-02 6.18785203e-01
 7.92578049e-03 4.51707721e-01 4.92421910e-02 7.72343338e-01
 2.28352204e-01 9.96387243e-01 1.97990745e-01 9.54231143e-01
 7.95652568e-01 7.48111844e-01 5.41868687e-01 3.55597734e-01
 1.02685757e-01 3.65103036e-02 1.89602554e-01 9.89648979e-04
 7.99605027e-02 2.80340314e-01 1.44114299e-02 9.38643992e-01
 9.92673576e-01 2.32146885e-02 3.13372552e-01 2.03295797e-02
 9.16642785e-01 9.58906412e-01 7.28196977e-03 5.36431633e-02
 8.54093015e-01 9.75242198e-01 9.98961210e-01 3.44847143e-01
 1.64300427e-01 8.76075804e-01 7.99990833e-01 1.12659566e-01
 9.80206847e-01 9.77835596e-01 2.25013606e-02 1.03022777e-01
 6.25423610e-01 8.49036872e-01 9.91551280e-01 9.57889795e-01
 5.57381511e-02 5.05609930e-01 6.89553559e-01 9.24085736e-01
 5.31744301e-01 7.34964088e-02 4.10592973e-01 9.96458709e-01
 1.58857442e-02 9.92755353e-01 9.05324101e-01 8.85775924e-01
 8.93933475e-01 9.94426012e-01 4.60839504e-03 3.05019408e-01
 9.94881153e-01 9.72424030e-01 2.33420376e-02 9.49092507e-01
 9.97301519e-01 4.13369328e-01 9.92563426e-01 7.88756683e-02
 8.79893005e-01 9.77359116e-01 9.87896919e-01 2.09382832e-01
 1.56856880e-01 1.53695047e-02 9.94352937e-01 9.94636714e-01
 9.23041224e-01 4.25963342e-04 5.16995676e-02 7.59617865e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 15:25:54, Dev, Step : 626, Loss : 0.46397, Acc : 0.802, Auc : 0.875, Sensitive_Loss : 0.24326, Sensitive_Acc : 16.821, Sensitive_Auc : 0.965, Mean auc: 0.875, Run Time : 92.34 sec
INFO:root:2024-04-26 15:25:59, Train, Epoch : 2, Step : 630, Loss : 0.22845, Acc : 0.312, Sensitive_Loss : 0.11693, Sensitive_Acc : 6.000, Run Time : 3.90 sec
INFO:root:2024-04-26 15:26:06, Train, Epoch : 2, Step : 640, Loss : 0.40182, Acc : 0.819, Sensitive_Loss : 0.26869, Sensitive_Acc : 14.900, Run Time : 7.28 sec
INFO:root:2024-04-26 15:26:14, Train, Epoch : 2, Step : 650, Loss : 0.38454, Acc : 0.787, Sensitive_Loss : 0.24964, Sensitive_Acc : 17.500, Run Time : 7.96 sec
INFO:root:2024-04-26 15:26:21, Train, Epoch : 2, Step : 660, Loss : 0.40016, Acc : 0.809, Sensitive_Loss : 0.30265, Sensitive_Acc : 17.200, Run Time : 7.19 sec
INFO:root:2024-04-26 15:26:28, Train, Epoch : 2, Step : 670, Loss : 0.51022, Acc : 0.756, Sensitive_Loss : 0.23640, Sensitive_Acc : 15.000, Run Time : 7.24 sec
INFO:root:2024-04-26 15:26:36, Train, Epoch : 2, Step : 680, Loss : 0.43072, Acc : 0.819, Sensitive_Loss : 0.22437, Sensitive_Acc : 16.400, Run Time : 7.16 sec
INFO:root:2024-04-26 15:26:44, Train, Epoch : 2, Step : 690, Loss : 0.53797, Acc : 0.759, Sensitive_Loss : 0.26670, Sensitive_Acc : 17.500, Run Time : 8.33 sec
INFO:root:2024-04-26 15:26:51, Train, Epoch : 2, Step : 700, Loss : 0.49180, Acc : 0.762, Sensitive_Loss : 0.21275, Sensitive_Acc : 16.600, Run Time : 7.61 sec
INFO:root:2024-04-26 15:28:26, Dev, Step : 700, Loss : 0.46955, Acc : 0.793, Auc : 0.871, Sensitive_Loss : 0.23338, Sensitive_Acc : 16.779, Sensitive_Auc : 0.967, Mean auc: 0.871, Run Time : 94.45 sec
INFO:root:2024-04-26 15:28:32, Train, Epoch : 2, Step : 710, Loss : 0.44627, Acc : 0.806, Sensitive_Loss : 0.22310, Sensitive_Acc : 16.100, Run Time : 100.15 sec
INFO:root:2024-04-26 15:28:39, Train, Epoch : 2, Step : 720, Loss : 0.39413, Acc : 0.800, Sensitive_Loss : 0.22230, Sensitive_Acc : 16.100, Run Time : 7.47 sec
INFO:root:2024-04-26 15:28:46, Train, Epoch : 2, Step : 730, Loss : 0.51990, Acc : 0.778, Sensitive_Loss : 0.17499, Sensitive_Acc : 15.400, Run Time : 7.40 sec
INFO:root:2024-04-26 15:28:53, Train, Epoch : 2, Step : 740, Loss : 0.48440, Acc : 0.750, Sensitive_Loss : 0.19992, Sensitive_Acc : 14.100, Run Time : 6.94 sec
INFO:root:2024-04-26 15:29:00, Train, Epoch : 2, Step : 750, Loss : 0.44294, Acc : 0.806, Sensitive_Loss : 0.25584, Sensitive_Acc : 15.100, Run Time : 7.00 sec
INFO:root:2024-04-26 15:29:08, Train, Epoch : 2, Step : 760, Loss : 0.41610, Acc : 0.831, Sensitive_Loss : 0.25718, Sensitive_Acc : 14.600, Run Time : 7.38 sec
INFO:root:2024-04-26 15:29:15, Train, Epoch : 2, Step : 770, Loss : 0.46074, Acc : 0.794, Sensitive_Loss : 0.23904, Sensitive_Acc : 16.400, Run Time : 7.44 sec
INFO:root:2024-04-26 15:29:23, Train, Epoch : 2, Step : 780, Loss : 0.44881, Acc : 0.806, Sensitive_Loss : 0.22828, Sensitive_Acc : 16.400, Run Time : 7.31 sec
INFO:root:2024-04-26 15:29:30, Train, Epoch : 2, Step : 790, Loss : 0.45952, Acc : 0.784, Sensitive_Loss : 0.23090, Sensitive_Acc : 17.400, Run Time : 7.21 sec
INFO:root:2024-04-26 15:29:38, Train, Epoch : 2, Step : 800, Loss : 0.44270, Acc : 0.791, Sensitive_Loss : 0.25307, Sensitive_Acc : 15.800, Run Time : 7.83 sec
INFO:root:2024-04-26 15:31:16, Dev, Step : 800, Loss : 0.45979, Acc : 0.791, Auc : 0.883, Sensitive_Loss : 0.19145, Sensitive_Acc : 16.850, Sensitive_Auc : 0.987, Mean auc: 0.883, Run Time : 98.05 sec
INFO:root:2024-04-26 15:31:16, Best, Step : 800, Loss : 0.45979, Acc : 0.791, Auc : 0.883, Sensitive_Loss : 0.19145, Sensitive_Acc : 16.850, Sensitive_Auc : 0.987, Best Auc : 0.883
INFO:root:2024-04-26 15:31:22, Train, Epoch : 2, Step : 810, Loss : 0.45621, Acc : 0.809, Sensitive_Loss : 0.20837, Sensitive_Acc : 17.000, Run Time : 104.24 sec
INFO:root:2024-04-26 15:31:29, Train, Epoch : 2, Step : 820, Loss : 0.44339, Acc : 0.809, Sensitive_Loss : 0.18880, Sensitive_Acc : 15.500, Run Time : 7.54 sec
INFO:root:2024-04-26 15:31:37, Train, Epoch : 2, Step : 830, Loss : 0.37713, Acc : 0.856, Sensitive_Loss : 0.27731, Sensitive_Acc : 16.700, Run Time : 7.16 sec
INFO:root:2024-04-26 15:31:44, Train, Epoch : 2, Step : 840, Loss : 0.48014, Acc : 0.791, Sensitive_Loss : 0.21701, Sensitive_Acc : 14.900, Run Time : 7.49 sec
INFO:root:2024-04-26 15:31:51, Train, Epoch : 2, Step : 850, Loss : 0.42021, Acc : 0.794, Sensitive_Loss : 0.22709, Sensitive_Acc : 15.300, Run Time : 7.02 sec
INFO:root:2024-04-26 15:31:58, Train, Epoch : 2, Step : 860, Loss : 0.46855, Acc : 0.759, Sensitive_Loss : 0.22754, Sensitive_Acc : 16.100, Run Time : 6.90 sec
INFO:root:2024-04-26 15:32:06, Train, Epoch : 2, Step : 870, Loss : 0.40378, Acc : 0.841, Sensitive_Loss : 0.15495, Sensitive_Acc : 16.600, Run Time : 7.68 sec
INFO:root:2024-04-26 15:32:13, Train, Epoch : 2, Step : 880, Loss : 0.43567, Acc : 0.825, Sensitive_Loss : 0.21666, Sensitive_Acc : 16.600, Run Time : 7.61 sec
INFO:root:2024-04-26 15:32:20, Train, Epoch : 2, Step : 890, Loss : 0.43339, Acc : 0.797, Sensitive_Loss : 0.20405, Sensitive_Acc : 17.100, Run Time : 6.87 sec
INFO:root:2024-04-26 15:32:27, Train, Epoch : 2, Step : 900, Loss : 0.51693, Acc : 0.772, Sensitive_Loss : 0.19539, Sensitive_Acc : 14.800, Run Time : 7.07 sec
INFO:root:2024-04-26 15:34:02, Dev, Step : 900, Loss : 0.58153, Acc : 0.735, Auc : 0.874, Sensitive_Loss : 0.36773, Sensitive_Acc : 16.521, Sensitive_Auc : 0.981, Mean auc: 0.874, Run Time : 94.81 sec
INFO:root:2024-04-26 15:34:08, Train, Epoch : 2, Step : 910, Loss : 0.35049, Acc : 0.850, Sensitive_Loss : 0.23842, Sensitive_Acc : 16.700, Run Time : 100.50 sec
INFO:root:2024-04-26 15:34:15, Train, Epoch : 2, Step : 920, Loss : 0.47486, Acc : 0.778, Sensitive_Loss : 0.20517, Sensitive_Acc : 16.500, Run Time : 7.33 sec
INFO:root:2024-04-26 15:34:22, Train, Epoch : 2, Step : 930, Loss : 0.45684, Acc : 0.809, Sensitive_Loss : 0.17579, Sensitive_Acc : 15.900, Run Time : 7.22 sec
INFO:root:2024-04-26 15:34:30, Train, Epoch : 2, Step : 940, Loss : 0.46153, Acc : 0.816, Sensitive_Loss : 0.19507, Sensitive_Acc : 15.500, Run Time : 7.47 sec
INFO:root:2024-04-26 15:34:37, Train, Epoch : 2, Step : 950, Loss : 0.42615, Acc : 0.819, Sensitive_Loss : 0.20772, Sensitive_Acc : 16.100, Run Time : 6.94 sec
INFO:root:2024-04-26 15:34:44, Train, Epoch : 2, Step : 960, Loss : 0.51823, Acc : 0.762, Sensitive_Loss : 0.20796, Sensitive_Acc : 16.700, Run Time : 7.42 sec
INFO:root:2024-04-26 15:34:51, Train, Epoch : 2, Step : 970, Loss : 0.43444, Acc : 0.825, Sensitive_Loss : 0.19561, Sensitive_Acc : 16.200, Run Time : 6.91 sec
INFO:root:2024-04-26 15:34:59, Train, Epoch : 2, Step : 980, Loss : 0.45629, Acc : 0.800, Sensitive_Loss : 0.19206, Sensitive_Acc : 16.400, Run Time : 7.69 sec
INFO:root:2024-04-26 15:35:06, Train, Epoch : 2, Step : 990, Loss : 0.49151, Acc : 0.791, Sensitive_Loss : 0.16157, Sensitive_Acc : 15.600, Run Time : 7.34 sec
INFO:root:2024-04-26 15:35:13, Train, Epoch : 2, Step : 1000, Loss : 0.38641, Acc : 0.822, Sensitive_Loss : 0.23545, Sensitive_Acc : 17.500, Run Time : 7.45 sec
INFO:root:2024-04-26 15:36:48, Dev, Step : 1000, Loss : 0.46820, Acc : 0.788, Auc : 0.878, Sensitive_Loss : 0.19660, Sensitive_Acc : 16.621, Sensitive_Auc : 0.984, Mean auc: 0.878, Run Time : 94.43 sec
INFO:root:2024-04-26 15:36:53, Train, Epoch : 2, Step : 1010, Loss : 0.40407, Acc : 0.819, Sensitive_Loss : 0.19724, Sensitive_Acc : 15.100, Run Time : 99.85 sec
INFO:root:2024-04-26 15:37:01, Train, Epoch : 2, Step : 1020, Loss : 0.42258, Acc : 0.809, Sensitive_Loss : 0.24298, Sensitive_Acc : 16.500, Run Time : 7.54 sec
INFO:root:2024-04-26 15:37:08, Train, Epoch : 2, Step : 1030, Loss : 0.49025, Acc : 0.800, Sensitive_Loss : 0.20884, Sensitive_Acc : 17.500, Run Time : 7.26 sec
INFO:root:2024-04-26 15:37:15, Train, Epoch : 2, Step : 1040, Loss : 0.42948, Acc : 0.791, Sensitive_Loss : 0.22956, Sensitive_Acc : 15.100, Run Time : 7.33 sec
INFO:root:2024-04-26 15:37:23, Train, Epoch : 2, Step : 1050, Loss : 0.44859, Acc : 0.812, Sensitive_Loss : 0.18096, Sensitive_Acc : 15.100, Run Time : 7.35 sec
INFO:root:2024-04-26 15:37:30, Train, Epoch : 2, Step : 1060, Loss : 0.49591, Acc : 0.781, Sensitive_Loss : 0.16793, Sensitive_Acc : 17.000, Run Time : 7.34 sec
INFO:root:2024-04-26 15:37:37, Train, Epoch : 2, Step : 1070, Loss : 0.38454, Acc : 0.828, Sensitive_Loss : 0.20832, Sensitive_Acc : 14.900, Run Time : 7.12 sec
INFO:root:2024-04-26 15:37:45, Train, Epoch : 2, Step : 1080, Loss : 0.35770, Acc : 0.822, Sensitive_Loss : 0.20196, Sensitive_Acc : 15.300, Run Time : 7.44 sec
INFO:root:2024-04-26 15:37:52, Train, Epoch : 2, Step : 1090, Loss : 0.35189, Acc : 0.853, Sensitive_Loss : 0.15958, Sensitive_Acc : 17.000, Run Time : 7.39 sec
INFO:root:2024-04-26 15:37:59, Train, Epoch : 2, Step : 1100, Loss : 0.41506, Acc : 0.809, Sensitive_Loss : 0.17483, Sensitive_Acc : 17.300, Run Time : 6.58 sec
INFO:root:2024-04-26 15:39:34, Dev, Step : 1100, Loss : 0.46962, Acc : 0.795, Auc : 0.892, Sensitive_Loss : 0.17375, Sensitive_Acc : 16.793, Sensitive_Auc : 0.996, Mean auc: 0.892, Run Time : 95.50 sec
INFO:root:2024-04-26 15:39:35, Best, Step : 1100, Loss : 0.46962, Acc : 0.795, Auc : 0.892, Sensitive_Loss : 0.17375, Sensitive_Acc : 16.793, Sensitive_Auc : 0.996, Best Auc : 0.892
INFO:root:2024-04-26 15:39:40, Train, Epoch : 2, Step : 1110, Loss : 0.35320, Acc : 0.828, Sensitive_Loss : 0.19372, Sensitive_Acc : 16.400, Run Time : 101.73 sec
INFO:root:2024-04-26 15:39:48, Train, Epoch : 2, Step : 1120, Loss : 0.37332, Acc : 0.844, Sensitive_Loss : 0.20442, Sensitive_Acc : 16.600, Run Time : 7.25 sec
INFO:root:2024-04-26 15:39:55, Train, Epoch : 2, Step : 1130, Loss : 0.42598, Acc : 0.828, Sensitive_Loss : 0.12814, Sensitive_Acc : 16.100, Run Time : 7.37 sec
INFO:root:2024-04-26 15:40:02, Train, Epoch : 2, Step : 1140, Loss : 0.39269, Acc : 0.816, Sensitive_Loss : 0.20792, Sensitive_Acc : 16.400, Run Time : 7.29 sec
INFO:root:2024-04-26 15:40:09, Train, Epoch : 2, Step : 1150, Loss : 0.43232, Acc : 0.822, Sensitive_Loss : 0.23393, Sensitive_Acc : 14.900, Run Time : 7.02 sec
INFO:root:2024-04-26 15:40:17, Train, Epoch : 2, Step : 1160, Loss : 0.51953, Acc : 0.778, Sensitive_Loss : 0.14042, Sensitive_Acc : 15.700, Run Time : 7.22 sec
INFO:root:2024-04-26 15:40:24, Train, Epoch : 2, Step : 1170, Loss : 0.39477, Acc : 0.822, Sensitive_Loss : 0.16828, Sensitive_Acc : 16.000, Run Time : 7.22 sec
INFO:root:2024-04-26 15:40:31, Train, Epoch : 2, Step : 1180, Loss : 0.35138, Acc : 0.844, Sensitive_Loss : 0.14989, Sensitive_Acc : 16.300, Run Time : 7.47 sec
INFO:root:2024-04-26 15:40:39, Train, Epoch : 2, Step : 1190, Loss : 0.40986, Acc : 0.809, Sensitive_Loss : 0.17414, Sensitive_Acc : 17.000, Run Time : 7.60 sec
INFO:root:2024-04-26 15:40:46, Train, Epoch : 2, Step : 1200, Loss : 0.40464, Acc : 0.809, Sensitive_Loss : 0.18766, Sensitive_Acc : 15.700, Run Time : 7.49 sec
INFO:root:2024-04-26 15:42:21, Dev, Step : 1200, Loss : 0.44770, Acc : 0.803, Auc : 0.898, Sensitive_Loss : 0.23042, Sensitive_Acc : 16.907, Sensitive_Auc : 0.992, Mean auc: 0.898, Run Time : 94.26 sec
INFO:root:2024-04-26 15:42:21, Best, Step : 1200, Loss : 0.44770, Acc : 0.803, Auc : 0.898, Sensitive_Loss : 0.23042, Sensitive_Acc : 16.907, Sensitive_Auc : 0.992, Best Auc : 0.898
INFO:root:2024-04-26 15:42:27, Train, Epoch : 2, Step : 1210, Loss : 0.39440, Acc : 0.847, Sensitive_Loss : 0.17040, Sensitive_Acc : 16.400, Run Time : 100.26 sec
INFO:root:2024-04-26 15:42:34, Train, Epoch : 2, Step : 1220, Loss : 0.39933, Acc : 0.806, Sensitive_Loss : 0.17934, Sensitive_Acc : 14.900, Run Time : 7.68 sec
INFO:root:2024-04-26 15:42:41, Train, Epoch : 2, Step : 1230, Loss : 0.40322, Acc : 0.819, Sensitive_Loss : 0.17686, Sensitive_Acc : 15.500, Run Time : 6.96 sec
INFO:root:2024-04-26 15:42:49, Train, Epoch : 2, Step : 1240, Loss : 0.48247, Acc : 0.803, Sensitive_Loss : 0.18281, Sensitive_Acc : 16.400, Run Time : 7.66 sec
INFO:root:2024-04-26 15:42:56, Train, Epoch : 2, Step : 1250, Loss : 0.41569, Acc : 0.812, Sensitive_Loss : 0.18650, Sensitive_Acc : 15.400, Run Time : 7.20 sec
INFO:root:2024-04-26 15:44:32
INFO:root:y_pred: [0.07023497 0.8565675  0.01450555 ... 0.8409039  0.01050889 0.78875184]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.79105771e-01 4.88604745e-03 3.83684903e-01 8.67508352e-03
 9.99782622e-01 3.40351183e-03 9.99875307e-01 9.94732499e-01
 7.27221444e-02 9.11231637e-01 9.95800793e-01 9.99006093e-01
 9.83498037e-01 9.48776782e-01 2.12608725e-01 8.24318528e-01
 9.99165773e-01 5.61236683e-03 3.64379197e-01 9.85474348e-01
 9.91535425e-01 8.32389854e-03 9.96489942e-01 9.79261875e-01
 9.93886292e-01 8.60208094e-01 4.60550422e-03 9.91935968e-01
 9.95786607e-01 9.15074348e-01 5.16706659e-03 2.21823767e-01
 6.86414540e-03 2.77959183e-02 5.47023058e-01 4.11113203e-02
 3.20825279e-01 3.36954631e-02 9.87624049e-01 9.97566342e-01
 1.20483828e-03 6.48458546e-04 9.81184363e-01 2.15113955e-03
 9.99607027e-01 9.93404686e-01 9.93708253e-01 8.33649576e-01
 6.90072849e-02 9.74871337e-01 9.92672622e-01 3.50565873e-02
 9.59809661e-01 6.94277277e-03 4.53846389e-03 1.92903131e-01
 3.66736293e-01 4.30372655e-01 2.87699793e-02 3.01646352e-01
 7.34960334e-03 5.74184179e-01 4.31087166e-01 9.80063438e-01
 1.60946026e-01 9.98767138e-01 2.65294407e-02 9.95632052e-01
 9.34606493e-01 4.42800909e-01 9.10300076e-01 8.57456386e-01
 5.93349077e-02 3.14619690e-01 2.59140693e-02 6.68338209e-04
 8.43941942e-02 6.98474169e-01 1.20809965e-03 9.91677165e-01
 9.97438073e-01 1.35243796e-02 6.40180945e-01 7.16474466e-03
 9.25500333e-01 9.88550842e-01 1.79636609e-02 4.03454937e-02
 9.18825567e-01 9.92107689e-01 9.99108970e-01 6.81190491e-02
 7.42489938e-03 9.94190335e-01 3.55931595e-02 4.41817520e-03
 9.89726961e-01 9.98062313e-01 9.85677447e-03 7.33528435e-02
 9.73511219e-01 9.80233192e-01 9.97639418e-01 9.91498053e-01
 3.10953129e-02 1.04316086e-01 8.87356341e-01 9.88286555e-01
 9.35964108e-01 3.77920158e-02 9.76371706e-01 9.99673128e-01
 1.79758608e-01 9.98464108e-01 9.93067384e-01 9.93864357e-01
 8.78145039e-01 9.98945415e-01 2.00309873e-01 7.04726756e-01
 9.97973263e-01 9.93425250e-01 2.26884359e-03 9.86497819e-01
 9.99109924e-01 2.23168179e-01 9.92798090e-01 1.67099714e-01
 7.24508703e-01 9.42363918e-01 9.97116685e-01 6.60459651e-03
 7.57210970e-01 4.93125468e-02 9.98518169e-01 9.96293366e-01
 9.63370919e-01 5.61569706e-02 7.58330673e-02 9.88165021e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 15:44:32, Dev, Step : 1252, Loss : 0.49596, Acc : 0.786, Auc : 0.890, Sensitive_Loss : 0.20805, Sensitive_Acc : 16.779, Sensitive_Auc : 0.989, Mean auc: 0.890, Run Time : 94.98 sec
INFO:root:2024-04-26 15:44:40, Train, Epoch : 3, Step : 1260, Loss : 0.34771, Acc : 0.650, Sensitive_Loss : 0.12225, Sensitive_Acc : 13.900, Run Time : 6.98 sec
INFO:root:2024-04-26 15:44:49, Train, Epoch : 3, Step : 1270, Loss : 0.37216, Acc : 0.834, Sensitive_Loss : 0.18016, Sensitive_Acc : 17.400, Run Time : 8.34 sec
INFO:root:2024-04-26 15:44:56, Train, Epoch : 3, Step : 1280, Loss : 0.39859, Acc : 0.822, Sensitive_Loss : 0.11001, Sensitive_Acc : 16.400, Run Time : 7.04 sec
INFO:root:2024-04-26 15:45:03, Train, Epoch : 3, Step : 1290, Loss : 0.36909, Acc : 0.866, Sensitive_Loss : 0.15111, Sensitive_Acc : 17.000, Run Time : 7.23 sec
INFO:root:2024-04-26 15:45:10, Train, Epoch : 3, Step : 1300, Loss : 0.34088, Acc : 0.841, Sensitive_Loss : 0.14721, Sensitive_Acc : 16.300, Run Time : 7.27 sec
INFO:root:2024-04-26 15:46:45, Dev, Step : 1300, Loss : 0.42659, Acc : 0.818, Auc : 0.901, Sensitive_Loss : 0.17140, Sensitive_Acc : 16.950, Sensitive_Auc : 0.992, Mean auc: 0.901, Run Time : 95.13 sec
INFO:root:2024-04-26 15:46:46, Best, Step : 1300, Loss : 0.42659, Acc : 0.818, Auc : 0.901, Sensitive_Loss : 0.17140, Sensitive_Acc : 16.950, Sensitive_Auc : 0.992, Best Auc : 0.901
INFO:root:2024-04-26 15:46:52, Train, Epoch : 3, Step : 1310, Loss : 0.35980, Acc : 0.841, Sensitive_Loss : 0.16525, Sensitive_Acc : 16.500, Run Time : 101.31 sec
INFO:root:2024-04-26 15:46:59, Train, Epoch : 3, Step : 1320, Loss : 0.40623, Acc : 0.803, Sensitive_Loss : 0.15493, Sensitive_Acc : 14.900, Run Time : 7.78 sec
INFO:root:2024-04-26 15:47:07, Train, Epoch : 3, Step : 1330, Loss : 0.34838, Acc : 0.856, Sensitive_Loss : 0.14047, Sensitive_Acc : 17.300, Run Time : 8.05 sec
INFO:root:2024-04-26 15:47:14, Train, Epoch : 3, Step : 1340, Loss : 0.35684, Acc : 0.828, Sensitive_Loss : 0.16793, Sensitive_Acc : 15.300, Run Time : 6.99 sec
INFO:root:2024-04-26 15:47:22, Train, Epoch : 3, Step : 1350, Loss : 0.30473, Acc : 0.869, Sensitive_Loss : 0.15320, Sensitive_Acc : 17.100, Run Time : 8.06 sec
INFO:root:2024-04-26 15:47:30, Train, Epoch : 3, Step : 1360, Loss : 0.38642, Acc : 0.841, Sensitive_Loss : 0.14439, Sensitive_Acc : 16.500, Run Time : 7.19 sec
INFO:root:2024-04-26 15:47:37, Train, Epoch : 3, Step : 1370, Loss : 0.38140, Acc : 0.828, Sensitive_Loss : 0.14187, Sensitive_Acc : 16.700, Run Time : 7.45 sec
INFO:root:2024-04-26 15:47:44, Train, Epoch : 3, Step : 1380, Loss : 0.35652, Acc : 0.850, Sensitive_Loss : 0.14153, Sensitive_Acc : 16.300, Run Time : 6.72 sec
INFO:root:2024-04-26 15:47:52, Train, Epoch : 3, Step : 1390, Loss : 0.36525, Acc : 0.828, Sensitive_Loss : 0.18479, Sensitive_Acc : 15.400, Run Time : 7.99 sec
INFO:root:2024-04-26 15:47:59, Train, Epoch : 3, Step : 1400, Loss : 0.39630, Acc : 0.828, Sensitive_Loss : 0.11498, Sensitive_Acc : 16.000, Run Time : 7.18 sec
INFO:root:2024-04-26 15:49:34, Dev, Step : 1400, Loss : 0.41751, Acc : 0.820, Auc : 0.904, Sensitive_Loss : 0.15167, Sensitive_Acc : 16.879, Sensitive_Auc : 0.992, Mean auc: 0.904, Run Time : 94.94 sec
INFO:root:2024-04-26 15:49:35, Best, Step : 1400, Loss : 0.41751, Acc : 0.820, Auc : 0.904, Sensitive_Loss : 0.15167, Sensitive_Acc : 16.879, Sensitive_Auc : 0.992, Best Auc : 0.904
INFO:root:2024-04-26 15:49:41, Train, Epoch : 3, Step : 1410, Loss : 0.35845, Acc : 0.847, Sensitive_Loss : 0.15478, Sensitive_Acc : 16.300, Run Time : 101.61 sec
INFO:root:2024-04-26 15:49:49, Train, Epoch : 3, Step : 1420, Loss : 0.33583, Acc : 0.853, Sensitive_Loss : 0.14259, Sensitive_Acc : 15.100, Run Time : 7.89 sec
INFO:root:2024-04-26 15:49:56, Train, Epoch : 3, Step : 1430, Loss : 0.38110, Acc : 0.819, Sensitive_Loss : 0.13800, Sensitive_Acc : 16.200, Run Time : 7.50 sec
INFO:root:2024-04-26 15:50:04, Train, Epoch : 3, Step : 1440, Loss : 0.30529, Acc : 0.875, Sensitive_Loss : 0.20092, Sensitive_Acc : 15.600, Run Time : 8.09 sec
INFO:root:2024-04-26 15:50:12, Train, Epoch : 3, Step : 1450, Loss : 0.34409, Acc : 0.850, Sensitive_Loss : 0.13976, Sensitive_Acc : 16.700, Run Time : 7.62 sec
INFO:root:2024-04-26 15:50:19, Train, Epoch : 3, Step : 1460, Loss : 0.37455, Acc : 0.841, Sensitive_Loss : 0.14795, Sensitive_Acc : 16.900, Run Time : 7.48 sec
INFO:root:2024-04-26 15:50:27, Train, Epoch : 3, Step : 1470, Loss : 0.40226, Acc : 0.850, Sensitive_Loss : 0.13953, Sensitive_Acc : 17.700, Run Time : 7.65 sec
INFO:root:2024-04-26 15:50:34, Train, Epoch : 3, Step : 1480, Loss : 0.33489, Acc : 0.822, Sensitive_Loss : 0.19317, Sensitive_Acc : 16.600, Run Time : 7.20 sec
INFO:root:2024-04-26 15:50:43, Train, Epoch : 3, Step : 1490, Loss : 0.36548, Acc : 0.859, Sensitive_Loss : 0.17066, Sensitive_Acc : 16.500, Run Time : 8.78 sec
INFO:root:2024-04-26 15:50:50, Train, Epoch : 3, Step : 1500, Loss : 0.45458, Acc : 0.822, Sensitive_Loss : 0.12513, Sensitive_Acc : 15.000, Run Time : 7.61 sec
INFO:root:2024-04-26 15:52:25, Dev, Step : 1500, Loss : 0.41251, Acc : 0.824, Auc : 0.905, Sensitive_Loss : 0.14153, Sensitive_Acc : 16.864, Sensitive_Auc : 0.993, Mean auc: 0.905, Run Time : 94.88 sec
INFO:root:2024-04-26 15:52:26, Best, Step : 1500, Loss : 0.41251, Acc : 0.824, Auc : 0.905, Sensitive_Loss : 0.14153, Sensitive_Acc : 16.864, Sensitive_Auc : 0.993, Best Auc : 0.905
INFO:root:2024-04-26 15:52:32, Train, Epoch : 3, Step : 1510, Loss : 0.37740, Acc : 0.825, Sensitive_Loss : 0.13763, Sensitive_Acc : 15.400, Run Time : 101.54 sec
INFO:root:2024-04-26 15:52:41, Train, Epoch : 3, Step : 1520, Loss : 0.39665, Acc : 0.806, Sensitive_Loss : 0.13962, Sensitive_Acc : 15.600, Run Time : 8.68 sec
INFO:root:2024-04-26 15:52:49, Train, Epoch : 3, Step : 1530, Loss : 0.35755, Acc : 0.838, Sensitive_Loss : 0.15357, Sensitive_Acc : 16.900, Run Time : 8.10 sec
INFO:root:2024-04-26 15:52:57, Train, Epoch : 3, Step : 1540, Loss : 0.38388, Acc : 0.838, Sensitive_Loss : 0.12354, Sensitive_Acc : 14.700, Run Time : 8.13 sec
INFO:root:2024-04-26 15:53:05, Train, Epoch : 3, Step : 1550, Loss : 0.46027, Acc : 0.791, Sensitive_Loss : 0.14289, Sensitive_Acc : 16.200, Run Time : 7.93 sec
INFO:root:2024-04-26 15:53:13, Train, Epoch : 3, Step : 1560, Loss : 0.39969, Acc : 0.816, Sensitive_Loss : 0.14229, Sensitive_Acc : 15.900, Run Time : 7.86 sec
INFO:root:2024-04-26 15:53:20, Train, Epoch : 3, Step : 1570, Loss : 0.25987, Acc : 0.875, Sensitive_Loss : 0.15523, Sensitive_Acc : 16.900, Run Time : 7.83 sec
INFO:root:2024-04-26 15:53:28, Train, Epoch : 3, Step : 1580, Loss : 0.39700, Acc : 0.834, Sensitive_Loss : 0.12884, Sensitive_Acc : 15.300, Run Time : 7.47 sec
INFO:root:2024-04-26 15:53:36, Train, Epoch : 3, Step : 1590, Loss : 0.37936, Acc : 0.853, Sensitive_Loss : 0.16374, Sensitive_Acc : 16.300, Run Time : 7.60 sec
INFO:root:2024-04-26 15:53:43, Train, Epoch : 3, Step : 1600, Loss : 0.39287, Acc : 0.822, Sensitive_Loss : 0.16317, Sensitive_Acc : 17.600, Run Time : 7.22 sec
INFO:root:2024-04-26 15:55:15, Dev, Step : 1600, Loss : 0.41542, Acc : 0.819, Auc : 0.905, Sensitive_Loss : 0.14699, Sensitive_Acc : 16.950, Sensitive_Auc : 0.992, Mean auc: 0.905, Run Time : 91.80 sec
INFO:root:2024-04-26 15:55:16, Best, Step : 1600, Loss : 0.41542, Acc : 0.819, Auc : 0.905, Sensitive_Loss : 0.14699, Sensitive_Acc : 16.950, Sensitive_Auc : 0.992, Best Auc : 0.905
INFO:root:2024-04-26 15:55:22, Train, Epoch : 3, Step : 1610, Loss : 0.33811, Acc : 0.863, Sensitive_Loss : 0.12061, Sensitive_Acc : 15.300, Run Time : 99.49 sec
INFO:root:2024-04-26 15:55:30, Train, Epoch : 3, Step : 1620, Loss : 0.30638, Acc : 0.847, Sensitive_Loss : 0.14664, Sensitive_Acc : 16.300, Run Time : 7.80 sec
INFO:root:2024-04-26 15:55:38, Train, Epoch : 3, Step : 1630, Loss : 0.27179, Acc : 0.863, Sensitive_Loss : 0.16576, Sensitive_Acc : 17.400, Run Time : 7.69 sec
INFO:root:2024-04-26 15:55:45, Train, Epoch : 3, Step : 1640, Loss : 0.35539, Acc : 0.847, Sensitive_Loss : 0.15200, Sensitive_Acc : 16.300, Run Time : 7.47 sec
INFO:root:2024-04-26 15:55:53, Train, Epoch : 3, Step : 1650, Loss : 0.33974, Acc : 0.859, Sensitive_Loss : 0.20840, Sensitive_Acc : 16.600, Run Time : 8.01 sec
INFO:root:2024-04-26 15:56:01, Train, Epoch : 3, Step : 1660, Loss : 0.36334, Acc : 0.853, Sensitive_Loss : 0.12631, Sensitive_Acc : 15.900, Run Time : 7.60 sec
INFO:root:2024-04-26 15:56:09, Train, Epoch : 3, Step : 1670, Loss : 0.37300, Acc : 0.859, Sensitive_Loss : 0.19041, Sensitive_Acc : 16.100, Run Time : 8.09 sec
INFO:root:2024-04-26 15:56:17, Train, Epoch : 3, Step : 1680, Loss : 0.45520, Acc : 0.794, Sensitive_Loss : 0.16299, Sensitive_Acc : 15.500, Run Time : 7.83 sec
INFO:root:2024-04-26 15:56:25, Train, Epoch : 3, Step : 1690, Loss : 0.33141, Acc : 0.859, Sensitive_Loss : 0.16088, Sensitive_Acc : 15.800, Run Time : 8.09 sec
INFO:root:2024-04-26 15:56:33, Train, Epoch : 3, Step : 1700, Loss : 0.33643, Acc : 0.850, Sensitive_Loss : 0.10198, Sensitive_Acc : 15.700, Run Time : 7.73 sec
INFO:root:2024-04-26 15:58:05, Dev, Step : 1700, Loss : 0.41176, Acc : 0.827, Auc : 0.908, Sensitive_Loss : 0.15007, Sensitive_Acc : 16.864, Sensitive_Auc : 0.990, Mean auc: 0.908, Run Time : 92.05 sec
INFO:root:2024-04-26 15:58:05, Best, Step : 1700, Loss : 0.41176, Acc : 0.827, Auc : 0.908, Sensitive_Loss : 0.15007, Sensitive_Acc : 16.864, Sensitive_Auc : 0.990, Best Auc : 0.908
INFO:root:2024-04-26 15:58:11, Train, Epoch : 3, Step : 1710, Loss : 0.36498, Acc : 0.825, Sensitive_Loss : 0.12095, Sensitive_Acc : 15.400, Run Time : 98.56 sec
INFO:root:2024-04-26 15:58:19, Train, Epoch : 3, Step : 1720, Loss : 0.31901, Acc : 0.863, Sensitive_Loss : 0.14466, Sensitive_Acc : 15.400, Run Time : 8.10 sec
INFO:root:2024-04-26 15:58:27, Train, Epoch : 3, Step : 1730, Loss : 0.33084, Acc : 0.863, Sensitive_Loss : 0.11922, Sensitive_Acc : 16.400, Run Time : 7.51 sec
INFO:root:2024-04-26 15:58:34, Train, Epoch : 3, Step : 1740, Loss : 0.39745, Acc : 0.806, Sensitive_Loss : 0.14920, Sensitive_Acc : 15.600, Run Time : 7.69 sec
INFO:root:2024-04-26 15:58:43, Train, Epoch : 3, Step : 1750, Loss : 0.37577, Acc : 0.844, Sensitive_Loss : 0.16916, Sensitive_Acc : 16.700, Run Time : 8.82 sec
INFO:root:2024-04-26 15:58:51, Train, Epoch : 3, Step : 1760, Loss : 0.36513, Acc : 0.853, Sensitive_Loss : 0.11377, Sensitive_Acc : 15.700, Run Time : 7.65 sec
INFO:root:2024-04-26 15:58:59, Train, Epoch : 3, Step : 1770, Loss : 0.35512, Acc : 0.841, Sensitive_Loss : 0.12973, Sensitive_Acc : 15.800, Run Time : 7.68 sec
INFO:root:2024-04-26 15:59:07, Train, Epoch : 3, Step : 1780, Loss : 0.32893, Acc : 0.853, Sensitive_Loss : 0.15337, Sensitive_Acc : 16.400, Run Time : 8.12 sec
INFO:root:2024-04-26 15:59:14, Train, Epoch : 3, Step : 1790, Loss : 0.35936, Acc : 0.872, Sensitive_Loss : 0.15265, Sensitive_Acc : 16.200, Run Time : 7.82 sec
INFO:root:2024-04-26 15:59:22, Train, Epoch : 3, Step : 1800, Loss : 0.32203, Acc : 0.869, Sensitive_Loss : 0.14745, Sensitive_Acc : 14.000, Run Time : 7.73 sec
INFO:root:2024-04-26 16:00:53, Dev, Step : 1800, Loss : 0.40461, Acc : 0.828, Auc : 0.908, Sensitive_Loss : 0.14391, Sensitive_Acc : 16.879, Sensitive_Auc : 0.991, Mean auc: 0.908, Run Time : 91.14 sec
INFO:root:2024-04-26 16:00:54, Best, Step : 1800, Loss : 0.40461, Acc : 0.828, Auc : 0.908, Sensitive_Loss : 0.14391, Sensitive_Acc : 16.879, Sensitive_Auc : 0.991, Best Auc : 0.908
INFO:root:2024-04-26 16:01:01, Train, Epoch : 3, Step : 1810, Loss : 0.34039, Acc : 0.859, Sensitive_Loss : 0.09909, Sensitive_Acc : 16.200, Run Time : 98.72 sec
INFO:root:2024-04-26 16:01:09, Train, Epoch : 3, Step : 1820, Loss : 0.36891, Acc : 0.853, Sensitive_Loss : 0.14296, Sensitive_Acc : 14.700, Run Time : 8.01 sec
INFO:root:2024-04-26 16:01:17, Train, Epoch : 3, Step : 1830, Loss : 0.34749, Acc : 0.834, Sensitive_Loss : 0.14120, Sensitive_Acc : 17.100, Run Time : 7.89 sec
INFO:root:2024-04-26 16:01:25, Train, Epoch : 3, Step : 1840, Loss : 0.36560, Acc : 0.828, Sensitive_Loss : 0.11465, Sensitive_Acc : 16.900, Run Time : 7.86 sec
INFO:root:2024-04-26 16:01:34, Train, Epoch : 3, Step : 1850, Loss : 0.33040, Acc : 0.859, Sensitive_Loss : 0.09551, Sensitive_Acc : 15.900, Run Time : 8.96 sec
INFO:root:2024-04-26 16:01:42, Train, Epoch : 3, Step : 1860, Loss : 0.33729, Acc : 0.856, Sensitive_Loss : 0.11460, Sensitive_Acc : 17.400, Run Time : 8.65 sec
INFO:root:2024-04-26 16:01:50, Train, Epoch : 3, Step : 1870, Loss : 0.40224, Acc : 0.831, Sensitive_Loss : 0.12468, Sensitive_Acc : 15.300, Run Time : 8.02 sec
INFO:root:2024-04-26 16:03:28
INFO:root:y_pred: [0.16035502 0.90528864 0.03344147 ... 0.7678842  0.0032454  0.8973101 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.2709863e-01 3.2831016e-03 2.4706489e-01 5.0577973e-03 9.9771571e-01
 4.2937533e-03 9.9916077e-01 9.8896849e-01 2.6707014e-02 9.1449952e-01
 9.8258185e-01 9.9891520e-01 9.8984629e-01 9.3761438e-01 1.6360766e-01
 9.4964892e-01 9.9916101e-01 3.0599656e-02 1.6990042e-01 9.6475995e-01
 9.5495594e-01 2.8381557e-03 9.9436581e-01 9.6843028e-01 9.9076813e-01
 8.5676289e-01 1.4388751e-02 9.7628766e-01 9.8980027e-01 4.4870922e-01
 3.9034698e-03 5.2033413e-02 2.4679089e-03 2.1086277e-02 8.7524801e-02
 1.6721066e-02 2.7610463e-01 1.9828228e-02 9.7891104e-01 9.9402148e-01
 7.6848075e-05 2.9498467e-04 9.7459161e-01 4.6281628e-03 9.9867862e-01
 9.8970544e-01 9.9340594e-01 8.0329716e-01 7.4719585e-02 9.8246378e-01
 9.9283946e-01 2.0333152e-02 4.9337798e-01 8.2935514e-03 4.6394384e-03
 1.1948222e-02 7.5809404e-02 1.0270459e-02 1.5379307e-02 2.4980369e-01
 4.6102898e-03 1.9784589e-01 6.8196140e-02 9.6154618e-01 1.5098612e-01
 9.9499905e-01 7.3088985e-03 9.9070716e-01 8.9359283e-01 2.5562641e-01
 9.2799425e-01 7.6874071e-01 4.2629741e-02 3.8054290e-01 1.5212719e-02
 6.2285323e-04 1.4929866e-02 2.4460097e-01 8.9137384e-04 9.9117762e-01
 9.9667525e-01 1.1645188e-02 4.9047753e-01 4.8601818e-03 9.3490696e-01
 9.4500160e-01 1.9000731e-02 5.9021845e-02 7.6925194e-01 9.8883867e-01
 9.9571878e-01 8.6746244e-03 2.1302963e-02 9.9073738e-01 8.7062083e-02
 1.5213094e-03 9.8838204e-01 9.8098320e-01 9.2118848e-03 8.6457301e-03
 9.3699741e-01 9.5447451e-01 9.9345946e-01 9.8420584e-01 7.2431695e-03
 2.5586285e-02 8.8243139e-01 9.8664165e-01 9.3283516e-01 2.1669671e-03
 9.4197989e-01 9.9649930e-01 9.7247161e-02 9.9589062e-01 9.7421515e-01
 9.9103034e-01 9.2069191e-01 9.9698383e-01 1.0247532e-01 2.6598814e-01
 9.9529415e-01 9.9027294e-01 2.4143988e-03 9.3313640e-01 9.9772185e-01
 3.5083070e-01 9.8535013e-01 5.9931286e-02 2.5144702e-01 9.7987616e-01
 9.9592894e-01 6.6105544e-04 8.6277716e-02 1.5552968e-02 9.9306470e-01
 9.9646193e-01 9.3325323e-01 1.9462040e-03 3.8690288e-02 9.5714128e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 16:03:28, Dev, Step : 1878, Loss : 0.41669, Acc : 0.822, Auc : 0.908, Sensitive_Loss : 0.16006, Sensitive_Acc : 16.864, Sensitive_Auc : 0.990, Mean auc: 0.908, Run Time : 91.67 sec
INFO:root:2024-04-26 16:03:29, Best, Step : 1878, Loss : 0.41669, Acc : 0.822,Auc : 0.908, Best Auc : 0.908, Sensitive_Loss : 0.16006, Sensitive_Acc : 16.864, Sensitive_Auc : 0.990
INFO:root:2024-04-26 16:03:33, Train, Epoch : 4, Step : 1880, Loss : 0.08789, Acc : 0.153, Sensitive_Loss : 0.01876, Sensitive_Acc : 2.800, Run Time : 2.50 sec
INFO:root:2024-04-26 16:03:40, Train, Epoch : 4, Step : 1890, Loss : 0.30224, Acc : 0.878, Sensitive_Loss : 0.14509, Sensitive_Acc : 15.600, Run Time : 7.72 sec
INFO:root:2024-04-26 16:03:48, Train, Epoch : 4, Step : 1900, Loss : 0.30557, Acc : 0.878, Sensitive_Loss : 0.17021, Sensitive_Acc : 18.400, Run Time : 7.82 sec
INFO:root:2024-04-26 16:05:23, Dev, Step : 1900, Loss : 0.41078, Acc : 0.825, Auc : 0.909, Sensitive_Loss : 0.15233, Sensitive_Acc : 16.821, Sensitive_Auc : 0.991, Mean auc: 0.909, Run Time : 94.78 sec
INFO:root:2024-04-26 16:05:24, Best, Step : 1900, Loss : 0.41078, Acc : 0.825, Auc : 0.909, Sensitive_Loss : 0.15233, Sensitive_Acc : 16.821, Sensitive_Auc : 0.991, Best Auc : 0.909
INFO:root:2024-04-26 16:05:29, Train, Epoch : 4, Step : 1910, Loss : 0.35785, Acc : 0.853, Sensitive_Loss : 0.12152, Sensitive_Acc : 16.100, Run Time : 101.00 sec
INFO:root:2024-04-26 16:05:38, Train, Epoch : 4, Step : 1920, Loss : 0.35135, Acc : 0.841, Sensitive_Loss : 0.11032, Sensitive_Acc : 16.800, Run Time : 8.92 sec
INFO:root:2024-04-26 16:05:46, Train, Epoch : 4, Step : 1930, Loss : 0.26823, Acc : 0.875, Sensitive_Loss : 0.14918, Sensitive_Acc : 15.900, Run Time : 7.93 sec
INFO:root:2024-04-26 16:05:55, Train, Epoch : 4, Step : 1940, Loss : 0.25703, Acc : 0.903, Sensitive_Loss : 0.11356, Sensitive_Acc : 16.100, Run Time : 8.71 sec
INFO:root:2024-04-26 16:06:03, Train, Epoch : 4, Step : 1950, Loss : 0.32646, Acc : 0.850, Sensitive_Loss : 0.11601, Sensitive_Acc : 17.200, Run Time : 7.99 sec
INFO:root:2024-04-26 16:06:12, Train, Epoch : 4, Step : 1960, Loss : 0.34394, Acc : 0.850, Sensitive_Loss : 0.14582, Sensitive_Acc : 15.900, Run Time : 8.97 sec
INFO:root:2024-04-26 16:06:20, Train, Epoch : 4, Step : 1970, Loss : 0.29107, Acc : 0.881, Sensitive_Loss : 0.13178, Sensitive_Acc : 16.800, Run Time : 7.91 sec
INFO:root:2024-04-26 16:06:27, Train, Epoch : 4, Step : 1980, Loss : 0.28342, Acc : 0.900, Sensitive_Loss : 0.15075, Sensitive_Acc : 15.600, Run Time : 7.67 sec
INFO:root:2024-04-26 16:06:35, Train, Epoch : 4, Step : 1990, Loss : 0.36270, Acc : 0.831, Sensitive_Loss : 0.15195, Sensitive_Acc : 17.300, Run Time : 7.81 sec
INFO:root:2024-04-26 16:06:44, Train, Epoch : 4, Step : 2000, Loss : 0.30503, Acc : 0.884, Sensitive_Loss : 0.11344, Sensitive_Acc : 17.200, Run Time : 9.30 sec
INFO:root:2024-04-26 16:08:20, Dev, Step : 2000, Loss : 0.40596, Acc : 0.828, Auc : 0.907, Sensitive_Loss : 0.13740, Sensitive_Acc : 16.864, Sensitive_Auc : 0.990, Mean auc: 0.907, Run Time : 95.11 sec
INFO:root:2024-04-26 16:08:26, Train, Epoch : 4, Step : 2010, Loss : 0.29559, Acc : 0.863, Sensitive_Loss : 0.14535, Sensitive_Acc : 17.100, Run Time : 101.72 sec
INFO:root:2024-04-26 16:08:37, Train, Epoch : 4, Step : 2020, Loss : 0.30028, Acc : 0.875, Sensitive_Loss : 0.12430, Sensitive_Acc : 17.300, Run Time : 10.53 sec
INFO:root:2024-04-26 16:08:45, Train, Epoch : 4, Step : 2030, Loss : 0.37705, Acc : 0.825, Sensitive_Loss : 0.13443, Sensitive_Acc : 14.800, Run Time : 8.07 sec
INFO:root:2024-04-26 16:08:53, Train, Epoch : 4, Step : 2040, Loss : 0.39844, Acc : 0.841, Sensitive_Loss : 0.12502, Sensitive_Acc : 17.000, Run Time : 8.57 sec
INFO:root:2024-04-26 16:09:02, Train, Epoch : 4, Step : 2050, Loss : 0.31435, Acc : 0.875, Sensitive_Loss : 0.14401, Sensitive_Acc : 15.900, Run Time : 8.75 sec
INFO:root:2024-04-26 16:09:11, Train, Epoch : 4, Step : 2060, Loss : 0.32963, Acc : 0.847, Sensitive_Loss : 0.09542, Sensitive_Acc : 16.800, Run Time : 8.72 sec
INFO:root:2024-04-26 16:09:19, Train, Epoch : 4, Step : 2070, Loss : 0.36418, Acc : 0.841, Sensitive_Loss : 0.11430, Sensitive_Acc : 17.700, Run Time : 8.30 sec
INFO:root:2024-04-26 16:09:27, Train, Epoch : 4, Step : 2080, Loss : 0.38463, Acc : 0.841, Sensitive_Loss : 0.15042, Sensitive_Acc : 16.000, Run Time : 8.08 sec
INFO:root:2024-04-26 16:09:35, Train, Epoch : 4, Step : 2090, Loss : 0.29483, Acc : 0.894, Sensitive_Loss : 0.12934, Sensitive_Acc : 16.700, Run Time : 8.03 sec
INFO:root:2024-04-26 16:09:44, Train, Epoch : 4, Step : 2100, Loss : 0.33430, Acc : 0.881, Sensitive_Loss : 0.12050, Sensitive_Acc : 16.600, Run Time : 8.83 sec
INFO:root:2024-04-26 16:11:18, Dev, Step : 2100, Loss : 0.40014, Acc : 0.832, Auc : 0.908, Sensitive_Loss : 0.13906, Sensitive_Acc : 16.864, Sensitive_Auc : 0.991, Mean auc: 0.908, Run Time : 94.06 sec
INFO:root:2024-04-26 16:11:24, Train, Epoch : 4, Step : 2110, Loss : 0.37461, Acc : 0.825, Sensitive_Loss : 0.11402, Sensitive_Acc : 14.700, Run Time : 100.09 sec
INFO:root:2024-04-26 16:11:32, Train, Epoch : 4, Step : 2120, Loss : 0.32896, Acc : 0.866, Sensitive_Loss : 0.14390, Sensitive_Acc : 17.200, Run Time : 7.71 sec
INFO:root:2024-04-26 16:11:40, Train, Epoch : 4, Step : 2130, Loss : 0.28963, Acc : 0.834, Sensitive_Loss : 0.18753, Sensitive_Acc : 17.200, Run Time : 7.78 sec
INFO:root:2024-04-26 16:11:47, Train, Epoch : 4, Step : 2140, Loss : 0.34506, Acc : 0.844, Sensitive_Loss : 0.12457, Sensitive_Acc : 16.000, Run Time : 7.56 sec
INFO:root:2024-04-26 16:11:54, Train, Epoch : 4, Step : 2150, Loss : 0.40333, Acc : 0.841, Sensitive_Loss : 0.12525, Sensitive_Acc : 17.200, Run Time : 7.29 sec
INFO:root:2024-04-26 16:12:02, Train, Epoch : 4, Step : 2160, Loss : 0.34938, Acc : 0.866, Sensitive_Loss : 0.10957, Sensitive_Acc : 16.400, Run Time : 7.64 sec
INFO:root:2024-04-26 16:12:10, Train, Epoch : 4, Step : 2170, Loss : 0.42259, Acc : 0.847, Sensitive_Loss : 0.12326, Sensitive_Acc : 16.300, Run Time : 7.45 sec
INFO:root:2024-04-26 16:12:18, Train, Epoch : 4, Step : 2180, Loss : 0.31449, Acc : 0.850, Sensitive_Loss : 0.13365, Sensitive_Acc : 17.700, Run Time : 8.22 sec
INFO:root:2024-04-26 16:12:25, Train, Epoch : 4, Step : 2190, Loss : 0.36984, Acc : 0.816, Sensitive_Loss : 0.16478, Sensitive_Acc : 15.600, Run Time : 7.30 sec
INFO:root:2024-04-26 16:12:33, Train, Epoch : 4, Step : 2200, Loss : 0.31950, Acc : 0.844, Sensitive_Loss : 0.11581, Sensitive_Acc : 17.000, Run Time : 7.53 sec
INFO:root:2024-04-26 16:14:08, Dev, Step : 2200, Loss : 0.40658, Acc : 0.824, Auc : 0.908, Sensitive_Loss : 0.15293, Sensitive_Acc : 16.793, Sensitive_Auc : 0.992, Mean auc: 0.908, Run Time : 94.90 sec
INFO:root:2024-04-26 16:14:13, Train, Epoch : 4, Step : 2210, Loss : 0.34400, Acc : 0.822, Sensitive_Loss : 0.11783, Sensitive_Acc : 16.500, Run Time : 100.60 sec
INFO:root:2024-04-26 16:14:21, Train, Epoch : 4, Step : 2220, Loss : 0.31617, Acc : 0.863, Sensitive_Loss : 0.11492, Sensitive_Acc : 17.100, Run Time : 8.20 sec
INFO:root:2024-04-26 16:14:29, Train, Epoch : 4, Step : 2230, Loss : 0.31714, Acc : 0.863, Sensitive_Loss : 0.13037, Sensitive_Acc : 15.700, Run Time : 7.42 sec
INFO:root:2024-04-26 16:14:36, Train, Epoch : 4, Step : 2240, Loss : 0.35855, Acc : 0.834, Sensitive_Loss : 0.14449, Sensitive_Acc : 15.100, Run Time : 7.23 sec
INFO:root:2024-04-26 16:14:44, Train, Epoch : 4, Step : 2250, Loss : 0.23887, Acc : 0.916, Sensitive_Loss : 0.11356, Sensitive_Acc : 15.900, Run Time : 8.34 sec
INFO:root:2024-04-26 16:14:52, Train, Epoch : 4, Step : 2260, Loss : 0.35620, Acc : 0.803, Sensitive_Loss : 0.11909, Sensitive_Acc : 16.200, Run Time : 7.61 sec
INFO:root:2024-04-26 16:15:00, Train, Epoch : 4, Step : 2270, Loss : 0.39029, Acc : 0.838, Sensitive_Loss : 0.11655, Sensitive_Acc : 16.100, Run Time : 8.02 sec
INFO:root:2024-04-26 16:15:08, Train, Epoch : 4, Step : 2280, Loss : 0.32452, Acc : 0.869, Sensitive_Loss : 0.15120, Sensitive_Acc : 16.100, Run Time : 7.56 sec
INFO:root:2024-04-26 16:15:15, Train, Epoch : 4, Step : 2290, Loss : 0.40617, Acc : 0.834, Sensitive_Loss : 0.12981, Sensitive_Acc : 16.500, Run Time : 7.86 sec
INFO:root:2024-04-26 16:15:23, Train, Epoch : 4, Step : 2300, Loss : 0.34417, Acc : 0.828, Sensitive_Loss : 0.10845, Sensitive_Acc : 16.800, Run Time : 7.65 sec
INFO:root:2024-04-26 16:16:58, Dev, Step : 2300, Loss : 0.41527, Acc : 0.818, Auc : 0.907, Sensitive_Loss : 0.14732, Sensitive_Acc : 16.921, Sensitive_Auc : 0.990, Mean auc: 0.907, Run Time : 94.56 sec
INFO:root:2024-04-26 16:17:03, Train, Epoch : 4, Step : 2310, Loss : 0.29875, Acc : 0.872, Sensitive_Loss : 0.14983, Sensitive_Acc : 17.200, Run Time : 100.09 sec
INFO:root:2024-04-26 16:17:11, Train, Epoch : 4, Step : 2320, Loss : 0.32702, Acc : 0.878, Sensitive_Loss : 0.11843, Sensitive_Acc : 16.000, Run Time : 8.01 sec
INFO:root:2024-04-26 16:17:19, Train, Epoch : 4, Step : 2330, Loss : 0.38848, Acc : 0.834, Sensitive_Loss : 0.12853, Sensitive_Acc : 17.500, Run Time : 8.11 sec
INFO:root:2024-04-26 16:17:27, Train, Epoch : 4, Step : 2340, Loss : 0.34071, Acc : 0.859, Sensitive_Loss : 0.15947, Sensitive_Acc : 16.000, Run Time : 7.41 sec
INFO:root:2024-04-26 16:17:34, Train, Epoch : 4, Step : 2350, Loss : 0.33300, Acc : 0.875, Sensitive_Loss : 0.09764, Sensitive_Acc : 16.500, Run Time : 7.43 sec
INFO:root:2024-04-26 16:17:42, Train, Epoch : 4, Step : 2360, Loss : 0.34143, Acc : 0.841, Sensitive_Loss : 0.11325, Sensitive_Acc : 15.300, Run Time : 7.61 sec
INFO:root:2024-04-26 16:17:50, Train, Epoch : 4, Step : 2370, Loss : 0.29976, Acc : 0.875, Sensitive_Loss : 0.11761, Sensitive_Acc : 15.100, Run Time : 7.78 sec
INFO:root:2024-04-26 16:17:57, Train, Epoch : 4, Step : 2380, Loss : 0.36256, Acc : 0.853, Sensitive_Loss : 0.13685, Sensitive_Acc : 17.200, Run Time : 7.80 sec
INFO:root:2024-04-26 16:18:05, Train, Epoch : 4, Step : 2390, Loss : 0.36028, Acc : 0.850, Sensitive_Loss : 0.15964, Sensitive_Acc : 16.600, Run Time : 7.39 sec
INFO:root:2024-04-26 16:18:12, Train, Epoch : 4, Step : 2400, Loss : 0.34236, Acc : 0.863, Sensitive_Loss : 0.09762, Sensitive_Acc : 16.900, Run Time : 7.61 sec
INFO:root:2024-04-26 16:19:47, Dev, Step : 2400, Loss : 0.40704, Acc : 0.825, Auc : 0.909, Sensitive_Loss : 0.13168, Sensitive_Acc : 16.936, Sensitive_Auc : 0.992, Mean auc: 0.909, Run Time : 94.74 sec
INFO:root:2024-04-26 16:19:48, Best, Step : 2400, Loss : 0.40704, Acc : 0.825, Auc : 0.909, Sensitive_Loss : 0.13168, Sensitive_Acc : 16.936, Sensitive_Auc : 0.992, Best Auc : 0.909
INFO:root:2024-04-26 16:19:53, Train, Epoch : 4, Step : 2410, Loss : 0.32719, Acc : 0.866, Sensitive_Loss : 0.09571, Sensitive_Acc : 16.300, Run Time : 100.86 sec
INFO:root:2024-04-26 16:20:01, Train, Epoch : 4, Step : 2420, Loss : 0.34549, Acc : 0.856, Sensitive_Loss : 0.08753, Sensitive_Acc : 15.900, Run Time : 7.62 sec
INFO:root:2024-04-26 16:20:09, Train, Epoch : 4, Step : 2430, Loss : 0.36074, Acc : 0.869, Sensitive_Loss : 0.15380, Sensitive_Acc : 15.800, Run Time : 7.88 sec
INFO:root:2024-04-26 16:20:16, Train, Epoch : 4, Step : 2440, Loss : 0.33652, Acc : 0.828, Sensitive_Loss : 0.16106, Sensitive_Acc : 17.100, Run Time : 7.03 sec
INFO:root:2024-04-26 16:20:24, Train, Epoch : 4, Step : 2450, Loss : 0.35996, Acc : 0.850, Sensitive_Loss : 0.09778, Sensitive_Acc : 16.700, Run Time : 7.77 sec
INFO:root:2024-04-26 16:20:31, Train, Epoch : 4, Step : 2460, Loss : 0.32227, Acc : 0.866, Sensitive_Loss : 0.16084, Sensitive_Acc : 15.700, Run Time : 7.23 sec
INFO:root:2024-04-26 16:20:38, Train, Epoch : 4, Step : 2470, Loss : 0.32344, Acc : 0.847, Sensitive_Loss : 0.10144, Sensitive_Acc : 17.100, Run Time : 7.55 sec
INFO:root:2024-04-26 16:20:46, Train, Epoch : 4, Step : 2480, Loss : 0.32571, Acc : 0.844, Sensitive_Loss : 0.15485, Sensitive_Acc : 17.100, Run Time : 8.02 sec
INFO:root:2024-04-26 16:20:53, Train, Epoch : 4, Step : 2490, Loss : 0.42454, Acc : 0.803, Sensitive_Loss : 0.16540, Sensitive_Acc : 15.000, Run Time : 7.02 sec
INFO:root:2024-04-26 16:21:01, Train, Epoch : 4, Step : 2500, Loss : 0.38059, Acc : 0.831, Sensitive_Loss : 0.19228, Sensitive_Acc : 16.100, Run Time : 7.29 sec
INFO:root:2024-04-26 16:22:35, Dev, Step : 2500, Loss : 0.42793, Acc : 0.821, Auc : 0.908, Sensitive_Loss : 0.14852, Sensitive_Acc : 16.850, Sensitive_Auc : 0.990, Mean auc: 0.908, Run Time : 94.75 sec
INFO:root:2024-04-26 16:24:10
INFO:root:y_pred: [0.22189255 0.91163754 0.01561881 ... 0.80823946 0.00139822 0.8847922 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.22375560e-01 5.87341283e-03 2.19746634e-01 4.56730789e-03
 9.98376250e-01 2.41763983e-03 9.99480188e-01 9.91825938e-01
 1.15906838e-02 9.03249979e-01 9.82397735e-01 9.99253929e-01
 9.95720446e-01 9.41327810e-01 1.06762759e-01 9.62524593e-01
 9.99507785e-01 5.04573807e-02 1.45169586e-01 9.70599711e-01
 9.75510478e-01 3.04366066e-03 9.94685292e-01 9.63457227e-01
 9.94736016e-01 9.42692220e-01 1.45787140e-02 9.83073235e-01
 9.90815043e-01 6.78133070e-01 2.62171519e-03 4.20587435e-02
 4.73987777e-04 3.39673087e-02 1.82124957e-01 7.49728223e-03
 1.42443672e-01 1.64891891e-02 9.76165295e-01 9.96969044e-01
 7.07845829e-05 2.19251524e-04 9.61475432e-01 4.90127783e-03
 9.98866320e-01 9.89635289e-01 9.94844317e-01 8.05569828e-01
 6.90657943e-02 9.83717084e-01 9.91789103e-01 2.71593016e-02
 8.15981328e-01 4.88502299e-03 2.68413406e-03 5.41732050e-02
 6.45641237e-02 4.30653170e-02 2.26621646e-02 3.37328792e-01
 7.80189363e-03 3.93146515e-01 4.96630445e-02 9.55647111e-01
 1.14897028e-01 9.97596085e-01 2.30584685e-02 9.88393128e-01
 9.13951337e-01 6.03063405e-02 9.40094709e-01 7.45150506e-01
 3.69931608e-02 2.26714477e-01 8.34220089e-03 1.29317038e-03
 4.18084785e-02 3.32510948e-01 4.95233980e-04 9.92660284e-01
 9.97434318e-01 1.22736795e-02 4.02404159e-01 5.55597851e-03
 9.18019831e-01 9.37675297e-01 9.02249571e-03 7.45851472e-02
 8.13653708e-01 9.83909786e-01 9.97780740e-01 2.27666330e-02
 3.60708497e-02 9.92229402e-01 7.50594437e-02 9.49310721e-04
 9.87018168e-01 9.82308626e-01 6.82952860e-03 2.94354022e-03
 9.66696322e-01 9.63792861e-01 9.92540777e-01 9.87261832e-01
 6.08621864e-03 7.28641031e-03 9.03102160e-01 9.89931345e-01
 9.08847988e-01 3.86930048e-03 9.59423244e-01 9.97735023e-01
 8.74214619e-02 9.95715916e-01 9.78037179e-01 9.90035713e-01
 8.26020002e-01 9.98254240e-01 8.60001445e-02 6.11708999e-01
 9.96208787e-01 9.95407760e-01 1.47652498e-03 8.93465281e-01
 9.98909950e-01 3.11797321e-01 9.87545550e-01 4.29674573e-02
 4.70456213e-01 9.80014980e-01 9.97922003e-01 7.44036050e-04
 1.17039412e-01 1.52100557e-02 9.95245397e-01 9.96911108e-01
 9.64225769e-01 3.98665434e-03 4.06778865e-02 9.70530868e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 16:24:10, Dev, Step : 2504, Loss : 0.43844, Acc : 0.816, Auc : 0.907, Sensitive_Loss : 0.15080, Sensitive_Acc : 16.850, Sensitive_Auc : 0.991, Mean auc: 0.907, Run Time : 92.90 sec
INFO:root:2024-04-26 16:24:16, Train, Epoch : 5, Step : 2510, Loss : 0.22517, Acc : 0.497, Sensitive_Loss : 0.06261, Sensitive_Acc : 9.500, Run Time : 5.26 sec
INFO:root:2024-04-26 16:24:24, Train, Epoch : 5, Step : 2520, Loss : 0.36675, Acc : 0.853, Sensitive_Loss : 0.13445, Sensitive_Acc : 16.000, Run Time : 7.75 sec
INFO:root:2024-04-26 16:24:31, Train, Epoch : 5, Step : 2530, Loss : 0.37122, Acc : 0.847, Sensitive_Loss : 0.10049, Sensitive_Acc : 15.900, Run Time : 7.25 sec
INFO:root:2024-04-26 16:24:39, Train, Epoch : 5, Step : 2540, Loss : 0.34413, Acc : 0.859, Sensitive_Loss : 0.13213, Sensitive_Acc : 16.700, Run Time : 7.76 sec
INFO:root:2024-04-26 16:24:46, Train, Epoch : 5, Step : 2550, Loss : 0.34236, Acc : 0.863, Sensitive_Loss : 0.11767, Sensitive_Acc : 16.200, Run Time : 7.24 sec
INFO:root:2024-04-26 16:24:53, Train, Epoch : 5, Step : 2560, Loss : 0.28727, Acc : 0.847, Sensitive_Loss : 0.18693, Sensitive_Acc : 17.000, Run Time : 7.41 sec
INFO:root:2024-04-26 16:25:01, Train, Epoch : 5, Step : 2570, Loss : 0.32761, Acc : 0.859, Sensitive_Loss : 0.11218, Sensitive_Acc : 16.600, Run Time : 7.51 sec
INFO:root:2024-04-26 16:25:08, Train, Epoch : 5, Step : 2580, Loss : 0.35650, Acc : 0.834, Sensitive_Loss : 0.14036, Sensitive_Acc : 16.000, Run Time : 7.23 sec
INFO:root:2024-04-26 16:25:15, Train, Epoch : 5, Step : 2590, Loss : 0.31465, Acc : 0.887, Sensitive_Loss : 0.14944, Sensitive_Acc : 18.100, Run Time : 7.40 sec
INFO:root:2024-04-26 16:25:23, Train, Epoch : 5, Step : 2600, Loss : 0.30608, Acc : 0.881, Sensitive_Loss : 0.14257, Sensitive_Acc : 16.600, Run Time : 7.99 sec
INFO:root:2024-04-26 16:29:27, Dev, Step : 2600, Loss : 0.40552, Acc : 0.825, Auc : 0.908, Sensitive_Loss : 0.14047, Sensitive_Acc : 16.993, Sensitive_Auc : 0.991, Mean auc: 0.908, Run Time : 243.39 sec
INFO:root:2024-04-26 16:29:42, Train, Epoch : 5, Step : 2610, Loss : 0.40853, Acc : 0.841, Sensitive_Loss : 0.11752, Sensitive_Acc : 15.700, Run Time : 258.67 sec
INFO:root:2024-04-26 16:30:14, Train, Epoch : 5, Step : 2620, Loss : 0.33970, Acc : 0.856, Sensitive_Loss : 0.13675, Sensitive_Acc : 16.000, Run Time : 31.57 sec
INFO:root:2024-04-26 16:30:39, Train, Epoch : 5, Step : 2630, Loss : 0.28185, Acc : 0.866, Sensitive_Loss : 0.14899, Sensitive_Acc : 15.500, Run Time : 25.46 sec
INFO:root:2024-04-26 16:30:48, Train, Epoch : 5, Step : 2640, Loss : 0.33169, Acc : 0.844, Sensitive_Loss : 0.12789, Sensitive_Acc : 15.300, Run Time : 8.41 sec
INFO:root:2024-04-26 16:30:56, Train, Epoch : 5, Step : 2650, Loss : 0.33845, Acc : 0.853, Sensitive_Loss : 0.10689, Sensitive_Acc : 17.400, Run Time : 8.14 sec
INFO:root:2024-04-26 16:31:04, Train, Epoch : 5, Step : 2660, Loss : 0.32893, Acc : 0.853, Sensitive_Loss : 0.08778, Sensitive_Acc : 16.500, Run Time : 7.85 sec
INFO:root:2024-04-26 16:31:12, Train, Epoch : 5, Step : 2670, Loss : 0.28133, Acc : 0.884, Sensitive_Loss : 0.13260, Sensitive_Acc : 16.900, Run Time : 8.07 sec
INFO:root:2024-04-26 16:31:20, Train, Epoch : 5, Step : 2680, Loss : 0.34417, Acc : 0.850, Sensitive_Loss : 0.11475, Sensitive_Acc : 15.400, Run Time : 8.61 sec
INFO:root:2024-04-26 16:31:28, Train, Epoch : 5, Step : 2690, Loss : 0.27662, Acc : 0.875, Sensitive_Loss : 0.08770, Sensitive_Acc : 15.900, Run Time : 7.85 sec
INFO:root:2024-04-26 16:31:36, Train, Epoch : 5, Step : 2700, Loss : 0.30658, Acc : 0.863, Sensitive_Loss : 0.11535, Sensitive_Acc : 15.300, Run Time : 8.08 sec
INFO:root:2024-04-26 16:33:10, Dev, Step : 2700, Loss : 0.43332, Acc : 0.818, Auc : 0.908, Sensitive_Loss : 0.15751, Sensitive_Acc : 16.850, Sensitive_Auc : 0.992, Mean auc: 0.908, Run Time : 93.85 sec
INFO:root:2024-04-26 16:33:15, Train, Epoch : 5, Step : 2710, Loss : 0.31828, Acc : 0.859, Sensitive_Loss : 0.11569, Sensitive_Acc : 16.100, Run Time : 99.33 sec
INFO:root:2024-04-26 16:33:24, Train, Epoch : 5, Step : 2720, Loss : 0.37972, Acc : 0.844, Sensitive_Loss : 0.13053, Sensitive_Acc : 16.000, Run Time : 8.12 sec
INFO:root:2024-04-26 16:33:31, Train, Epoch : 5, Step : 2730, Loss : 0.33526, Acc : 0.847, Sensitive_Loss : 0.09862, Sensitive_Acc : 16.900, Run Time : 7.65 sec
INFO:root:2024-04-26 16:33:39, Train, Epoch : 5, Step : 2740, Loss : 0.27731, Acc : 0.884, Sensitive_Loss : 0.12401, Sensitive_Acc : 16.700, Run Time : 7.38 sec
INFO:root:2024-04-26 16:33:47, Train, Epoch : 5, Step : 2750, Loss : 0.30962, Acc : 0.872, Sensitive_Loss : 0.15730, Sensitive_Acc : 16.900, Run Time : 8.39 sec
INFO:root:2024-04-26 16:33:55, Train, Epoch : 5, Step : 2760, Loss : 0.30542, Acc : 0.859, Sensitive_Loss : 0.10594, Sensitive_Acc : 16.500, Run Time : 7.51 sec
INFO:root:2024-04-26 16:34:02, Train, Epoch : 5, Step : 2770, Loss : 0.30524, Acc : 0.863, Sensitive_Loss : 0.10282, Sensitive_Acc : 16.300, Run Time : 7.85 sec
INFO:root:2024-04-26 16:34:11, Train, Epoch : 5, Step : 2780, Loss : 0.33020, Acc : 0.872, Sensitive_Loss : 0.12281, Sensitive_Acc : 17.300, Run Time : 8.11 sec
INFO:root:2024-04-26 16:34:18, Train, Epoch : 5, Step : 2790, Loss : 0.28738, Acc : 0.875, Sensitive_Loss : 0.13479, Sensitive_Acc : 16.700, Run Time : 7.74 sec
INFO:root:2024-04-26 16:34:26, Train, Epoch : 5, Step : 2800, Loss : 0.29878, Acc : 0.866, Sensitive_Loss : 0.13343, Sensitive_Acc : 16.000, Run Time : 7.62 sec
INFO:root:2024-04-26 16:36:01, Dev, Step : 2800, Loss : 0.41564, Acc : 0.826, Auc : 0.909, Sensitive_Loss : 0.14775, Sensitive_Acc : 16.879, Sensitive_Auc : 0.993, Mean auc: 0.909, Run Time : 94.66 sec
INFO:root:2024-04-26 16:36:06, Train, Epoch : 5, Step : 2810, Loss : 0.35136, Acc : 0.863, Sensitive_Loss : 0.12006, Sensitive_Acc : 17.800, Run Time : 100.50 sec
INFO:root:2024-04-26 16:36:14, Train, Epoch : 5, Step : 2820, Loss : 0.30789, Acc : 0.853, Sensitive_Loss : 0.15952, Sensitive_Acc : 17.300, Run Time : 7.82 sec
INFO:root:2024-04-26 16:36:22, Train, Epoch : 5, Step : 2830, Loss : 0.32429, Acc : 0.866, Sensitive_Loss : 0.14299, Sensitive_Acc : 16.900, Run Time : 7.98 sec
INFO:root:2024-04-26 16:36:30, Train, Epoch : 5, Step : 2840, Loss : 0.29397, Acc : 0.878, Sensitive_Loss : 0.15540, Sensitive_Acc : 15.100, Run Time : 7.76 sec
INFO:root:2024-04-26 16:36:38, Train, Epoch : 5, Step : 2850, Loss : 0.30157, Acc : 0.878, Sensitive_Loss : 0.11758, Sensitive_Acc : 14.900, Run Time : 7.61 sec
INFO:root:2024-04-26 16:36:45, Train, Epoch : 5, Step : 2860, Loss : 0.30167, Acc : 0.869, Sensitive_Loss : 0.12330, Sensitive_Acc : 17.200, Run Time : 7.47 sec
INFO:root:2024-04-26 16:36:53, Train, Epoch : 5, Step : 2870, Loss : 0.31927, Acc : 0.869, Sensitive_Loss : 0.13694, Sensitive_Acc : 16.100, Run Time : 7.91 sec
INFO:root:2024-04-26 16:37:01, Train, Epoch : 5, Step : 2880, Loss : 0.32541, Acc : 0.884, Sensitive_Loss : 0.11467, Sensitive_Acc : 15.500, Run Time : 7.74 sec
INFO:root:2024-04-26 16:37:08, Train, Epoch : 5, Step : 2890, Loss : 0.30987, Acc : 0.859, Sensitive_Loss : 0.12256, Sensitive_Acc : 16.800, Run Time : 7.59 sec
INFO:root:2024-04-26 16:37:16, Train, Epoch : 5, Step : 2900, Loss : 0.31409, Acc : 0.866, Sensitive_Loss : 0.15855, Sensitive_Acc : 16.100, Run Time : 7.70 sec
INFO:root:2024-04-26 16:39:45, Dev, Step : 2900, Loss : 0.43469, Acc : 0.815, Auc : 0.910, Sensitive_Loss : 0.16526, Sensitive_Acc : 16.793, Sensitive_Auc : 0.993, Mean auc: 0.910, Run Time : 148.60 sec
INFO:root:2024-04-26 16:39:45, Best, Step : 2900, Loss : 0.43469, Acc : 0.815, Auc : 0.910, Sensitive_Loss : 0.16526, Sensitive_Acc : 16.793, Sensitive_Auc : 0.993, Best Auc : 0.910
INFO:root:2024-04-26 16:39:51, Train, Epoch : 5, Step : 2910, Loss : 0.29762, Acc : 0.866, Sensitive_Loss : 0.10101, Sensitive_Acc : 17.900, Run Time : 154.79 sec
INFO:root:2024-04-26 16:39:58, Train, Epoch : 5, Step : 2920, Loss : 0.39885, Acc : 0.834, Sensitive_Loss : 0.13191, Sensitive_Acc : 16.000, Run Time : 7.18 sec
INFO:root:2024-04-26 16:40:05, Train, Epoch : 5, Step : 2930, Loss : 0.28659, Acc : 0.869, Sensitive_Loss : 0.13709, Sensitive_Acc : 15.000, Run Time : 7.55 sec
INFO:root:2024-04-26 16:40:14, Train, Epoch : 5, Step : 2940, Loss : 0.25942, Acc : 0.872, Sensitive_Loss : 0.11108, Sensitive_Acc : 16.800, Run Time : 8.70 sec
INFO:root:2024-04-26 16:40:23, Train, Epoch : 5, Step : 2950, Loss : 0.29206, Acc : 0.881, Sensitive_Loss : 0.09202, Sensitive_Acc : 16.600, Run Time : 8.77 sec
INFO:root:2024-04-26 16:40:32, Train, Epoch : 5, Step : 2960, Loss : 0.27115, Acc : 0.863, Sensitive_Loss : 0.10626, Sensitive_Acc : 16.100, Run Time : 8.63 sec
INFO:root:2024-04-26 16:40:41, Train, Epoch : 5, Step : 2970, Loss : 0.33889, Acc : 0.856, Sensitive_Loss : 0.13766, Sensitive_Acc : 15.400, Run Time : 9.49 sec
INFO:root:2024-04-26 16:41:21, Train, Epoch : 5, Step : 2980, Loss : 0.28185, Acc : 0.869, Sensitive_Loss : 0.14634, Sensitive_Acc : 16.000, Run Time : 40.13 sec
INFO:root:2024-04-26 16:41:29, Train, Epoch : 5, Step : 2990, Loss : 0.30823, Acc : 0.844, Sensitive_Loss : 0.11031, Sensitive_Acc : 15.700, Run Time : 7.83 sec
INFO:root:2024-04-26 16:41:44, Train, Epoch : 5, Step : 3000, Loss : 0.33205, Acc : 0.869, Sensitive_Loss : 0.11448, Sensitive_Acc : 16.300, Run Time : 15.17 sec
INFO:root:2024-04-26 16:43:59, Dev, Step : 3000, Loss : 0.41505, Acc : 0.824, Auc : 0.906, Sensitive_Loss : 0.14992, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.906, Run Time : 134.88 sec
INFO:root:2024-04-26 16:44:06, Train, Epoch : 5, Step : 3010, Loss : 0.36616, Acc : 0.828, Sensitive_Loss : 0.09962, Sensitive_Acc : 16.200, Run Time : 141.46 sec
INFO:root:2024-04-26 16:44:18, Train, Epoch : 5, Step : 3020, Loss : 0.30419, Acc : 0.875, Sensitive_Loss : 0.11814, Sensitive_Acc : 17.300, Run Time : 12.28 sec
INFO:root:2024-04-26 16:44:29, Train, Epoch : 5, Step : 3030, Loss : 0.32368, Acc : 0.863, Sensitive_Loss : 0.10401, Sensitive_Acc : 17.000, Run Time : 10.85 sec
INFO:root:2024-04-26 16:44:39, Train, Epoch : 5, Step : 3040, Loss : 0.35522, Acc : 0.819, Sensitive_Loss : 0.13746, Sensitive_Acc : 15.200, Run Time : 9.77 sec
INFO:root:2024-04-26 16:44:52, Train, Epoch : 5, Step : 3050, Loss : 0.29687, Acc : 0.891, Sensitive_Loss : 0.11277, Sensitive_Acc : 16.300, Run Time : 13.07 sec
INFO:root:2024-04-26 16:45:03, Train, Epoch : 5, Step : 3060, Loss : 0.35007, Acc : 0.866, Sensitive_Loss : 0.10483, Sensitive_Acc : 14.800, Run Time : 11.25 sec
INFO:root:2024-04-26 16:45:12, Train, Epoch : 5, Step : 3070, Loss : 0.29794, Acc : 0.872, Sensitive_Loss : 0.11607, Sensitive_Acc : 17.600, Run Time : 8.75 sec
INFO:root:2024-04-26 16:45:21, Train, Epoch : 5, Step : 3080, Loss : 0.29818, Acc : 0.881, Sensitive_Loss : 0.13085, Sensitive_Acc : 14.200, Run Time : 8.92 sec
INFO:root:2024-04-26 16:45:30, Train, Epoch : 5, Step : 3090, Loss : 0.31792, Acc : 0.856, Sensitive_Loss : 0.15028, Sensitive_Acc : 17.100, Run Time : 8.98 sec
INFO:root:2024-04-26 16:45:39, Train, Epoch : 5, Step : 3100, Loss : 0.27822, Acc : 0.894, Sensitive_Loss : 0.08972, Sensitive_Acc : 16.800, Run Time : 9.11 sec
INFO:root:2024-04-26 16:47:22, Dev, Step : 3100, Loss : 0.42311, Acc : 0.820, Auc : 0.909, Sensitive_Loss : 0.14166, Sensitive_Acc : 16.921, Sensitive_Auc : 0.993, Mean auc: 0.909, Run Time : 103.66 sec
INFO:root:2024-04-26 16:47:29, Train, Epoch : 5, Step : 3110, Loss : 0.32931, Acc : 0.853, Sensitive_Loss : 0.14718, Sensitive_Acc : 15.800, Run Time : 110.67 sec
INFO:root:2024-04-26 16:47:39, Train, Epoch : 5, Step : 3120, Loss : 0.30836, Acc : 0.872, Sensitive_Loss : 0.08855, Sensitive_Acc : 15.800, Run Time : 9.35 sec
INFO:root:2024-04-26 16:48:03, Train, Epoch : 5, Step : 3130, Loss : 0.36350, Acc : 0.847, Sensitive_Loss : 0.14757, Sensitive_Acc : 16.100, Run Time : 24.09 sec
INFO:root:2024-04-26 16:50:12
INFO:root:y_pred: [0.3488549  0.92901236 0.02967374 ... 0.8758892  0.00207614 0.8879442 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [8.82163107e-01 8.05409346e-03 1.19032137e-01 5.38273947e-03
 9.96624708e-01 2.39073462e-03 9.98709202e-01 9.91578102e-01
 9.30484198e-03 9.01223004e-01 9.79974687e-01 9.99041498e-01
 9.93432522e-01 9.14936185e-01 7.86347836e-02 9.35052037e-01
 9.99579728e-01 4.13476378e-02 1.29547253e-01 9.76369083e-01
 9.68991399e-01 1.26454672e-02 9.88380432e-01 9.53750432e-01
 9.91216898e-01 9.48572516e-01 4.85441973e-03 9.76097822e-01
 9.87541378e-01 6.90064251e-01 3.56267323e-03 4.38654721e-02
 8.65093898e-04 2.54098903e-02 1.28494263e-01 5.12876315e-03
 3.84854414e-02 1.18429707e-02 9.69020963e-01 9.92776513e-01
 5.17919470e-05 4.12450201e-04 9.43681955e-01 2.92532658e-03
 9.98448849e-01 9.90372181e-01 9.89887893e-01 8.41910779e-01
 5.56040220e-02 9.79126692e-01 9.90253866e-01 2.61916965e-02
 7.62089729e-01 5.05253067e-03 2.43535941e-03 3.67470942e-02
 1.00918807e-01 2.86216252e-02 1.50706526e-02 3.29901099e-01
 8.32767412e-03 3.21715236e-01 5.33893369e-02 9.57828701e-01
 5.44000901e-02 9.94751632e-01 1.19907139e-02 9.87111628e-01
 8.77186954e-01 1.05689943e-01 9.44914162e-01 6.24176443e-01
 2.05045100e-02 1.07713431e-01 1.06513407e-02 1.19306054e-03
 2.41236854e-02 2.47256175e-01 4.74209490e-04 9.91308689e-01
 9.96259212e-01 5.03237313e-03 2.66757458e-01 5.07664680e-03
 7.47205734e-01 9.32553291e-01 3.61779379e-03 4.64645363e-02
 8.10783505e-01 9.71540689e-01 9.93013442e-01 9.44627449e-03
 4.29932065e-02 9.92209196e-01 1.37612641e-01 9.70667053e-04
 9.81099427e-01 9.67126191e-01 3.33464006e-03 3.59188672e-03
 9.48996365e-01 9.43719745e-01 9.92351115e-01 9.74362135e-01
 1.25739500e-02 3.22407763e-03 8.76693904e-01 9.86993313e-01
 8.46480906e-01 3.17963352e-03 9.56876516e-01 9.95745957e-01
 7.35984966e-02 9.94497836e-01 9.79006231e-01 9.89609241e-01
 8.59684706e-01 9.96947825e-01 8.09176266e-02 5.27286589e-01
 9.95659411e-01 9.92991209e-01 2.68155825e-03 8.39658618e-01
 9.97322977e-01 2.61208296e-01 9.80245531e-01 3.07480954e-02
 5.91193616e-01 9.56899643e-01 9.95486438e-01 1.16332981e-03
 1.06645316e-01 2.32412871e-02 9.88501072e-01 9.96741474e-01
 9.22728896e-01 4.37539723e-03 3.87482643e-02 9.64229107e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 16:50:12, Dev, Step : 3130, Loss : 0.41412, Acc : 0.828, Auc : 0.908, Sensitive_Loss : 0.13944, Sensitive_Acc : 16.921, Sensitive_Auc : 0.992, Mean auc: 0.908, Run Time : 129.39 sec
INFO:root:2024-04-26 16:50:23, Train, Epoch : 6, Step : 3140, Loss : 0.31551, Acc : 0.850, Sensitive_Loss : 0.12176, Sensitive_Acc : 16.000, Run Time : 9.36 sec
INFO:root:2024-04-26 16:50:31, Train, Epoch : 6, Step : 3150, Loss : 0.31390, Acc : 0.859, Sensitive_Loss : 0.14349, Sensitive_Acc : 14.600, Run Time : 8.59 sec
INFO:root:2024-04-26 16:50:55, Train, Epoch : 6, Step : 3160, Loss : 0.36723, Acc : 0.853, Sensitive_Loss : 0.17307, Sensitive_Acc : 15.800, Run Time : 23.56 sec
INFO:root:2024-04-26 16:51:04, Train, Epoch : 6, Step : 3170, Loss : 0.33549, Acc : 0.859, Sensitive_Loss : 0.10653, Sensitive_Acc : 16.000, Run Time : 8.95 sec
INFO:root:2024-04-26 16:51:14, Train, Epoch : 6, Step : 3180, Loss : 0.30040, Acc : 0.853, Sensitive_Loss : 0.11663, Sensitive_Acc : 15.200, Run Time : 9.68 sec
INFO:root:2024-04-26 16:51:23, Train, Epoch : 6, Step : 3190, Loss : 0.34503, Acc : 0.847, Sensitive_Loss : 0.12403, Sensitive_Acc : 16.300, Run Time : 8.99 sec
INFO:root:2024-04-26 16:51:31, Train, Epoch : 6, Step : 3200, Loss : 0.33058, Acc : 0.869, Sensitive_Loss : 0.10703, Sensitive_Acc : 17.600, Run Time : 8.78 sec
INFO:root:2024-04-26 16:54:19, Dev, Step : 3200, Loss : 0.41682, Acc : 0.823, Auc : 0.908, Sensitive_Loss : 0.13581, Sensitive_Acc : 16.850, Sensitive_Auc : 0.993, Mean auc: 0.908, Run Time : 167.85 sec
INFO:root:2024-04-26 16:54:26, Train, Epoch : 6, Step : 3210, Loss : 0.30429, Acc : 0.859, Sensitive_Loss : 0.12323, Sensitive_Acc : 15.100, Run Time : 174.25 sec
INFO:root:2024-04-26 16:54:34, Train, Epoch : 6, Step : 3220, Loss : 0.31053, Acc : 0.856, Sensitive_Loss : 0.14917, Sensitive_Acc : 17.400, Run Time : 8.92 sec
INFO:root:2024-04-26 16:54:44, Train, Epoch : 6, Step : 3230, Loss : 0.33291, Acc : 0.853, Sensitive_Loss : 0.11783, Sensitive_Acc : 16.000, Run Time : 9.35 sec
INFO:root:2024-04-26 16:54:54, Train, Epoch : 6, Step : 3240, Loss : 0.30008, Acc : 0.887, Sensitive_Loss : 0.13763, Sensitive_Acc : 15.800, Run Time : 9.94 sec
INFO:root:2024-04-26 16:55:03, Train, Epoch : 6, Step : 3250, Loss : 0.28268, Acc : 0.872, Sensitive_Loss : 0.18773, Sensitive_Acc : 16.000, Run Time : 9.10 sec
INFO:root:2024-04-26 16:55:27, Train, Epoch : 6, Step : 3260, Loss : 0.30171, Acc : 0.863, Sensitive_Loss : 0.14131, Sensitive_Acc : 16.300, Run Time : 23.81 sec
INFO:root:2024-04-26 16:55:36, Train, Epoch : 6, Step : 3270, Loss : 0.30006, Acc : 0.881, Sensitive_Loss : 0.08379, Sensitive_Acc : 16.300, Run Time : 9.00 sec
INFO:root:2024-04-26 16:56:00, Train, Epoch : 6, Step : 3280, Loss : 0.26323, Acc : 0.912, Sensitive_Loss : 0.11915, Sensitive_Acc : 18.700, Run Time : 24.46 sec
INFO:root:2024-04-26 16:56:09, Train, Epoch : 6, Step : 3290, Loss : 0.33191, Acc : 0.878, Sensitive_Loss : 0.08072, Sensitive_Acc : 17.000, Run Time : 9.01 sec
INFO:root:2024-04-26 16:56:23, Train, Epoch : 6, Step : 3300, Loss : 0.32239, Acc : 0.856, Sensitive_Loss : 0.09477, Sensitive_Acc : 15.900, Run Time : 13.51 sec
INFO:root:2024-04-26 16:59:06, Dev, Step : 3300, Loss : 0.43539, Acc : 0.818, Auc : 0.905, Sensitive_Loss : 0.15100, Sensitive_Acc : 16.921, Sensitive_Auc : 0.991, Mean auc: 0.905, Run Time : 163.27 sec
INFO:root:2024-04-26 16:59:18, Train, Epoch : 6, Step : 3310, Loss : 0.30271, Acc : 0.863, Sensitive_Loss : 0.10626, Sensitive_Acc : 16.300, Run Time : 175.18 sec
INFO:root:2024-04-26 16:59:41, Train, Epoch : 6, Step : 3320, Loss : 0.31088, Acc : 0.872, Sensitive_Loss : 0.11311, Sensitive_Acc : 16.000, Run Time : 22.79 sec
INFO:root:2024-04-26 16:59:51, Train, Epoch : 6, Step : 3330, Loss : 0.30135, Acc : 0.859, Sensitive_Loss : 0.09727, Sensitive_Acc : 15.800, Run Time : 10.66 sec
INFO:root:2024-04-26 17:00:02, Train, Epoch : 6, Step : 3340, Loss : 0.32268, Acc : 0.863, Sensitive_Loss : 0.14406, Sensitive_Acc : 15.700, Run Time : 10.19 sec
INFO:root:2024-04-26 17:00:22, Train, Epoch : 6, Step : 3350, Loss : 0.30632, Acc : 0.850, Sensitive_Loss : 0.16169, Sensitive_Acc : 15.400, Run Time : 20.72 sec
INFO:root:2024-04-26 17:00:59, Train, Epoch : 6, Step : 3360, Loss : 0.28165, Acc : 0.878, Sensitive_Loss : 0.16364, Sensitive_Acc : 15.800, Run Time : 36.11 sec
INFO:root:2024-04-26 17:01:16, Train, Epoch : 6, Step : 3370, Loss : 0.29427, Acc : 0.891, Sensitive_Loss : 0.14782, Sensitive_Acc : 16.500, Run Time : 17.17 sec
INFO:root:2024-04-26 17:01:24, Train, Epoch : 6, Step : 3380, Loss : 0.31293, Acc : 0.881, Sensitive_Loss : 0.12102, Sensitive_Acc : 15.800, Run Time : 8.59 sec
INFO:root:2024-04-26 17:01:33, Train, Epoch : 6, Step : 3390, Loss : 0.27342, Acc : 0.897, Sensitive_Loss : 0.10345, Sensitive_Acc : 16.600, Run Time : 8.95 sec
INFO:root:2024-04-26 17:01:42, Train, Epoch : 6, Step : 3400, Loss : 0.28771, Acc : 0.881, Sensitive_Loss : 0.13000, Sensitive_Acc : 17.000, Run Time : 9.23 sec
INFO:root:2024-04-26 17:03:57, Dev, Step : 3400, Loss : 0.42666, Acc : 0.822, Auc : 0.908, Sensitive_Loss : 0.13084, Sensitive_Acc : 16.850, Sensitive_Auc : 0.994, Mean auc: 0.908, Run Time : 134.70 sec
INFO:root:2024-04-26 17:04:04, Train, Epoch : 6, Step : 3410, Loss : 0.26733, Acc : 0.887, Sensitive_Loss : 0.11829, Sensitive_Acc : 16.100, Run Time : 141.47 sec
INFO:root:2024-04-26 17:04:13, Train, Epoch : 6, Step : 3420, Loss : 0.30313, Acc : 0.859, Sensitive_Loss : 0.13855, Sensitive_Acc : 17.000, Run Time : 9.09 sec
INFO:root:2024-04-26 17:04:22, Train, Epoch : 6, Step : 3430, Loss : 0.27449, Acc : 0.875, Sensitive_Loss : 0.12906, Sensitive_Acc : 17.000, Run Time : 8.89 sec
INFO:root:2024-04-26 17:04:31, Train, Epoch : 6, Step : 3440, Loss : 0.24435, Acc : 0.897, Sensitive_Loss : 0.10667, Sensitive_Acc : 16.100, Run Time : 8.98 sec
INFO:root:2024-04-26 17:04:40, Train, Epoch : 6, Step : 3450, Loss : 0.28202, Acc : 0.872, Sensitive_Loss : 0.10366, Sensitive_Acc : 16.700, Run Time : 9.20 sec
INFO:root:2024-04-26 17:04:49, Train, Epoch : 6, Step : 3460, Loss : 0.32375, Acc : 0.884, Sensitive_Loss : 0.16328, Sensitive_Acc : 16.200, Run Time : 9.31 sec
INFO:root:2024-04-26 17:04:59, Train, Epoch : 6, Step : 3470, Loss : 0.32189, Acc : 0.856, Sensitive_Loss : 0.11737, Sensitive_Acc : 16.300, Run Time : 9.20 sec
INFO:root:2024-04-26 17:05:08, Train, Epoch : 6, Step : 3480, Loss : 0.28790, Acc : 0.878, Sensitive_Loss : 0.14771, Sensitive_Acc : 16.300, Run Time : 9.16 sec
INFO:root:2024-04-26 17:05:17, Train, Epoch : 6, Step : 3490, Loss : 0.24722, Acc : 0.878, Sensitive_Loss : 0.14311, Sensitive_Acc : 16.700, Run Time : 9.08 sec
INFO:root:2024-04-26 17:05:27, Train, Epoch : 6, Step : 3500, Loss : 0.30629, Acc : 0.872, Sensitive_Loss : 0.13430, Sensitive_Acc : 16.200, Run Time : 9.71 sec
INFO:root:2024-04-26 17:07:04, Dev, Step : 3500, Loss : 0.44665, Acc : 0.815, Auc : 0.905, Sensitive_Loss : 0.13638, Sensitive_Acc : 16.921, Sensitive_Auc : 0.993, Mean auc: 0.905, Run Time : 97.68 sec
INFO:root:2024-04-26 17:07:11, Train, Epoch : 6, Step : 3510, Loss : 0.33468, Acc : 0.850, Sensitive_Loss : 0.13639, Sensitive_Acc : 17.700, Run Time : 104.17 sec
INFO:root:2024-04-26 17:07:20, Train, Epoch : 6, Step : 3520, Loss : 0.31238, Acc : 0.887, Sensitive_Loss : 0.09752, Sensitive_Acc : 16.400, Run Time : 9.36 sec
INFO:root:2024-04-26 17:07:28, Train, Epoch : 6, Step : 3530, Loss : 0.31011, Acc : 0.841, Sensitive_Loss : 0.09186, Sensitive_Acc : 17.600, Run Time : 8.41 sec
INFO:root:2024-04-26 17:07:38, Train, Epoch : 6, Step : 3540, Loss : 0.32269, Acc : 0.875, Sensitive_Loss : 0.11431, Sensitive_Acc : 16.900, Run Time : 9.06 sec
INFO:root:2024-04-26 17:07:47, Train, Epoch : 6, Step : 3550, Loss : 0.25859, Acc : 0.866, Sensitive_Loss : 0.13292, Sensitive_Acc : 16.800, Run Time : 9.12 sec
INFO:root:2024-04-26 17:07:56, Train, Epoch : 6, Step : 3560, Loss : 0.33562, Acc : 0.853, Sensitive_Loss : 0.12649, Sensitive_Acc : 17.900, Run Time : 9.00 sec
INFO:root:2024-04-26 17:08:05, Train, Epoch : 6, Step : 3570, Loss : 0.25315, Acc : 0.884, Sensitive_Loss : 0.11461, Sensitive_Acc : 15.800, Run Time : 9.30 sec
INFO:root:2024-04-26 17:08:15, Train, Epoch : 6, Step : 3580, Loss : 0.31322, Acc : 0.878, Sensitive_Loss : 0.09207, Sensitive_Acc : 16.400, Run Time : 9.60 sec
INFO:root:2024-04-26 17:08:24, Train, Epoch : 6, Step : 3590, Loss : 0.26823, Acc : 0.859, Sensitive_Loss : 0.10490, Sensitive_Acc : 16.000, Run Time : 9.34 sec
INFO:root:2024-04-26 17:08:33, Train, Epoch : 6, Step : 3600, Loss : 0.35691, Acc : 0.850, Sensitive_Loss : 0.10881, Sensitive_Acc : 15.600, Run Time : 9.33 sec
INFO:root:2024-04-26 17:10:11, Dev, Step : 3600, Loss : 0.43351, Acc : 0.817, Auc : 0.907, Sensitive_Loss : 0.14169, Sensitive_Acc : 16.936, Sensitive_Auc : 0.991, Mean auc: 0.907, Run Time : 97.49 sec
INFO:root:2024-04-26 17:10:17, Train, Epoch : 6, Step : 3610, Loss : 0.31829, Acc : 0.841, Sensitive_Loss : 0.12450, Sensitive_Acc : 16.200, Run Time : 103.92 sec
INFO:root:2024-04-26 17:10:27, Train, Epoch : 6, Step : 3620, Loss : 0.32534, Acc : 0.847, Sensitive_Loss : 0.15408, Sensitive_Acc : 16.100, Run Time : 9.59 sec
INFO:root:2024-04-26 17:10:36, Train, Epoch : 6, Step : 3630, Loss : 0.32299, Acc : 0.869, Sensitive_Loss : 0.11056, Sensitive_Acc : 16.300, Run Time : 9.70 sec
INFO:root:2024-04-26 17:10:46, Train, Epoch : 6, Step : 3640, Loss : 0.29964, Acc : 0.872, Sensitive_Loss : 0.11714, Sensitive_Acc : 16.100, Run Time : 9.19 sec
INFO:root:2024-04-26 17:10:55, Train, Epoch : 6, Step : 3650, Loss : 0.28294, Acc : 0.869, Sensitive_Loss : 0.12371, Sensitive_Acc : 17.000, Run Time : 9.30 sec
INFO:root:2024-04-26 17:11:05, Train, Epoch : 6, Step : 3660, Loss : 0.28748, Acc : 0.881, Sensitive_Loss : 0.13884, Sensitive_Acc : 15.900, Run Time : 9.72 sec
INFO:root:2024-04-26 17:11:14, Train, Epoch : 6, Step : 3670, Loss : 0.25610, Acc : 0.891, Sensitive_Loss : 0.10308, Sensitive_Acc : 16.800, Run Time : 9.07 sec
INFO:root:2024-04-26 17:11:23, Train, Epoch : 6, Step : 3680, Loss : 0.30507, Acc : 0.872, Sensitive_Loss : 0.10594, Sensitive_Acc : 16.000, Run Time : 9.16 sec
INFO:root:2024-04-26 17:11:32, Train, Epoch : 6, Step : 3690, Loss : 0.32212, Acc : 0.856, Sensitive_Loss : 0.08990, Sensitive_Acc : 17.200, Run Time : 9.11 sec
INFO:root:2024-04-26 17:11:44, Train, Epoch : 6, Step : 3700, Loss : 0.30688, Acc : 0.866, Sensitive_Loss : 0.12562, Sensitive_Acc : 17.000, Run Time : 11.69 sec
INFO:root:2024-04-26 17:13:34, Dev, Step : 3700, Loss : 0.43814, Acc : 0.816, Auc : 0.903, Sensitive_Loss : 0.16136, Sensitive_Acc : 16.893, Sensitive_Auc : 0.995, Mean auc: 0.903, Run Time : 110.41 sec
INFO:root:2024-04-26 17:13:41, Train, Epoch : 6, Step : 3710, Loss : 0.29815, Acc : 0.866, Sensitive_Loss : 0.12187, Sensitive_Acc : 17.000, Run Time : 117.18 sec
INFO:root:2024-04-26 17:13:51, Train, Epoch : 6, Step : 3720, Loss : 0.28555, Acc : 0.894, Sensitive_Loss : 0.09491, Sensitive_Acc : 17.700, Run Time : 9.79 sec
INFO:root:2024-04-26 17:14:01, Train, Epoch : 6, Step : 3730, Loss : 0.23898, Acc : 0.891, Sensitive_Loss : 0.10634, Sensitive_Acc : 16.100, Run Time : 10.20 sec
INFO:root:2024-04-26 17:14:10, Train, Epoch : 6, Step : 3740, Loss : 0.27832, Acc : 0.881, Sensitive_Loss : 0.13401, Sensitive_Acc : 14.900, Run Time : 9.39 sec
INFO:root:2024-04-26 17:14:20, Train, Epoch : 6, Step : 3750, Loss : 0.32561, Acc : 0.872, Sensitive_Loss : 0.13419, Sensitive_Acc : 17.300, Run Time : 9.78 sec
INFO:root:2024-04-26 17:16:37
INFO:root:y_pred: [2.66245723e-01 9.46154416e-01 1.54885305e-02 ... 9.07110333e-01
 5.19091089e-04 9.16523993e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.12744761e-01 7.74618564e-03 2.17487797e-01 6.61089784e-03
 9.98334229e-01 2.00734404e-03 9.99376357e-01 9.95350599e-01
 1.23934112e-02 9.45484161e-01 9.87127662e-01 9.99267042e-01
 9.95908141e-01 8.99284065e-01 5.94676398e-02 9.59967196e-01
 9.99754131e-01 6.13932945e-02 5.48251152e-01 9.83275890e-01
 9.82324362e-01 3.49765667e-03 9.92415667e-01 9.10769522e-01
 9.93834019e-01 9.51810241e-01 3.34958755e-03 9.85094190e-01
 9.88961399e-01 6.52254999e-01 3.69830965e-03 3.90537120e-02
 3.11633851e-03 2.38958634e-02 6.67986050e-02 7.45334150e-03
 6.99832588e-02 1.34625481e-02 9.70453382e-01 9.94704545e-01
 8.78030696e-05 9.76199633e-04 9.64600444e-01 6.63196202e-03
 9.98981416e-01 9.95135605e-01 9.94117737e-01 9.31769192e-01
 1.43601269e-01 9.78160977e-01 9.90190089e-01 1.51124205e-02
 4.82842386e-01 8.16612598e-03 1.53470458e-03 1.03581831e-01
 1.49952844e-01 2.66589709e-02 1.52422292e-02 1.20689712e-01
 7.85040576e-03 1.93281263e-01 7.83450007e-02 9.55536366e-01
 5.47424220e-02 9.97479737e-01 3.00563569e-03 9.91582334e-01
 9.04141843e-01 1.62144437e-01 9.49068725e-01 7.82864749e-01
 2.12379582e-02 8.88181701e-02 1.34265544e-02 6.08751201e-04
 2.11197659e-02 1.97935492e-01 8.39865592e-04 9.92207468e-01
 9.97838318e-01 9.20273270e-03 2.67834961e-01 7.10811606e-03
 7.66715944e-01 9.56936836e-01 2.52097682e-03 6.51354864e-02
 7.97108173e-01 9.78273392e-01 9.96536374e-01 7.77484290e-03
 8.00481588e-02 9.94060099e-01 5.07702902e-02 7.14723428e-04
 9.86745059e-01 9.83482838e-01 2.09428091e-03 2.05394300e-03
 9.56459045e-01 9.55288291e-01 9.93830264e-01 9.73876715e-01
 2.28694100e-02 1.40685658e-03 9.24863398e-01 9.85441804e-01
 8.45897555e-01 3.35916341e-03 9.61378813e-01 9.98378038e-01
 1.01068676e-01 9.96208668e-01 9.76997733e-01 9.83860612e-01
 6.72883630e-01 9.98003185e-01 1.24104083e-01 7.26641119e-01
 9.96166110e-01 9.92869437e-01 2.15229345e-03 8.83873284e-01
 9.98486757e-01 2.80454308e-01 9.88518178e-01 3.93179841e-02
 5.26419163e-01 9.23326135e-01 9.97630358e-01 3.95466603e-04
 1.19656011e-01 3.35466340e-02 9.93786097e-01 9.97369647e-01
 9.42980468e-01 4.85771941e-03 4.96124215e-02 9.82321262e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 17:16:37, Dev, Step : 3756, Loss : 0.43529, Acc : 0.822, Auc : 0.907, Sensitive_Loss : 0.14466, Sensitive_Acc : 16.836, Sensitive_Auc : 0.994, Mean auc: 0.907, Run Time : 107.68 sec
INFO:root:2024-04-26 17:16:42, Train, Epoch : 7, Step : 3760, Loss : 0.11775, Acc : 0.359, Sensitive_Loss : 0.04737, Sensitive_Acc : 6.900, Run Time : 4.46 sec
INFO:root:2024-04-26 17:16:52, Train, Epoch : 7, Step : 3770, Loss : 0.33213, Acc : 0.850, Sensitive_Loss : 0.12470, Sensitive_Acc : 16.300, Run Time : 9.58 sec
INFO:root:2024-04-26 17:17:05, Train, Epoch : 7, Step : 3780, Loss : 0.24494, Acc : 0.881, Sensitive_Loss : 0.11522, Sensitive_Acc : 16.600, Run Time : 13.34 sec
INFO:root:2024-04-26 17:17:14, Train, Epoch : 7, Step : 3790, Loss : 0.25967, Acc : 0.887, Sensitive_Loss : 0.14472, Sensitive_Acc : 16.900, Run Time : 9.27 sec
INFO:root:2024-04-26 17:17:38, Train, Epoch : 7, Step : 3800, Loss : 0.26435, Acc : 0.884, Sensitive_Loss : 0.14671, Sensitive_Acc : 18.400, Run Time : 23.23 sec
INFO:root:2024-04-26 17:19:32, Dev, Step : 3800, Loss : 0.42268, Acc : 0.824, Auc : 0.908, Sensitive_Loss : 0.14121, Sensitive_Acc : 16.793, Sensitive_Auc : 0.994, Mean auc: 0.908, Run Time : 113.75 sec
INFO:root:2024-04-26 17:19:38, Train, Epoch : 7, Step : 3810, Loss : 0.30351, Acc : 0.872, Sensitive_Loss : 0.11045, Sensitive_Acc : 15.100, Run Time : 120.08 sec
INFO:root:2024-04-26 17:19:47, Train, Epoch : 7, Step : 3820, Loss : 0.31193, Acc : 0.884, Sensitive_Loss : 0.09968, Sensitive_Acc : 16.600, Run Time : 8.77 sec
INFO:root:2024-04-26 17:19:56, Train, Epoch : 7, Step : 3830, Loss : 0.28073, Acc : 0.884, Sensitive_Loss : 0.09466, Sensitive_Acc : 16.200, Run Time : 9.05 sec
INFO:root:2024-04-26 17:20:04, Train, Epoch : 7, Step : 3840, Loss : 0.30055, Acc : 0.863, Sensitive_Loss : 0.12015, Sensitive_Acc : 16.700, Run Time : 8.49 sec
INFO:root:2024-04-26 17:20:14, Train, Epoch : 7, Step : 3850, Loss : 0.24027, Acc : 0.912, Sensitive_Loss : 0.10571, Sensitive_Acc : 16.300, Run Time : 9.69 sec
INFO:root:2024-04-26 17:20:23, Train, Epoch : 7, Step : 3860, Loss : 0.28562, Acc : 0.887, Sensitive_Loss : 0.10975, Sensitive_Acc : 16.600, Run Time : 8.59 sec
INFO:root:2024-04-26 17:20:32, Train, Epoch : 7, Step : 3870, Loss : 0.24608, Acc : 0.884, Sensitive_Loss : 0.11192, Sensitive_Acc : 16.900, Run Time : 9.09 sec
INFO:root:2024-04-26 17:20:40, Train, Epoch : 7, Step : 3880, Loss : 0.24737, Acc : 0.894, Sensitive_Loss : 0.10277, Sensitive_Acc : 15.900, Run Time : 8.62 sec
INFO:root:2024-04-26 17:20:49, Train, Epoch : 7, Step : 3890, Loss : 0.33405, Acc : 0.838, Sensitive_Loss : 0.11207, Sensitive_Acc : 15.200, Run Time : 8.76 sec
INFO:root:2024-04-26 17:20:58, Train, Epoch : 7, Step : 3900, Loss : 0.23838, Acc : 0.897, Sensitive_Loss : 0.13670, Sensitive_Acc : 17.300, Run Time : 8.87 sec
INFO:root:2024-04-26 17:22:33, Dev, Step : 3900, Loss : 0.46226, Acc : 0.814, Auc : 0.903, Sensitive_Loss : 0.15422, Sensitive_Acc : 16.850, Sensitive_Auc : 0.994, Mean auc: 0.903, Run Time : 94.76 sec
INFO:root:2024-04-26 17:22:39, Train, Epoch : 7, Step : 3910, Loss : 0.27771, Acc : 0.897, Sensitive_Loss : 0.10830, Sensitive_Acc : 14.700, Run Time : 101.11 sec
INFO:root:2024-04-26 17:22:48, Train, Epoch : 7, Step : 3920, Loss : 0.28405, Acc : 0.866, Sensitive_Loss : 0.10510, Sensitive_Acc : 16.000, Run Time : 9.27 sec
INFO:root:2024-04-26 17:22:57, Train, Epoch : 7, Step : 3930, Loss : 0.22699, Acc : 0.900, Sensitive_Loss : 0.15078, Sensitive_Acc : 16.000, Run Time : 9.08 sec
INFO:root:2024-04-26 17:23:06, Train, Epoch : 7, Step : 3940, Loss : 0.27906, Acc : 0.872, Sensitive_Loss : 0.09822, Sensitive_Acc : 16.900, Run Time : 8.50 sec
INFO:root:2024-04-26 17:23:15, Train, Epoch : 7, Step : 3950, Loss : 0.26407, Acc : 0.897, Sensitive_Loss : 0.11745, Sensitive_Acc : 14.800, Run Time : 9.13 sec
INFO:root:2024-04-26 17:23:24, Train, Epoch : 7, Step : 3960, Loss : 0.26920, Acc : 0.872, Sensitive_Loss : 0.14426, Sensitive_Acc : 16.300, Run Time : 9.41 sec
INFO:root:2024-04-26 17:23:34, Train, Epoch : 7, Step : 3970, Loss : 0.35435, Acc : 0.859, Sensitive_Loss : 0.13563, Sensitive_Acc : 16.800, Run Time : 9.07 sec
INFO:root:2024-04-26 17:23:42, Train, Epoch : 7, Step : 3980, Loss : 0.30780, Acc : 0.866, Sensitive_Loss : 0.18504, Sensitive_Acc : 16.700, Run Time : 8.80 sec
INFO:root:2024-04-26 17:23:51, Train, Epoch : 7, Step : 3990, Loss : 0.28331, Acc : 0.869, Sensitive_Loss : 0.11556, Sensitive_Acc : 15.700, Run Time : 8.95 sec
INFO:root:2024-04-26 17:24:00, Train, Epoch : 7, Step : 4000, Loss : 0.31241, Acc : 0.872, Sensitive_Loss : 0.12025, Sensitive_Acc : 15.900, Run Time : 9.13 sec
INFO:root:2024-04-26 17:25:35, Dev, Step : 4000, Loss : 0.45996, Acc : 0.811, Auc : 0.902, Sensitive_Loss : 0.13292, Sensitive_Acc : 16.921, Sensitive_Auc : 0.992, Mean auc: 0.902, Run Time : 94.63 sec
INFO:root:2024-04-26 17:25:41, Train, Epoch : 7, Step : 4010, Loss : 0.32655, Acc : 0.866, Sensitive_Loss : 0.09706, Sensitive_Acc : 16.500, Run Time : 100.80 sec
INFO:root:2024-04-26 17:25:50, Train, Epoch : 7, Step : 4020, Loss : 0.25954, Acc : 0.869, Sensitive_Loss : 0.13378, Sensitive_Acc : 16.900, Run Time : 9.20 sec
INFO:root:2024-04-26 17:26:00, Train, Epoch : 7, Step : 4030, Loss : 0.29301, Acc : 0.878, Sensitive_Loss : 0.11376, Sensitive_Acc : 15.000, Run Time : 9.42 sec
INFO:root:2024-04-26 17:26:09, Train, Epoch : 7, Step : 4040, Loss : 0.30520, Acc : 0.881, Sensitive_Loss : 0.11279, Sensitive_Acc : 14.900, Run Time : 8.92 sec
INFO:root:2024-04-26 17:26:18, Train, Epoch : 7, Step : 4050, Loss : 0.25271, Acc : 0.897, Sensitive_Loss : 0.13223, Sensitive_Acc : 17.600, Run Time : 8.84 sec
INFO:root:2024-04-26 17:26:26, Train, Epoch : 7, Step : 4060, Loss : 0.35615, Acc : 0.838, Sensitive_Loss : 0.12739, Sensitive_Acc : 15.700, Run Time : 8.86 sec
INFO:root:2024-04-26 17:26:35, Train, Epoch : 7, Step : 4070, Loss : 0.28010, Acc : 0.891, Sensitive_Loss : 0.11851, Sensitive_Acc : 15.600, Run Time : 9.08 sec
INFO:root:2024-04-26 17:26:44, Train, Epoch : 7, Step : 4080, Loss : 0.29817, Acc : 0.875, Sensitive_Loss : 0.11227, Sensitive_Acc : 16.000, Run Time : 8.13 sec
INFO:root:2024-04-26 17:26:52, Train, Epoch : 7, Step : 4090, Loss : 0.30449, Acc : 0.881, Sensitive_Loss : 0.11155, Sensitive_Acc : 17.700, Run Time : 8.63 sec
INFO:root:2024-04-26 17:27:02, Train, Epoch : 7, Step : 4100, Loss : 0.30399, Acc : 0.866, Sensitive_Loss : 0.11634, Sensitive_Acc : 16.400, Run Time : 9.29 sec
INFO:root:2024-04-26 17:28:36, Dev, Step : 4100, Loss : 0.44400, Acc : 0.816, Auc : 0.904, Sensitive_Loss : 0.13767, Sensitive_Acc : 16.879, Sensitive_Auc : 0.995, Mean auc: 0.904, Run Time : 94.53 sec
INFO:root:2024-04-26 17:28:44, Train, Epoch : 7, Step : 4110, Loss : 0.27905, Acc : 0.875, Sensitive_Loss : 0.13231, Sensitive_Acc : 16.400, Run Time : 102.36 sec
INFO:root:2024-04-26 17:28:53, Train, Epoch : 7, Step : 4120, Loss : 0.26753, Acc : 0.891, Sensitive_Loss : 0.11237, Sensitive_Acc : 16.300, Run Time : 8.72 sec
INFO:root:2024-04-26 17:29:01, Train, Epoch : 7, Step : 4130, Loss : 0.30205, Acc : 0.881, Sensitive_Loss : 0.13306, Sensitive_Acc : 15.800, Run Time : 8.86 sec
INFO:root:2024-04-26 17:29:10, Train, Epoch : 7, Step : 4140, Loss : 0.30607, Acc : 0.869, Sensitive_Loss : 0.12982, Sensitive_Acc : 18.500, Run Time : 8.67 sec
INFO:root:2024-04-26 17:29:19, Train, Epoch : 7, Step : 4150, Loss : 0.27167, Acc : 0.900, Sensitive_Loss : 0.11185, Sensitive_Acc : 17.400, Run Time : 9.05 sec
INFO:root:2024-04-26 17:29:28, Train, Epoch : 7, Step : 4160, Loss : 0.36028, Acc : 0.838, Sensitive_Loss : 0.09459, Sensitive_Acc : 16.200, Run Time : 8.86 sec
INFO:root:2024-04-26 17:29:38, Train, Epoch : 7, Step : 4170, Loss : 0.28484, Acc : 0.878, Sensitive_Loss : 0.09008, Sensitive_Acc : 16.800, Run Time : 9.59 sec
INFO:root:2024-04-26 17:29:46, Train, Epoch : 7, Step : 4180, Loss : 0.23696, Acc : 0.900, Sensitive_Loss : 0.11418, Sensitive_Acc : 16.900, Run Time : 8.73 sec
INFO:root:2024-04-26 17:29:55, Train, Epoch : 7, Step : 4190, Loss : 0.27866, Acc : 0.884, Sensitive_Loss : 0.11931, Sensitive_Acc : 16.800, Run Time : 8.70 sec
INFO:root:2024-04-26 17:30:04, Train, Epoch : 7, Step : 4200, Loss : 0.23966, Acc : 0.900, Sensitive_Loss : 0.14412, Sensitive_Acc : 16.000, Run Time : 9.35 sec
INFO:root:2024-04-26 17:31:38, Dev, Step : 4200, Loss : 0.43679, Acc : 0.818, Auc : 0.907, Sensitive_Loss : 0.12689, Sensitive_Acc : 16.821, Sensitive_Auc : 0.994, Mean auc: 0.907, Run Time : 93.67 sec
INFO:root:2024-04-26 17:31:44, Train, Epoch : 7, Step : 4210, Loss : 0.28353, Acc : 0.887, Sensitive_Loss : 0.10087, Sensitive_Acc : 16.500, Run Time : 99.66 sec
INFO:root:2024-04-26 17:31:54, Train, Epoch : 7, Step : 4220, Loss : 0.25707, Acc : 0.878, Sensitive_Loss : 0.08025, Sensitive_Acc : 17.200, Run Time : 9.72 sec
INFO:root:2024-04-26 17:32:03, Train, Epoch : 7, Step : 4230, Loss : 0.27098, Acc : 0.869, Sensitive_Loss : 0.12158, Sensitive_Acc : 17.000, Run Time : 9.38 sec
INFO:root:2024-04-26 17:32:12, Train, Epoch : 7, Step : 4240, Loss : 0.34321, Acc : 0.853, Sensitive_Loss : 0.10665, Sensitive_Acc : 16.200, Run Time : 9.24 sec
INFO:root:2024-04-26 17:32:22, Train, Epoch : 7, Step : 4250, Loss : 0.26901, Acc : 0.884, Sensitive_Loss : 0.10938, Sensitive_Acc : 16.400, Run Time : 9.23 sec
INFO:root:2024-04-26 17:32:31, Train, Epoch : 7, Step : 4260, Loss : 0.28811, Acc : 0.884, Sensitive_Loss : 0.07808, Sensitive_Acc : 17.000, Run Time : 8.96 sec
INFO:root:2024-04-26 17:32:40, Train, Epoch : 7, Step : 4270, Loss : 0.27082, Acc : 0.875, Sensitive_Loss : 0.11054, Sensitive_Acc : 17.800, Run Time : 9.13 sec
INFO:root:2024-04-26 17:32:49, Train, Epoch : 7, Step : 4280, Loss : 0.27392, Acc : 0.887, Sensitive_Loss : 0.12492, Sensitive_Acc : 17.600, Run Time : 8.82 sec
INFO:root:2024-04-26 17:32:58, Train, Epoch : 7, Step : 4290, Loss : 0.23660, Acc : 0.925, Sensitive_Loss : 0.13780, Sensitive_Acc : 15.400, Run Time : 9.34 sec
INFO:root:2024-04-26 17:33:07, Train, Epoch : 7, Step : 4300, Loss : 0.28330, Acc : 0.872, Sensitive_Loss : 0.08787, Sensitive_Acc : 15.400, Run Time : 9.22 sec
INFO:root:2024-04-26 17:34:41, Dev, Step : 4300, Loss : 0.44468, Acc : 0.820, Auc : 0.908, Sensitive_Loss : 0.12211, Sensitive_Acc : 16.836, Sensitive_Auc : 0.996, Mean auc: 0.908, Run Time : 93.69 sec
INFO:root:2024-04-26 17:34:47, Train, Epoch : 7, Step : 4310, Loss : 0.27470, Acc : 0.872, Sensitive_Loss : 0.07819, Sensitive_Acc : 15.900, Run Time : 100.03 sec
INFO:root:2024-04-26 17:34:57, Train, Epoch : 7, Step : 4320, Loss : 0.29678, Acc : 0.872, Sensitive_Loss : 0.11906, Sensitive_Acc : 15.600, Run Time : 9.94 sec
INFO:root:2024-04-26 17:35:06, Train, Epoch : 7, Step : 4330, Loss : 0.24364, Acc : 0.891, Sensitive_Loss : 0.09788, Sensitive_Acc : 17.600, Run Time : 9.25 sec
INFO:root:2024-04-26 17:35:17, Train, Epoch : 7, Step : 4340, Loss : 0.27857, Acc : 0.894, Sensitive_Loss : 0.10171, Sensitive_Acc : 14.900, Run Time : 10.53 sec
INFO:root:2024-04-26 17:35:28, Train, Epoch : 7, Step : 4350, Loss : 0.28712, Acc : 0.866, Sensitive_Loss : 0.10936, Sensitive_Acc : 15.800, Run Time : 11.57 sec
INFO:root:2024-04-26 17:35:38, Train, Epoch : 7, Step : 4360, Loss : 0.27332, Acc : 0.872, Sensitive_Loss : 0.12082, Sensitive_Acc : 16.100, Run Time : 9.67 sec
INFO:root:2024-04-26 17:35:47, Train, Epoch : 7, Step : 4370, Loss : 0.28192, Acc : 0.869, Sensitive_Loss : 0.12291, Sensitive_Acc : 15.100, Run Time : 9.35 sec
INFO:root:2024-04-26 17:35:57, Train, Epoch : 7, Step : 4380, Loss : 0.26020, Acc : 0.891, Sensitive_Loss : 0.13527, Sensitive_Acc : 14.900, Run Time : 9.53 sec
INFO:root:2024-04-26 17:37:36
INFO:root:y_pred: [2.0179638e-01 9.2201817e-01 7.4443896e-03 ... 8.6427116e-01 3.1127216e-04
 8.9523691e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.3374187e-01 1.0092478e-02 2.0252895e-01 5.4007182e-03 9.9863964e-01
 1.7659914e-03 9.9921942e-01 9.9759310e-01 1.2159639e-02 9.4729823e-01
 9.8952150e-01 9.9954152e-01 9.9777031e-01 9.5267886e-01 6.4421989e-02
 9.8603398e-01 9.9991775e-01 8.0216855e-02 4.3372026e-01 9.8853707e-01
 9.8174244e-01 1.2991452e-02 9.9688417e-01 9.4770503e-01 9.9633670e-01
 9.7961408e-01 3.3570572e-03 9.8799545e-01 9.8670721e-01 6.6192693e-01
 4.5054597e-03 3.7224293e-02 3.3403467e-03 4.4016205e-02 3.9803952e-02
 1.2180757e-02 5.0399311e-02 2.5728157e-02 9.9135220e-01 9.9749643e-01
 1.3538194e-04 8.2219433e-04 9.7839546e-01 5.8129779e-03 9.9948198e-01
 9.9463224e-01 9.9693441e-01 9.6046817e-01 1.3070157e-01 9.9014050e-01
 9.9574810e-01 2.2532415e-02 3.3398229e-01 6.9006868e-03 1.3625933e-03
 1.2908089e-01 2.3763780e-01 1.0519105e-02 2.3536474e-02 1.8646741e-01
 7.9648355e-03 1.4550172e-01 9.4933420e-02 9.7162712e-01 1.4148209e-02
 9.9796069e-01 2.3980332e-03 9.9431968e-01 9.1569000e-01 8.1281185e-02
 9.8010224e-01 8.2207018e-01 2.9680114e-02 6.2620737e-02 1.2803061e-02
 2.8909249e-03 1.7480109e-02 2.9540855e-01 1.3289700e-03 9.9551129e-01
 9.9872833e-01 1.1085712e-02 2.9879850e-01 7.8523029e-03 6.8235892e-01
 9.7487313e-01 3.8304252e-03 1.3292444e-01 8.4702098e-01 9.7851300e-01
 9.9626046e-01 8.5602561e-03 1.0542085e-01 9.9660492e-01 2.2751208e-02
 1.0006484e-03 9.9359471e-01 9.9015325e-01 1.0253579e-03 2.8749555e-04
 9.6784991e-01 9.7754788e-01 9.9448848e-01 9.8991430e-01 1.6087174e-02
 2.2755296e-03 9.3340844e-01 9.9222201e-01 8.8457793e-01 4.2313887e-03
 9.6776599e-01 9.9855739e-01 1.7705421e-01 9.9827123e-01 9.9107403e-01
 9.8866266e-01 7.5425982e-01 9.9848962e-01 1.6050048e-01 6.9479072e-01
 9.9826282e-01 9.9245065e-01 3.0353034e-03 9.0235305e-01 9.9925107e-01
 4.0752703e-01 9.9290621e-01 4.6019059e-02 6.1701268e-01 9.6273518e-01
 9.9778700e-01 2.6371612e-04 7.4309677e-02 4.6775766e-02 9.9360883e-01
 9.9880898e-01 9.5389187e-01 6.6275527e-03 1.0136422e-01 9.8908603e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 17:37:36, Dev, Step : 4382, Loss : 0.49632, Acc : 0.799, Auc : 0.905, Sensitive_Loss : 0.14942, Sensitive_Acc : 16.864, Sensitive_Auc : 0.992, Mean auc: 0.905, Run Time : 97.07 sec
INFO:root:2024-04-26 17:37:45, Train, Epoch : 8, Step : 4390, Loss : 0.21166, Acc : 0.709, Sensitive_Loss : 0.08621, Sensitive_Acc : 13.200, Run Time : 8.06 sec
INFO:root:2024-04-26 17:37:53, Train, Epoch : 8, Step : 4400, Loss : 0.21965, Acc : 0.881, Sensitive_Loss : 0.13030, Sensitive_Acc : 15.300, Run Time : 8.44 sec
INFO:root:2024-04-26 17:39:40, Dev, Step : 4400, Loss : 0.46382, Acc : 0.812, Auc : 0.906, Sensitive_Loss : 0.14073, Sensitive_Acc : 16.864, Sensitive_Auc : 0.992, Mean auc: 0.906, Run Time : 106.26 sec
INFO:root:2024-04-26 17:39:46, Train, Epoch : 8, Step : 4410, Loss : 0.24317, Acc : 0.903, Sensitive_Loss : 0.09529, Sensitive_Acc : 15.100, Run Time : 112.58 sec
INFO:root:2024-04-26 17:39:55, Train, Epoch : 8, Step : 4420, Loss : 0.27138, Acc : 0.863, Sensitive_Loss : 0.11458, Sensitive_Acc : 17.100, Run Time : 8.93 sec
INFO:root:2024-04-26 17:40:03, Train, Epoch : 8, Step : 4430, Loss : 0.25734, Acc : 0.903, Sensitive_Loss : 0.11503, Sensitive_Acc : 16.000, Run Time : 8.50 sec
INFO:root:2024-04-26 17:40:12, Train, Epoch : 8, Step : 4440, Loss : 0.27513, Acc : 0.894, Sensitive_Loss : 0.08609, Sensitive_Acc : 16.100, Run Time : 8.38 sec
INFO:root:2024-04-26 17:40:21, Train, Epoch : 8, Step : 4450, Loss : 0.26754, Acc : 0.887, Sensitive_Loss : 0.09840, Sensitive_Acc : 16.800, Run Time : 8.89 sec
INFO:root:2024-04-26 17:40:29, Train, Epoch : 8, Step : 4460, Loss : 0.27859, Acc : 0.859, Sensitive_Loss : 0.11088, Sensitive_Acc : 16.400, Run Time : 8.68 sec
INFO:root:2024-04-26 17:40:38, Train, Epoch : 8, Step : 4470, Loss : 0.30501, Acc : 0.869, Sensitive_Loss : 0.13633, Sensitive_Acc : 18.200, Run Time : 8.58 sec
INFO:root:2024-04-26 17:40:47, Train, Epoch : 8, Step : 4480, Loss : 0.29382, Acc : 0.859, Sensitive_Loss : 0.12930, Sensitive_Acc : 17.800, Run Time : 8.84 sec
INFO:root:2024-04-26 17:40:55, Train, Epoch : 8, Step : 4490, Loss : 0.23314, Acc : 0.897, Sensitive_Loss : 0.11233, Sensitive_Acc : 17.700, Run Time : 8.61 sec
INFO:root:2024-04-26 17:41:05, Train, Epoch : 8, Step : 4500, Loss : 0.29755, Acc : 0.869, Sensitive_Loss : 0.13904, Sensitive_Acc : 16.600, Run Time : 9.55 sec
INFO:root:2024-04-26 17:42:40, Dev, Step : 4500, Loss : 0.43335, Acc : 0.825, Auc : 0.904, Sensitive_Loss : 0.14342, Sensitive_Acc : 16.864, Sensitive_Auc : 0.992, Mean auc: 0.904, Run Time : 94.77 sec
INFO:root:2024-04-26 17:42:46, Train, Epoch : 8, Step : 4510, Loss : 0.31233, Acc : 0.869, Sensitive_Loss : 0.07618, Sensitive_Acc : 16.000, Run Time : 101.02 sec
INFO:root:2024-04-26 17:42:55, Train, Epoch : 8, Step : 4520, Loss : 0.22282, Acc : 0.916, Sensitive_Loss : 0.08746, Sensitive_Acc : 16.500, Run Time : 9.04 sec
INFO:root:2024-04-26 17:43:04, Train, Epoch : 8, Step : 4530, Loss : 0.25952, Acc : 0.894, Sensitive_Loss : 0.10924, Sensitive_Acc : 17.100, Run Time : 9.10 sec
INFO:root:2024-04-26 17:43:13, Train, Epoch : 8, Step : 4540, Loss : 0.26491, Acc : 0.881, Sensitive_Loss : 0.13635, Sensitive_Acc : 15.400, Run Time : 8.93 sec
INFO:root:2024-04-26 17:43:22, Train, Epoch : 8, Step : 4550, Loss : 0.26498, Acc : 0.875, Sensitive_Loss : 0.11703, Sensitive_Acc : 17.900, Run Time : 9.28 sec
INFO:root:2024-04-26 17:43:32, Train, Epoch : 8, Step : 4560, Loss : 0.21903, Acc : 0.938, Sensitive_Loss : 0.10012, Sensitive_Acc : 16.900, Run Time : 9.23 sec
INFO:root:2024-04-26 17:43:41, Train, Epoch : 8, Step : 4570, Loss : 0.29490, Acc : 0.900, Sensitive_Loss : 0.11270, Sensitive_Acc : 17.200, Run Time : 8.97 sec
INFO:root:2024-04-26 17:43:50, Train, Epoch : 8, Step : 4580, Loss : 0.24731, Acc : 0.887, Sensitive_Loss : 0.11216, Sensitive_Acc : 17.900, Run Time : 9.59 sec
INFO:root:2024-04-26 17:44:00, Train, Epoch : 8, Step : 4590, Loss : 0.27886, Acc : 0.891, Sensitive_Loss : 0.12616, Sensitive_Acc : 15.700, Run Time : 9.57 sec
INFO:root:2024-04-26 17:44:09, Train, Epoch : 8, Step : 4600, Loss : 0.32989, Acc : 0.878, Sensitive_Loss : 0.13474, Sensitive_Acc : 16.700, Run Time : 8.98 sec
INFO:root:2024-04-26 17:45:43, Dev, Step : 4600, Loss : 0.42335, Acc : 0.824, Auc : 0.901, Sensitive_Loss : 0.13382, Sensitive_Acc : 16.864, Sensitive_Auc : 0.993, Mean auc: 0.901, Run Time : 94.27 sec
INFO:root:2024-04-26 17:45:50, Train, Epoch : 8, Step : 4610, Loss : 0.29578, Acc : 0.856, Sensitive_Loss : 0.12856, Sensitive_Acc : 15.800, Run Time : 100.78 sec
INFO:root:2024-04-26 17:45:59, Train, Epoch : 8, Step : 4620, Loss : 0.22566, Acc : 0.884, Sensitive_Loss : 0.12605, Sensitive_Acc : 15.800, Run Time : 9.42 sec
INFO:root:2024-04-26 17:46:08, Train, Epoch : 8, Step : 4630, Loss : 0.25278, Acc : 0.906, Sensitive_Loss : 0.10249, Sensitive_Acc : 15.900, Run Time : 9.22 sec
INFO:root:2024-04-26 17:46:17, Train, Epoch : 8, Step : 4640, Loss : 0.31694, Acc : 0.875, Sensitive_Loss : 0.09605, Sensitive_Acc : 16.100, Run Time : 8.91 sec
INFO:root:2024-04-26 17:46:27, Train, Epoch : 8, Step : 4650, Loss : 0.34865, Acc : 0.863, Sensitive_Loss : 0.12712, Sensitive_Acc : 16.700, Run Time : 9.59 sec
INFO:root:2024-04-26 17:46:36, Train, Epoch : 8, Step : 4660, Loss : 0.22210, Acc : 0.909, Sensitive_Loss : 0.11001, Sensitive_Acc : 16.200, Run Time : 8.92 sec
INFO:root:2024-04-26 17:46:44, Train, Epoch : 8, Step : 4670, Loss : 0.25629, Acc : 0.881, Sensitive_Loss : 0.10561, Sensitive_Acc : 17.900, Run Time : 8.73 sec
INFO:root:2024-04-26 17:46:53, Train, Epoch : 8, Step : 4680, Loss : 0.23934, Acc : 0.909, Sensitive_Loss : 0.10328, Sensitive_Acc : 16.600, Run Time : 9.13 sec
INFO:root:2024-04-26 17:47:02, Train, Epoch : 8, Step : 4690, Loss : 0.26368, Acc : 0.903, Sensitive_Loss : 0.09277, Sensitive_Acc : 16.800, Run Time : 8.92 sec
INFO:root:2024-04-26 17:47:12, Train, Epoch : 8, Step : 4700, Loss : 0.25920, Acc : 0.900, Sensitive_Loss : 0.11814, Sensitive_Acc : 18.000, Run Time : 9.91 sec
INFO:root:2024-04-26 17:48:47, Dev, Step : 4700, Loss : 0.45042, Acc : 0.818, Auc : 0.903, Sensitive_Loss : 0.14248, Sensitive_Acc : 16.850, Sensitive_Auc : 0.994, Mean auc: 0.903, Run Time : 94.42 sec
INFO:root:2024-04-26 17:48:53, Train, Epoch : 8, Step : 4710, Loss : 0.23489, Acc : 0.900, Sensitive_Loss : 0.09215, Sensitive_Acc : 15.900, Run Time : 101.14 sec
INFO:root:2024-04-26 17:49:03, Train, Epoch : 8, Step : 4720, Loss : 0.24531, Acc : 0.912, Sensitive_Loss : 0.09241, Sensitive_Acc : 15.800, Run Time : 9.42 sec
INFO:root:2024-04-26 17:49:12, Train, Epoch : 8, Step : 4730, Loss : 0.23697, Acc : 0.869, Sensitive_Loss : 0.11407, Sensitive_Acc : 16.300, Run Time : 9.20 sec
INFO:root:2024-04-26 17:49:22, Train, Epoch : 8, Step : 4740, Loss : 0.20625, Acc : 0.916, Sensitive_Loss : 0.07521, Sensitive_Acc : 15.700, Run Time : 9.47 sec
INFO:root:2024-04-26 17:49:31, Train, Epoch : 8, Step : 4750, Loss : 0.23694, Acc : 0.894, Sensitive_Loss : 0.13424, Sensitive_Acc : 15.400, Run Time : 9.15 sec
INFO:root:2024-04-26 17:49:40, Train, Epoch : 8, Step : 4760, Loss : 0.22685, Acc : 0.919, Sensitive_Loss : 0.09798, Sensitive_Acc : 15.800, Run Time : 9.31 sec
INFO:root:2024-04-26 17:49:49, Train, Epoch : 8, Step : 4770, Loss : 0.27315, Acc : 0.894, Sensitive_Loss : 0.09226, Sensitive_Acc : 15.700, Run Time : 9.06 sec
INFO:root:2024-04-26 17:49:58, Train, Epoch : 8, Step : 4780, Loss : 0.23327, Acc : 0.897, Sensitive_Loss : 0.11798, Sensitive_Acc : 16.700, Run Time : 9.30 sec
INFO:root:2024-04-26 17:50:08, Train, Epoch : 8, Step : 4790, Loss : 0.31035, Acc : 0.856, Sensitive_Loss : 0.12855, Sensitive_Acc : 17.600, Run Time : 9.36 sec
INFO:root:2024-04-26 17:50:17, Train, Epoch : 8, Step : 4800, Loss : 0.25047, Acc : 0.881, Sensitive_Loss : 0.10529, Sensitive_Acc : 16.000, Run Time : 9.07 sec
INFO:root:2024-04-26 17:52:07, Dev, Step : 4800, Loss : 0.44973, Acc : 0.820, Auc : 0.901, Sensitive_Loss : 0.13964, Sensitive_Acc : 16.864, Sensitive_Auc : 0.995, Mean auc: 0.901, Run Time : 109.98 sec
INFO:root:2024-04-26 17:52:13, Train, Epoch : 8, Step : 4810, Loss : 0.30549, Acc : 0.872, Sensitive_Loss : 0.10244, Sensitive_Acc : 17.700, Run Time : 116.46 sec
INFO:root:2024-04-26 17:52:22, Train, Epoch : 8, Step : 4820, Loss : 0.29468, Acc : 0.887, Sensitive_Loss : 0.16971, Sensitive_Acc : 14.900, Run Time : 9.24 sec
INFO:root:2024-04-26 17:52:32, Train, Epoch : 8, Step : 4830, Loss : 0.19503, Acc : 0.922, Sensitive_Loss : 0.13478, Sensitive_Acc : 16.400, Run Time : 9.89 sec
INFO:root:2024-04-26 17:52:42, Train, Epoch : 8, Step : 4840, Loss : 0.26831, Acc : 0.900, Sensitive_Loss : 0.09850, Sensitive_Acc : 16.000, Run Time : 9.76 sec
INFO:root:2024-04-26 17:52:52, Train, Epoch : 8, Step : 4850, Loss : 0.28369, Acc : 0.853, Sensitive_Loss : 0.10677, Sensitive_Acc : 16.500, Run Time : 9.40 sec
INFO:root:2024-04-26 17:53:02, Train, Epoch : 8, Step : 4860, Loss : 0.24593, Acc : 0.903, Sensitive_Loss : 0.09444, Sensitive_Acc : 16.500, Run Time : 10.59 sec
INFO:root:2024-04-26 17:53:11, Train, Epoch : 8, Step : 4870, Loss : 0.30502, Acc : 0.872, Sensitive_Loss : 0.13528, Sensitive_Acc : 16.700, Run Time : 9.10 sec
INFO:root:2024-04-26 17:53:20, Train, Epoch : 8, Step : 4880, Loss : 0.25341, Acc : 0.894, Sensitive_Loss : 0.15466, Sensitive_Acc : 16.200, Run Time : 8.67 sec
INFO:root:2024-04-26 17:53:31, Train, Epoch : 8, Step : 4890, Loss : 0.23819, Acc : 0.897, Sensitive_Loss : 0.10076, Sensitive_Acc : 16.200, Run Time : 10.98 sec
INFO:root:2024-04-26 17:53:40, Train, Epoch : 8, Step : 4900, Loss : 0.21779, Acc : 0.916, Sensitive_Loss : 0.13990, Sensitive_Acc : 16.800, Run Time : 9.40 sec
INFO:root:2024-04-26 17:56:13, Dev, Step : 4900, Loss : 0.42313, Acc : 0.827, Auc : 0.906, Sensitive_Loss : 0.12697, Sensitive_Acc : 16.936, Sensitive_Auc : 0.996, Mean auc: 0.906, Run Time : 152.62 sec
INFO:root:2024-04-26 17:56:20, Train, Epoch : 8, Step : 4910, Loss : 0.29923, Acc : 0.891, Sensitive_Loss : 0.10033, Sensitive_Acc : 17.300, Run Time : 159.37 sec
INFO:root:2024-04-26 17:56:29, Train, Epoch : 8, Step : 4920, Loss : 0.28975, Acc : 0.878, Sensitive_Loss : 0.12024, Sensitive_Acc : 18.200, Run Time : 9.45 sec
INFO:root:2024-04-26 17:56:38, Train, Epoch : 8, Step : 4930, Loss : 0.26949, Acc : 0.884, Sensitive_Loss : 0.10583, Sensitive_Acc : 16.100, Run Time : 9.21 sec
INFO:root:2024-04-26 17:56:48, Train, Epoch : 8, Step : 4940, Loss : 0.26122, Acc : 0.891, Sensitive_Loss : 0.09289, Sensitive_Acc : 15.600, Run Time : 9.32 sec
INFO:root:2024-04-26 17:56:57, Train, Epoch : 8, Step : 4950, Loss : 0.28712, Acc : 0.863, Sensitive_Loss : 0.10186, Sensitive_Acc : 16.500, Run Time : 9.25 sec
INFO:root:2024-04-26 17:57:06, Train, Epoch : 8, Step : 4960, Loss : 0.24404, Acc : 0.906, Sensitive_Loss : 0.12153, Sensitive_Acc : 16.400, Run Time : 9.01 sec
INFO:root:2024-04-26 17:57:16, Train, Epoch : 8, Step : 4970, Loss : 0.28195, Acc : 0.847, Sensitive_Loss : 0.09958, Sensitive_Acc : 16.100, Run Time : 9.70 sec
INFO:root:2024-04-26 17:57:25, Train, Epoch : 8, Step : 4980, Loss : 0.21974, Acc : 0.900, Sensitive_Loss : 0.12632, Sensitive_Acc : 14.000, Run Time : 9.85 sec
INFO:root:2024-04-26 17:57:35, Train, Epoch : 8, Step : 4990, Loss : 0.25715, Acc : 0.897, Sensitive_Loss : 0.10354, Sensitive_Acc : 16.600, Run Time : 9.29 sec
INFO:root:2024-04-26 17:57:44, Train, Epoch : 8, Step : 5000, Loss : 0.26756, Acc : 0.869, Sensitive_Loss : 0.10554, Sensitive_Acc : 17.200, Run Time : 9.27 sec
INFO:root:2024-04-26 17:59:21, Dev, Step : 5000, Loss : 0.43783, Acc : 0.822, Auc : 0.907, Sensitive_Loss : 0.12971, Sensitive_Acc : 16.936, Sensitive_Auc : 0.996, Mean auc: 0.907, Run Time : 96.68 sec
INFO:root:2024-04-26 18:00:59
INFO:root:y_pred: [4.9649537e-01 9.8595017e-01 1.5896631e-02 ... 9.2009819e-01 3.3274444e-04
 9.5491105e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [8.5163081e-01 6.4186775e-03 1.7848301e-01 5.7776328e-03 9.9662387e-01
 1.2002919e-03 9.9896181e-01 9.9478722e-01 7.4379770e-03 9.4177431e-01
 9.9001861e-01 9.9935907e-01 9.9555993e-01 8.6706883e-01 1.7631071e-02
 9.6260875e-01 9.9989522e-01 5.0970383e-02 3.5251445e-01 9.7051293e-01
 9.6865284e-01 6.9923524e-04 9.8251420e-01 9.2192507e-01 9.9559069e-01
 9.6687651e-01 3.3074624e-03 9.7609633e-01 9.8225647e-01 3.6543682e-01
 2.9494744e-03 2.3165502e-02 1.1367855e-03 1.4373774e-02 2.6899876e-02
 2.9040731e-02 3.5722628e-02 1.3536796e-02 9.9005800e-01 9.9375027e-01
 5.3806889e-05 4.2591887e-04 9.8011017e-01 1.0358498e-02 9.9903297e-01
 9.9408478e-01 9.8912072e-01 9.6462047e-01 1.9386321e-01 9.6082318e-01
 9.9407721e-01 7.7933460e-03 1.9021654e-01 5.7716835e-03 1.0779415e-03
 6.4443089e-02 2.4591208e-01 6.2161149e-03 1.5557672e-02 5.4038007e-02
 5.9199329e-03 4.7810778e-02 3.4929171e-02 9.2915875e-01 1.4594376e-02
 9.9638981e-01 1.9604594e-03 9.8480844e-01 8.8909888e-01 3.6036909e-02
 8.8090557e-01 7.4898881e-01 2.2615431e-02 3.4681424e-02 2.0380007e-02
 2.2780283e-03 3.5909878e-03 2.1551472e-01 8.4750436e-04 9.9018586e-01
 9.9648488e-01 1.3668588e-02 2.7109930e-01 8.5147442e-03 6.2158573e-01
 9.2402077e-01 3.4752751e-03 4.3256469e-02 8.2127053e-01 9.7265851e-01
 9.9324459e-01 5.8542690e-03 5.9491731e-02 9.9487114e-01 1.7129047e-02
 5.7323568e-04 9.8508203e-01 9.7402722e-01 5.8517174e-04 5.7035353e-04
 9.4660008e-01 9.5680881e-01 9.9212486e-01 9.7660124e-01 1.1377430e-02
 3.4189897e-04 9.3542802e-01 9.8244345e-01 8.4288812e-01 3.7562023e-03
 9.3976253e-01 9.9673468e-01 6.5391690e-02 9.9688476e-01 9.8171014e-01
 9.7745252e-01 7.7718627e-01 9.9749583e-01 1.0284472e-01 3.0795136e-01
 9.9726617e-01 9.8811871e-01 1.0555263e-03 8.5259211e-01 9.9796909e-01
 2.3472691e-01 9.8631185e-01 1.9169630e-02 5.1162410e-01 9.4269168e-01
 9.9686193e-01 4.0762150e-04 3.0635122e-02 2.3836065e-02 9.8420542e-01
 9.9784279e-01 9.2267871e-01 3.5987096e-03 2.6838802e-02 9.5984703e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 18:00:59, Dev, Step : 5008, Loss : 0.42984, Acc : 0.826, Auc : 0.907, Sensitive_Loss : 0.12742, Sensitive_Acc : 16.936, Sensitive_Auc : 0.996, Mean auc: 0.907, Run Time : 93.72 sec
INFO:root:2024-04-26 18:01:02, Train, Epoch : 9, Step : 5010, Loss : 0.03092, Acc : 0.188, Sensitive_Loss : 0.02597, Sensitive_Acc : 3.300, Run Time : 2.55 sec
INFO:root:2024-04-26 18:01:10, Train, Epoch : 9, Step : 5020, Loss : 0.23855, Acc : 0.909, Sensitive_Loss : 0.10520, Sensitive_Acc : 16.600, Run Time : 7.94 sec
INFO:root:2024-04-26 18:01:19, Train, Epoch : 9, Step : 5030, Loss : 0.25985, Acc : 0.878, Sensitive_Loss : 0.11447, Sensitive_Acc : 15.900, Run Time : 8.81 sec
INFO:root:2024-04-26 18:01:27, Train, Epoch : 9, Step : 5040, Loss : 0.26465, Acc : 0.891, Sensitive_Loss : 0.11070, Sensitive_Acc : 16.100, Run Time : 7.79 sec
INFO:root:2024-04-26 18:01:35, Train, Epoch : 9, Step : 5050, Loss : 0.24736, Acc : 0.894, Sensitive_Loss : 0.09066, Sensitive_Acc : 15.400, Run Time : 8.45 sec
INFO:root:2024-04-26 18:01:44, Train, Epoch : 9, Step : 5060, Loss : 0.24719, Acc : 0.897, Sensitive_Loss : 0.09844, Sensitive_Acc : 16.000, Run Time : 8.37 sec
INFO:root:2024-04-26 18:01:52, Train, Epoch : 9, Step : 5070, Loss : 0.20619, Acc : 0.906, Sensitive_Loss : 0.13552, Sensitive_Acc : 17.000, Run Time : 8.77 sec
INFO:root:2024-04-26 18:02:01, Train, Epoch : 9, Step : 5080, Loss : 0.22375, Acc : 0.906, Sensitive_Loss : 0.09302, Sensitive_Acc : 16.600, Run Time : 8.35 sec
INFO:root:2024-04-26 18:02:09, Train, Epoch : 9, Step : 5090, Loss : 0.23442, Acc : 0.900, Sensitive_Loss : 0.09411, Sensitive_Acc : 16.500, Run Time : 7.97 sec
INFO:root:2024-04-26 18:02:17, Train, Epoch : 9, Step : 5100, Loss : 0.20530, Acc : 0.909, Sensitive_Loss : 0.10694, Sensitive_Acc : 15.800, Run Time : 7.93 sec
INFO:root:2024-04-26 18:03:51, Dev, Step : 5100, Loss : 0.43248, Acc : 0.823, Auc : 0.905, Sensitive_Loss : 0.12514, Sensitive_Acc : 16.821, Sensitive_Auc : 0.995, Mean auc: 0.905, Run Time : 94.77 sec
INFO:root:2024-04-26 18:03:58, Train, Epoch : 9, Step : 5110, Loss : 0.22223, Acc : 0.912, Sensitive_Loss : 0.09953, Sensitive_Acc : 15.700, Run Time : 101.48 sec
INFO:root:2024-04-26 18:04:07, Train, Epoch : 9, Step : 5120, Loss : 0.23651, Acc : 0.906, Sensitive_Loss : 0.11706, Sensitive_Acc : 14.600, Run Time : 8.57 sec
INFO:root:2024-04-26 18:04:15, Train, Epoch : 9, Step : 5130, Loss : 0.19710, Acc : 0.919, Sensitive_Loss : 0.13746, Sensitive_Acc : 17.300, Run Time : 8.37 sec
INFO:root:2024-04-26 18:04:24, Train, Epoch : 9, Step : 5140, Loss : 0.23089, Acc : 0.900, Sensitive_Loss : 0.10483, Sensitive_Acc : 16.800, Run Time : 8.82 sec
INFO:root:2024-04-26 18:04:32, Train, Epoch : 9, Step : 5150, Loss : 0.20480, Acc : 0.906, Sensitive_Loss : 0.07900, Sensitive_Acc : 15.600, Run Time : 8.60 sec
INFO:root:2024-04-26 18:04:41, Train, Epoch : 9, Step : 5160, Loss : 0.25340, Acc : 0.891, Sensitive_Loss : 0.11428, Sensitive_Acc : 16.700, Run Time : 8.80 sec
INFO:root:2024-04-26 18:04:50, Train, Epoch : 9, Step : 5170, Loss : 0.24747, Acc : 0.906, Sensitive_Loss : 0.09063, Sensitive_Acc : 16.600, Run Time : 8.44 sec
INFO:root:2024-04-26 18:04:58, Train, Epoch : 9, Step : 5180, Loss : 0.25353, Acc : 0.919, Sensitive_Loss : 0.09295, Sensitive_Acc : 16.900, Run Time : 8.65 sec
INFO:root:2024-04-26 18:05:07, Train, Epoch : 9, Step : 5190, Loss : 0.31325, Acc : 0.850, Sensitive_Loss : 0.13740, Sensitive_Acc : 17.900, Run Time : 8.69 sec
INFO:root:2024-04-26 18:05:15, Train, Epoch : 9, Step : 5200, Loss : 0.20928, Acc : 0.919, Sensitive_Loss : 0.10161, Sensitive_Acc : 15.200, Run Time : 8.41 sec
INFO:root:2024-04-26 18:06:50, Dev, Step : 5200, Loss : 0.45887, Acc : 0.815, Auc : 0.902, Sensitive_Loss : 0.13895, Sensitive_Acc : 16.893, Sensitive_Auc : 0.997, Mean auc: 0.902, Run Time : 94.75 sec
INFO:root:2024-04-26 18:06:56, Train, Epoch : 9, Step : 5210, Loss : 0.33315, Acc : 0.853, Sensitive_Loss : 0.11175, Sensitive_Acc : 15.800, Run Time : 100.72 sec
INFO:root:2024-04-26 18:07:05, Train, Epoch : 9, Step : 5220, Loss : 0.27347, Acc : 0.906, Sensitive_Loss : 0.12084, Sensitive_Acc : 15.300, Run Time : 8.91 sec
INFO:root:2024-04-26 18:07:14, Train, Epoch : 9, Step : 5230, Loss : 0.24360, Acc : 0.912, Sensitive_Loss : 0.11054, Sensitive_Acc : 15.900, Run Time : 8.66 sec
INFO:root:2024-04-26 18:07:23, Train, Epoch : 9, Step : 5240, Loss : 0.24305, Acc : 0.912, Sensitive_Loss : 0.10063, Sensitive_Acc : 15.500, Run Time : 9.00 sec
INFO:root:2024-04-26 18:07:31, Train, Epoch : 9, Step : 5250, Loss : 0.24172, Acc : 0.903, Sensitive_Loss : 0.13409, Sensitive_Acc : 16.900, Run Time : 8.55 sec
INFO:root:2024-04-26 18:07:40, Train, Epoch : 9, Step : 5260, Loss : 0.23167, Acc : 0.900, Sensitive_Loss : 0.10210, Sensitive_Acc : 16.100, Run Time : 8.70 sec
INFO:root:2024-04-26 18:07:49, Train, Epoch : 9, Step : 5270, Loss : 0.24080, Acc : 0.925, Sensitive_Loss : 0.07975, Sensitive_Acc : 16.700, Run Time : 8.84 sec
INFO:root:2024-04-26 18:07:58, Train, Epoch : 9, Step : 5280, Loss : 0.21358, Acc : 0.922, Sensitive_Loss : 0.12100, Sensitive_Acc : 16.900, Run Time : 9.02 sec
INFO:root:2024-04-26 18:08:06, Train, Epoch : 9, Step : 5290, Loss : 0.23786, Acc : 0.909, Sensitive_Loss : 0.09354, Sensitive_Acc : 17.400, Run Time : 8.23 sec
INFO:root:2024-04-26 18:08:15, Train, Epoch : 9, Step : 5300, Loss : 0.25193, Acc : 0.919, Sensitive_Loss : 0.09660, Sensitive_Acc : 18.000, Run Time : 8.64 sec
INFO:root:2024-04-26 18:09:50, Dev, Step : 5300, Loss : 0.48306, Acc : 0.810, Auc : 0.901, Sensitive_Loss : 0.15883, Sensitive_Acc : 16.793, Sensitive_Auc : 0.995, Mean auc: 0.901, Run Time : 94.86 sec
INFO:root:2024-04-26 18:09:56, Train, Epoch : 9, Step : 5310, Loss : 0.23055, Acc : 0.916, Sensitive_Loss : 0.06964, Sensitive_Acc : 15.600, Run Time : 100.98 sec
INFO:root:2024-04-26 18:10:05, Train, Epoch : 9, Step : 5320, Loss : 0.24663, Acc : 0.900, Sensitive_Loss : 0.10424, Sensitive_Acc : 15.700, Run Time : 8.80 sec
INFO:root:2024-04-26 18:10:13, Train, Epoch : 9, Step : 5330, Loss : 0.22975, Acc : 0.881, Sensitive_Loss : 0.10621, Sensitive_Acc : 15.300, Run Time : 8.52 sec
INFO:root:2024-04-26 18:10:22, Train, Epoch : 9, Step : 5340, Loss : 0.27186, Acc : 0.881, Sensitive_Loss : 0.09216, Sensitive_Acc : 16.400, Run Time : 8.56 sec
INFO:root:2024-04-26 18:10:30, Train, Epoch : 9, Step : 5350, Loss : 0.28823, Acc : 0.897, Sensitive_Loss : 0.10687, Sensitive_Acc : 16.300, Run Time : 8.52 sec
INFO:root:2024-04-26 18:10:39, Train, Epoch : 9, Step : 5360, Loss : 0.27765, Acc : 0.875, Sensitive_Loss : 0.10653, Sensitive_Acc : 16.500, Run Time : 8.52 sec
INFO:root:2024-04-26 18:10:48, Train, Epoch : 9, Step : 5370, Loss : 0.27451, Acc : 0.881, Sensitive_Loss : 0.12462, Sensitive_Acc : 18.100, Run Time : 8.89 sec
INFO:root:2024-04-26 18:10:56, Train, Epoch : 9, Step : 5380, Loss : 0.23389, Acc : 0.903, Sensitive_Loss : 0.10664, Sensitive_Acc : 15.400, Run Time : 8.69 sec
INFO:root:2024-04-26 18:11:05, Train, Epoch : 9, Step : 5390, Loss : 0.22997, Acc : 0.894, Sensitive_Loss : 0.08778, Sensitive_Acc : 18.200, Run Time : 8.43 sec
INFO:root:2024-04-26 18:11:13, Train, Epoch : 9, Step : 5400, Loss : 0.25015, Acc : 0.894, Sensitive_Loss : 0.10067, Sensitive_Acc : 16.000, Run Time : 8.51 sec
INFO:root:2024-04-26 18:12:48, Dev, Step : 5400, Loss : 0.45946, Acc : 0.814, Auc : 0.897, Sensitive_Loss : 0.14353, Sensitive_Acc : 16.921, Sensitive_Auc : 0.994, Mean auc: 0.897, Run Time : 94.67 sec
INFO:root:2024-04-26 18:12:54, Train, Epoch : 9, Step : 5410, Loss : 0.26062, Acc : 0.906, Sensitive_Loss : 0.11983, Sensitive_Acc : 16.200, Run Time : 100.75 sec
INFO:root:2024-04-26 18:13:03, Train, Epoch : 9, Step : 5420, Loss : 0.20044, Acc : 0.887, Sensitive_Loss : 0.15281, Sensitive_Acc : 17.100, Run Time : 9.28 sec
INFO:root:2024-04-26 18:13:11, Train, Epoch : 9, Step : 5430, Loss : 0.21623, Acc : 0.903, Sensitive_Loss : 0.12143, Sensitive_Acc : 16.600, Run Time : 8.20 sec
INFO:root:2024-04-26 18:13:20, Train, Epoch : 9, Step : 5440, Loss : 0.21927, Acc : 0.912, Sensitive_Loss : 0.11800, Sensitive_Acc : 16.900, Run Time : 8.89 sec
INFO:root:2024-04-26 18:13:29, Train, Epoch : 9, Step : 5450, Loss : 0.25225, Acc : 0.891, Sensitive_Loss : 0.09324, Sensitive_Acc : 16.300, Run Time : 8.63 sec
INFO:root:2024-04-26 18:13:38, Train, Epoch : 9, Step : 5460, Loss : 0.20617, Acc : 0.909, Sensitive_Loss : 0.10705, Sensitive_Acc : 15.900, Run Time : 8.78 sec
INFO:root:2024-04-26 18:13:46, Train, Epoch : 9, Step : 5470, Loss : 0.22446, Acc : 0.909, Sensitive_Loss : 0.11275, Sensitive_Acc : 16.500, Run Time : 8.61 sec
INFO:root:2024-04-26 18:13:55, Train, Epoch : 9, Step : 5480, Loss : 0.36301, Acc : 0.838, Sensitive_Loss : 0.10431, Sensitive_Acc : 17.300, Run Time : 8.95 sec
INFO:root:2024-04-26 18:14:04, Train, Epoch : 9, Step : 5490, Loss : 0.20846, Acc : 0.922, Sensitive_Loss : 0.08505, Sensitive_Acc : 14.700, Run Time : 9.18 sec
INFO:root:2024-04-26 18:14:13, Train, Epoch : 9, Step : 5500, Loss : 0.24885, Acc : 0.878, Sensitive_Loss : 0.12179, Sensitive_Acc : 17.400, Run Time : 8.48 sec
INFO:root:2024-04-26 18:15:47, Dev, Step : 5500, Loss : 0.45299, Acc : 0.820, Auc : 0.902, Sensitive_Loss : 0.12773, Sensitive_Acc : 16.864, Sensitive_Auc : 0.997, Mean auc: 0.902, Run Time : 94.38 sec
INFO:root:2024-04-26 18:15:53, Train, Epoch : 9, Step : 5510, Loss : 0.30740, Acc : 0.878, Sensitive_Loss : 0.09194, Sensitive_Acc : 16.600, Run Time : 100.58 sec
INFO:root:2024-04-26 18:16:02, Train, Epoch : 9, Step : 5520, Loss : 0.27975, Acc : 0.891, Sensitive_Loss : 0.10782, Sensitive_Acc : 15.100, Run Time : 8.73 sec
INFO:root:2024-04-26 18:16:11, Train, Epoch : 9, Step : 5530, Loss : 0.19391, Acc : 0.909, Sensitive_Loss : 0.11168, Sensitive_Acc : 16.500, Run Time : 8.50 sec
INFO:root:2024-04-26 18:16:20, Train, Epoch : 9, Step : 5540, Loss : 0.21145, Acc : 0.897, Sensitive_Loss : 0.09971, Sensitive_Acc : 15.500, Run Time : 9.15 sec
INFO:root:2024-04-26 18:16:28, Train, Epoch : 9, Step : 5550, Loss : 0.24317, Acc : 0.887, Sensitive_Loss : 0.09545, Sensitive_Acc : 15.400, Run Time : 8.61 sec
INFO:root:2024-04-26 18:16:38, Train, Epoch : 9, Step : 5560, Loss : 0.30861, Acc : 0.869, Sensitive_Loss : 0.13393, Sensitive_Acc : 15.600, Run Time : 9.18 sec
INFO:root:2024-04-26 18:16:48, Train, Epoch : 9, Step : 5570, Loss : 0.24587, Acc : 0.887, Sensitive_Loss : 0.15320, Sensitive_Acc : 16.700, Run Time : 9.99 sec
INFO:root:2024-04-26 18:17:03, Train, Epoch : 9, Step : 5580, Loss : 0.28654, Acc : 0.900, Sensitive_Loss : 0.09819, Sensitive_Acc : 16.500, Run Time : 14.93 sec
INFO:root:2024-04-26 18:17:22, Train, Epoch : 9, Step : 5590, Loss : 0.30785, Acc : 0.853, Sensitive_Loss : 0.12467, Sensitive_Acc : 16.600, Run Time : 19.54 sec
INFO:root:2024-04-26 18:17:34, Train, Epoch : 9, Step : 5600, Loss : 0.31012, Acc : 0.875, Sensitive_Loss : 0.12585, Sensitive_Acc : 17.400, Run Time : 11.62 sec
INFO:root:2024-04-26 18:19:45, Dev, Step : 5600, Loss : 0.44013, Acc : 0.821, Auc : 0.901, Sensitive_Loss : 0.12813, Sensitive_Acc : 16.821, Sensitive_Auc : 0.997, Mean auc: 0.901, Run Time : 131.15 sec
INFO:root:2024-04-26 18:19:51, Train, Epoch : 9, Step : 5610, Loss : 0.24348, Acc : 0.903, Sensitive_Loss : 0.10867, Sensitive_Acc : 16.800, Run Time : 137.49 sec
INFO:root:2024-04-26 18:20:00, Train, Epoch : 9, Step : 5620, Loss : 0.25764, Acc : 0.887, Sensitive_Loss : 0.09601, Sensitive_Acc : 16.800, Run Time : 8.78 sec
INFO:root:2024-04-26 18:20:09, Train, Epoch : 9, Step : 5630, Loss : 0.22473, Acc : 0.906, Sensitive_Loss : 0.11752, Sensitive_Acc : 16.300, Run Time : 8.61 sec
INFO:root:2024-04-26 18:21:57
INFO:root:y_pred: [5.8032608e-01 9.7070718e-01 2.9437725e-02 ... 8.7193459e-01 7.5470330e-04
 9.6556413e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [8.3838940e-01 1.7780631e-03 9.3155526e-02 3.2461656e-03 9.9748093e-01
 1.1604351e-03 9.9819165e-01 9.9525344e-01 6.2044458e-03 9.4253206e-01
 9.7979593e-01 9.9908316e-01 9.9517870e-01 9.2934787e-01 2.5272608e-02
 9.7851288e-01 9.9988747e-01 8.4413499e-02 1.4374229e-01 9.6197307e-01
 9.5905471e-01 1.7961256e-04 9.8176092e-01 9.4438761e-01 9.9457258e-01
 9.5477605e-01 2.3991184e-03 9.6218425e-01 9.8290133e-01 4.3754658e-01
 2.0760659e-03 2.2485763e-02 3.9029708e-03 1.1667315e-02 2.5338444e-03
 1.3158779e-02 1.3136494e-02 1.1750972e-02 9.8492187e-01 9.9372357e-01
 2.1670246e-05 6.0170999e-04 9.7935975e-01 4.9576019e-03 9.9803406e-01
 9.9128151e-01 9.9110854e-01 9.5952690e-01 9.5176086e-02 9.7179240e-01
 9.9533349e-01 1.3283966e-02 8.9616947e-02 2.7109652e-03 2.0087924e-04
 4.0455531e-02 2.3733689e-01 2.1509328e-03 1.3676041e-02 3.0125376e-02
 7.3475935e-03 7.9590857e-02 8.0056740e-03 9.3708009e-01 7.6415627e-03
 9.9187684e-01 1.4905460e-03 9.8327339e-01 8.6429918e-01 2.4298567e-02
 8.5081494e-01 7.2345370e-01 6.1630597e-03 2.1109696e-02 2.1310441e-02
 8.5731922e-04 3.0229320e-03 1.8333995e-01 1.3066840e-03 9.8931551e-01
 9.9677318e-01 5.9360275e-03 2.0227036e-01 3.0526170e-03 4.9802408e-01
 8.5799992e-01 2.7205169e-03 4.7829609e-02 8.1560379e-01 9.6902990e-01
 9.8689950e-01 4.4857264e-03 9.5940113e-02 9.9241406e-01 2.0137187e-02
 3.8662247e-04 9.7930324e-01 9.8186010e-01 5.3417252e-04 1.6209047e-04
 9.5129114e-01 9.2573154e-01 9.8525238e-01 9.7533071e-01 9.0990113e-03
 1.9089143e-04 9.4509536e-01 9.9008864e-01 8.1997186e-01 2.2645344e-03
 9.5579255e-01 9.9704689e-01 6.3489281e-02 9.9737322e-01 9.8054290e-01
 9.8343962e-01 8.4316516e-01 9.9724126e-01 1.1606965e-01 2.5482553e-01
 9.9597520e-01 9.8262894e-01 1.0619608e-03 8.7464017e-01 9.9759346e-01
 2.7227867e-01 9.8643965e-01 2.0625848e-02 1.7089728e-01 8.9560032e-01
 9.9369401e-01 2.0419108e-04 4.1044685e-03 1.4431037e-02 9.6555692e-01
 9.9739432e-01 8.8946748e-01 1.8471911e-03 3.2452229e-02 9.7111803e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 18:21:57, Dev, Step : 5634, Loss : 0.44396, Acc : 0.819, Auc : 0.899, Sensitive_Loss : 0.12604, Sensitive_Acc : 16.836, Sensitive_Auc : 0.996, Mean auc: 0.899, Run Time : 105.17 sec
INFO:root:2024-04-26 18:22:04, Train, Epoch : 10, Step : 5640, Loss : 0.12893, Acc : 0.547, Sensitive_Loss : 0.05107, Sensitive_Acc : 9.500, Run Time : 5.79 sec
INFO:root:2024-04-26 18:22:14, Train, Epoch : 10, Step : 5650, Loss : 0.24987, Acc : 0.900, Sensitive_Loss : 0.13653, Sensitive_Acc : 16.100, Run Time : 9.28 sec
INFO:root:2024-04-26 18:22:37, Train, Epoch : 10, Step : 5660, Loss : 0.24827, Acc : 0.903, Sensitive_Loss : 0.13349, Sensitive_Acc : 17.100, Run Time : 23.67 sec
INFO:root:2024-04-26 18:22:47, Train, Epoch : 10, Step : 5670, Loss : 0.18347, Acc : 0.903, Sensitive_Loss : 0.10017, Sensitive_Acc : 16.000, Run Time : 9.29 sec
INFO:root:2024-04-26 18:23:15, Train, Epoch : 10, Step : 5680, Loss : 0.21646, Acc : 0.878, Sensitive_Loss : 0.14866, Sensitive_Acc : 17.600, Run Time : 28.35 sec
INFO:root:2024-04-26 18:23:33, Train, Epoch : 10, Step : 5690, Loss : 0.27145, Acc : 0.878, Sensitive_Loss : 0.11053, Sensitive_Acc : 15.900, Run Time : 18.05 sec
INFO:root:2024-04-26 18:23:49, Train, Epoch : 10, Step : 5700, Loss : 0.20106, Acc : 0.938, Sensitive_Loss : 0.12074, Sensitive_Acc : 18.600, Run Time : 16.34 sec
INFO:root:2024-04-26 18:25:29, Dev, Step : 5700, Loss : 0.48000, Acc : 0.808, Auc : 0.894, Sensitive_Loss : 0.14357, Sensitive_Acc : 16.793, Sensitive_Auc : 0.995, Mean auc: 0.894, Run Time : 100.16 sec
INFO:root:2024-04-26 18:25:36, Train, Epoch : 10, Step : 5710, Loss : 0.21310, Acc : 0.906, Sensitive_Loss : 0.10042, Sensitive_Acc : 16.200, Run Time : 106.65 sec
INFO:root:2024-04-26 18:25:45, Train, Epoch : 10, Step : 5720, Loss : 0.22441, Acc : 0.884, Sensitive_Loss : 0.08385, Sensitive_Acc : 16.700, Run Time : 9.41 sec
INFO:root:2024-04-26 18:25:54, Train, Epoch : 10, Step : 5730, Loss : 0.25821, Acc : 0.903, Sensitive_Loss : 0.08921, Sensitive_Acc : 16.700, Run Time : 8.95 sec
INFO:root:2024-04-26 18:26:03, Train, Epoch : 10, Step : 5740, Loss : 0.22023, Acc : 0.909, Sensitive_Loss : 0.09207, Sensitive_Acc : 16.400, Run Time : 8.72 sec
INFO:root:2024-04-26 18:26:12, Train, Epoch : 10, Step : 5750, Loss : 0.19214, Acc : 0.916, Sensitive_Loss : 0.09468, Sensitive_Acc : 17.100, Run Time : 9.15 sec
INFO:root:2024-04-26 18:26:21, Train, Epoch : 10, Step : 5760, Loss : 0.20525, Acc : 0.903, Sensitive_Loss : 0.09213, Sensitive_Acc : 17.000, Run Time : 9.01 sec
INFO:root:2024-04-26 18:26:30, Train, Epoch : 10, Step : 5770, Loss : 0.22834, Acc : 0.912, Sensitive_Loss : 0.09662, Sensitive_Acc : 15.800, Run Time : 9.11 sec
INFO:root:2024-04-26 18:26:39, Train, Epoch : 10, Step : 5780, Loss : 0.24335, Acc : 0.881, Sensitive_Loss : 0.11411, Sensitive_Acc : 15.800, Run Time : 8.93 sec
INFO:root:2024-04-26 18:26:48, Train, Epoch : 10, Step : 5790, Loss : 0.22442, Acc : 0.919, Sensitive_Loss : 0.11449, Sensitive_Acc : 18.000, Run Time : 9.25 sec
INFO:root:2024-04-26 18:26:58, Train, Epoch : 10, Step : 5800, Loss : 0.21067, Acc : 0.912, Sensitive_Loss : 0.10590, Sensitive_Acc : 16.900, Run Time : 9.17 sec
INFO:root:2024-04-26 18:32:02, Dev, Step : 5800, Loss : 0.45835, Acc : 0.816, Auc : 0.898, Sensitive_Loss : 0.13992, Sensitive_Acc : 16.879, Sensitive_Auc : 0.996, Mean auc: 0.898, Run Time : 304.34 sec
INFO:root:2024-04-26 18:32:08, Train, Epoch : 10, Step : 5810, Loss : 0.20928, Acc : 0.912, Sensitive_Loss : 0.09605, Sensitive_Acc : 15.800, Run Time : 310.46 sec
INFO:root:2024-04-26 18:32:17, Train, Epoch : 10, Step : 5820, Loss : 0.20064, Acc : 0.922, Sensitive_Loss : 0.11330, Sensitive_Acc : 16.200, Run Time : 9.07 sec
INFO:root:2024-04-26 18:32:27, Train, Epoch : 10, Step : 5830, Loss : 0.21983, Acc : 0.909, Sensitive_Loss : 0.09566, Sensitive_Acc : 16.900, Run Time : 10.04 sec
INFO:root:2024-04-26 18:32:36, Train, Epoch : 10, Step : 5840, Loss : 0.18030, Acc : 0.938, Sensitive_Loss : 0.09797, Sensitive_Acc : 17.000, Run Time : 9.03 sec
INFO:root:2024-04-26 18:32:45, Train, Epoch : 10, Step : 5850, Loss : 0.23399, Acc : 0.909, Sensitive_Loss : 0.09602, Sensitive_Acc : 17.600, Run Time : 9.24 sec
INFO:root:2024-04-26 18:32:54, Train, Epoch : 10, Step : 5860, Loss : 0.23177, Acc : 0.903, Sensitive_Loss : 0.11280, Sensitive_Acc : 16.600, Run Time : 8.51 sec
INFO:root:2024-04-26 18:33:04, Train, Epoch : 10, Step : 5870, Loss : 0.24174, Acc : 0.900, Sensitive_Loss : 0.10856, Sensitive_Acc : 15.100, Run Time : 9.73 sec
INFO:root:2024-04-26 18:33:12, Train, Epoch : 10, Step : 5880, Loss : 0.23190, Acc : 0.919, Sensitive_Loss : 0.08671, Sensitive_Acc : 16.700, Run Time : 8.77 sec
INFO:root:2024-04-26 18:33:21, Train, Epoch : 10, Step : 5890, Loss : 0.25898, Acc : 0.891, Sensitive_Loss : 0.10269, Sensitive_Acc : 15.700, Run Time : 8.59 sec
INFO:root:2024-04-26 18:33:31, Train, Epoch : 10, Step : 5900, Loss : 0.21837, Acc : 0.912, Sensitive_Loss : 0.13059, Sensitive_Acc : 16.300, Run Time : 9.64 sec
INFO:root:2024-04-26 18:35:05, Dev, Step : 5900, Loss : 0.48233, Acc : 0.809, Auc : 0.897, Sensitive_Loss : 0.14647, Sensitive_Acc : 16.864, Sensitive_Auc : 0.994, Mean auc: 0.897, Run Time : 94.19 sec
INFO:root:2024-04-26 18:35:11, Train, Epoch : 10, Step : 5910, Loss : 0.21785, Acc : 0.894, Sensitive_Loss : 0.08859, Sensitive_Acc : 16.300, Run Time : 100.53 sec
INFO:root:2024-04-26 18:35:21, Train, Epoch : 10, Step : 5920, Loss : 0.20661, Acc : 0.912, Sensitive_Loss : 0.11925, Sensitive_Acc : 17.800, Run Time : 9.55 sec
INFO:root:2024-04-26 18:35:30, Train, Epoch : 10, Step : 5930, Loss : 0.21718, Acc : 0.906, Sensitive_Loss : 0.10281, Sensitive_Acc : 17.300, Run Time : 9.13 sec
INFO:root:2024-04-26 18:35:39, Train, Epoch : 10, Step : 5940, Loss : 0.17828, Acc : 0.912, Sensitive_Loss : 0.13241, Sensitive_Acc : 16.500, Run Time : 8.89 sec
INFO:root:2024-04-26 18:35:48, Train, Epoch : 10, Step : 5950, Loss : 0.22840, Acc : 0.906, Sensitive_Loss : 0.10870, Sensitive_Acc : 16.700, Run Time : 9.60 sec
INFO:root:2024-04-26 18:35:58, Train, Epoch : 10, Step : 5960, Loss : 0.23928, Acc : 0.894, Sensitive_Loss : 0.10431, Sensitive_Acc : 16.900, Run Time : 9.65 sec
INFO:root:2024-04-26 18:36:07, Train, Epoch : 10, Step : 5970, Loss : 0.24526, Acc : 0.916, Sensitive_Loss : 0.10629, Sensitive_Acc : 15.400, Run Time : 8.65 sec
INFO:root:2024-04-26 18:36:16, Train, Epoch : 10, Step : 5980, Loss : 0.22941, Acc : 0.900, Sensitive_Loss : 0.13613, Sensitive_Acc : 16.900, Run Time : 8.98 sec
INFO:root:2024-04-26 18:36:25, Train, Epoch : 10, Step : 5990, Loss : 0.22397, Acc : 0.878, Sensitive_Loss : 0.11786, Sensitive_Acc : 18.600, Run Time : 9.26 sec
INFO:root:2024-04-26 18:36:34, Train, Epoch : 10, Step : 6000, Loss : 0.27194, Acc : 0.866, Sensitive_Loss : 0.14608, Sensitive_Acc : 15.500, Run Time : 8.95 sec
INFO:root:2024-04-26 18:38:08, Dev, Step : 6000, Loss : 0.46857, Acc : 0.814, Auc : 0.895, Sensitive_Loss : 0.14827, Sensitive_Acc : 16.821, Sensitive_Auc : 0.997, Mean auc: 0.895, Run Time : 94.09 sec
INFO:root:2024-04-26 18:38:14, Train, Epoch : 10, Step : 6010, Loss : 0.19119, Acc : 0.922, Sensitive_Loss : 0.05639, Sensitive_Acc : 16.900, Run Time : 100.44 sec
INFO:root:2024-04-26 18:38:23, Train, Epoch : 10, Step : 6020, Loss : 0.25267, Acc : 0.912, Sensitive_Loss : 0.07996, Sensitive_Acc : 15.900, Run Time : 9.07 sec
INFO:root:2024-04-26 18:38:33, Train, Epoch : 10, Step : 6030, Loss : 0.18883, Acc : 0.906, Sensitive_Loss : 0.11199, Sensitive_Acc : 15.500, Run Time : 9.79 sec
INFO:root:2024-04-26 18:38:45, Train, Epoch : 10, Step : 6040, Loss : 0.20809, Acc : 0.891, Sensitive_Loss : 0.08909, Sensitive_Acc : 17.100, Run Time : 11.47 sec
INFO:root:2024-04-26 18:38:53, Train, Epoch : 10, Step : 6050, Loss : 0.23256, Acc : 0.875, Sensitive_Loss : 0.14339, Sensitive_Acc : 17.800, Run Time : 8.62 sec
INFO:root:2024-04-26 18:39:02, Train, Epoch : 10, Step : 6060, Loss : 0.25592, Acc : 0.912, Sensitive_Loss : 0.10879, Sensitive_Acc : 16.700, Run Time : 8.93 sec
INFO:root:2024-04-26 18:39:12, Train, Epoch : 10, Step : 6070, Loss : 0.22737, Acc : 0.894, Sensitive_Loss : 0.10648, Sensitive_Acc : 15.900, Run Time : 9.66 sec
INFO:root:2024-04-26 18:39:21, Train, Epoch : 10, Step : 6080, Loss : 0.22482, Acc : 0.906, Sensitive_Loss : 0.09249, Sensitive_Acc : 16.500, Run Time : 9.22 sec
INFO:root:2024-04-26 18:39:30, Train, Epoch : 10, Step : 6090, Loss : 0.20991, Acc : 0.903, Sensitive_Loss : 0.10094, Sensitive_Acc : 17.300, Run Time : 8.78 sec
INFO:root:2024-04-26 18:39:39, Train, Epoch : 10, Step : 6100, Loss : 0.22572, Acc : 0.909, Sensitive_Loss : 0.18912, Sensitive_Acc : 15.400, Run Time : 9.11 sec
INFO:root:2024-04-26 18:41:13, Dev, Step : 6100, Loss : 0.49612, Acc : 0.808, Auc : 0.895, Sensitive_Loss : 0.15168, Sensitive_Acc : 16.850, Sensitive_Auc : 0.994, Mean auc: 0.895, Run Time : 93.83 sec
INFO:root:2024-04-26 18:41:19, Train, Epoch : 10, Step : 6110, Loss : 0.29760, Acc : 0.875, Sensitive_Loss : 0.09261, Sensitive_Acc : 17.200, Run Time : 100.38 sec
INFO:root:2024-04-26 18:41:29, Train, Epoch : 10, Step : 6120, Loss : 0.23371, Acc : 0.887, Sensitive_Loss : 0.10879, Sensitive_Acc : 15.300, Run Time : 9.54 sec
INFO:root:2024-04-26 18:41:37, Train, Epoch : 10, Step : 6130, Loss : 0.22445, Acc : 0.909, Sensitive_Loss : 0.10693, Sensitive_Acc : 18.000, Run Time : 8.28 sec
INFO:root:2024-04-26 18:41:46, Train, Epoch : 10, Step : 6140, Loss : 0.22180, Acc : 0.900, Sensitive_Loss : 0.08660, Sensitive_Acc : 17.000, Run Time : 8.60 sec
INFO:root:2024-04-26 18:41:55, Train, Epoch : 10, Step : 6150, Loss : 0.23494, Acc : 0.891, Sensitive_Loss : 0.11665, Sensitive_Acc : 16.000, Run Time : 9.02 sec
INFO:root:2024-04-26 18:42:05, Train, Epoch : 10, Step : 6160, Loss : 0.22970, Acc : 0.906, Sensitive_Loss : 0.12582, Sensitive_Acc : 15.600, Run Time : 10.01 sec
INFO:root:2024-04-26 18:42:14, Train, Epoch : 10, Step : 6170, Loss : 0.29376, Acc : 0.869, Sensitive_Loss : 0.12423, Sensitive_Acc : 16.800, Run Time : 8.87 sec
INFO:root:2024-04-26 18:42:22, Train, Epoch : 10, Step : 6180, Loss : 0.23044, Acc : 0.909, Sensitive_Loss : 0.09353, Sensitive_Acc : 15.600, Run Time : 8.75 sec
INFO:root:2024-04-26 18:42:32, Train, Epoch : 10, Step : 6190, Loss : 0.25120, Acc : 0.884, Sensitive_Loss : 0.10806, Sensitive_Acc : 17.300, Run Time : 9.10 sec
INFO:root:2024-04-26 18:42:42, Train, Epoch : 10, Step : 6200, Loss : 0.22404, Acc : 0.925, Sensitive_Loss : 0.09538, Sensitive_Acc : 15.800, Run Time : 10.37 sec
INFO:root:2024-04-26 18:44:16, Dev, Step : 6200, Loss : 0.47750, Acc : 0.814, Auc : 0.899, Sensitive_Loss : 0.13223, Sensitive_Acc : 16.864, Sensitive_Auc : 0.996, Mean auc: 0.899, Run Time : 93.85 sec
INFO:root:2024-04-26 18:44:22, Train, Epoch : 10, Step : 6210, Loss : 0.24549, Acc : 0.909, Sensitive_Loss : 0.07302, Sensitive_Acc : 16.300, Run Time : 100.34 sec
INFO:root:2024-04-26 18:44:31, Train, Epoch : 10, Step : 6220, Loss : 0.27076, Acc : 0.884, Sensitive_Loss : 0.08381, Sensitive_Acc : 16.700, Run Time : 8.64 sec
INFO:root:2024-04-26 18:44:40, Train, Epoch : 10, Step : 6230, Loss : 0.22449, Acc : 0.903, Sensitive_Loss : 0.12728, Sensitive_Acc : 18.000, Run Time : 8.97 sec
INFO:root:2024-04-26 18:44:49, Train, Epoch : 10, Step : 6240, Loss : 0.17202, Acc : 0.928, Sensitive_Loss : 0.08547, Sensitive_Acc : 15.400, Run Time : 8.80 sec
INFO:root:2024-04-26 18:44:58, Train, Epoch : 10, Step : 6250, Loss : 0.20718, Acc : 0.922, Sensitive_Loss : 0.06057, Sensitive_Acc : 16.300, Run Time : 9.59 sec
INFO:root:2024-04-26 18:45:07, Train, Epoch : 10, Step : 6260, Loss : 0.19298, Acc : 0.912, Sensitive_Loss : 0.13342, Sensitive_Acc : 15.700, Run Time : 8.89 sec
INFO:root:2024-04-26 18:46:40
INFO:root:y_pred: [7.7678603e-01 9.7378063e-01 7.0098937e-03 ... 8.0082506e-01 2.0698574e-04
 9.6658731e-01]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [8.34163785e-01 2.98696407e-03 9.95055810e-02 2.53103417e-03
 9.96737301e-01 1.30481110e-03 9.99447882e-01 9.97590065e-01
 7.55388383e-03 9.34448957e-01 9.91870642e-01 9.99142885e-01
 9.97660518e-01 9.11566913e-01 3.39574851e-02 9.70145524e-01
 9.99936581e-01 5.21121211e-02 3.25240791e-01 9.75207508e-01
 9.76753831e-01 9.61080135e-04 9.89091992e-01 9.57623303e-01
 9.97073054e-01 9.75034714e-01 1.03951362e-03 9.76634860e-01
 9.87708211e-01 3.82841617e-01 1.83116575e-03 2.05312353e-02
 1.91786431e-03 6.53475570e-03 9.03494470e-03 8.22496135e-03
 8.58292077e-03 1.32377800e-02 9.92057860e-01 9.95602846e-01
 2.33609335e-05 3.48394766e-04 9.80083108e-01 3.04561900e-03
 9.99144316e-01 9.90862966e-01 9.89937425e-01 9.74771202e-01
 8.43263343e-02 9.42289412e-01 9.95644927e-01 1.24839265e-02
 4.53194171e-01 2.50305352e-03 4.07322223e-04 4.07745801e-02
 1.67845413e-01 3.42456205e-03 9.50790197e-03 6.10853359e-02
 6.59364043e-03 6.39311522e-02 2.23730560e-02 9.36055124e-01
 7.52896210e-03 9.96815860e-01 6.17327110e-04 9.89529550e-01
 8.83992851e-01 2.37516742e-02 9.36172247e-01 7.70619273e-01
 1.46578411e-02 1.17889643e-02 2.18326543e-02 1.18471705e-03
 3.85245099e-03 1.26097605e-01 7.50039471e-04 9.93691325e-01
 9.97068584e-01 2.17984268e-03 1.29812881e-01 2.93085910e-03
 5.05422175e-01 9.56525385e-01 4.56526456e-03 4.97182608e-02
 8.78775954e-01 9.68304217e-01 9.93003547e-01 3.74664506e-03
 8.49768519e-02 9.95616555e-01 1.41180726e-02 6.69160101e-04
 9.88340020e-01 9.89345849e-01 5.78411738e-04 4.32468842e-05
 9.61687982e-01 9.60800409e-01 9.90680635e-01 9.81227398e-01
 4.69372049e-03 1.93745771e-04 8.81355226e-01 9.88381922e-01
 8.33277166e-01 1.73748750e-03 9.17365253e-01 9.97705638e-01
 6.16224073e-02 9.98609781e-01 9.91146147e-01 9.84457552e-01
 8.22120845e-01 9.98665094e-01 8.61096680e-02 2.43606299e-01
 9.98304963e-01 9.91988957e-01 7.19093950e-04 8.84176791e-01
 9.98343825e-01 2.38302186e-01 9.95774686e-01 1.29642161e-02
 1.31512612e-01 9.32887375e-01 9.96737063e-01 3.69386544e-04
 2.76969597e-02 1.87647715e-02 9.82677639e-01 9.96999145e-01
 9.36503232e-01 2.27759057e-03 3.24668773e-02 9.76390243e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 18:46:40, Dev, Step : 6260, Loss : 0.46881, Acc : 0.816, Auc : 0.899, Sensitive_Loss : 0.12489, Sensitive_Acc : 16.864, Sensitive_Auc : 0.994, Mean auc: 0.899, Run Time : 92.37 sec
