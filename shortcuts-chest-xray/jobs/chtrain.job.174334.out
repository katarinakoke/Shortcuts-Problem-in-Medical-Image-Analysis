Running on desktop22:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/biased_dataset_val.csv",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.05,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-11 21:39:59, Train, Epoch : 1, Step : 10, Loss : 0.75905, Acc : 0.566, Sensitive_Loss : 1.13665, Sensitive_Acc : 10.200, Run Time : 9.26 sec
INFO:root:2024-04-11 21:40:07, Train, Epoch : 1, Step : 20, Loss : 0.75035, Acc : 0.591, Sensitive_Loss : 1.04161, Sensitive_Acc : 16.000, Run Time : 7.94 sec
INFO:root:2024-04-11 21:40:15, Train, Epoch : 1, Step : 30, Loss : 0.66817, Acc : 0.656, Sensitive_Loss : 1.04594, Sensitive_Acc : 16.200, Run Time : 8.00 sec
INFO:root:2024-04-11 21:40:23, Train, Epoch : 1, Step : 40, Loss : 0.60798, Acc : 0.694, Sensitive_Loss : 0.88143, Sensitive_Acc : 21.800, Run Time : 7.94 sec
INFO:root:2024-04-11 21:40:31, Train, Epoch : 1, Step : 50, Loss : 0.71079, Acc : 0.609, Sensitive_Loss : 0.89376, Sensitive_Acc : 22.700, Run Time : 7.78 sec
INFO:root:2024-04-11 21:40:38, Train, Epoch : 1, Step : 60, Loss : 0.62811, Acc : 0.678, Sensitive_Loss : 0.93558, Sensitive_Acc : 21.100, Run Time : 7.54 sec
INFO:root:2024-04-11 21:40:46, Train, Epoch : 1, Step : 70, Loss : 0.64377, Acc : 0.606, Sensitive_Loss : 0.89367, Sensitive_Acc : 18.000, Run Time : 7.99 sec
INFO:root:2024-04-11 21:40:54, Train, Epoch : 1, Step : 80, Loss : 0.56340, Acc : 0.684, Sensitive_Loss : 0.74197, Sensitive_Acc : 17.600, Run Time : 8.21 sec
INFO:root:2024-04-11 21:41:02, Train, Epoch : 1, Step : 90, Loss : 0.61636, Acc : 0.688, Sensitive_Loss : 0.86880, Sensitive_Acc : 18.700, Run Time : 7.83 sec
INFO:root:2024-04-11 21:41:10, Train, Epoch : 1, Step : 100, Loss : 0.64855, Acc : 0.659, Sensitive_Loss : 0.72492, Sensitive_Acc : 25.700, Run Time : 7.63 sec
INFO:root:2024-04-11 21:42:52, Dev, Step : 100, Loss : 0.68206, Acc : 0.625, Auc : 0.741, Sensitive_Loss : 0.76245, Sensitive_Acc : 21.466, Sensitive_Auc : 0.830, Mean auc: 0.741, Run Time : 101.99 sec
INFO:root:2024-04-11 21:42:52, Best, Step : 100, Loss : 0.68206, Acc : 0.625, Auc : 0.741, Sensitive_Loss : 0.76245, Sensitive_Acc : 21.466, Sensitive_Auc : 0.830, Best Auc : 0.741
INFO:root:2024-04-11 21:42:58, Train, Epoch : 1, Step : 110, Loss : 0.57789, Acc : 0.706, Sensitive_Loss : 0.73366, Sensitive_Acc : 16.300, Run Time : 108.35 sec
INFO:root:2024-04-11 21:43:06, Train, Epoch : 1, Step : 120, Loss : 0.59949, Acc : 0.666, Sensitive_Loss : 0.71078, Sensitive_Acc : 20.600, Run Time : 7.92 sec
INFO:root:2024-04-11 21:43:14, Train, Epoch : 1, Step : 130, Loss : 0.56626, Acc : 0.706, Sensitive_Loss : 0.70241, Sensitive_Acc : 20.900, Run Time : 7.92 sec
INFO:root:2024-04-11 21:43:22, Train, Epoch : 1, Step : 140, Loss : 0.74256, Acc : 0.641, Sensitive_Loss : 0.78105, Sensitive_Acc : 16.200, Run Time : 7.74 sec
INFO:root:2024-04-11 21:43:30, Train, Epoch : 1, Step : 150, Loss : 0.62526, Acc : 0.653, Sensitive_Loss : 0.67363, Sensitive_Acc : 22.800, Run Time : 8.18 sec
INFO:root:2024-04-11 21:43:37, Train, Epoch : 1, Step : 160, Loss : 0.60409, Acc : 0.741, Sensitive_Loss : 0.69565, Sensitive_Acc : 18.300, Run Time : 7.58 sec
INFO:root:2024-04-11 21:43:45, Train, Epoch : 1, Step : 170, Loss : 0.60714, Acc : 0.716, Sensitive_Loss : 0.66613, Sensitive_Acc : 21.000, Run Time : 7.63 sec
INFO:root:2024-04-11 21:43:53, Train, Epoch : 1, Step : 180, Loss : 0.66643, Acc : 0.662, Sensitive_Loss : 0.65603, Sensitive_Acc : 17.900, Run Time : 8.01 sec
INFO:root:2024-04-11 21:44:01, Train, Epoch : 1, Step : 190, Loss : 0.60400, Acc : 0.713, Sensitive_Loss : 0.65272, Sensitive_Acc : 21.700, Run Time : 7.95 sec
INFO:root:2024-04-11 21:44:08, Train, Epoch : 1, Step : 200, Loss : 0.59491, Acc : 0.672, Sensitive_Loss : 0.51477, Sensitive_Acc : 21.100, Run Time : 7.21 sec
INFO:root:2024-04-11 21:45:36, Dev, Step : 200, Loss : 0.62868, Acc : 0.697, Auc : 0.782, Sensitive_Loss : 0.55078, Sensitive_Acc : 19.887, Sensitive_Auc : 0.883, Mean auc: 0.782, Run Time : 87.93 sec
INFO:root:2024-04-11 21:45:37, Best, Step : 200, Loss : 0.62868, Acc : 0.697, Auc : 0.782, Sensitive_Loss : 0.55078, Sensitive_Acc : 19.887, Sensitive_Auc : 0.883, Best Auc : 0.782
INFO:root:2024-04-11 21:45:43, Train, Epoch : 1, Step : 210, Loss : 0.57009, Acc : 0.725, Sensitive_Loss : 0.58937, Sensitive_Acc : 22.400, Run Time : 94.40 sec
INFO:root:2024-04-11 21:45:51, Train, Epoch : 1, Step : 220, Loss : 0.57685, Acc : 0.694, Sensitive_Loss : 0.60476, Sensitive_Acc : 22.800, Run Time : 7.99 sec
INFO:root:2024-04-11 21:45:58, Train, Epoch : 1, Step : 230, Loss : 0.54098, Acc : 0.713, Sensitive_Loss : 0.57045, Sensitive_Acc : 23.400, Run Time : 7.75 sec
INFO:root:2024-04-11 21:46:06, Train, Epoch : 1, Step : 240, Loss : 0.62417, Acc : 0.662, Sensitive_Loss : 0.52652, Sensitive_Acc : 23.000, Run Time : 7.37 sec
INFO:root:2024-04-11 21:46:13, Train, Epoch : 1, Step : 250, Loss : 0.62180, Acc : 0.691, Sensitive_Loss : 0.61651, Sensitive_Acc : 21.400, Run Time : 7.72 sec
INFO:root:2024-04-11 21:46:22, Train, Epoch : 1, Step : 260, Loss : 0.61329, Acc : 0.675, Sensitive_Loss : 0.46398, Sensitive_Acc : 20.000, Run Time : 8.05 sec
INFO:root:2024-04-11 21:46:29, Train, Epoch : 1, Step : 270, Loss : 0.59780, Acc : 0.697, Sensitive_Loss : 0.61888, Sensitive_Acc : 21.100, Run Time : 7.79 sec
INFO:root:2024-04-11 21:46:38, Train, Epoch : 1, Step : 280, Loss : 0.56530, Acc : 0.694, Sensitive_Loss : 0.49191, Sensitive_Acc : 20.500, Run Time : 8.21 sec
INFO:root:2024-04-11 21:46:45, Train, Epoch : 1, Step : 290, Loss : 0.64256, Acc : 0.713, Sensitive_Loss : 0.40790, Sensitive_Acc : 21.300, Run Time : 7.95 sec
INFO:root:2024-04-11 21:46:53, Train, Epoch : 1, Step : 300, Loss : 0.57855, Acc : 0.731, Sensitive_Loss : 0.48672, Sensitive_Acc : 20.200, Run Time : 7.92 sec
INFO:root:2024-04-11 21:48:22, Dev, Step : 300, Loss : 0.59707, Acc : 0.711, Auc : 0.780, Sensitive_Loss : 0.64892, Sensitive_Acc : 16.308, Sensitive_Auc : 0.908, Mean auc: 0.780, Run Time : 88.53 sec
INFO:root:2024-04-11 21:48:28, Train, Epoch : 1, Step : 310, Loss : 0.64937, Acc : 0.647, Sensitive_Loss : 0.56380, Sensitive_Acc : 22.800, Run Time : 94.38 sec
INFO:root:2024-04-11 21:48:36, Train, Epoch : 1, Step : 320, Loss : 0.57323, Acc : 0.722, Sensitive_Loss : 0.44293, Sensitive_Acc : 19.900, Run Time : 8.06 sec
INFO:root:2024-04-11 21:48:43, Train, Epoch : 1, Step : 330, Loss : 0.60121, Acc : 0.731, Sensitive_Loss : 0.43440, Sensitive_Acc : 24.400, Run Time : 7.67 sec
INFO:root:2024-04-11 21:48:51, Train, Epoch : 1, Step : 340, Loss : 0.51715, Acc : 0.738, Sensitive_Loss : 0.55240, Sensitive_Acc : 21.500, Run Time : 7.59 sec
INFO:root:2024-04-11 21:48:59, Train, Epoch : 1, Step : 350, Loss : 0.52101, Acc : 0.713, Sensitive_Loss : 0.45798, Sensitive_Acc : 23.800, Run Time : 8.17 sec
INFO:root:2024-04-11 21:49:07, Train, Epoch : 1, Step : 360, Loss : 0.58256, Acc : 0.684, Sensitive_Loss : 0.45634, Sensitive_Acc : 22.100, Run Time : 7.69 sec
INFO:root:2024-04-11 21:49:15, Train, Epoch : 1, Step : 370, Loss : 0.68357, Acc : 0.694, Sensitive_Loss : 0.43524, Sensitive_Acc : 15.800, Run Time : 7.91 sec
INFO:root:2024-04-11 21:49:23, Train, Epoch : 1, Step : 380, Loss : 0.62303, Acc : 0.675, Sensitive_Loss : 0.51338, Sensitive_Acc : 18.900, Run Time : 7.77 sec
INFO:root:2024-04-11 21:49:30, Train, Epoch : 1, Step : 390, Loss : 0.63329, Acc : 0.697, Sensitive_Loss : 0.40220, Sensitive_Acc : 21.700, Run Time : 7.73 sec
INFO:root:2024-04-11 21:49:38, Train, Epoch : 1, Step : 400, Loss : 0.55739, Acc : 0.706, Sensitive_Loss : 0.34031, Sensitive_Acc : 23.400, Run Time : 8.03 sec
INFO:root:2024-04-11 21:51:07, Dev, Step : 400, Loss : 0.56930, Acc : 0.729, Auc : 0.804, Sensitive_Loss : 0.49620, Sensitive_Acc : 18.684, Sensitive_Auc : 0.962, Mean auc: 0.804, Run Time : 88.19 sec
INFO:root:2024-04-11 21:51:08, Best, Step : 400, Loss : 0.56930, Acc : 0.729, Auc : 0.804, Sensitive_Loss : 0.49620, Sensitive_Acc : 18.684, Sensitive_Auc : 0.962, Best Auc : 0.804
INFO:root:2024-04-11 21:51:14, Train, Epoch : 1, Step : 410, Loss : 0.58413, Acc : 0.731, Sensitive_Loss : 0.43850, Sensitive_Acc : 22.300, Run Time : 95.30 sec
INFO:root:2024-04-11 21:51:22, Train, Epoch : 1, Step : 420, Loss : 0.60173, Acc : 0.706, Sensitive_Loss : 0.41761, Sensitive_Acc : 22.800, Run Time : 8.26 sec
INFO:root:2024-04-11 21:51:30, Train, Epoch : 1, Step : 430, Loss : 0.57933, Acc : 0.675, Sensitive_Loss : 0.36914, Sensitive_Acc : 23.400, Run Time : 7.82 sec
INFO:root:2024-04-11 21:51:38, Train, Epoch : 1, Step : 440, Loss : 0.60340, Acc : 0.703, Sensitive_Loss : 0.43505, Sensitive_Acc : 9.700, Run Time : 7.79 sec
INFO:root:2024-04-11 21:51:46, Train, Epoch : 1, Step : 450, Loss : 0.63710, Acc : 0.681, Sensitive_Loss : 0.35225, Sensitive_Acc : 19.900, Run Time : 8.09 sec
INFO:root:2024-04-11 21:51:53, Train, Epoch : 1, Step : 460, Loss : 0.53580, Acc : 0.719, Sensitive_Loss : 0.35034, Sensitive_Acc : 18.300, Run Time : 7.81 sec
INFO:root:2024-04-11 21:52:01, Train, Epoch : 1, Step : 470, Loss : 0.55050, Acc : 0.738, Sensitive_Loss : 0.48606, Sensitive_Acc : 23.600, Run Time : 7.76 sec
INFO:root:2024-04-11 21:52:10, Train, Epoch : 1, Step : 480, Loss : 0.60522, Acc : 0.709, Sensitive_Loss : 0.27374, Sensitive_Acc : 21.800, Run Time : 8.43 sec
INFO:root:2024-04-11 21:52:17, Train, Epoch : 1, Step : 490, Loss : 0.53558, Acc : 0.744, Sensitive_Loss : 0.44305, Sensitive_Acc : 25.500, Run Time : 7.78 sec
INFO:root:2024-04-11 21:52:25, Train, Epoch : 1, Step : 500, Loss : 0.52758, Acc : 0.734, Sensitive_Loss : 0.51198, Sensitive_Acc : 25.300, Run Time : 7.73 sec
INFO:root:2024-04-11 21:53:54, Dev, Step : 500, Loss : 0.56878, Acc : 0.737, Auc : 0.813, Sensitive_Loss : 0.54135, Sensitive_Acc : 17.857, Sensitive_Auc : 0.932, Mean auc: 0.813, Run Time : 88.37 sec
INFO:root:2024-04-11 21:53:54, Best, Step : 500, Loss : 0.56878, Acc : 0.737, Auc : 0.813, Sensitive_Loss : 0.54135, Sensitive_Acc : 17.857, Sensitive_Auc : 0.932, Best Auc : 0.813
INFO:root:2024-04-11 21:54:00, Train, Epoch : 1, Step : 510, Loss : 0.58457, Acc : 0.722, Sensitive_Loss : 0.42328, Sensitive_Acc : 18.000, Run Time : 94.62 sec
INFO:root:2024-04-11 21:54:08, Train, Epoch : 1, Step : 520, Loss : 0.51381, Acc : 0.722, Sensitive_Loss : 0.33501, Sensitive_Acc : 20.800, Run Time : 7.84 sec
INFO:root:2024-04-11 21:54:16, Train, Epoch : 1, Step : 530, Loss : 0.48786, Acc : 0.766, Sensitive_Loss : 0.31914, Sensitive_Acc : 22.500, Run Time : 8.13 sec
INFO:root:2024-04-11 21:54:23, Train, Epoch : 1, Step : 540, Loss : 0.55854, Acc : 0.756, Sensitive_Loss : 0.34420, Sensitive_Acc : 21.900, Run Time : 7.73 sec
INFO:root:2024-04-11 21:54:32, Train, Epoch : 1, Step : 550, Loss : 0.57576, Acc : 0.713, Sensitive_Loss : 0.52741, Sensitive_Acc : 24.700, Run Time : 8.38 sec
INFO:root:2024-04-11 21:54:40, Train, Epoch : 1, Step : 560, Loss : 0.62077, Acc : 0.722, Sensitive_Loss : 0.32935, Sensitive_Acc : 21.200, Run Time : 7.84 sec
INFO:root:2024-04-11 21:54:47, Train, Epoch : 1, Step : 570, Loss : 0.59116, Acc : 0.731, Sensitive_Loss : 0.29805, Sensitive_Acc : 22.200, Run Time : 7.77 sec
INFO:root:2024-04-11 21:54:55, Train, Epoch : 1, Step : 580, Loss : 0.56453, Acc : 0.713, Sensitive_Loss : 0.44612, Sensitive_Acc : 21.600, Run Time : 7.95 sec
INFO:root:2024-04-11 21:55:03, Train, Epoch : 1, Step : 590, Loss : 0.56050, Acc : 0.719, Sensitive_Loss : 0.34806, Sensitive_Acc : 21.700, Run Time : 7.77 sec
INFO:root:2024-04-11 21:55:11, Train, Epoch : 1, Step : 600, Loss : 0.57283, Acc : 0.747, Sensitive_Loss : 0.34535, Sensitive_Acc : 19.400, Run Time : 7.76 sec
INFO:root:2024-04-11 21:56:39, Dev, Step : 600, Loss : 0.58472, Acc : 0.712, Auc : 0.795, Sensitive_Loss : 0.42644, Sensitive_Acc : 19.451, Sensitive_Auc : 0.968, Mean auc: 0.795, Run Time : 87.81 sec
INFO:root:2024-04-11 21:56:45, Train, Epoch : 1, Step : 610, Loss : 0.57166, Acc : 0.678, Sensitive_Loss : 0.38562, Sensitive_Acc : 23.500, Run Time : 93.75 sec
INFO:root:2024-04-11 21:56:53, Train, Epoch : 1, Step : 620, Loss : 0.55714, Acc : 0.706, Sensitive_Loss : 0.32253, Sensitive_Acc : 19.800, Run Time : 8.09 sec
INFO:root:2024-04-11 21:57:01, Train, Epoch : 1, Step : 630, Loss : 0.63250, Acc : 0.678, Sensitive_Loss : 0.31745, Sensitive_Acc : 23.200, Run Time : 8.26 sec
INFO:root:2024-04-11 21:58:31
INFO:root:y_pred: [0.20148645 0.07402711 0.34118512 ... 0.2651332  0.18839078 0.23210596]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.3973714e-02 2.7690439e-02 2.4622208e-02 1.3265516e-01 7.2831828e-03
 2.3938399e-02 3.6930249e-03 1.0014209e-03 3.6497718e-01 9.9717450e-01
 3.9497614e-01 7.1118930e-03 4.6613123e-02 3.1443857e-04 9.8712885e-01
 1.5990825e-01 6.9943875e-02 9.9840707e-01 9.9392658e-01 5.2249786e-03
 5.8990383e-01 1.2772619e-02 1.9307438e-01 4.1282590e-02 1.5800071e-01
 8.7082523e-01 2.7013069e-03 2.6829294e-03 4.9230154e-04 9.6610449e-02
 1.4020824e-01 9.0553671e-01 1.5193418e-01 2.8719938e-01 1.1087271e-03
 8.6183669e-03 3.0089652e-02 1.6988511e-01 4.7703154e-02 4.7748867e-02
 7.7501915e-02 9.2432541e-01 9.9243686e-02 6.5113112e-02 8.2676947e-01
 6.7073488e-01 8.3455235e-01 1.8220863e-01 4.8129046e-01 9.8899931e-01
 3.4109855e-01 9.9874145e-01 9.7601277e-01 1.7661403e-01 5.0745571e-01
 6.4745897e-01 4.5219492e-02 9.4922438e-02 9.6299267e-01 1.8626407e-02
 2.3151482e-03 3.1967820e-03 2.6468033e-02 4.9347011e-03 9.9882501e-01
 1.1266735e-01 3.7356693e-04 1.4613447e-01 8.7953217e-02 9.7320020e-01
 9.4338268e-01 9.9786550e-01 1.7378889e-02 2.8290522e-01 9.8105974e-02
 5.9336352e-01 1.3582233e-02 2.2929922e-02 2.2003387e-01 8.5682362e-02
 4.7207841e-01 7.6965667e-02 9.9654549e-01 9.6874893e-01 1.5890646e-01
 3.4614101e-01 2.2203283e-01 4.7695477e-02 2.5000561e-02 2.6380508e-03
 8.8839373e-03 5.6006354e-01 1.0795179e-03 1.3348089e-03 8.2695838e-03
 1.4276057e-01 2.1021113e-04 2.0095506e-01 5.6972326e-04 8.6851098e-02
 1.4098607e-02 4.6222994e-01 1.9807005e-01 1.0535769e-02 1.9791283e-01
 2.1748662e-02 2.6019663e-01 7.9665613e-01 7.1124351e-01 9.3150783e-01
 3.0837188e-04 9.9647874e-01 9.5737046e-01 1.7874074e-03 1.2631157e-01
 3.9468214e-01 3.7242109e-01 1.6741037e-02 1.9306186e-01 2.3025402e-01
 9.4829544e-02 4.9514475e-04 1.4535882e-01 1.1897233e-02 2.2575101e-03
 9.1317785e-01 2.1589013e-02 9.7828501e-01 2.1163811e-01 3.0805281e-01
 3.0179657e-02 2.4152312e-01 5.4616790e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 21:58:31, Dev, Step : 634, Loss : 0.56550, Acc : 0.740, Auc : 0.821, Sensitive_Loss : 0.32900, Sensitive_Acc : 21.075, Sensitive_Auc : 0.969, Mean auc: 0.821, Run Time : 86.96 sec
INFO:root:2024-04-11 21:58:32, Best, Step : 634, Loss : 0.56550, Acc : 0.740,Auc : 0.821, Best Auc : 0.821, Sensitive_Loss : 0.32900, Sensitive_Acc : 21.075, Sensitive_Auc : 0.969
INFO:root:2024-04-11 21:58:38, Train, Epoch : 2, Step : 640, Loss : 0.27678, Acc : 0.444, Sensitive_Loss : 0.17495, Sensitive_Acc : 11.100, Run Time : 5.16 sec
INFO:root:2024-04-11 21:58:45, Train, Epoch : 2, Step : 650, Loss : 0.48805, Acc : 0.778, Sensitive_Loss : 0.23466, Sensitive_Acc : 22.500, Run Time : 6.98 sec
INFO:root:2024-04-11 21:58:52, Train, Epoch : 2, Step : 660, Loss : 0.59886, Acc : 0.716, Sensitive_Loss : 0.27898, Sensitive_Acc : 22.400, Run Time : 7.32 sec
INFO:root:2024-04-11 21:58:59, Train, Epoch : 2, Step : 670, Loss : 0.52848, Acc : 0.744, Sensitive_Loss : 0.37250, Sensitive_Acc : 25.500, Run Time : 7.00 sec
INFO:root:2024-04-11 21:59:06, Train, Epoch : 2, Step : 680, Loss : 0.56118, Acc : 0.716, Sensitive_Loss : 0.33128, Sensitive_Acc : 24.300, Run Time : 6.97 sec
INFO:root:2024-04-11 21:59:13, Train, Epoch : 2, Step : 690, Loss : 0.54993, Acc : 0.734, Sensitive_Loss : 0.39027, Sensitive_Acc : 19.700, Run Time : 6.93 sec
INFO:root:2024-04-11 21:59:21, Train, Epoch : 2, Step : 700, Loss : 0.52156, Acc : 0.769, Sensitive_Loss : 0.34242, Sensitive_Acc : 19.000, Run Time : 7.75 sec
INFO:root:2024-04-11 22:00:48, Dev, Step : 700, Loss : 0.56545, Acc : 0.725, Auc : 0.807, Sensitive_Loss : 0.35713, Sensitive_Acc : 20.248, Sensitive_Auc : 0.983, Mean auc: 0.807, Run Time : 87.65 sec
INFO:root:2024-04-11 22:00:54, Train, Epoch : 2, Step : 710, Loss : 0.49833, Acc : 0.766, Sensitive_Loss : 0.23702, Sensitive_Acc : 20.100, Run Time : 93.66 sec
INFO:root:2024-04-11 22:01:01, Train, Epoch : 2, Step : 720, Loss : 0.43270, Acc : 0.787, Sensitive_Loss : 0.37647, Sensitive_Acc : 15.500, Run Time : 6.91 sec
INFO:root:2024-04-11 22:01:08, Train, Epoch : 2, Step : 730, Loss : 0.51032, Acc : 0.762, Sensitive_Loss : 0.28788, Sensitive_Acc : 17.300, Run Time : 7.02 sec
INFO:root:2024-04-11 22:01:15, Train, Epoch : 2, Step : 740, Loss : 0.49570, Acc : 0.750, Sensitive_Loss : 0.33640, Sensitive_Acc : 21.900, Run Time : 6.70 sec
INFO:root:2024-04-11 22:01:22, Train, Epoch : 2, Step : 750, Loss : 0.50691, Acc : 0.778, Sensitive_Loss : 0.34845, Sensitive_Acc : 22.900, Run Time : 7.15 sec
INFO:root:2024-04-11 22:01:29, Train, Epoch : 2, Step : 760, Loss : 0.50715, Acc : 0.722, Sensitive_Loss : 0.24412, Sensitive_Acc : 22.100, Run Time : 7.03 sec
INFO:root:2024-04-11 22:01:36, Train, Epoch : 2, Step : 770, Loss : 0.50743, Acc : 0.775, Sensitive_Loss : 0.36559, Sensitive_Acc : 18.400, Run Time : 7.14 sec
INFO:root:2024-04-11 22:01:44, Train, Epoch : 2, Step : 780, Loss : 0.54582, Acc : 0.775, Sensitive_Loss : 0.35258, Sensitive_Acc : 18.700, Run Time : 7.31 sec
INFO:root:2024-04-11 22:01:51, Train, Epoch : 2, Step : 790, Loss : 0.43551, Acc : 0.766, Sensitive_Loss : 0.22551, Sensitive_Acc : 20.600, Run Time : 7.07 sec
INFO:root:2024-04-11 22:01:58, Train, Epoch : 2, Step : 800, Loss : 0.55329, Acc : 0.731, Sensitive_Loss : 0.33594, Sensitive_Acc : 22.400, Run Time : 6.95 sec
INFO:root:2024-04-11 22:03:26, Dev, Step : 800, Loss : 0.63368, Acc : 0.710, Auc : 0.819, Sensitive_Loss : 0.34730, Sensitive_Acc : 19.556, Sensitive_Auc : 0.972, Mean auc: 0.819, Run Time : 88.14 sec
INFO:root:2024-04-11 22:03:31, Train, Epoch : 2, Step : 810, Loss : 0.52703, Acc : 0.750, Sensitive_Loss : 0.29820, Sensitive_Acc : 23.900, Run Time : 93.63 sec
INFO:root:2024-04-11 22:03:38, Train, Epoch : 2, Step : 820, Loss : 0.62267, Acc : 0.709, Sensitive_Loss : 0.29117, Sensitive_Acc : 20.500, Run Time : 7.18 sec
INFO:root:2024-04-11 22:03:45, Train, Epoch : 2, Step : 830, Loss : 0.60989, Acc : 0.722, Sensitive_Loss : 0.27963, Sensitive_Acc : 24.800, Run Time : 7.02 sec
INFO:root:2024-04-11 22:03:53, Train, Epoch : 2, Step : 840, Loss : 0.54431, Acc : 0.762, Sensitive_Loss : 0.28170, Sensitive_Acc : 24.000, Run Time : 7.57 sec
INFO:root:2024-04-11 22:04:00, Train, Epoch : 2, Step : 850, Loss : 0.53807, Acc : 0.741, Sensitive_Loss : 0.24071, Sensitive_Acc : 20.900, Run Time : 6.59 sec
INFO:root:2024-04-11 22:04:07, Train, Epoch : 2, Step : 860, Loss : 0.41203, Acc : 0.762, Sensitive_Loss : 0.30728, Sensitive_Acc : 21.100, Run Time : 7.28 sec
INFO:root:2024-04-11 22:04:14, Train, Epoch : 2, Step : 870, Loss : 0.47436, Acc : 0.766, Sensitive_Loss : 0.22325, Sensitive_Acc : 15.900, Run Time : 7.03 sec
INFO:root:2024-04-11 22:04:21, Train, Epoch : 2, Step : 880, Loss : 0.58937, Acc : 0.713, Sensitive_Loss : 0.28776, Sensitive_Acc : 22.900, Run Time : 6.91 sec
INFO:root:2024-04-11 22:04:28, Train, Epoch : 2, Step : 890, Loss : 0.52086, Acc : 0.731, Sensitive_Loss : 0.26586, Sensitive_Acc : 24.400, Run Time : 7.26 sec
INFO:root:2024-04-11 22:04:35, Train, Epoch : 2, Step : 900, Loss : 0.50457, Acc : 0.744, Sensitive_Loss : 0.23026, Sensitive_Acc : 19.500, Run Time : 7.24 sec
INFO:root:2024-04-11 22:06:03, Dev, Step : 900, Loss : 0.53460, Acc : 0.750, Auc : 0.832, Sensitive_Loss : 0.37127, Sensitive_Acc : 19.752, Sensitive_Auc : 0.990, Mean auc: 0.832, Run Time : 88.22 sec
INFO:root:2024-04-11 22:06:04, Best, Step : 900, Loss : 0.53460, Acc : 0.750, Auc : 0.832, Sensitive_Loss : 0.37127, Sensitive_Acc : 19.752, Sensitive_Auc : 0.990, Best Auc : 0.832
INFO:root:2024-04-11 22:06:10, Train, Epoch : 2, Step : 910, Loss : 0.52163, Acc : 0.744, Sensitive_Loss : 0.29664, Sensitive_Acc : 22.100, Run Time : 94.45 sec
INFO:root:2024-04-11 22:06:17, Train, Epoch : 2, Step : 920, Loss : 0.58071, Acc : 0.709, Sensitive_Loss : 0.22481, Sensitive_Acc : 17.800, Run Time : 7.54 sec
INFO:root:2024-04-11 22:06:24, Train, Epoch : 2, Step : 930, Loss : 0.56671, Acc : 0.762, Sensitive_Loss : 0.30984, Sensitive_Acc : 19.900, Run Time : 7.07 sec
INFO:root:2024-04-11 22:06:31, Train, Epoch : 2, Step : 940, Loss : 0.51128, Acc : 0.781, Sensitive_Loss : 0.42506, Sensitive_Acc : 22.000, Run Time : 7.15 sec
INFO:root:2024-04-11 22:06:38, Train, Epoch : 2, Step : 950, Loss : 0.50232, Acc : 0.719, Sensitive_Loss : 0.28532, Sensitive_Acc : 20.000, Run Time : 6.89 sec
INFO:root:2024-04-11 22:06:45, Train, Epoch : 2, Step : 960, Loss : 0.52295, Acc : 0.762, Sensitive_Loss : 0.29576, Sensitive_Acc : 20.600, Run Time : 7.02 sec
INFO:root:2024-04-11 22:06:52, Train, Epoch : 2, Step : 970, Loss : 0.51418, Acc : 0.762, Sensitive_Loss : 0.22440, Sensitive_Acc : 24.500, Run Time : 6.68 sec
INFO:root:2024-04-11 22:06:59, Train, Epoch : 2, Step : 980, Loss : 0.57686, Acc : 0.728, Sensitive_Loss : 0.24042, Sensitive_Acc : 23.400, Run Time : 6.81 sec
INFO:root:2024-04-11 22:07:06, Train, Epoch : 2, Step : 990, Loss : 0.50081, Acc : 0.734, Sensitive_Loss : 0.24488, Sensitive_Acc : 19.400, Run Time : 7.41 sec
INFO:root:2024-04-11 22:07:13, Train, Epoch : 2, Step : 1000, Loss : 0.53269, Acc : 0.759, Sensitive_Loss : 0.39992, Sensitive_Acc : 21.000, Run Time : 6.88 sec
INFO:root:2024-04-11 22:08:41, Dev, Step : 1000, Loss : 0.54709, Acc : 0.755, Auc : 0.825, Sensitive_Loss : 0.43976, Sensitive_Acc : 19.571, Sensitive_Auc : 0.979, Mean auc: 0.825, Run Time : 88.19 sec
INFO:root:2024-04-11 22:08:47, Train, Epoch : 2, Step : 1010, Loss : 0.57575, Acc : 0.722, Sensitive_Loss : 0.32279, Sensitive_Acc : 24.900, Run Time : 93.80 sec
INFO:root:2024-04-11 22:08:54, Train, Epoch : 2, Step : 1020, Loss : 0.57994, Acc : 0.744, Sensitive_Loss : 0.26821, Sensitive_Acc : 22.600, Run Time : 7.17 sec
INFO:root:2024-04-11 22:09:01, Train, Epoch : 2, Step : 1030, Loss : 0.51177, Acc : 0.731, Sensitive_Loss : 0.27278, Sensitive_Acc : 18.500, Run Time : 6.63 sec
INFO:root:2024-04-11 22:09:08, Train, Epoch : 2, Step : 1040, Loss : 0.49742, Acc : 0.756, Sensitive_Loss : 0.43715, Sensitive_Acc : 16.800, Run Time : 7.36 sec
INFO:root:2024-04-11 22:09:15, Train, Epoch : 2, Step : 1050, Loss : 0.54519, Acc : 0.766, Sensitive_Loss : 0.19318, Sensitive_Acc : 24.100, Run Time : 7.36 sec
INFO:root:2024-04-11 22:09:23, Train, Epoch : 2, Step : 1060, Loss : 0.55129, Acc : 0.747, Sensitive_Loss : 0.22641, Sensitive_Acc : 20.600, Run Time : 7.71 sec
INFO:root:2024-04-11 22:09:30, Train, Epoch : 2, Step : 1070, Loss : 0.46095, Acc : 0.716, Sensitive_Loss : 0.28179, Sensitive_Acc : 22.500, Run Time : 6.59 sec
INFO:root:2024-04-11 22:09:37, Train, Epoch : 2, Step : 1080, Loss : 0.50887, Acc : 0.762, Sensitive_Loss : 0.22047, Sensitive_Acc : 22.000, Run Time : 6.75 sec
INFO:root:2024-04-11 22:09:44, Train, Epoch : 2, Step : 1090, Loss : 0.61729, Acc : 0.741, Sensitive_Loss : 0.33142, Sensitive_Acc : 20.500, Run Time : 7.32 sec
INFO:root:2024-04-11 22:09:51, Train, Epoch : 2, Step : 1100, Loss : 0.59153, Acc : 0.716, Sensitive_Loss : 0.21293, Sensitive_Acc : 22.800, Run Time : 6.91 sec
INFO:root:2024-04-11 22:11:19, Dev, Step : 1100, Loss : 0.60324, Acc : 0.726, Auc : 0.827, Sensitive_Loss : 0.44707, Sensitive_Acc : 19.195, Sensitive_Auc : 0.976, Mean auc: 0.827, Run Time : 88.07 sec
INFO:root:2024-04-11 22:11:24, Train, Epoch : 2, Step : 1110, Loss : 0.49845, Acc : 0.744, Sensitive_Loss : 0.37338, Sensitive_Acc : 18.900, Run Time : 93.54 sec
INFO:root:2024-04-11 22:11:31, Train, Epoch : 2, Step : 1120, Loss : 0.50799, Acc : 0.762, Sensitive_Loss : 0.18664, Sensitive_Acc : 22.300, Run Time : 7.13 sec
INFO:root:2024-04-11 22:11:39, Train, Epoch : 2, Step : 1130, Loss : 0.47184, Acc : 0.753, Sensitive_Loss : 0.25713, Sensitive_Acc : 23.700, Run Time : 7.44 sec
INFO:root:2024-04-11 22:11:46, Train, Epoch : 2, Step : 1140, Loss : 0.58595, Acc : 0.697, Sensitive_Loss : 0.20163, Sensitive_Acc : 18.800, Run Time : 7.09 sec
INFO:root:2024-04-11 22:11:53, Train, Epoch : 2, Step : 1150, Loss : 0.52974, Acc : 0.709, Sensitive_Loss : 0.32103, Sensitive_Acc : 19.200, Run Time : 7.10 sec
INFO:root:2024-04-11 22:12:01, Train, Epoch : 2, Step : 1160, Loss : 0.57691, Acc : 0.744, Sensitive_Loss : 0.23442, Sensitive_Acc : 21.200, Run Time : 7.58 sec
INFO:root:2024-04-11 22:12:08, Train, Epoch : 2, Step : 1170, Loss : 0.48613, Acc : 0.759, Sensitive_Loss : 0.26288, Sensitive_Acc : 20.100, Run Time : 7.10 sec
INFO:root:2024-04-11 22:12:14, Train, Epoch : 2, Step : 1180, Loss : 0.48145, Acc : 0.772, Sensitive_Loss : 0.21621, Sensitive_Acc : 24.300, Run Time : 6.49 sec
INFO:root:2024-04-11 22:12:21, Train, Epoch : 2, Step : 1190, Loss : 0.53240, Acc : 0.734, Sensitive_Loss : 0.24809, Sensitive_Acc : 23.600, Run Time : 7.19 sec
INFO:root:2024-04-11 22:12:28, Train, Epoch : 2, Step : 1200, Loss : 0.57530, Acc : 0.738, Sensitive_Loss : 0.21505, Sensitive_Acc : 17.300, Run Time : 6.88 sec
INFO:root:2024-04-11 22:13:57, Dev, Step : 1200, Loss : 0.54053, Acc : 0.758, Auc : 0.837, Sensitive_Loss : 0.28983, Sensitive_Acc : 21.226, Sensitive_Auc : 0.976, Mean auc: 0.837, Run Time : 88.24 sec
INFO:root:2024-04-11 22:13:57, Best, Step : 1200, Loss : 0.54053, Acc : 0.758, Auc : 0.837, Sensitive_Loss : 0.28983, Sensitive_Acc : 21.226, Sensitive_Auc : 0.976, Best Auc : 0.837
INFO:root:2024-04-11 22:14:03, Train, Epoch : 2, Step : 1210, Loss : 0.50763, Acc : 0.769, Sensitive_Loss : 0.25515, Sensitive_Acc : 26.400, Run Time : 94.61 sec
INFO:root:2024-04-11 22:14:10, Train, Epoch : 2, Step : 1220, Loss : 0.54032, Acc : 0.784, Sensitive_Loss : 0.19082, Sensitive_Acc : 20.900, Run Time : 6.88 sec
INFO:root:2024-04-11 22:14:17, Train, Epoch : 2, Step : 1230, Loss : 0.51964, Acc : 0.725, Sensitive_Loss : 0.28523, Sensitive_Acc : 20.200, Run Time : 6.90 sec
INFO:root:2024-04-11 22:14:24, Train, Epoch : 2, Step : 1240, Loss : 0.50773, Acc : 0.759, Sensitive_Loss : 0.20836, Sensitive_Acc : 24.200, Run Time : 7.33 sec
INFO:root:2024-04-11 22:14:31, Train, Epoch : 2, Step : 1250, Loss : 0.47177, Acc : 0.775, Sensitive_Loss : 0.23518, Sensitive_Acc : 23.100, Run Time : 7.40 sec
INFO:root:2024-04-11 22:14:38, Train, Epoch : 2, Step : 1260, Loss : 0.55037, Acc : 0.738, Sensitive_Loss : 0.28968, Sensitive_Acc : 14.400, Run Time : 6.83 sec
INFO:root:2024-04-11 22:16:10
INFO:root:y_pred: [0.10679637 0.0417335  0.32198367 ... 0.50459146 0.09292784 0.1478431 ]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [2.39425921e-03 1.67000364e-03 6.27260655e-03 2.10988112e-02
 3.44126346e-03 8.35188758e-03 2.38077249e-03 9.61811561e-03
 2.32521459e-01 9.99854922e-01 6.52523041e-01 2.85794819e-03
 9.20525715e-02 7.18761294e-04 9.91239250e-01 6.99213594e-02
 1.27494605e-02 9.94919837e-01 9.99784052e-01 3.92450625e-03
 9.16426301e-01 2.55236146e-03 5.65608442e-01 3.26239392e-02
 5.66179395e-01 8.44893336e-01 6.92462481e-06 3.74367373e-05
 6.33144402e-04 3.39761898e-02 3.34243439e-02 9.27764714e-01
 2.33941138e-01 3.45089912e-01 1.57036446e-03 1.38565141e-03
 3.82218100e-02 2.93536466e-02 5.84345832e-02 1.45864055e-01
 8.24785829e-02 9.14260805e-01 8.97219870e-03 5.56686707e-03
 9.94808376e-01 8.32713187e-01 7.67570376e-01 4.26677525e-01
 3.23468417e-01 9.81149197e-01 9.71633554e-01 9.99701798e-01
 9.94819820e-01 8.45610723e-03 2.86300272e-01 2.65539944e-01
 1.57938302e-02 5.29163191e-03 9.95088756e-01 1.44883068e-02
 2.41720490e-03 1.80963310e-03 1.72347240e-02 1.65010581e-03
 9.91230726e-01 1.06050968e-01 7.36232242e-03 1.74291328e-01
 2.09993824e-01 9.90490556e-01 9.91221070e-01 9.99969840e-01
 1.73564777e-02 3.34661394e-01 1.32757390e-03 6.22057855e-01
 3.48486677e-02 1.34044490e-03 1.49711803e-01 7.47770024e-03
 7.98309296e-02 6.40645972e-04 9.95918572e-01 9.95853066e-01
 1.72679089e-02 4.37558778e-02 9.56024751e-02 1.87282469e-02
 3.33070122e-02 2.94189621e-03 5.54690361e-02 7.60314345e-01
 1.41227953e-04 2.63480982e-03 1.37598645e-02 1.73480622e-02
 9.18540522e-04 3.13498378e-02 1.37586556e-02 9.43879969e-03
 5.20802625e-02 2.28477418e-02 2.83083260e-01 1.76139567e-02
 8.43667239e-02 2.07883073e-03 5.23475945e-01 5.06558657e-01
 9.28262889e-01 2.62385815e-01 1.73847936e-03 9.99936223e-01
 9.99645829e-01 1.70507235e-04 4.13577139e-01 4.78325129e-01
 3.37444633e-01 1.83030423e-02 1.25598043e-01 3.54352519e-02
 3.04688625e-02 3.07640666e-03 1.88937977e-01 8.52876576e-04
 7.79497325e-02 9.80071783e-01 2.41285935e-03 9.98585939e-01
 3.25151324e-01 1.45325914e-01 4.27735969e-03 1.10730715e-01
 2.65224426e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 22:16:10, Dev, Step : 1268, Loss : 0.54364, Acc : 0.752, Auc : 0.829, Sensitive_Loss : 0.29296, Sensitive_Acc : 20.669, Sensitive_Auc : 0.972, Mean auc: 0.829, Run Time : 86.77 sec
INFO:root:2024-04-11 22:16:14, Train, Epoch : 3, Step : 1270, Loss : 0.10382, Acc : 0.156, Sensitive_Loss : 0.03867, Sensitive_Acc : 3.400, Run Time : 3.08 sec
INFO:root:2024-04-11 22:16:21, Train, Epoch : 3, Step : 1280, Loss : 0.54589, Acc : 0.741, Sensitive_Loss : 0.27698, Sensitive_Acc : 25.300, Run Time : 6.71 sec
INFO:root:2024-04-11 22:16:28, Train, Epoch : 3, Step : 1290, Loss : 0.56167, Acc : 0.769, Sensitive_Loss : 0.27767, Sensitive_Acc : 19.800, Run Time : 6.55 sec
INFO:root:2024-04-11 22:16:35, Train, Epoch : 3, Step : 1300, Loss : 0.43845, Acc : 0.803, Sensitive_Loss : 0.27915, Sensitive_Acc : 20.100, Run Time : 7.84 sec
INFO:root:2024-04-11 22:18:03, Dev, Step : 1300, Loss : 0.52769, Acc : 0.761, Auc : 0.841, Sensitive_Loss : 0.27790, Sensitive_Acc : 20.789, Sensitive_Auc : 0.974, Mean auc: 0.841, Run Time : 87.49 sec
INFO:root:2024-04-11 22:18:04, Best, Step : 1300, Loss : 0.52769, Acc : 0.761, Auc : 0.841, Sensitive_Loss : 0.27790, Sensitive_Acc : 20.789, Sensitive_Auc : 0.974, Best Auc : 0.841
INFO:root:2024-04-11 22:18:09, Train, Epoch : 3, Step : 1310, Loss : 0.49388, Acc : 0.766, Sensitive_Loss : 0.21016, Sensitive_Acc : 23.200, Run Time : 93.91 sec
INFO:root:2024-04-11 22:18:17, Train, Epoch : 3, Step : 1320, Loss : 0.46742, Acc : 0.791, Sensitive_Loss : 0.31091, Sensitive_Acc : 17.100, Run Time : 7.19 sec
INFO:root:2024-04-11 22:18:23, Train, Epoch : 3, Step : 1330, Loss : 0.45676, Acc : 0.809, Sensitive_Loss : 0.25833, Sensitive_Acc : 27.000, Run Time : 6.76 sec
INFO:root:2024-04-11 22:18:31, Train, Epoch : 3, Step : 1340, Loss : 0.50015, Acc : 0.791, Sensitive_Loss : 0.16240, Sensitive_Acc : 18.000, Run Time : 7.22 sec
INFO:root:2024-04-11 22:18:38, Train, Epoch : 3, Step : 1350, Loss : 0.45287, Acc : 0.753, Sensitive_Loss : 0.23277, Sensitive_Acc : 22.400, Run Time : 7.55 sec
INFO:root:2024-04-11 22:18:45, Train, Epoch : 3, Step : 1360, Loss : 0.50359, Acc : 0.791, Sensitive_Loss : 0.19930, Sensitive_Acc : 22.000, Run Time : 7.07 sec
INFO:root:2024-04-11 22:18:52, Train, Epoch : 3, Step : 1370, Loss : 0.43525, Acc : 0.794, Sensitive_Loss : 0.17179, Sensitive_Acc : 20.300, Run Time : 6.85 sec
INFO:root:2024-04-11 22:18:59, Train, Epoch : 3, Step : 1380, Loss : 0.45395, Acc : 0.775, Sensitive_Loss : 0.21795, Sensitive_Acc : 20.500, Run Time : 7.18 sec
INFO:root:2024-04-11 22:19:06, Train, Epoch : 3, Step : 1390, Loss : 0.46059, Acc : 0.762, Sensitive_Loss : 0.17596, Sensitive_Acc : 21.400, Run Time : 7.02 sec
INFO:root:2024-04-11 22:19:13, Train, Epoch : 3, Step : 1400, Loss : 0.42405, Acc : 0.816, Sensitive_Loss : 0.24306, Sensitive_Acc : 23.100, Run Time : 7.12 sec
INFO:root:2024-04-11 22:20:41, Dev, Step : 1400, Loss : 0.52458, Acc : 0.768, Auc : 0.849, Sensitive_Loss : 0.26476, Sensitive_Acc : 21.000, Sensitive_Auc : 0.980, Mean auc: 0.849, Run Time : 87.83 sec
INFO:root:2024-04-11 22:20:42, Best, Step : 1400, Loss : 0.52458, Acc : 0.768, Auc : 0.849, Sensitive_Loss : 0.26476, Sensitive_Acc : 21.000, Sensitive_Auc : 0.980, Best Auc : 0.849
INFO:root:2024-04-11 22:20:47, Train, Epoch : 3, Step : 1410, Loss : 0.51100, Acc : 0.741, Sensitive_Loss : 0.14269, Sensitive_Acc : 19.300, Run Time : 93.82 sec
INFO:root:2024-04-11 22:20:55, Train, Epoch : 3, Step : 1420, Loss : 0.48246, Acc : 0.772, Sensitive_Loss : 0.22900, Sensitive_Acc : 19.700, Run Time : 7.37 sec
INFO:root:2024-04-11 22:21:02, Train, Epoch : 3, Step : 1430, Loss : 0.40130, Acc : 0.816, Sensitive_Loss : 0.22519, Sensitive_Acc : 26.000, Run Time : 7.22 sec
INFO:root:2024-04-11 22:21:09, Train, Epoch : 3, Step : 1440, Loss : 0.40877, Acc : 0.803, Sensitive_Loss : 0.25876, Sensitive_Acc : 18.100, Run Time : 7.21 sec
INFO:root:2024-04-11 22:21:16, Train, Epoch : 3, Step : 1450, Loss : 0.49371, Acc : 0.803, Sensitive_Loss : 0.27186, Sensitive_Acc : 20.100, Run Time : 6.82 sec
INFO:root:2024-04-11 22:21:23, Train, Epoch : 3, Step : 1460, Loss : 0.48213, Acc : 0.784, Sensitive_Loss : 0.28149, Sensitive_Acc : 25.300, Run Time : 7.38 sec
INFO:root:2024-04-11 22:21:30, Train, Epoch : 3, Step : 1470, Loss : 0.46964, Acc : 0.769, Sensitive_Loss : 0.26308, Sensitive_Acc : 23.700, Run Time : 7.07 sec
INFO:root:2024-04-11 22:21:37, Train, Epoch : 3, Step : 1480, Loss : 0.46247, Acc : 0.816, Sensitive_Loss : 0.18812, Sensitive_Acc : 23.900, Run Time : 6.85 sec
INFO:root:2024-04-11 22:21:44, Train, Epoch : 3, Step : 1490, Loss : 0.39036, Acc : 0.831, Sensitive_Loss : 0.23870, Sensitive_Acc : 21.700, Run Time : 6.83 sec
INFO:root:2024-04-11 22:21:51, Train, Epoch : 3, Step : 1500, Loss : 0.47005, Acc : 0.800, Sensitive_Loss : 0.23637, Sensitive_Acc : 24.000, Run Time : 7.01 sec
INFO:root:2024-04-11 22:23:20, Dev, Step : 1500, Loss : 0.52020, Acc : 0.769, Auc : 0.853, Sensitive_Loss : 0.26404, Sensitive_Acc : 21.105, Sensitive_Auc : 0.983, Mean auc: 0.853, Run Time : 88.77 sec
INFO:root:2024-04-11 22:23:21, Best, Step : 1500, Loss : 0.52020, Acc : 0.769, Auc : 0.853, Sensitive_Loss : 0.26404, Sensitive_Acc : 21.105, Sensitive_Auc : 0.983, Best Auc : 0.853
INFO:root:2024-04-11 22:23:26, Train, Epoch : 3, Step : 1510, Loss : 0.51942, Acc : 0.766, Sensitive_Loss : 0.19455, Sensitive_Acc : 27.000, Run Time : 95.13 sec
INFO:root:2024-04-11 22:23:33, Train, Epoch : 3, Step : 1520, Loss : 0.43971, Acc : 0.819, Sensitive_Loss : 0.16322, Sensitive_Acc : 22.600, Run Time : 7.05 sec
INFO:root:2024-04-11 22:23:41, Train, Epoch : 3, Step : 1530, Loss : 0.40704, Acc : 0.787, Sensitive_Loss : 0.18742, Sensitive_Acc : 20.200, Run Time : 7.62 sec
INFO:root:2024-04-11 22:23:48, Train, Epoch : 3, Step : 1540, Loss : 0.51791, Acc : 0.772, Sensitive_Loss : 0.24858, Sensitive_Acc : 22.600, Run Time : 6.96 sec
INFO:root:2024-04-11 22:23:55, Train, Epoch : 3, Step : 1550, Loss : 0.41664, Acc : 0.797, Sensitive_Loss : 0.13191, Sensitive_Acc : 23.300, Run Time : 6.99 sec
INFO:root:2024-04-11 22:24:02, Train, Epoch : 3, Step : 1560, Loss : 0.40922, Acc : 0.853, Sensitive_Loss : 0.20397, Sensitive_Acc : 22.200, Run Time : 7.08 sec
INFO:root:2024-04-11 22:24:09, Train, Epoch : 3, Step : 1570, Loss : 0.46102, Acc : 0.781, Sensitive_Loss : 0.20751, Sensitive_Acc : 22.000, Run Time : 7.17 sec
INFO:root:2024-04-11 22:24:16, Train, Epoch : 3, Step : 1580, Loss : 0.44179, Acc : 0.809, Sensitive_Loss : 0.19449, Sensitive_Acc : 25.100, Run Time : 6.72 sec
INFO:root:2024-04-11 22:24:23, Train, Epoch : 3, Step : 1590, Loss : 0.52446, Acc : 0.781, Sensitive_Loss : 0.16324, Sensitive_Acc : 20.800, Run Time : 7.23 sec
INFO:root:2024-04-11 22:24:30, Train, Epoch : 3, Step : 1600, Loss : 0.48307, Acc : 0.787, Sensitive_Loss : 0.18434, Sensitive_Acc : 19.000, Run Time : 7.05 sec
INFO:root:2024-04-11 22:25:58, Dev, Step : 1600, Loss : 0.51401, Acc : 0.774, Auc : 0.854, Sensitive_Loss : 0.22562, Sensitive_Acc : 21.782, Sensitive_Auc : 0.982, Mean auc: 0.854, Run Time : 87.93 sec
INFO:root:2024-04-11 22:25:59, Best, Step : 1600, Loss : 0.51401, Acc : 0.774, Auc : 0.854, Sensitive_Loss : 0.22562, Sensitive_Acc : 21.782, Sensitive_Auc : 0.982, Best Auc : 0.854
INFO:root:2024-04-11 22:26:04, Train, Epoch : 3, Step : 1610, Loss : 0.47162, Acc : 0.791, Sensitive_Loss : 0.15058, Sensitive_Acc : 23.800, Run Time : 94.49 sec
INFO:root:2024-04-11 22:26:12, Train, Epoch : 3, Step : 1620, Loss : 0.40545, Acc : 0.809, Sensitive_Loss : 0.20906, Sensitive_Acc : 24.000, Run Time : 7.11 sec
INFO:root:2024-04-11 22:26:19, Train, Epoch : 3, Step : 1630, Loss : 0.46017, Acc : 0.800, Sensitive_Loss : 0.20375, Sensitive_Acc : 15.000, Run Time : 7.34 sec
INFO:root:2024-04-11 22:26:26, Train, Epoch : 3, Step : 1640, Loss : 0.39930, Acc : 0.791, Sensitive_Loss : 0.24077, Sensitive_Acc : 20.400, Run Time : 7.08 sec
INFO:root:2024-04-11 22:26:33, Train, Epoch : 3, Step : 1650, Loss : 0.40789, Acc : 0.847, Sensitive_Loss : 0.14933, Sensitive_Acc : 22.400, Run Time : 7.08 sec
INFO:root:2024-04-11 22:26:40, Train, Epoch : 3, Step : 1660, Loss : 0.44378, Acc : 0.812, Sensitive_Loss : 0.16623, Sensitive_Acc : 20.200, Run Time : 7.00 sec
INFO:root:2024-04-11 22:26:47, Train, Epoch : 3, Step : 1670, Loss : 0.46424, Acc : 0.775, Sensitive_Loss : 0.19332, Sensitive_Acc : 19.100, Run Time : 6.90 sec
INFO:root:2024-04-11 22:26:54, Train, Epoch : 3, Step : 1680, Loss : 0.46062, Acc : 0.828, Sensitive_Loss : 0.21325, Sensitive_Acc : 23.700, Run Time : 7.19 sec
INFO:root:2024-04-11 22:27:01, Train, Epoch : 3, Step : 1690, Loss : 0.51544, Acc : 0.775, Sensitive_Loss : 0.23890, Sensitive_Acc : 21.200, Run Time : 7.22 sec
INFO:root:2024-04-11 22:27:08, Train, Epoch : 3, Step : 1700, Loss : 0.35647, Acc : 0.841, Sensitive_Loss : 0.25456, Sensitive_Acc : 20.400, Run Time : 7.14 sec
INFO:root:2024-04-11 22:28:36, Dev, Step : 1700, Loss : 0.52599, Acc : 0.772, Auc : 0.851, Sensitive_Loss : 0.26552, Sensitive_Acc : 21.120, Sensitive_Auc : 0.985, Mean auc: 0.851, Run Time : 87.82 sec
INFO:root:2024-04-11 22:28:42, Train, Epoch : 3, Step : 1710, Loss : 0.45958, Acc : 0.809, Sensitive_Loss : 0.19454, Sensitive_Acc : 24.200, Run Time : 93.26 sec
INFO:root:2024-04-11 22:28:49, Train, Epoch : 3, Step : 1720, Loss : 0.42473, Acc : 0.822, Sensitive_Loss : 0.25285, Sensitive_Acc : 22.100, Run Time : 7.18 sec
INFO:root:2024-04-11 22:28:56, Train, Epoch : 3, Step : 1730, Loss : 0.49085, Acc : 0.775, Sensitive_Loss : 0.22780, Sensitive_Acc : 20.400, Run Time : 7.03 sec
INFO:root:2024-04-11 22:29:03, Train, Epoch : 3, Step : 1740, Loss : 0.43576, Acc : 0.822, Sensitive_Loss : 0.20372, Sensitive_Acc : 16.100, Run Time : 7.42 sec
INFO:root:2024-04-11 22:29:10, Train, Epoch : 3, Step : 1750, Loss : 0.47227, Acc : 0.784, Sensitive_Loss : 0.25840, Sensitive_Acc : 25.200, Run Time : 7.10 sec
INFO:root:2024-04-11 22:29:18, Train, Epoch : 3, Step : 1760, Loss : 0.46399, Acc : 0.803, Sensitive_Loss : 0.17815, Sensitive_Acc : 22.400, Run Time : 7.08 sec
INFO:root:2024-04-11 22:29:24, Train, Epoch : 3, Step : 1770, Loss : 0.37526, Acc : 0.822, Sensitive_Loss : 0.24052, Sensitive_Acc : 21.600, Run Time : 6.71 sec
INFO:root:2024-04-11 22:29:31, Train, Epoch : 3, Step : 1780, Loss : 0.36687, Acc : 0.838, Sensitive_Loss : 0.19384, Sensitive_Acc : 23.500, Run Time : 6.98 sec
INFO:root:2024-04-11 22:29:38, Train, Epoch : 3, Step : 1790, Loss : 0.43255, Acc : 0.828, Sensitive_Loss : 0.20803, Sensitive_Acc : 18.700, Run Time : 7.12 sec
INFO:root:2024-04-11 22:29:46, Train, Epoch : 3, Step : 1800, Loss : 0.41842, Acc : 0.822, Sensitive_Loss : 0.16535, Sensitive_Acc : 23.000, Run Time : 7.31 sec
INFO:root:2024-04-11 22:31:14, Dev, Step : 1800, Loss : 0.50769, Acc : 0.776, Auc : 0.857, Sensitive_Loss : 0.25660, Sensitive_Acc : 21.030, Sensitive_Auc : 0.986, Mean auc: 0.857, Run Time : 87.85 sec
INFO:root:2024-04-11 22:31:14, Best, Step : 1800, Loss : 0.50769, Acc : 0.776, Auc : 0.857, Sensitive_Loss : 0.25660, Sensitive_Acc : 21.030, Sensitive_Auc : 0.986, Best Auc : 0.857
INFO:root:2024-04-11 22:31:20, Train, Epoch : 3, Step : 1810, Loss : 0.41292, Acc : 0.828, Sensitive_Loss : 0.18441, Sensitive_Acc : 20.600, Run Time : 94.48 sec
INFO:root:2024-04-11 22:31:27, Train, Epoch : 3, Step : 1820, Loss : 0.43443, Acc : 0.791, Sensitive_Loss : 0.13181, Sensitive_Acc : 21.400, Run Time : 6.71 sec
INFO:root:2024-04-11 22:31:34, Train, Epoch : 3, Step : 1830, Loss : 0.44752, Acc : 0.781, Sensitive_Loss : 0.18910, Sensitive_Acc : 20.500, Run Time : 6.96 sec
INFO:root:2024-04-11 22:31:41, Train, Epoch : 3, Step : 1840, Loss : 0.41855, Acc : 0.812, Sensitive_Loss : 0.23748, Sensitive_Acc : 19.800, Run Time : 7.06 sec
INFO:root:2024-04-11 22:31:48, Train, Epoch : 3, Step : 1850, Loss : 0.37249, Acc : 0.847, Sensitive_Loss : 0.16112, Sensitive_Acc : 25.200, Run Time : 7.58 sec
INFO:root:2024-04-11 22:31:55, Train, Epoch : 3, Step : 1860, Loss : 0.41073, Acc : 0.825, Sensitive_Loss : 0.17912, Sensitive_Acc : 20.900, Run Time : 6.78 sec
INFO:root:2024-04-11 22:32:02, Train, Epoch : 3, Step : 1870, Loss : 0.45550, Acc : 0.809, Sensitive_Loss : 0.20401, Sensitive_Acc : 18.500, Run Time : 6.76 sec
INFO:root:2024-04-11 22:32:09, Train, Epoch : 3, Step : 1880, Loss : 0.43859, Acc : 0.844, Sensitive_Loss : 0.15480, Sensitive_Acc : 18.800, Run Time : 6.98 sec
INFO:root:2024-04-11 22:32:16, Train, Epoch : 3, Step : 1890, Loss : 0.46654, Acc : 0.784, Sensitive_Loss : 0.25451, Sensitive_Acc : 24.500, Run Time : 7.35 sec
INFO:root:2024-04-11 22:32:24, Train, Epoch : 3, Step : 1900, Loss : 0.43957, Acc : 0.797, Sensitive_Loss : 0.15959, Sensitive_Acc : 24.100, Run Time : 7.28 sec
INFO:root:2024-04-11 22:33:51, Dev, Step : 1900, Loss : 0.49998, Acc : 0.777, Auc : 0.860, Sensitive_Loss : 0.24811, Sensitive_Acc : 21.120, Sensitive_Auc : 0.985, Mean auc: 0.860, Run Time : 87.02 sec
INFO:root:2024-04-11 22:33:51, Best, Step : 1900, Loss : 0.49998, Acc : 0.777, Auc : 0.860, Sensitive_Loss : 0.24811, Sensitive_Acc : 21.120, Sensitive_Auc : 0.985, Best Auc : 0.860
INFO:root:2024-04-11 22:35:18
INFO:root:y_pred: [0.285991   0.0192932  0.08254953 ... 0.17914653 0.03812969 0.05358024]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [4.4644682e-04 1.9295032e-03 2.7738528e-03 1.9744091e-02 1.5426333e-02
 3.9958954e-03 9.6311048e-03 9.6192230e-03 3.3601932e-02 9.9986851e-01
 4.3750015e-01 3.9253938e-03 8.8634221e-03 3.0590722e-04 9.9867451e-01
 1.4224814e-01 5.8494125e-02 9.9908221e-01 9.9986482e-01 7.6287205e-04
 4.8765534e-01 1.7806607e-03 1.6880210e-01 2.2749994e-02 2.2649944e-01
 4.9455470e-01 5.5147721e-05 7.0175761e-04 8.6760183e-04 5.3817704e-02
 4.3860480e-02 9.8250842e-01 8.1617720e-02 3.9629701e-01 3.1677778e-03
 1.2909854e-03 7.3158802e-03 1.6921329e-01 1.2379464e-01 1.3835418e-01
 6.0642436e-02 9.7658914e-01 2.2405216e-03 1.2279588e-03 8.1611049e-01
 6.3315398e-01 5.1572984e-01 4.6981502e-01 5.0849086e-01 9.7156477e-01
 9.6242225e-01 9.9986446e-01 9.3221682e-01 5.7524769e-03 2.4029152e-01
 1.3410696e-01 2.0478973e-02 5.1543852e-03 9.9574775e-01 7.4281897e-03
 4.0919342e-04 3.5917128e-03 1.3553234e-02 1.4856700e-03 9.8307735e-01
 8.9970119e-02 2.3364772e-03 3.6560428e-01 2.8082928e-02 9.8118925e-01
 9.9323034e-01 9.9988818e-01 7.7585676e-03 7.0395446e-01 6.9991276e-03
 8.2925785e-01 3.3162013e-02 8.2463061e-04 3.5023917e-02 4.6359543e-03
 5.1240567e-02 1.3432500e-03 9.9883312e-01 9.9088228e-01 1.2453183e-02
 4.5483030e-02 8.8898107e-02 1.5443211e-02 2.1395246e-02 1.8027681e-03
 1.2646930e-01 4.4838092e-01 1.2186928e-04 4.4546148e-04 6.4556464e-03
 1.0007399e-02 3.7109666e-04 3.7273040e-01 5.4010870e-03 1.0327968e-02
 4.3651829e-03 2.5519978e-02 1.8870275e-01 4.2342683e-03 5.5448115e-02
 1.0736564e-02 2.7510989e-01 4.3471736e-01 5.6692946e-01 4.0381736e-01
 2.4583742e-03 9.9977976e-01 9.9808156e-01 4.8561249e-04 5.7873946e-01
 1.1597360e-01 1.3562292e-01 6.1905398e-03 4.2605570e-01 3.6038321e-02
 1.0926377e-02 3.2617333e-03 7.1707994e-02 3.1723978e-04 1.9010929e-02
 9.2030829e-01 7.9159264e-04 9.9172556e-01 8.4770039e-02 2.4919525e-01
 3.4075608e-03 7.2890706e-02 1.0790388e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 22:35:18, Dev, Step : 1902, Loss : 0.50408, Acc : 0.780, Auc : 0.860, Sensitive_Loss : 0.24237, Sensitive_Acc : 21.316, Sensitive_Auc : 0.984, Mean auc: 0.860, Run Time : 86.29 sec
INFO:root:2024-04-11 22:35:26, Train, Epoch : 4, Step : 1910, Loss : 0.36867, Acc : 0.666, Sensitive_Loss : 0.14401, Sensitive_Acc : 15.000, Run Time : 6.71 sec
INFO:root:2024-04-11 22:35:34, Train, Epoch : 4, Step : 1920, Loss : 0.47605, Acc : 0.812, Sensitive_Loss : 0.15416, Sensitive_Acc : 21.400, Run Time : 7.39 sec
INFO:root:2024-04-11 22:35:41, Train, Epoch : 4, Step : 1930, Loss : 0.46864, Acc : 0.787, Sensitive_Loss : 0.16686, Sensitive_Acc : 19.200, Run Time : 7.51 sec
INFO:root:2024-04-11 22:35:48, Train, Epoch : 4, Step : 1940, Loss : 0.49595, Acc : 0.778, Sensitive_Loss : 0.15632, Sensitive_Acc : 24.200, Run Time : 6.87 sec
INFO:root:2024-04-11 22:35:55, Train, Epoch : 4, Step : 1950, Loss : 0.35369, Acc : 0.856, Sensitive_Loss : 0.16837, Sensitive_Acc : 22.300, Run Time : 7.10 sec
INFO:root:2024-04-11 22:36:02, Train, Epoch : 4, Step : 1960, Loss : 0.40735, Acc : 0.819, Sensitive_Loss : 0.18722, Sensitive_Acc : 17.800, Run Time : 7.16 sec
INFO:root:2024-04-11 22:36:09, Train, Epoch : 4, Step : 1970, Loss : 0.38995, Acc : 0.806, Sensitive_Loss : 0.25897, Sensitive_Acc : 21.200, Run Time : 6.90 sec
INFO:root:2024-04-11 22:36:16, Train, Epoch : 4, Step : 1980, Loss : 0.40876, Acc : 0.822, Sensitive_Loss : 0.18297, Sensitive_Acc : 20.000, Run Time : 6.94 sec
INFO:root:2024-04-11 22:36:24, Train, Epoch : 4, Step : 1990, Loss : 0.35845, Acc : 0.819, Sensitive_Loss : 0.11472, Sensitive_Acc : 21.000, Run Time : 7.55 sec
INFO:root:2024-04-11 22:36:31, Train, Epoch : 4, Step : 2000, Loss : 0.44910, Acc : 0.797, Sensitive_Loss : 0.18276, Sensitive_Acc : 20.000, Run Time : 7.16 sec
INFO:root:2024-04-11 22:37:59, Dev, Step : 2000, Loss : 0.51550, Acc : 0.776, Auc : 0.860, Sensitive_Loss : 0.29924, Sensitive_Acc : 20.083, Sensitive_Auc : 0.986, Mean auc: 0.860, Run Time : 87.93 sec
INFO:root:2024-04-11 22:38:00, Best, Step : 2000, Loss : 0.51550, Acc : 0.776, Auc : 0.860, Sensitive_Loss : 0.29924, Sensitive_Acc : 20.083, Sensitive_Auc : 0.986, Best Auc : 0.860
INFO:root:2024-04-11 22:38:05, Train, Epoch : 4, Step : 2010, Loss : 0.42457, Acc : 0.794, Sensitive_Loss : 0.20604, Sensitive_Acc : 21.500, Run Time : 94.33 sec
INFO:root:2024-04-11 22:38:12, Train, Epoch : 4, Step : 2020, Loss : 0.38330, Acc : 0.819, Sensitive_Loss : 0.16576, Sensitive_Acc : 23.100, Run Time : 6.98 sec
INFO:root:2024-04-11 22:38:19, Train, Epoch : 4, Step : 2030, Loss : 0.35842, Acc : 0.831, Sensitive_Loss : 0.21870, Sensitive_Acc : 20.100, Run Time : 6.79 sec
INFO:root:2024-04-11 22:38:26, Train, Epoch : 4, Step : 2040, Loss : 0.44140, Acc : 0.791, Sensitive_Loss : 0.23677, Sensitive_Acc : 23.700, Run Time : 7.35 sec
INFO:root:2024-04-11 22:38:34, Train, Epoch : 4, Step : 2050, Loss : 0.36900, Acc : 0.863, Sensitive_Loss : 0.15149, Sensitive_Acc : 25.100, Run Time : 7.84 sec
INFO:root:2024-04-11 22:38:40, Train, Epoch : 4, Step : 2060, Loss : 0.42554, Acc : 0.816, Sensitive_Loss : 0.27415, Sensitive_Acc : 19.500, Run Time : 6.44 sec
INFO:root:2024-04-11 22:38:47, Train, Epoch : 4, Step : 2070, Loss : 0.40981, Acc : 0.769, Sensitive_Loss : 0.13118, Sensitive_Acc : 21.800, Run Time : 7.01 sec
INFO:root:2024-04-11 22:38:54, Train, Epoch : 4, Step : 2080, Loss : 0.44015, Acc : 0.806, Sensitive_Loss : 0.17307, Sensitive_Acc : 23.200, Run Time : 6.86 sec
INFO:root:2024-04-11 22:39:01, Train, Epoch : 4, Step : 2090, Loss : 0.39505, Acc : 0.850, Sensitive_Loss : 0.24737, Sensitive_Acc : 24.500, Run Time : 7.05 sec
INFO:root:2024-04-11 22:39:09, Train, Epoch : 4, Step : 2100, Loss : 0.41636, Acc : 0.809, Sensitive_Loss : 0.29664, Sensitive_Acc : 22.700, Run Time : 7.40 sec
INFO:root:2024-04-11 22:40:37, Dev, Step : 2100, Loss : 0.52716, Acc : 0.774, Auc : 0.860, Sensitive_Loss : 0.25439, Sensitive_Acc : 20.489, Sensitive_Auc : 0.985, Mean auc: 0.860, Run Time : 87.85 sec
INFO:root:2024-04-11 22:40:37, Best, Step : 2100, Loss : 0.52716, Acc : 0.774, Auc : 0.860, Sensitive_Loss : 0.25439, Sensitive_Acc : 20.489, Sensitive_Auc : 0.985, Best Auc : 0.860
INFO:root:2024-04-11 22:40:43, Train, Epoch : 4, Step : 2110, Loss : 0.40464, Acc : 0.816, Sensitive_Loss : 0.16211, Sensitive_Acc : 17.500, Run Time : 94.19 sec
INFO:root:2024-04-11 22:40:50, Train, Epoch : 4, Step : 2120, Loss : 0.50697, Acc : 0.784, Sensitive_Loss : 0.19486, Sensitive_Acc : 26.100, Run Time : 6.86 sec
INFO:root:2024-04-11 22:40:57, Train, Epoch : 4, Step : 2130, Loss : 0.41202, Acc : 0.812, Sensitive_Loss : 0.19467, Sensitive_Acc : 20.700, Run Time : 7.22 sec
INFO:root:2024-04-11 22:41:04, Train, Epoch : 4, Step : 2140, Loss : 0.49047, Acc : 0.778, Sensitive_Loss : 0.17856, Sensitive_Acc : 20.300, Run Time : 6.67 sec
INFO:root:2024-04-11 22:41:12, Train, Epoch : 4, Step : 2150, Loss : 0.43847, Acc : 0.819, Sensitive_Loss : 0.23233, Sensitive_Acc : 21.500, Run Time : 7.78 sec
INFO:root:2024-04-11 22:41:19, Train, Epoch : 4, Step : 2160, Loss : 0.36532, Acc : 0.803, Sensitive_Loss : 0.14382, Sensitive_Acc : 20.000, Run Time : 7.14 sec
INFO:root:2024-04-11 22:41:25, Train, Epoch : 4, Step : 2170, Loss : 0.39560, Acc : 0.825, Sensitive_Loss : 0.18362, Sensitive_Acc : 22.400, Run Time : 6.85 sec
INFO:root:2024-04-11 22:41:33, Train, Epoch : 4, Step : 2180, Loss : 0.40833, Acc : 0.812, Sensitive_Loss : 0.21533, Sensitive_Acc : 21.700, Run Time : 7.33 sec
INFO:root:2024-04-11 22:41:40, Train, Epoch : 4, Step : 2190, Loss : 0.43183, Acc : 0.797, Sensitive_Loss : 0.19336, Sensitive_Acc : 21.200, Run Time : 7.07 sec
INFO:root:2024-04-11 22:41:47, Train, Epoch : 4, Step : 2200, Loss : 0.41742, Acc : 0.825, Sensitive_Loss : 0.12371, Sensitive_Acc : 20.700, Run Time : 7.08 sec
INFO:root:2024-04-11 22:43:15, Dev, Step : 2200, Loss : 0.50616, Acc : 0.778, Auc : 0.860, Sensitive_Loss : 0.27275, Sensitive_Acc : 20.218, Sensitive_Auc : 0.983, Mean auc: 0.860, Run Time : 87.92 sec
INFO:root:2024-04-11 22:43:16, Best, Step : 2200, Loss : 0.50616, Acc : 0.778, Auc : 0.860, Sensitive_Loss : 0.27275, Sensitive_Acc : 20.218, Sensitive_Auc : 0.983, Best Auc : 0.860
INFO:root:2024-04-11 22:43:21, Train, Epoch : 4, Step : 2210, Loss : 0.46640, Acc : 0.791, Sensitive_Loss : 0.20329, Sensitive_Acc : 19.800, Run Time : 93.92 sec
INFO:root:2024-04-11 22:43:28, Train, Epoch : 4, Step : 2220, Loss : 0.50481, Acc : 0.781, Sensitive_Loss : 0.16270, Sensitive_Acc : 19.200, Run Time : 7.43 sec
INFO:root:2024-04-11 22:43:36, Train, Epoch : 4, Step : 2230, Loss : 0.44297, Acc : 0.809, Sensitive_Loss : 0.15032, Sensitive_Acc : 18.300, Run Time : 7.30 sec
INFO:root:2024-04-11 22:43:42, Train, Epoch : 4, Step : 2240, Loss : 0.42223, Acc : 0.816, Sensitive_Loss : 0.14088, Sensitive_Acc : 24.500, Run Time : 6.70 sec
INFO:root:2024-04-11 22:43:49, Train, Epoch : 4, Step : 2250, Loss : 0.44362, Acc : 0.787, Sensitive_Loss : 0.29447, Sensitive_Acc : 22.700, Run Time : 6.92 sec
INFO:root:2024-04-11 22:43:56, Train, Epoch : 4, Step : 2260, Loss : 0.39514, Acc : 0.822, Sensitive_Loss : 0.20443, Sensitive_Acc : 22.600, Run Time : 6.81 sec
INFO:root:2024-04-11 22:44:04, Train, Epoch : 4, Step : 2270, Loss : 0.42193, Acc : 0.806, Sensitive_Loss : 0.27031, Sensitive_Acc : 17.700, Run Time : 7.59 sec
INFO:root:2024-04-11 22:44:11, Train, Epoch : 4, Step : 2280, Loss : 0.42342, Acc : 0.809, Sensitive_Loss : 0.16115, Sensitive_Acc : 22.800, Run Time : 6.88 sec
INFO:root:2024-04-11 22:44:17, Train, Epoch : 4, Step : 2290, Loss : 0.46195, Acc : 0.775, Sensitive_Loss : 0.25544, Sensitive_Acc : 19.200, Run Time : 6.85 sec
INFO:root:2024-04-11 22:44:25, Train, Epoch : 4, Step : 2300, Loss : 0.41783, Acc : 0.822, Sensitive_Loss : 0.17107, Sensitive_Acc : 25.500, Run Time : 7.63 sec
INFO:root:2024-04-11 22:45:52, Dev, Step : 2300, Loss : 0.50637, Acc : 0.781, Auc : 0.860, Sensitive_Loss : 0.25676, Sensitive_Acc : 21.000, Sensitive_Auc : 0.983, Mean auc: 0.860, Run Time : 87.34 sec
INFO:root:2024-04-11 22:45:53, Best, Step : 2300, Loss : 0.50637, Acc : 0.781, Auc : 0.860, Sensitive_Loss : 0.25676, Sensitive_Acc : 21.000, Sensitive_Auc : 0.983, Best Auc : 0.860
INFO:root:2024-04-11 22:45:58, Train, Epoch : 4, Step : 2310, Loss : 0.39702, Acc : 0.809, Sensitive_Loss : 0.13203, Sensitive_Acc : 23.300, Run Time : 93.45 sec
INFO:root:2024-04-11 22:46:05, Train, Epoch : 4, Step : 2320, Loss : 0.43163, Acc : 0.841, Sensitive_Loss : 0.21314, Sensitive_Acc : 16.300, Run Time : 6.99 sec
INFO:root:2024-04-11 22:46:13, Train, Epoch : 4, Step : 2330, Loss : 0.50191, Acc : 0.781, Sensitive_Loss : 0.18017, Sensitive_Acc : 15.500, Run Time : 7.21 sec
INFO:root:2024-04-11 22:46:20, Train, Epoch : 4, Step : 2340, Loss : 0.47384, Acc : 0.787, Sensitive_Loss : 0.17337, Sensitive_Acc : 24.100, Run Time : 6.97 sec
INFO:root:2024-04-11 22:46:27, Train, Epoch : 4, Step : 2350, Loss : 0.45085, Acc : 0.803, Sensitive_Loss : 0.17508, Sensitive_Acc : 20.600, Run Time : 7.62 sec
INFO:root:2024-04-11 22:46:34, Train, Epoch : 4, Step : 2360, Loss : 0.35703, Acc : 0.875, Sensitive_Loss : 0.15747, Sensitive_Acc : 26.900, Run Time : 6.50 sec
INFO:root:2024-04-11 22:46:41, Train, Epoch : 4, Step : 2370, Loss : 0.40384, Acc : 0.778, Sensitive_Loss : 0.16774, Sensitive_Acc : 22.600, Run Time : 7.61 sec
INFO:root:2024-04-11 22:46:48, Train, Epoch : 4, Step : 2380, Loss : 0.41676, Acc : 0.831, Sensitive_Loss : 0.14155, Sensitive_Acc : 21.300, Run Time : 6.83 sec
INFO:root:2024-04-11 22:46:56, Train, Epoch : 4, Step : 2390, Loss : 0.41113, Acc : 0.797, Sensitive_Loss : 0.21760, Sensitive_Acc : 22.600, Run Time : 7.55 sec
INFO:root:2024-04-11 22:47:02, Train, Epoch : 4, Step : 2400, Loss : 0.40062, Acc : 0.844, Sensitive_Loss : 0.20792, Sensitive_Acc : 15.200, Run Time : 6.53 sec
INFO:root:2024-04-11 22:48:30, Dev, Step : 2400, Loss : 0.49762, Acc : 0.783, Auc : 0.863, Sensitive_Loss : 0.23211, Sensitive_Acc : 21.241, Sensitive_Auc : 0.983, Mean auc: 0.863, Run Time : 87.90 sec
INFO:root:2024-04-11 22:48:31, Best, Step : 2400, Loss : 0.49762, Acc : 0.783, Auc : 0.863, Sensitive_Loss : 0.23211, Sensitive_Acc : 21.241, Sensitive_Auc : 0.983, Best Auc : 0.863
INFO:root:2024-04-11 22:48:37, Train, Epoch : 4, Step : 2410, Loss : 0.45315, Acc : 0.781, Sensitive_Loss : 0.14260, Sensitive_Acc : 21.700, Run Time : 94.30 sec
INFO:root:2024-04-11 22:48:44, Train, Epoch : 4, Step : 2420, Loss : 0.34226, Acc : 0.797, Sensitive_Loss : 0.15879, Sensitive_Acc : 21.800, Run Time : 7.11 sec
INFO:root:2024-04-11 22:48:51, Train, Epoch : 4, Step : 2430, Loss : 0.42087, Acc : 0.825, Sensitive_Loss : 0.13671, Sensitive_Acc : 25.500, Run Time : 7.51 sec
INFO:root:2024-04-11 22:48:58, Train, Epoch : 4, Step : 2440, Loss : 0.43905, Acc : 0.819, Sensitive_Loss : 0.16169, Sensitive_Acc : 20.200, Run Time : 7.20 sec
INFO:root:2024-04-11 22:49:06, Train, Epoch : 4, Step : 2450, Loss : 0.43449, Acc : 0.828, Sensitive_Loss : 0.13834, Sensitive_Acc : 13.500, Run Time : 7.22 sec
INFO:root:2024-04-11 22:49:12, Train, Epoch : 4, Step : 2460, Loss : 0.40385, Acc : 0.816, Sensitive_Loss : 0.19078, Sensitive_Acc : 24.800, Run Time : 6.59 sec
INFO:root:2024-04-11 22:49:20, Train, Epoch : 4, Step : 2470, Loss : 0.43745, Acc : 0.797, Sensitive_Loss : 0.18114, Sensitive_Acc : 23.400, Run Time : 7.29 sec
INFO:root:2024-04-11 22:49:27, Train, Epoch : 4, Step : 2480, Loss : 0.41004, Acc : 0.819, Sensitive_Loss : 0.15593, Sensitive_Acc : 23.300, Run Time : 7.06 sec
INFO:root:2024-04-11 22:49:34, Train, Epoch : 4, Step : 2490, Loss : 0.46314, Acc : 0.800, Sensitive_Loss : 0.16973, Sensitive_Acc : 19.000, Run Time : 6.96 sec
INFO:root:2024-04-11 22:49:41, Train, Epoch : 4, Step : 2500, Loss : 0.36093, Acc : 0.812, Sensitive_Loss : 0.12181, Sensitive_Acc : 22.200, Run Time : 7.48 sec
INFO:root:2024-04-11 22:51:09, Dev, Step : 2500, Loss : 0.49635, Acc : 0.782, Auc : 0.864, Sensitive_Loss : 0.24375, Sensitive_Acc : 20.985, Sensitive_Auc : 0.984, Mean auc: 0.864, Run Time : 88.08 sec
INFO:root:2024-04-11 22:51:10, Best, Step : 2500, Loss : 0.49635, Acc : 0.782, Auc : 0.864, Sensitive_Loss : 0.24375, Sensitive_Acc : 20.985, Sensitive_Auc : 0.984, Best Auc : 0.864
INFO:root:2024-04-11 22:51:15, Train, Epoch : 4, Step : 2510, Loss : 0.39352, Acc : 0.828, Sensitive_Loss : 0.22317, Sensitive_Acc : 20.000, Run Time : 94.13 sec
INFO:root:2024-04-11 22:51:22, Train, Epoch : 4, Step : 2520, Loss : 0.42877, Acc : 0.822, Sensitive_Loss : 0.14805, Sensitive_Acc : 17.900, Run Time : 6.86 sec
INFO:root:2024-04-11 22:51:30, Train, Epoch : 4, Step : 2530, Loss : 0.47808, Acc : 0.806, Sensitive_Loss : 0.24250, Sensitive_Acc : 23.200, Run Time : 7.53 sec
INFO:root:2024-04-11 22:53:00
INFO:root:y_pred: [0.06263649 0.01349758 0.13253985 ... 0.2084045  0.03784093 0.03748744]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [6.11940457e-04 1.94291596e-03 4.55429312e-03 1.27148405e-02
 1.29932687e-02 2.16048188e-03 5.91213070e-03 1.28441332e-02
 4.39606756e-02 9.99870181e-01 6.22830927e-01 9.72975791e-03
 7.37784198e-03 3.59150494e-04 9.99121249e-01 1.36004537e-01
 2.67748348e-02 9.99230027e-01 9.99959946e-01 1.30058639e-03
 4.23091590e-01 2.11848365e-03 4.57089365e-01 4.34227660e-02
 2.23199695e-01 6.74483120e-01 4.16438161e-05 3.94280854e-04
 7.02262798e-04 6.75539300e-02 4.54433411e-02 9.82880056e-01
 9.22219381e-02 6.24296844e-01 3.64216603e-03 1.81859045e-03
 1.17578134e-02 2.30610490e-01 1.84727311e-01 1.24491781e-01
 6.42443150e-02 9.83977854e-01 3.82277975e-03 3.84886982e-03
 8.10133994e-01 5.45585692e-01 6.68475091e-01 4.09124941e-01
 7.63821423e-01 9.88839090e-01 9.88078892e-01 9.99963403e-01
 9.53393698e-01 6.21730229e-03 3.56201977e-01 3.31975371e-01
 1.99309345e-02 9.83248837e-03 9.98276591e-01 8.70240852e-03
 5.88464900e-04 5.54279191e-03 1.82739254e-02 3.01387743e-03
 9.94109869e-01 1.23523101e-01 2.37771752e-03 5.23235202e-01
 3.15028727e-02 9.88266170e-01 9.94695365e-01 9.99928713e-01
 6.84222206e-03 7.23199368e-01 7.93720875e-03 8.57683539e-01
 6.85847700e-02 5.24570642e-04 6.18767776e-02 1.84807589e-03
 8.59204307e-02 4.38766059e-04 9.99612510e-01 9.93713439e-01
 1.32775297e-02 4.15327884e-02 1.41379535e-01 1.15879364e-02
 5.99992275e-02 4.01503081e-03 4.94758226e-02 5.31095982e-01
 9.17540165e-05 5.31787402e-04 1.39791612e-02 1.39154010e-02
 8.30450503e-04 2.99677908e-01 4.86201746e-03 2.12031715e-02
 8.58658552e-03 2.08829232e-02 2.16192052e-01 5.85437473e-03
 3.14647332e-02 1.83091350e-02 4.64446932e-01 5.01154780e-01
 6.12332821e-01 3.80354315e-01 1.75535865e-03 9.99851942e-01
 9.98751521e-01 6.71869551e-04 6.53516710e-01 1.05678692e-01
 1.36948168e-01 1.06625212e-02 5.02989531e-01 2.60085370e-02
 1.66335236e-02 6.52158167e-03 1.06770240e-01 4.07602260e-04
 3.16902697e-02 9.47153389e-01 1.03598798e-03 9.96743143e-01
 1.07538521e-01 2.28395030e-01 5.90987224e-03 9.06701460e-02
 1.27551015e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 22:53:00, Dev, Step : 2536, Loss : 0.51207, Acc : 0.777, Auc : 0.863, Sensitive_Loss : 0.26615, Sensitive_Acc : 20.444, Sensitive_Auc : 0.983, Mean auc: 0.863, Run Time : 87.21 sec
INFO:root:2024-04-11 22:53:05, Train, Epoch : 5, Step : 2540, Loss : 0.13696, Acc : 0.338, Sensitive_Loss : 0.06455, Sensitive_Acc : 10.000, Run Time : 4.25 sec
INFO:root:2024-04-11 22:53:12, Train, Epoch : 5, Step : 2550, Loss : 0.32163, Acc : 0.872, Sensitive_Loss : 0.16849, Sensitive_Acc : 17.300, Run Time : 7.27 sec
INFO:root:2024-04-11 22:53:19, Train, Epoch : 5, Step : 2560, Loss : 0.34331, Acc : 0.806, Sensitive_Loss : 0.13483, Sensitive_Acc : 23.700, Run Time : 6.72 sec
INFO:root:2024-04-11 22:53:26, Train, Epoch : 5, Step : 2570, Loss : 0.43581, Acc : 0.816, Sensitive_Loss : 0.10666, Sensitive_Acc : 18.600, Run Time : 7.05 sec
INFO:root:2024-04-11 22:53:33, Train, Epoch : 5, Step : 2580, Loss : 0.36125, Acc : 0.841, Sensitive_Loss : 0.19699, Sensitive_Acc : 24.400, Run Time : 7.26 sec
INFO:root:2024-04-11 22:53:41, Train, Epoch : 5, Step : 2590, Loss : 0.40795, Acc : 0.828, Sensitive_Loss : 0.14103, Sensitive_Acc : 26.500, Run Time : 7.15 sec
INFO:root:2024-04-11 22:53:48, Train, Epoch : 5, Step : 2600, Loss : 0.45593, Acc : 0.841, Sensitive_Loss : 0.18885, Sensitive_Acc : 20.200, Run Time : 6.93 sec
INFO:root:2024-04-11 22:55:16, Dev, Step : 2600, Loss : 0.49752, Acc : 0.787, Auc : 0.865, Sensitive_Loss : 0.24468, Sensitive_Acc : 21.135, Sensitive_Auc : 0.982, Mean auc: 0.865, Run Time : 88.16 sec
INFO:root:2024-04-11 22:55:16, Best, Step : 2600, Loss : 0.49752, Acc : 0.787, Auc : 0.865, Sensitive_Loss : 0.24468, Sensitive_Acc : 21.135, Sensitive_Auc : 0.982, Best Auc : 0.865
INFO:root:2024-04-11 22:55:22, Train, Epoch : 5, Step : 2610, Loss : 0.44609, Acc : 0.822, Sensitive_Loss : 0.15489, Sensitive_Acc : 22.500, Run Time : 94.69 sec
INFO:root:2024-04-11 22:55:29, Train, Epoch : 5, Step : 2620, Loss : 0.34018, Acc : 0.872, Sensitive_Loss : 0.20583, Sensitive_Acc : 23.500, Run Time : 7.00 sec
INFO:root:2024-04-11 22:55:37, Train, Epoch : 5, Step : 2630, Loss : 0.41182, Acc : 0.838, Sensitive_Loss : 0.13765, Sensitive_Acc : 25.000, Run Time : 7.30 sec
INFO:root:2024-04-11 22:55:43, Train, Epoch : 5, Step : 2640, Loss : 0.37751, Acc : 0.834, Sensitive_Loss : 0.21035, Sensitive_Acc : 24.300, Run Time : 6.81 sec
INFO:root:2024-04-11 22:55:51, Train, Epoch : 5, Step : 2650, Loss : 0.39732, Acc : 0.828, Sensitive_Loss : 0.22797, Sensitive_Acc : 21.600, Run Time : 7.29 sec
INFO:root:2024-04-11 22:55:58, Train, Epoch : 5, Step : 2660, Loss : 0.37828, Acc : 0.834, Sensitive_Loss : 0.37551, Sensitive_Acc : 21.400, Run Time : 7.01 sec
INFO:root:2024-04-11 22:56:05, Train, Epoch : 5, Step : 2670, Loss : 0.43073, Acc : 0.803, Sensitive_Loss : 0.17144, Sensitive_Acc : 20.100, Run Time : 7.31 sec
INFO:root:2024-04-11 22:56:12, Train, Epoch : 5, Step : 2680, Loss : 0.43322, Acc : 0.819, Sensitive_Loss : 0.19558, Sensitive_Acc : 22.400, Run Time : 7.27 sec
INFO:root:2024-04-11 22:56:19, Train, Epoch : 5, Step : 2690, Loss : 0.41874, Acc : 0.825, Sensitive_Loss : 0.17606, Sensitive_Acc : 19.700, Run Time : 6.74 sec
INFO:root:2024-04-11 22:56:26, Train, Epoch : 5, Step : 2700, Loss : 0.46821, Acc : 0.806, Sensitive_Loss : 0.13493, Sensitive_Acc : 22.500, Run Time : 7.39 sec
INFO:root:2024-04-11 22:57:54, Dev, Step : 2700, Loss : 0.51425, Acc : 0.776, Auc : 0.865, Sensitive_Loss : 0.25264, Sensitive_Acc : 20.970, Sensitive_Auc : 0.984, Mean auc: 0.865, Run Time : 87.59 sec
INFO:root:2024-04-11 22:58:00, Train, Epoch : 5, Step : 2710, Loss : 0.37469, Acc : 0.847, Sensitive_Loss : 0.14772, Sensitive_Acc : 21.000, Run Time : 93.63 sec
INFO:root:2024-04-11 22:58:07, Train, Epoch : 5, Step : 2720, Loss : 0.34503, Acc : 0.844, Sensitive_Loss : 0.15826, Sensitive_Acc : 22.500, Run Time : 6.75 sec
INFO:root:2024-04-11 22:58:14, Train, Epoch : 5, Step : 2730, Loss : 0.40037, Acc : 0.850, Sensitive_Loss : 0.19346, Sensitive_Acc : 19.600, Run Time : 7.08 sec
INFO:root:2024-04-11 22:58:21, Train, Epoch : 5, Step : 2740, Loss : 0.36174, Acc : 0.828, Sensitive_Loss : 0.24045, Sensitive_Acc : 22.900, Run Time : 6.97 sec
INFO:root:2024-04-11 22:58:28, Train, Epoch : 5, Step : 2750, Loss : 0.42024, Acc : 0.819, Sensitive_Loss : 0.13412, Sensitive_Acc : 16.700, Run Time : 7.21 sec
INFO:root:2024-04-11 22:58:35, Train, Epoch : 5, Step : 2760, Loss : 0.46654, Acc : 0.775, Sensitive_Loss : 0.15760, Sensitive_Acc : 18.600, Run Time : 7.15 sec
INFO:root:2024-04-11 22:58:42, Train, Epoch : 5, Step : 2770, Loss : 0.43334, Acc : 0.816, Sensitive_Loss : 0.10781, Sensitive_Acc : 21.300, Run Time : 7.16 sec
INFO:root:2024-04-11 22:58:49, Train, Epoch : 5, Step : 2780, Loss : 0.41996, Acc : 0.822, Sensitive_Loss : 0.11871, Sensitive_Acc : 21.800, Run Time : 7.16 sec
INFO:root:2024-04-11 22:58:57, Train, Epoch : 5, Step : 2790, Loss : 0.38383, Acc : 0.822, Sensitive_Loss : 0.16932, Sensitive_Acc : 24.400, Run Time : 7.84 sec
INFO:root:2024-04-11 22:59:04, Train, Epoch : 5, Step : 2800, Loss : 0.40584, Acc : 0.794, Sensitive_Loss : 0.17413, Sensitive_Acc : 20.600, Run Time : 6.74 sec
INFO:root:2024-04-11 23:00:32, Dev, Step : 2800, Loss : 0.50293, Acc : 0.778, Auc : 0.864, Sensitive_Loss : 0.21255, Sensitive_Acc : 21.271, Sensitive_Auc : 0.986, Mean auc: 0.864, Run Time : 88.01 sec
INFO:root:2024-04-11 23:00:37, Train, Epoch : 5, Step : 2810, Loss : 0.38061, Acc : 0.869, Sensitive_Loss : 0.18316, Sensitive_Acc : 17.700, Run Time : 93.44 sec
INFO:root:2024-04-11 23:00:45, Train, Epoch : 5, Step : 2820, Loss : 0.37124, Acc : 0.859, Sensitive_Loss : 0.19377, Sensitive_Acc : 24.200, Run Time : 7.52 sec
INFO:root:2024-04-11 23:00:52, Train, Epoch : 5, Step : 2830, Loss : 0.47047, Acc : 0.822, Sensitive_Loss : 0.19168, Sensitive_Acc : 21.600, Run Time : 6.98 sec
INFO:root:2024-04-11 23:00:59, Train, Epoch : 5, Step : 2840, Loss : 0.40721, Acc : 0.797, Sensitive_Loss : 0.16074, Sensitive_Acc : 20.400, Run Time : 7.08 sec
INFO:root:2024-04-11 23:01:06, Train, Epoch : 5, Step : 2850, Loss : 0.40647, Acc : 0.809, Sensitive_Loss : 0.21621, Sensitive_Acc : 21.100, Run Time : 7.21 sec
INFO:root:2024-04-11 23:01:13, Train, Epoch : 5, Step : 2860, Loss : 0.37063, Acc : 0.825, Sensitive_Loss : 0.11447, Sensitive_Acc : 25.300, Run Time : 7.11 sec
INFO:root:2024-04-11 23:01:20, Train, Epoch : 5, Step : 2870, Loss : 0.37136, Acc : 0.856, Sensitive_Loss : 0.22031, Sensitive_Acc : 18.700, Run Time : 7.07 sec
INFO:root:2024-04-11 23:01:28, Train, Epoch : 5, Step : 2880, Loss : 0.40211, Acc : 0.825, Sensitive_Loss : 0.10178, Sensitive_Acc : 20.800, Run Time : 7.38 sec
INFO:root:2024-04-11 23:01:35, Train, Epoch : 5, Step : 2890, Loss : 0.35324, Acc : 0.847, Sensitive_Loss : 0.22993, Sensitive_Acc : 22.000, Run Time : 7.57 sec
INFO:root:2024-04-11 23:01:42, Train, Epoch : 5, Step : 2900, Loss : 0.36986, Acc : 0.844, Sensitive_Loss : 0.17397, Sensitive_Acc : 24.900, Run Time : 6.63 sec
INFO:root:2024-04-11 23:03:10, Dev, Step : 2900, Loss : 0.51529, Acc : 0.780, Auc : 0.862, Sensitive_Loss : 0.26393, Sensitive_Acc : 20.865, Sensitive_Auc : 0.982, Mean auc: 0.862, Run Time : 88.18 sec
INFO:root:2024-04-11 23:03:16, Train, Epoch : 5, Step : 2910, Loss : 0.39784, Acc : 0.856, Sensitive_Loss : 0.18154, Sensitive_Acc : 17.400, Run Time : 93.72 sec
INFO:root:2024-04-11 23:03:23, Train, Epoch : 5, Step : 2920, Loss : 0.41267, Acc : 0.844, Sensitive_Loss : 0.24267, Sensitive_Acc : 24.400, Run Time : 7.48 sec
INFO:root:2024-04-11 23:03:30, Train, Epoch : 5, Step : 2930, Loss : 0.42649, Acc : 0.806, Sensitive_Loss : 0.15092, Sensitive_Acc : 23.300, Run Time : 7.10 sec
INFO:root:2024-04-11 23:03:37, Train, Epoch : 5, Step : 2940, Loss : 0.39313, Acc : 0.819, Sensitive_Loss : 0.14014, Sensitive_Acc : 18.000, Run Time : 6.85 sec
INFO:root:2024-04-11 23:03:45, Train, Epoch : 5, Step : 2950, Loss : 0.38671, Acc : 0.819, Sensitive_Loss : 0.19545, Sensitive_Acc : 25.100, Run Time : 7.33 sec
INFO:root:2024-04-11 23:03:51, Train, Epoch : 5, Step : 2960, Loss : 0.35024, Acc : 0.838, Sensitive_Loss : 0.15089, Sensitive_Acc : 19.500, Run Time : 6.60 sec
INFO:root:2024-04-11 23:03:58, Train, Epoch : 5, Step : 2970, Loss : 0.46053, Acc : 0.809, Sensitive_Loss : 0.18471, Sensitive_Acc : 26.700, Run Time : 7.36 sec
INFO:root:2024-04-11 23:04:06, Train, Epoch : 5, Step : 2980, Loss : 0.40528, Acc : 0.800, Sensitive_Loss : 0.19574, Sensitive_Acc : 24.600, Run Time : 7.15 sec
INFO:root:2024-04-11 23:04:13, Train, Epoch : 5, Step : 2990, Loss : 0.38032, Acc : 0.822, Sensitive_Loss : 0.13922, Sensitive_Acc : 22.300, Run Time : 7.02 sec
INFO:root:2024-04-11 23:04:20, Train, Epoch : 5, Step : 3000, Loss : 0.47799, Acc : 0.803, Sensitive_Loss : 0.17231, Sensitive_Acc : 19.300, Run Time : 7.14 sec
INFO:root:2024-04-11 23:05:49, Dev, Step : 3000, Loss : 0.50449, Acc : 0.784, Auc : 0.864, Sensitive_Loss : 0.21732, Sensitive_Acc : 21.662, Sensitive_Auc : 0.985, Mean auc: 0.864, Run Time : 88.84 sec
INFO:root:2024-04-11 23:05:54, Train, Epoch : 5, Step : 3010, Loss : 0.42095, Acc : 0.809, Sensitive_Loss : 0.13908, Sensitive_Acc : 16.400, Run Time : 94.65 sec
INFO:root:2024-04-11 23:06:02, Train, Epoch : 5, Step : 3020, Loss : 0.37594, Acc : 0.831, Sensitive_Loss : 0.12993, Sensitive_Acc : 18.400, Run Time : 7.10 sec
INFO:root:2024-04-11 23:06:08, Train, Epoch : 5, Step : 3030, Loss : 0.37460, Acc : 0.806, Sensitive_Loss : 0.16798, Sensitive_Acc : 22.500, Run Time : 6.54 sec
INFO:root:2024-04-11 23:06:16, Train, Epoch : 5, Step : 3040, Loss : 0.34545, Acc : 0.831, Sensitive_Loss : 0.23894, Sensitive_Acc : 17.300, Run Time : 7.88 sec
INFO:root:2024-04-11 23:06:22, Train, Epoch : 5, Step : 3050, Loss : 0.34169, Acc : 0.866, Sensitive_Loss : 0.14219, Sensitive_Acc : 24.200, Run Time : 6.46 sec
INFO:root:2024-04-11 23:06:30, Train, Epoch : 5, Step : 3060, Loss : 0.41630, Acc : 0.816, Sensitive_Loss : 0.19263, Sensitive_Acc : 21.700, Run Time : 7.21 sec
INFO:root:2024-04-11 23:06:37, Train, Epoch : 5, Step : 3070, Loss : 0.39680, Acc : 0.831, Sensitive_Loss : 0.16278, Sensitive_Acc : 24.100, Run Time : 6.97 sec
INFO:root:2024-04-11 23:06:44, Train, Epoch : 5, Step : 3080, Loss : 0.36471, Acc : 0.859, Sensitive_Loss : 0.19977, Sensitive_Acc : 19.300, Run Time : 7.06 sec
INFO:root:2024-04-11 23:06:51, Train, Epoch : 5, Step : 3090, Loss : 0.33951, Acc : 0.838, Sensitive_Loss : 0.14618, Sensitive_Acc : 22.600, Run Time : 7.36 sec
INFO:root:2024-04-11 23:06:58, Train, Epoch : 5, Step : 3100, Loss : 0.42274, Acc : 0.825, Sensitive_Loss : 0.15392, Sensitive_Acc : 21.500, Run Time : 7.14 sec
INFO:root:2024-04-11 23:08:26, Dev, Step : 3100, Loss : 0.50395, Acc : 0.785, Auc : 0.863, Sensitive_Loss : 0.22020, Sensitive_Acc : 21.271, Sensitive_Auc : 0.985, Mean auc: 0.863, Run Time : 87.87 sec
INFO:root:2024-04-11 23:08:32, Train, Epoch : 5, Step : 3110, Loss : 0.37251, Acc : 0.806, Sensitive_Loss : 0.24389, Sensitive_Acc : 22.300, Run Time : 93.68 sec
INFO:root:2024-04-11 23:08:39, Train, Epoch : 5, Step : 3120, Loss : 0.36690, Acc : 0.828, Sensitive_Loss : 0.24011, Sensitive_Acc : 26.100, Run Time : 7.20 sec
INFO:root:2024-04-11 23:08:46, Train, Epoch : 5, Step : 3130, Loss : 0.44313, Acc : 0.838, Sensitive_Loss : 0.14467, Sensitive_Acc : 25.900, Run Time : 6.49 sec
INFO:root:2024-04-11 23:08:53, Train, Epoch : 5, Step : 3140, Loss : 0.42170, Acc : 0.819, Sensitive_Loss : 0.15975, Sensitive_Acc : 25.900, Run Time : 7.54 sec
INFO:root:2024-04-11 23:09:00, Train, Epoch : 5, Step : 3150, Loss : 0.38273, Acc : 0.838, Sensitive_Loss : 0.17038, Sensitive_Acc : 18.800, Run Time : 7.32 sec
INFO:root:2024-04-11 23:09:07, Train, Epoch : 5, Step : 3160, Loss : 0.41878, Acc : 0.794, Sensitive_Loss : 0.14207, Sensitive_Acc : 23.900, Run Time : 6.81 sec
INFO:root:2024-04-11 23:09:14, Train, Epoch : 5, Step : 3170, Loss : 0.40593, Acc : 0.816, Sensitive_Loss : 0.19350, Sensitive_Acc : 24.800, Run Time : 6.67 sec
INFO:root:2024-04-11 23:10:42
INFO:root:y_pred: [0.10142858 0.01672563 0.17451246 ... 0.2205056  0.04554926 0.02674108]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [5.90436277e-04 7.98544206e-04 2.61854776e-03 9.19369329e-03
 1.11000501e-02 2.79310672e-03 7.92566780e-03 8.87434091e-03
 2.79355235e-02 9.99868870e-01 6.15974784e-01 3.16262199e-03
 4.41840570e-03 2.92989600e-04 9.99046385e-01 7.41294548e-02
 2.81595979e-02 9.99250352e-01 9.99940753e-01 5.48585609e-04
 2.11034879e-01 1.65158405e-03 2.60812610e-01 1.99867114e-02
 1.55348092e-01 4.15011317e-01 6.73207905e-05 5.65911003e-04
 5.97409438e-04 4.68841270e-02 2.63776798e-02 9.81603265e-01
 4.78997156e-02 6.27637684e-01 1.63304561e-03 4.10142587e-04
 3.98932910e-03 1.13978662e-01 1.33542955e-01 6.14844374e-02
 4.76722941e-02 9.85829294e-01 3.56297265e-03 1.01882755e-03
 7.77866781e-01 5.12057960e-01 5.54113507e-01 2.09769115e-01
 7.78156042e-01 9.75724638e-01 9.81994689e-01 9.99947548e-01
 8.90815675e-01 4.06355131e-03 2.40339547e-01 1.66535646e-01
 1.35250343e-02 7.90658779e-03 9.98721421e-01 2.27560848e-03
 1.78492250e-04 7.19372649e-03 8.07757210e-03 9.81206074e-04
 9.94585574e-01 1.05536856e-01 1.05733436e-03 4.21297938e-01
 1.93749238e-02 9.87205744e-01 9.94066596e-01 9.99919653e-01
 1.87887880e-03 6.19090736e-01 5.94290858e-03 8.03002238e-01
 6.68742806e-02 4.83039592e-04 4.09721322e-02 2.13912292e-03
 3.67387943e-02 2.86202558e-04 9.99696493e-01 9.92440343e-01
 1.06582157e-02 3.45330387e-02 1.52029023e-01 4.21044882e-03
 3.73551883e-02 3.44230537e-03 4.02857773e-02 3.11551452e-01
 1.09339344e-04 2.41122223e-04 5.94984507e-03 1.05509618e-02
 4.94786946e-04 3.73726279e-01 2.96455482e-03 2.37410180e-02
 8.35305266e-03 1.86224785e-02 4.17245537e-01 4.33069421e-03
 1.01396339e-02 4.00847057e-03 3.09436440e-01 4.57521230e-01
 4.57859516e-01 2.94528484e-01 7.49011990e-04 9.99722540e-01
 9.98706460e-01 3.02906410e-04 5.80332398e-01 7.14281946e-02
 4.70540188e-02 4.35334072e-03 4.72887278e-01 1.77221969e-02
 8.35910626e-03 3.42168123e-03 6.57269061e-02 2.52768776e-04
 1.04450667e-02 9.05736804e-01 5.60561952e-04 9.91479516e-01
 5.31140938e-02 1.68954134e-01 2.64125457e-03 3.58965844e-02
 8.98303115e-05]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 23:10:42, Dev, Step : 3170, Loss : 0.50061, Acc : 0.788, Auc : 0.865, Sensitive_Loss : 0.22950, Sensitive_Acc : 21.165, Sensitive_Auc : 0.985, Mean auc: 0.865, Run Time : 87.82 sec
INFO:root:2024-04-11 23:10:51, Train, Epoch : 6, Step : 3180, Loss : 0.39915, Acc : 0.791, Sensitive_Loss : 0.12542, Sensitive_Acc : 23.900, Run Time : 8.14 sec
INFO:root:2024-04-11 23:10:58, Train, Epoch : 6, Step : 3190, Loss : 0.43752, Acc : 0.822, Sensitive_Loss : 0.17417, Sensitive_Acc : 22.800, Run Time : 7.48 sec
INFO:root:2024-04-11 23:11:05, Train, Epoch : 6, Step : 3200, Loss : 0.30033, Acc : 0.881, Sensitive_Loss : 0.19580, Sensitive_Acc : 21.700, Run Time : 6.80 sec
INFO:root:2024-04-11 23:12:34, Dev, Step : 3200, Loss : 0.50609, Acc : 0.782, Auc : 0.863, Sensitive_Loss : 0.23034, Sensitive_Acc : 21.211, Sensitive_Auc : 0.985, Mean auc: 0.863, Run Time : 88.41 sec
INFO:root:2024-04-11 23:12:39, Train, Epoch : 6, Step : 3210, Loss : 0.38939, Acc : 0.828, Sensitive_Loss : 0.13436, Sensitive_Acc : 21.600, Run Time : 94.03 sec
INFO:root:2024-04-11 23:12:47, Train, Epoch : 6, Step : 3220, Loss : 0.40714, Acc : 0.828, Sensitive_Loss : 0.13566, Sensitive_Acc : 25.100, Run Time : 7.73 sec
INFO:root:2024-04-11 23:12:54, Train, Epoch : 6, Step : 3230, Loss : 0.38816, Acc : 0.856, Sensitive_Loss : 0.15029, Sensitive_Acc : 23.200, Run Time : 6.81 sec
INFO:root:2024-04-11 23:13:01, Train, Epoch : 6, Step : 3240, Loss : 0.32791, Acc : 0.831, Sensitive_Loss : 0.17954, Sensitive_Acc : 24.000, Run Time : 7.28 sec
INFO:root:2024-04-11 23:13:08, Train, Epoch : 6, Step : 3250, Loss : 0.33212, Acc : 0.856, Sensitive_Loss : 0.14256, Sensitive_Acc : 18.200, Run Time : 7.20 sec
INFO:root:2024-04-11 23:13:15, Train, Epoch : 6, Step : 3260, Loss : 0.34638, Acc : 0.869, Sensitive_Loss : 0.14901, Sensitive_Acc : 22.000, Run Time : 6.84 sec
INFO:root:2024-04-11 23:13:22, Train, Epoch : 6, Step : 3270, Loss : 0.31410, Acc : 0.863, Sensitive_Loss : 0.17878, Sensitive_Acc : 22.400, Run Time : 7.31 sec
INFO:root:2024-04-11 23:13:29, Train, Epoch : 6, Step : 3280, Loss : 0.38970, Acc : 0.791, Sensitive_Loss : 0.24258, Sensitive_Acc : 25.700, Run Time : 7.12 sec
INFO:root:2024-04-11 23:13:36, Train, Epoch : 6, Step : 3290, Loss : 0.38143, Acc : 0.822, Sensitive_Loss : 0.16395, Sensitive_Acc : 16.500, Run Time : 6.61 sec
INFO:root:2024-04-11 23:13:43, Train, Epoch : 6, Step : 3300, Loss : 0.35081, Acc : 0.831, Sensitive_Loss : 0.21480, Sensitive_Acc : 23.500, Run Time : 7.42 sec
INFO:root:2024-04-11 23:15:12, Dev, Step : 3300, Loss : 0.56508, Acc : 0.767, Auc : 0.861, Sensitive_Loss : 0.29077, Sensitive_Acc : 20.068, Sensitive_Auc : 0.985, Mean auc: 0.861, Run Time : 88.19 sec
INFO:root:2024-04-11 23:15:18, Train, Epoch : 6, Step : 3310, Loss : 0.37277, Acc : 0.831, Sensitive_Loss : 0.18746, Sensitive_Acc : 16.300, Run Time : 94.08 sec
INFO:root:2024-04-11 23:15:24, Train, Epoch : 6, Step : 3320, Loss : 0.46737, Acc : 0.819, Sensitive_Loss : 0.13474, Sensitive_Acc : 16.500, Run Time : 6.87 sec
INFO:root:2024-04-11 23:15:32, Train, Epoch : 6, Step : 3330, Loss : 0.36935, Acc : 0.819, Sensitive_Loss : 0.17951, Sensitive_Acc : 24.900, Run Time : 7.27 sec
INFO:root:2024-04-11 23:15:39, Train, Epoch : 6, Step : 3340, Loss : 0.42551, Acc : 0.806, Sensitive_Loss : 0.16958, Sensitive_Acc : 23.800, Run Time : 7.13 sec
INFO:root:2024-04-11 23:15:46, Train, Epoch : 6, Step : 3350, Loss : 0.37117, Acc : 0.834, Sensitive_Loss : 0.18143, Sensitive_Acc : 21.900, Run Time : 7.01 sec
INFO:root:2024-04-11 23:15:53, Train, Epoch : 6, Step : 3360, Loss : 0.37808, Acc : 0.841, Sensitive_Loss : 0.18761, Sensitive_Acc : 24.600, Run Time : 7.30 sec
INFO:root:2024-04-11 23:16:00, Train, Epoch : 6, Step : 3370, Loss : 0.34705, Acc : 0.863, Sensitive_Loss : 0.18814, Sensitive_Acc : 23.000, Run Time : 7.18 sec
INFO:root:2024-04-11 23:16:07, Train, Epoch : 6, Step : 3380, Loss : 0.34252, Acc : 0.859, Sensitive_Loss : 0.17393, Sensitive_Acc : 24.300, Run Time : 6.91 sec
INFO:root:2024-04-11 23:16:14, Train, Epoch : 6, Step : 3390, Loss : 0.36390, Acc : 0.834, Sensitive_Loss : 0.13026, Sensitive_Acc : 25.100, Run Time : 7.23 sec
INFO:root:2024-04-11 23:16:21, Train, Epoch : 6, Step : 3400, Loss : 0.38378, Acc : 0.819, Sensitive_Loss : 0.19912, Sensitive_Acc : 22.900, Run Time : 7.00 sec
INFO:root:2024-04-11 23:17:50, Dev, Step : 3400, Loss : 0.51107, Acc : 0.785, Auc : 0.866, Sensitive_Loss : 0.28465, Sensitive_Acc : 20.323, Sensitive_Auc : 0.984, Mean auc: 0.866, Run Time : 88.38 sec
INFO:root:2024-04-11 23:17:50, Best, Step : 3400, Loss : 0.51107, Acc : 0.785, Auc : 0.866, Sensitive_Loss : 0.28465, Sensitive_Acc : 20.323, Sensitive_Auc : 0.984, Best Auc : 0.866
INFO:root:2024-04-11 23:17:56, Train, Epoch : 6, Step : 3410, Loss : 0.40545, Acc : 0.812, Sensitive_Loss : 0.12085, Sensitive_Acc : 26.400, Run Time : 95.04 sec
INFO:root:2024-04-11 23:18:03, Train, Epoch : 6, Step : 3420, Loss : 0.34851, Acc : 0.819, Sensitive_Loss : 0.14664, Sensitive_Acc : 19.700, Run Time : 6.97 sec
INFO:root:2024-04-11 23:18:10, Train, Epoch : 6, Step : 3430, Loss : 0.39171, Acc : 0.834, Sensitive_Loss : 0.13119, Sensitive_Acc : 20.400, Run Time : 6.88 sec
INFO:root:2024-04-11 23:18:18, Train, Epoch : 6, Step : 3440, Loss : 0.33885, Acc : 0.834, Sensitive_Loss : 0.21074, Sensitive_Acc : 25.300, Run Time : 7.39 sec
INFO:root:2024-04-11 23:18:25, Train, Epoch : 6, Step : 3450, Loss : 0.36194, Acc : 0.822, Sensitive_Loss : 0.18764, Sensitive_Acc : 21.800, Run Time : 6.80 sec
INFO:root:2024-04-11 23:18:32, Train, Epoch : 6, Step : 3460, Loss : 0.36561, Acc : 0.838, Sensitive_Loss : 0.20751, Sensitive_Acc : 22.700, Run Time : 7.28 sec
INFO:root:2024-04-11 23:18:39, Train, Epoch : 6, Step : 3470, Loss : 0.43408, Acc : 0.825, Sensitive_Loss : 0.14629, Sensitive_Acc : 18.000, Run Time : 6.96 sec
INFO:root:2024-04-11 23:18:46, Train, Epoch : 6, Step : 3480, Loss : 0.32901, Acc : 0.859, Sensitive_Loss : 0.16365, Sensitive_Acc : 18.700, Run Time : 7.35 sec
INFO:root:2024-04-11 23:18:54, Train, Epoch : 6, Step : 3490, Loss : 0.36955, Acc : 0.847, Sensitive_Loss : 0.22608, Sensitive_Acc : 22.200, Run Time : 7.55 sec
INFO:root:2024-04-11 23:19:00, Train, Epoch : 6, Step : 3500, Loss : 0.39248, Acc : 0.838, Sensitive_Loss : 0.29049, Sensitive_Acc : 22.500, Run Time : 6.30 sec
INFO:root:2024-04-11 23:20:28, Dev, Step : 3500, Loss : 0.51135, Acc : 0.781, Auc : 0.865, Sensitive_Loss : 0.29683, Sensitive_Acc : 19.917, Sensitive_Auc : 0.987, Mean auc: 0.865, Run Time : 88.01 sec
INFO:root:2024-04-11 23:20:33, Train, Epoch : 6, Step : 3510, Loss : 0.39147, Acc : 0.791, Sensitive_Loss : 0.15400, Sensitive_Acc : 20.300, Run Time : 93.51 sec
INFO:root:2024-04-11 23:20:41, Train, Epoch : 6, Step : 3520, Loss : 0.40092, Acc : 0.806, Sensitive_Loss : 0.20553, Sensitive_Acc : 23.000, Run Time : 7.09 sec
INFO:root:2024-04-11 23:20:48, Train, Epoch : 6, Step : 3530, Loss : 0.40923, Acc : 0.825, Sensitive_Loss : 0.10220, Sensitive_Acc : 23.100, Run Time : 7.78 sec
INFO:root:2024-04-11 23:20:56, Train, Epoch : 6, Step : 3540, Loss : 0.36863, Acc : 0.853, Sensitive_Loss : 0.18125, Sensitive_Acc : 18.100, Run Time : 7.48 sec
INFO:root:2024-04-11 23:21:02, Train, Epoch : 6, Step : 3550, Loss : 0.38810, Acc : 0.841, Sensitive_Loss : 0.11357, Sensitive_Acc : 20.000, Run Time : 6.52 sec
INFO:root:2024-04-11 23:21:09, Train, Epoch : 6, Step : 3560, Loss : 0.32894, Acc : 0.831, Sensitive_Loss : 0.16217, Sensitive_Acc : 21.900, Run Time : 7.07 sec
INFO:root:2024-04-11 23:21:16, Train, Epoch : 6, Step : 3570, Loss : 0.41033, Acc : 0.838, Sensitive_Loss : 0.16750, Sensitive_Acc : 22.500, Run Time : 7.07 sec
INFO:root:2024-04-11 23:21:23, Train, Epoch : 6, Step : 3580, Loss : 0.41688, Acc : 0.828, Sensitive_Loss : 0.12307, Sensitive_Acc : 20.200, Run Time : 6.86 sec
INFO:root:2024-04-11 23:21:31, Train, Epoch : 6, Step : 3590, Loss : 0.31390, Acc : 0.884, Sensitive_Loss : 0.14330, Sensitive_Acc : 19.600, Run Time : 7.27 sec
INFO:root:2024-04-11 23:21:37, Train, Epoch : 6, Step : 3600, Loss : 0.38150, Acc : 0.841, Sensitive_Loss : 0.15481, Sensitive_Acc : 21.900, Run Time : 6.78 sec
INFO:root:2024-04-11 23:23:06, Dev, Step : 3600, Loss : 0.50892, Acc : 0.787, Auc : 0.865, Sensitive_Loss : 0.23957, Sensitive_Acc : 20.970, Sensitive_Auc : 0.988, Mean auc: 0.865, Run Time : 88.26 sec
INFO:root:2024-04-11 23:23:11, Train, Epoch : 6, Step : 3610, Loss : 0.38601, Acc : 0.841, Sensitive_Loss : 0.19970, Sensitive_Acc : 20.100, Run Time : 93.90 sec
INFO:root:2024-04-11 23:23:19, Train, Epoch : 6, Step : 3620, Loss : 0.49585, Acc : 0.809, Sensitive_Loss : 0.22687, Sensitive_Acc : 20.800, Run Time : 7.21 sec
INFO:root:2024-04-11 23:23:26, Train, Epoch : 6, Step : 3630, Loss : 0.42981, Acc : 0.838, Sensitive_Loss : 0.14758, Sensitive_Acc : 23.900, Run Time : 7.00 sec
INFO:root:2024-04-11 23:23:33, Train, Epoch : 6, Step : 3640, Loss : 0.39106, Acc : 0.822, Sensitive_Loss : 0.15147, Sensitive_Acc : 14.900, Run Time : 7.70 sec
INFO:root:2024-04-11 23:23:40, Train, Epoch : 6, Step : 3650, Loss : 0.40097, Acc : 0.831, Sensitive_Loss : 0.16569, Sensitive_Acc : 20.800, Run Time : 7.21 sec
INFO:root:2024-04-11 23:23:47, Train, Epoch : 6, Step : 3660, Loss : 0.31751, Acc : 0.850, Sensitive_Loss : 0.18222, Sensitive_Acc : 20.400, Run Time : 6.86 sec
INFO:root:2024-04-11 23:23:54, Train, Epoch : 6, Step : 3670, Loss : 0.35883, Acc : 0.850, Sensitive_Loss : 0.17866, Sensitive_Acc : 19.100, Run Time : 7.00 sec
INFO:root:2024-04-11 23:24:01, Train, Epoch : 6, Step : 3680, Loss : 0.39329, Acc : 0.841, Sensitive_Loss : 0.13791, Sensitive_Acc : 22.100, Run Time : 7.15 sec
INFO:root:2024-04-11 23:24:09, Train, Epoch : 6, Step : 3690, Loss : 0.33376, Acc : 0.853, Sensitive_Loss : 0.17490, Sensitive_Acc : 21.400, Run Time : 7.46 sec
INFO:root:2024-04-11 23:24:16, Train, Epoch : 6, Step : 3700, Loss : 0.32121, Acc : 0.878, Sensitive_Loss : 0.12272, Sensitive_Acc : 21.100, Run Time : 6.95 sec
INFO:root:2024-04-11 23:25:44, Dev, Step : 3700, Loss : 0.50859, Acc : 0.788, Auc : 0.865, Sensitive_Loss : 0.22282, Sensitive_Acc : 21.150, Sensitive_Auc : 0.986, Mean auc: 0.865, Run Time : 88.16 sec
INFO:root:2024-04-11 23:25:49, Train, Epoch : 6, Step : 3710, Loss : 0.39206, Acc : 0.825, Sensitive_Loss : 0.14062, Sensitive_Acc : 21.900, Run Time : 93.62 sec
INFO:root:2024-04-11 23:25:57, Train, Epoch : 6, Step : 3720, Loss : 0.39558, Acc : 0.834, Sensitive_Loss : 0.16470, Sensitive_Acc : 17.300, Run Time : 7.09 sec
INFO:root:2024-04-11 23:26:04, Train, Epoch : 6, Step : 3730, Loss : 0.40407, Acc : 0.850, Sensitive_Loss : 0.19612, Sensitive_Acc : 18.700, Run Time : 7.61 sec
INFO:root:2024-04-11 23:26:11, Train, Epoch : 6, Step : 3740, Loss : 0.38759, Acc : 0.828, Sensitive_Loss : 0.13360, Sensitive_Acc : 16.900, Run Time : 6.96 sec
INFO:root:2024-04-11 23:26:18, Train, Epoch : 6, Step : 3750, Loss : 0.38704, Acc : 0.831, Sensitive_Loss : 0.19833, Sensitive_Acc : 21.900, Run Time : 6.99 sec
INFO:root:2024-04-11 23:26:25, Train, Epoch : 6, Step : 3760, Loss : 0.47067, Acc : 0.812, Sensitive_Loss : 0.19619, Sensitive_Acc : 22.800, Run Time : 7.11 sec
INFO:root:2024-04-11 23:26:32, Train, Epoch : 6, Step : 3770, Loss : 0.41399, Acc : 0.850, Sensitive_Loss : 0.18416, Sensitive_Acc : 24.000, Run Time : 6.86 sec
INFO:root:2024-04-11 23:26:39, Train, Epoch : 6, Step : 3780, Loss : 0.30956, Acc : 0.856, Sensitive_Loss : 0.17960, Sensitive_Acc : 21.000, Run Time : 7.30 sec
INFO:root:2024-04-11 23:26:47, Train, Epoch : 6, Step : 3790, Loss : 0.39052, Acc : 0.838, Sensitive_Loss : 0.14075, Sensitive_Acc : 25.100, Run Time : 7.57 sec
INFO:root:2024-04-11 23:26:54, Train, Epoch : 6, Step : 3800, Loss : 0.32773, Acc : 0.859, Sensitive_Loss : 0.16087, Sensitive_Acc : 17.700, Run Time : 7.18 sec
INFO:root:2024-04-11 23:28:22, Dev, Step : 3800, Loss : 0.52688, Acc : 0.780, Auc : 0.863, Sensitive_Loss : 0.20329, Sensitive_Acc : 21.241, Sensitive_Auc : 0.987, Mean auc: 0.863, Run Time : 87.71 sec
INFO:root:2024-04-11 23:29:50
INFO:root:y_pred: [0.04535131 0.00770122 0.15274256 ... 0.16938645 0.04453256 0.02055871]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [2.59507127e-04 3.43105785e-04 1.49309810e-03 5.39262826e-03
 7.86471646e-03 1.76790141e-04 2.86588166e-03 6.87725469e-03
 2.07780544e-02 9.99622941e-01 5.11739612e-01 2.09310534e-03
 3.28128715e-03 8.83623870e-05 9.98545766e-01 6.01041839e-02
 1.51476925e-02 9.98506844e-01 9.99939680e-01 1.50299878e-04
 2.80713320e-01 8.98155791e-04 2.64581621e-01 1.17166173e-02
 8.04098994e-02 4.18483049e-01 4.60706869e-05 1.92391060e-04
 4.09313187e-04 4.68242392e-02 1.26825450e-02 9.74059165e-01
 3.49194482e-02 5.22886157e-01 1.13841903e-03 1.93408749e-04
 2.57179397e-03 8.65269676e-02 5.79001382e-02 3.03814467e-02
 1.83778349e-02 9.59798455e-01 2.17330782e-03 6.43333944e-04
 7.25722730e-01 1.77199572e-01 5.67325115e-01 1.27063170e-01
 7.17167735e-01 9.78920758e-01 9.75121915e-01 9.99908090e-01
 8.42442036e-01 2.86702765e-03 2.96870977e-01 1.90904349e-01
 1.04793403e-02 3.23311286e-03 9.97981250e-01 1.47218606e-03
 4.63731085e-05 2.52892845e-03 5.59394993e-03 7.30313652e-04
 9.92494941e-01 8.97834003e-02 1.11123302e-03 1.93980619e-01
 1.18680475e-02 9.89754379e-01 9.87945318e-01 9.99882817e-01
 3.52214091e-04 3.97221476e-01 3.54620861e-03 6.56628430e-01
 4.89681140e-02 2.15258711e-04 3.35751995e-02 6.64706982e-04
 1.96464844e-02 9.71264308e-05 9.99285161e-01 9.93120193e-01
 5.43629099e-03 4.22864184e-02 6.90777749e-02 1.86061917e-03
 4.57203425e-02 2.37036776e-03 1.16718905e-02 1.54207051e-01
 6.20449937e-05 1.55213071e-04 4.17696266e-03 8.09928309e-03
 4.12124937e-04 2.56545484e-01 1.45349104e-03 1.79523677e-02
 5.55439573e-03 1.54565349e-02 3.62359673e-01 6.65395614e-03
 3.70493950e-03 3.17738811e-03 2.19196826e-01 2.47336060e-01
 2.97799170e-01 1.59446433e-01 8.98015918e-04 9.99271214e-01
 9.98499393e-01 4.13546630e-04 5.23964643e-01 2.44702287e-02
 2.75420044e-02 2.30411650e-03 5.81218719e-01 1.19688539e-02
 4.71684244e-03 1.89026166e-03 3.30507904e-02 2.53796781e-04
 5.58582088e-03 7.68149555e-01 3.67590983e-04 9.85552013e-01
 4.36955206e-02 7.79900998e-02 2.17300304e-03 5.81553318e-02
 2.19825495e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 23:29:50, Dev, Step : 3804, Loss : 0.52461, Acc : 0.782, Auc : 0.863, Sensitive_Loss : 0.20182, Sensitive_Acc : 21.241, Sensitive_Auc : 0.988, Mean auc: 0.863, Run Time : 86.78 sec
INFO:root:2024-04-11 23:29:56, Train, Epoch : 7, Step : 3810, Loss : 0.18953, Acc : 0.509, Sensitive_Loss : 0.08981, Sensitive_Acc : 15.600, Run Time : 5.36 sec
INFO:root:2024-04-11 23:30:04, Train, Epoch : 7, Step : 3820, Loss : 0.34012, Acc : 0.850, Sensitive_Loss : 0.14367, Sensitive_Acc : 21.700, Run Time : 7.16 sec
INFO:root:2024-04-11 23:30:11, Train, Epoch : 7, Step : 3830, Loss : 0.33129, Acc : 0.875, Sensitive_Loss : 0.13802, Sensitive_Acc : 20.700, Run Time : 7.33 sec
INFO:root:2024-04-11 23:30:18, Train, Epoch : 7, Step : 3840, Loss : 0.36390, Acc : 0.844, Sensitive_Loss : 0.15521, Sensitive_Acc : 21.400, Run Time : 7.16 sec
INFO:root:2024-04-11 23:30:25, Train, Epoch : 7, Step : 3850, Loss : 0.31118, Acc : 0.894, Sensitive_Loss : 0.28076, Sensitive_Acc : 24.400, Run Time : 7.40 sec
INFO:root:2024-04-11 23:30:32, Train, Epoch : 7, Step : 3860, Loss : 0.31705, Acc : 0.856, Sensitive_Loss : 0.12105, Sensitive_Acc : 18.600, Run Time : 6.58 sec
INFO:root:2024-04-11 23:30:39, Train, Epoch : 7, Step : 3870, Loss : 0.39708, Acc : 0.819, Sensitive_Loss : 0.12195, Sensitive_Acc : 19.400, Run Time : 7.07 sec
INFO:root:2024-04-11 23:30:46, Train, Epoch : 7, Step : 3880, Loss : 0.28760, Acc : 0.887, Sensitive_Loss : 0.14175, Sensitive_Acc : 19.300, Run Time : 7.19 sec
INFO:root:2024-04-11 23:30:54, Train, Epoch : 7, Step : 3890, Loss : 0.40174, Acc : 0.838, Sensitive_Loss : 0.15766, Sensitive_Acc : 23.900, Run Time : 7.82 sec
INFO:root:2024-04-11 23:31:01, Train, Epoch : 7, Step : 3900, Loss : 0.41019, Acc : 0.816, Sensitive_Loss : 0.14729, Sensitive_Acc : 19.500, Run Time : 6.68 sec
INFO:root:2024-04-11 23:32:28, Dev, Step : 3900, Loss : 0.50858, Acc : 0.784, Auc : 0.864, Sensitive_Loss : 0.21780, Sensitive_Acc : 21.075, Sensitive_Auc : 0.988, Mean auc: 0.864, Run Time : 87.68 sec
INFO:root:2024-04-11 23:32:34, Train, Epoch : 7, Step : 3910, Loss : 0.35393, Acc : 0.853, Sensitive_Loss : 0.14041, Sensitive_Acc : 26.200, Run Time : 93.18 sec
INFO:root:2024-04-11 23:32:41, Train, Epoch : 7, Step : 3920, Loss : 0.34630, Acc : 0.856, Sensitive_Loss : 0.19441, Sensitive_Acc : 22.900, Run Time : 7.40 sec
INFO:root:2024-04-11 23:32:48, Train, Epoch : 7, Step : 3930, Loss : 0.35436, Acc : 0.831, Sensitive_Loss : 0.13807, Sensitive_Acc : 22.000, Run Time : 7.01 sec
INFO:root:2024-04-11 23:32:56, Train, Epoch : 7, Step : 3940, Loss : 0.31967, Acc : 0.869, Sensitive_Loss : 0.22579, Sensitive_Acc : 26.500, Run Time : 7.26 sec
INFO:root:2024-04-11 23:33:03, Train, Epoch : 7, Step : 3950, Loss : 0.41135, Acc : 0.809, Sensitive_Loss : 0.19124, Sensitive_Acc : 23.700, Run Time : 7.17 sec
INFO:root:2024-04-11 23:33:10, Train, Epoch : 7, Step : 3960, Loss : 0.33813, Acc : 0.878, Sensitive_Loss : 0.15541, Sensitive_Acc : 18.600, Run Time : 7.36 sec
INFO:root:2024-04-11 23:33:17, Train, Epoch : 7, Step : 3970, Loss : 0.32815, Acc : 0.878, Sensitive_Loss : 0.18117, Sensitive_Acc : 19.200, Run Time : 7.14 sec
INFO:root:2024-04-11 23:33:24, Train, Epoch : 7, Step : 3980, Loss : 0.37947, Acc : 0.841, Sensitive_Loss : 0.15633, Sensitive_Acc : 19.600, Run Time : 7.13 sec
INFO:root:2024-04-11 23:33:31, Train, Epoch : 7, Step : 3990, Loss : 0.34329, Acc : 0.869, Sensitive_Loss : 0.17622, Sensitive_Acc : 20.200, Run Time : 6.77 sec
INFO:root:2024-04-11 23:33:38, Train, Epoch : 7, Step : 4000, Loss : 0.37260, Acc : 0.825, Sensitive_Loss : 0.10296, Sensitive_Acc : 25.000, Run Time : 6.78 sec
INFO:root:2024-04-11 23:35:07, Dev, Step : 4000, Loss : 0.52021, Acc : 0.783, Auc : 0.864, Sensitive_Loss : 0.23099, Sensitive_Acc : 21.030, Sensitive_Auc : 0.989, Mean auc: 0.864, Run Time : 88.65 sec
INFO:root:2024-04-11 23:35:13, Train, Epoch : 7, Step : 4010, Loss : 0.40363, Acc : 0.838, Sensitive_Loss : 0.17462, Sensitive_Acc : 20.700, Run Time : 94.59 sec
INFO:root:2024-04-11 23:35:19, Train, Epoch : 7, Step : 4020, Loss : 0.36868, Acc : 0.853, Sensitive_Loss : 0.14705, Sensitive_Acc : 19.500, Run Time : 6.79 sec
INFO:root:2024-04-11 23:35:27, Train, Epoch : 7, Step : 4030, Loss : 0.36941, Acc : 0.847, Sensitive_Loss : 0.11555, Sensitive_Acc : 23.300, Run Time : 7.35 sec
INFO:root:2024-04-11 23:35:34, Train, Epoch : 7, Step : 4040, Loss : 0.32891, Acc : 0.831, Sensitive_Loss : 0.14533, Sensitive_Acc : 22.300, Run Time : 6.86 sec
INFO:root:2024-04-11 23:35:41, Train, Epoch : 7, Step : 4050, Loss : 0.28917, Acc : 0.859, Sensitive_Loss : 0.21526, Sensitive_Acc : 18.700, Run Time : 7.03 sec
INFO:root:2024-04-11 23:35:48, Train, Epoch : 7, Step : 4060, Loss : 0.34143, Acc : 0.856, Sensitive_Loss : 0.15695, Sensitive_Acc : 22.000, Run Time : 7.76 sec
INFO:root:2024-04-11 23:35:56, Train, Epoch : 7, Step : 4070, Loss : 0.41402, Acc : 0.831, Sensitive_Loss : 0.21120, Sensitive_Acc : 22.500, Run Time : 7.18 sec
INFO:root:2024-04-11 23:36:03, Train, Epoch : 7, Step : 4080, Loss : 0.39439, Acc : 0.834, Sensitive_Loss : 0.18923, Sensitive_Acc : 21.400, Run Time : 7.30 sec
INFO:root:2024-04-11 23:36:10, Train, Epoch : 7, Step : 4090, Loss : 0.40485, Acc : 0.825, Sensitive_Loss : 0.13054, Sensitive_Acc : 22.800, Run Time : 6.88 sec
INFO:root:2024-04-11 23:36:17, Train, Epoch : 7, Step : 4100, Loss : 0.35610, Acc : 0.853, Sensitive_Loss : 0.12998, Sensitive_Acc : 19.400, Run Time : 7.08 sec
INFO:root:2024-04-11 23:37:45, Dev, Step : 4100, Loss : 0.51447, Acc : 0.785, Auc : 0.866, Sensitive_Loss : 0.22240, Sensitive_Acc : 20.820, Sensitive_Auc : 0.988, Mean auc: 0.866, Run Time : 88.20 sec
INFO:root:2024-04-11 23:37:50, Train, Epoch : 7, Step : 4110, Loss : 0.34215, Acc : 0.859, Sensitive_Loss : 0.14863, Sensitive_Acc : 25.900, Run Time : 93.58 sec
INFO:root:2024-04-11 23:37:58, Train, Epoch : 7, Step : 4120, Loss : 0.37150, Acc : 0.834, Sensitive_Loss : 0.18460, Sensitive_Acc : 15.800, Run Time : 7.15 sec
INFO:root:2024-04-11 23:38:05, Train, Epoch : 7, Step : 4130, Loss : 0.36904, Acc : 0.834, Sensitive_Loss : 0.15513, Sensitive_Acc : 17.900, Run Time : 7.54 sec
INFO:root:2024-04-11 23:38:12, Train, Epoch : 7, Step : 4140, Loss : 0.37790, Acc : 0.869, Sensitive_Loss : 0.13557, Sensitive_Acc : 21.800, Run Time : 6.61 sec
INFO:root:2024-04-11 23:38:19, Train, Epoch : 7, Step : 4150, Loss : 0.42040, Acc : 0.825, Sensitive_Loss : 0.15339, Sensitive_Acc : 17.800, Run Time : 6.96 sec
INFO:root:2024-04-11 23:38:26, Train, Epoch : 7, Step : 4160, Loss : 0.31053, Acc : 0.863, Sensitive_Loss : 0.10092, Sensitive_Acc : 23.600, Run Time : 7.13 sec
INFO:root:2024-04-11 23:38:33, Train, Epoch : 7, Step : 4170, Loss : 0.33569, Acc : 0.844, Sensitive_Loss : 0.16637, Sensitive_Acc : 26.600, Run Time : 7.15 sec
INFO:root:2024-04-11 23:38:40, Train, Epoch : 7, Step : 4180, Loss : 0.36719, Acc : 0.838, Sensitive_Loss : 0.17185, Sensitive_Acc : 20.200, Run Time : 7.38 sec
INFO:root:2024-04-11 23:38:47, Train, Epoch : 7, Step : 4190, Loss : 0.37600, Acc : 0.844, Sensitive_Loss : 0.21846, Sensitive_Acc : 25.500, Run Time : 7.09 sec
INFO:root:2024-04-11 23:38:55, Train, Epoch : 7, Step : 4200, Loss : 0.37496, Acc : 0.819, Sensitive_Loss : 0.11101, Sensitive_Acc : 22.400, Run Time : 7.24 sec
INFO:root:2024-04-11 23:40:22, Dev, Step : 4200, Loss : 0.50525, Acc : 0.787, Auc : 0.865, Sensitive_Loss : 0.23334, Sensitive_Acc : 20.865, Sensitive_Auc : 0.986, Mean auc: 0.865, Run Time : 87.87 sec
INFO:root:2024-04-11 23:40:28, Train, Epoch : 7, Step : 4210, Loss : 0.39934, Acc : 0.816, Sensitive_Loss : 0.12404, Sensitive_Acc : 18.200, Run Time : 93.87 sec
INFO:root:2024-04-11 23:40:36, Train, Epoch : 7, Step : 4220, Loss : 0.30648, Acc : 0.875, Sensitive_Loss : 0.16763, Sensitive_Acc : 21.500, Run Time : 7.13 sec
INFO:root:2024-04-11 23:40:43, Train, Epoch : 7, Step : 4230, Loss : 0.30252, Acc : 0.897, Sensitive_Loss : 0.12581, Sensitive_Acc : 20.200, Run Time : 7.01 sec
INFO:root:2024-04-11 23:40:49, Train, Epoch : 7, Step : 4240, Loss : 0.35696, Acc : 0.856, Sensitive_Loss : 0.13981, Sensitive_Acc : 18.700, Run Time : 6.79 sec
INFO:root:2024-04-11 23:40:56, Train, Epoch : 7, Step : 4250, Loss : 0.33204, Acc : 0.856, Sensitive_Loss : 0.14445, Sensitive_Acc : 22.800, Run Time : 6.96 sec
INFO:root:2024-04-11 23:41:04, Train, Epoch : 7, Step : 4260, Loss : 0.36444, Acc : 0.866, Sensitive_Loss : 0.22611, Sensitive_Acc : 20.600, Run Time : 7.38 sec
INFO:root:2024-04-11 23:41:11, Train, Epoch : 7, Step : 4270, Loss : 0.30881, Acc : 0.878, Sensitive_Loss : 0.20036, Sensitive_Acc : 18.900, Run Time : 7.32 sec
INFO:root:2024-04-11 23:41:19, Train, Epoch : 7, Step : 4280, Loss : 0.34948, Acc : 0.847, Sensitive_Loss : 0.20449, Sensitive_Acc : 18.500, Run Time : 7.53 sec
INFO:root:2024-04-11 23:41:26, Train, Epoch : 7, Step : 4290, Loss : 0.38540, Acc : 0.828, Sensitive_Loss : 0.21617, Sensitive_Acc : 16.600, Run Time : 7.36 sec
INFO:root:2024-04-11 23:41:33, Train, Epoch : 7, Step : 4300, Loss : 0.34094, Acc : 0.863, Sensitive_Loss : 0.20331, Sensitive_Acc : 24.600, Run Time : 7.19 sec
INFO:root:2024-04-11 23:43:01, Dev, Step : 4300, Loss : 0.51601, Acc : 0.782, Auc : 0.863, Sensitive_Loss : 0.23052, Sensitive_Acc : 20.639, Sensitive_Auc : 0.986, Mean auc: 0.863, Run Time : 87.46 sec
INFO:root:2024-04-11 23:43:06, Train, Epoch : 7, Step : 4310, Loss : 0.37988, Acc : 0.822, Sensitive_Loss : 0.15150, Sensitive_Acc : 25.200, Run Time : 92.96 sec
INFO:root:2024-04-11 23:43:13, Train, Epoch : 7, Step : 4320, Loss : 0.34206, Acc : 0.831, Sensitive_Loss : 0.12836, Sensitive_Acc : 22.100, Run Time : 6.83 sec
INFO:root:2024-04-11 23:43:20, Train, Epoch : 7, Step : 4330, Loss : 0.32787, Acc : 0.856, Sensitive_Loss : 0.16998, Sensitive_Acc : 21.800, Run Time : 7.42 sec
INFO:root:2024-04-11 23:43:27, Train, Epoch : 7, Step : 4340, Loss : 0.36497, Acc : 0.825, Sensitive_Loss : 0.16506, Sensitive_Acc : 24.600, Run Time : 6.99 sec
INFO:root:2024-04-11 23:43:35, Train, Epoch : 7, Step : 4350, Loss : 0.39204, Acc : 0.844, Sensitive_Loss : 0.11982, Sensitive_Acc : 20.000, Run Time : 7.51 sec
INFO:root:2024-04-11 23:43:42, Train, Epoch : 7, Step : 4360, Loss : 0.31722, Acc : 0.850, Sensitive_Loss : 0.13706, Sensitive_Acc : 23.200, Run Time : 7.03 sec
INFO:root:2024-04-11 23:43:49, Train, Epoch : 7, Step : 4370, Loss : 0.34385, Acc : 0.831, Sensitive_Loss : 0.19256, Sensitive_Acc : 21.700, Run Time : 7.03 sec
INFO:root:2024-04-11 23:43:56, Train, Epoch : 7, Step : 4380, Loss : 0.32294, Acc : 0.859, Sensitive_Loss : 0.14392, Sensitive_Acc : 14.300, Run Time : 7.16 sec
INFO:root:2024-04-11 23:44:03, Train, Epoch : 7, Step : 4390, Loss : 0.31877, Acc : 0.875, Sensitive_Loss : 0.13749, Sensitive_Acc : 22.500, Run Time : 7.23 sec
INFO:root:2024-04-11 23:44:11, Train, Epoch : 7, Step : 4400, Loss : 0.43904, Acc : 0.834, Sensitive_Loss : 0.12874, Sensitive_Acc : 18.400, Run Time : 7.83 sec
INFO:root:2024-04-11 23:45:39, Dev, Step : 4400, Loss : 0.51954, Acc : 0.781, Auc : 0.861, Sensitive_Loss : 0.23108, Sensitive_Acc : 20.744, Sensitive_Auc : 0.987, Mean auc: 0.861, Run Time : 87.57 sec
INFO:root:2024-04-11 23:45:44, Train, Epoch : 7, Step : 4410, Loss : 0.33467, Acc : 0.847, Sensitive_Loss : 0.11914, Sensitive_Acc : 23.600, Run Time : 93.33 sec
INFO:root:2024-04-11 23:45:52, Train, Epoch : 7, Step : 4420, Loss : 0.40438, Acc : 0.812, Sensitive_Loss : 0.14372, Sensitive_Acc : 19.200, Run Time : 7.47 sec
INFO:root:2024-04-11 23:45:59, Train, Epoch : 7, Step : 4430, Loss : 0.35963, Acc : 0.834, Sensitive_Loss : 0.17983, Sensitive_Acc : 24.300, Run Time : 6.58 sec
INFO:root:2024-04-11 23:47:31
INFO:root:y_pred: [0.04389508 0.01142517 0.23114786 ... 0.145858   0.04299928 0.02307395]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [4.69926104e-04 4.69190913e-04 4.38762829e-03 6.76779542e-03
 9.34079196e-03 4.70044091e-04 4.47366899e-03 8.66497215e-03
 2.14879960e-02 9.99835968e-01 5.19624889e-01 1.61423988e-03
 3.07824090e-03 1.09631139e-04 9.99497414e-01 1.47757992e-01
 1.82006694e-02 9.99077916e-01 9.99962926e-01 4.89282189e-04
 4.02787685e-01 1.40064640e-03 3.07405025e-01 3.29776295e-02
 9.94215757e-02 4.73766714e-01 5.96880309e-05 4.07090440e-04
 6.07541006e-04 3.61134671e-02 1.12706795e-02 9.76472735e-01
 6.77261800e-02 7.47008204e-01 7.19984993e-04 2.32002523e-04
 1.92061358e-03 9.14142802e-02 1.28625348e-01 5.04538417e-02
 2.76700705e-02 9.69001830e-01 5.27809793e-03 9.98345786e-04
 7.61310041e-01 2.13492259e-01 6.72476411e-01 8.13370347e-02
 8.06169331e-01 9.87749994e-01 9.91368711e-01 9.99951482e-01
 8.65035653e-01 4.90939664e-03 4.31984156e-01 4.30232197e-01
 7.42007978e-03 1.81033872e-02 9.98683035e-01 2.86682323e-03
 5.47153504e-05 4.65463754e-03 5.54879848e-03 1.64819660e-03
 9.96037841e-01 1.70095339e-01 1.19394623e-03 2.10926294e-01
 1.76432319e-02 9.93635774e-01 9.96536851e-01 9.99931335e-01
 6.11184456e-04 4.77346510e-01 5.09073911e-03 7.41028965e-01
 6.00176342e-02 3.41260864e-04 1.96581651e-02 1.11942890e-03
 2.63496079e-02 1.98318230e-04 9.99817908e-01 9.94649351e-01
 1.14793433e-02 3.79397571e-02 1.03362493e-01 1.50634511e-03
 1.04957111e-01 2.55702971e-03 1.79203246e-02 2.52318829e-01
 2.47138145e-04 3.46820336e-04 6.92227576e-03 1.18745361e-02
 5.69442811e-04 3.42291117e-01 1.92719407e-03 1.42480405e-02
 6.49055326e-03 2.28793677e-02 5.44187307e-01 3.23368143e-03
 4.50899592e-03 1.20531889e-02 1.99744493e-01 5.04095316e-01
 4.09539133e-01 1.38365179e-01 1.41973712e-03 9.99745667e-01
 9.98385191e-01 7.77292298e-04 7.74534285e-01 2.57994384e-02
 1.96223948e-02 9.51336115e-04 5.52233636e-01 9.63460747e-03
 1.05900159e-02 2.87889107e-03 5.05744927e-02 2.23235795e-04
 1.88207552e-02 7.98384070e-01 3.73726478e-04 9.91413474e-01
 8.15500468e-02 1.45784065e-01 1.83863658e-03 6.31483495e-02
 3.77425778e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-11 23:47:31, Dev, Step : 4438, Loss : 0.51955, Acc : 0.783, Auc : 0.864, Sensitive_Loss : 0.21282, Sensitive_Acc : 21.015, Sensitive_Auc : 0.987, Mean auc: 0.864, Run Time : 87.06 sec
INFO:root:2024-04-11 23:47:35, Train, Epoch : 8, Step : 4440, Loss : 0.06904, Acc : 0.175, Sensitive_Loss : 0.03065, Sensitive_Acc : 5.600, Run Time : 2.76 sec
INFO:root:2024-04-11 23:47:42, Train, Epoch : 8, Step : 4450, Loss : 0.33584, Acc : 0.828, Sensitive_Loss : 0.11663, Sensitive_Acc : 23.500, Run Time : 6.89 sec
INFO:root:2024-04-11 23:47:49, Train, Epoch : 8, Step : 4460, Loss : 0.31438, Acc : 0.872, Sensitive_Loss : 0.13201, Sensitive_Acc : 19.600, Run Time : 7.28 sec
INFO:root:2024-04-11 23:47:56, Train, Epoch : 8, Step : 4470, Loss : 0.32839, Acc : 0.850, Sensitive_Loss : 0.12750, Sensitive_Acc : 20.600, Run Time : 7.14 sec
INFO:root:2024-04-11 23:48:03, Train, Epoch : 8, Step : 4480, Loss : 0.32537, Acc : 0.878, Sensitive_Loss : 0.15111, Sensitive_Acc : 18.700, Run Time : 7.42 sec
INFO:root:2024-04-11 23:48:11, Train, Epoch : 8, Step : 4490, Loss : 0.39346, Acc : 0.863, Sensitive_Loss : 0.13682, Sensitive_Acc : 21.800, Run Time : 7.12 sec
INFO:root:2024-04-11 23:48:17, Train, Epoch : 8, Step : 4500, Loss : 0.32565, Acc : 0.863, Sensitive_Loss : 0.17515, Sensitive_Acc : 17.400, Run Time : 6.71 sec
INFO:root:2024-04-11 23:49:46, Dev, Step : 4500, Loss : 0.52604, Acc : 0.782, Auc : 0.863, Sensitive_Loss : 0.21876, Sensitive_Acc : 20.970, Sensitive_Auc : 0.987, Mean auc: 0.863, Run Time : 88.25 sec
INFO:root:2024-04-11 23:49:51, Train, Epoch : 8, Step : 4510, Loss : 0.25793, Acc : 0.875, Sensitive_Loss : 0.17364, Sensitive_Acc : 25.100, Run Time : 94.07 sec
INFO:root:2024-04-11 23:49:59, Train, Epoch : 8, Step : 4520, Loss : 0.26002, Acc : 0.903, Sensitive_Loss : 0.12615, Sensitive_Acc : 20.000, Run Time : 7.20 sec
INFO:root:2024-04-11 23:50:05, Train, Epoch : 8, Step : 4530, Loss : 0.33136, Acc : 0.881, Sensitive_Loss : 0.17334, Sensitive_Acc : 21.900, Run Time : 6.74 sec
INFO:root:2024-04-11 23:50:13, Train, Epoch : 8, Step : 4540, Loss : 0.33845, Acc : 0.831, Sensitive_Loss : 0.14256, Sensitive_Acc : 19.400, Run Time : 7.47 sec
INFO:root:2024-04-11 23:50:20, Train, Epoch : 8, Step : 4550, Loss : 0.37726, Acc : 0.831, Sensitive_Loss : 0.09778, Sensitive_Acc : 23.100, Run Time : 6.96 sec
INFO:root:2024-04-11 23:50:27, Train, Epoch : 8, Step : 4560, Loss : 0.32610, Acc : 0.869, Sensitive_Loss : 0.13019, Sensitive_Acc : 18.800, Run Time : 7.09 sec
INFO:root:2024-04-11 23:50:34, Train, Epoch : 8, Step : 4570, Loss : 0.30943, Acc : 0.869, Sensitive_Loss : 0.17724, Sensitive_Acc : 25.400, Run Time : 7.26 sec
INFO:root:2024-04-11 23:50:41, Train, Epoch : 8, Step : 4580, Loss : 0.29744, Acc : 0.875, Sensitive_Loss : 0.13743, Sensitive_Acc : 25.400, Run Time : 7.09 sec
INFO:root:2024-04-11 23:50:48, Train, Epoch : 8, Step : 4590, Loss : 0.30911, Acc : 0.859, Sensitive_Loss : 0.10943, Sensitive_Acc : 23.900, Run Time : 7.14 sec
INFO:root:2024-04-11 23:50:56, Train, Epoch : 8, Step : 4600, Loss : 0.34633, Acc : 0.850, Sensitive_Loss : 0.11397, Sensitive_Acc : 22.000, Run Time : 7.71 sec
INFO:root:2024-04-11 23:52:24, Dev, Step : 4600, Loss : 0.51879, Acc : 0.788, Auc : 0.864, Sensitive_Loss : 0.24012, Sensitive_Acc : 20.654, Sensitive_Auc : 0.989, Mean auc: 0.864, Run Time : 88.10 sec
INFO:root:2024-04-11 23:52:30, Train, Epoch : 8, Step : 4610, Loss : 0.37913, Acc : 0.847, Sensitive_Loss : 0.09071, Sensitive_Acc : 22.000, Run Time : 93.56 sec
INFO:root:2024-04-11 23:52:37, Train, Epoch : 8, Step : 4620, Loss : 0.35994, Acc : 0.850, Sensitive_Loss : 0.12396, Sensitive_Acc : 23.100, Run Time : 7.63 sec
INFO:root:2024-04-11 23:52:44, Train, Epoch : 8, Step : 4630, Loss : 0.33437, Acc : 0.841, Sensitive_Loss : 0.16014, Sensitive_Acc : 24.000, Run Time : 7.14 sec
INFO:root:2024-04-11 23:52:51, Train, Epoch : 8, Step : 4640, Loss : 0.29001, Acc : 0.884, Sensitive_Loss : 0.15527, Sensitive_Acc : 19.700, Run Time : 7.05 sec
INFO:root:2024-04-11 23:52:59, Train, Epoch : 8, Step : 4650, Loss : 0.28531, Acc : 0.878, Sensitive_Loss : 0.18687, Sensitive_Acc : 24.200, Run Time : 7.42 sec
INFO:root:2024-04-11 23:53:05, Train, Epoch : 8, Step : 4660, Loss : 0.31823, Acc : 0.869, Sensitive_Loss : 0.13646, Sensitive_Acc : 24.200, Run Time : 6.58 sec
INFO:root:2024-04-11 23:53:13, Train, Epoch : 8, Step : 4670, Loss : 0.28461, Acc : 0.853, Sensitive_Loss : 0.14428, Sensitive_Acc : 19.900, Run Time : 7.42 sec
INFO:root:2024-04-11 23:53:20, Train, Epoch : 8, Step : 4680, Loss : 0.34494, Acc : 0.831, Sensitive_Loss : 0.17201, Sensitive_Acc : 21.000, Run Time : 7.37 sec
INFO:root:2024-04-11 23:53:27, Train, Epoch : 8, Step : 4690, Loss : 0.29850, Acc : 0.887, Sensitive_Loss : 0.12922, Sensitive_Acc : 20.000, Run Time : 7.21 sec
INFO:root:2024-04-11 23:53:34, Train, Epoch : 8, Step : 4700, Loss : 0.33025, Acc : 0.881, Sensitive_Loss : 0.20648, Sensitive_Acc : 20.700, Run Time : 6.85 sec
INFO:root:2024-04-11 23:55:02, Dev, Step : 4700, Loss : 0.52147, Acc : 0.781, Auc : 0.863, Sensitive_Loss : 0.24308, Sensitive_Acc : 20.489, Sensitive_Auc : 0.989, Mean auc: 0.863, Run Time : 87.77 sec
INFO:root:2024-04-11 23:55:08, Train, Epoch : 8, Step : 4710, Loss : 0.36500, Acc : 0.819, Sensitive_Loss : 0.20511, Sensitive_Acc : 22.900, Run Time : 93.34 sec
INFO:root:2024-04-11 23:55:15, Train, Epoch : 8, Step : 4720, Loss : 0.29875, Acc : 0.850, Sensitive_Loss : 0.22463, Sensitive_Acc : 18.800, Run Time : 6.93 sec
INFO:root:2024-04-11 23:55:22, Train, Epoch : 8, Step : 4730, Loss : 0.35672, Acc : 0.856, Sensitive_Loss : 0.17506, Sensitive_Acc : 24.300, Run Time : 7.20 sec
INFO:root:2024-04-11 23:55:29, Train, Epoch : 8, Step : 4740, Loss : 0.32039, Acc : 0.863, Sensitive_Loss : 0.19420, Sensitive_Acc : 21.800, Run Time : 7.10 sec
INFO:root:2024-04-11 23:55:36, Train, Epoch : 8, Step : 4750, Loss : 0.34185, Acc : 0.841, Sensitive_Loss : 0.17172, Sensitive_Acc : 21.200, Run Time : 7.38 sec
INFO:root:2024-04-11 23:55:43, Train, Epoch : 8, Step : 4760, Loss : 0.31845, Acc : 0.881, Sensitive_Loss : 0.12045, Sensitive_Acc : 23.100, Run Time : 7.11 sec
INFO:root:2024-04-11 23:55:50, Train, Epoch : 8, Step : 4770, Loss : 0.34029, Acc : 0.850, Sensitive_Loss : 0.13406, Sensitive_Acc : 18.400, Run Time : 6.98 sec
INFO:root:2024-04-11 23:55:57, Train, Epoch : 8, Step : 4780, Loss : 0.30618, Acc : 0.884, Sensitive_Loss : 0.11072, Sensitive_Acc : 23.800, Run Time : 6.80 sec
INFO:root:2024-04-11 23:56:04, Train, Epoch : 8, Step : 4790, Loss : 0.32104, Acc : 0.853, Sensitive_Loss : 0.10601, Sensitive_Acc : 20.700, Run Time : 6.96 sec
INFO:root:2024-04-11 23:56:12, Train, Epoch : 8, Step : 4800, Loss : 0.35288, Acc : 0.872, Sensitive_Loss : 0.13858, Sensitive_Acc : 19.900, Run Time : 7.78 sec
INFO:root:2024-04-11 23:57:40, Dev, Step : 4800, Loss : 0.53166, Acc : 0.783, Auc : 0.864, Sensitive_Loss : 0.22637, Sensitive_Acc : 21.030, Sensitive_Auc : 0.988, Mean auc: 0.864, Run Time : 87.95 sec
INFO:root:2024-04-11 23:57:46, Train, Epoch : 8, Step : 4810, Loss : 0.37159, Acc : 0.841, Sensitive_Loss : 0.13555, Sensitive_Acc : 20.100, Run Time : 93.97 sec
INFO:root:2024-04-11 23:57:53, Train, Epoch : 8, Step : 4820, Loss : 0.29066, Acc : 0.866, Sensitive_Loss : 0.20385, Sensitive_Acc : 24.000, Run Time : 6.85 sec
INFO:root:2024-04-11 23:58:00, Train, Epoch : 8, Step : 4830, Loss : 0.32271, Acc : 0.859, Sensitive_Loss : 0.14987, Sensitive_Acc : 24.500, Run Time : 6.97 sec
INFO:root:2024-04-11 23:58:07, Train, Epoch : 8, Step : 4840, Loss : 0.37005, Acc : 0.853, Sensitive_Loss : 0.13749, Sensitive_Acc : 24.100, Run Time : 6.97 sec
INFO:root:2024-04-11 23:58:14, Train, Epoch : 8, Step : 4850, Loss : 0.37214, Acc : 0.831, Sensitive_Loss : 0.12194, Sensitive_Acc : 22.200, Run Time : 7.72 sec
INFO:root:2024-04-11 23:58:21, Train, Epoch : 8, Step : 4860, Loss : 0.35203, Acc : 0.853, Sensitive_Loss : 0.16449, Sensitive_Acc : 21.800, Run Time : 6.59 sec
INFO:root:2024-04-11 23:58:28, Train, Epoch : 8, Step : 4870, Loss : 0.34234, Acc : 0.850, Sensitive_Loss : 0.12247, Sensitive_Acc : 20.100, Run Time : 7.16 sec
INFO:root:2024-04-11 23:58:36, Train, Epoch : 8, Step : 4880, Loss : 0.32821, Acc : 0.863, Sensitive_Loss : 0.13496, Sensitive_Acc : 20.000, Run Time : 7.81 sec
INFO:root:2024-04-11 23:58:43, Train, Epoch : 8, Step : 4890, Loss : 0.34449, Acc : 0.844, Sensitive_Loss : 0.16531, Sensitive_Acc : 18.400, Run Time : 6.73 sec
INFO:root:2024-04-11 23:58:50, Train, Epoch : 8, Step : 4900, Loss : 0.34700, Acc : 0.844, Sensitive_Loss : 0.20073, Sensitive_Acc : 23.400, Run Time : 7.21 sec
INFO:root:2024-04-12 00:00:18, Dev, Step : 4900, Loss : 0.52470, Acc : 0.783, Auc : 0.863, Sensitive_Loss : 0.20943, Sensitive_Acc : 20.925, Sensitive_Auc : 0.990, Mean auc: 0.863, Run Time : 87.94 sec
INFO:root:2024-04-12 00:00:24, Train, Epoch : 8, Step : 4910, Loss : 0.29930, Acc : 0.872, Sensitive_Loss : 0.13059, Sensitive_Acc : 20.900, Run Time : 93.76 sec
INFO:root:2024-04-12 00:00:31, Train, Epoch : 8, Step : 4920, Loss : 0.37874, Acc : 0.838, Sensitive_Loss : 0.13030, Sensitive_Acc : 24.300, Run Time : 7.29 sec
INFO:root:2024-04-12 00:00:38, Train, Epoch : 8, Step : 4930, Loss : 0.36601, Acc : 0.878, Sensitive_Loss : 0.15159, Sensitive_Acc : 20.900, Run Time : 7.10 sec
INFO:root:2024-04-12 00:00:45, Train, Epoch : 8, Step : 4940, Loss : 0.35737, Acc : 0.853, Sensitive_Loss : 0.15801, Sensitive_Acc : 23.700, Run Time : 6.76 sec
INFO:root:2024-04-12 00:00:52, Train, Epoch : 8, Step : 4950, Loss : 0.36997, Acc : 0.831, Sensitive_Loss : 0.14681, Sensitive_Acc : 21.900, Run Time : 7.09 sec
INFO:root:2024-04-12 00:00:59, Train, Epoch : 8, Step : 4960, Loss : 0.43287, Acc : 0.825, Sensitive_Loss : 0.16653, Sensitive_Acc : 25.300, Run Time : 7.12 sec
INFO:root:2024-04-12 00:01:07, Train, Epoch : 8, Step : 4970, Loss : 0.29678, Acc : 0.891, Sensitive_Loss : 0.14672, Sensitive_Acc : 23.700, Run Time : 7.58 sec
INFO:root:2024-04-12 00:01:14, Train, Epoch : 8, Step : 4980, Loss : 0.33737, Acc : 0.834, Sensitive_Loss : 0.09412, Sensitive_Acc : 23.600, Run Time : 7.16 sec
INFO:root:2024-04-12 00:01:20, Train, Epoch : 8, Step : 4990, Loss : 0.34021, Acc : 0.875, Sensitive_Loss : 0.11078, Sensitive_Acc : 23.300, Run Time : 6.69 sec
INFO:root:2024-04-12 00:01:28, Train, Epoch : 8, Step : 5000, Loss : 0.35106, Acc : 0.850, Sensitive_Loss : 0.19190, Sensitive_Acc : 22.700, Run Time : 7.33 sec
INFO:root:2024-04-12 00:02:55, Dev, Step : 5000, Loss : 0.55000, Acc : 0.779, Auc : 0.862, Sensitive_Loss : 0.23840, Sensitive_Acc : 20.774, Sensitive_Auc : 0.988, Mean auc: 0.862, Run Time : 87.53 sec
INFO:root:2024-04-12 00:03:01, Train, Epoch : 8, Step : 5010, Loss : 0.34732, Acc : 0.859, Sensitive_Loss : 0.15431, Sensitive_Acc : 24.000, Run Time : 93.04 sec
INFO:root:2024-04-12 00:03:08, Train, Epoch : 8, Step : 5020, Loss : 0.29601, Acc : 0.859, Sensitive_Loss : 0.11524, Sensitive_Acc : 19.200, Run Time : 7.26 sec
INFO:root:2024-04-12 00:03:15, Train, Epoch : 8, Step : 5030, Loss : 0.28490, Acc : 0.891, Sensitive_Loss : 0.27550, Sensitive_Acc : 20.500, Run Time : 6.83 sec
INFO:root:2024-04-12 00:03:22, Train, Epoch : 8, Step : 5040, Loss : 0.37492, Acc : 0.859, Sensitive_Loss : 0.19457, Sensitive_Acc : 19.500, Run Time : 7.33 sec
INFO:root:2024-04-12 00:03:30, Train, Epoch : 8, Step : 5050, Loss : 0.31788, Acc : 0.866, Sensitive_Loss : 0.17263, Sensitive_Acc : 20.400, Run Time : 7.28 sec
INFO:root:2024-04-12 00:03:37, Train, Epoch : 8, Step : 5060, Loss : 0.34378, Acc : 0.838, Sensitive_Loss : 0.17329, Sensitive_Acc : 20.800, Run Time : 7.47 sec
INFO:root:2024-04-12 00:03:44, Train, Epoch : 8, Step : 5070, Loss : 0.28681, Acc : 0.872, Sensitive_Loss : 0.14957, Sensitive_Acc : 20.400, Run Time : 6.78 sec
INFO:root:2024-04-12 00:05:12
INFO:root:y_pred: [0.02126964 0.00569201 0.14152984 ... 0.09164552 0.02470991 0.01919186]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [5.72280376e-04 2.50550336e-04 9.29386821e-03 9.80555173e-03
 9.92804859e-03 7.13742629e-04 9.01156291e-03 3.76156271e-02
 5.77424690e-02 9.99706089e-01 6.89059317e-01 1.51118683e-03
 5.25557203e-03 1.14613358e-04 9.99237537e-01 1.30171165e-01
 1.00196274e-02 9.98956442e-01 9.99969244e-01 7.67038320e-04
 5.11160672e-01 1.19560666e-03 4.63974357e-01 2.62148138e-02
 1.12810932e-01 5.87393105e-01 4.19361713e-05 3.13292519e-04
 6.29820453e-04 2.54482422e-02 1.20793935e-02 9.87358391e-01
 6.60845265e-02 8.12151313e-01 3.89465014e-04 3.67693836e-04
 6.11370010e-03 8.27706009e-02 2.64133632e-01 2.79111341e-02
 3.90019156e-02 9.73393321e-01 5.66895120e-03 7.21598160e-04
 9.10825670e-01 1.31720975e-01 8.17528784e-01 9.79726315e-02
 5.68642139e-01 9.87991273e-01 9.94132817e-01 9.99888659e-01
 8.61400843e-01 3.81427119e-03 6.35987818e-01 6.14278197e-01
 8.51645041e-03 1.84834804e-02 9.98441637e-01 3.42710642e-03
 9.00780433e-05 2.19572824e-03 6.59559900e-03 1.64847751e-03
 9.97281671e-01 1.94935590e-01 3.66986659e-03 2.54881263e-01
 9.52590480e-02 9.98436987e-01 9.95527208e-01 9.99915600e-01
 3.87307926e-04 3.09080452e-01 5.63827390e-03 7.78445184e-01
 1.19228333e-01 4.97772824e-04 4.88706864e-02 1.39472913e-03
 4.03205566e-02 1.62639772e-04 9.99815166e-01 9.98752236e-01
 1.68782957e-02 6.32671937e-02 1.27358377e-01 2.20874720e-03
 1.57969818e-01 4.20856196e-03 3.55775617e-02 1.64483517e-01
 3.60639038e-04 3.65777581e-04 1.27154617e-02 2.02230345e-02
 3.79375706e-04 3.67077589e-01 4.43247519e-03 2.48925928e-02
 1.69021282e-02 3.90736014e-02 5.58104098e-01 5.85364969e-03
 4.49230662e-03 1.60326008e-02 2.32895881e-01 5.44115663e-01
 6.28400564e-01 1.08851254e-01 2.12218449e-03 9.99641538e-01
 9.98855352e-01 9.74294904e-04 8.10930729e-01 3.06683518e-02
 3.79646942e-02 1.94407953e-03 7.38974750e-01 9.86803323e-03
 1.82424933e-02 1.12870452e-03 5.85961230e-02 4.27625317e-04
 8.45543947e-03 8.43881190e-01 8.65516951e-04 9.93696749e-01
 1.04351401e-01 2.30010554e-01 3.52350180e-03 1.18555531e-01
 3.21442378e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 00:05:12, Dev, Step : 5072, Loss : 0.55729, Acc : 0.778, Auc : 0.860, Sensitive_Loss : 0.23718, Sensitive_Acc : 20.489, Sensitive_Auc : 0.989, Mean auc: 0.860, Run Time : 87.34 sec
INFO:root:2024-04-12 00:05:20, Train, Epoch : 9, Step : 5080, Loss : 0.22495, Acc : 0.706, Sensitive_Loss : 0.08666, Sensitive_Acc : 17.200, Run Time : 6.76 sec
INFO:root:2024-04-12 00:05:27, Train, Epoch : 9, Step : 5090, Loss : 0.32900, Acc : 0.897, Sensitive_Loss : 0.13654, Sensitive_Acc : 23.400, Run Time : 6.95 sec
INFO:root:2024-04-12 00:05:34, Train, Epoch : 9, Step : 5100, Loss : 0.31669, Acc : 0.897, Sensitive_Loss : 0.16260, Sensitive_Acc : 23.200, Run Time : 7.21 sec
INFO:root:2024-04-12 00:07:02, Dev, Step : 5100, Loss : 0.54150, Acc : 0.785, Auc : 0.859, Sensitive_Loss : 0.25929, Sensitive_Acc : 20.323, Sensitive_Auc : 0.990, Mean auc: 0.859, Run Time : 87.46 sec
INFO:root:2024-04-12 00:07:07, Train, Epoch : 9, Step : 5110, Loss : 0.33689, Acc : 0.866, Sensitive_Loss : 0.19768, Sensitive_Acc : 24.000, Run Time : 93.00 sec
INFO:root:2024-04-12 00:07:14, Train, Epoch : 9, Step : 5120, Loss : 0.31066, Acc : 0.866, Sensitive_Loss : 0.20790, Sensitive_Acc : 22.800, Run Time : 6.89 sec
INFO:root:2024-04-12 00:07:21, Train, Epoch : 9, Step : 5130, Loss : 0.34779, Acc : 0.850, Sensitive_Loss : 0.10731, Sensitive_Acc : 13.500, Run Time : 7.46 sec
INFO:root:2024-04-12 00:07:28, Train, Epoch : 9, Step : 5140, Loss : 0.37823, Acc : 0.844, Sensitive_Loss : 0.12358, Sensitive_Acc : 22.600, Run Time : 6.92 sec
INFO:root:2024-04-12 00:07:35, Train, Epoch : 9, Step : 5150, Loss : 0.33984, Acc : 0.850, Sensitive_Loss : 0.14792, Sensitive_Acc : 23.700, Run Time : 7.11 sec
INFO:root:2024-04-12 00:07:43, Train, Epoch : 9, Step : 5160, Loss : 0.27768, Acc : 0.875, Sensitive_Loss : 0.15773, Sensitive_Acc : 22.700, Run Time : 7.13 sec
INFO:root:2024-04-12 00:07:50, Train, Epoch : 9, Step : 5170, Loss : 0.29493, Acc : 0.891, Sensitive_Loss : 0.10835, Sensitive_Acc : 22.900, Run Time : 7.43 sec
INFO:root:2024-04-12 00:07:57, Train, Epoch : 9, Step : 5180, Loss : 0.29329, Acc : 0.884, Sensitive_Loss : 0.14321, Sensitive_Acc : 21.900, Run Time : 7.30 sec
INFO:root:2024-04-12 00:08:04, Train, Epoch : 9, Step : 5190, Loss : 0.28034, Acc : 0.875, Sensitive_Loss : 0.09136, Sensitive_Acc : 22.700, Run Time : 6.93 sec
INFO:root:2024-04-12 00:08:12, Train, Epoch : 9, Step : 5200, Loss : 0.31145, Acc : 0.863, Sensitive_Loss : 0.11748, Sensitive_Acc : 22.800, Run Time : 7.39 sec
INFO:root:2024-04-12 00:09:40, Dev, Step : 5200, Loss : 0.57080, Acc : 0.772, Auc : 0.862, Sensitive_Loss : 0.20389, Sensitive_Acc : 21.211, Sensitive_Auc : 0.990, Mean auc: 0.862, Run Time : 88.12 sec
INFO:root:2024-04-12 00:09:45, Train, Epoch : 9, Step : 5210, Loss : 0.26638, Acc : 0.894, Sensitive_Loss : 0.21801, Sensitive_Acc : 18.800, Run Time : 93.73 sec
INFO:root:2024-04-12 00:09:53, Train, Epoch : 9, Step : 5220, Loss : 0.34599, Acc : 0.856, Sensitive_Loss : 0.15522, Sensitive_Acc : 21.400, Run Time : 7.44 sec
INFO:root:2024-04-12 00:09:59, Train, Epoch : 9, Step : 5230, Loss : 0.30234, Acc : 0.878, Sensitive_Loss : 0.12719, Sensitive_Acc : 22.300, Run Time : 6.60 sec
INFO:root:2024-04-12 00:10:06, Train, Epoch : 9, Step : 5240, Loss : 0.28615, Acc : 0.853, Sensitive_Loss : 0.19947, Sensitive_Acc : 22.000, Run Time : 6.96 sec
INFO:root:2024-04-12 00:10:14, Train, Epoch : 9, Step : 5250, Loss : 0.30228, Acc : 0.863, Sensitive_Loss : 0.13217, Sensitive_Acc : 20.800, Run Time : 7.45 sec
INFO:root:2024-04-12 00:10:21, Train, Epoch : 9, Step : 5260, Loss : 0.34163, Acc : 0.844, Sensitive_Loss : 0.18928, Sensitive_Acc : 23.600, Run Time : 7.14 sec
INFO:root:2024-04-12 00:10:28, Train, Epoch : 9, Step : 5270, Loss : 0.30477, Acc : 0.884, Sensitive_Loss : 0.11588, Sensitive_Acc : 23.300, Run Time : 7.03 sec
INFO:root:2024-04-12 00:10:35, Train, Epoch : 9, Step : 5280, Loss : 0.38629, Acc : 0.847, Sensitive_Loss : 0.17774, Sensitive_Acc : 21.700, Run Time : 6.67 sec
INFO:root:2024-04-12 00:10:42, Train, Epoch : 9, Step : 5290, Loss : 0.39506, Acc : 0.844, Sensitive_Loss : 0.12395, Sensitive_Acc : 19.400, Run Time : 7.30 sec
INFO:root:2024-04-12 00:10:49, Train, Epoch : 9, Step : 5300, Loss : 0.27100, Acc : 0.891, Sensitive_Loss : 0.11240, Sensitive_Acc : 16.800, Run Time : 7.32 sec
INFO:root:2024-04-12 00:12:17, Dev, Step : 5300, Loss : 0.55151, Acc : 0.775, Auc : 0.858, Sensitive_Loss : 0.20168, Sensitive_Acc : 21.195, Sensitive_Auc : 0.990, Mean auc: 0.858, Run Time : 87.91 sec
INFO:root:2024-04-12 00:12:23, Train, Epoch : 9, Step : 5310, Loss : 0.38317, Acc : 0.816, Sensitive_Loss : 0.15730, Sensitive_Acc : 23.500, Run Time : 93.74 sec
INFO:root:2024-04-12 00:12:30, Train, Epoch : 9, Step : 5320, Loss : 0.30108, Acc : 0.875, Sensitive_Loss : 0.11813, Sensitive_Acc : 19.100, Run Time : 7.10 sec
INFO:root:2024-04-12 00:12:37, Train, Epoch : 9, Step : 5330, Loss : 0.26414, Acc : 0.856, Sensitive_Loss : 0.14871, Sensitive_Acc : 21.800, Run Time : 7.00 sec
INFO:root:2024-04-12 00:12:45, Train, Epoch : 9, Step : 5340, Loss : 0.28558, Acc : 0.875, Sensitive_Loss : 0.20599, Sensitive_Acc : 23.000, Run Time : 7.86 sec
INFO:root:2024-04-12 00:12:52, Train, Epoch : 9, Step : 5350, Loss : 0.30658, Acc : 0.878, Sensitive_Loss : 0.14505, Sensitive_Acc : 21.200, Run Time : 6.85 sec
INFO:root:2024-04-12 00:12:59, Train, Epoch : 9, Step : 5360, Loss : 0.27860, Acc : 0.897, Sensitive_Loss : 0.13580, Sensitive_Acc : 20.600, Run Time : 7.39 sec
INFO:root:2024-04-12 00:13:06, Train, Epoch : 9, Step : 5370, Loss : 0.28001, Acc : 0.884, Sensitive_Loss : 0.10029, Sensitive_Acc : 23.300, Run Time : 6.44 sec
INFO:root:2024-04-12 00:13:13, Train, Epoch : 9, Step : 5380, Loss : 0.28161, Acc : 0.856, Sensitive_Loss : 0.17020, Sensitive_Acc : 25.200, Run Time : 7.66 sec
INFO:root:2024-04-12 00:13:20, Train, Epoch : 9, Step : 5390, Loss : 0.34622, Acc : 0.856, Sensitive_Loss : 0.12774, Sensitive_Acc : 26.600, Run Time : 6.81 sec
INFO:root:2024-04-12 00:13:27, Train, Epoch : 9, Step : 5400, Loss : 0.35153, Acc : 0.863, Sensitive_Loss : 0.17914, Sensitive_Acc : 24.000, Run Time : 7.00 sec
INFO:root:2024-04-12 00:14:55, Dev, Step : 5400, Loss : 0.53419, Acc : 0.783, Auc : 0.858, Sensitive_Loss : 0.21500, Sensitive_Acc : 21.045, Sensitive_Auc : 0.989, Mean auc: 0.858, Run Time : 88.12 sec
INFO:root:2024-04-12 00:15:01, Train, Epoch : 9, Step : 5410, Loss : 0.27228, Acc : 0.900, Sensitive_Loss : 0.11121, Sensitive_Acc : 22.600, Run Time : 93.63 sec
INFO:root:2024-04-12 00:15:08, Train, Epoch : 9, Step : 5420, Loss : 0.36696, Acc : 0.863, Sensitive_Loss : 0.27330, Sensitive_Acc : 20.100, Run Time : 7.23 sec
INFO:root:2024-04-12 00:15:15, Train, Epoch : 9, Step : 5430, Loss : 0.30517, Acc : 0.872, Sensitive_Loss : 0.14047, Sensitive_Acc : 17.600, Run Time : 7.44 sec
INFO:root:2024-04-12 00:15:22, Train, Epoch : 9, Step : 5440, Loss : 0.25713, Acc : 0.891, Sensitive_Loss : 0.14465, Sensitive_Acc : 25.500, Run Time : 6.59 sec
INFO:root:2024-04-12 00:15:29, Train, Epoch : 9, Step : 5450, Loss : 0.24062, Acc : 0.891, Sensitive_Loss : 0.13596, Sensitive_Acc : 21.600, Run Time : 7.06 sec
INFO:root:2024-04-12 00:15:36, Train, Epoch : 9, Step : 5460, Loss : 0.29312, Acc : 0.891, Sensitive_Loss : 0.10910, Sensitive_Acc : 21.800, Run Time : 7.15 sec
INFO:root:2024-04-12 00:15:44, Train, Epoch : 9, Step : 5470, Loss : 0.34517, Acc : 0.869, Sensitive_Loss : 0.15524, Sensitive_Acc : 14.100, Run Time : 7.83 sec
INFO:root:2024-04-12 00:15:51, Train, Epoch : 9, Step : 5480, Loss : 0.30623, Acc : 0.850, Sensitive_Loss : 0.11625, Sensitive_Acc : 20.900, Run Time : 6.93 sec
INFO:root:2024-04-12 00:15:59, Train, Epoch : 9, Step : 5490, Loss : 0.29899, Acc : 0.841, Sensitive_Loss : 0.13925, Sensitive_Acc : 22.600, Run Time : 7.58 sec
INFO:root:2024-04-12 00:16:05, Train, Epoch : 9, Step : 5500, Loss : 0.32127, Acc : 0.863, Sensitive_Loss : 0.11636, Sensitive_Acc : 21.100, Run Time : 6.72 sec
INFO:root:2024-04-12 00:17:33, Dev, Step : 5500, Loss : 0.56558, Acc : 0.778, Auc : 0.855, Sensitive_Loss : 0.22551, Sensitive_Acc : 20.970, Sensitive_Auc : 0.990, Mean auc: 0.855, Run Time : 87.88 sec
INFO:root:2024-04-12 00:17:39, Train, Epoch : 9, Step : 5510, Loss : 0.31127, Acc : 0.863, Sensitive_Loss : 0.22834, Sensitive_Acc : 24.000, Run Time : 93.99 sec
INFO:root:2024-04-12 00:17:46, Train, Epoch : 9, Step : 5520, Loss : 0.25557, Acc : 0.884, Sensitive_Loss : 0.12920, Sensitive_Acc : 21.700, Run Time : 6.90 sec
INFO:root:2024-04-12 00:17:53, Train, Epoch : 9, Step : 5530, Loss : 0.32346, Acc : 0.853, Sensitive_Loss : 0.13995, Sensitive_Acc : 23.500, Run Time : 7.18 sec
INFO:root:2024-04-12 00:18:00, Train, Epoch : 9, Step : 5540, Loss : 0.32846, Acc : 0.841, Sensitive_Loss : 0.14822, Sensitive_Acc : 23.600, Run Time : 7.00 sec
INFO:root:2024-04-12 00:18:08, Train, Epoch : 9, Step : 5550, Loss : 0.32701, Acc : 0.875, Sensitive_Loss : 0.18311, Sensitive_Acc : 21.900, Run Time : 7.37 sec
INFO:root:2024-04-12 00:18:15, Train, Epoch : 9, Step : 5560, Loss : 0.31088, Acc : 0.881, Sensitive_Loss : 0.14577, Sensitive_Acc : 22.100, Run Time : 7.28 sec
INFO:root:2024-04-12 00:18:22, Train, Epoch : 9, Step : 5570, Loss : 0.34409, Acc : 0.872, Sensitive_Loss : 0.14465, Sensitive_Acc : 20.900, Run Time : 7.28 sec
INFO:root:2024-04-12 00:18:29, Train, Epoch : 9, Step : 5580, Loss : 0.30920, Acc : 0.881, Sensitive_Loss : 0.18123, Sensitive_Acc : 19.400, Run Time : 6.95 sec
INFO:root:2024-04-12 00:18:36, Train, Epoch : 9, Step : 5590, Loss : 0.32691, Acc : 0.841, Sensitive_Loss : 0.17073, Sensitive_Acc : 20.300, Run Time : 6.93 sec
INFO:root:2024-04-12 00:18:44, Train, Epoch : 9, Step : 5600, Loss : 0.29405, Acc : 0.850, Sensitive_Loss : 0.15385, Sensitive_Acc : 20.700, Run Time : 7.58 sec
INFO:root:2024-04-12 00:20:12, Dev, Step : 5600, Loss : 0.60869, Acc : 0.766, Auc : 0.857, Sensitive_Loss : 0.23488, Sensitive_Acc : 20.820, Sensitive_Auc : 0.989, Mean auc: 0.857, Run Time : 87.94 sec
INFO:root:2024-04-12 00:20:17, Train, Epoch : 9, Step : 5610, Loss : 0.27877, Acc : 0.859, Sensitive_Loss : 0.16991, Sensitive_Acc : 23.600, Run Time : 93.39 sec
INFO:root:2024-04-12 00:20:25, Train, Epoch : 9, Step : 5620, Loss : 0.35999, Acc : 0.853, Sensitive_Loss : 0.14323, Sensitive_Acc : 22.200, Run Time : 7.37 sec
INFO:root:2024-04-12 00:20:32, Train, Epoch : 9, Step : 5630, Loss : 0.33473, Acc : 0.841, Sensitive_Loss : 0.11613, Sensitive_Acc : 20.100, Run Time : 6.92 sec
INFO:root:2024-04-12 00:20:39, Train, Epoch : 9, Step : 5640, Loss : 0.26797, Acc : 0.881, Sensitive_Loss : 0.15349, Sensitive_Acc : 23.900, Run Time : 7.21 sec
INFO:root:2024-04-12 00:20:46, Train, Epoch : 9, Step : 5650, Loss : 0.30235, Acc : 0.875, Sensitive_Loss : 0.09889, Sensitive_Acc : 23.800, Run Time : 7.57 sec
INFO:root:2024-04-12 00:20:53, Train, Epoch : 9, Step : 5660, Loss : 0.32523, Acc : 0.847, Sensitive_Loss : 0.13250, Sensitive_Acc : 21.200, Run Time : 7.02 sec
INFO:root:2024-04-12 00:21:01, Train, Epoch : 9, Step : 5670, Loss : 0.30382, Acc : 0.887, Sensitive_Loss : 0.10505, Sensitive_Acc : 24.000, Run Time : 7.19 sec
INFO:root:2024-04-12 00:21:07, Train, Epoch : 9, Step : 5680, Loss : 0.37160, Acc : 0.828, Sensitive_Loss : 0.11962, Sensitive_Acc : 23.100, Run Time : 6.75 sec
INFO:root:2024-04-12 00:21:15, Train, Epoch : 9, Step : 5690, Loss : 0.28216, Acc : 0.875, Sensitive_Loss : 0.19874, Sensitive_Acc : 23.600, Run Time : 7.33 sec
INFO:root:2024-04-12 00:21:22, Train, Epoch : 9, Step : 5700, Loss : 0.29450, Acc : 0.903, Sensitive_Loss : 0.12699, Sensitive_Acc : 21.100, Run Time : 7.29 sec
INFO:root:2024-04-12 00:22:50, Dev, Step : 5700, Loss : 0.54384, Acc : 0.783, Auc : 0.861, Sensitive_Loss : 0.20601, Sensitive_Acc : 21.211, Sensitive_Auc : 0.990, Mean auc: 0.861, Run Time : 87.98 sec
INFO:root:2024-04-12 00:24:19
INFO:root:y_pred: [0.03628203 0.00417951 0.34995294 ... 0.02491958 0.01351508 0.01633643]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [1.6434016e-03 3.6007288e-04 7.5768032e-03 4.3929680e-03 8.2620978e-03
 8.9754921e-04 5.4013617e-03 1.0137223e-02 3.2854293e-02 9.9977750e-01
 5.5315024e-01 1.0285787e-03 2.8903985e-03 1.2629562e-04 9.9958950e-01
 1.6709562e-01 8.0924928e-03 9.9879849e-01 9.9994659e-01 5.5827852e-04
 4.8235139e-01 1.9651037e-03 1.6775140e-01 2.6612135e-02 9.9172048e-02
 1.9613513e-01 2.3751107e-05 4.2664932e-04 6.6083658e-04 2.2670463e-02
 6.2186723e-03 9.8648071e-01 5.5077795e-02 8.3805239e-01 3.4629850e-04
 2.3074397e-04 5.8768326e-03 1.7651077e-01 2.0485169e-01 3.2784402e-02
 6.2679194e-02 9.5829314e-01 5.7094414e-03 6.2705029e-04 7.9660761e-01
 1.4960848e-01 6.6736174e-01 9.8284699e-02 5.8981407e-01 9.8429072e-01
 9.8344308e-01 9.9989498e-01 8.4104156e-01 2.3338611e-03 4.7722828e-01
 5.7225496e-01 7.5798738e-03 1.8263893e-02 9.9814582e-01 3.6728967e-03
 1.9306652e-05 3.6077676e-03 5.1303450e-03 1.5411086e-03 9.9400008e-01
 2.0490044e-01 9.4277010e-04 1.9461682e-01 9.1800261e-03 9.9698973e-01
 9.9691808e-01 9.9991763e-01 8.1494928e-04 4.6427378e-01 3.5270983e-03
 8.0320585e-01 9.0600327e-02 5.6110963e-04 1.1925637e-02 1.1311258e-03
 5.7705794e-02 1.9650841e-04 9.9972016e-01 9.9243474e-01 2.8327761e-02
 1.5664361e-02 7.6808877e-02 1.0853290e-03 1.0392181e-01 2.5217191e-03
 3.0675896e-02 1.8816383e-01 1.7442189e-04 2.8134949e-04 9.1775106e-03
 1.8739397e-02 6.8014354e-04 3.2104433e-01 3.2578274e-03 5.2897584e-02
 7.8140730e-03 2.6948605e-02 4.1110760e-01 4.2825518e-03 1.9108397e-03
 1.3654645e-02 2.3743400e-01 5.7327443e-01 4.3471283e-01 1.4769173e-01
 7.9641188e-04 9.9949539e-01 9.9751258e-01 8.4076158e-04 8.3755314e-01
 2.5304241e-02 2.6868789e-02 3.1998588e-03 4.3726322e-01 5.9315865e-03
 8.9972783e-03 6.9815916e-04 5.9012759e-02 2.4990330e-04 4.8075849e-03
 8.2257116e-01 4.4599531e-04 9.8225236e-01 2.0738766e-02 7.5636342e-02
 3.8195888e-03 1.4926356e-01 7.1931531e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 00:24:19, Dev, Step : 5706, Loss : 0.55171, Acc : 0.779, Auc : 0.860, Sensitive_Loss : 0.20544, Sensitive_Acc : 21.211, Sensitive_Auc : 0.991, Mean auc: 0.860, Run Time : 86.98 sec
INFO:root:2024-04-12 00:24:24, Train, Epoch : 10, Step : 5710, Loss : 0.12501, Acc : 0.347, Sensitive_Loss : 0.11422, Sensitive_Acc : 10.700, Run Time : 3.95 sec
INFO:root:2024-04-12 00:24:31, Train, Epoch : 10, Step : 5720, Loss : 0.29817, Acc : 0.863, Sensitive_Loss : 0.14539, Sensitive_Acc : 19.100, Run Time : 6.82 sec
INFO:root:2024-04-12 00:24:38, Train, Epoch : 10, Step : 5730, Loss : 0.28231, Acc : 0.881, Sensitive_Loss : 0.10075, Sensitive_Acc : 22.000, Run Time : 7.19 sec
INFO:root:2024-04-12 00:24:46, Train, Epoch : 10, Step : 5740, Loss : 0.28633, Acc : 0.887, Sensitive_Loss : 0.16685, Sensitive_Acc : 21.400, Run Time : 7.59 sec
INFO:root:2024-04-12 00:24:53, Train, Epoch : 10, Step : 5750, Loss : 0.34418, Acc : 0.847, Sensitive_Loss : 0.22046, Sensitive_Acc : 16.300, Run Time : 7.24 sec
INFO:root:2024-04-12 00:25:00, Train, Epoch : 10, Step : 5760, Loss : 0.31784, Acc : 0.866, Sensitive_Loss : 0.09116, Sensitive_Acc : 19.100, Run Time : 7.29 sec
INFO:root:2024-04-12 00:25:07, Train, Epoch : 10, Step : 5770, Loss : 0.25124, Acc : 0.906, Sensitive_Loss : 0.11202, Sensitive_Acc : 19.500, Run Time : 6.96 sec
INFO:root:2024-04-12 00:25:14, Train, Epoch : 10, Step : 5780, Loss : 0.28173, Acc : 0.884, Sensitive_Loss : 0.11617, Sensitive_Acc : 24.200, Run Time : 7.01 sec
INFO:root:2024-04-12 00:25:21, Train, Epoch : 10, Step : 5790, Loss : 0.27458, Acc : 0.884, Sensitive_Loss : 0.16281, Sensitive_Acc : 15.500, Run Time : 6.74 sec
INFO:root:2024-04-12 00:25:29, Train, Epoch : 10, Step : 5800, Loss : 0.29728, Acc : 0.872, Sensitive_Loss : 0.14927, Sensitive_Acc : 25.300, Run Time : 7.61 sec
INFO:root:2024-04-12 00:26:57, Dev, Step : 5800, Loss : 0.53377, Acc : 0.782, Auc : 0.858, Sensitive_Loss : 0.21935, Sensitive_Acc : 20.564, Sensitive_Auc : 0.992, Mean auc: 0.858, Run Time : 88.01 sec
INFO:root:2024-04-12 00:27:02, Train, Epoch : 10, Step : 5810, Loss : 0.30511, Acc : 0.869, Sensitive_Loss : 0.16564, Sensitive_Acc : 25.200, Run Time : 93.44 sec
INFO:root:2024-04-12 00:27:10, Train, Epoch : 10, Step : 5820, Loss : 0.27129, Acc : 0.875, Sensitive_Loss : 0.11006, Sensitive_Acc : 17.900, Run Time : 7.50 sec
INFO:root:2024-04-12 00:27:17, Train, Epoch : 10, Step : 5830, Loss : 0.27316, Acc : 0.900, Sensitive_Loss : 0.14530, Sensitive_Acc : 16.100, Run Time : 7.30 sec
INFO:root:2024-04-12 00:27:24, Train, Epoch : 10, Step : 5840, Loss : 0.26914, Acc : 0.894, Sensitive_Loss : 0.13675, Sensitive_Acc : 21.600, Run Time : 6.52 sec
INFO:root:2024-04-12 00:27:31, Train, Epoch : 10, Step : 5850, Loss : 0.30068, Acc : 0.856, Sensitive_Loss : 0.17726, Sensitive_Acc : 21.000, Run Time : 7.53 sec
INFO:root:2024-04-12 00:27:38, Train, Epoch : 10, Step : 5860, Loss : 0.28188, Acc : 0.872, Sensitive_Loss : 0.11735, Sensitive_Acc : 23.800, Run Time : 6.86 sec
INFO:root:2024-04-12 00:27:45, Train, Epoch : 10, Step : 5870, Loss : 0.25059, Acc : 0.891, Sensitive_Loss : 0.15610, Sensitive_Acc : 19.800, Run Time : 7.52 sec
INFO:root:2024-04-12 00:27:53, Train, Epoch : 10, Step : 5880, Loss : 0.25533, Acc : 0.909, Sensitive_Loss : 0.11116, Sensitive_Acc : 21.000, Run Time : 7.41 sec
INFO:root:2024-04-12 00:27:59, Train, Epoch : 10, Step : 5890, Loss : 0.32683, Acc : 0.856, Sensitive_Loss : 0.16540, Sensitive_Acc : 21.900, Run Time : 6.64 sec
INFO:root:2024-04-12 00:28:06, Train, Epoch : 10, Step : 5900, Loss : 0.26898, Acc : 0.872, Sensitive_Loss : 0.14052, Sensitive_Acc : 24.300, Run Time : 6.86 sec
INFO:root:2024-04-12 00:29:34, Dev, Step : 5900, Loss : 0.59452, Acc : 0.773, Auc : 0.858, Sensitive_Loss : 0.23082, Sensitive_Acc : 20.729, Sensitive_Auc : 0.990, Mean auc: 0.858, Run Time : 87.88 sec
INFO:root:2024-04-12 00:29:40, Train, Epoch : 10, Step : 5910, Loss : 0.27868, Acc : 0.894, Sensitive_Loss : 0.10455, Sensitive_Acc : 22.500, Run Time : 93.30 sec
INFO:root:2024-04-12 00:29:47, Train, Epoch : 10, Step : 5920, Loss : 0.30691, Acc : 0.884, Sensitive_Loss : 0.08251, Sensitive_Acc : 19.600, Run Time : 7.52 sec
INFO:root:2024-04-12 00:29:54, Train, Epoch : 10, Step : 5930, Loss : 0.23323, Acc : 0.903, Sensitive_Loss : 0.15457, Sensitive_Acc : 25.100, Run Time : 6.78 sec
INFO:root:2024-04-12 00:30:01, Train, Epoch : 10, Step : 5940, Loss : 0.27421, Acc : 0.881, Sensitive_Loss : 0.16169, Sensitive_Acc : 20.100, Run Time : 7.01 sec
INFO:root:2024-04-12 00:30:08, Train, Epoch : 10, Step : 5950, Loss : 0.33692, Acc : 0.856, Sensitive_Loss : 0.16298, Sensitive_Acc : 23.500, Run Time : 7.25 sec
INFO:root:2024-04-12 00:30:16, Train, Epoch : 10, Step : 5960, Loss : 0.29216, Acc : 0.872, Sensitive_Loss : 0.08579, Sensitive_Acc : 21.100, Run Time : 7.65 sec
INFO:root:2024-04-12 00:30:23, Train, Epoch : 10, Step : 5970, Loss : 0.29235, Acc : 0.859, Sensitive_Loss : 0.09378, Sensitive_Acc : 19.800, Run Time : 7.02 sec
INFO:root:2024-04-12 00:30:30, Train, Epoch : 10, Step : 5980, Loss : 0.28493, Acc : 0.881, Sensitive_Loss : 0.12565, Sensitive_Acc : 16.300, Run Time : 6.89 sec
INFO:root:2024-04-12 00:30:37, Train, Epoch : 10, Step : 5990, Loss : 0.25904, Acc : 0.881, Sensitive_Loss : 0.11342, Sensitive_Acc : 21.300, Run Time : 7.32 sec
INFO:root:2024-04-12 00:30:44, Train, Epoch : 10, Step : 6000, Loss : 0.27306, Acc : 0.897, Sensitive_Loss : 0.14654, Sensitive_Acc : 22.200, Run Time : 6.87 sec
INFO:root:2024-04-12 00:32:12, Dev, Step : 6000, Loss : 0.57461, Acc : 0.775, Auc : 0.858, Sensitive_Loss : 0.21122, Sensitive_Acc : 21.135, Sensitive_Auc : 0.994, Mean auc: 0.858, Run Time : 88.32 sec
INFO:root:2024-04-12 00:32:18, Train, Epoch : 10, Step : 6010, Loss : 0.20205, Acc : 0.919, Sensitive_Loss : 0.12049, Sensitive_Acc : 16.200, Run Time : 94.02 sec
INFO:root:2024-04-12 00:32:25, Train, Epoch : 10, Step : 6020, Loss : 0.30587, Acc : 0.878, Sensitive_Loss : 0.14126, Sensitive_Acc : 16.500, Run Time : 6.92 sec
INFO:root:2024-04-12 00:32:32, Train, Epoch : 10, Step : 6030, Loss : 0.30247, Acc : 0.863, Sensitive_Loss : 0.13031, Sensitive_Acc : 24.200, Run Time : 7.01 sec
INFO:root:2024-04-12 00:32:39, Train, Epoch : 10, Step : 6040, Loss : 0.27548, Acc : 0.897, Sensitive_Loss : 0.17373, Sensitive_Acc : 21.300, Run Time : 7.19 sec
INFO:root:2024-04-12 00:32:46, Train, Epoch : 10, Step : 6050, Loss : 0.27319, Acc : 0.878, Sensitive_Loss : 0.12221, Sensitive_Acc : 27.200, Run Time : 7.20 sec
INFO:root:2024-04-12 00:32:54, Train, Epoch : 10, Step : 6060, Loss : 0.29649, Acc : 0.878, Sensitive_Loss : 0.12740, Sensitive_Acc : 21.700, Run Time : 7.35 sec
INFO:root:2024-04-12 00:33:00, Train, Epoch : 10, Step : 6070, Loss : 0.28744, Acc : 0.912, Sensitive_Loss : 0.15188, Sensitive_Acc : 17.700, Run Time : 6.83 sec
INFO:root:2024-04-12 00:33:07, Train, Epoch : 10, Step : 6080, Loss : 0.28524, Acc : 0.856, Sensitive_Loss : 0.19266, Sensitive_Acc : 22.900, Run Time : 6.92 sec
INFO:root:2024-04-12 00:33:15, Train, Epoch : 10, Step : 6090, Loss : 0.30472, Acc : 0.878, Sensitive_Loss : 0.14235, Sensitive_Acc : 19.500, Run Time : 7.70 sec
INFO:root:2024-04-12 00:33:22, Train, Epoch : 10, Step : 6100, Loss : 0.23501, Acc : 0.891, Sensitive_Loss : 0.14021, Sensitive_Acc : 17.500, Run Time : 7.05 sec
INFO:root:2024-04-12 00:34:50, Dev, Step : 6100, Loss : 0.60622, Acc : 0.770, Auc : 0.854, Sensitive_Loss : 0.21617, Sensitive_Acc : 20.684, Sensitive_Auc : 0.990, Mean auc: 0.854, Run Time : 87.66 sec
INFO:root:2024-04-12 00:34:55, Train, Epoch : 10, Step : 6110, Loss : 0.31244, Acc : 0.869, Sensitive_Loss : 0.13898, Sensitive_Acc : 19.200, Run Time : 93.31 sec
INFO:root:2024-04-12 00:35:02, Train, Epoch : 10, Step : 6120, Loss : 0.28066, Acc : 0.872, Sensitive_Loss : 0.16230, Sensitive_Acc : 18.600, Run Time : 6.93 sec
INFO:root:2024-04-12 00:35:10, Train, Epoch : 10, Step : 6130, Loss : 0.37317, Acc : 0.856, Sensitive_Loss : 0.20401, Sensitive_Acc : 23.000, Run Time : 7.22 sec
INFO:root:2024-04-12 00:35:17, Train, Epoch : 10, Step : 6140, Loss : 0.31279, Acc : 0.847, Sensitive_Loss : 0.15806, Sensitive_Acc : 23.300, Run Time : 7.40 sec
INFO:root:2024-04-12 00:35:25, Train, Epoch : 10, Step : 6150, Loss : 0.27810, Acc : 0.859, Sensitive_Loss : 0.07862, Sensitive_Acc : 20.700, Run Time : 7.50 sec
INFO:root:2024-04-12 00:35:32, Train, Epoch : 10, Step : 6160, Loss : 0.30224, Acc : 0.891, Sensitive_Loss : 0.14088, Sensitive_Acc : 22.000, Run Time : 7.34 sec
INFO:root:2024-04-12 00:35:38, Train, Epoch : 10, Step : 6170, Loss : 0.30280, Acc : 0.878, Sensitive_Loss : 0.11371, Sensitive_Acc : 20.200, Run Time : 6.60 sec
INFO:root:2024-04-12 00:35:45, Train, Epoch : 10, Step : 6180, Loss : 0.24031, Acc : 0.900, Sensitive_Loss : 0.12158, Sensitive_Acc : 20.500, Run Time : 6.92 sec
INFO:root:2024-04-12 00:35:52, Train, Epoch : 10, Step : 6190, Loss : 0.25601, Acc : 0.884, Sensitive_Loss : 0.18094, Sensitive_Acc : 22.600, Run Time : 7.12 sec
INFO:root:2024-04-12 00:36:00, Train, Epoch : 10, Step : 6200, Loss : 0.26004, Acc : 0.894, Sensitive_Loss : 0.16224, Sensitive_Acc : 24.300, Run Time : 7.24 sec
INFO:root:2024-04-12 00:37:28, Dev, Step : 6200, Loss : 0.55296, Acc : 0.781, Auc : 0.858, Sensitive_Loss : 0.22060, Sensitive_Acc : 20.609, Sensitive_Auc : 0.991, Mean auc: 0.858, Run Time : 88.30 sec
INFO:root:2024-04-12 00:37:34, Train, Epoch : 10, Step : 6210, Loss : 0.25692, Acc : 0.866, Sensitive_Loss : 0.14181, Sensitive_Acc : 17.900, Run Time : 93.82 sec
INFO:root:2024-04-12 00:37:41, Train, Epoch : 10, Step : 6220, Loss : 0.25697, Acc : 0.891, Sensitive_Loss : 0.18061, Sensitive_Acc : 22.400, Run Time : 7.18 sec
INFO:root:2024-04-12 00:37:48, Train, Epoch : 10, Step : 6230, Loss : 0.33772, Acc : 0.847, Sensitive_Loss : 0.16679, Sensitive_Acc : 24.900, Run Time : 7.03 sec
INFO:root:2024-04-12 00:37:55, Train, Epoch : 10, Step : 6240, Loss : 0.33167, Acc : 0.887, Sensitive_Loss : 0.15310, Sensitive_Acc : 27.000, Run Time : 7.24 sec
INFO:root:2024-04-12 00:38:02, Train, Epoch : 10, Step : 6250, Loss : 0.29103, Acc : 0.881, Sensitive_Loss : 0.13150, Sensitive_Acc : 26.100, Run Time : 6.95 sec
INFO:root:2024-04-12 00:38:09, Train, Epoch : 10, Step : 6260, Loss : 0.27762, Acc : 0.891, Sensitive_Loss : 0.12651, Sensitive_Acc : 23.000, Run Time : 7.28 sec
INFO:root:2024-04-12 00:38:16, Train, Epoch : 10, Step : 6270, Loss : 0.24967, Acc : 0.875, Sensitive_Loss : 0.10782, Sensitive_Acc : 23.600, Run Time : 7.10 sec
INFO:root:2024-04-12 00:38:23, Train, Epoch : 10, Step : 6280, Loss : 0.30615, Acc : 0.834, Sensitive_Loss : 0.14785, Sensitive_Acc : 21.500, Run Time : 6.98 sec
INFO:root:2024-04-12 00:38:30, Train, Epoch : 10, Step : 6290, Loss : 0.32063, Acc : 0.831, Sensitive_Loss : 0.13339, Sensitive_Acc : 22.000, Run Time : 6.96 sec
INFO:root:2024-04-12 00:38:37, Train, Epoch : 10, Step : 6300, Loss : 0.29632, Acc : 0.875, Sensitive_Loss : 0.13271, Sensitive_Acc : 21.300, Run Time : 7.16 sec
INFO:root:2024-04-12 00:40:06, Dev, Step : 6300, Loss : 0.57169, Acc : 0.777, Auc : 0.855, Sensitive_Loss : 0.21197, Sensitive_Acc : 20.880, Sensitive_Auc : 0.993, Mean auc: 0.855, Run Time : 88.34 sec
INFO:root:2024-04-12 00:40:11, Train, Epoch : 10, Step : 6310, Loss : 0.27823, Acc : 0.869, Sensitive_Loss : 0.19636, Sensitive_Acc : 21.800, Run Time : 93.62 sec
INFO:root:2024-04-12 00:40:19, Train, Epoch : 10, Step : 6320, Loss : 0.32678, Acc : 0.856, Sensitive_Loss : 0.18545, Sensitive_Acc : 18.100, Run Time : 7.55 sec
INFO:root:2024-04-12 00:40:26, Train, Epoch : 10, Step : 6330, Loss : 0.27924, Acc : 0.866, Sensitive_Loss : 0.16276, Sensitive_Acc : 18.200, Run Time : 7.18 sec
INFO:root:2024-04-12 00:40:32, Train, Epoch : 10, Step : 6340, Loss : 0.32527, Acc : 0.881, Sensitive_Loss : 0.20805, Sensitive_Acc : 23.100, Run Time : 6.34 sec
INFO:root:2024-04-12 00:42:00
INFO:root:y_pred: [0.00807321 0.00249697 0.41459516 ... 0.0444878  0.01511338 0.01811488]
INFO:root:y_true: [0. 0. 0. ... 1. 0. 0.]
INFO:root:sensitive_y_pred: [6.45785592e-04 8.45055620e-05 7.84813613e-03 6.05120650e-03
 8.02698079e-03 3.63871106e-04 5.05089154e-03 1.21583911e-02
 4.17406522e-02 9.99843359e-01 6.11691773e-01 1.12611521e-03
 1.84697413e-03 1.69172490e-04 9.99520302e-01 1.31701723e-01
 2.47800606e-03 9.98545527e-01 9.99962211e-01 7.00731180e-04
 4.88884687e-01 1.14827347e-03 9.00786743e-02 1.32577028e-02
 7.48015419e-02 3.75830263e-01 4.56745256e-05 5.61150839e-04
 3.61792103e-04 1.66828092e-02 8.34814832e-03 9.85652685e-01
 6.73286170e-02 8.08562994e-01 2.23485389e-04 1.58890849e-04
 3.89754307e-03 7.66740218e-02 8.62907395e-02 1.56048387e-02
 7.67647177e-02 9.70290184e-01 8.22836999e-03 3.54849151e-04
 8.07021677e-01 8.28865319e-02 6.54108226e-01 8.98946151e-02
 5.98596573e-01 9.76369560e-01 9.89586294e-01 9.99902368e-01
 8.75647068e-01 1.83963962e-03 6.37106895e-01 5.13749599e-01
 5.02197398e-03 1.35496585e-02 9.98385072e-01 2.37268046e-03
 1.63336717e-05 3.30054085e-03 3.08363349e-03 1.25878933e-03
 9.95653152e-01 1.08549118e-01 1.65928423e-03 1.39286116e-01
 1.49795497e-02 9.98029053e-01 9.97978508e-01 9.99956846e-01
 2.23906012e-04 3.40931565e-01 4.90269251e-03 7.53189325e-01
 6.60301298e-02 2.05395278e-04 1.80566031e-02 1.21197931e-03
 4.45003361e-02 1.00940044e-04 9.99666572e-01 9.98673558e-01
 2.31878553e-02 2.73056086e-02 5.37636615e-02 8.73361889e-04
 8.44789222e-02 2.20022933e-03 3.88726890e-02 2.52434462e-01
 1.86110745e-04 5.72270539e-04 5.78173716e-03 1.14192860e-02
 3.19217885e-04 4.06283855e-01 4.78545111e-03 2.77593993e-02
 8.14956613e-03 1.64970700e-02 3.20537657e-01 2.32840935e-03
 1.68601365e-03 1.33693125e-02 1.56678587e-01 4.89648104e-01
 6.07046664e-01 1.63538799e-01 1.18580891e-03 9.99611557e-01
 9.98278856e-01 2.64067581e-04 7.56513894e-01 2.33445335e-02
 1.39234513e-02 9.00357030e-04 5.49475551e-01 3.10667721e-03
 8.97336472e-03 9.03083652e-04 5.67609929e-02 1.69950319e-04
 3.75690381e-03 7.66249478e-01 6.43187726e-04 9.90957618e-01
 2.66471095e-02 9.08154026e-02 1.35719578e-03 1.22554436e-01
 5.22858347e-04]
INFO:root:sensitive_y_true: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.
 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
INFO:root:2024-04-12 00:42:00, Dev, Step : 6340, Loss : 0.55954, Acc : 0.780, Auc : 0.855, Sensitive_Loss : 0.20291, Sensitive_Acc : 20.910, Sensitive_Auc : 0.992, Mean auc: 0.855, Run Time : 87.46 sec
