Running on desktop22:
stdin: is not a tty
Activating chexpert environment...
/home/katkr/.conda/envs/chexpert/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/random_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/random_dataset_val.csv",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": 0.1,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-03-29 20:13:09, Train, Epoch : 1, Step : 10, Loss : 1.23600, Acc : 0.528, Sensitive_Loss : 0.78104, Sensitive_Acc : 15.000, Run Time : 16.65 sec
INFO:root:2024-03-29 20:13:21, Train, Epoch : 1, Step : 20, Loss : 0.98068, Acc : 0.512, Sensitive_Loss : 0.69896, Sensitive_Acc : 15.500, Run Time : 11.53 sec
INFO:root:2024-03-29 20:13:35, Train, Epoch : 1, Step : 30, Loss : 1.13185, Acc : 0.509, Sensitive_Loss : 0.78928, Sensitive_Acc : 16.200, Run Time : 14.52 sec
INFO:root:2024-03-29 20:13:47, Train, Epoch : 1, Step : 40, Loss : 1.49480, Acc : 0.506, Sensitive_Loss : 0.72863, Sensitive_Acc : 16.700, Run Time : 11.43 sec
INFO:root:2024-03-29 20:13:58, Train, Epoch : 1, Step : 50, Loss : 1.28289, Acc : 0.491, Sensitive_Loss : 0.69234, Sensitive_Acc : 16.000, Run Time : 11.46 sec
INFO:root:2024-03-29 20:14:13, Train, Epoch : 1, Step : 60, Loss : 1.06582, Acc : 0.519, Sensitive_Loss : 0.75256, Sensitive_Acc : 17.900, Run Time : 14.16 sec
INFO:root:2024-03-29 20:14:24, Train, Epoch : 1, Step : 70, Loss : 1.30661, Acc : 0.553, Sensitive_Loss : 0.71197, Sensitive_Acc : 16.900, Run Time : 11.48 sec
INFO:root:2024-03-29 20:14:37, Train, Epoch : 1, Step : 80, Loss : 0.79276, Acc : 0.512, Sensitive_Loss : 0.67403, Sensitive_Acc : 15.800, Run Time : 12.59 sec
INFO:root:2024-03-29 20:14:51, Train, Epoch : 1, Step : 90, Loss : 1.22345, Acc : 0.534, Sensitive_Loss : 0.64621, Sensitive_Acc : 16.300, Run Time : 14.65 sec
INFO:root:2024-03-29 20:15:03, Train, Epoch : 1, Step : 100, Loss : 1.28301, Acc : 0.509, Sensitive_Loss : 0.63303, Sensitive_Acc : 15.900, Run Time : 11.35 sec
INFO:root:2024-03-29 20:17:59, Dev, Step : 100, Loss : 1.11953, Acc : 0.402, Auc : 0.661, Sensitive_Loss : 0.71431, Sensitive_Acc : 15.404, Sensitive_Auc : 0.698, Mean auc: 0.661, Run Time : 176.81 sec
INFO:root:2024-03-29 20:18:00, Best, Step : 100, Loss : 1.11953, Acc : 0.402, Auc : 0.661, Sensitive_Loss : 0.71431, Sensitive_Acc : 15.404, Sensitive_Auc : 0.698, Best Auc : 0.661
INFO:root:2024-03-29 20:18:08, Train, Epoch : 1, Step : 110, Loss : 0.79803, Acc : 0.481, Sensitive_Loss : 0.72033, Sensitive_Acc : 16.800, Run Time : 185.14 sec
INFO:root:2024-03-29 20:18:21, Train, Epoch : 1, Step : 120, Loss : 1.29916, Acc : 0.584, Sensitive_Loss : 0.69947, Sensitive_Acc : 16.500, Run Time : 13.48 sec
INFO:root:2024-03-29 20:18:38, Train, Epoch : 1, Step : 130, Loss : 1.19758, Acc : 0.562, Sensitive_Loss : 0.62324, Sensitive_Acc : 17.200, Run Time : 16.30 sec
INFO:root:2024-03-29 20:18:49, Train, Epoch : 1, Step : 140, Loss : 0.96900, Acc : 0.584, Sensitive_Loss : 0.57259, Sensitive_Acc : 17.400, Run Time : 11.62 sec
INFO:root:2024-03-29 20:19:05, Train, Epoch : 1, Step : 150, Loss : 1.31569, Acc : 0.522, Sensitive_Loss : 0.62785, Sensitive_Acc : 15.000, Run Time : 16.00 sec
INFO:root:2024-03-29 20:19:17, Train, Epoch : 1, Step : 160, Loss : 1.09746, Acc : 0.559, Sensitive_Loss : 0.56717, Sensitive_Acc : 15.500, Run Time : 12.10 sec
INFO:root:2024-03-29 20:19:29, Train, Epoch : 1, Step : 170, Loss : 0.99691, Acc : 0.541, Sensitive_Loss : 0.62717, Sensitive_Acc : 18.200, Run Time : 11.81 sec
INFO:root:2024-03-29 20:19:41, Train, Epoch : 1, Step : 180, Loss : 1.20155, Acc : 0.566, Sensitive_Loss : 0.57092, Sensitive_Acc : 15.500, Run Time : 12.24 sec
INFO:root:2024-03-29 20:19:54, Train, Epoch : 1, Step : 190, Loss : 0.92737, Acc : 0.572, Sensitive_Loss : 0.62004, Sensitive_Acc : 14.700, Run Time : 12.39 sec
INFO:root:2024-03-29 20:20:06, Train, Epoch : 1, Step : 200, Loss : 1.15143, Acc : 0.556, Sensitive_Loss : 0.62104, Sensitive_Acc : 16.200, Run Time : 12.01 sec
INFO:root:2024-03-29 20:22:37, Dev, Step : 200, Loss : 1.06782, Acc : 0.584, Auc : 0.697, Sensitive_Loss : 0.61576, Sensitive_Acc : 16.312, Sensitive_Auc : 0.821, Mean auc: 0.697, Run Time : 151.04 sec
INFO:root:2024-03-29 20:22:38, Best, Step : 200, Loss : 1.06782, Acc : 0.584, Auc : 0.697, Sensitive_Loss : 0.61576, Sensitive_Acc : 16.312, Sensitive_Auc : 0.821, Best Auc : 0.697
INFO:root:2024-03-29 20:22:46, Train, Epoch : 1, Step : 210, Loss : 1.17712, Acc : 0.547, Sensitive_Loss : 0.63870, Sensitive_Acc : 17.500, Run Time : 160.46 sec
INFO:root:2024-03-29 20:22:57, Train, Epoch : 1, Step : 220, Loss : 1.07848, Acc : 0.503, Sensitive_Loss : 0.55733, Sensitive_Acc : 15.400, Run Time : 11.33 sec
INFO:root:2024-03-29 20:23:13, Train, Epoch : 1, Step : 230, Loss : 1.15596, Acc : 0.537, Sensitive_Loss : 0.64315, Sensitive_Acc : 16.200, Run Time : 15.58 sec
INFO:root:2024-03-29 20:23:25, Train, Epoch : 1, Step : 240, Loss : 1.08553, Acc : 0.584, Sensitive_Loss : 0.56599, Sensitive_Acc : 15.900, Run Time : 11.74 sec
INFO:root:2024-03-29 20:23:37, Train, Epoch : 1, Step : 250, Loss : 0.90313, Acc : 0.578, Sensitive_Loss : 0.56461, Sensitive_Acc : 16.800, Run Time : 11.96 sec
INFO:root:2024-03-29 20:23:50, Train, Epoch : 1, Step : 260, Loss : 1.26466, Acc : 0.544, Sensitive_Loss : 0.56562, Sensitive_Acc : 16.200, Run Time : 12.94 sec
INFO:root:2024-03-29 20:24:01, Train, Epoch : 1, Step : 270, Loss : 0.96594, Acc : 0.572, Sensitive_Loss : 0.43833, Sensitive_Acc : 17.400, Run Time : 11.56 sec
INFO:root:2024-03-29 20:24:16, Train, Epoch : 1, Step : 280, Loss : 1.22039, Acc : 0.591, Sensitive_Loss : 0.47679, Sensitive_Acc : 16.700, Run Time : 14.89 sec
INFO:root:2024-03-29 20:24:28, Train, Epoch : 1, Step : 290, Loss : 1.18539, Acc : 0.566, Sensitive_Loss : 0.40536, Sensitive_Acc : 14.500, Run Time : 11.75 sec
INFO:root:2024-03-29 20:24:40, Train, Epoch : 1, Step : 300, Loss : 1.13033, Acc : 0.544, Sensitive_Loss : 0.45462, Sensitive_Acc : 16.200, Run Time : 11.65 sec
INFO:root:2024-03-29 20:26:41, Dev, Step : 300, Loss : 1.17337, Acc : 0.418, Auc : 0.710, Sensitive_Loss : 0.49580, Sensitive_Acc : 16.270, Sensitive_Auc : 0.935, Mean auc: 0.710, Run Time : 120.95 sec
INFO:root:2024-03-29 20:26:41, Best, Step : 300, Loss : 1.17337, Acc : 0.418, Auc : 0.710, Sensitive_Loss : 0.49580, Sensitive_Acc : 16.270, Sensitive_Auc : 0.935, Best Auc : 0.710
INFO:root:2024-03-29 20:26:48, Train, Epoch : 1, Step : 310, Loss : 0.77367, Acc : 0.591, Sensitive_Loss : 0.49500, Sensitive_Acc : 17.100, Run Time : 128.73 sec
INFO:root:2024-03-29 20:27:00, Train, Epoch : 1, Step : 320, Loss : 1.04974, Acc : 0.594, Sensitive_Loss : 0.49227, Sensitive_Acc : 17.500, Run Time : 11.64 sec
INFO:root:2024-03-29 20:27:14, Train, Epoch : 1, Step : 330, Loss : 1.27139, Acc : 0.566, Sensitive_Loss : 0.41004, Sensitive_Acc : 15.000, Run Time : 14.51 sec
INFO:root:2024-03-29 20:27:26, Train, Epoch : 1, Step : 340, Loss : 1.23174, Acc : 0.569, Sensitive_Loss : 0.45861, Sensitive_Acc : 15.200, Run Time : 11.14 sec
INFO:root:2024-03-29 20:27:40, Train, Epoch : 1, Step : 350, Loss : 1.06486, Acc : 0.628, Sensitive_Loss : 0.54414, Sensitive_Acc : 17.200, Run Time : 14.36 sec
INFO:root:2024-03-29 20:27:52, Train, Epoch : 1, Step : 360, Loss : 1.18746, Acc : 0.581, Sensitive_Loss : 0.52921, Sensitive_Acc : 16.200, Run Time : 12.42 sec
INFO:root:2024-03-29 20:28:03, Train, Epoch : 1, Step : 370, Loss : 1.21149, Acc : 0.575, Sensitive_Loss : 0.51482, Sensitive_Acc : 16.500, Run Time : 11.04 sec
INFO:root:2024-03-29 20:28:18, Train, Epoch : 1, Step : 380, Loss : 1.10818, Acc : 0.562, Sensitive_Loss : 0.43156, Sensitive_Acc : 15.800, Run Time : 14.74 sec
INFO:root:2024-03-29 20:28:30, Train, Epoch : 1, Step : 390, Loss : 1.22156, Acc : 0.544, Sensitive_Loss : 0.45180, Sensitive_Acc : 14.600, Run Time : 11.83 sec
INFO:root:2024-03-29 20:28:43, Train, Epoch : 1, Step : 400, Loss : 1.17361, Acc : 0.619, Sensitive_Loss : 0.45372, Sensitive_Acc : 17.400, Run Time : 13.32 sec
INFO:root:2024-03-29 20:30:21, Dev, Step : 400, Loss : 1.43478, Acc : 0.185, Auc : 0.649, Sensitive_Loss : 0.55354, Sensitive_Acc : 15.787, Sensitive_Auc : 0.934, Mean auc: 0.649, Run Time : 97.64 sec
INFO:root:2024-03-29 20:30:30, Train, Epoch : 1, Step : 410, Loss : 1.34715, Acc : 0.581, Sensitive_Loss : 0.45543, Sensitive_Acc : 15.300, Run Time : 106.22 sec
INFO:root:2024-03-29 20:30:41, Train, Epoch : 1, Step : 420, Loss : 0.86427, Acc : 0.603, Sensitive_Loss : 0.45396, Sensitive_Acc : 15.200, Run Time : 11.90 sec
INFO:root:2024-03-29 20:30:55, Train, Epoch : 1, Step : 430, Loss : 1.27108, Acc : 0.572, Sensitive_Loss : 0.49603, Sensitive_Acc : 18.100, Run Time : 14.03 sec
INFO:root:2024-03-29 20:31:10, Train, Epoch : 1, Step : 440, Loss : 1.19538, Acc : 0.597, Sensitive_Loss : 0.45851, Sensitive_Acc : 16.800, Run Time : 14.97 sec
INFO:root:2024-03-29 20:31:23, Train, Epoch : 1, Step : 450, Loss : 1.03643, Acc : 0.594, Sensitive_Loss : 0.42629, Sensitive_Acc : 15.400, Run Time : 12.26 sec
INFO:root:2024-03-29 20:31:35, Train, Epoch : 1, Step : 460, Loss : 0.91439, Acc : 0.566, Sensitive_Loss : 0.41558, Sensitive_Acc : 16.000, Run Time : 12.35 sec
INFO:root:2024-03-29 20:31:49, Train, Epoch : 1, Step : 470, Loss : 1.01603, Acc : 0.559, Sensitive_Loss : 0.45716, Sensitive_Acc : 16.300, Run Time : 13.86 sec
INFO:root:2024-03-29 20:32:01, Train, Epoch : 1, Step : 480, Loss : 1.30548, Acc : 0.600, Sensitive_Loss : 0.38452, Sensitive_Acc : 14.200, Run Time : 12.42 sec
INFO:root:2024-03-29 20:32:18, Train, Epoch : 1, Step : 490, Loss : 1.00398, Acc : 0.616, Sensitive_Loss : 0.39819, Sensitive_Acc : 16.100, Run Time : 17.07 sec
INFO:root:2024-03-29 20:33:56
INFO:root:y_pred: [0.32734993 0.0471265  0.20489728 ... 0.10905294 0.09481464 0.13161974]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [3.72941308e-02 6.08965695e-01 6.52771056e-01 4.84004885e-01
 9.08366084e-01 1.35492891e-01 4.73273933e-01 4.23898488e-01
 1.96066648e-02 2.84245253e-01 1.75267562e-01 1.40568148e-03
 9.09274161e-01 1.71272736e-02 9.90351260e-01 9.35384214e-01
 1.89055931e-02 1.35189548e-01 7.15780377e-01 8.68004560e-01
 2.24076152e-01 4.16749306e-02 9.98097956e-01 2.46412739e-01
 6.40650690e-01 5.36274552e-01 9.02651399e-02 9.57823217e-01
 1.32123232e-01 6.70908511e-01 9.80399489e-01 7.70333529e-01
 5.03039025e-02 1.13615826e-01 4.55669135e-01 6.03049397e-02
 8.86751771e-01 8.40149701e-01 8.05354357e-01 9.33110476e-01
 3.60792577e-01 2.18306147e-02 9.27784622e-01 9.83495653e-01
 3.78156304e-01 3.56757194e-02 7.04237595e-02 3.78600538e-01
 7.42492139e-01 2.16407657e-01 9.61709559e-01 2.95547783e-01
 9.87279058e-01 4.00411963e-01 8.02832782e-01 8.54826450e-01
 8.58682036e-01 4.39860858e-02 9.59878564e-01 7.71168992e-02
 2.53639324e-03 7.13607073e-01 4.68297750e-01 8.50087941e-01
 9.27868009e-01 9.92812335e-01 3.54684561e-01 9.83120918e-01
 9.79598224e-01 8.85423362e-01 9.19411361e-01 5.77406406e-01
 1.34268060e-01 6.22466624e-01 7.54227459e-01 4.94497977e-02
 8.80449831e-01 7.08978847e-02 9.67302501e-01 2.62389511e-01
 8.09249103e-01 3.24575752e-02 5.28543862e-03 7.93075323e-01
 8.84471238e-01 5.17551899e-01 5.61137259e-01 8.17873061e-01
 2.74124444e-01 5.09734869e-01 1.06090136e-01 6.65800214e-01
 6.88156188e-02 2.74480823e-02 6.92119002e-01 8.46073806e-01
 9.83164236e-02 1.56427901e-02 8.84370506e-01 5.96353352e-01
 8.60898256e-01 9.38180983e-02 9.49784517e-01 2.69826859e-01
 2.07653970e-01 8.23343635e-01 5.81504703e-01 5.08748032e-02
 8.42091218e-02 5.07725775e-02 1.08865481e-02 9.71383214e-01
 4.65392560e-01 3.14446598e-01 6.52420595e-02 9.81982681e-04
 9.78334546e-01 2.04354208e-02 5.95787346e-01 5.07597290e-02
 6.85228229e-01 6.64258003e-02 9.68612432e-01 4.03639674e-02
 9.14172351e-01 9.85297740e-01 1.64985415e-02 7.80519605e-01
 5.82782149e-01 9.78357017e-01 5.81697235e-03 9.40934598e-01
 1.73545759e-02 9.47276711e-01 9.80336487e-01 7.99892604e-01
 8.38082135e-01 7.91684687e-01 1.25779375e-01 9.56349373e-01
 7.33267009e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 20:33:56, Dev, Step : 492, Loss : 1.18279, Acc : 0.842, Auc : 0.703, Sensitive_Loss : 0.41504, Sensitive_Acc : 16.539, Sensitive_Auc : 0.939, Mean auc: 0.703, Run Time : 95.71 sec
INFO:root:2024-03-29 20:34:07, Train, Epoch : 2, Step : 500, Loss : 0.81535, Acc : 0.463, Sensitive_Loss : 0.33145, Sensitive_Acc : 11.100, Run Time : 8.84 sec
INFO:root:2024-03-29 20:35:43, Dev, Step : 500, Loss : 1.10476, Acc : 0.765, Auc : 0.706, Sensitive_Loss : 0.39550, Sensitive_Acc : 16.652, Sensitive_Auc : 0.948, Mean auc: 0.706, Run Time : 96.36 sec
INFO:root:2024-03-29 20:35:50, Train, Epoch : 2, Step : 510, Loss : 0.94950, Acc : 0.637, Sensitive_Loss : 0.34467, Sensitive_Acc : 14.600, Run Time : 103.62 sec
INFO:root:2024-03-29 20:36:02, Train, Epoch : 2, Step : 520, Loss : 1.05042, Acc : 0.584, Sensitive_Loss : 0.39102, Sensitive_Acc : 14.700, Run Time : 11.24 sec
INFO:root:2024-03-29 20:36:14, Train, Epoch : 2, Step : 530, Loss : 0.95147, Acc : 0.619, Sensitive_Loss : 0.45037, Sensitive_Acc : 16.800, Run Time : 12.29 sec
INFO:root:2024-03-29 20:36:27, Train, Epoch : 2, Step : 540, Loss : 1.17568, Acc : 0.584, Sensitive_Loss : 0.36931, Sensitive_Acc : 17.200, Run Time : 13.39 sec
INFO:root:2024-03-29 20:36:39, Train, Epoch : 2, Step : 550, Loss : 0.94609, Acc : 0.641, Sensitive_Loss : 0.42454, Sensitive_Acc : 15.800, Run Time : 11.25 sec
INFO:root:2024-03-29 20:36:51, Train, Epoch : 2, Step : 560, Loss : 1.08668, Acc : 0.619, Sensitive_Loss : 0.34471, Sensitive_Acc : 18.200, Run Time : 12.71 sec
INFO:root:2024-03-29 20:37:07, Train, Epoch : 2, Step : 570, Loss : 1.13377, Acc : 0.591, Sensitive_Loss : 0.34141, Sensitive_Acc : 16.000, Run Time : 15.33 sec
INFO:root:2024-03-29 20:37:18, Train, Epoch : 2, Step : 580, Loss : 0.96630, Acc : 0.566, Sensitive_Loss : 0.38909, Sensitive_Acc : 16.300, Run Time : 10.95 sec
INFO:root:2024-03-29 20:37:29, Train, Epoch : 2, Step : 590, Loss : 1.28312, Acc : 0.613, Sensitive_Loss : 0.35803, Sensitive_Acc : 16.300, Run Time : 11.34 sec
INFO:root:2024-03-29 20:37:41, Train, Epoch : 2, Step : 600, Loss : 1.02585, Acc : 0.628, Sensitive_Loss : 0.36577, Sensitive_Acc : 17.000, Run Time : 12.06 sec
INFO:root:2024-03-29 20:39:19, Dev, Step : 600, Loss : 1.12839, Acc : 0.849, Auc : 0.693, Sensitive_Loss : 0.38916, Sensitive_Acc : 16.553, Sensitive_Auc : 0.964, Mean auc: 0.693, Run Time : 98.16 sec
INFO:root:2024-03-29 20:39:27, Train, Epoch : 2, Step : 610, Loss : 0.94882, Acc : 0.641, Sensitive_Loss : 0.35388, Sensitive_Acc : 16.500, Run Time : 105.56 sec
INFO:root:2024-03-29 20:39:37, Train, Epoch : 2, Step : 620, Loss : 0.91302, Acc : 0.637, Sensitive_Loss : 0.33655, Sensitive_Acc : 15.400, Run Time : 10.54 sec
INFO:root:2024-03-29 20:39:50, Train, Epoch : 2, Step : 630, Loss : 0.89007, Acc : 0.688, Sensitive_Loss : 0.32926, Sensitive_Acc : 15.400, Run Time : 12.60 sec
INFO:root:2024-03-29 20:40:00, Train, Epoch : 2, Step : 640, Loss : 0.99554, Acc : 0.616, Sensitive_Loss : 0.40021, Sensitive_Acc : 15.600, Run Time : 10.09 sec
INFO:root:2024-03-29 20:40:10, Train, Epoch : 2, Step : 650, Loss : 1.01082, Acc : 0.666, Sensitive_Loss : 0.38269, Sensitive_Acc : 18.700, Run Time : 10.24 sec
INFO:root:2024-03-29 20:40:21, Train, Epoch : 2, Step : 660, Loss : 1.02209, Acc : 0.641, Sensitive_Loss : 0.33296, Sensitive_Acc : 18.300, Run Time : 11.29 sec
INFO:root:2024-03-29 20:40:34, Train, Epoch : 2, Step : 670, Loss : 1.09435, Acc : 0.628, Sensitive_Loss : 0.33679, Sensitive_Acc : 17.400, Run Time : 12.71 sec
INFO:root:2024-03-29 20:40:44, Train, Epoch : 2, Step : 680, Loss : 1.14169, Acc : 0.622, Sensitive_Loss : 0.33826, Sensitive_Acc : 16.100, Run Time : 10.19 sec
INFO:root:2024-03-29 20:40:54, Train, Epoch : 2, Step : 690, Loss : 0.94492, Acc : 0.603, Sensitive_Loss : 0.34572, Sensitive_Acc : 15.400, Run Time : 9.63 sec
INFO:root:2024-03-29 20:41:07, Train, Epoch : 2, Step : 700, Loss : 1.11158, Acc : 0.653, Sensitive_Loss : 0.35953, Sensitive_Acc : 15.300, Run Time : 13.06 sec
INFO:root:2024-03-29 20:42:44, Dev, Step : 700, Loss : 1.12537, Acc : 0.838, Auc : 0.706, Sensitive_Loss : 0.31260, Sensitive_Acc : 16.908, Sensitive_Auc : 0.968, Mean auc: 0.706, Run Time : 96.89 sec
INFO:root:2024-03-29 20:42:51, Train, Epoch : 2, Step : 710, Loss : 0.81810, Acc : 0.647, Sensitive_Loss : 0.30612, Sensitive_Acc : 14.000, Run Time : 104.43 sec
INFO:root:2024-03-29 20:43:02, Train, Epoch : 2, Step : 720, Loss : 0.79693, Acc : 0.606, Sensitive_Loss : 0.32194, Sensitive_Acc : 16.200, Run Time : 10.79 sec
INFO:root:2024-03-29 20:43:13, Train, Epoch : 2, Step : 730, Loss : 0.96860, Acc : 0.634, Sensitive_Loss : 0.31607, Sensitive_Acc : 16.700, Run Time : 10.55 sec
INFO:root:2024-03-29 20:43:24, Train, Epoch : 2, Step : 740, Loss : 1.22781, Acc : 0.641, Sensitive_Loss : 0.35893, Sensitive_Acc : 16.500, Run Time : 11.55 sec
INFO:root:2024-03-29 20:43:34, Train, Epoch : 2, Step : 750, Loss : 1.22026, Acc : 0.609, Sensitive_Loss : 0.34308, Sensitive_Acc : 18.900, Run Time : 10.25 sec
INFO:root:2024-03-29 20:43:45, Train, Epoch : 2, Step : 760, Loss : 0.97779, Acc : 0.688, Sensitive_Loss : 0.32932, Sensitive_Acc : 18.500, Run Time : 10.64 sec
INFO:root:2024-03-29 20:43:57, Train, Epoch : 2, Step : 770, Loss : 1.13447, Acc : 0.672, Sensitive_Loss : 0.38240, Sensitive_Acc : 15.600, Run Time : 11.86 sec
INFO:root:2024-03-29 20:44:09, Train, Epoch : 2, Step : 780, Loss : 1.12090, Acc : 0.634, Sensitive_Loss : 0.32505, Sensitive_Acc : 16.600, Run Time : 11.76 sec
INFO:root:2024-03-29 20:44:19, Train, Epoch : 2, Step : 790, Loss : 0.98798, Acc : 0.625, Sensitive_Loss : 0.30787, Sensitive_Acc : 15.700, Run Time : 10.09 sec
INFO:root:2024-03-29 20:44:29, Train, Epoch : 2, Step : 800, Loss : 1.13236, Acc : 0.634, Sensitive_Loss : 0.36693, Sensitive_Acc : 17.700, Run Time : 10.14 sec
INFO:root:2024-03-29 20:46:05, Dev, Step : 800, Loss : 1.02049, Acc : 0.545, Auc : 0.752, Sensitive_Loss : 0.32511, Sensitive_Acc : 16.894, Sensitive_Auc : 0.983, Mean auc: 0.752, Run Time : 96.29 sec
INFO:root:2024-03-29 20:46:07, Best, Step : 800, Loss : 1.02049, Acc : 0.545, Auc : 0.752, Sensitive_Loss : 0.32511, Sensitive_Acc : 16.894, Sensitive_Auc : 0.983, Best Auc : 0.752
INFO:root:2024-03-29 20:46:17, Train, Epoch : 2, Step : 810, Loss : 1.12711, Acc : 0.609, Sensitive_Loss : 0.30559, Sensitive_Acc : 15.800, Run Time : 107.83 sec
INFO:root:2024-03-29 20:46:28, Train, Epoch : 2, Step : 820, Loss : 1.15537, Acc : 0.619, Sensitive_Loss : 0.30693, Sensitive_Acc : 15.400, Run Time : 11.38 sec
INFO:root:2024-03-29 20:46:39, Train, Epoch : 2, Step : 830, Loss : 1.25517, Acc : 0.622, Sensitive_Loss : 0.30817, Sensitive_Acc : 16.900, Run Time : 11.00 sec
INFO:root:2024-03-29 20:46:54, Train, Epoch : 2, Step : 840, Loss : 1.22368, Acc : 0.637, Sensitive_Loss : 0.38212, Sensitive_Acc : 15.100, Run Time : 14.91 sec
INFO:root:2024-03-29 20:47:05, Train, Epoch : 2, Step : 850, Loss : 0.94431, Acc : 0.678, Sensitive_Loss : 0.31681, Sensitive_Acc : 18.500, Run Time : 10.44 sec
INFO:root:2024-03-29 20:47:16, Train, Epoch : 2, Step : 860, Loss : 0.88686, Acc : 0.706, Sensitive_Loss : 0.33790, Sensitive_Acc : 17.300, Run Time : 11.20 sec
INFO:root:2024-03-29 20:47:29, Train, Epoch : 2, Step : 870, Loss : 1.11626, Acc : 0.678, Sensitive_Loss : 0.29457, Sensitive_Acc : 17.100, Run Time : 13.38 sec
INFO:root:2024-03-29 20:47:39, Train, Epoch : 2, Step : 880, Loss : 1.26665, Acc : 0.597, Sensitive_Loss : 0.41583, Sensitive_Acc : 17.000, Run Time : 9.90 sec
INFO:root:2024-03-29 20:47:51, Train, Epoch : 2, Step : 890, Loss : 1.16607, Acc : 0.650, Sensitive_Loss : 0.29957, Sensitive_Acc : 16.300, Run Time : 11.84 sec
INFO:root:2024-03-29 20:48:01, Train, Epoch : 2, Step : 900, Loss : 0.73902, Acc : 0.700, Sensitive_Loss : 0.30933, Sensitive_Acc : 17.000, Run Time : 10.56 sec
INFO:root:2024-03-29 20:49:38, Dev, Step : 900, Loss : 0.96580, Acc : 0.756, Auc : 0.765, Sensitive_Loss : 0.38289, Sensitive_Acc : 16.411, Sensitive_Auc : 0.974, Mean auc: 0.765, Run Time : 96.80 sec
INFO:root:2024-03-29 20:49:39, Best, Step : 900, Loss : 0.96580, Acc : 0.756, Auc : 0.765, Sensitive_Loss : 0.38289, Sensitive_Acc : 16.411, Sensitive_Auc : 0.974, Best Auc : 0.765
INFO:root:2024-03-29 20:49:47, Train, Epoch : 2, Step : 910, Loss : 1.11659, Acc : 0.619, Sensitive_Loss : 0.30830, Sensitive_Acc : 15.800, Run Time : 105.63 sec
INFO:root:2024-03-29 20:50:00, Train, Epoch : 2, Step : 920, Loss : 0.89592, Acc : 0.656, Sensitive_Loss : 0.27744, Sensitive_Acc : 15.300, Run Time : 12.75 sec
INFO:root:2024-03-29 20:50:11, Train, Epoch : 2, Step : 930, Loss : 0.97951, Acc : 0.675, Sensitive_Loss : 0.25608, Sensitive_Acc : 17.600, Run Time : 10.83 sec
INFO:root:2024-03-29 20:50:21, Train, Epoch : 2, Step : 940, Loss : 1.02883, Acc : 0.659, Sensitive_Loss : 0.30017, Sensitive_Acc : 14.500, Run Time : 10.65 sec
INFO:root:2024-03-29 20:50:31, Train, Epoch : 2, Step : 950, Loss : 1.01588, Acc : 0.662, Sensitive_Loss : 0.25660, Sensitive_Acc : 17.900, Run Time : 10.06 sec
INFO:root:2024-03-29 20:50:42, Train, Epoch : 2, Step : 960, Loss : 0.97256, Acc : 0.716, Sensitive_Loss : 0.30400, Sensitive_Acc : 17.100, Run Time : 10.27 sec
INFO:root:2024-03-29 20:50:52, Train, Epoch : 2, Step : 970, Loss : 0.97419, Acc : 0.678, Sensitive_Loss : 0.28445, Sensitive_Acc : 18.700, Run Time : 10.77 sec
INFO:root:2024-03-29 20:51:05, Train, Epoch : 2, Step : 980, Loss : 0.86382, Acc : 0.637, Sensitive_Loss : 0.27115, Sensitive_Acc : 15.600, Run Time : 12.21 sec
INFO:root:2024-03-29 20:52:45
INFO:root:y_pred: [0.46012804 0.14486022 0.28506312 ... 0.68537647 0.8627999  0.09341849]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [5.95090818e-03 7.31502891e-01 9.97308135e-01 1.75205544e-01
 9.75986302e-01 4.34182882e-02 5.70527792e-01 7.87447914e-02
 1.42899118e-02 1.90947115e-01 2.51854844e-02 3.50116760e-01
 9.03259695e-01 1.08404430e-02 9.74812329e-01 9.96275663e-01
 1.95606425e-03 4.09865052e-01 9.53080416e-01 9.28716421e-01
 6.12961054e-01 8.27299058e-02 9.99700546e-01 9.30253685e-01
 3.10817033e-01 9.25063372e-01 1.76039264e-02 9.92388904e-01
 1.84646562e-01 9.54203248e-01 9.97188270e-01 8.54518771e-01
 1.86719038e-02 3.40937317e-04 6.97886586e-01 1.17524534e-01
 4.54715788e-02 4.93955433e-01 9.79484499e-01 9.94643331e-01
 3.31524387e-02 1.07549354e-02 9.67282355e-01 9.99000967e-01
 9.52877402e-02 7.56298453e-02 1.28312372e-02 4.28012133e-01
 9.82616723e-01 6.38974309e-01 9.97179389e-01 7.30969906e-01
 9.95825171e-01 9.11404192e-01 9.34450626e-01 9.34444547e-01
 9.26979244e-01 3.85811031e-02 9.99203980e-01 8.27042684e-02
 5.54622675e-04 2.12970585e-01 7.08891809e-01 9.92453992e-01
 9.83805001e-01 9.96920824e-01 8.55546772e-01 9.98167276e-01
 9.12315249e-01 9.81811225e-01 9.88600314e-01 5.37200980e-02
 4.07725349e-02 8.88696969e-01 8.51879060e-01 1.45971319e-02
 9.62195516e-01 9.85622592e-03 9.98916268e-01 4.39181626e-01
 4.65848684e-01 4.92771603e-02 3.68593745e-02 1.45808697e-01
 9.65641499e-01 1.69396177e-01 7.41598904e-01 5.30723393e-01
 9.35974717e-02 3.09777427e-02 8.93043540e-03 1.13060161e-01
 1.60977602e-01 3.23698781e-02 9.15276706e-01 7.85581350e-01
 1.95841007e-02 2.06457507e-02 9.32105362e-01 6.86242759e-01
 9.67794299e-01 6.23204075e-02 9.99617338e-01 1.65545449e-01
 4.24270667e-02 9.44116771e-01 3.41565311e-01 1.27000093e-01
 1.21511417e-02 7.02258050e-02 1.29898256e-02 9.80887890e-01
 2.84046113e-01 1.29648745e-02 2.43836939e-01 1.09832324e-02
 9.78220105e-01 8.96038637e-02 5.91273367e-01 3.55711882e-03
 6.53605044e-01 1.84777472e-02 9.95517910e-01 7.70090288e-03
 6.26751706e-02 9.99314308e-01 6.99686259e-02 9.07558262e-01
 8.74520957e-01 9.84041452e-01 5.80408319e-04 9.68068421e-01
 1.79440044e-02 9.99973774e-01 9.98095572e-01 3.55296761e-01
 6.67042553e-01 6.61387324e-01 1.69946924e-02 8.84746790e-01
 9.94263232e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 20:52:45, Dev, Step : 984, Loss : 0.93787, Acc : 0.729, Auc : 0.780, Sensitive_Loss : 0.31596, Sensitive_Acc : 16.723, Sensitive_Auc : 0.988, Mean auc: 0.780, Run Time : 95.78 sec
INFO:root:2024-03-29 20:52:45, Best, Step : 984, Loss : 0.93787, Acc : 0.729,Auc : 0.780, Best Auc : 0.780, Sensitive_Loss : 0.31596, Sensitive_Acc : 16.723, Sensitive_Auc : 0.988
INFO:root:2024-03-29 20:52:53, Train, Epoch : 3, Step : 990, Loss : 0.60844, Acc : 0.431, Sensitive_Loss : 0.16292, Sensitive_Acc : 9.500, Run Time : 7.10 sec
INFO:root:2024-03-29 20:53:03, Train, Epoch : 3, Step : 1000, Loss : 0.80194, Acc : 0.697, Sensitive_Loss : 0.32749, Sensitive_Acc : 19.600, Run Time : 9.91 sec
INFO:root:2024-03-29 20:54:39, Dev, Step : 1000, Loss : 0.94317, Acc : 0.677, Auc : 0.780, Sensitive_Loss : 0.29785, Sensitive_Acc : 16.879, Sensitive_Auc : 0.990, Mean auc: 0.780, Run Time : 96.22 sec
INFO:root:2024-03-29 20:54:47, Train, Epoch : 3, Step : 1010, Loss : 0.73125, Acc : 0.697, Sensitive_Loss : 0.29776, Sensitive_Acc : 15.900, Run Time : 103.50 sec
INFO:root:2024-03-29 20:54:58, Train, Epoch : 3, Step : 1020, Loss : 0.87358, Acc : 0.659, Sensitive_Loss : 0.25203, Sensitive_Acc : 18.100, Run Time : 11.40 sec
INFO:root:2024-03-29 20:55:10, Train, Epoch : 3, Step : 1030, Loss : 1.08061, Acc : 0.669, Sensitive_Loss : 0.28359, Sensitive_Acc : 17.700, Run Time : 12.34 sec
INFO:root:2024-03-29 20:55:22, Train, Epoch : 3, Step : 1040, Loss : 0.95986, Acc : 0.700, Sensitive_Loss : 0.27638, Sensitive_Acc : 16.200, Run Time : 11.45 sec
INFO:root:2024-03-29 20:55:33, Train, Epoch : 3, Step : 1050, Loss : 0.98591, Acc : 0.681, Sensitive_Loss : 0.29326, Sensitive_Acc : 16.400, Run Time : 10.95 sec
INFO:root:2024-03-29 20:55:48, Train, Epoch : 3, Step : 1060, Loss : 0.88592, Acc : 0.666, Sensitive_Loss : 0.24204, Sensitive_Acc : 15.400, Run Time : 15.60 sec
INFO:root:2024-03-29 20:55:59, Train, Epoch : 3, Step : 1070, Loss : 0.80552, Acc : 0.669, Sensitive_Loss : 0.24443, Sensitive_Acc : 16.000, Run Time : 10.96 sec
INFO:root:2024-03-29 20:56:12, Train, Epoch : 3, Step : 1080, Loss : 1.08988, Acc : 0.637, Sensitive_Loss : 0.28966, Sensitive_Acc : 16.800, Run Time : 12.32 sec
INFO:root:2024-03-29 20:56:24, Train, Epoch : 3, Step : 1090, Loss : 0.98471, Acc : 0.709, Sensitive_Loss : 0.27146, Sensitive_Acc : 16.800, Run Time : 12.79 sec
INFO:root:2024-03-29 20:56:35, Train, Epoch : 3, Step : 1100, Loss : 0.98673, Acc : 0.697, Sensitive_Loss : 0.23136, Sensitive_Acc : 17.300, Run Time : 10.19 sec
INFO:root:2024-03-29 20:58:11, Dev, Step : 1100, Loss : 0.92929, Acc : 0.693, Auc : 0.787, Sensitive_Loss : 0.31992, Sensitive_Acc : 16.667, Sensitive_Auc : 0.991, Mean auc: 0.787, Run Time : 96.74 sec
INFO:root:2024-03-29 20:58:12, Best, Step : 1100, Loss : 0.92929, Acc : 0.693, Auc : 0.787, Sensitive_Loss : 0.31992, Sensitive_Acc : 16.667, Sensitive_Auc : 0.991, Best Auc : 0.787
INFO:root:2024-03-29 20:58:20, Train, Epoch : 3, Step : 1110, Loss : 1.00409, Acc : 0.669, Sensitive_Loss : 0.25643, Sensitive_Acc : 17.700, Run Time : 104.98 sec
INFO:root:2024-03-29 20:58:30, Train, Epoch : 3, Step : 1120, Loss : 0.79533, Acc : 0.747, Sensitive_Loss : 0.29781, Sensitive_Acc : 14.600, Run Time : 10.56 sec
INFO:root:2024-03-29 20:58:41, Train, Epoch : 3, Step : 1130, Loss : 0.84625, Acc : 0.637, Sensitive_Loss : 0.24949, Sensitive_Acc : 17.200, Run Time : 10.49 sec
INFO:root:2024-03-29 20:58:51, Train, Epoch : 3, Step : 1140, Loss : 0.65330, Acc : 0.681, Sensitive_Loss : 0.18912, Sensitive_Acc : 15.000, Run Time : 10.31 sec
INFO:root:2024-03-29 20:59:02, Train, Epoch : 3, Step : 1150, Loss : 0.93729, Acc : 0.719, Sensitive_Loss : 0.25249, Sensitive_Acc : 16.100, Run Time : 10.78 sec
INFO:root:2024-03-29 20:59:16, Train, Epoch : 3, Step : 1160, Loss : 0.88472, Acc : 0.694, Sensitive_Loss : 0.22960, Sensitive_Acc : 16.600, Run Time : 14.43 sec
INFO:root:2024-03-29 20:59:26, Train, Epoch : 3, Step : 1170, Loss : 0.77521, Acc : 0.706, Sensitive_Loss : 0.24705, Sensitive_Acc : 17.700, Run Time : 10.22 sec
INFO:root:2024-03-29 20:59:38, Train, Epoch : 3, Step : 1180, Loss : 0.91321, Acc : 0.678, Sensitive_Loss : 0.29515, Sensitive_Acc : 13.000, Run Time : 11.62 sec
INFO:root:2024-03-29 20:59:52, Train, Epoch : 3, Step : 1190, Loss : 1.00511, Acc : 0.709, Sensitive_Loss : 0.24420, Sensitive_Acc : 16.800, Run Time : 14.38 sec
INFO:root:2024-03-29 21:00:02, Train, Epoch : 3, Step : 1200, Loss : 0.84137, Acc : 0.719, Sensitive_Loss : 0.25860, Sensitive_Acc : 16.100, Run Time : 9.84 sec
INFO:root:2024-03-29 21:01:39, Dev, Step : 1200, Loss : 0.92948, Acc : 0.756, Auc : 0.786, Sensitive_Loss : 0.32576, Sensitive_Acc : 16.582, Sensitive_Auc : 0.992, Mean auc: 0.786, Run Time : 97.02 sec
INFO:root:2024-03-29 21:01:47, Train, Epoch : 3, Step : 1210, Loss : 0.96199, Acc : 0.669, Sensitive_Loss : 0.27636, Sensitive_Acc : 15.800, Run Time : 104.45 sec
INFO:root:2024-03-29 21:01:57, Train, Epoch : 3, Step : 1220, Loss : 0.77135, Acc : 0.713, Sensitive_Loss : 0.27280, Sensitive_Acc : 16.500, Run Time : 10.59 sec
INFO:root:2024-03-29 21:02:08, Train, Epoch : 3, Step : 1230, Loss : 0.89888, Acc : 0.691, Sensitive_Loss : 0.21563, Sensitive_Acc : 18.100, Run Time : 10.48 sec
INFO:root:2024-03-29 21:02:18, Train, Epoch : 3, Step : 1240, Loss : 0.92413, Acc : 0.694, Sensitive_Loss : 0.24701, Sensitive_Acc : 16.300, Run Time : 10.56 sec
INFO:root:2024-03-29 21:02:28, Train, Epoch : 3, Step : 1250, Loss : 0.82227, Acc : 0.694, Sensitive_Loss : 0.28436, Sensitive_Acc : 15.600, Run Time : 10.11 sec
INFO:root:2024-03-29 21:02:42, Train, Epoch : 3, Step : 1260, Loss : 0.80902, Acc : 0.659, Sensitive_Loss : 0.21242, Sensitive_Acc : 17.000, Run Time : 13.19 sec
INFO:root:2024-03-29 21:02:53, Train, Epoch : 3, Step : 1270, Loss : 1.04600, Acc : 0.688, Sensitive_Loss : 0.28114, Sensitive_Acc : 15.400, Run Time : 11.32 sec
INFO:root:2024-03-29 21:03:03, Train, Epoch : 3, Step : 1280, Loss : 1.03740, Acc : 0.694, Sensitive_Loss : 0.25698, Sensitive_Acc : 19.900, Run Time : 10.39 sec
INFO:root:2024-03-29 21:03:14, Train, Epoch : 3, Step : 1290, Loss : 0.85438, Acc : 0.706, Sensitive_Loss : 0.24346, Sensitive_Acc : 17.700, Run Time : 10.75 sec
INFO:root:2024-03-29 21:03:25, Train, Epoch : 3, Step : 1300, Loss : 0.84613, Acc : 0.684, Sensitive_Loss : 0.24856, Sensitive_Acc : 16.200, Run Time : 10.55 sec
INFO:root:2024-03-29 21:05:02, Dev, Step : 1300, Loss : 0.91062, Acc : 0.700, Auc : 0.796, Sensitive_Loss : 0.26387, Sensitive_Acc : 17.177, Sensitive_Auc : 0.991, Mean auc: 0.796, Run Time : 96.88 sec
INFO:root:2024-03-29 21:05:02, Best, Step : 1300, Loss : 0.91062, Acc : 0.700, Auc : 0.796, Sensitive_Loss : 0.26387, Sensitive_Acc : 17.177, Sensitive_Auc : 0.991, Best Auc : 0.796
INFO:root:2024-03-29 21:05:10, Train, Epoch : 3, Step : 1310, Loss : 0.79671, Acc : 0.725, Sensitive_Loss : 0.25149, Sensitive_Acc : 17.000, Run Time : 104.92 sec
INFO:root:2024-03-29 21:05:20, Train, Epoch : 3, Step : 1320, Loss : 0.78478, Acc : 0.694, Sensitive_Loss : 0.24933, Sensitive_Acc : 16.900, Run Time : 10.21 sec
INFO:root:2024-03-29 21:05:31, Train, Epoch : 3, Step : 1330, Loss : 0.91363, Acc : 0.703, Sensitive_Loss : 0.26147, Sensitive_Acc : 17.900, Run Time : 10.89 sec
INFO:root:2024-03-29 21:05:43, Train, Epoch : 3, Step : 1340, Loss : 0.85508, Acc : 0.719, Sensitive_Loss : 0.25181, Sensitive_Acc : 18.600, Run Time : 12.08 sec
INFO:root:2024-03-29 21:05:53, Train, Epoch : 3, Step : 1350, Loss : 0.72628, Acc : 0.703, Sensitive_Loss : 0.24798, Sensitive_Acc : 17.300, Run Time : 10.73 sec
INFO:root:2024-03-29 21:06:04, Train, Epoch : 3, Step : 1360, Loss : 0.81698, Acc : 0.678, Sensitive_Loss : 0.23416, Sensitive_Acc : 17.300, Run Time : 10.42 sec
INFO:root:2024-03-29 21:06:16, Train, Epoch : 3, Step : 1370, Loss : 0.79899, Acc : 0.681, Sensitive_Loss : 0.22096, Sensitive_Acc : 16.100, Run Time : 11.96 sec
INFO:root:2024-03-29 21:06:27, Train, Epoch : 3, Step : 1380, Loss : 1.00050, Acc : 0.688, Sensitive_Loss : 0.25602, Sensitive_Acc : 17.900, Run Time : 10.77 sec
INFO:root:2024-03-29 21:06:37, Train, Epoch : 3, Step : 1390, Loss : 1.00761, Acc : 0.688, Sensitive_Loss : 0.19723, Sensitive_Acc : 17.600, Run Time : 10.26 sec
INFO:root:2024-03-29 21:06:47, Train, Epoch : 3, Step : 1400, Loss : 0.92922, Acc : 0.719, Sensitive_Loss : 0.24052, Sensitive_Acc : 14.700, Run Time : 10.24 sec
INFO:root:2024-03-29 21:08:23, Dev, Step : 1400, Loss : 0.91704, Acc : 0.734, Auc : 0.790, Sensitive_Loss : 0.27059, Sensitive_Acc : 17.206, Sensitive_Auc : 0.990, Mean auc: 0.790, Run Time : 95.94 sec
INFO:root:2024-03-29 21:08:30, Train, Epoch : 3, Step : 1410, Loss : 0.91314, Acc : 0.700, Sensitive_Loss : 0.28916, Sensitive_Acc : 16.300, Run Time : 103.02 sec
INFO:root:2024-03-29 21:08:41, Train, Epoch : 3, Step : 1420, Loss : 0.91680, Acc : 0.706, Sensitive_Loss : 0.20479, Sensitive_Acc : 17.800, Run Time : 11.01 sec
INFO:root:2024-03-29 21:08:54, Train, Epoch : 3, Step : 1430, Loss : 0.82956, Acc : 0.669, Sensitive_Loss : 0.30412, Sensitive_Acc : 16.800, Run Time : 13.36 sec
INFO:root:2024-03-29 21:09:05, Train, Epoch : 3, Step : 1440, Loss : 0.75510, Acc : 0.684, Sensitive_Loss : 0.18950, Sensitive_Acc : 17.600, Run Time : 10.69 sec
INFO:root:2024-03-29 21:09:15, Train, Epoch : 3, Step : 1450, Loss : 0.87702, Acc : 0.691, Sensitive_Loss : 0.28056, Sensitive_Acc : 16.100, Run Time : 9.85 sec
INFO:root:2024-03-29 21:09:26, Train, Epoch : 3, Step : 1460, Loss : 0.91561, Acc : 0.694, Sensitive_Loss : 0.25080, Sensitive_Acc : 13.900, Run Time : 10.80 sec
INFO:root:2024-03-29 21:09:40, Train, Epoch : 3, Step : 1470, Loss : 0.79518, Acc : 0.713, Sensitive_Loss : 0.24438, Sensitive_Acc : 16.900, Run Time : 14.28 sec
INFO:root:2024-03-29 21:11:22
INFO:root:y_pred: [0.24186552 0.1183301  0.17869316 ... 0.5529702  0.7801174  0.04328495]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [4.7172117e-03 5.5298841e-01 9.9968815e-01 1.5152834e-01 9.9609369e-01
 1.3342280e-02 4.5245042e-01 1.4046170e-01 4.9674888e-03 1.4993583e-01
 3.8835261e-02 9.7334318e-02 9.8471886e-01 4.9072844e-03 9.8923069e-01
 9.9335128e-01 5.0584308e-04 4.2277476e-01 9.7570354e-01 9.3873382e-01
 5.9602147e-01 1.6569525e-02 9.9978870e-01 9.9148589e-01 1.4164148e-01
 9.0554261e-01 1.0925964e-02 9.9824995e-01 6.8816185e-02 9.4684559e-01
 9.9867886e-01 8.5604721e-01 4.4504542e-02 1.2697509e-04 4.4085452e-01
 8.0132686e-02 1.3466208e-01 2.8913498e-01 9.8755437e-01 9.9543577e-01
 8.3662830e-03 2.2739353e-02 9.7341508e-01 9.9952102e-01 8.2367048e-02
 3.8445354e-02 6.5568471e-03 4.5518807e-01 9.8985589e-01 6.3719964e-01
 9.9685788e-01 6.0461789e-01 9.9637896e-01 8.9869773e-01 9.7899938e-01
 9.4725585e-01 9.4723016e-01 5.9822038e-02 9.9948525e-01 3.8575828e-02
 1.5899616e-04 1.6385621e-01 3.8270029e-01 9.8618186e-01 9.8512256e-01
 9.9804676e-01 6.7044824e-01 9.9916780e-01 9.8483169e-01 9.9227059e-01
 9.8700631e-01 2.0228038e-02 8.5488141e-02 7.4523723e-01 9.3786395e-01
 1.2889979e-02 9.4161683e-01 3.9658453e-03 9.9788070e-01 3.9212155e-01
 2.2255224e-01 4.5425802e-02 9.6097030e-03 1.5273295e-01 9.4475979e-01
 9.2226490e-02 7.3090434e-01 4.8155412e-01 3.7725117e-02 4.0732563e-02
 1.1939067e-02 1.4257318e-01 7.5559326e-02 2.1489749e-02 9.8389232e-01
 8.6898911e-01 9.7031975e-03 1.7792553e-02 9.6312934e-01 7.5561142e-01
 9.8036879e-01 8.8241629e-02 9.9969482e-01 3.0673893e-02 1.1147414e-02
 9.7718561e-01 3.8100657e-01 7.7838436e-02 6.7776879e-03 2.8087035e-02
 2.5491272e-03 9.8592341e-01 1.3722129e-01 5.6149415e-03 1.2774223e-01
 1.1390744e-02 9.9040234e-01 6.7147791e-02 5.1481938e-01 7.2125718e-04
 7.3998177e-01 1.0733714e-02 9.9840444e-01 3.0648820e-03 1.0981744e-01
 9.9974471e-01 3.2611690e-02 9.1991162e-01 8.4824812e-01 9.9670237e-01
 1.3751177e-04 9.8082149e-01 1.9245246e-02 9.9998939e-01 9.9790156e-01
 4.0037465e-01 5.5365199e-01 4.3705049e-01 2.1196749e-02 9.2432660e-01
 9.7506022e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 21:11:22, Dev, Step : 1476, Loss : 0.92556, Acc : 0.781, Auc : 0.789, Sensitive_Loss : 0.28460, Sensitive_Acc : 17.078, Sensitive_Auc : 0.992, Mean auc: 0.789, Run Time : 95.47 sec
INFO:root:2024-03-29 21:11:28, Train, Epoch : 4, Step : 1480, Loss : 0.27121, Acc : 0.281, Sensitive_Loss : 0.08598, Sensitive_Acc : 6.900, Run Time : 5.24 sec
INFO:root:2024-03-29 21:11:38, Train, Epoch : 4, Step : 1490, Loss : 0.86340, Acc : 0.688, Sensitive_Loss : 0.23312, Sensitive_Acc : 18.000, Run Time : 10.23 sec
INFO:root:2024-03-29 21:11:48, Train, Epoch : 4, Step : 1500, Loss : 0.74602, Acc : 0.694, Sensitive_Loss : 0.27727, Sensitive_Acc : 18.000, Run Time : 9.90 sec
INFO:root:2024-03-29 21:13:25, Dev, Step : 1500, Loss : 0.92046, Acc : 0.775, Auc : 0.790, Sensitive_Loss : 0.30936, Sensitive_Acc : 16.851, Sensitive_Auc : 0.992, Mean auc: 0.790, Run Time : 96.55 sec
INFO:root:2024-03-29 21:13:34, Train, Epoch : 4, Step : 1510, Loss : 0.98826, Acc : 0.684, Sensitive_Loss : 0.23309, Sensitive_Acc : 18.600, Run Time : 106.07 sec
INFO:root:2024-03-29 21:13:48, Train, Epoch : 4, Step : 1520, Loss : 0.80190, Acc : 0.713, Sensitive_Loss : 0.25309, Sensitive_Acc : 17.600, Run Time : 13.49 sec
INFO:root:2024-03-29 21:13:59, Train, Epoch : 4, Step : 1530, Loss : 0.80858, Acc : 0.694, Sensitive_Loss : 0.22058, Sensitive_Acc : 17.700, Run Time : 11.14 sec
INFO:root:2024-03-29 21:14:11, Train, Epoch : 4, Step : 1540, Loss : 0.74385, Acc : 0.688, Sensitive_Loss : 0.21396, Sensitive_Acc : 15.100, Run Time : 12.54 sec
INFO:root:2024-03-29 21:14:22, Train, Epoch : 4, Step : 1550, Loss : 0.82481, Acc : 0.700, Sensitive_Loss : 0.25144, Sensitive_Acc : 18.500, Run Time : 11.16 sec
INFO:root:2024-03-29 21:14:33, Train, Epoch : 4, Step : 1560, Loss : 0.86643, Acc : 0.678, Sensitive_Loss : 0.20425, Sensitive_Acc : 17.700, Run Time : 10.59 sec
INFO:root:2024-03-29 21:14:45, Train, Epoch : 4, Step : 1570, Loss : 0.88869, Acc : 0.681, Sensitive_Loss : 0.25941, Sensitive_Acc : 18.100, Run Time : 11.74 sec
INFO:root:2024-03-29 21:14:58, Train, Epoch : 4, Step : 1580, Loss : 0.95792, Acc : 0.697, Sensitive_Loss : 0.24314, Sensitive_Acc : 17.000, Run Time : 13.41 sec
INFO:root:2024-03-29 21:15:09, Train, Epoch : 4, Step : 1590, Loss : 0.94448, Acc : 0.719, Sensitive_Loss : 0.17895, Sensitive_Acc : 15.500, Run Time : 10.77 sec
INFO:root:2024-03-29 21:15:20, Train, Epoch : 4, Step : 1600, Loss : 0.86897, Acc : 0.734, Sensitive_Loss : 0.25879, Sensitive_Acc : 15.100, Run Time : 10.83 sec
INFO:root:2024-03-29 21:16:57, Dev, Step : 1600, Loss : 0.94734, Acc : 0.795, Auc : 0.782, Sensitive_Loss : 0.30011, Sensitive_Acc : 16.936, Sensitive_Auc : 0.992, Mean auc: 0.782, Run Time : 97.59 sec
INFO:root:2024-03-29 21:17:05, Train, Epoch : 4, Step : 1610, Loss : 0.90131, Acc : 0.681, Sensitive_Loss : 0.28327, Sensitive_Acc : 16.000, Run Time : 104.97 sec
INFO:root:2024-03-29 21:17:16, Train, Epoch : 4, Step : 1620, Loss : 0.86980, Acc : 0.719, Sensitive_Loss : 0.23971, Sensitive_Acc : 16.100, Run Time : 11.12 sec
INFO:root:2024-03-29 21:17:27, Train, Epoch : 4, Step : 1630, Loss : 1.00424, Acc : 0.700, Sensitive_Loss : 0.29187, Sensitive_Acc : 15.800, Run Time : 10.76 sec
INFO:root:2024-03-29 21:17:37, Train, Epoch : 4, Step : 1640, Loss : 0.84158, Acc : 0.666, Sensitive_Loss : 0.26518, Sensitive_Acc : 13.600, Run Time : 10.23 sec
INFO:root:2024-03-29 21:17:48, Train, Epoch : 4, Step : 1650, Loss : 1.09051, Acc : 0.688, Sensitive_Loss : 0.27921, Sensitive_Acc : 16.600, Run Time : 11.22 sec
INFO:root:2024-03-29 21:18:02, Train, Epoch : 4, Step : 1660, Loss : 0.82227, Acc : 0.703, Sensitive_Loss : 0.26901, Sensitive_Acc : 14.500, Run Time : 13.52 sec
INFO:root:2024-03-29 21:18:12, Train, Epoch : 4, Step : 1670, Loss : 0.68226, Acc : 0.703, Sensitive_Loss : 0.27007, Sensitive_Acc : 17.000, Run Time : 10.18 sec
INFO:root:2024-03-29 21:18:23, Train, Epoch : 4, Step : 1680, Loss : 0.92186, Acc : 0.694, Sensitive_Loss : 0.21102, Sensitive_Acc : 17.300, Run Time : 11.53 sec
INFO:root:2024-03-29 21:18:34, Train, Epoch : 4, Step : 1690, Loss : 0.82518, Acc : 0.672, Sensitive_Loss : 0.26552, Sensitive_Acc : 15.700, Run Time : 10.32 sec
INFO:root:2024-03-29 21:18:44, Train, Epoch : 4, Step : 1700, Loss : 0.82780, Acc : 0.719, Sensitive_Loss : 0.15246, Sensitive_Acc : 17.200, Run Time : 10.22 sec
INFO:root:2024-03-29 21:20:20, Dev, Step : 1700, Loss : 0.91885, Acc : 0.781, Auc : 0.795, Sensitive_Loss : 0.23896, Sensitive_Acc : 17.177, Sensitive_Auc : 0.994, Mean auc: 0.795, Run Time : 96.44 sec
INFO:root:2024-03-29 21:20:28, Train, Epoch : 4, Step : 1710, Loss : 0.67684, Acc : 0.709, Sensitive_Loss : 0.17135, Sensitive_Acc : 18.500, Run Time : 104.00 sec
INFO:root:2024-03-29 21:20:38, Train, Epoch : 4, Step : 1720, Loss : 0.85899, Acc : 0.700, Sensitive_Loss : 0.22720, Sensitive_Acc : 15.400, Run Time : 10.62 sec
INFO:root:2024-03-29 21:20:52, Train, Epoch : 4, Step : 1730, Loss : 0.75477, Acc : 0.675, Sensitive_Loss : 0.29056, Sensitive_Acc : 16.700, Run Time : 13.78 sec
INFO:root:2024-03-29 21:21:04, Train, Epoch : 4, Step : 1740, Loss : 0.87570, Acc : 0.744, Sensitive_Loss : 0.22448, Sensitive_Acc : 15.300, Run Time : 11.39 sec
INFO:root:2024-03-29 21:21:14, Train, Epoch : 4, Step : 1750, Loss : 0.93265, Acc : 0.706, Sensitive_Loss : 0.24633, Sensitive_Acc : 17.100, Run Time : 10.84 sec
INFO:root:2024-03-29 21:21:25, Train, Epoch : 4, Step : 1760, Loss : 0.92844, Acc : 0.700, Sensitive_Loss : 0.27994, Sensitive_Acc : 13.500, Run Time : 10.43 sec
INFO:root:2024-03-29 21:21:36, Train, Epoch : 4, Step : 1770, Loss : 0.84913, Acc : 0.694, Sensitive_Loss : 0.22518, Sensitive_Acc : 16.500, Run Time : 11.59 sec
INFO:root:2024-03-29 21:21:47, Train, Epoch : 4, Step : 1780, Loss : 0.91381, Acc : 0.697, Sensitive_Loss : 0.24048, Sensitive_Acc : 17.000, Run Time : 10.30 sec
INFO:root:2024-03-29 21:21:57, Train, Epoch : 4, Step : 1790, Loss : 0.90957, Acc : 0.675, Sensitive_Loss : 0.24144, Sensitive_Acc : 17.900, Run Time : 10.45 sec
INFO:root:2024-03-29 21:22:09, Train, Epoch : 4, Step : 1800, Loss : 0.76873, Acc : 0.703, Sensitive_Loss : 0.20024, Sensitive_Acc : 15.900, Run Time : 11.49 sec
INFO:root:2024-03-29 21:23:46, Dev, Step : 1800, Loss : 0.91404, Acc : 0.773, Auc : 0.797, Sensitive_Loss : 0.25684, Sensitive_Acc : 17.064, Sensitive_Auc : 0.995, Mean auc: 0.797, Run Time : 97.49 sec
INFO:root:2024-03-29 21:23:47, Best, Step : 1800, Loss : 0.91404, Acc : 0.773, Auc : 0.797, Sensitive_Loss : 0.25684, Sensitive_Acc : 17.064, Sensitive_Auc : 0.995, Best Auc : 0.797
INFO:root:2024-03-29 21:23:54, Train, Epoch : 4, Step : 1810, Loss : 0.79234, Acc : 0.659, Sensitive_Loss : 0.23494, Sensitive_Acc : 16.300, Run Time : 105.74 sec
INFO:root:2024-03-29 21:24:07, Train, Epoch : 4, Step : 1820, Loss : 0.69223, Acc : 0.719, Sensitive_Loss : 0.18283, Sensitive_Acc : 15.100, Run Time : 12.27 sec
INFO:root:2024-03-29 21:24:19, Train, Epoch : 4, Step : 1830, Loss : 0.82018, Acc : 0.731, Sensitive_Loss : 0.27406, Sensitive_Acc : 17.300, Run Time : 12.71 sec
INFO:root:2024-03-29 21:24:30, Train, Epoch : 4, Step : 1840, Loss : 0.87460, Acc : 0.662, Sensitive_Loss : 0.21850, Sensitive_Acc : 16.000, Run Time : 10.18 sec
INFO:root:2024-03-29 21:24:40, Train, Epoch : 4, Step : 1850, Loss : 0.73495, Acc : 0.703, Sensitive_Loss : 0.22174, Sensitive_Acc : 16.900, Run Time : 10.73 sec
INFO:root:2024-03-29 21:24:51, Train, Epoch : 4, Step : 1860, Loss : 0.74320, Acc : 0.694, Sensitive_Loss : 0.25838, Sensitive_Acc : 17.200, Run Time : 10.39 sec
INFO:root:2024-03-29 21:25:01, Train, Epoch : 4, Step : 1870, Loss : 0.73993, Acc : 0.688, Sensitive_Loss : 0.17900, Sensitive_Acc : 16.300, Run Time : 10.63 sec
INFO:root:2024-03-29 21:25:12, Train, Epoch : 4, Step : 1880, Loss : 0.90699, Acc : 0.691, Sensitive_Loss : 0.23251, Sensitive_Acc : 16.100, Run Time : 10.28 sec
INFO:root:2024-03-29 21:25:21, Train, Epoch : 4, Step : 1890, Loss : 0.80599, Acc : 0.700, Sensitive_Loss : 0.25984, Sensitive_Acc : 17.700, Run Time : 9.84 sec
INFO:root:2024-03-29 21:25:32, Train, Epoch : 4, Step : 1900, Loss : 0.74767, Acc : 0.703, Sensitive_Loss : 0.25127, Sensitive_Acc : 15.400, Run Time : 10.30 sec
INFO:root:2024-03-29 21:27:09, Dev, Step : 1900, Loss : 0.91557, Acc : 0.772, Auc : 0.795, Sensitive_Loss : 0.29892, Sensitive_Acc : 16.908, Sensitive_Auc : 0.995, Mean auc: 0.795, Run Time : 97.22 sec
INFO:root:2024-03-29 21:27:16, Train, Epoch : 4, Step : 1910, Loss : 0.68884, Acc : 0.697, Sensitive_Loss : 0.23729, Sensitive_Acc : 16.300, Run Time : 104.53 sec
INFO:root:2024-03-29 21:27:27, Train, Epoch : 4, Step : 1920, Loss : 1.18888, Acc : 0.631, Sensitive_Loss : 0.24817, Sensitive_Acc : 15.500, Run Time : 10.96 sec
INFO:root:2024-03-29 21:27:40, Train, Epoch : 4, Step : 1930, Loss : 0.79173, Acc : 0.700, Sensitive_Loss : 0.24845, Sensitive_Acc : 16.900, Run Time : 12.49 sec
INFO:root:2024-03-29 21:27:50, Train, Epoch : 4, Step : 1940, Loss : 0.83593, Acc : 0.741, Sensitive_Loss : 0.26768, Sensitive_Acc : 16.700, Run Time : 10.25 sec
INFO:root:2024-03-29 21:28:00, Train, Epoch : 4, Step : 1950, Loss : 0.73285, Acc : 0.731, Sensitive_Loss : 0.22480, Sensitive_Acc : 18.000, Run Time : 10.44 sec
INFO:root:2024-03-29 21:28:13, Train, Epoch : 4, Step : 1960, Loss : 0.91441, Acc : 0.700, Sensitive_Loss : 0.29331, Sensitive_Acc : 15.800, Run Time : 12.04 sec
INFO:root:2024-03-29 21:29:57
INFO:root:y_pred: [0.23483516 0.07752332 0.21273273 ... 0.44289798 0.76422095 0.07391339]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [6.79141423e-03 5.08396208e-01 9.99686122e-01 1.14244774e-01
 9.98587251e-01 1.05199544e-02 3.22191060e-01 8.60554874e-02
 3.22500803e-03 1.47881731e-01 3.38034891e-02 1.23698778e-01
 9.93729472e-01 4.24963469e-03 9.95357931e-01 9.96626854e-01
 4.03350394e-04 3.62133443e-01 9.90941465e-01 9.50608134e-01
 4.18461919e-01 1.56684071e-02 9.99843597e-01 9.96473253e-01
 1.88417271e-01 8.75617564e-01 1.98035012e-03 9.98062193e-01
 3.09182908e-02 9.64099228e-01 9.99271214e-01 9.02377427e-01
 7.09411800e-02 7.23390040e-05 2.84081817e-01 5.68803214e-02
 1.49998024e-01 3.00785214e-01 9.91876960e-01 9.96764541e-01
 7.39948638e-03 8.70701205e-03 9.90613222e-01 9.99746859e-01
 8.49282518e-02 3.29857431e-02 4.40595765e-03 4.85321522e-01
 9.95006144e-01 6.40764892e-01 9.99153376e-01 4.96195287e-01
 9.95985150e-01 9.54744339e-01 9.88466263e-01 9.28998470e-01
 9.47185576e-01 6.99526072e-02 9.99687433e-01 3.34388092e-02
 1.75201130e-04 3.19568038e-01 6.08543336e-01 9.95055079e-01
 9.92707193e-01 9.98679817e-01 7.32345223e-01 9.99094725e-01
 9.94047523e-01 9.93175924e-01 9.91058111e-01 2.37323046e-02
 3.48197781e-02 7.22527742e-01 9.67582226e-01 1.22651281e-02
 9.51213360e-01 2.43095565e-03 9.98679936e-01 3.70683193e-01
 2.24461749e-01 5.11553399e-02 8.55528470e-03 1.28624290e-01
 9.77915466e-01 9.98745784e-02 8.09243083e-01 2.68606156e-01
 4.22266684e-02 3.81399728e-02 7.45169120e-03 1.04226977e-01
 9.83759835e-02 2.71224827e-02 9.88642752e-01 8.78678381e-01
 7.19788251e-03 4.05134493e-03 9.65376258e-01 8.88763726e-01
 9.89613235e-01 1.19127490e-01 9.99754608e-01 2.67599225e-02
 9.81330033e-03 9.88967001e-01 3.65269512e-01 5.63346334e-02
 3.26446746e-03 2.77682804e-02 1.98015966e-03 9.80862379e-01
 1.33574277e-01 3.85903870e-03 2.19897598e-01 5.64426603e-03
 9.95251060e-01 8.28891322e-02 4.69274223e-01 4.18238895e-04
 8.89812231e-01 4.16829856e-03 9.98744249e-01 2.36337120e-03
 1.09206513e-01 9.99884129e-01 3.63017917e-02 9.54462290e-01
 8.62710238e-01 9.97542620e-01 1.39554744e-04 9.87940907e-01
 1.44245410e-02 9.99987721e-01 9.98567462e-01 4.31050658e-01
 5.33761084e-01 3.16210210e-01 9.66829434e-03 9.80867803e-01
 9.83435333e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 21:29:57, Dev, Step : 1968, Loss : 0.89752, Acc : 0.730, Auc : 0.803, Sensitive_Loss : 0.27883, Sensitive_Acc : 17.035, Sensitive_Auc : 0.996, Mean auc: 0.803, Run Time : 95.93 sec
INFO:root:2024-03-29 21:29:58, Best, Step : 1968, Loss : 0.89752, Acc : 0.730,Auc : 0.803, Best Auc : 0.803, Sensitive_Loss : 0.27883, Sensitive_Acc : 17.035, Sensitive_Auc : 0.996
INFO:root:2024-03-29 21:30:01, Train, Epoch : 5, Step : 1970, Loss : 0.16602, Acc : 0.138, Sensitive_Loss : 0.03605, Sensitive_Acc : 3.500, Run Time : 2.80 sec
INFO:root:2024-03-29 21:30:12, Train, Epoch : 5, Step : 1980, Loss : 0.78249, Acc : 0.722, Sensitive_Loss : 0.25346, Sensitive_Acc : 16.900, Run Time : 10.37 sec
INFO:root:2024-03-29 21:30:22, Train, Epoch : 5, Step : 1990, Loss : 0.82742, Acc : 0.741, Sensitive_Loss : 0.21753, Sensitive_Acc : 13.800, Run Time : 9.94 sec
INFO:root:2024-03-29 21:30:33, Train, Epoch : 5, Step : 2000, Loss : 0.80845, Acc : 0.738, Sensitive_Loss : 0.22514, Sensitive_Acc : 17.700, Run Time : 11.72 sec
INFO:root:2024-03-29 21:32:10, Dev, Step : 2000, Loss : 0.89445, Acc : 0.769, Auc : 0.804, Sensitive_Loss : 0.27047, Sensitive_Acc : 17.121, Sensitive_Auc : 0.995, Mean auc: 0.804, Run Time : 96.84 sec
INFO:root:2024-03-29 21:32:12, Best, Step : 2000, Loss : 0.89445, Acc : 0.769, Auc : 0.804, Sensitive_Loss : 0.27047, Sensitive_Acc : 17.121, Sensitive_Auc : 0.995, Best Auc : 0.804
INFO:root:2024-03-29 21:32:22, Train, Epoch : 5, Step : 2010, Loss : 0.83650, Acc : 0.666, Sensitive_Loss : 0.19238, Sensitive_Acc : 17.600, Run Time : 108.14 sec
INFO:root:2024-03-29 21:32:33, Train, Epoch : 5, Step : 2020, Loss : 0.79173, Acc : 0.703, Sensitive_Loss : 0.18345, Sensitive_Acc : 16.100, Run Time : 11.58 sec
INFO:root:2024-03-29 21:32:44, Train, Epoch : 5, Step : 2030, Loss : 0.97391, Acc : 0.681, Sensitive_Loss : 0.22117, Sensitive_Acc : 14.300, Run Time : 11.14 sec
INFO:root:2024-03-29 21:32:56, Train, Epoch : 5, Step : 2040, Loss : 0.74748, Acc : 0.762, Sensitive_Loss : 0.22411, Sensitive_Acc : 16.900, Run Time : 11.78 sec
INFO:root:2024-03-29 21:33:07, Train, Epoch : 5, Step : 2050, Loss : 0.74751, Acc : 0.675, Sensitive_Loss : 0.25765, Sensitive_Acc : 15.800, Run Time : 11.35 sec
INFO:root:2024-03-29 21:33:18, Train, Epoch : 5, Step : 2060, Loss : 0.86359, Acc : 0.684, Sensitive_Loss : 0.18381, Sensitive_Acc : 16.600, Run Time : 10.49 sec
INFO:root:2024-03-29 21:33:34, Train, Epoch : 5, Step : 2070, Loss : 0.72393, Acc : 0.706, Sensitive_Loss : 0.21044, Sensitive_Acc : 15.100, Run Time : 16.14 sec
INFO:root:2024-03-29 21:33:45, Train, Epoch : 5, Step : 2080, Loss : 0.86784, Acc : 0.713, Sensitive_Loss : 0.23834, Sensitive_Acc : 15.900, Run Time : 10.90 sec
INFO:root:2024-03-29 21:33:57, Train, Epoch : 5, Step : 2090, Loss : 0.61312, Acc : 0.725, Sensitive_Loss : 0.29109, Sensitive_Acc : 16.400, Run Time : 12.48 sec
INFO:root:2024-03-29 21:34:10, Train, Epoch : 5, Step : 2100, Loss : 0.68675, Acc : 0.688, Sensitive_Loss : 0.16948, Sensitive_Acc : 16.000, Run Time : 12.83 sec
INFO:root:2024-03-29 21:35:47, Dev, Step : 2100, Loss : 0.90773, Acc : 0.767, Auc : 0.798, Sensitive_Loss : 0.23881, Sensitive_Acc : 17.121, Sensitive_Auc : 0.996, Mean auc: 0.798, Run Time : 97.12 sec
INFO:root:2024-03-29 21:35:55, Train, Epoch : 5, Step : 2110, Loss : 0.89672, Acc : 0.709, Sensitive_Loss : 0.19793, Sensitive_Acc : 16.000, Run Time : 104.39 sec
INFO:root:2024-03-29 21:36:06, Train, Epoch : 5, Step : 2120, Loss : 0.82102, Acc : 0.678, Sensitive_Loss : 0.17323, Sensitive_Acc : 17.500, Run Time : 10.90 sec
INFO:root:2024-03-29 21:36:17, Train, Epoch : 5, Step : 2130, Loss : 0.61086, Acc : 0.753, Sensitive_Loss : 0.21195, Sensitive_Acc : 16.400, Run Time : 11.69 sec
INFO:root:2024-03-29 21:36:28, Train, Epoch : 5, Step : 2140, Loss : 0.79050, Acc : 0.722, Sensitive_Loss : 0.16658, Sensitive_Acc : 15.400, Run Time : 10.47 sec
INFO:root:2024-03-29 21:36:38, Train, Epoch : 5, Step : 2150, Loss : 0.77246, Acc : 0.694, Sensitive_Loss : 0.19381, Sensitive_Acc : 17.600, Run Time : 10.62 sec
INFO:root:2024-03-29 21:36:51, Train, Epoch : 5, Step : 2160, Loss : 0.73961, Acc : 0.691, Sensitive_Loss : 0.21305, Sensitive_Acc : 16.100, Run Time : 12.23 sec
INFO:root:2024-03-29 21:37:02, Train, Epoch : 5, Step : 2170, Loss : 0.75511, Acc : 0.756, Sensitive_Loss : 0.24133, Sensitive_Acc : 18.200, Run Time : 11.80 sec
INFO:root:2024-03-29 21:37:13, Train, Epoch : 5, Step : 2180, Loss : 0.89547, Acc : 0.725, Sensitive_Loss : 0.22323, Sensitive_Acc : 17.700, Run Time : 10.52 sec
INFO:root:2024-03-29 21:37:24, Train, Epoch : 5, Step : 2190, Loss : 0.91855, Acc : 0.706, Sensitive_Loss : 0.26888, Sensitive_Acc : 17.900, Run Time : 10.96 sec
INFO:root:2024-03-29 21:37:38, Train, Epoch : 5, Step : 2200, Loss : 0.79083, Acc : 0.722, Sensitive_Loss : 0.24339, Sensitive_Acc : 14.700, Run Time : 13.99 sec
INFO:root:2024-03-29 21:39:15, Dev, Step : 2200, Loss : 0.91630, Acc : 0.781, Auc : 0.796, Sensitive_Loss : 0.27247, Sensitive_Acc : 17.035, Sensitive_Auc : 0.996, Mean auc: 0.796, Run Time : 97.14 sec
INFO:root:2024-03-29 21:39:23, Train, Epoch : 5, Step : 2210, Loss : 0.98435, Acc : 0.722, Sensitive_Loss : 0.19221, Sensitive_Acc : 15.800, Run Time : 105.32 sec
INFO:root:2024-03-29 21:39:33, Train, Epoch : 5, Step : 2220, Loss : 0.71819, Acc : 0.678, Sensitive_Loss : 0.29024, Sensitive_Acc : 17.500, Run Time : 10.29 sec
INFO:root:2024-03-29 21:39:45, Train, Epoch : 5, Step : 2230, Loss : 0.89522, Acc : 0.719, Sensitive_Loss : 0.21567, Sensitive_Acc : 17.600, Run Time : 11.19 sec
INFO:root:2024-03-29 21:39:58, Train, Epoch : 5, Step : 2240, Loss : 0.84928, Acc : 0.691, Sensitive_Loss : 0.29712, Sensitive_Acc : 18.500, Run Time : 13.78 sec
INFO:root:2024-03-29 21:40:09, Train, Epoch : 5, Step : 2250, Loss : 0.88056, Acc : 0.734, Sensitive_Loss : 0.21353, Sensitive_Acc : 16.300, Run Time : 10.33 sec
INFO:root:2024-03-29 21:40:21, Train, Epoch : 5, Step : 2260, Loss : 0.73392, Acc : 0.725, Sensitive_Loss : 0.23865, Sensitive_Acc : 14.700, Run Time : 11.93 sec
INFO:root:2024-03-29 21:40:31, Train, Epoch : 5, Step : 2270, Loss : 0.86540, Acc : 0.722, Sensitive_Loss : 0.21000, Sensitive_Acc : 17.600, Run Time : 10.61 sec
INFO:root:2024-03-29 21:40:42, Train, Epoch : 5, Step : 2280, Loss : 0.99630, Acc : 0.706, Sensitive_Loss : 0.26280, Sensitive_Acc : 15.300, Run Time : 10.23 sec
INFO:root:2024-03-29 21:40:52, Train, Epoch : 5, Step : 2290, Loss : 0.83444, Acc : 0.744, Sensitive_Loss : 0.22276, Sensitive_Acc : 16.500, Run Time : 10.37 sec
INFO:root:2024-03-29 21:41:07, Train, Epoch : 5, Step : 2300, Loss : 0.76471, Acc : 0.725, Sensitive_Loss : 0.23262, Sensitive_Acc : 16.800, Run Time : 14.84 sec
INFO:root:2024-03-29 21:42:44, Dev, Step : 2300, Loss : 0.90818, Acc : 0.755, Auc : 0.796, Sensitive_Loss : 0.26310, Sensitive_Acc : 17.234, Sensitive_Auc : 0.995, Mean auc: 0.796, Run Time : 97.60 sec
INFO:root:2024-03-29 21:42:51, Train, Epoch : 5, Step : 2310, Loss : 0.78592, Acc : 0.738, Sensitive_Loss : 0.18515, Sensitive_Acc : 17.300, Run Time : 104.63 sec
INFO:root:2024-03-29 21:43:02, Train, Epoch : 5, Step : 2320, Loss : 0.76358, Acc : 0.716, Sensitive_Loss : 0.20680, Sensitive_Acc : 16.400, Run Time : 10.49 sec
INFO:root:2024-03-29 21:43:13, Train, Epoch : 5, Step : 2330, Loss : 0.72676, Acc : 0.734, Sensitive_Loss : 0.20833, Sensitive_Acc : 16.500, Run Time : 10.74 sec
INFO:root:2024-03-29 21:43:26, Train, Epoch : 5, Step : 2340, Loss : 0.88309, Acc : 0.713, Sensitive_Loss : 0.21338, Sensitive_Acc : 14.300, Run Time : 13.25 sec
INFO:root:2024-03-29 21:43:36, Train, Epoch : 5, Step : 2350, Loss : 0.80342, Acc : 0.725, Sensitive_Loss : 0.21445, Sensitive_Acc : 16.100, Run Time : 10.09 sec
INFO:root:2024-03-29 21:43:46, Train, Epoch : 5, Step : 2360, Loss : 0.93520, Acc : 0.747, Sensitive_Loss : 0.23052, Sensitive_Acc : 17.600, Run Time : 10.41 sec
INFO:root:2024-03-29 21:43:56, Train, Epoch : 5, Step : 2370, Loss : 0.79336, Acc : 0.722, Sensitive_Loss : 0.21352, Sensitive_Acc : 16.600, Run Time : 9.75 sec
INFO:root:2024-03-29 21:44:06, Train, Epoch : 5, Step : 2380, Loss : 0.74334, Acc : 0.725, Sensitive_Loss : 0.21277, Sensitive_Acc : 17.200, Run Time : 9.93 sec
INFO:root:2024-03-29 21:44:16, Train, Epoch : 5, Step : 2390, Loss : 0.82402, Acc : 0.734, Sensitive_Loss : 0.21831, Sensitive_Acc : 14.100, Run Time : 10.17 sec
INFO:root:2024-03-29 21:44:26, Train, Epoch : 5, Step : 2400, Loss : 0.88992, Acc : 0.741, Sensitive_Loss : 0.24415, Sensitive_Acc : 17.100, Run Time : 10.01 sec
INFO:root:2024-03-29 21:46:03, Dev, Step : 2400, Loss : 0.90405, Acc : 0.759, Auc : 0.801, Sensitive_Loss : 0.27443, Sensitive_Acc : 17.035, Sensitive_Auc : 0.995, Mean auc: 0.801, Run Time : 97.19 sec
INFO:root:2024-03-29 21:46:11, Train, Epoch : 5, Step : 2410, Loss : 0.90954, Acc : 0.728, Sensitive_Loss : 0.14421, Sensitive_Acc : 15.000, Run Time : 104.53 sec
INFO:root:2024-03-29 21:46:22, Train, Epoch : 5, Step : 2420, Loss : 0.83949, Acc : 0.759, Sensitive_Loss : 0.19036, Sensitive_Acc : 16.500, Run Time : 10.85 sec
INFO:root:2024-03-29 21:46:32, Train, Epoch : 5, Step : 2430, Loss : 0.76990, Acc : 0.750, Sensitive_Loss : 0.18062, Sensitive_Acc : 14.700, Run Time : 10.48 sec
INFO:root:2024-03-29 21:46:45, Train, Epoch : 5, Step : 2440, Loss : 0.75230, Acc : 0.706, Sensitive_Loss : 0.18183, Sensitive_Acc : 15.800, Run Time : 13.28 sec
INFO:root:2024-03-29 21:46:56, Train, Epoch : 5, Step : 2450, Loss : 0.80713, Acc : 0.728, Sensitive_Loss : 0.20450, Sensitive_Acc : 16.100, Run Time : 10.73 sec
INFO:root:2024-03-29 21:47:06, Train, Epoch : 5, Step : 2460, Loss : 0.88980, Acc : 0.762, Sensitive_Loss : 0.25092, Sensitive_Acc : 16.200, Run Time : 10.00 sec
INFO:root:2024-03-29 21:48:42
INFO:root:y_pred: [0.18252203 0.06448132 0.1754242  ... 0.3162714  0.5537063  0.02611306]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [3.67763336e-03 4.78902429e-01 9.99421716e-01 1.07881464e-01
 9.98367727e-01 3.87767213e-03 1.51438639e-01 5.30651323e-02
 2.43279967e-03 7.43546560e-02 2.38734894e-02 8.22324455e-02
 9.90240574e-01 3.67990206e-03 9.96226072e-01 9.96685326e-01
 2.12458050e-04 3.22249025e-01 9.90145624e-01 9.58607256e-01
 2.76020110e-01 6.81985542e-03 9.99815166e-01 9.97434556e-01
 6.60312325e-02 9.03641701e-01 9.33129748e-04 9.98971939e-01
 1.53594268e-02 9.74056780e-01 9.99672413e-01 9.32718396e-01
 6.71064928e-02 3.29700633e-05 2.47303262e-01 2.32180692e-02
 1.20304450e-01 2.09640384e-01 9.93884265e-01 9.97563362e-01
 5.65986848e-03 4.93605761e-03 9.80603397e-01 9.99740064e-01
 4.84078377e-02 3.96748297e-02 2.17686477e-03 4.30090576e-01
 9.95665610e-01 6.50447488e-01 9.99489188e-01 5.33795953e-01
 9.97416139e-01 9.67011809e-01 9.90672648e-01 9.54256475e-01
 9.66980159e-01 8.89418945e-02 9.99681354e-01 3.01202629e-02
 1.03725644e-04 1.24474742e-01 5.47385454e-01 9.96583819e-01
 9.94339406e-01 9.99109328e-01 7.26889729e-01 9.99309778e-01
 9.93095219e-01 9.96437788e-01 9.86857891e-01 2.14272756e-02
 2.57061813e-02 8.15313876e-01 9.84259069e-01 9.93006490e-03
 9.62973773e-01 1.02685066e-03 9.99042332e-01 3.23149353e-01
 1.10530257e-01 5.34040667e-02 6.26035780e-03 2.24301770e-01
 9.83632386e-01 8.94898549e-02 8.35316122e-01 1.88458875e-01
 3.20274793e-02 4.18046191e-02 6.32718019e-03 9.40197483e-02
 8.09729621e-02 2.44551115e-02 9.93833542e-01 9.06254888e-01
 7.42651662e-03 2.57748296e-03 9.54024434e-01 9.17008817e-01
 9.69429910e-01 8.60972255e-02 9.99808133e-01 1.73301902e-02
 1.12288641e-02 9.89321351e-01 3.27151448e-01 3.61962505e-02
 3.11284279e-03 1.15259215e-02 1.19830074e-03 9.86776412e-01
 1.43743113e-01 2.41260999e-03 2.07693756e-01 2.96125002e-03
 9.94780600e-01 5.49792089e-02 4.86418098e-01 1.76935151e-04
 8.91029239e-01 1.53567677e-03 9.99118388e-01 9.68785957e-04
 1.99593216e-01 9.99889255e-01 4.20290492e-02 9.76751804e-01
 8.22604895e-01 9.97867942e-01 8.56348488e-05 9.85315323e-01
 1.13309305e-02 9.99985814e-01 9.98345494e-01 2.76266605e-01
 6.70863152e-01 3.40541422e-01 5.68035292e-03 9.88729298e-01
 9.72954094e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 21:48:42, Dev, Step : 2460, Loss : 0.92347, Acc : 0.801, Auc : 0.797, Sensitive_Loss : 0.26456, Sensitive_Acc : 17.121, Sensitive_Auc : 0.996, Mean auc: 0.797, Run Time : 95.50 sec
INFO:root:2024-03-29 21:48:54, Train, Epoch : 6, Step : 2470, Loss : 0.72725, Acc : 0.731, Sensitive_Loss : 0.18590, Sensitive_Acc : 17.600, Run Time : 11.34 sec
INFO:root:2024-03-29 21:49:05, Train, Epoch : 6, Step : 2480, Loss : 0.86494, Acc : 0.753, Sensitive_Loss : 0.19060, Sensitive_Acc : 17.200, Run Time : 11.35 sec
INFO:root:2024-03-29 21:49:16, Train, Epoch : 6, Step : 2490, Loss : 0.60181, Acc : 0.747, Sensitive_Loss : 0.25146, Sensitive_Acc : 19.000, Run Time : 10.83 sec
INFO:root:2024-03-29 21:49:27, Train, Epoch : 6, Step : 2500, Loss : 0.77627, Acc : 0.709, Sensitive_Loss : 0.23569, Sensitive_Acc : 18.000, Run Time : 10.90 sec
INFO:root:2024-03-29 21:51:03, Dev, Step : 2500, Loss : 0.90041, Acc : 0.775, Auc : 0.803, Sensitive_Loss : 0.25957, Sensitive_Acc : 17.064, Sensitive_Auc : 0.996, Mean auc: 0.803, Run Time : 96.09 sec
INFO:root:2024-03-29 21:51:10, Train, Epoch : 6, Step : 2510, Loss : 0.93869, Acc : 0.713, Sensitive_Loss : 0.19023, Sensitive_Acc : 18.200, Run Time : 103.53 sec
INFO:root:2024-03-29 21:51:22, Train, Epoch : 6, Step : 2520, Loss : 0.89968, Acc : 0.731, Sensitive_Loss : 0.19009, Sensitive_Acc : 16.400, Run Time : 11.51 sec
INFO:root:2024-03-29 21:51:34, Train, Epoch : 6, Step : 2530, Loss : 0.63854, Acc : 0.716, Sensitive_Loss : 0.17506, Sensitive_Acc : 17.400, Run Time : 11.55 sec
INFO:root:2024-03-29 21:51:45, Train, Epoch : 6, Step : 2540, Loss : 0.64474, Acc : 0.703, Sensitive_Loss : 0.24146, Sensitive_Acc : 18.300, Run Time : 11.15 sec
INFO:root:2024-03-29 21:51:56, Train, Epoch : 6, Step : 2550, Loss : 0.72546, Acc : 0.734, Sensitive_Loss : 0.25056, Sensitive_Acc : 17.300, Run Time : 11.30 sec
INFO:root:2024-03-29 21:52:10, Train, Epoch : 6, Step : 2560, Loss : 0.69532, Acc : 0.731, Sensitive_Loss : 0.25513, Sensitive_Acc : 16.400, Run Time : 14.16 sec
INFO:root:2024-03-29 21:52:21, Train, Epoch : 6, Step : 2570, Loss : 0.82375, Acc : 0.775, Sensitive_Loss : 0.24730, Sensitive_Acc : 18.200, Run Time : 10.66 sec
INFO:root:2024-03-29 21:52:32, Train, Epoch : 6, Step : 2580, Loss : 0.75924, Acc : 0.709, Sensitive_Loss : 0.25995, Sensitive_Acc : 14.300, Run Time : 11.11 sec
INFO:root:2024-03-29 21:52:42, Train, Epoch : 6, Step : 2590, Loss : 0.62674, Acc : 0.734, Sensitive_Loss : 0.20766, Sensitive_Acc : 16.700, Run Time : 10.52 sec
INFO:root:2024-03-29 21:52:53, Train, Epoch : 6, Step : 2600, Loss : 0.72980, Acc : 0.719, Sensitive_Loss : 0.21113, Sensitive_Acc : 18.300, Run Time : 10.36 sec
INFO:root:2024-03-29 21:54:30, Dev, Step : 2600, Loss : 0.89321, Acc : 0.750, Auc : 0.804, Sensitive_Loss : 0.25792, Sensitive_Acc : 17.092, Sensitive_Auc : 0.996, Mean auc: 0.804, Run Time : 96.92 sec
INFO:root:2024-03-29 21:54:37, Train, Epoch : 6, Step : 2610, Loss : 0.66315, Acc : 0.738, Sensitive_Loss : 0.21380, Sensitive_Acc : 15.600, Run Time : 104.34 sec
INFO:root:2024-03-29 21:54:48, Train, Epoch : 6, Step : 2620, Loss : 0.78779, Acc : 0.716, Sensitive_Loss : 0.23803, Sensitive_Acc : 17.200, Run Time : 10.97 sec
INFO:root:2024-03-29 21:55:02, Train, Epoch : 6, Step : 2630, Loss : 0.70221, Acc : 0.728, Sensitive_Loss : 0.23075, Sensitive_Acc : 17.300, Run Time : 13.49 sec
INFO:root:2024-03-29 21:55:13, Train, Epoch : 6, Step : 2640, Loss : 0.64886, Acc : 0.725, Sensitive_Loss : 0.23109, Sensitive_Acc : 17.000, Run Time : 11.47 sec
INFO:root:2024-03-29 21:55:23, Train, Epoch : 6, Step : 2650, Loss : 0.80197, Acc : 0.744, Sensitive_Loss : 0.23875, Sensitive_Acc : 18.300, Run Time : 10.35 sec
INFO:root:2024-03-29 21:55:34, Train, Epoch : 6, Step : 2660, Loss : 0.85360, Acc : 0.697, Sensitive_Loss : 0.24677, Sensitive_Acc : 15.600, Run Time : 10.93 sec
INFO:root:2024-03-29 21:55:45, Train, Epoch : 6, Step : 2670, Loss : 1.14825, Acc : 0.719, Sensitive_Loss : 0.16148, Sensitive_Acc : 16.100, Run Time : 10.78 sec
INFO:root:2024-03-29 21:56:04, Train, Epoch : 6, Step : 2680, Loss : 0.77359, Acc : 0.713, Sensitive_Loss : 0.19341, Sensitive_Acc : 19.000, Run Time : 18.66 sec
INFO:root:2024-03-29 21:56:16, Train, Epoch : 6, Step : 2690, Loss : 0.74688, Acc : 0.728, Sensitive_Loss : 0.24557, Sensitive_Acc : 16.600, Run Time : 12.51 sec
INFO:root:2024-03-29 21:56:27, Train, Epoch : 6, Step : 2700, Loss : 0.82787, Acc : 0.713, Sensitive_Loss : 0.22825, Sensitive_Acc : 19.200, Run Time : 10.33 sec
INFO:root:2024-03-29 22:00:15, Dev, Step : 2700, Loss : 0.91282, Acc : 0.747, Auc : 0.796, Sensitive_Loss : 0.24398, Sensitive_Acc : 17.092, Sensitive_Auc : 0.997, Mean auc: 0.796, Run Time : 228.62 sec
INFO:root:2024-03-29 22:00:23, Train, Epoch : 6, Step : 2710, Loss : 0.79853, Acc : 0.750, Sensitive_Loss : 0.20509, Sensitive_Acc : 16.100, Run Time : 235.99 sec
INFO:root:2024-03-29 22:00:33, Train, Epoch : 6, Step : 2720, Loss : 0.82604, Acc : 0.753, Sensitive_Loss : 0.19918, Sensitive_Acc : 17.600, Run Time : 10.87 sec
INFO:root:2024-03-29 22:00:45, Train, Epoch : 6, Step : 2730, Loss : 0.81060, Acc : 0.756, Sensitive_Loss : 0.18238, Sensitive_Acc : 18.000, Run Time : 11.84 sec
INFO:root:2024-03-29 22:00:59, Train, Epoch : 6, Step : 2740, Loss : 0.71693, Acc : 0.731, Sensitive_Loss : 0.20455, Sensitive_Acc : 15.900, Run Time : 13.52 sec
INFO:root:2024-03-29 22:01:09, Train, Epoch : 6, Step : 2750, Loss : 0.65641, Acc : 0.738, Sensitive_Loss : 0.19715, Sensitive_Acc : 17.200, Run Time : 10.16 sec
INFO:root:2024-03-29 22:01:21, Train, Epoch : 6, Step : 2760, Loss : 0.78563, Acc : 0.719, Sensitive_Loss : 0.22487, Sensitive_Acc : 16.000, Run Time : 12.36 sec
INFO:root:2024-03-29 22:01:34, Train, Epoch : 6, Step : 2770, Loss : 0.86263, Acc : 0.713, Sensitive_Loss : 0.19473, Sensitive_Acc : 15.400, Run Time : 12.19 sec
INFO:root:2024-03-29 22:01:44, Train, Epoch : 6, Step : 2780, Loss : 0.62165, Acc : 0.697, Sensitive_Loss : 0.21811, Sensitive_Acc : 19.900, Run Time : 10.17 sec
INFO:root:2024-03-29 22:01:54, Train, Epoch : 6, Step : 2790, Loss : 0.59643, Acc : 0.750, Sensitive_Loss : 0.19000, Sensitive_Acc : 16.800, Run Time : 10.47 sec
INFO:root:2024-03-29 22:02:05, Train, Epoch : 6, Step : 2800, Loss : 0.86926, Acc : 0.734, Sensitive_Loss : 0.21657, Sensitive_Acc : 17.900, Run Time : 11.02 sec
INFO:root:2024-03-29 22:04:28, Dev, Step : 2800, Loss : 0.90485, Acc : 0.788, Auc : 0.804, Sensitive_Loss : 0.22886, Sensitive_Acc : 17.177, Sensitive_Auc : 0.996, Mean auc: 0.804, Run Time : 142.60 sec
INFO:root:2024-03-29 22:04:35, Train, Epoch : 6, Step : 2810, Loss : 0.68410, Acc : 0.713, Sensitive_Loss : 0.20165, Sensitive_Acc : 16.800, Run Time : 149.30 sec
INFO:root:2024-03-29 22:04:44, Train, Epoch : 6, Step : 2820, Loss : 0.74129, Acc : 0.772, Sensitive_Loss : 0.18943, Sensitive_Acc : 16.000, Run Time : 9.99 sec
INFO:root:2024-03-29 22:04:57, Train, Epoch : 6, Step : 2830, Loss : 0.77850, Acc : 0.713, Sensitive_Loss : 0.15876, Sensitive_Acc : 13.900, Run Time : 12.15 sec
INFO:root:2024-03-29 22:05:11, Train, Epoch : 6, Step : 2840, Loss : 0.84438, Acc : 0.725, Sensitive_Loss : 0.25033, Sensitive_Acc : 14.500, Run Time : 14.39 sec
INFO:root:2024-03-29 22:05:21, Train, Epoch : 6, Step : 2850, Loss : 0.61648, Acc : 0.747, Sensitive_Loss : 0.19443, Sensitive_Acc : 16.700, Run Time : 9.59 sec
INFO:root:2024-03-29 22:05:35, Train, Epoch : 6, Step : 2860, Loss : 0.62197, Acc : 0.738, Sensitive_Loss : 0.19413, Sensitive_Acc : 16.800, Run Time : 14.32 sec
INFO:root:2024-03-29 22:05:44, Train, Epoch : 6, Step : 2870, Loss : 0.74104, Acc : 0.750, Sensitive_Loss : 0.21917, Sensitive_Acc : 16.000, Run Time : 9.42 sec
INFO:root:2024-03-29 22:05:54, Train, Epoch : 6, Step : 2880, Loss : 0.70114, Acc : 0.716, Sensitive_Loss : 0.20158, Sensitive_Acc : 18.300, Run Time : 9.62 sec
INFO:root:2024-03-29 22:06:04, Train, Epoch : 6, Step : 2890, Loss : 0.84912, Acc : 0.706, Sensitive_Loss : 0.19423, Sensitive_Acc : 14.400, Run Time : 10.19 sec
INFO:root:2024-03-29 22:06:15, Train, Epoch : 6, Step : 2900, Loss : 0.80863, Acc : 0.681, Sensitive_Loss : 0.19588, Sensitive_Acc : 14.900, Run Time : 10.36 sec
INFO:root:2024-03-29 22:07:57, Dev, Step : 2900, Loss : 0.92709, Acc : 0.711, Auc : 0.789, Sensitive_Loss : 0.22132, Sensitive_Acc : 17.305, Sensitive_Auc : 0.996, Mean auc: 0.789, Run Time : 102.07 sec
INFO:root:2024-03-29 22:08:04, Train, Epoch : 6, Step : 2910, Loss : 0.81380, Acc : 0.750, Sensitive_Loss : 0.22224, Sensitive_Acc : 18.500, Run Time : 109.08 sec
INFO:root:2024-03-29 22:08:14, Train, Epoch : 6, Step : 2920, Loss : 0.79188, Acc : 0.734, Sensitive_Loss : 0.17406, Sensitive_Acc : 14.800, Run Time : 10.39 sec
INFO:root:2024-03-29 22:08:24, Train, Epoch : 6, Step : 2930, Loss : 0.81193, Acc : 0.741, Sensitive_Loss : 0.18061, Sensitive_Acc : 17.000, Run Time : 10.40 sec
INFO:root:2024-03-29 22:08:35, Train, Epoch : 6, Step : 2940, Loss : 0.66791, Acc : 0.759, Sensitive_Loss : 0.19704, Sensitive_Acc : 17.100, Run Time : 10.11 sec
INFO:root:2024-03-29 22:08:44, Train, Epoch : 6, Step : 2950, Loss : 0.71809, Acc : 0.750, Sensitive_Loss : 0.22481, Sensitive_Acc : 15.500, Run Time : 9.88 sec
INFO:root:2024-03-29 22:10:24
INFO:root:y_pred: [0.23828273 0.07855276 0.21836363 ... 0.33426553 0.67237765 0.03156956]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [4.14007949e-03 3.66571784e-01 9.98999417e-01 1.03069223e-01
 9.97882187e-01 4.87340987e-03 2.48374194e-01 6.39565811e-02
 1.28406659e-03 3.65482531e-02 1.44180153e-02 7.81076699e-02
 9.85551476e-01 1.51989679e-03 9.93155658e-01 9.95625198e-01
 2.58952001e-04 2.40845487e-01 9.77337360e-01 9.30197656e-01
 2.11991265e-01 4.54891566e-03 9.99700904e-01 9.95876670e-01
 2.28684787e-02 8.88839185e-01 7.21440359e-04 9.97805655e-01
 7.44179729e-03 9.73557413e-01 9.99808371e-01 9.50767219e-01
 6.85589239e-02 1.73406042e-05 2.87088484e-01 2.68143229e-02
 8.85046050e-02 1.06204756e-01 9.93559182e-01 9.98237967e-01
 3.17063415e-03 5.47521701e-03 9.80626881e-01 9.99466956e-01
 2.37604287e-02 2.53409483e-02 2.69138650e-03 5.16252756e-01
 9.94849145e-01 3.84285241e-01 9.99569237e-01 4.00990069e-01
 9.97186244e-01 9.65497792e-01 9.89949346e-01 9.30305064e-01
 9.26857591e-01 4.08174470e-02 9.99369323e-01 3.34929042e-02
 7.98592882e-05 1.54537231e-01 4.80659604e-01 9.95640397e-01
 9.94342685e-01 9.98695076e-01 5.86845279e-01 9.99645710e-01
 9.89460349e-01 9.96133566e-01 9.84567761e-01 1.07134646e-02
 2.45903954e-02 7.53682613e-01 9.84048367e-01 1.97770819e-02
 9.57183063e-01 9.23975313e-04 9.98869836e-01 3.52076739e-01
 7.82230645e-02 3.51719223e-02 4.11185808e-03 1.94887385e-01
 9.78412807e-01 8.99772570e-02 7.44120598e-01 1.11394249e-01
 2.33424250e-02 4.54107523e-02 3.27723939e-03 8.00975785e-02
 8.24992284e-02 1.40795112e-02 9.92962778e-01 8.95220280e-01
 3.98417888e-03 3.83485993e-03 9.42219257e-01 9.14932489e-01
 9.53608394e-01 4.86565381e-02 9.99834776e-01 1.59814749e-02
 4.30337433e-03 9.83683944e-01 2.19081134e-01 4.38453555e-02
 2.85182893e-03 1.23680718e-02 1.54474552e-03 9.87502575e-01
 2.02628046e-01 1.37563876e-03 1.40723243e-01 3.11856577e-03
 9.94491458e-01 3.14153694e-02 2.69493282e-01 2.54591956e-04
 8.53842735e-01 2.03399011e-03 9.98840392e-01 7.57563219e-04
 1.60563409e-01 9.99775231e-01 2.85714716e-02 9.80424285e-01
 6.66669190e-01 9.97351646e-01 5.25382166e-05 9.86065149e-01
 9.18023009e-03 9.99968886e-01 9.98642385e-01 1.54882953e-01
 5.97331226e-01 4.36511636e-01 5.72674302e-03 9.84911203e-01
 9.64602113e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 22:10:24, Dev, Step : 2952, Loss : 0.90940, Acc : 0.745, Auc : 0.796, Sensitive_Loss : 0.24344, Sensitive_Acc : 17.206, Sensitive_Auc : 0.995, Mean auc: 0.796, Run Time : 97.57 sec
INFO:root:2024-03-29 22:10:34, Train, Epoch : 7, Step : 2960, Loss : 0.56873, Acc : 0.575, Sensitive_Loss : 0.13228, Sensitive_Acc : 13.200, Run Time : 9.13 sec
INFO:root:2024-03-29 22:10:45, Train, Epoch : 7, Step : 2970, Loss : 0.62685, Acc : 0.775, Sensitive_Loss : 0.20507, Sensitive_Acc : 16.900, Run Time : 10.95 sec
INFO:root:2024-03-29 22:10:56, Train, Epoch : 7, Step : 2980, Loss : 0.78477, Acc : 0.722, Sensitive_Loss : 0.24047, Sensitive_Acc : 19.100, Run Time : 11.15 sec
INFO:root:2024-03-29 22:11:07, Train, Epoch : 7, Step : 2990, Loss : 0.62598, Acc : 0.741, Sensitive_Loss : 0.25513, Sensitive_Acc : 15.500, Run Time : 11.02 sec
INFO:root:2024-03-29 22:11:22, Train, Epoch : 7, Step : 3000, Loss : 0.73792, Acc : 0.759, Sensitive_Loss : 0.14000, Sensitive_Acc : 17.700, Run Time : 14.79 sec
INFO:root:2024-03-29 22:13:01, Dev, Step : 3000, Loss : 0.90551, Acc : 0.784, Auc : 0.798, Sensitive_Loss : 0.23434, Sensitive_Acc : 17.234, Sensitive_Auc : 0.996, Mean auc: 0.798, Run Time : 98.67 sec
INFO:root:2024-03-29 22:13:09, Train, Epoch : 7, Step : 3010, Loss : 0.85524, Acc : 0.725, Sensitive_Loss : 0.20331, Sensitive_Acc : 15.900, Run Time : 106.61 sec
INFO:root:2024-03-29 22:13:21, Train, Epoch : 7, Step : 3020, Loss : 0.76750, Acc : 0.725, Sensitive_Loss : 0.20099, Sensitive_Acc : 15.900, Run Time : 12.13 sec
INFO:root:2024-03-29 22:13:32, Train, Epoch : 7, Step : 3030, Loss : 0.82372, Acc : 0.756, Sensitive_Loss : 0.19136, Sensitive_Acc : 17.200, Run Time : 11.38 sec
INFO:root:2024-03-29 22:13:48, Train, Epoch : 7, Step : 3040, Loss : 0.62221, Acc : 0.725, Sensitive_Loss : 0.22445, Sensitive_Acc : 18.500, Run Time : 15.83 sec
INFO:root:2024-03-29 22:13:59, Train, Epoch : 7, Step : 3050, Loss : 0.95000, Acc : 0.703, Sensitive_Loss : 0.18338, Sensitive_Acc : 16.300, Run Time : 10.70 sec
INFO:root:2024-03-29 22:14:10, Train, Epoch : 7, Step : 3060, Loss : 0.63404, Acc : 0.766, Sensitive_Loss : 0.24474, Sensitive_Acc : 18.600, Run Time : 11.39 sec
INFO:root:2024-03-29 22:14:24, Train, Epoch : 7, Step : 3070, Loss : 0.73255, Acc : 0.769, Sensitive_Loss : 0.18318, Sensitive_Acc : 16.900, Run Time : 13.88 sec
INFO:root:2024-03-29 22:14:35, Train, Epoch : 7, Step : 3080, Loss : 0.63534, Acc : 0.700, Sensitive_Loss : 0.17674, Sensitive_Acc : 16.000, Run Time : 11.20 sec
INFO:root:2024-03-29 22:14:46, Train, Epoch : 7, Step : 3090, Loss : 0.80101, Acc : 0.738, Sensitive_Loss : 0.24063, Sensitive_Acc : 17.600, Run Time : 10.42 sec
INFO:root:2024-03-29 22:14:57, Train, Epoch : 7, Step : 3100, Loss : 0.72338, Acc : 0.781, Sensitive_Loss : 0.32443, Sensitive_Acc : 16.800, Run Time : 11.71 sec
INFO:root:2024-03-29 22:16:46, Dev, Step : 3100, Loss : 0.90102, Acc : 0.779, Auc : 0.801, Sensitive_Loss : 0.24749, Sensitive_Acc : 17.092, Sensitive_Auc : 0.996, Mean auc: 0.801, Run Time : 108.36 sec
INFO:root:2024-03-29 22:16:54, Train, Epoch : 7, Step : 3110, Loss : 0.72890, Acc : 0.747, Sensitive_Loss : 0.17686, Sensitive_Acc : 16.500, Run Time : 116.29 sec
INFO:root:2024-03-29 22:17:04, Train, Epoch : 7, Step : 3120, Loss : 0.79520, Acc : 0.744, Sensitive_Loss : 0.20768, Sensitive_Acc : 16.800, Run Time : 10.48 sec
INFO:root:2024-03-29 22:17:14, Train, Epoch : 7, Step : 3130, Loss : 0.89770, Acc : 0.753, Sensitive_Loss : 0.20281, Sensitive_Acc : 16.300, Run Time : 10.18 sec
INFO:root:2024-03-29 22:17:24, Train, Epoch : 7, Step : 3140, Loss : 0.64586, Acc : 0.722, Sensitive_Loss : 0.19401, Sensitive_Acc : 17.000, Run Time : 9.71 sec
INFO:root:2024-03-29 22:17:34, Train, Epoch : 7, Step : 3150, Loss : 0.80498, Acc : 0.766, Sensitive_Loss : 0.22399, Sensitive_Acc : 15.500, Run Time : 10.29 sec
INFO:root:2024-03-29 22:17:45, Train, Epoch : 7, Step : 3160, Loss : 0.75370, Acc : 0.741, Sensitive_Loss : 0.18473, Sensitive_Acc : 15.200, Run Time : 10.40 sec
INFO:root:2024-03-29 22:17:56, Train, Epoch : 7, Step : 3170, Loss : 0.80752, Acc : 0.750, Sensitive_Loss : 0.19764, Sensitive_Acc : 16.200, Run Time : 11.65 sec
INFO:root:2024-03-29 22:18:10, Train, Epoch : 7, Step : 3180, Loss : 0.74177, Acc : 0.750, Sensitive_Loss : 0.16945, Sensitive_Acc : 18.300, Run Time : 13.82 sec
INFO:root:2024-03-29 22:18:20, Train, Epoch : 7, Step : 3190, Loss : 0.83620, Acc : 0.750, Sensitive_Loss : 0.21906, Sensitive_Acc : 16.800, Run Time : 9.65 sec
INFO:root:2024-03-29 22:18:30, Train, Epoch : 7, Step : 3200, Loss : 0.62834, Acc : 0.725, Sensitive_Loss : 0.21058, Sensitive_Acc : 16.600, Run Time : 10.14 sec
INFO:root:2024-03-29 22:20:06, Dev, Step : 3200, Loss : 0.93031, Acc : 0.762, Auc : 0.788, Sensitive_Loss : 0.26201, Sensitive_Acc : 16.865, Sensitive_Auc : 0.996, Mean auc: 0.788, Run Time : 96.29 sec
INFO:root:2024-03-29 22:20:14, Train, Epoch : 7, Step : 3210, Loss : 0.73458, Acc : 0.703, Sensitive_Loss : 0.17290, Sensitive_Acc : 16.400, Run Time : 103.96 sec
INFO:root:2024-03-29 22:20:24, Train, Epoch : 7, Step : 3220, Loss : 0.64852, Acc : 0.766, Sensitive_Loss : 0.18168, Sensitive_Acc : 18.000, Run Time : 10.36 sec
INFO:root:2024-03-29 22:20:35, Train, Epoch : 7, Step : 3230, Loss : 0.78919, Acc : 0.744, Sensitive_Loss : 0.18940, Sensitive_Acc : 17.400, Run Time : 10.54 sec
INFO:root:2024-03-29 22:20:45, Train, Epoch : 7, Step : 3240, Loss : 0.66138, Acc : 0.784, Sensitive_Loss : 0.14907, Sensitive_Acc : 17.100, Run Time : 10.43 sec
INFO:root:2024-03-29 22:20:58, Train, Epoch : 7, Step : 3250, Loss : 0.72928, Acc : 0.725, Sensitive_Loss : 0.26239, Sensitive_Acc : 17.200, Run Time : 12.80 sec
INFO:root:2024-03-29 22:21:09, Train, Epoch : 7, Step : 3260, Loss : 0.72830, Acc : 0.747, Sensitive_Loss : 0.18861, Sensitive_Acc : 15.500, Run Time : 10.48 sec
INFO:root:2024-03-29 22:21:19, Train, Epoch : 7, Step : 3270, Loss : 0.79750, Acc : 0.713, Sensitive_Loss : 0.20075, Sensitive_Acc : 15.600, Run Time : 10.19 sec
INFO:root:2024-03-29 22:21:29, Train, Epoch : 7, Step : 3280, Loss : 0.64390, Acc : 0.772, Sensitive_Loss : 0.16986, Sensitive_Acc : 15.900, Run Time : 10.51 sec
INFO:root:2024-03-29 22:21:43, Train, Epoch : 7, Step : 3290, Loss : 0.76138, Acc : 0.784, Sensitive_Loss : 0.15485, Sensitive_Acc : 16.800, Run Time : 14.06 sec
INFO:root:2024-03-29 22:21:54, Train, Epoch : 7, Step : 3300, Loss : 0.83338, Acc : 0.747, Sensitive_Loss : 0.14583, Sensitive_Acc : 18.300, Run Time : 10.34 sec
INFO:root:2024-03-29 22:23:32, Dev, Step : 3300, Loss : 0.93340, Acc : 0.709, Auc : 0.791, Sensitive_Loss : 0.23919, Sensitive_Acc : 16.993, Sensitive_Auc : 0.997, Mean auc: 0.791, Run Time : 97.85 sec
INFO:root:2024-03-29 22:23:39, Train, Epoch : 7, Step : 3310, Loss : 0.79053, Acc : 0.731, Sensitive_Loss : 0.18043, Sensitive_Acc : 15.200, Run Time : 105.14 sec
INFO:root:2024-03-29 22:23:51, Train, Epoch : 7, Step : 3320, Loss : 0.78673, Acc : 0.784, Sensitive_Loss : 0.19607, Sensitive_Acc : 17.900, Run Time : 11.64 sec
INFO:root:2024-03-29 22:24:03, Train, Epoch : 7, Step : 3330, Loss : 0.65374, Acc : 0.738, Sensitive_Loss : 0.22267, Sensitive_Acc : 16.900, Run Time : 12.07 sec
INFO:root:2024-03-29 22:24:13, Train, Epoch : 7, Step : 3340, Loss : 0.70387, Acc : 0.719, Sensitive_Loss : 0.21807, Sensitive_Acc : 16.700, Run Time : 10.82 sec
INFO:root:2024-03-29 22:24:24, Train, Epoch : 7, Step : 3350, Loss : 0.83666, Acc : 0.738, Sensitive_Loss : 0.16340, Sensitive_Acc : 18.900, Run Time : 10.59 sec
INFO:root:2024-03-29 22:24:35, Train, Epoch : 7, Step : 3360, Loss : 0.57922, Acc : 0.709, Sensitive_Loss : 0.22917, Sensitive_Acc : 17.900, Run Time : 11.33 sec
INFO:root:2024-03-29 22:24:46, Train, Epoch : 7, Step : 3370, Loss : 0.80618, Acc : 0.756, Sensitive_Loss : 0.19663, Sensitive_Acc : 14.800, Run Time : 10.69 sec
INFO:root:2024-03-29 22:24:57, Train, Epoch : 7, Step : 3380, Loss : 0.67086, Acc : 0.750, Sensitive_Loss : 0.15154, Sensitive_Acc : 16.900, Run Time : 10.70 sec
INFO:root:2024-03-29 22:25:09, Train, Epoch : 7, Step : 3390, Loss : 0.84675, Acc : 0.731, Sensitive_Loss : 0.20901, Sensitive_Acc : 15.500, Run Time : 12.17 sec
INFO:root:2024-03-29 22:25:22, Train, Epoch : 7, Step : 3400, Loss : 0.70350, Acc : 0.769, Sensitive_Loss : 0.22299, Sensitive_Acc : 17.000, Run Time : 13.23 sec
INFO:root:2024-03-29 22:27:00, Dev, Step : 3400, Loss : 0.91764, Acc : 0.747, Auc : 0.797, Sensitive_Loss : 0.27975, Sensitive_Acc : 16.894, Sensitive_Auc : 0.997, Mean auc: 0.797, Run Time : 98.02 sec
INFO:root:2024-03-29 22:27:10, Train, Epoch : 7, Step : 3410, Loss : 0.76613, Acc : 0.722, Sensitive_Loss : 0.12320, Sensitive_Acc : 18.000, Run Time : 108.07 sec
INFO:root:2024-03-29 22:27:23, Train, Epoch : 7, Step : 3420, Loss : 0.57381, Acc : 0.716, Sensitive_Loss : 0.20170, Sensitive_Acc : 15.300, Run Time : 13.24 sec
INFO:root:2024-03-29 22:27:35, Train, Epoch : 7, Step : 3430, Loss : 0.89323, Acc : 0.744, Sensitive_Loss : 0.18330, Sensitive_Acc : 16.400, Run Time : 11.36 sec
INFO:root:2024-03-29 22:27:45, Train, Epoch : 7, Step : 3440, Loss : 0.70835, Acc : 0.750, Sensitive_Loss : 0.21335, Sensitive_Acc : 18.500, Run Time : 10.63 sec
INFO:root:2024-03-29 22:29:27
INFO:root:y_pred: [0.18553415 0.0645117  0.2799121  ... 0.2949037  0.7398424  0.03769205]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [3.4853430e-03 5.4050928e-01 9.9915814e-01 1.0583766e-01 9.9951768e-01
 1.1656026e-02 3.7845916e-01 9.1500953e-02 2.3727913e-03 4.5765515e-02
 1.2973971e-02 9.2271000e-02 9.9327660e-01 2.4143001e-03 9.9677485e-01
 9.9650913e-01 2.6218532e-04 3.2729548e-01 9.9280798e-01 9.4539714e-01
 3.8431400e-01 4.3358905e-03 9.9974507e-01 9.9782145e-01 4.8286520e-02
 9.0607947e-01 6.9915625e-04 9.9857974e-01 5.6552328e-03 9.8204833e-01
 9.9986613e-01 9.7197717e-01 5.9784930e-02 2.6635464e-05 3.0747777e-01
 1.9268377e-02 2.3171921e-01 1.1322935e-01 9.9585885e-01 9.9910957e-01
 3.5588106e-03 3.5761287e-03 9.8756194e-01 9.9967563e-01 2.5869749e-02
 4.2732581e-02 6.3102702e-03 4.6754029e-01 9.9529487e-01 7.1960795e-01
 9.9988544e-01 4.3953094e-01 9.9793410e-01 9.8749626e-01 9.9516857e-01
 9.5045066e-01 9.6620107e-01 9.5456563e-02 9.9983597e-01 3.8080350e-02
 1.3458713e-04 3.0987144e-01 7.6636887e-01 9.9771154e-01 9.9807042e-01
 9.9933869e-01 7.4563342e-01 9.9978310e-01 9.9505901e-01 9.9884099e-01
 9.9399513e-01 1.6710948e-02 2.8203679e-02 8.8084984e-01 9.9087346e-01
 1.5352354e-02 9.7411978e-01 8.7254168e-04 9.9958557e-01 3.9586598e-01
 1.8429510e-01 4.9030907e-02 1.0381321e-02 2.8177243e-01 9.8677641e-01
 2.1250130e-01 8.6891556e-01 1.6590558e-01 5.4546967e-02 7.6199397e-02
 5.9012333e-03 8.2975715e-02 1.2410091e-01 3.0739129e-02 9.9743265e-01
 9.4734025e-01 4.8714378e-03 2.6881569e-03 9.7056133e-01 9.5449579e-01
 9.6672082e-01 4.4099171e-02 9.9996102e-01 3.2134376e-02 6.7594573e-03
 9.9431914e-01 2.7656174e-01 9.0423927e-02 2.1963720e-03 1.6046273e-02
 2.7015014e-03 9.9360728e-01 3.7066630e-01 9.4693172e-04 3.6502668e-01
 2.6091570e-03 9.9712640e-01 1.0879237e-01 4.8246926e-01 2.9406924e-04
 9.2565137e-01 3.0295399e-03 9.9936253e-01 1.2236894e-03 2.6848421e-01
 9.9986935e-01 5.3895872e-02 9.9304092e-01 8.5363901e-01 9.9881017e-01
 4.6798836e-05 9.8892128e-01 1.5863374e-02 9.9999344e-01 9.9957019e-01
 2.1772157e-01 6.1451548e-01 5.3213251e-01 3.9892551e-03 9.9523938e-01
 9.8414099e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 22:29:27, Dev, Step : 3444, Loss : 0.93069, Acc : 0.683, Auc : 0.800, Sensitive_Loss : 0.29170, Sensitive_Acc : 17.007, Sensitive_Auc : 0.997, Mean auc: 0.800, Run Time : 96.82 sec
INFO:root:2024-03-29 22:29:35, Train, Epoch : 8, Step : 3450, Loss : 0.49178, Acc : 0.450, Sensitive_Loss : 0.16628, Sensitive_Acc : 8.400, Run Time : 7.19 sec
INFO:root:2024-03-29 22:29:46, Train, Epoch : 8, Step : 3460, Loss : 0.67617, Acc : 0.753, Sensitive_Loss : 0.16016, Sensitive_Acc : 16.700, Run Time : 11.26 sec
INFO:root:2024-03-29 22:29:57, Train, Epoch : 8, Step : 3470, Loss : 1.00388, Acc : 0.728, Sensitive_Loss : 0.21505, Sensitive_Acc : 16.700, Run Time : 11.00 sec
INFO:root:2024-03-29 22:30:12, Train, Epoch : 8, Step : 3480, Loss : 0.71049, Acc : 0.753, Sensitive_Loss : 0.19303, Sensitive_Acc : 16.200, Run Time : 14.81 sec
INFO:root:2024-03-29 22:30:23, Train, Epoch : 8, Step : 3490, Loss : 0.69224, Acc : 0.762, Sensitive_Loss : 0.15899, Sensitive_Acc : 17.800, Run Time : 10.60 sec
INFO:root:2024-03-29 22:30:34, Train, Epoch : 8, Step : 3500, Loss : 0.63184, Acc : 0.734, Sensitive_Loss : 0.21155, Sensitive_Acc : 19.800, Run Time : 11.27 sec
INFO:root:2024-03-29 22:32:16, Dev, Step : 3500, Loss : 0.90651, Acc : 0.746, Auc : 0.801, Sensitive_Loss : 0.24614, Sensitive_Acc : 17.106, Sensitive_Auc : 0.997, Mean auc: 0.801, Run Time : 101.89 sec
INFO:root:2024-03-29 22:32:25, Train, Epoch : 8, Step : 3510, Loss : 0.61659, Acc : 0.741, Sensitive_Loss : 0.23002, Sensitive_Acc : 17.600, Run Time : 111.40 sec
INFO:root:2024-03-29 22:32:37, Train, Epoch : 8, Step : 3520, Loss : 0.71162, Acc : 0.766, Sensitive_Loss : 0.17109, Sensitive_Acc : 17.700, Run Time : 11.25 sec
INFO:root:2024-03-29 22:32:48, Train, Epoch : 8, Step : 3530, Loss : 0.77261, Acc : 0.738, Sensitive_Loss : 0.21598, Sensitive_Acc : 18.400, Run Time : 11.12 sec
INFO:root:2024-03-29 22:32:59, Train, Epoch : 8, Step : 3540, Loss : 0.74715, Acc : 0.778, Sensitive_Loss : 0.23061, Sensitive_Acc : 15.100, Run Time : 10.85 sec
INFO:root:2024-03-29 22:33:09, Train, Epoch : 8, Step : 3550, Loss : 0.65418, Acc : 0.784, Sensitive_Loss : 0.19776, Sensitive_Acc : 16.200, Run Time : 10.10 sec
INFO:root:2024-03-29 22:33:19, Train, Epoch : 8, Step : 3560, Loss : 0.76154, Acc : 0.747, Sensitive_Loss : 0.17377, Sensitive_Acc : 16.500, Run Time : 10.58 sec
INFO:root:2024-03-29 22:33:30, Train, Epoch : 8, Step : 3570, Loss : 0.68766, Acc : 0.741, Sensitive_Loss : 0.19949, Sensitive_Acc : 18.200, Run Time : 10.45 sec
INFO:root:2024-03-29 22:33:40, Train, Epoch : 8, Step : 3580, Loss : 0.70856, Acc : 0.794, Sensitive_Loss : 0.21396, Sensitive_Acc : 14.000, Run Time : 10.23 sec
INFO:root:2024-03-29 22:33:50, Train, Epoch : 8, Step : 3590, Loss : 0.62099, Acc : 0.800, Sensitive_Loss : 0.17965, Sensitive_Acc : 19.300, Run Time : 9.93 sec
INFO:root:2024-03-29 22:34:00, Train, Epoch : 8, Step : 3600, Loss : 0.74035, Acc : 0.756, Sensitive_Loss : 0.17922, Sensitive_Acc : 17.300, Run Time : 9.64 sec
INFO:root:2024-03-29 22:36:23, Dev, Step : 3600, Loss : 0.90663, Acc : 0.743, Auc : 0.802, Sensitive_Loss : 0.21987, Sensitive_Acc : 17.106, Sensitive_Auc : 0.997, Mean auc: 0.802, Run Time : 143.23 sec
INFO:root:2024-03-29 22:36:32, Train, Epoch : 8, Step : 3610, Loss : 0.64144, Acc : 0.734, Sensitive_Loss : 0.16002, Sensitive_Acc : 15.900, Run Time : 152.89 sec
INFO:root:2024-03-29 22:36:46, Train, Epoch : 8, Step : 3620, Loss : 0.60302, Acc : 0.744, Sensitive_Loss : 0.19220, Sensitive_Acc : 17.800, Run Time : 13.89 sec
INFO:root:2024-03-29 22:37:00, Train, Epoch : 8, Step : 3630, Loss : 0.73170, Acc : 0.759, Sensitive_Loss : 0.22738, Sensitive_Acc : 16.500, Run Time : 13.75 sec
INFO:root:2024-03-29 22:37:13, Train, Epoch : 8, Step : 3640, Loss : 0.55089, Acc : 0.738, Sensitive_Loss : 0.18508, Sensitive_Acc : 17.600, Run Time : 13.39 sec
INFO:root:2024-03-29 22:37:27, Train, Epoch : 8, Step : 3650, Loss : 0.74430, Acc : 0.738, Sensitive_Loss : 0.18443, Sensitive_Acc : 16.800, Run Time : 13.43 sec
INFO:root:2024-03-29 22:37:42, Train, Epoch : 8, Step : 3660, Loss : 0.69378, Acc : 0.809, Sensitive_Loss : 0.16676, Sensitive_Acc : 16.600, Run Time : 14.99 sec
INFO:root:2024-03-29 22:37:55, Train, Epoch : 8, Step : 3670, Loss : 0.85870, Acc : 0.756, Sensitive_Loss : 0.19474, Sensitive_Acc : 16.100, Run Time : 12.68 sec
INFO:root:2024-03-29 22:38:09, Train, Epoch : 8, Step : 3680, Loss : 0.68598, Acc : 0.744, Sensitive_Loss : 0.18034, Sensitive_Acc : 16.600, Run Time : 14.27 sec
INFO:root:2024-03-29 22:38:22, Train, Epoch : 8, Step : 3690, Loss : 0.67498, Acc : 0.728, Sensitive_Loss : 0.17862, Sensitive_Acc : 17.300, Run Time : 13.66 sec
INFO:root:2024-03-29 22:38:36, Train, Epoch : 8, Step : 3700, Loss : 0.63923, Acc : 0.734, Sensitive_Loss : 0.18261, Sensitive_Acc : 17.300, Run Time : 13.06 sec
INFO:root:2024-03-29 22:41:54, Dev, Step : 3700, Loss : 0.92398, Acc : 0.803, Auc : 0.798, Sensitive_Loss : 0.23232, Sensitive_Acc : 17.106, Sensitive_Auc : 0.997, Mean auc: 0.798, Run Time : 197.96 sec
INFO:root:2024-03-29 22:42:02, Train, Epoch : 8, Step : 3710, Loss : 0.79312, Acc : 0.762, Sensitive_Loss : 0.19112, Sensitive_Acc : 16.200, Run Time : 206.37 sec
INFO:root:2024-03-29 22:42:14, Train, Epoch : 8, Step : 3720, Loss : 0.57966, Acc : 0.781, Sensitive_Loss : 0.18286, Sensitive_Acc : 16.700, Run Time : 12.51 sec
INFO:root:2024-03-29 22:42:29, Train, Epoch : 8, Step : 3730, Loss : 0.75157, Acc : 0.753, Sensitive_Loss : 0.18757, Sensitive_Acc : 18.200, Run Time : 14.63 sec
INFO:root:2024-03-29 22:42:43, Train, Epoch : 8, Step : 3740, Loss : 0.68657, Acc : 0.756, Sensitive_Loss : 0.23674, Sensitive_Acc : 16.600, Run Time : 13.70 sec
INFO:root:2024-03-29 22:42:55, Train, Epoch : 8, Step : 3750, Loss : 0.71614, Acc : 0.744, Sensitive_Loss : 0.22943, Sensitive_Acc : 17.500, Run Time : 12.66 sec
INFO:root:2024-03-29 22:43:09, Train, Epoch : 8, Step : 3760, Loss : 0.67635, Acc : 0.775, Sensitive_Loss : 0.18656, Sensitive_Acc : 16.500, Run Time : 13.34 sec
INFO:root:2024-03-29 22:43:23, Train, Epoch : 8, Step : 3770, Loss : 0.81748, Acc : 0.744, Sensitive_Loss : 0.21944, Sensitive_Acc : 18.500, Run Time : 14.35 sec
INFO:root:2024-03-29 22:43:35, Train, Epoch : 8, Step : 3780, Loss : 0.74907, Acc : 0.744, Sensitive_Loss : 0.18294, Sensitive_Acc : 16.300, Run Time : 12.33 sec
INFO:root:2024-03-29 22:43:49, Train, Epoch : 8, Step : 3790, Loss : 0.68984, Acc : 0.766, Sensitive_Loss : 0.19287, Sensitive_Acc : 16.800, Run Time : 13.23 sec
INFO:root:2024-03-29 22:44:04, Train, Epoch : 8, Step : 3800, Loss : 0.61802, Acc : 0.778, Sensitive_Loss : 0.14760, Sensitive_Acc : 16.400, Run Time : 15.04 sec
INFO:root:2024-03-29 22:47:13, Dev, Step : 3800, Loss : 0.92530, Acc : 0.768, Auc : 0.790, Sensitive_Loss : 0.24607, Sensitive_Acc : 17.035, Sensitive_Auc : 0.996, Mean auc: 0.790, Run Time : 188.92 sec
INFO:root:2024-03-29 22:47:22, Train, Epoch : 8, Step : 3810, Loss : 0.76131, Acc : 0.741, Sensitive_Loss : 0.24552, Sensitive_Acc : 18.200, Run Time : 198.71 sec
INFO:root:2024-03-29 22:47:37, Train, Epoch : 8, Step : 3820, Loss : 0.74245, Acc : 0.769, Sensitive_Loss : 0.17702, Sensitive_Acc : 17.400, Run Time : 14.23 sec
INFO:root:2024-03-29 22:47:50, Train, Epoch : 8, Step : 3830, Loss : 0.78248, Acc : 0.778, Sensitive_Loss : 0.19014, Sensitive_Acc : 14.900, Run Time : 13.20 sec
INFO:root:2024-03-29 22:48:03, Train, Epoch : 8, Step : 3840, Loss : 0.65592, Acc : 0.725, Sensitive_Loss : 0.22128, Sensitive_Acc : 15.900, Run Time : 12.96 sec
INFO:root:2024-03-29 22:48:16, Train, Epoch : 8, Step : 3850, Loss : 0.67650, Acc : 0.778, Sensitive_Loss : 0.15124, Sensitive_Acc : 17.700, Run Time : 12.74 sec
INFO:root:2024-03-29 22:48:30, Train, Epoch : 8, Step : 3860, Loss : 0.93528, Acc : 0.731, Sensitive_Loss : 0.13362, Sensitive_Acc : 17.400, Run Time : 14.83 sec
INFO:root:2024-03-29 22:48:44, Train, Epoch : 8, Step : 3870, Loss : 0.59961, Acc : 0.734, Sensitive_Loss : 0.22374, Sensitive_Acc : 15.900, Run Time : 13.80 sec
INFO:root:2024-03-29 22:48:57, Train, Epoch : 8, Step : 3880, Loss : 0.72303, Acc : 0.734, Sensitive_Loss : 0.16441, Sensitive_Acc : 15.800, Run Time : 12.56 sec
INFO:root:2024-03-29 22:49:09, Train, Epoch : 8, Step : 3890, Loss : 0.77350, Acc : 0.750, Sensitive_Loss : 0.20410, Sensitive_Acc : 15.600, Run Time : 12.29 sec
INFO:root:2024-03-29 22:49:23, Train, Epoch : 8, Step : 3900, Loss : 0.60601, Acc : 0.797, Sensitive_Loss : 0.16120, Sensitive_Acc : 15.900, Run Time : 13.47 sec
INFO:root:2024-03-29 22:52:34, Dev, Step : 3900, Loss : 0.91256, Acc : 0.784, Auc : 0.798, Sensitive_Loss : 0.25272, Sensitive_Acc : 16.936, Sensitive_Auc : 0.997, Mean auc: 0.798, Run Time : 191.95 sec
INFO:root:2024-03-29 22:52:43, Train, Epoch : 8, Step : 3910, Loss : 0.80198, Acc : 0.734, Sensitive_Loss : 0.14137, Sensitive_Acc : 16.900, Run Time : 200.04 sec
INFO:root:2024-03-29 22:52:56, Train, Epoch : 8, Step : 3920, Loss : 0.64688, Acc : 0.794, Sensitive_Loss : 0.21807, Sensitive_Acc : 18.900, Run Time : 13.19 sec
INFO:root:2024-03-29 22:53:09, Train, Epoch : 8, Step : 3930, Loss : 0.82358, Acc : 0.756, Sensitive_Loss : 0.22377, Sensitive_Acc : 16.600, Run Time : 13.06 sec
INFO:root:2024-03-29 22:55:14
INFO:root:y_pred: [0.14537811 0.03312763 0.19064763 ... 0.15500873 0.5041234  0.03501535]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [1.51596172e-03 3.38479996e-01 9.96103764e-01 9.12574902e-02
 9.99442995e-01 4.60388279e-03 1.35280907e-01 5.22008389e-02
 4.64155077e-04 3.18320915e-02 1.02154650e-02 8.30289200e-02
 9.85743821e-01 1.70512835e-03 9.95552957e-01 9.97032166e-01
 7.31939508e-05 1.74744755e-01 9.94466901e-01 8.94097984e-01
 6.03595972e-02 2.84095318e-03 9.99797881e-01 9.97333646e-01
 2.51666494e-02 8.85607600e-01 3.26094858e-04 9.98959541e-01
 2.55938387e-03 9.64333594e-01 9.99827325e-01 9.45540249e-01
 3.04432996e-02 1.08462455e-05 1.49405286e-01 1.01116085e-02
 1.88009724e-01 8.84634554e-02 9.95430350e-01 9.98242617e-01
 3.20480880e-03 9.77915828e-04 9.83930051e-01 9.99476373e-01
 1.57131162e-02 3.08348909e-02 4.06472944e-03 3.26974869e-01
 9.88763928e-01 5.75132072e-01 9.99385834e-01 3.09469283e-01
 9.97322619e-01 9.88717198e-01 9.93403375e-01 9.56635654e-01
 9.29294765e-01 5.86677901e-02 9.99675035e-01 1.49082541e-02
 6.56632110e-05 2.38202065e-01 7.42008746e-01 9.96147990e-01
 9.97919381e-01 9.99317646e-01 5.26154220e-01 9.98974085e-01
 9.90669727e-01 9.98157322e-01 9.92633164e-01 9.58141498e-03
 1.00646382e-02 7.40161180e-01 9.83764946e-01 2.89277602e-02
 9.72356975e-01 5.43198839e-04 9.99629140e-01 3.03567767e-01
 1.39136329e-01 3.13432887e-02 1.10549284e-02 1.75937608e-01
 9.80274200e-01 1.31909043e-01 8.32780480e-01 5.63601032e-02
 3.31061780e-02 2.25804951e-02 5.78807527e-03 3.59594449e-02
 5.71266450e-02 5.54673374e-03 9.96691465e-01 9.23441172e-01
 6.09744107e-03 1.05987024e-03 9.63457644e-01 9.20929313e-01
 8.95700037e-01 4.24197577e-02 9.99946713e-01 2.33278032e-02
 3.73571855e-03 9.92582440e-01 2.37207696e-01 3.89699452e-02
 2.06757337e-03 4.92249662e-03 1.95962470e-03 9.94250774e-01
 1.69813380e-01 7.46426813e-04 2.34095231e-01 3.19848798e-04
 9.96164680e-01 4.90040779e-02 3.75688910e-01 2.20514674e-04
 9.17745531e-01 1.49634900e-03 9.99153852e-01 7.87046563e-04
 1.17303818e-01 9.99907136e-01 3.12498957e-02 9.89555120e-01
 7.66179323e-01 9.98941004e-01 2.58379441e-05 9.87433136e-01
 5.90604404e-03 9.99986053e-01 9.99671817e-01 1.29124388e-01
 4.27489638e-01 3.12836975e-01 1.64812431e-03 9.94212806e-01
 9.82781351e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 22:55:14, Dev, Step : 3936, Loss : 0.93021, Acc : 0.796, Auc : 0.793, Sensitive_Loss : 0.23436, Sensitive_Acc : 17.035, Sensitive_Auc : 0.997, Mean auc: 0.793, Run Time : 115.88 sec
INFO:root:2024-03-29 22:55:19, Train, Epoch : 9, Step : 3940, Loss : 0.27309, Acc : 0.306, Sensitive_Loss : 0.06826, Sensitive_Acc : 5.400, Run Time : 4.65 sec
INFO:root:2024-03-29 22:55:30, Train, Epoch : 9, Step : 3950, Loss : 0.60058, Acc : 0.762, Sensitive_Loss : 0.24802, Sensitive_Acc : 16.700, Run Time : 10.59 sec
INFO:root:2024-03-29 22:55:40, Train, Epoch : 9, Step : 3960, Loss : 0.71519, Acc : 0.747, Sensitive_Loss : 0.19179, Sensitive_Acc : 18.900, Run Time : 10.04 sec
INFO:root:2024-03-29 22:55:51, Train, Epoch : 9, Step : 3970, Loss : 0.62698, Acc : 0.791, Sensitive_Loss : 0.16000, Sensitive_Acc : 14.900, Run Time : 10.97 sec
INFO:root:2024-03-29 22:56:04, Train, Epoch : 9, Step : 3980, Loss : 0.50411, Acc : 0.725, Sensitive_Loss : 0.14886, Sensitive_Acc : 17.300, Run Time : 13.18 sec
INFO:root:2024-03-29 22:56:15, Train, Epoch : 9, Step : 3990, Loss : 0.74100, Acc : 0.772, Sensitive_Loss : 0.15828, Sensitive_Acc : 16.900, Run Time : 10.50 sec
INFO:root:2024-03-29 22:56:25, Train, Epoch : 9, Step : 4000, Loss : 0.77099, Acc : 0.750, Sensitive_Loss : 0.29420, Sensitive_Acc : 17.400, Run Time : 10.35 sec
INFO:root:2024-03-29 22:58:02, Dev, Step : 4000, Loss : 0.93060, Acc : 0.756, Auc : 0.793, Sensitive_Loss : 0.22933, Sensitive_Acc : 17.106, Sensitive_Auc : 0.997, Mean auc: 0.793, Run Time : 97.59 sec
INFO:root:2024-03-29 22:58:10, Train, Epoch : 9, Step : 4010, Loss : 0.64272, Acc : 0.759, Sensitive_Loss : 0.18824, Sensitive_Acc : 16.800, Run Time : 104.84 sec
INFO:root:2024-03-29 22:58:21, Train, Epoch : 9, Step : 4020, Loss : 0.54471, Acc : 0.791, Sensitive_Loss : 0.19291, Sensitive_Acc : 15.300, Run Time : 10.78 sec
INFO:root:2024-03-29 22:58:35, Train, Epoch : 9, Step : 4030, Loss : 0.57471, Acc : 0.709, Sensitive_Loss : 0.14553, Sensitive_Acc : 14.700, Run Time : 14.49 sec
INFO:root:2024-03-29 22:58:46, Train, Epoch : 9, Step : 4040, Loss : 0.73740, Acc : 0.781, Sensitive_Loss : 0.22938, Sensitive_Acc : 17.700, Run Time : 10.84 sec
INFO:root:2024-03-29 22:58:56, Train, Epoch : 9, Step : 4050, Loss : 0.73286, Acc : 0.750, Sensitive_Loss : 0.23375, Sensitive_Acc : 18.800, Run Time : 10.24 sec
INFO:root:2024-03-29 22:59:11, Train, Epoch : 9, Step : 4060, Loss : 0.58971, Acc : 0.750, Sensitive_Loss : 0.21970, Sensitive_Acc : 17.500, Run Time : 14.54 sec
INFO:root:2024-03-29 22:59:21, Train, Epoch : 9, Step : 4070, Loss : 0.72834, Acc : 0.772, Sensitive_Loss : 0.15651, Sensitive_Acc : 16.400, Run Time : 10.46 sec
INFO:root:2024-03-29 22:59:32, Train, Epoch : 9, Step : 4080, Loss : 0.61134, Acc : 0.772, Sensitive_Loss : 0.16862, Sensitive_Acc : 16.100, Run Time : 10.72 sec
INFO:root:2024-03-29 22:59:42, Train, Epoch : 9, Step : 4090, Loss : 0.68486, Acc : 0.791, Sensitive_Loss : 0.13777, Sensitive_Acc : 13.500, Run Time : 10.27 sec
INFO:root:2024-03-29 22:59:56, Train, Epoch : 9, Step : 4100, Loss : 0.66720, Acc : 0.759, Sensitive_Loss : 0.19402, Sensitive_Acc : 16.200, Run Time : 14.06 sec
INFO:root:2024-03-29 23:01:34, Dev, Step : 4100, Loss : 0.93979, Acc : 0.768, Auc : 0.790, Sensitive_Loss : 0.21614, Sensitive_Acc : 17.035, Sensitive_Auc : 0.997, Mean auc: 0.790, Run Time : 97.83 sec
INFO:root:2024-03-29 23:01:44, Train, Epoch : 9, Step : 4110, Loss : 0.72650, Acc : 0.762, Sensitive_Loss : 0.20265, Sensitive_Acc : 17.500, Run Time : 107.47 sec
INFO:root:2024-03-29 23:01:55, Train, Epoch : 9, Step : 4120, Loss : 0.60413, Acc : 0.778, Sensitive_Loss : 0.19144, Sensitive_Acc : 16.800, Run Time : 11.24 sec
INFO:root:2024-03-29 23:02:06, Train, Epoch : 9, Step : 4130, Loss : 0.78037, Acc : 0.725, Sensitive_Loss : 0.20647, Sensitive_Acc : 18.200, Run Time : 10.78 sec
INFO:root:2024-03-29 23:02:18, Train, Epoch : 9, Step : 4140, Loss : 0.63363, Acc : 0.772, Sensitive_Loss : 0.16491, Sensitive_Acc : 15.800, Run Time : 12.49 sec
INFO:root:2024-03-29 23:02:29, Train, Epoch : 9, Step : 4150, Loss : 0.57078, Acc : 0.759, Sensitive_Loss : 0.18807, Sensitive_Acc : 16.000, Run Time : 10.48 sec
INFO:root:2024-03-29 23:02:39, Train, Epoch : 9, Step : 4160, Loss : 0.56149, Acc : 0.778, Sensitive_Loss : 0.17933, Sensitive_Acc : 17.600, Run Time : 10.61 sec
INFO:root:2024-03-29 23:02:50, Train, Epoch : 9, Step : 4170, Loss : 0.62603, Acc : 0.766, Sensitive_Loss : 0.17671, Sensitive_Acc : 15.000, Run Time : 11.15 sec
INFO:root:2024-03-29 23:03:01, Train, Epoch : 9, Step : 4180, Loss : 0.64281, Acc : 0.791, Sensitive_Loss : 0.26835, Sensitive_Acc : 17.800, Run Time : 10.99 sec
INFO:root:2024-03-29 23:03:13, Train, Epoch : 9, Step : 4190, Loss : 0.73094, Acc : 0.775, Sensitive_Loss : 0.20274, Sensitive_Acc : 17.600, Run Time : 11.27 sec
INFO:root:2024-03-29 23:03:25, Train, Epoch : 9, Step : 4200, Loss : 0.69704, Acc : 0.762, Sensitive_Loss : 0.17418, Sensitive_Acc : 14.500, Run Time : 12.87 sec
INFO:root:2024-03-29 23:05:03, Dev, Step : 4200, Loss : 0.93938, Acc : 0.791, Auc : 0.791, Sensitive_Loss : 0.21835, Sensitive_Acc : 17.035, Sensitive_Auc : 0.997, Mean auc: 0.791, Run Time : 97.89 sec
INFO:root:2024-03-29 23:05:11, Train, Epoch : 9, Step : 4210, Loss : 0.50733, Acc : 0.806, Sensitive_Loss : 0.15475, Sensitive_Acc : 17.500, Run Time : 105.58 sec
INFO:root:2024-03-29 23:05:22, Train, Epoch : 9, Step : 4220, Loss : 0.59181, Acc : 0.803, Sensitive_Loss : 0.18311, Sensitive_Acc : 14.200, Run Time : 10.85 sec
INFO:root:2024-03-29 23:05:33, Train, Epoch : 9, Step : 4230, Loss : 0.72270, Acc : 0.784, Sensitive_Loss : 0.15745, Sensitive_Acc : 16.000, Run Time : 10.77 sec
INFO:root:2024-03-29 23:05:47, Train, Epoch : 9, Step : 4240, Loss : 0.54637, Acc : 0.769, Sensitive_Loss : 0.18761, Sensitive_Acc : 12.900, Run Time : 14.66 sec
INFO:root:2024-03-29 23:05:58, Train, Epoch : 9, Step : 4250, Loss : 0.66174, Acc : 0.762, Sensitive_Loss : 0.17792, Sensitive_Acc : 15.200, Run Time : 10.31 sec
INFO:root:2024-03-29 23:06:09, Train, Epoch : 9, Step : 4260, Loss : 0.61554, Acc : 0.759, Sensitive_Loss : 0.19256, Sensitive_Acc : 13.500, Run Time : 11.67 sec
INFO:root:2024-03-29 23:06:20, Train, Epoch : 9, Step : 4270, Loss : 0.66556, Acc : 0.800, Sensitive_Loss : 0.17880, Sensitive_Acc : 16.500, Run Time : 11.11 sec
INFO:root:2024-03-29 23:06:31, Train, Epoch : 9, Step : 4280, Loss : 0.75437, Acc : 0.756, Sensitive_Loss : 0.17066, Sensitive_Acc : 16.300, Run Time : 10.60 sec
INFO:root:2024-03-29 23:06:41, Train, Epoch : 9, Step : 4290, Loss : 0.61285, Acc : 0.822, Sensitive_Loss : 0.19093, Sensitive_Acc : 16.500, Run Time : 10.10 sec
INFO:root:2024-03-29 23:06:54, Train, Epoch : 9, Step : 4300, Loss : 0.79737, Acc : 0.759, Sensitive_Loss : 0.14915, Sensitive_Acc : 17.400, Run Time : 13.31 sec
INFO:root:2024-03-29 23:08:32, Dev, Step : 4300, Loss : 0.93781, Acc : 0.780, Auc : 0.793, Sensitive_Loss : 0.21323, Sensitive_Acc : 17.106, Sensitive_Auc : 0.997, Mean auc: 0.793, Run Time : 97.62 sec
INFO:root:2024-03-29 23:08:40, Train, Epoch : 9, Step : 4310, Loss : 0.73367, Acc : 0.756, Sensitive_Loss : 0.16936, Sensitive_Acc : 16.900, Run Time : 105.36 sec
INFO:root:2024-03-29 23:08:50, Train, Epoch : 9, Step : 4320, Loss : 0.63153, Acc : 0.791, Sensitive_Loss : 0.15535, Sensitive_Acc : 17.700, Run Time : 10.55 sec
INFO:root:2024-03-29 23:09:01, Train, Epoch : 9, Step : 4330, Loss : 0.63942, Acc : 0.816, Sensitive_Loss : 0.12843, Sensitive_Acc : 17.200, Run Time : 10.26 sec
INFO:root:2024-03-29 23:09:12, Train, Epoch : 9, Step : 4340, Loss : 0.66101, Acc : 0.778, Sensitive_Loss : 0.19445, Sensitive_Acc : 18.100, Run Time : 11.75 sec
INFO:root:2024-03-29 23:09:23, Train, Epoch : 9, Step : 4350, Loss : 0.63008, Acc : 0.787, Sensitive_Loss : 0.17825, Sensitive_Acc : 16.300, Run Time : 10.81 sec
INFO:root:2024-03-29 23:09:33, Train, Epoch : 9, Step : 4360, Loss : 0.63842, Acc : 0.787, Sensitive_Loss : 0.23460, Sensitive_Acc : 17.600, Run Time : 10.12 sec
INFO:root:2024-03-29 23:09:44, Train, Epoch : 9, Step : 4370, Loss : 0.62291, Acc : 0.794, Sensitive_Loss : 0.15892, Sensitive_Acc : 16.600, Run Time : 11.19 sec
INFO:root:2024-03-29 23:09:57, Train, Epoch : 9, Step : 4380, Loss : 0.72410, Acc : 0.772, Sensitive_Loss : 0.16921, Sensitive_Acc : 15.900, Run Time : 12.32 sec
INFO:root:2024-03-29 23:10:07, Train, Epoch : 9, Step : 4390, Loss : 0.58313, Acc : 0.781, Sensitive_Loss : 0.16121, Sensitive_Acc : 16.700, Run Time : 10.17 sec
INFO:root:2024-03-29 23:10:17, Train, Epoch : 9, Step : 4400, Loss : 0.67418, Acc : 0.800, Sensitive_Loss : 0.18137, Sensitive_Acc : 18.700, Run Time : 10.13 sec
INFO:root:2024-03-29 23:11:55, Dev, Step : 4400, Loss : 0.95409, Acc : 0.814, Auc : 0.792, Sensitive_Loss : 0.22414, Sensitive_Acc : 17.106, Sensitive_Auc : 0.997, Mean auc: 0.792, Run Time : 97.61 sec
INFO:root:2024-03-29 23:12:02, Train, Epoch : 9, Step : 4410, Loss : 0.63134, Acc : 0.797, Sensitive_Loss : 0.18326, Sensitive_Acc : 14.100, Run Time : 104.62 sec
INFO:root:2024-03-29 23:12:12, Train, Epoch : 9, Step : 4420, Loss : 0.72136, Acc : 0.762, Sensitive_Loss : 0.16256, Sensitive_Acc : 17.100, Run Time : 10.66 sec
INFO:root:2024-03-29 23:14:00
INFO:root:y_pred: [0.13319835 0.03483433 0.19712447 ... 0.15669075 0.53991234 0.03445781]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [7.92396022e-04 4.73657280e-01 9.95396674e-01 2.60874350e-02
 9.99468029e-01 2.81628361e-03 8.60596448e-02 3.88467871e-02
 5.73999772e-04 1.87186711e-02 4.13239561e-03 6.42534792e-02
 9.65878129e-01 7.65839883e-04 9.94024098e-01 9.95995879e-01
 7.30188258e-05 1.07982837e-01 9.93339121e-01 8.88406813e-01
 3.06455139e-02 1.71853043e-03 9.99715626e-01 9.97941911e-01
 1.54271396e-02 8.88221562e-01 2.05440316e-04 9.98304605e-01
 8.13590188e-04 9.65603054e-01 9.99736130e-01 9.23136592e-01
 2.52210218e-02 5.53778000e-06 1.53040409e-01 4.40128054e-03
 3.90905403e-02 5.97820170e-02 9.93061602e-01 9.96785760e-01
 1.31601747e-03 1.07852591e-03 9.69701886e-01 9.99445021e-01
 1.88027117e-02 3.62371542e-02 1.94753287e-03 1.83229193e-01
 9.88986731e-01 6.54459476e-01 9.99614358e-01 1.45848915e-01
 9.94677782e-01 9.88248587e-01 9.86884952e-01 9.15517867e-01
 9.11932707e-01 6.32188842e-02 9.99675155e-01 1.34701133e-02
 9.41803810e-05 1.04042955e-01 5.81931829e-01 9.96544063e-01
 9.96118188e-01 9.98711705e-01 4.65016723e-01 9.99360621e-01
 9.90957618e-01 9.97674406e-01 9.91290927e-01 6.29675156e-03
 7.78606767e-03 5.77191651e-01 9.79776919e-01 1.69532783e-02
 9.34577525e-01 1.96866400e-04 9.95897651e-01 2.85723329e-01
 7.28074759e-02 1.46938842e-02 3.05581745e-03 6.76752254e-02
 9.65912938e-01 8.72194022e-02 7.27367640e-01 6.39653206e-02
 2.58747544e-02 1.87134352e-02 4.62216558e-03 1.24194091e-02
 4.89410721e-02 6.62911637e-03 9.97257769e-01 8.76948953e-01
 3.78569472e-03 8.75454454e-04 8.78782868e-01 9.27849233e-01
 9.31723356e-01 1.75120924e-02 9.99923110e-01 1.78616475e-02
 1.76029454e-03 9.94517267e-01 1.33061454e-01 2.02259533e-02
 1.36113341e-03 1.42237998e-03 2.46733730e-03 9.94759262e-01
 1.35457709e-01 3.25130677e-04 3.05832833e-01 1.43211248e-04
 9.90832865e-01 4.00018133e-02 3.05850357e-01 3.40367493e-04
 8.05670500e-01 6.20936975e-04 9.98612046e-01 6.61800848e-04
 5.48701137e-02 9.99926209e-01 1.81008056e-02 9.88709748e-01
 7.71940231e-01 9.98659253e-01 1.00901998e-05 9.86540258e-01
 6.98058633e-03 9.99967217e-01 9.98712897e-01 3.16263027e-02
 4.26270276e-01 2.67174453e-01 1.37857057e-03 9.92257595e-01
 9.31951702e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 23:14:00, Dev, Step : 4428, Loss : 0.94269, Acc : 0.785, Auc : 0.790, Sensitive_Loss : 0.21050, Sensitive_Acc : 17.106, Sensitive_Auc : 0.997, Mean auc: 0.790, Run Time : 97.68 sec
INFO:root:2024-03-29 23:14:06, Train, Epoch : 10, Step : 4430, Loss : 0.12373, Acc : 0.159, Sensitive_Loss : 0.02224, Sensitive_Acc : 2.700, Run Time : 3.01 sec
INFO:root:2024-03-29 23:14:18, Train, Epoch : 10, Step : 4440, Loss : 0.72119, Acc : 0.769, Sensitive_Loss : 0.17838, Sensitive_Acc : 16.600, Run Time : 11.24 sec
INFO:root:2024-03-29 23:14:29, Train, Epoch : 10, Step : 4450, Loss : 0.73426, Acc : 0.756, Sensitive_Loss : 0.18088, Sensitive_Acc : 16.800, Run Time : 11.45 sec
INFO:root:2024-03-29 23:14:45, Train, Epoch : 10, Step : 4460, Loss : 0.71849, Acc : 0.787, Sensitive_Loss : 0.17555, Sensitive_Acc : 16.300, Run Time : 15.59 sec
INFO:root:2024-03-29 23:14:57, Train, Epoch : 10, Step : 4470, Loss : 0.57979, Acc : 0.772, Sensitive_Loss : 0.18620, Sensitive_Acc : 17.000, Run Time : 12.80 sec
INFO:root:2024-03-29 23:15:09, Train, Epoch : 10, Step : 4480, Loss : 0.53924, Acc : 0.797, Sensitive_Loss : 0.15625, Sensitive_Acc : 16.300, Run Time : 11.28 sec
INFO:root:2024-03-29 23:15:23, Train, Epoch : 10, Step : 4490, Loss : 0.59274, Acc : 0.838, Sensitive_Loss : 0.18109, Sensitive_Acc : 14.200, Run Time : 14.34 sec
INFO:root:2024-03-29 23:15:34, Train, Epoch : 10, Step : 4500, Loss : 0.66604, Acc : 0.794, Sensitive_Loss : 0.19249, Sensitive_Acc : 14.900, Run Time : 10.49 sec
INFO:root:2024-03-29 23:17:12, Dev, Step : 4500, Loss : 0.95239, Acc : 0.840, Auc : 0.803, Sensitive_Loss : 0.22217, Sensitive_Acc : 17.106, Sensitive_Auc : 0.997, Mean auc: 0.803, Run Time : 98.51 sec
INFO:root:2024-03-29 23:17:20, Train, Epoch : 10, Step : 4510, Loss : 0.62067, Acc : 0.769, Sensitive_Loss : 0.13853, Sensitive_Acc : 17.300, Run Time : 106.00 sec
INFO:root:2024-03-29 23:17:32, Train, Epoch : 10, Step : 4520, Loss : 0.65549, Acc : 0.756, Sensitive_Loss : 0.18375, Sensitive_Acc : 14.700, Run Time : 12.81 sec
INFO:root:2024-03-29 23:17:45, Train, Epoch : 10, Step : 4530, Loss : 0.46970, Acc : 0.759, Sensitive_Loss : 0.16056, Sensitive_Acc : 17.200, Run Time : 12.17 sec
INFO:root:2024-03-29 23:17:56, Train, Epoch : 10, Step : 4540, Loss : 0.56377, Acc : 0.791, Sensitive_Loss : 0.16920, Sensitive_Acc : 15.000, Run Time : 11.16 sec
INFO:root:2024-03-29 23:18:08, Train, Epoch : 10, Step : 4550, Loss : 0.52257, Acc : 0.794, Sensitive_Loss : 0.18873, Sensitive_Acc : 16.600, Run Time : 11.82 sec
INFO:root:2024-03-29 23:18:18, Train, Epoch : 10, Step : 4560, Loss : 0.66584, Acc : 0.800, Sensitive_Loss : 0.18654, Sensitive_Acc : 17.300, Run Time : 10.40 sec
INFO:root:2024-03-29 23:18:28, Train, Epoch : 10, Step : 4570, Loss : 0.64887, Acc : 0.797, Sensitive_Loss : 0.16858, Sensitive_Acc : 17.300, Run Time : 10.53 sec
INFO:root:2024-03-29 23:18:40, Train, Epoch : 10, Step : 4580, Loss : 0.53139, Acc : 0.809, Sensitive_Loss : 0.14746, Sensitive_Acc : 15.400, Run Time : 11.40 sec
INFO:root:2024-03-29 23:18:56, Train, Epoch : 10, Step : 4590, Loss : 0.61823, Acc : 0.812, Sensitive_Loss : 0.16452, Sensitive_Acc : 14.900, Run Time : 16.04 sec
INFO:root:2024-03-29 23:19:06, Train, Epoch : 10, Step : 4600, Loss : 0.67946, Acc : 0.775, Sensitive_Loss : 0.16268, Sensitive_Acc : 16.300, Run Time : 10.49 sec
INFO:root:2024-03-29 23:21:54, Dev, Step : 4600, Loss : 0.99147, Acc : 0.824, Auc : 0.788, Sensitive_Loss : 0.22694, Sensitive_Acc : 17.106, Sensitive_Auc : 0.995, Mean auc: 0.788, Run Time : 167.46 sec
INFO:root:2024-03-29 23:22:02, Train, Epoch : 10, Step : 4610, Loss : 0.47521, Acc : 0.772, Sensitive_Loss : 0.18864, Sensitive_Acc : 17.000, Run Time : 175.17 sec
INFO:root:2024-03-29 23:22:13, Train, Epoch : 10, Step : 4620, Loss : 0.55596, Acc : 0.784, Sensitive_Loss : 0.29222, Sensitive_Acc : 15.800, Run Time : 11.46 sec
INFO:root:2024-03-29 23:22:27, Train, Epoch : 10, Step : 4630, Loss : 0.71516, Acc : 0.738, Sensitive_Loss : 0.16921, Sensitive_Acc : 17.200, Run Time : 14.01 sec
INFO:root:2024-03-29 23:22:38, Train, Epoch : 10, Step : 4640, Loss : 0.53193, Acc : 0.784, Sensitive_Loss : 0.20776, Sensitive_Acc : 17.100, Run Time : 11.08 sec
INFO:root:2024-03-29 23:22:50, Train, Epoch : 10, Step : 4650, Loss : 0.61183, Acc : 0.806, Sensitive_Loss : 0.18738, Sensitive_Acc : 17.900, Run Time : 11.48 sec
INFO:root:2024-03-29 23:23:02, Train, Epoch : 10, Step : 4660, Loss : 0.61982, Acc : 0.794, Sensitive_Loss : 0.16540, Sensitive_Acc : 17.000, Run Time : 12.80 sec
INFO:root:2024-03-29 23:23:14, Train, Epoch : 10, Step : 4670, Loss : 0.76658, Acc : 0.787, Sensitive_Loss : 0.13096, Sensitive_Acc : 17.100, Run Time : 11.55 sec
INFO:root:2024-03-29 23:23:25, Train, Epoch : 10, Step : 4680, Loss : 0.55081, Acc : 0.834, Sensitive_Loss : 0.19524, Sensitive_Acc : 16.400, Run Time : 11.00 sec
INFO:root:2024-03-29 23:23:39, Train, Epoch : 10, Step : 4690, Loss : 0.77153, Acc : 0.781, Sensitive_Loss : 0.15653, Sensitive_Acc : 15.100, Run Time : 13.78 sec
INFO:root:2024-03-29 23:23:53, Train, Epoch : 10, Step : 4700, Loss : 0.57572, Acc : 0.809, Sensitive_Loss : 0.16755, Sensitive_Acc : 15.500, Run Time : 14.18 sec
INFO:root:2024-03-29 23:25:31, Dev, Step : 4700, Loss : 0.93903, Acc : 0.741, Auc : 0.793, Sensitive_Loss : 0.24961, Sensitive_Acc : 16.936, Sensitive_Auc : 0.996, Mean auc: 0.793, Run Time : 98.03 sec
INFO:root:2024-03-29 23:25:39, Train, Epoch : 10, Step : 4710, Loss : 0.62777, Acc : 0.781, Sensitive_Loss : 0.15622, Sensitive_Acc : 14.900, Run Time : 106.11 sec
INFO:root:2024-03-29 23:25:51, Train, Epoch : 10, Step : 4720, Loss : 0.56075, Acc : 0.819, Sensitive_Loss : 0.15582, Sensitive_Acc : 17.200, Run Time : 12.12 sec
INFO:root:2024-03-29 23:26:05, Train, Epoch : 10, Step : 4730, Loss : 0.64482, Acc : 0.794, Sensitive_Loss : 0.14047, Sensitive_Acc : 14.700, Run Time : 14.21 sec
INFO:root:2024-03-29 23:26:17, Train, Epoch : 10, Step : 4740, Loss : 0.54234, Acc : 0.750, Sensitive_Loss : 0.17215, Sensitive_Acc : 17.600, Run Time : 11.63 sec
INFO:root:2024-03-29 23:26:29, Train, Epoch : 10, Step : 4750, Loss : 0.66932, Acc : 0.803, Sensitive_Loss : 0.22875, Sensitive_Acc : 16.200, Run Time : 11.75 sec
INFO:root:2024-03-29 23:26:45, Train, Epoch : 10, Step : 4760, Loss : 0.58221, Acc : 0.775, Sensitive_Loss : 0.15635, Sensitive_Acc : 17.500, Run Time : 15.99 sec
INFO:root:2024-03-29 23:26:57, Train, Epoch : 10, Step : 4770, Loss : 0.68336, Acc : 0.806, Sensitive_Loss : 0.16777, Sensitive_Acc : 14.800, Run Time : 12.55 sec
INFO:root:2024-03-29 23:27:10, Train, Epoch : 10, Step : 4780, Loss : 0.50796, Acc : 0.800, Sensitive_Loss : 0.13627, Sensitive_Acc : 15.300, Run Time : 12.24 sec
INFO:root:2024-03-29 23:27:22, Train, Epoch : 10, Step : 4790, Loss : 0.63049, Acc : 0.794, Sensitive_Loss : 0.13257, Sensitive_Acc : 15.300, Run Time : 12.02 sec
INFO:root:2024-03-29 23:27:33, Train, Epoch : 10, Step : 4800, Loss : 0.62923, Acc : 0.750, Sensitive_Loss : 0.21660, Sensitive_Acc : 19.800, Run Time : 11.45 sec
INFO:root:2024-03-29 23:29:41, Dev, Step : 4800, Loss : 0.97374, Acc : 0.770, Auc : 0.779, Sensitive_Loss : 0.21825, Sensitive_Acc : 17.149, Sensitive_Auc : 0.997, Mean auc: 0.779, Run Time : 127.96 sec
INFO:root:2024-03-29 23:29:49, Train, Epoch : 10, Step : 4810, Loss : 0.53457, Acc : 0.794, Sensitive_Loss : 0.22219, Sensitive_Acc : 17.000, Run Time : 136.43 sec
INFO:root:2024-03-29 23:30:01, Train, Epoch : 10, Step : 4820, Loss : 0.75091, Acc : 0.781, Sensitive_Loss : 0.15152, Sensitive_Acc : 18.600, Run Time : 11.53 sec
INFO:root:2024-03-29 23:30:14, Train, Epoch : 10, Step : 4830, Loss : 0.60551, Acc : 0.841, Sensitive_Loss : 0.14784, Sensitive_Acc : 15.300, Run Time : 12.68 sec
INFO:root:2024-03-29 23:30:25, Train, Epoch : 10, Step : 4840, Loss : 0.67730, Acc : 0.822, Sensitive_Loss : 0.21363, Sensitive_Acc : 16.700, Run Time : 11.28 sec
INFO:root:2024-03-29 23:30:36, Train, Epoch : 10, Step : 4850, Loss : 0.65902, Acc : 0.769, Sensitive_Loss : 0.17425, Sensitive_Acc : 14.800, Run Time : 11.44 sec
INFO:root:2024-03-29 23:30:51, Train, Epoch : 10, Step : 4860, Loss : 0.57607, Acc : 0.825, Sensitive_Loss : 0.19112, Sensitive_Acc : 15.400, Run Time : 14.33 sec
INFO:root:2024-03-29 23:31:03, Train, Epoch : 10, Step : 4870, Loss : 0.72081, Acc : 0.778, Sensitive_Loss : 0.16943, Sensitive_Acc : 16.800, Run Time : 12.55 sec
INFO:root:2024-03-29 23:31:15, Train, Epoch : 10, Step : 4880, Loss : 0.51909, Acc : 0.781, Sensitive_Loss : 0.18818, Sensitive_Acc : 16.200, Run Time : 11.52 sec
INFO:root:2024-03-29 23:31:28, Train, Epoch : 10, Step : 4890, Loss : 0.68548, Acc : 0.787, Sensitive_Loss : 0.17587, Sensitive_Acc : 17.500, Run Time : 13.14 sec
INFO:root:2024-03-29 23:31:40, Train, Epoch : 10, Step : 4900, Loss : 0.73198, Acc : 0.772, Sensitive_Loss : 0.21192, Sensitive_Acc : 15.900, Run Time : 12.21 sec
INFO:root:2024-03-29 23:33:30, Dev, Step : 4900, Loss : 0.92992, Acc : 0.754, Auc : 0.794, Sensitive_Loss : 0.23359, Sensitive_Acc : 16.993, Sensitive_Auc : 0.998, Mean auc: 0.794, Run Time : 109.48 sec
INFO:root:2024-03-29 23:33:37, Train, Epoch : 10, Step : 4910, Loss : 0.58834, Acc : 0.784, Sensitive_Loss : 0.16235, Sensitive_Acc : 16.900, Run Time : 117.35 sec
INFO:root:2024-03-29 23:33:50, Train, Epoch : 10, Step : 4920, Loss : 0.62257, Acc : 0.794, Sensitive_Loss : 0.12371, Sensitive_Acc : 17.800, Run Time : 12.56 sec
INFO:root:2024-03-29 23:35:26
INFO:root:y_pred: [0.10569635 0.03792961 0.13497902 ... 0.05407226 0.8372008  0.02489733]
INFO:root:y_true: [0. 0. 0. ... 0. 0. 0.]
INFO:root:sensitive_y_pred: [5.91081567e-04 3.47635418e-01 9.95614529e-01 2.78885234e-02
 9.99690413e-01 4.87872912e-03 5.68109900e-02 2.10157242e-02
 5.98155777e-04 1.20798433e-02 6.69422699e-03 1.10873789e-01
 9.63026166e-01 8.09506397e-04 9.96094763e-01 9.96404052e-01
 8.06967801e-05 1.11280300e-01 9.94506180e-01 8.73561263e-01
 9.18956473e-02 1.26447377e-03 9.99934554e-01 9.96625423e-01
 1.38382465e-02 8.79460394e-01 9.13091280e-05 9.98938739e-01
 1.40759908e-03 9.67485368e-01 9.99933004e-01 9.41781819e-01
 1.29606100e-02 2.05939978e-06 2.67713368e-01 1.02139274e-02
 1.05055332e-01 7.25218132e-02 9.96374905e-01 9.97739077e-01
 1.72016886e-03 4.68248822e-04 9.86014962e-01 9.99644279e-01
 1.04472609e-02 1.50793828e-02 3.92445223e-03 2.09228769e-01
 9.97213900e-01 7.78735936e-01 9.99870300e-01 2.53091156e-01
 9.99276459e-01 9.91684437e-01 9.91089582e-01 9.55766320e-01
 9.05095577e-01 7.30239823e-02 9.99748051e-01 1.73973255e-02
 5.35374129e-05 7.07788989e-02 8.04361165e-01 9.98181462e-01
 9.98420119e-01 9.99560893e-01 4.56416488e-01 9.99710739e-01
 9.91821289e-01 9.98278141e-01 9.95881796e-01 5.05759334e-03
 8.23512767e-03 6.95695400e-01 9.81174588e-01 9.94321331e-03
 9.41256523e-01 1.31366702e-04 9.99615192e-01 3.48940820e-01
 6.98327050e-02 9.29752644e-03 1.03405695e-02 1.37164623e-01
 9.88988578e-01 9.78249088e-02 7.36972690e-01 1.39714777e-01
 3.56870256e-02 2.51690410e-02 2.46759225e-03 1.13676833e-02
 2.48968117e-02 4.27179039e-03 9.96258855e-01 8.73430908e-01
 5.29593043e-03 4.26125567e-04 9.56357896e-01 9.25402164e-01
 9.74924505e-01 2.54659820e-02 9.99959826e-01 1.29928580e-02
 2.87065213e-03 9.92794573e-01 1.24042600e-01 3.29427272e-02
 7.33373978e-04 1.34464016e-03 1.59550656e-03 9.96481180e-01
 9.51515809e-02 8.18007538e-05 2.53714114e-01 1.54584908e-04
 9.90598023e-01 6.73709810e-02 5.67386568e-01 1.41078825e-04
 8.31276894e-01 1.78354952e-04 9.99216080e-01 4.32665664e-04
 4.56480205e-01 9.99945521e-01 1.06606409e-02 9.95040476e-01
 8.40431929e-01 9.98693049e-01 1.25556890e-05 9.90141332e-01
 1.21233845e-02 9.99997139e-01 9.99766529e-01 2.40838621e-02
 5.04001915e-01 3.66871953e-01 4.15631896e-03 9.96489823e-01
 9.65313733e-01]
INFO:root:sensitive_y_true: [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1.
 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.
 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0.
 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.
 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.
 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1.]
INFO:root:2024-03-29 23:35:26, Dev, Step : 4920, Loss : 0.94215, Acc : 0.799, Auc : 0.797, Sensitive_Loss : 0.22153, Sensitive_Acc : 17.064, Sensitive_Auc : 0.998, Mean auc: 0.797, Run Time : 95.47 sec
