Running on desktop25:
stdin: is not a tty
Activating chexpert environment...
4
Using the specified args:
Namespace(cfg_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/config/config_katkr.json', device_ids='0', logtofile=False, num_workers=2, pre_train=None, resume=0, save_path='/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2', verbose=True)
{
    "base_path": "/home/data_shares/purrlab/CheXpert/CheXpert-v1.0-small",
    "train_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_train.csv",
    "dev_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/preprocess/datasets/biased_pneumothorax_dataset_val.csv",
    "pred_csv": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/predictions/Pred_Biased_Sex_1_pos01.csv",
    "pred_model": "/home/katkr/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray/logdirs/logdir-30k-2/Best_Biased_Sex_1_pos011.ckpt",
    "backbone": "densenet121",
    "sensitive_attribute": "Sex",
    "lambda_val": -0.05,
    "num_heads": 2,
    "width": 512,
    "height": 512,
    "long_side": 512,
    "fix_ratio": true,
    "pixel_mean": 128.0,
    "pixel_std": 64.0,
    "use_pixel_std": true,
    "use_equalizeHist": true,
    "use_transforms_type": "Aug",
    "gaussian_blur": 3,
    "border_pad": "pixel_mean",
    "num_classes": [
        1
    ],
    "batch_weight": true,
    "batch_weight_sensitive": true,
    "enhance_index": [
        2,
        6
    ],
    "enhance_times": 1,
    "pos_weight": [
        1
    ],
    "sensitive_pos_weight": [
        1
    ],
    "train_batch_size": 32,
    "dev_batch_size": 32,
    "pretrained": true,
    "log_every": 10,
    "test_every": 100,
    "epoch": 10,
    "norm_type": "BatchNorm",
    "global_pool": "PCAM",
    "fc_bn": true,
    "attention_map": "FPA",
    "lse_gamma": 0.5,
    "fc_drop": 0,
    "optimizer": "Adam",
    "criterion": "BCE",
    "sensitive_criterion": "BCE",
    "lr": 0.0001,
    "lr_factor": 0.1,
    "lr_epochs": [
        2
    ],
    "momentum": 0.9,
    "weight_decay": 0.0,
    "best_target": "auc",
    "save_top_k": 3,
    "save_index": [
        0
    ]
}
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 256, 256]           9,408
       BatchNorm2d-2         [-1, 64, 256, 256]             128
              ReLU-3         [-1, 64, 256, 256]               0
         MaxPool2d-4         [-1, 64, 128, 128]               0
       BatchNorm2d-5         [-1, 64, 128, 128]             128
              ReLU-6         [-1, 64, 128, 128]               0
            Conv2d-7        [-1, 128, 128, 128]           8,192
       BatchNorm2d-8        [-1, 128, 128, 128]             256
              ReLU-9        [-1, 128, 128, 128]               0
           Conv2d-10         [-1, 32, 128, 128]          36,864
      BatchNorm2d-11         [-1, 96, 128, 128]             192
             ReLU-12         [-1, 96, 128, 128]               0
           Conv2d-13        [-1, 128, 128, 128]          12,288
      BatchNorm2d-14        [-1, 128, 128, 128]             256
             ReLU-15        [-1, 128, 128, 128]               0
           Conv2d-16         [-1, 32, 128, 128]          36,864
      BatchNorm2d-17        [-1, 128, 128, 128]             256
             ReLU-18        [-1, 128, 128, 128]               0
           Conv2d-19        [-1, 128, 128, 128]          16,384
      BatchNorm2d-20        [-1, 128, 128, 128]             256
             ReLU-21        [-1, 128, 128, 128]               0
           Conv2d-22         [-1, 32, 128, 128]          36,864
      BatchNorm2d-23        [-1, 160, 128, 128]             320
             ReLU-24        [-1, 160, 128, 128]               0
           Conv2d-25        [-1, 128, 128, 128]          20,480
      BatchNorm2d-26        [-1, 128, 128, 128]             256
             ReLU-27        [-1, 128, 128, 128]               0
           Conv2d-28         [-1, 32, 128, 128]          36,864
      BatchNorm2d-29        [-1, 192, 128, 128]             384
             ReLU-30        [-1, 192, 128, 128]               0
           Conv2d-31        [-1, 128, 128, 128]          24,576
      BatchNorm2d-32        [-1, 128, 128, 128]             256
             ReLU-33        [-1, 128, 128, 128]               0
           Conv2d-34         [-1, 32, 128, 128]          36,864
      BatchNorm2d-35        [-1, 224, 128, 128]             448
             ReLU-36        [-1, 224, 128, 128]               0
           Conv2d-37        [-1, 128, 128, 128]          28,672
      BatchNorm2d-38        [-1, 128, 128, 128]             256
             ReLU-39        [-1, 128, 128, 128]               0
           Conv2d-40         [-1, 32, 128, 128]          36,864
      BatchNorm2d-41        [-1, 256, 128, 128]             512
             ReLU-42        [-1, 256, 128, 128]               0
           Conv2d-43        [-1, 128, 128, 128]          32,768
        AvgPool2d-44          [-1, 128, 64, 64]               0
      BatchNorm2d-45          [-1, 128, 64, 64]             256
             ReLU-46          [-1, 128, 64, 64]               0
           Conv2d-47          [-1, 128, 64, 64]          16,384
      BatchNorm2d-48          [-1, 128, 64, 64]             256
             ReLU-49          [-1, 128, 64, 64]               0
           Conv2d-50           [-1, 32, 64, 64]          36,864
      BatchNorm2d-51          [-1, 160, 64, 64]             320
             ReLU-52          [-1, 160, 64, 64]               0
           Conv2d-53          [-1, 128, 64, 64]          20,480
      BatchNorm2d-54          [-1, 128, 64, 64]             256
             ReLU-55          [-1, 128, 64, 64]               0
           Conv2d-56           [-1, 32, 64, 64]          36,864
      BatchNorm2d-57          [-1, 192, 64, 64]             384
             ReLU-58          [-1, 192, 64, 64]               0
           Conv2d-59          [-1, 128, 64, 64]          24,576
      BatchNorm2d-60          [-1, 128, 64, 64]             256
             ReLU-61          [-1, 128, 64, 64]               0
           Conv2d-62           [-1, 32, 64, 64]          36,864
      BatchNorm2d-63          [-1, 224, 64, 64]             448
             ReLU-64          [-1, 224, 64, 64]               0
           Conv2d-65          [-1, 128, 64, 64]          28,672
      BatchNorm2d-66          [-1, 128, 64, 64]             256
             ReLU-67          [-1, 128, 64, 64]               0
           Conv2d-68           [-1, 32, 64, 64]          36,864
      BatchNorm2d-69          [-1, 256, 64, 64]             512
             ReLU-70          [-1, 256, 64, 64]               0
           Conv2d-71          [-1, 128, 64, 64]          32,768
      BatchNorm2d-72          [-1, 128, 64, 64]             256
             ReLU-73          [-1, 128, 64, 64]               0
           Conv2d-74           [-1, 32, 64, 64]          36,864
      BatchNorm2d-75          [-1, 288, 64, 64]             576
             ReLU-76          [-1, 288, 64, 64]               0
           Conv2d-77          [-1, 128, 64, 64]          36,864
      BatchNorm2d-78          [-1, 128, 64, 64]             256
             ReLU-79          [-1, 128, 64, 64]               0
           Conv2d-80           [-1, 32, 64, 64]          36,864
      BatchNorm2d-81          [-1, 320, 64, 64]             640
             ReLU-82          [-1, 320, 64, 64]               0
           Conv2d-83          [-1, 128, 64, 64]          40,960
      BatchNorm2d-84          [-1, 128, 64, 64]             256
             ReLU-85          [-1, 128, 64, 64]               0
           Conv2d-86           [-1, 32, 64, 64]          36,864
      BatchNorm2d-87          [-1, 352, 64, 64]             704
             ReLU-88          [-1, 352, 64, 64]               0
           Conv2d-89          [-1, 128, 64, 64]          45,056
      BatchNorm2d-90          [-1, 128, 64, 64]             256
             ReLU-91          [-1, 128, 64, 64]               0
           Conv2d-92           [-1, 32, 64, 64]          36,864
      BatchNorm2d-93          [-1, 384, 64, 64]             768
             ReLU-94          [-1, 384, 64, 64]               0
           Conv2d-95          [-1, 128, 64, 64]          49,152
      BatchNorm2d-96          [-1, 128, 64, 64]             256
             ReLU-97          [-1, 128, 64, 64]               0
           Conv2d-98           [-1, 32, 64, 64]          36,864
      BatchNorm2d-99          [-1, 416, 64, 64]             832
            ReLU-100          [-1, 416, 64, 64]               0
          Conv2d-101          [-1, 128, 64, 64]          53,248
     BatchNorm2d-102          [-1, 128, 64, 64]             256
            ReLU-103          [-1, 128, 64, 64]               0
          Conv2d-104           [-1, 32, 64, 64]          36,864
     BatchNorm2d-105          [-1, 448, 64, 64]             896
            ReLU-106          [-1, 448, 64, 64]               0
          Conv2d-107          [-1, 128, 64, 64]          57,344
     BatchNorm2d-108          [-1, 128, 64, 64]             256
            ReLU-109          [-1, 128, 64, 64]               0
          Conv2d-110           [-1, 32, 64, 64]          36,864
     BatchNorm2d-111          [-1, 480, 64, 64]             960
            ReLU-112          [-1, 480, 64, 64]               0
          Conv2d-113          [-1, 128, 64, 64]          61,440
     BatchNorm2d-114          [-1, 128, 64, 64]             256
            ReLU-115          [-1, 128, 64, 64]               0
          Conv2d-116           [-1, 32, 64, 64]          36,864
     BatchNorm2d-117          [-1, 512, 64, 64]           1,024
            ReLU-118          [-1, 512, 64, 64]               0
          Conv2d-119          [-1, 256, 64, 64]         131,072
       AvgPool2d-120          [-1, 256, 32, 32]               0
     BatchNorm2d-121          [-1, 256, 32, 32]             512
            ReLU-122          [-1, 256, 32, 32]               0
          Conv2d-123          [-1, 128, 32, 32]          32,768
     BatchNorm2d-124          [-1, 128, 32, 32]             256
            ReLU-125          [-1, 128, 32, 32]               0
          Conv2d-126           [-1, 32, 32, 32]          36,864
     BatchNorm2d-127          [-1, 288, 32, 32]             576
            ReLU-128          [-1, 288, 32, 32]               0
          Conv2d-129          [-1, 128, 32, 32]          36,864
     BatchNorm2d-130          [-1, 128, 32, 32]             256
            ReLU-131          [-1, 128, 32, 32]               0
          Conv2d-132           [-1, 32, 32, 32]          36,864
     BatchNorm2d-133          [-1, 320, 32, 32]             640
            ReLU-134          [-1, 320, 32, 32]               0
          Conv2d-135          [-1, 128, 32, 32]          40,960
     BatchNorm2d-136          [-1, 128, 32, 32]             256
            ReLU-137          [-1, 128, 32, 32]               0
          Conv2d-138           [-1, 32, 32, 32]          36,864
     BatchNorm2d-139          [-1, 352, 32, 32]             704
            ReLU-140          [-1, 352, 32, 32]               0
          Conv2d-141          [-1, 128, 32, 32]          45,056
     BatchNorm2d-142          [-1, 128, 32, 32]             256
            ReLU-143          [-1, 128, 32, 32]               0
          Conv2d-144           [-1, 32, 32, 32]          36,864
     BatchNorm2d-145          [-1, 384, 32, 32]             768
            ReLU-146          [-1, 384, 32, 32]               0
          Conv2d-147          [-1, 128, 32, 32]          49,152
     BatchNorm2d-148          [-1, 128, 32, 32]             256
            ReLU-149          [-1, 128, 32, 32]               0
          Conv2d-150           [-1, 32, 32, 32]          36,864
     BatchNorm2d-151          [-1, 416, 32, 32]             832
            ReLU-152          [-1, 416, 32, 32]               0
          Conv2d-153          [-1, 128, 32, 32]          53,248
     BatchNorm2d-154          [-1, 128, 32, 32]             256
            ReLU-155          [-1, 128, 32, 32]               0
          Conv2d-156           [-1, 32, 32, 32]          36,864
     BatchNorm2d-157          [-1, 448, 32, 32]             896
            ReLU-158          [-1, 448, 32, 32]               0
          Conv2d-159          [-1, 128, 32, 32]          57,344
     BatchNorm2d-160          [-1, 128, 32, 32]             256
            ReLU-161          [-1, 128, 32, 32]               0
          Conv2d-162           [-1, 32, 32, 32]          36,864
     BatchNorm2d-163          [-1, 480, 32, 32]             960
            ReLU-164          [-1, 480, 32, 32]               0
          Conv2d-165          [-1, 128, 32, 32]          61,440
     BatchNorm2d-166          [-1, 128, 32, 32]             256
            ReLU-167          [-1, 128, 32, 32]               0
          Conv2d-168           [-1, 32, 32, 32]          36,864
     BatchNorm2d-169          [-1, 512, 32, 32]           1,024
            ReLU-170          [-1, 512, 32, 32]               0
          Conv2d-171          [-1, 128, 32, 32]          65,536
     BatchNorm2d-172          [-1, 128, 32, 32]             256
            ReLU-173          [-1, 128, 32, 32]               0
          Conv2d-174           [-1, 32, 32, 32]          36,864
     BatchNorm2d-175          [-1, 544, 32, 32]           1,088
            ReLU-176          [-1, 544, 32, 32]               0
          Conv2d-177          [-1, 128, 32, 32]          69,632
     BatchNorm2d-178          [-1, 128, 32, 32]             256
            ReLU-179          [-1, 128, 32, 32]               0
          Conv2d-180           [-1, 32, 32, 32]          36,864
     BatchNorm2d-181          [-1, 576, 32, 32]           1,152
            ReLU-182          [-1, 576, 32, 32]               0
          Conv2d-183          [-1, 128, 32, 32]          73,728
     BatchNorm2d-184          [-1, 128, 32, 32]             256
            ReLU-185          [-1, 128, 32, 32]               0
          Conv2d-186           [-1, 32, 32, 32]          36,864
     BatchNorm2d-187          [-1, 608, 32, 32]           1,216
            ReLU-188          [-1, 608, 32, 32]               0
          Conv2d-189          [-1, 128, 32, 32]          77,824
     BatchNorm2d-190          [-1, 128, 32, 32]             256
            ReLU-191          [-1, 128, 32, 32]               0
          Conv2d-192           [-1, 32, 32, 32]          36,864
     BatchNorm2d-193          [-1, 640, 32, 32]           1,280
            ReLU-194          [-1, 640, 32, 32]               0
          Conv2d-195          [-1, 128, 32, 32]          81,920
     BatchNorm2d-196          [-1, 128, 32, 32]             256
            ReLU-197          [-1, 128, 32, 32]               0
          Conv2d-198           [-1, 32, 32, 32]          36,864
     BatchNorm2d-199          [-1, 672, 32, 32]           1,344
            ReLU-200          [-1, 672, 32, 32]               0
          Conv2d-201          [-1, 128, 32, 32]          86,016
     BatchNorm2d-202          [-1, 128, 32, 32]             256
            ReLU-203          [-1, 128, 32, 32]               0
          Conv2d-204           [-1, 32, 32, 32]          36,864
     BatchNorm2d-205          [-1, 704, 32, 32]           1,408
            ReLU-206          [-1, 704, 32, 32]               0
          Conv2d-207          [-1, 128, 32, 32]          90,112
     BatchNorm2d-208          [-1, 128, 32, 32]             256
            ReLU-209          [-1, 128, 32, 32]               0
          Conv2d-210           [-1, 32, 32, 32]          36,864
     BatchNorm2d-211          [-1, 736, 32, 32]           1,472
            ReLU-212          [-1, 736, 32, 32]               0
          Conv2d-213          [-1, 128, 32, 32]          94,208
     BatchNorm2d-214          [-1, 128, 32, 32]             256
            ReLU-215          [-1, 128, 32, 32]               0
          Conv2d-216           [-1, 32, 32, 32]          36,864
     BatchNorm2d-217          [-1, 768, 32, 32]           1,536
            ReLU-218          [-1, 768, 32, 32]               0
          Conv2d-219          [-1, 128, 32, 32]          98,304
     BatchNorm2d-220          [-1, 128, 32, 32]             256
            ReLU-221          [-1, 128, 32, 32]               0
          Conv2d-222           [-1, 32, 32, 32]          36,864
     BatchNorm2d-223          [-1, 800, 32, 32]           1,600
            ReLU-224          [-1, 800, 32, 32]               0
          Conv2d-225          [-1, 128, 32, 32]         102,400
     BatchNorm2d-226          [-1, 128, 32, 32]             256
            ReLU-227          [-1, 128, 32, 32]               0
          Conv2d-228           [-1, 32, 32, 32]          36,864
     BatchNorm2d-229          [-1, 832, 32, 32]           1,664
            ReLU-230          [-1, 832, 32, 32]               0
          Conv2d-231          [-1, 128, 32, 32]         106,496
     BatchNorm2d-232          [-1, 128, 32, 32]             256
            ReLU-233          [-1, 128, 32, 32]               0
          Conv2d-234           [-1, 32, 32, 32]          36,864
     BatchNorm2d-235          [-1, 864, 32, 32]           1,728
            ReLU-236          [-1, 864, 32, 32]               0
          Conv2d-237          [-1, 128, 32, 32]         110,592
     BatchNorm2d-238          [-1, 128, 32, 32]             256
            ReLU-239          [-1, 128, 32, 32]               0
          Conv2d-240           [-1, 32, 32, 32]          36,864
     BatchNorm2d-241          [-1, 896, 32, 32]           1,792
            ReLU-242          [-1, 896, 32, 32]               0
          Conv2d-243          [-1, 128, 32, 32]         114,688
     BatchNorm2d-244          [-1, 128, 32, 32]             256
            ReLU-245          [-1, 128, 32, 32]               0
          Conv2d-246           [-1, 32, 32, 32]          36,864
     BatchNorm2d-247          [-1, 928, 32, 32]           1,856
            ReLU-248          [-1, 928, 32, 32]               0
          Conv2d-249          [-1, 128, 32, 32]         118,784
     BatchNorm2d-250          [-1, 128, 32, 32]             256
            ReLU-251          [-1, 128, 32, 32]               0
          Conv2d-252           [-1, 32, 32, 32]          36,864
     BatchNorm2d-253          [-1, 960, 32, 32]           1,920
            ReLU-254          [-1, 960, 32, 32]               0
          Conv2d-255          [-1, 128, 32, 32]         122,880
     BatchNorm2d-256          [-1, 128, 32, 32]             256
            ReLU-257          [-1, 128, 32, 32]               0
          Conv2d-258           [-1, 32, 32, 32]          36,864
     BatchNorm2d-259          [-1, 992, 32, 32]           1,984
            ReLU-260          [-1, 992, 32, 32]               0
          Conv2d-261          [-1, 128, 32, 32]         126,976
     BatchNorm2d-262          [-1, 128, 32, 32]             256
            ReLU-263          [-1, 128, 32, 32]               0
          Conv2d-264           [-1, 32, 32, 32]          36,864
     BatchNorm2d-265         [-1, 1024, 32, 32]           2,048
            ReLU-266         [-1, 1024, 32, 32]               0
          Conv2d-267          [-1, 512, 32, 32]         524,288
       AvgPool2d-268          [-1, 512, 16, 16]               0
     BatchNorm2d-269          [-1, 512, 16, 16]           1,024
            ReLU-270          [-1, 512, 16, 16]               0
          Conv2d-271          [-1, 128, 16, 16]          65,536
     BatchNorm2d-272          [-1, 128, 16, 16]             256
            ReLU-273          [-1, 128, 16, 16]               0
          Conv2d-274           [-1, 32, 16, 16]          36,864
     BatchNorm2d-275          [-1, 544, 16, 16]           1,088
            ReLU-276          [-1, 544, 16, 16]               0
          Conv2d-277          [-1, 128, 16, 16]          69,632
     BatchNorm2d-278          [-1, 128, 16, 16]             256
            ReLU-279          [-1, 128, 16, 16]               0
          Conv2d-280           [-1, 32, 16, 16]          36,864
     BatchNorm2d-281          [-1, 576, 16, 16]           1,152
            ReLU-282          [-1, 576, 16, 16]               0
          Conv2d-283          [-1, 128, 16, 16]          73,728
     BatchNorm2d-284          [-1, 128, 16, 16]             256
            ReLU-285          [-1, 128, 16, 16]               0
          Conv2d-286           [-1, 32, 16, 16]          36,864
     BatchNorm2d-287          [-1, 608, 16, 16]           1,216
            ReLU-288          [-1, 608, 16, 16]               0
          Conv2d-289          [-1, 128, 16, 16]          77,824
     BatchNorm2d-290          [-1, 128, 16, 16]             256
            ReLU-291          [-1, 128, 16, 16]               0
          Conv2d-292           [-1, 32, 16, 16]          36,864
     BatchNorm2d-293          [-1, 640, 16, 16]           1,280
            ReLU-294          [-1, 640, 16, 16]               0
          Conv2d-295          [-1, 128, 16, 16]          81,920
     BatchNorm2d-296          [-1, 128, 16, 16]             256
            ReLU-297          [-1, 128, 16, 16]               0
          Conv2d-298           [-1, 32, 16, 16]          36,864
     BatchNorm2d-299          [-1, 672, 16, 16]           1,344
            ReLU-300          [-1, 672, 16, 16]               0
          Conv2d-301          [-1, 128, 16, 16]          86,016
     BatchNorm2d-302          [-1, 128, 16, 16]             256
            ReLU-303          [-1, 128, 16, 16]               0
          Conv2d-304           [-1, 32, 16, 16]          36,864
     BatchNorm2d-305          [-1, 704, 16, 16]           1,408
            ReLU-306          [-1, 704, 16, 16]               0
          Conv2d-307          [-1, 128, 16, 16]          90,112
     BatchNorm2d-308          [-1, 128, 16, 16]             256
            ReLU-309          [-1, 128, 16, 16]               0
          Conv2d-310           [-1, 32, 16, 16]          36,864
     BatchNorm2d-311          [-1, 736, 16, 16]           1,472
            ReLU-312          [-1, 736, 16, 16]               0
          Conv2d-313          [-1, 128, 16, 16]          94,208
     BatchNorm2d-314          [-1, 128, 16, 16]             256
            ReLU-315          [-1, 128, 16, 16]               0
          Conv2d-316           [-1, 32, 16, 16]          36,864
     BatchNorm2d-317          [-1, 768, 16, 16]           1,536
            ReLU-318          [-1, 768, 16, 16]               0
          Conv2d-319          [-1, 128, 16, 16]          98,304
     BatchNorm2d-320          [-1, 128, 16, 16]             256
            ReLU-321          [-1, 128, 16, 16]               0
          Conv2d-322           [-1, 32, 16, 16]          36,864
     BatchNorm2d-323          [-1, 800, 16, 16]           1,600
            ReLU-324          [-1, 800, 16, 16]               0
          Conv2d-325          [-1, 128, 16, 16]         102,400
     BatchNorm2d-326          [-1, 128, 16, 16]             256
            ReLU-327          [-1, 128, 16, 16]               0
          Conv2d-328           [-1, 32, 16, 16]          36,864
     BatchNorm2d-329          [-1, 832, 16, 16]           1,664
            ReLU-330          [-1, 832, 16, 16]               0
          Conv2d-331          [-1, 128, 16, 16]         106,496
     BatchNorm2d-332          [-1, 128, 16, 16]             256
            ReLU-333          [-1, 128, 16, 16]               0
          Conv2d-334           [-1, 32, 16, 16]          36,864
     BatchNorm2d-335          [-1, 864, 16, 16]           1,728
            ReLU-336          [-1, 864, 16, 16]               0
          Conv2d-337          [-1, 128, 16, 16]         110,592
     BatchNorm2d-338          [-1, 128, 16, 16]             256
            ReLU-339          [-1, 128, 16, 16]               0
          Conv2d-340           [-1, 32, 16, 16]          36,864
     BatchNorm2d-341          [-1, 896, 16, 16]           1,792
            ReLU-342          [-1, 896, 16, 16]               0
          Conv2d-343          [-1, 128, 16, 16]         114,688
     BatchNorm2d-344          [-1, 128, 16, 16]             256
            ReLU-345          [-1, 128, 16, 16]               0
          Conv2d-346           [-1, 32, 16, 16]          36,864
     BatchNorm2d-347          [-1, 928, 16, 16]           1,856
            ReLU-348          [-1, 928, 16, 16]               0
          Conv2d-349          [-1, 128, 16, 16]         118,784
     BatchNorm2d-350          [-1, 128, 16, 16]             256
            ReLU-351          [-1, 128, 16, 16]               0
          Conv2d-352           [-1, 32, 16, 16]          36,864
     BatchNorm2d-353          [-1, 960, 16, 16]           1,920
            ReLU-354          [-1, 960, 16, 16]               0
          Conv2d-355          [-1, 128, 16, 16]         122,880
     BatchNorm2d-356          [-1, 128, 16, 16]             256
            ReLU-357          [-1, 128, 16, 16]               0
          Conv2d-358           [-1, 32, 16, 16]          36,864
     BatchNorm2d-359          [-1, 992, 16, 16]           1,984
            ReLU-360          [-1, 992, 16, 16]               0
          Conv2d-361          [-1, 128, 16, 16]         126,976
     BatchNorm2d-362          [-1, 128, 16, 16]             256
            ReLU-363          [-1, 128, 16, 16]               0
          Conv2d-364           [-1, 32, 16, 16]          36,864
     BatchNorm2d-365         [-1, 1024, 16, 16]           2,048
        DenseNet-366         [-1, 1024, 16, 16]               0
AdaptiveAvgPool2d-367           [-1, 1024, 1, 1]               0
          Conv2d-368           [-1, 1024, 1, 1]       1,049,600
     BatchNorm2d-369           [-1, 1024, 1, 1]           2,048
            ReLU-370           [-1, 1024, 1, 1]               0
  Conv2dNormRelu-371           [-1, 1024, 1, 1]               0
          Conv2d-372         [-1, 1024, 16, 16]       1,049,600
     BatchNorm2d-373         [-1, 1024, 16, 16]           2,048
            ReLU-374         [-1, 1024, 16, 16]               0
  Conv2dNormRelu-375         [-1, 1024, 16, 16]               0
          Conv2d-376              [-1, 1, 8, 8]          50,177
     BatchNorm2d-377              [-1, 1, 8, 8]               2
            ReLU-378              [-1, 1, 8, 8]               0
  Conv2dNormRelu-379              [-1, 1, 8, 8]               0
          Conv2d-380              [-1, 1, 4, 4]              26
     BatchNorm2d-381              [-1, 1, 4, 4]               2
            ReLU-382              [-1, 1, 4, 4]               0
  Conv2dNormRelu-383              [-1, 1, 4, 4]               0
          Conv2d-384              [-1, 1, 2, 2]              10
     BatchNorm2d-385              [-1, 1, 2, 2]               2
            ReLU-386              [-1, 1, 2, 2]               0
  Conv2dNormRelu-387              [-1, 1, 2, 2]               0
          Conv2d-388              [-1, 1, 2, 2]              10
     BatchNorm2d-389              [-1, 1, 2, 2]               2
            ReLU-390              [-1, 1, 2, 2]               0
  Conv2dNormRelu-391              [-1, 1, 2, 2]               0
          Conv2d-392              [-1, 1, 4, 4]              26
     BatchNorm2d-393              [-1, 1, 4, 4]               2
            ReLU-394              [-1, 1, 4, 4]               0
  Conv2dNormRelu-395              [-1, 1, 4, 4]               0
          Conv2d-396              [-1, 1, 8, 8]              50
     BatchNorm2d-397              [-1, 1, 8, 8]               2
            ReLU-398              [-1, 1, 8, 8]               0
  Conv2dNormRelu-399              [-1, 1, 8, 8]               0
       FPAModule-400         [-1, 1024, 16, 16]               0
    AttentionMap-401         [-1, 1024, 16, 16]               0
          Conv2d-402            [-1, 1, 16, 16]           1,025
        PcamPool-403           [-1, 1024, 1, 1]               0
      GlobalPool-404           [-1, 1024, 1, 1]               0
     BatchNorm2d-405           [-1, 1024, 1, 1]           2,048
          Conv2d-406              [-1, 1, 1, 1]           1,025
        PcamPool-407           [-1, 1024, 1, 1]               0
      GlobalPool-408           [-1, 1024, 1, 1]               0
          Linear-409                    [-1, 1]           1,025
================================================================
Total params: 9,112,586
Trainable params: 9,112,586
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.00
Forward/backward pass size (MB): 1551.09
Params size (MB): 34.76
Estimated Total Size (MB): 1588.85
----------------------------------------------------------------
INFO:root:2024-04-26 09:26:34, Train, Epoch : 1, Step : 10, Loss : 0.71728, Acc : 0.559, Sensitive_Loss : 0.73518, Sensitive_Acc : 17.000, Run Time : 13.19 sec
INFO:root:2024-04-26 09:26:45, Train, Epoch : 1, Step : 20, Loss : 0.67095, Acc : 0.637, Sensitive_Loss : 0.63041, Sensitive_Acc : 14.000, Run Time : 11.19 sec
INFO:root:2024-04-26 09:26:57, Train, Epoch : 1, Step : 30, Loss : 0.61267, Acc : 0.672, Sensitive_Loss : 0.73786, Sensitive_Acc : 15.900, Run Time : 11.96 sec
INFO:root:2024-04-26 09:27:09, Train, Epoch : 1, Step : 40, Loss : 0.58789, Acc : 0.703, Sensitive_Loss : 0.81100, Sensitive_Acc : 16.300, Run Time : 11.74 sec
INFO:root:2024-04-26 09:27:21, Train, Epoch : 1, Step : 50, Loss : 0.52144, Acc : 0.731, Sensitive_Loss : 0.88280, Sensitive_Acc : 16.500, Run Time : 12.29 sec
INFO:root:2024-04-26 09:27:32, Train, Epoch : 1, Step : 60, Loss : 0.59218, Acc : 0.719, Sensitive_Loss : 0.92981, Sensitive_Acc : 16.100, Run Time : 10.66 sec
INFO:root:2024-04-26 09:27:43, Train, Epoch : 1, Step : 70, Loss : 0.59594, Acc : 0.728, Sensitive_Loss : 0.90379, Sensitive_Acc : 15.200, Run Time : 11.50 sec
INFO:root:2024-04-26 09:28:00, Train, Epoch : 1, Step : 80, Loss : 0.46178, Acc : 0.778, Sensitive_Loss : 1.18834, Sensitive_Acc : 16.400, Run Time : 16.58 sec
INFO:root:2024-04-26 09:28:12, Train, Epoch : 1, Step : 90, Loss : 0.56288, Acc : 0.766, Sensitive_Loss : 1.15338, Sensitive_Acc : 16.200, Run Time : 12.33 sec
INFO:root:2024-04-26 09:28:24, Train, Epoch : 1, Step : 100, Loss : 0.58922, Acc : 0.713, Sensitive_Loss : 1.19414, Sensitive_Acc : 15.700, Run Time : 11.76 sec
INFO:root:2024-04-26 09:31:05, Dev, Step : 100, Loss : 0.53561, Acc : 0.752, Auc : 0.835, Sensitive_Loss : 1.48110, Sensitive_Acc : 15.629, Sensitive_Auc : 0.101, Mean auc: 0.835, Run Time : 160.51 sec
INFO:root:2024-04-26 09:31:05, Best, Step : 100, Loss : 0.53561, Acc : 0.752, Auc : 0.835, Sensitive_Loss : 1.48110, Sensitive_Acc : 15.629, Sensitive_Auc : 0.101, Best Auc : 0.835
INFO:root:2024-04-26 09:31:14, Train, Epoch : 1, Step : 110, Loss : 0.52623, Acc : 0.741, Sensitive_Loss : 1.51840, Sensitive_Acc : 14.500, Run Time : 169.61 sec
INFO:root:2024-04-26 09:31:26, Train, Epoch : 1, Step : 120, Loss : 0.53380, Acc : 0.744, Sensitive_Loss : 1.62650, Sensitive_Acc : 16.500, Run Time : 11.91 sec
INFO:root:2024-04-26 09:31:38, Train, Epoch : 1, Step : 130, Loss : 0.53966, Acc : 0.741, Sensitive_Loss : 1.48076, Sensitive_Acc : 16.800, Run Time : 11.98 sec
INFO:root:2024-04-26 09:31:50, Train, Epoch : 1, Step : 140, Loss : 0.54767, Acc : 0.681, Sensitive_Loss : 1.57843, Sensitive_Acc : 16.800, Run Time : 12.12 sec
INFO:root:2024-04-26 09:32:00, Train, Epoch : 1, Step : 150, Loss : 0.61278, Acc : 0.697, Sensitive_Loss : 1.89916, Sensitive_Acc : 15.900, Run Time : 10.72 sec
INFO:root:2024-04-26 09:32:13, Train, Epoch : 1, Step : 160, Loss : 0.57516, Acc : 0.731, Sensitive_Loss : 1.83617, Sensitive_Acc : 15.000, Run Time : 12.28 sec
INFO:root:2024-04-26 09:32:25, Train, Epoch : 1, Step : 170, Loss : 0.52755, Acc : 0.791, Sensitive_Loss : 1.36728, Sensitive_Acc : 17.100, Run Time : 11.99 sec
INFO:root:2024-04-26 09:32:37, Train, Epoch : 1, Step : 180, Loss : 0.50394, Acc : 0.781, Sensitive_Loss : 1.18889, Sensitive_Acc : 14.400, Run Time : 11.82 sec
INFO:root:2024-04-26 09:32:48, Train, Epoch : 1, Step : 190, Loss : 0.65814, Acc : 0.728, Sensitive_Loss : 1.12577, Sensitive_Acc : 16.900, Run Time : 11.52 sec
INFO:root:2024-04-26 09:33:00, Train, Epoch : 1, Step : 200, Loss : 0.59047, Acc : 0.713, Sensitive_Loss : 0.99428, Sensitive_Acc : 15.500, Run Time : 12.10 sec
INFO:root:2024-04-26 09:35:36, Dev, Step : 200, Loss : 0.70182, Acc : 0.634, Auc : 0.692, Sensitive_Loss : 1.19146, Sensitive_Acc : 14.743, Sensitive_Auc : 0.078, Mean auc: 0.692, Run Time : 155.55 sec
INFO:root:2024-04-26 09:35:44, Train, Epoch : 1, Step : 210, Loss : 0.62680, Acc : 0.709, Sensitive_Loss : 1.06732, Sensitive_Acc : 15.300, Run Time : 164.02 sec
INFO:root:2024-04-26 09:35:56, Train, Epoch : 1, Step : 220, Loss : 0.52878, Acc : 0.738, Sensitive_Loss : 0.80860, Sensitive_Acc : 15.100, Run Time : 11.61 sec
INFO:root:2024-04-26 09:36:07, Train, Epoch : 1, Step : 230, Loss : 0.58219, Acc : 0.728, Sensitive_Loss : 0.73363, Sensitive_Acc : 14.700, Run Time : 11.68 sec
INFO:root:2024-04-26 09:36:19, Train, Epoch : 1, Step : 240, Loss : 0.52898, Acc : 0.741, Sensitive_Loss : 0.76748, Sensitive_Acc : 17.100, Run Time : 11.52 sec
INFO:root:2024-04-26 09:36:31, Train, Epoch : 1, Step : 250, Loss : 0.47491, Acc : 0.762, Sensitive_Loss : 0.70552, Sensitive_Acc : 16.000, Run Time : 11.56 sec
INFO:root:2024-04-26 09:36:44, Train, Epoch : 1, Step : 260, Loss : 0.61113, Acc : 0.741, Sensitive_Loss : 0.51722, Sensitive_Acc : 14.700, Run Time : 13.10 sec
INFO:root:2024-04-26 09:36:55, Train, Epoch : 1, Step : 270, Loss : 0.45808, Acc : 0.784, Sensitive_Loss : 0.46397, Sensitive_Acc : 16.300, Run Time : 11.49 sec
INFO:root:2024-04-26 09:37:07, Train, Epoch : 1, Step : 280, Loss : 0.46768, Acc : 0.769, Sensitive_Loss : 0.59602, Sensitive_Acc : 15.100, Run Time : 11.39 sec
INFO:root:2024-04-26 09:37:19, Train, Epoch : 1, Step : 290, Loss : 0.49835, Acc : 0.753, Sensitive_Loss : 0.39642, Sensitive_Acc : 15.100, Run Time : 11.98 sec
INFO:root:2024-04-26 09:37:30, Train, Epoch : 1, Step : 300, Loss : 0.49116, Acc : 0.800, Sensitive_Loss : 0.37312, Sensitive_Acc : 14.800, Run Time : 11.71 sec
INFO:root:2024-04-26 09:40:05, Dev, Step : 300, Loss : 0.51577, Acc : 0.776, Auc : 0.841, Sensitive_Loss : 0.53947, Sensitive_Acc : 15.579, Sensitive_Auc : 0.724, Mean auc: 0.841, Run Time : 154.50 sec
INFO:root:2024-04-26 09:40:05, Best, Step : 300, Loss : 0.51577, Acc : 0.776, Auc : 0.841, Sensitive_Loss : 0.53947, Sensitive_Acc : 15.579, Sensitive_Auc : 0.724, Best Auc : 0.841
INFO:root:2024-04-26 09:40:13, Train, Epoch : 1, Step : 310, Loss : 0.57136, Acc : 0.741, Sensitive_Loss : 0.38679, Sensitive_Acc : 15.500, Run Time : 163.15 sec
INFO:root:2024-04-26 09:40:26, Train, Epoch : 1, Step : 320, Loss : 0.48753, Acc : 0.766, Sensitive_Loss : 0.38067, Sensitive_Acc : 16.100, Run Time : 12.75 sec
INFO:root:2024-04-26 09:40:38, Train, Epoch : 1, Step : 330, Loss : 0.51182, Acc : 0.762, Sensitive_Loss : 0.34190, Sensitive_Acc : 15.000, Run Time : 11.97 sec
INFO:root:2024-04-26 09:40:50, Train, Epoch : 1, Step : 340, Loss : 0.40661, Acc : 0.797, Sensitive_Loss : 0.30970, Sensitive_Acc : 17.200, Run Time : 11.86 sec
INFO:root:2024-04-26 09:41:02, Train, Epoch : 1, Step : 350, Loss : 0.48071, Acc : 0.781, Sensitive_Loss : 0.33285, Sensitive_Acc : 17.000, Run Time : 12.33 sec
INFO:root:2024-04-26 09:41:14, Train, Epoch : 1, Step : 360, Loss : 0.44262, Acc : 0.781, Sensitive_Loss : 0.32871, Sensitive_Acc : 16.000, Run Time : 11.90 sec
INFO:root:2024-04-26 09:41:25, Train, Epoch : 1, Step : 370, Loss : 0.44656, Acc : 0.784, Sensitive_Loss : 0.37796, Sensitive_Acc : 16.500, Run Time : 11.01 sec
INFO:root:2024-04-26 09:41:37, Train, Epoch : 1, Step : 380, Loss : 0.46144, Acc : 0.787, Sensitive_Loss : 0.25759, Sensitive_Acc : 17.100, Run Time : 11.98 sec
INFO:root:2024-04-26 09:41:50, Train, Epoch : 1, Step : 390, Loss : 0.45811, Acc : 0.803, Sensitive_Loss : 0.26161, Sensitive_Acc : 17.600, Run Time : 12.38 sec
INFO:root:2024-04-26 09:42:02, Train, Epoch : 1, Step : 400, Loss : 0.49473, Acc : 0.797, Sensitive_Loss : 0.24616, Sensitive_Acc : 14.800, Run Time : 12.31 sec
INFO:root:2024-04-26 09:44:38, Dev, Step : 400, Loss : 0.57689, Acc : 0.745, Auc : 0.860, Sensitive_Loss : 0.30498, Sensitive_Acc : 16.864, Sensitive_Auc : 0.959, Mean auc: 0.860, Run Time : 155.64 sec
INFO:root:2024-04-26 09:44:38, Best, Step : 400, Loss : 0.57689, Acc : 0.745, Auc : 0.860, Sensitive_Loss : 0.30498, Sensitive_Acc : 16.864, Sensitive_Auc : 0.959, Best Auc : 0.860
INFO:root:2024-04-26 09:44:47, Train, Epoch : 1, Step : 410, Loss : 0.54866, Acc : 0.778, Sensitive_Loss : 0.25496, Sensitive_Acc : 15.300, Run Time : 165.00 sec
INFO:root:2024-04-26 09:44:59, Train, Epoch : 1, Step : 420, Loss : 0.42021, Acc : 0.809, Sensitive_Loss : 0.22927, Sensitive_Acc : 16.700, Run Time : 12.18 sec
INFO:root:2024-04-26 09:45:11, Train, Epoch : 1, Step : 430, Loss : 0.41764, Acc : 0.819, Sensitive_Loss : 0.27370, Sensitive_Acc : 17.200, Run Time : 12.34 sec
INFO:root:2024-04-26 09:45:24, Train, Epoch : 1, Step : 440, Loss : 0.52152, Acc : 0.772, Sensitive_Loss : 0.21692, Sensitive_Acc : 16.500, Run Time : 12.65 sec
INFO:root:2024-04-26 09:45:36, Train, Epoch : 1, Step : 450, Loss : 0.54649, Acc : 0.753, Sensitive_Loss : 0.28455, Sensitive_Acc : 17.200, Run Time : 11.77 sec
INFO:root:2024-04-26 09:45:48, Train, Epoch : 1, Step : 460, Loss : 0.46603, Acc : 0.778, Sensitive_Loss : 0.20110, Sensitive_Acc : 16.800, Run Time : 12.54 sec
INFO:root:2024-04-26 09:46:01, Train, Epoch : 1, Step : 470, Loss : 0.51798, Acc : 0.766, Sensitive_Loss : 0.23696, Sensitive_Acc : 17.000, Run Time : 12.14 sec
INFO:root:2024-04-26 09:46:13, Train, Epoch : 1, Step : 480, Loss : 0.41692, Acc : 0.841, Sensitive_Loss : 0.20621, Sensitive_Acc : 16.700, Run Time : 12.13 sec
INFO:root:2024-04-26 09:46:25, Train, Epoch : 1, Step : 490, Loss : 0.46490, Acc : 0.819, Sensitive_Loss : 0.23794, Sensitive_Acc : 16.600, Run Time : 12.56 sec
INFO:root:2024-04-26 09:46:37, Train, Epoch : 1, Step : 500, Loss : 0.47017, Acc : 0.803, Sensitive_Loss : 0.22174, Sensitive_Acc : 15.600, Run Time : 11.82 sec
INFO:root:2024-04-26 09:49:13, Dev, Step : 500, Loss : 0.52589, Acc : 0.762, Auc : 0.873, Sensitive_Loss : 0.30741, Sensitive_Acc : 16.464, Sensitive_Auc : 0.966, Mean auc: 0.873, Run Time : 155.50 sec
INFO:root:2024-04-26 09:49:13, Best, Step : 500, Loss : 0.52589, Acc : 0.762, Auc : 0.873, Sensitive_Loss : 0.30741, Sensitive_Acc : 16.464, Sensitive_Auc : 0.966, Best Auc : 0.873
INFO:root:2024-04-26 09:49:22, Train, Epoch : 1, Step : 510, Loss : 0.45116, Acc : 0.819, Sensitive_Loss : 0.18558, Sensitive_Acc : 15.500, Run Time : 164.66 sec
INFO:root:2024-04-26 09:49:34, Train, Epoch : 1, Step : 520, Loss : 0.50571, Acc : 0.778, Sensitive_Loss : 0.16241, Sensitive_Acc : 16.000, Run Time : 12.20 sec
INFO:root:2024-04-26 09:49:46, Train, Epoch : 1, Step : 530, Loss : 0.47819, Acc : 0.769, Sensitive_Loss : 0.20489, Sensitive_Acc : 16.800, Run Time : 12.07 sec
INFO:root:2024-04-26 09:49:57, Train, Epoch : 1, Step : 540, Loss : 0.45520, Acc : 0.756, Sensitive_Loss : 0.20733, Sensitive_Acc : 16.800, Run Time : 11.51 sec
INFO:root:2024-04-26 09:50:10, Train, Epoch : 1, Step : 550, Loss : 0.50960, Acc : 0.769, Sensitive_Loss : 0.15129, Sensitive_Acc : 15.000, Run Time : 12.33 sec
INFO:root:2024-04-26 09:50:21, Train, Epoch : 1, Step : 560, Loss : 0.48850, Acc : 0.787, Sensitive_Loss : 0.22814, Sensitive_Acc : 16.800, Run Time : 11.41 sec
INFO:root:2024-04-26 09:50:33, Train, Epoch : 1, Step : 570, Loss : 0.51170, Acc : 0.781, Sensitive_Loss : 0.16844, Sensitive_Acc : 16.600, Run Time : 11.64 sec
INFO:root:2024-04-26 09:50:45, Train, Epoch : 1, Step : 580, Loss : 0.52122, Acc : 0.781, Sensitive_Loss : 0.20131, Sensitive_Acc : 18.100, Run Time : 12.41 sec
INFO:root:2024-04-26 09:50:57, Train, Epoch : 1, Step : 590, Loss : 0.42701, Acc : 0.784, Sensitive_Loss : 0.19606, Sensitive_Acc : 16.400, Run Time : 12.03 sec
INFO:root:2024-04-26 09:51:09, Train, Epoch : 1, Step : 600, Loss : 0.48680, Acc : 0.787, Sensitive_Loss : 0.17446, Sensitive_Acc : 17.300, Run Time : 11.43 sec
INFO:root:2024-04-26 09:53:44, Dev, Step : 600, Loss : 0.46963, Acc : 0.796, Auc : 0.881, Sensitive_Loss : 0.17908, Sensitive_Acc : 16.750, Sensitive_Auc : 0.982, Mean auc: 0.881, Run Time : 154.98 sec
INFO:root:2024-04-26 09:53:44, Best, Step : 600, Loss : 0.46963, Acc : 0.796, Auc : 0.881, Sensitive_Loss : 0.17908, Sensitive_Acc : 16.750, Sensitive_Auc : 0.982, Best Auc : 0.881
INFO:root:2024-04-26 09:53:54, Train, Epoch : 1, Step : 610, Loss : 0.54699, Acc : 0.756, Sensitive_Loss : 0.20899, Sensitive_Acc : 17.300, Run Time : 165.22 sec
INFO:root:2024-04-26 09:54:06, Train, Epoch : 1, Step : 620, Loss : 0.47811, Acc : 0.775, Sensitive_Loss : 0.16918, Sensitive_Acc : 15.800, Run Time : 12.37 sec
INFO:root:2024-04-26 09:56:50
INFO:root:y_pred: [0.22918968 0.9011371  0.13176134 ... 0.8741792  0.01456969 0.7108182 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [8.80731225e-01 1.26292044e-03 1.02907121e-01 2.36796157e-04
 9.73120153e-01 9.97576164e-04 9.60227728e-01 8.63633633e-01
 5.92446215e-02 8.19831073e-01 9.80442703e-01 9.61089909e-01
 9.46346641e-01 8.32527816e-01 2.83040494e-01 7.72645831e-01
 9.72583532e-01 7.92049840e-02 5.87545514e-01 9.53197420e-01
 9.36362982e-01 4.01300907e-01 9.82375205e-01 8.98950338e-01
 8.59503865e-01 6.10859454e-01 7.10381707e-03 9.39756215e-01
 9.21019018e-01 8.92425776e-01 1.81230173e-01 5.80907583e-01
 2.25808501e-01 1.54922500e-01 4.34112340e-01 1.62749644e-03
 6.98138177e-01 2.16370970e-02 9.26654160e-01 9.28079307e-01
 1.68027371e-04 2.92873252e-02 9.71417308e-01 6.17404818e-04
 9.78821993e-01 9.50513959e-01 9.04932380e-01 9.73775327e-01
 2.69129872e-01 9.36828196e-01 9.63588774e-01 1.96321681e-01
 3.11614931e-01 3.13564041e-03 1.74494053e-03 1.38776541e-01
 4.02083874e-01 7.49144256e-01 3.82621214e-02 3.48050557e-02
 6.39223680e-02 3.48909110e-01 2.45466739e-01 7.74667144e-01
 4.58017290e-01 9.42534626e-01 1.60227623e-02 9.66708243e-01
 8.75814557e-01 8.26995492e-01 4.84512985e-01 4.95189548e-01
 2.19230782e-02 6.85997307e-01 9.63588618e-03 8.34490452e-03
 1.04195833e-01 6.80421770e-01 2.06693402e-03 9.27332580e-01
 9.67352688e-01 6.77148055e-04 5.39382339e-01 3.62818353e-02
 9.15862918e-01 3.36709321e-01 2.82150835e-01 1.67513844e-02
 8.63539040e-01 9.44173336e-01 9.46911991e-01 3.94515485e-01
 2.61469232e-03 9.53849554e-01 5.00308514e-01 3.21399532e-02
 9.60509181e-01 9.33020771e-01 5.59065081e-02 4.54341248e-02
 9.31352496e-01 7.24760234e-01 9.89381790e-01 8.99708331e-01
 2.27860198e-03 1.78753108e-01 6.81786239e-01 8.86724293e-01
 8.96629274e-01 2.09639268e-03 8.42796147e-01 9.83580530e-01
 5.04629850e-01 9.63988781e-01 7.39278376e-01 9.43632722e-01
 7.77445853e-01 8.60561490e-01 3.22651088e-01 7.77828932e-01
 9.00944710e-01 8.82386088e-01 1.05202314e-03 8.78200293e-01
 9.55046415e-01 6.09032273e-01 9.43148911e-01 5.93267474e-03
 9.24668238e-02 9.09529805e-01 9.40247476e-01 4.02862579e-02
 4.46403623e-01 1.18113026e-01 9.55984235e-01 9.74602938e-01
 8.01200271e-01 3.98376817e-03 4.94795758e-03 8.58583510e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 09:56:50, Dev, Step : 626, Loss : 0.46773, Acc : 0.792, Auc : 0.872, Sensitive_Loss : 0.23258, Sensitive_Acc : 16.779, Sensitive_Auc : 0.978, Mean auc: 0.872, Run Time : 153.06 sec
INFO:root:2024-04-26 09:56:57, Train, Epoch : 2, Step : 630, Loss : 0.20565, Acc : 0.297, Sensitive_Loss : 0.06368, Sensitive_Acc : 7.300, Run Time : 6.23 sec
INFO:root:2024-04-26 09:57:09, Train, Epoch : 2, Step : 640, Loss : 0.45072, Acc : 0.791, Sensitive_Loss : 0.16337, Sensitive_Acc : 14.800, Run Time : 11.79 sec
INFO:root:2024-04-26 09:57:20, Train, Epoch : 2, Step : 650, Loss : 0.49799, Acc : 0.775, Sensitive_Loss : 0.16676, Sensitive_Acc : 16.400, Run Time : 11.39 sec
INFO:root:2024-04-26 09:57:31, Train, Epoch : 2, Step : 660, Loss : 0.41212, Acc : 0.825, Sensitive_Loss : 0.13840, Sensitive_Acc : 15.800, Run Time : 10.86 sec
INFO:root:2024-04-26 09:57:43, Train, Epoch : 2, Step : 670, Loss : 0.46485, Acc : 0.800, Sensitive_Loss : 0.16681, Sensitive_Acc : 14.700, Run Time : 11.97 sec
INFO:root:2024-04-26 09:57:56, Train, Epoch : 2, Step : 680, Loss : 0.45360, Acc : 0.784, Sensitive_Loss : 0.17915, Sensitive_Acc : 16.800, Run Time : 12.55 sec
INFO:root:2024-04-26 09:58:06, Train, Epoch : 2, Step : 690, Loss : 0.44621, Acc : 0.797, Sensitive_Loss : 0.17405, Sensitive_Acc : 15.700, Run Time : 10.30 sec
INFO:root:2024-04-26 09:58:18, Train, Epoch : 2, Step : 700, Loss : 0.39099, Acc : 0.812, Sensitive_Loss : 0.17063, Sensitive_Acc : 15.000, Run Time : 11.76 sec
INFO:root:2024-04-26 10:00:52, Dev, Step : 700, Loss : 0.54280, Acc : 0.777, Auc : 0.876, Sensitive_Loss : 0.30390, Sensitive_Acc : 16.579, Sensitive_Auc : 0.981, Mean auc: 0.876, Run Time : 154.57 sec
INFO:root:2024-04-26 10:01:01, Train, Epoch : 2, Step : 710, Loss : 0.51658, Acc : 0.766, Sensitive_Loss : 0.15617, Sensitive_Acc : 16.200, Run Time : 162.89 sec
INFO:root:2024-04-26 10:01:12, Train, Epoch : 2, Step : 720, Loss : 0.42594, Acc : 0.822, Sensitive_Loss : 0.18964, Sensitive_Acc : 15.300, Run Time : 11.84 sec
INFO:root:2024-04-26 10:01:24, Train, Epoch : 2, Step : 730, Loss : 0.47269, Acc : 0.806, Sensitive_Loss : 0.18448, Sensitive_Acc : 17.300, Run Time : 11.05 sec
INFO:root:2024-04-26 10:01:35, Train, Epoch : 2, Step : 740, Loss : 0.45343, Acc : 0.816, Sensitive_Loss : 0.17341, Sensitive_Acc : 15.700, Run Time : 11.22 sec
INFO:root:2024-04-26 10:01:47, Train, Epoch : 2, Step : 750, Loss : 0.58942, Acc : 0.747, Sensitive_Loss : 0.13736, Sensitive_Acc : 15.200, Run Time : 12.21 sec
INFO:root:2024-04-26 10:01:59, Train, Epoch : 2, Step : 760, Loss : 0.43712, Acc : 0.800, Sensitive_Loss : 0.18897, Sensitive_Acc : 15.900, Run Time : 11.84 sec
INFO:root:2024-04-26 10:02:10, Train, Epoch : 2, Step : 770, Loss : 0.39640, Acc : 0.816, Sensitive_Loss : 0.21894, Sensitive_Acc : 16.900, Run Time : 11.62 sec
INFO:root:2024-04-26 10:02:22, Train, Epoch : 2, Step : 780, Loss : 0.40059, Acc : 0.803, Sensitive_Loss : 0.16477, Sensitive_Acc : 17.700, Run Time : 11.58 sec
INFO:root:2024-04-26 10:02:33, Train, Epoch : 2, Step : 790, Loss : 0.43938, Acc : 0.806, Sensitive_Loss : 0.16239, Sensitive_Acc : 15.800, Run Time : 10.91 sec
INFO:root:2024-04-26 10:02:48, Train, Epoch : 2, Step : 800, Loss : 0.41567, Acc : 0.803, Sensitive_Loss : 0.13999, Sensitive_Acc : 17.100, Run Time : 15.49 sec
INFO:root:2024-04-26 10:05:49, Dev, Step : 800, Loss : 0.44916, Acc : 0.807, Auc : 0.885, Sensitive_Loss : 0.13562, Sensitive_Acc : 16.907, Sensitive_Auc : 0.986, Mean auc: 0.885, Run Time : 180.74 sec
INFO:root:2024-04-26 10:05:50, Best, Step : 800, Loss : 0.44916, Acc : 0.807, Auc : 0.885, Sensitive_Loss : 0.13562, Sensitive_Acc : 16.907, Sensitive_Auc : 0.986, Best Auc : 0.885
INFO:root:2024-04-26 10:05:58, Train, Epoch : 2, Step : 810, Loss : 0.37737, Acc : 0.797, Sensitive_Loss : 0.16622, Sensitive_Acc : 14.900, Run Time : 189.65 sec
INFO:root:2024-04-26 10:06:09, Train, Epoch : 2, Step : 820, Loss : 0.47390, Acc : 0.784, Sensitive_Loss : 0.16738, Sensitive_Acc : 18.700, Run Time : 11.32 sec
INFO:root:2024-04-26 10:06:21, Train, Epoch : 2, Step : 830, Loss : 0.38804, Acc : 0.834, Sensitive_Loss : 0.11424, Sensitive_Acc : 16.000, Run Time : 11.55 sec
INFO:root:2024-04-26 10:06:32, Train, Epoch : 2, Step : 840, Loss : 0.39792, Acc : 0.800, Sensitive_Loss : 0.17072, Sensitive_Acc : 16.900, Run Time : 11.33 sec
INFO:root:2024-04-26 10:06:47, Train, Epoch : 2, Step : 850, Loss : 0.49039, Acc : 0.797, Sensitive_Loss : 0.11283, Sensitive_Acc : 17.100, Run Time : 14.21 sec
INFO:root:2024-04-26 10:06:58, Train, Epoch : 2, Step : 860, Loss : 0.43632, Acc : 0.781, Sensitive_Loss : 0.16982, Sensitive_Acc : 16.900, Run Time : 11.49 sec
INFO:root:2024-04-26 10:07:10, Train, Epoch : 2, Step : 870, Loss : 0.43327, Acc : 0.822, Sensitive_Loss : 0.19224, Sensitive_Acc : 15.900, Run Time : 12.28 sec
INFO:root:2024-04-26 10:07:26, Train, Epoch : 2, Step : 880, Loss : 0.48010, Acc : 0.781, Sensitive_Loss : 0.13819, Sensitive_Acc : 16.300, Run Time : 15.31 sec
INFO:root:2024-04-26 10:07:48, Train, Epoch : 2, Step : 890, Loss : 0.57687, Acc : 0.741, Sensitive_Loss : 0.15419, Sensitive_Acc : 16.400, Run Time : 21.99 sec
INFO:root:2024-04-26 10:08:12, Train, Epoch : 2, Step : 900, Loss : 0.39643, Acc : 0.800, Sensitive_Loss : 0.12413, Sensitive_Acc : 16.600, Run Time : 23.94 sec
INFO:root:2024-04-26 10:10:48, Dev, Step : 900, Loss : 0.49356, Acc : 0.783, Auc : 0.883, Sensitive_Loss : 0.15706, Sensitive_Acc : 16.864, Sensitive_Auc : 0.989, Mean auc: 0.883, Run Time : 155.99 sec
INFO:root:2024-04-26 10:10:56, Train, Epoch : 2, Step : 910, Loss : 0.38887, Acc : 0.841, Sensitive_Loss : 0.11929, Sensitive_Acc : 14.200, Run Time : 164.72 sec
INFO:root:2024-04-26 10:11:09, Train, Epoch : 2, Step : 920, Loss : 0.39482, Acc : 0.803, Sensitive_Loss : 0.15982, Sensitive_Acc : 17.200, Run Time : 12.56 sec
INFO:root:2024-04-26 10:11:25, Train, Epoch : 2, Step : 930, Loss : 0.41196, Acc : 0.819, Sensitive_Loss : 0.13614, Sensitive_Acc : 16.300, Run Time : 15.70 sec
INFO:root:2024-04-26 10:11:37, Train, Epoch : 2, Step : 940, Loss : 0.43973, Acc : 0.825, Sensitive_Loss : 0.14356, Sensitive_Acc : 17.700, Run Time : 11.99 sec
INFO:root:2024-04-26 10:11:49, Train, Epoch : 2, Step : 950, Loss : 0.46544, Acc : 0.800, Sensitive_Loss : 0.20884, Sensitive_Acc : 15.700, Run Time : 12.25 sec
INFO:root:2024-04-26 10:12:02, Train, Epoch : 2, Step : 960, Loss : 0.46829, Acc : 0.831, Sensitive_Loss : 0.14957, Sensitive_Acc : 16.400, Run Time : 13.15 sec
INFO:root:2024-04-26 10:12:15, Train, Epoch : 2, Step : 970, Loss : 0.45382, Acc : 0.775, Sensitive_Loss : 0.16127, Sensitive_Acc : 16.400, Run Time : 13.46 sec
INFO:root:2024-04-26 10:12:28, Train, Epoch : 2, Step : 980, Loss : 0.35787, Acc : 0.812, Sensitive_Loss : 0.17540, Sensitive_Acc : 16.200, Run Time : 12.97 sec
INFO:root:2024-04-26 10:12:41, Train, Epoch : 2, Step : 990, Loss : 0.41582, Acc : 0.831, Sensitive_Loss : 0.16032, Sensitive_Acc : 16.200, Run Time : 12.37 sec
INFO:root:2024-04-26 10:12:53, Train, Epoch : 2, Step : 1000, Loss : 0.43945, Acc : 0.816, Sensitive_Loss : 0.13280, Sensitive_Acc : 16.200, Run Time : 12.78 sec
INFO:root:2024-04-26 10:15:29, Dev, Step : 1000, Loss : 0.44574, Acc : 0.805, Auc : 0.884, Sensitive_Loss : 0.15900, Sensitive_Acc : 16.879, Sensitive_Auc : 0.990, Mean auc: 0.884, Run Time : 155.57 sec
INFO:root:2024-04-26 10:15:39, Train, Epoch : 2, Step : 1010, Loss : 0.47801, Acc : 0.784, Sensitive_Loss : 0.14673, Sensitive_Acc : 16.500, Run Time : 165.35 sec
INFO:root:2024-04-26 10:15:52, Train, Epoch : 2, Step : 1020, Loss : 0.42247, Acc : 0.803, Sensitive_Loss : 0.13759, Sensitive_Acc : 15.400, Run Time : 12.77 sec
INFO:root:2024-04-26 10:16:05, Train, Epoch : 2, Step : 1030, Loss : 0.42975, Acc : 0.781, Sensitive_Loss : 0.14005, Sensitive_Acc : 15.200, Run Time : 13.33 sec
INFO:root:2024-04-26 10:16:17, Train, Epoch : 2, Step : 1040, Loss : 0.37391, Acc : 0.825, Sensitive_Loss : 0.11934, Sensitive_Acc : 16.000, Run Time : 12.51 sec
INFO:root:2024-04-26 10:16:30, Train, Epoch : 2, Step : 1050, Loss : 0.43525, Acc : 0.797, Sensitive_Loss : 0.13940, Sensitive_Acc : 16.100, Run Time : 12.50 sec
INFO:root:2024-04-26 10:16:42, Train, Epoch : 2, Step : 1060, Loss : 0.54123, Acc : 0.756, Sensitive_Loss : 0.16149, Sensitive_Acc : 17.300, Run Time : 12.23 sec
INFO:root:2024-04-26 10:16:55, Train, Epoch : 2, Step : 1070, Loss : 0.49197, Acc : 0.772, Sensitive_Loss : 0.15033, Sensitive_Acc : 15.300, Run Time : 12.85 sec
INFO:root:2024-04-26 10:17:08, Train, Epoch : 2, Step : 1080, Loss : 0.40079, Acc : 0.816, Sensitive_Loss : 0.11010, Sensitive_Acc : 16.800, Run Time : 12.78 sec
INFO:root:2024-04-26 10:17:20, Train, Epoch : 2, Step : 1090, Loss : 0.48979, Acc : 0.778, Sensitive_Loss : 0.15618, Sensitive_Acc : 18.000, Run Time : 12.66 sec
INFO:root:2024-04-26 10:17:33, Train, Epoch : 2, Step : 1100, Loss : 0.45238, Acc : 0.787, Sensitive_Loss : 0.11118, Sensitive_Acc : 17.500, Run Time : 12.25 sec
INFO:root:2024-04-26 10:20:29, Dev, Step : 1100, Loss : 0.48168, Acc : 0.776, Auc : 0.885, Sensitive_Loss : 0.18682, Sensitive_Acc : 16.793, Sensitive_Auc : 0.970, Mean auc: 0.885, Run Time : 176.18 sec
INFO:root:2024-04-26 10:20:42, Train, Epoch : 2, Step : 1110, Loss : 0.34308, Acc : 0.856, Sensitive_Loss : 0.12852, Sensitive_Acc : 16.300, Run Time : 189.08 sec
INFO:root:2024-04-26 10:20:59, Train, Epoch : 2, Step : 1120, Loss : 0.36693, Acc : 0.812, Sensitive_Loss : 0.16694, Sensitive_Acc : 17.600, Run Time : 17.01 sec
INFO:root:2024-04-26 10:21:12, Train, Epoch : 2, Step : 1130, Loss : 0.40457, Acc : 0.825, Sensitive_Loss : 0.12354, Sensitive_Acc : 15.800, Run Time : 12.93 sec
INFO:root:2024-04-26 10:21:24, Train, Epoch : 2, Step : 1140, Loss : 0.46892, Acc : 0.819, Sensitive_Loss : 0.10528, Sensitive_Acc : 16.200, Run Time : 12.33 sec
INFO:root:2024-04-26 10:21:36, Train, Epoch : 2, Step : 1150, Loss : 0.39494, Acc : 0.828, Sensitive_Loss : 0.16742, Sensitive_Acc : 18.100, Run Time : 12.26 sec
INFO:root:2024-04-26 10:21:48, Train, Epoch : 2, Step : 1160, Loss : 0.40397, Acc : 0.834, Sensitive_Loss : 0.09400, Sensitive_Acc : 16.700, Run Time : 11.87 sec
INFO:root:2024-04-26 10:21:59, Train, Epoch : 2, Step : 1170, Loss : 0.43347, Acc : 0.812, Sensitive_Loss : 0.15471, Sensitive_Acc : 16.000, Run Time : 11.21 sec
INFO:root:2024-04-26 10:22:12, Train, Epoch : 2, Step : 1180, Loss : 0.44293, Acc : 0.822, Sensitive_Loss : 0.16798, Sensitive_Acc : 16.900, Run Time : 12.30 sec
INFO:root:2024-04-26 10:22:24, Train, Epoch : 2, Step : 1190, Loss : 0.47977, Acc : 0.797, Sensitive_Loss : 0.09380, Sensitive_Acc : 16.000, Run Time : 12.64 sec
INFO:root:2024-04-26 10:22:36, Train, Epoch : 2, Step : 1200, Loss : 0.46169, Acc : 0.803, Sensitive_Loss : 0.09621, Sensitive_Acc : 16.700, Run Time : 11.86 sec
INFO:root:2024-04-26 10:25:30, Dev, Step : 1200, Loss : 0.59968, Acc : 0.741, Auc : 0.893, Sensitive_Loss : 0.27190, Sensitive_Acc : 16.664, Sensitive_Auc : 0.985, Mean auc: 0.893, Run Time : 173.43 sec
INFO:root:2024-04-26 10:25:30, Best, Step : 1200, Loss : 0.59968, Acc : 0.741, Auc : 0.893, Sensitive_Loss : 0.27190, Sensitive_Acc : 16.664, Sensitive_Auc : 0.985, Best Auc : 0.893
INFO:root:2024-04-26 10:25:39, Train, Epoch : 2, Step : 1210, Loss : 0.41470, Acc : 0.803, Sensitive_Loss : 0.10964, Sensitive_Acc : 16.000, Run Time : 182.91 sec
INFO:root:2024-04-26 10:25:51, Train, Epoch : 2, Step : 1220, Loss : 0.43278, Acc : 0.809, Sensitive_Loss : 0.15398, Sensitive_Acc : 16.200, Run Time : 12.00 sec
INFO:root:2024-04-26 10:26:04, Train, Epoch : 2, Step : 1230, Loss : 0.36969, Acc : 0.819, Sensitive_Loss : 0.13280, Sensitive_Acc : 15.800, Run Time : 12.66 sec
INFO:root:2024-04-26 10:26:15, Train, Epoch : 2, Step : 1240, Loss : 0.42685, Acc : 0.812, Sensitive_Loss : 0.21993, Sensitive_Acc : 17.300, Run Time : 11.57 sec
INFO:root:2024-04-26 10:26:28, Train, Epoch : 2, Step : 1250, Loss : 0.39015, Acc : 0.803, Sensitive_Loss : 0.13769, Sensitive_Acc : 17.500, Run Time : 12.89 sec
INFO:root:2024-04-26 10:29:02
INFO:root:y_pred: [0.30649024 0.975874   0.14349967 ... 0.90946275 0.01913043 0.8952857 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.4961530e-01 5.4704495e-05 9.7240232e-02 1.1263215e-05 9.8198456e-01
 2.6605240e-04 9.9248284e-01 9.7821611e-01 2.0805739e-02 9.4578075e-01
 9.9585730e-01 9.9655342e-01 9.8795998e-01 8.2660019e-01 7.6326043e-03
 7.9584712e-01 9.9785221e-01 3.4925062e-02 7.0769608e-01 9.9805766e-01
 9.1031331e-01 1.8993312e-02 9.9342269e-01 9.0369153e-01 9.7964954e-01
 6.3593054e-01 1.2998025e-04 9.7671741e-01 9.8133022e-01 6.5632439e-01
 2.1522403e-02 5.0239253e-01 1.6301514e-03 1.9227643e-02 7.1910121e-02
 1.3165114e-03 8.8136010e-02 5.2858252e-02 9.7005683e-01 9.8567528e-01
 2.3095054e-05 6.2780157e-03 9.9047333e-01 1.2037955e-03 9.9881762e-01
 9.8766255e-01 9.6828461e-01 9.9450737e-01 1.8786477e-01 9.8633713e-01
 9.9714029e-01 7.6241411e-02 3.6454760e-02 7.9138794e-05 2.0560122e-03
 3.2799695e-02 9.1421470e-02 6.6951162e-01 1.2541392e-03 2.5407159e-01
 6.4045591e-05 3.7611939e-02 2.8516102e-01 9.7387367e-01 6.1314899e-02
 9.9650627e-01 6.8242615e-04 9.9621308e-01 7.3414677e-01 3.6629799e-01
 7.9540306e-01 5.3859311e-01 1.1753349e-03 4.9360208e-02 1.3642773e-04
 2.9235693e-02 2.3290325e-02 4.2320091e-01 1.8501280e-02 9.7698170e-01
 9.9845028e-01 4.6799335e-04 8.4528393e-01 1.8818962e-04 9.6793759e-01
 8.8563520e-01 8.3684824e-02 1.3295001e-02 8.6982214e-01 9.9077892e-01
 9.9445927e-01 1.8520069e-01 6.1042965e-03 9.8960871e-01 4.4249174e-01
 4.6259192e-05 9.6743953e-01 9.2270631e-01 2.2806223e-03 4.5768213e-02
 9.6661979e-01 8.4995770e-01 9.9951673e-01 9.8821694e-01 1.2399766e-03
 1.6486132e-01 6.6904187e-01 9.1024131e-01 8.7365609e-01 3.2471868e-04
 8.3467323e-01 9.6247572e-01 3.1092089e-01 9.9176180e-01 9.4710141e-01
 9.8578829e-01 7.3194724e-01 9.8249650e-01 4.5654529e-01 7.9692274e-01
 9.8998088e-01 9.7563499e-01 1.5765244e-04 8.3414942e-01 9.9415612e-01
 2.0140862e-01 9.6642995e-01 5.0694342e-03 1.5807062e-01 9.3259978e-01
 9.7001135e-01 1.0838245e-04 2.9511394e-03 3.9431009e-01 9.9198723e-01
 9.9869788e-01 7.7596903e-01 4.2542139e-05 3.8831539e-02 9.5653975e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 10:29:02, Dev, Step : 1252, Loss : 0.44614, Acc : 0.803, Auc : 0.895, Sensitive_Loss : 0.15891, Sensitive_Acc : 16.721, Sensitive_Auc : 0.987, Mean auc: 0.895, Run Time : 152.48 sec
INFO:root:2024-04-26 10:29:03, Best, Step : 1252, Loss : 0.44614, Acc : 0.803,Auc : 0.895, Best Auc : 0.895, Sensitive_Loss : 0.15891, Sensitive_Acc : 16.721, Sensitive_Auc : 0.987
INFO:root:2024-04-26 10:29:15, Train, Epoch : 3, Step : 1260, Loss : 0.29458, Acc : 0.644, Sensitive_Loss : 0.12299, Sensitive_Acc : 13.700, Run Time : 10.80 sec
INFO:root:2024-04-26 10:29:26, Train, Epoch : 3, Step : 1270, Loss : 0.39631, Acc : 0.834, Sensitive_Loss : 0.09815, Sensitive_Acc : 16.500, Run Time : 11.42 sec
INFO:root:2024-04-26 10:29:38, Train, Epoch : 3, Step : 1280, Loss : 0.33947, Acc : 0.863, Sensitive_Loss : 0.11321, Sensitive_Acc : 16.100, Run Time : 11.70 sec
INFO:root:2024-04-26 10:29:49, Train, Epoch : 3, Step : 1290, Loss : 0.33481, Acc : 0.853, Sensitive_Loss : 0.09549, Sensitive_Acc : 16.000, Run Time : 11.40 sec
INFO:root:2024-04-26 10:30:01, Train, Epoch : 3, Step : 1300, Loss : 0.42698, Acc : 0.816, Sensitive_Loss : 0.12578, Sensitive_Acc : 16.500, Run Time : 11.41 sec
INFO:root:2024-04-26 10:32:36, Dev, Step : 1300, Loss : 0.42004, Acc : 0.822, Auc : 0.899, Sensitive_Loss : 0.18125, Sensitive_Acc : 16.764, Sensitive_Auc : 0.983, Mean auc: 0.899, Run Time : 155.16 sec
INFO:root:2024-04-26 10:32:37, Best, Step : 1300, Loss : 0.42004, Acc : 0.822, Auc : 0.899, Sensitive_Loss : 0.18125, Sensitive_Acc : 16.764, Sensitive_Auc : 0.983, Best Auc : 0.899
INFO:root:2024-04-26 10:32:45, Train, Epoch : 3, Step : 1310, Loss : 0.39898, Acc : 0.819, Sensitive_Loss : 0.17445, Sensitive_Acc : 16.300, Run Time : 164.38 sec
INFO:root:2024-04-26 10:32:56, Train, Epoch : 3, Step : 1320, Loss : 0.38516, Acc : 0.847, Sensitive_Loss : 0.08790, Sensitive_Acc : 17.400, Run Time : 11.50 sec
INFO:root:2024-04-26 10:33:08, Train, Epoch : 3, Step : 1330, Loss : 0.42733, Acc : 0.825, Sensitive_Loss : 0.11339, Sensitive_Acc : 14.200, Run Time : 11.98 sec
INFO:root:2024-04-26 10:33:20, Train, Epoch : 3, Step : 1340, Loss : 0.38937, Acc : 0.822, Sensitive_Loss : 0.11708, Sensitive_Acc : 15.900, Run Time : 11.71 sec
INFO:root:2024-04-26 10:33:31, Train, Epoch : 3, Step : 1350, Loss : 0.42917, Acc : 0.819, Sensitive_Loss : 0.09640, Sensitive_Acc : 15.300, Run Time : 10.63 sec
INFO:root:2024-04-26 10:33:44, Train, Epoch : 3, Step : 1360, Loss : 0.32774, Acc : 0.844, Sensitive_Loss : 0.10603, Sensitive_Acc : 15.700, Run Time : 12.94 sec
INFO:root:2024-04-26 10:33:55, Train, Epoch : 3, Step : 1370, Loss : 0.38833, Acc : 0.819, Sensitive_Loss : 0.10664, Sensitive_Acc : 18.000, Run Time : 10.93 sec
INFO:root:2024-04-26 10:34:06, Train, Epoch : 3, Step : 1380, Loss : 0.36053, Acc : 0.834, Sensitive_Loss : 0.09444, Sensitive_Acc : 16.800, Run Time : 11.31 sec
INFO:root:2024-04-26 10:34:17, Train, Epoch : 3, Step : 1390, Loss : 0.41298, Acc : 0.825, Sensitive_Loss : 0.13656, Sensitive_Acc : 17.100, Run Time : 10.95 sec
INFO:root:2024-04-26 10:34:29, Train, Epoch : 3, Step : 1400, Loss : 0.39129, Acc : 0.822, Sensitive_Loss : 0.12212, Sensitive_Acc : 15.000, Run Time : 12.15 sec
INFO:root:2024-04-26 10:37:04, Dev, Step : 1400, Loss : 0.43096, Acc : 0.812, Auc : 0.902, Sensitive_Loss : 0.15877, Sensitive_Acc : 16.764, Sensitive_Auc : 0.984, Mean auc: 0.902, Run Time : 154.54 sec
INFO:root:2024-04-26 10:37:04, Best, Step : 1400, Loss : 0.43096, Acc : 0.812, Auc : 0.902, Sensitive_Loss : 0.15877, Sensitive_Acc : 16.764, Sensitive_Auc : 0.984, Best Auc : 0.902
INFO:root:2024-04-26 10:37:13, Train, Epoch : 3, Step : 1410, Loss : 0.37695, Acc : 0.828, Sensitive_Loss : 0.11634, Sensitive_Acc : 18.100, Run Time : 163.91 sec
INFO:root:2024-04-26 10:37:25, Train, Epoch : 3, Step : 1420, Loss : 0.37161, Acc : 0.803, Sensitive_Loss : 0.14506, Sensitive_Acc : 15.600, Run Time : 11.71 sec
INFO:root:2024-04-26 10:37:36, Train, Epoch : 3, Step : 1430, Loss : 0.36568, Acc : 0.847, Sensitive_Loss : 0.12332, Sensitive_Acc : 16.100, Run Time : 11.08 sec
INFO:root:2024-04-26 10:37:48, Train, Epoch : 3, Step : 1440, Loss : 0.39195, Acc : 0.834, Sensitive_Loss : 0.07936, Sensitive_Acc : 16.300, Run Time : 12.03 sec
INFO:root:2024-04-26 10:38:00, Train, Epoch : 3, Step : 1450, Loss : 0.30743, Acc : 0.866, Sensitive_Loss : 0.11408, Sensitive_Acc : 15.600, Run Time : 11.91 sec
INFO:root:2024-04-26 10:38:11, Train, Epoch : 3, Step : 1460, Loss : 0.44939, Acc : 0.800, Sensitive_Loss : 0.13153, Sensitive_Acc : 14.800, Run Time : 11.37 sec
INFO:root:2024-04-26 10:38:22, Train, Epoch : 3, Step : 1470, Loss : 0.41359, Acc : 0.803, Sensitive_Loss : 0.13319, Sensitive_Acc : 17.500, Run Time : 11.36 sec
INFO:root:2024-04-26 10:38:34, Train, Epoch : 3, Step : 1480, Loss : 0.41517, Acc : 0.819, Sensitive_Loss : 0.07928, Sensitive_Acc : 15.800, Run Time : 11.16 sec
INFO:root:2024-04-26 10:38:45, Train, Epoch : 3, Step : 1490, Loss : 0.33328, Acc : 0.863, Sensitive_Loss : 0.06951, Sensitive_Acc : 16.700, Run Time : 11.47 sec
INFO:root:2024-04-26 10:38:56, Train, Epoch : 3, Step : 1500, Loss : 0.33811, Acc : 0.844, Sensitive_Loss : 0.10874, Sensitive_Acc : 16.200, Run Time : 10.52 sec
INFO:root:2024-04-26 10:41:34, Dev, Step : 1500, Loss : 0.40561, Acc : 0.827, Auc : 0.905, Sensitive_Loss : 0.12170, Sensitive_Acc : 16.750, Sensitive_Auc : 0.984, Mean auc: 0.905, Run Time : 158.74 sec
INFO:root:2024-04-26 10:41:35, Best, Step : 1500, Loss : 0.40561, Acc : 0.827, Auc : 0.905, Sensitive_Loss : 0.12170, Sensitive_Acc : 16.750, Sensitive_Auc : 0.984, Best Auc : 0.905
INFO:root:2024-04-26 10:41:43, Train, Epoch : 3, Step : 1510, Loss : 0.34518, Acc : 0.853, Sensitive_Loss : 0.08164, Sensitive_Acc : 16.300, Run Time : 167.45 sec
INFO:root:2024-04-26 10:41:55, Train, Epoch : 3, Step : 1520, Loss : 0.35638, Acc : 0.841, Sensitive_Loss : 0.12991, Sensitive_Acc : 16.300, Run Time : 11.80 sec
INFO:root:2024-04-26 10:42:06, Train, Epoch : 3, Step : 1530, Loss : 0.31170, Acc : 0.841, Sensitive_Loss : 0.09537, Sensitive_Acc : 15.300, Run Time : 11.34 sec
INFO:root:2024-04-26 10:42:18, Train, Epoch : 3, Step : 1540, Loss : 0.44659, Acc : 0.803, Sensitive_Loss : 0.07766, Sensitive_Acc : 15.600, Run Time : 11.46 sec
INFO:root:2024-04-26 10:42:30, Train, Epoch : 3, Step : 1550, Loss : 0.34065, Acc : 0.831, Sensitive_Loss : 0.08081, Sensitive_Acc : 16.800, Run Time : 11.98 sec
INFO:root:2024-04-26 10:42:41, Train, Epoch : 3, Step : 1560, Loss : 0.42765, Acc : 0.831, Sensitive_Loss : 0.11396, Sensitive_Acc : 16.200, Run Time : 11.64 sec
INFO:root:2024-04-26 10:42:52, Train, Epoch : 3, Step : 1570, Loss : 0.41286, Acc : 0.841, Sensitive_Loss : 0.11613, Sensitive_Acc : 16.400, Run Time : 10.90 sec
INFO:root:2024-04-26 10:43:04, Train, Epoch : 3, Step : 1580, Loss : 0.32912, Acc : 0.866, Sensitive_Loss : 0.06801, Sensitive_Acc : 17.600, Run Time : 11.87 sec
INFO:root:2024-04-26 10:43:16, Train, Epoch : 3, Step : 1590, Loss : 0.33616, Acc : 0.812, Sensitive_Loss : 0.11343, Sensitive_Acc : 17.700, Run Time : 11.57 sec
INFO:root:2024-04-26 10:43:28, Train, Epoch : 3, Step : 1600, Loss : 0.34658, Acc : 0.847, Sensitive_Loss : 0.11753, Sensitive_Acc : 15.300, Run Time : 12.19 sec
INFO:root:2024-04-26 10:46:02, Dev, Step : 1600, Loss : 0.40927, Acc : 0.824, Auc : 0.905, Sensitive_Loss : 0.13414, Sensitive_Acc : 16.793, Sensitive_Auc : 0.985, Mean auc: 0.905, Run Time : 153.95 sec
INFO:root:2024-04-26 10:46:10, Train, Epoch : 3, Step : 1610, Loss : 0.40391, Acc : 0.825, Sensitive_Loss : 0.10018, Sensitive_Acc : 17.000, Run Time : 162.26 sec
INFO:root:2024-04-26 10:46:21, Train, Epoch : 3, Step : 1620, Loss : 0.39999, Acc : 0.841, Sensitive_Loss : 0.11217, Sensitive_Acc : 16.100, Run Time : 11.04 sec
INFO:root:2024-04-26 10:46:33, Train, Epoch : 3, Step : 1630, Loss : 0.33332, Acc : 0.847, Sensitive_Loss : 0.08612, Sensitive_Acc : 14.600, Run Time : 12.31 sec
INFO:root:2024-04-26 10:46:45, Train, Epoch : 3, Step : 1640, Loss : 0.37008, Acc : 0.834, Sensitive_Loss : 0.09874, Sensitive_Acc : 14.900, Run Time : 11.51 sec
INFO:root:2024-04-26 10:46:56, Train, Epoch : 3, Step : 1650, Loss : 0.40426, Acc : 0.819, Sensitive_Loss : 0.09579, Sensitive_Acc : 15.800, Run Time : 11.37 sec
INFO:root:2024-04-26 10:47:08, Train, Epoch : 3, Step : 1660, Loss : 0.38173, Acc : 0.822, Sensitive_Loss : 0.08361, Sensitive_Acc : 18.100, Run Time : 11.85 sec
INFO:root:2024-04-26 10:47:20, Train, Epoch : 3, Step : 1670, Loss : 0.32987, Acc : 0.850, Sensitive_Loss : 0.09185, Sensitive_Acc : 17.000, Run Time : 12.18 sec
INFO:root:2024-04-26 10:47:31, Train, Epoch : 3, Step : 1680, Loss : 0.33652, Acc : 0.863, Sensitive_Loss : 0.11352, Sensitive_Acc : 15.800, Run Time : 11.14 sec
INFO:root:2024-04-26 10:47:42, Train, Epoch : 3, Step : 1690, Loss : 0.37561, Acc : 0.841, Sensitive_Loss : 0.12416, Sensitive_Acc : 16.700, Run Time : 10.46 sec
INFO:root:2024-04-26 10:47:54, Train, Epoch : 3, Step : 1700, Loss : 0.33468, Acc : 0.887, Sensitive_Loss : 0.10666, Sensitive_Acc : 17.500, Run Time : 11.91 sec
INFO:root:2024-04-26 10:50:29, Dev, Step : 1700, Loss : 0.40892, Acc : 0.827, Auc : 0.906, Sensitive_Loss : 0.12731, Sensitive_Acc : 16.793, Sensitive_Auc : 0.987, Mean auc: 0.906, Run Time : 155.54 sec
INFO:root:2024-04-26 10:50:30, Best, Step : 1700, Loss : 0.40892, Acc : 0.827, Auc : 0.906, Sensitive_Loss : 0.12731, Sensitive_Acc : 16.793, Sensitive_Auc : 0.987, Best Auc : 0.906
INFO:root:2024-04-26 10:50:38, Train, Epoch : 3, Step : 1710, Loss : 0.38760, Acc : 0.825, Sensitive_Loss : 0.12824, Sensitive_Acc : 15.800, Run Time : 164.65 sec
INFO:root:2024-04-26 10:50:50, Train, Epoch : 3, Step : 1720, Loss : 0.43983, Acc : 0.803, Sensitive_Loss : 0.08625, Sensitive_Acc : 18.800, Run Time : 11.20 sec
INFO:root:2024-04-26 10:51:02, Train, Epoch : 3, Step : 1730, Loss : 0.33004, Acc : 0.875, Sensitive_Loss : 0.08135, Sensitive_Acc : 17.500, Run Time : 12.33 sec
INFO:root:2024-04-26 10:51:13, Train, Epoch : 3, Step : 1740, Loss : 0.38411, Acc : 0.825, Sensitive_Loss : 0.09645, Sensitive_Acc : 16.100, Run Time : 11.32 sec
INFO:root:2024-04-26 10:51:25, Train, Epoch : 3, Step : 1750, Loss : 0.39072, Acc : 0.803, Sensitive_Loss : 0.12615, Sensitive_Acc : 15.300, Run Time : 11.40 sec
INFO:root:2024-04-26 10:51:36, Train, Epoch : 3, Step : 1760, Loss : 0.38968, Acc : 0.822, Sensitive_Loss : 0.10500, Sensitive_Acc : 16.000, Run Time : 10.91 sec
INFO:root:2024-04-26 10:51:47, Train, Epoch : 3, Step : 1770, Loss : 0.33864, Acc : 0.866, Sensitive_Loss : 0.08637, Sensitive_Acc : 16.100, Run Time : 11.21 sec
INFO:root:2024-04-26 10:51:59, Train, Epoch : 3, Step : 1780, Loss : 0.42039, Acc : 0.809, Sensitive_Loss : 0.12739, Sensitive_Acc : 16.200, Run Time : 12.52 sec
INFO:root:2024-04-26 10:52:11, Train, Epoch : 3, Step : 1790, Loss : 0.35702, Acc : 0.847, Sensitive_Loss : 0.09843, Sensitive_Acc : 16.700, Run Time : 11.82 sec
INFO:root:2024-04-26 10:52:23, Train, Epoch : 3, Step : 1800, Loss : 0.31869, Acc : 0.859, Sensitive_Loss : 0.08106, Sensitive_Acc : 16.300, Run Time : 11.56 sec
INFO:root:2024-04-26 10:54:57, Dev, Step : 1800, Loss : 0.41118, Acc : 0.823, Auc : 0.905, Sensitive_Loss : 0.12366, Sensitive_Acc : 16.793, Sensitive_Auc : 0.984, Mean auc: 0.905, Run Time : 154.36 sec
INFO:root:2024-04-26 10:55:06, Train, Epoch : 3, Step : 1810, Loss : 0.39076, Acc : 0.838, Sensitive_Loss : 0.09630, Sensitive_Acc : 16.300, Run Time : 163.06 sec
INFO:root:2024-04-26 10:55:16, Train, Epoch : 3, Step : 1820, Loss : 0.37019, Acc : 0.856, Sensitive_Loss : 0.07354, Sensitive_Acc : 17.100, Run Time : 10.71 sec
INFO:root:2024-04-26 10:55:28, Train, Epoch : 3, Step : 1830, Loss : 0.30206, Acc : 0.878, Sensitive_Loss : 0.07802, Sensitive_Acc : 14.800, Run Time : 11.06 sec
INFO:root:2024-04-26 10:55:39, Train, Epoch : 3, Step : 1840, Loss : 0.34148, Acc : 0.859, Sensitive_Loss : 0.07172, Sensitive_Acc : 15.000, Run Time : 11.79 sec
INFO:root:2024-04-26 10:55:51, Train, Epoch : 3, Step : 1850, Loss : 0.38226, Acc : 0.825, Sensitive_Loss : 0.08881, Sensitive_Acc : 15.100, Run Time : 11.95 sec
INFO:root:2024-04-26 10:56:02, Train, Epoch : 3, Step : 1860, Loss : 0.43104, Acc : 0.822, Sensitive_Loss : 0.09395, Sensitive_Acc : 12.800, Run Time : 10.77 sec
INFO:root:2024-04-26 10:56:14, Train, Epoch : 3, Step : 1870, Loss : 0.31599, Acc : 0.859, Sensitive_Loss : 0.06926, Sensitive_Acc : 14.900, Run Time : 11.99 sec
INFO:root:2024-04-26 10:58:55
INFO:root:y_pred: [0.08279246 0.9001282  0.0417643  ... 0.8439249  0.00246356 0.7593767 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.85340774e-01 1.31177818e-04 1.10171195e-02 2.90697676e-06
 9.88718033e-01 3.17039267e-05 9.98193085e-01 9.92173076e-01
 9.16520250e-04 9.71714675e-01 9.98598278e-01 9.97692466e-01
 9.92438495e-01 8.71372104e-01 5.16444957e-03 9.39815283e-01
 9.98547733e-01 9.11017507e-03 7.20244706e-01 9.90210414e-01
 9.78971601e-01 9.73776504e-02 9.98536348e-01 9.36388969e-01
 9.87162411e-01 8.26310873e-01 1.24508559e-04 9.88196909e-01
 9.89558816e-01 9.30679619e-01 2.03852798e-03 4.47129816e-01
 1.90441050e-02 3.66432667e-02 2.49786377e-01 1.84256205e-04
 4.91527691e-02 7.87947606e-03 9.86656666e-01 9.89034534e-01
 6.06441718e-05 6.16422901e-03 9.97424483e-01 9.48666639e-05
 9.99292374e-01 9.94526923e-01 9.72624123e-01 9.97726738e-01
 8.25995505e-02 9.85222280e-01 9.98239875e-01 1.35615822e-02
 2.22199500e-01 4.54326000e-05 3.30323353e-04 8.88164900e-03
 1.02566972e-01 6.49822056e-01 2.36878870e-03 1.67233467e-01
 3.59967526e-04 1.67140856e-01 1.82017699e-01 9.82889175e-01
 4.84357923e-02 9.97387826e-01 1.29310251e-03 9.97988343e-01
 8.43481719e-01 8.12605262e-01 9.40770626e-01 5.38376451e-01
 8.65910028e-04 1.57967526e-02 8.29047203e-05 6.35771500e-03
 1.84886120e-02 6.02335572e-01 3.52474919e-04 9.92049396e-01
 9.98989046e-01 6.28236885e-05 6.67517841e-01 5.29021956e-04
 9.93762672e-01 9.72157717e-01 6.20070696e-02 9.57767386e-03
 8.76033187e-01 9.95572805e-01 9.96808589e-01 5.37849605e-01
 5.57125360e-03 9.92067933e-01 6.46038830e-01 3.57357785e-04
 9.81961250e-01 9.32795763e-01 1.02683356e-04 9.80130304e-03
 9.76003885e-01 9.20299709e-01 9.99839544e-01 9.93937969e-01
 5.35165484e-04 7.11778760e-01 6.41304493e-01 9.20989513e-01
 8.69410753e-01 3.74048337e-04 8.48545134e-01 9.92556155e-01
 1.53768003e-01 9.97310400e-01 9.55886126e-01 9.91837323e-01
 6.92071438e-01 9.95591521e-01 1.93261817e-01 4.23302352e-01
 9.97022212e-01 9.90699232e-01 9.17666985e-05 8.72237742e-01
 9.94998932e-01 1.31460875e-01 9.94027853e-01 4.22135257e-04
 1.05104931e-01 9.93105352e-01 9.94852245e-01 1.54408583e-04
 5.24927629e-03 1.13642879e-01 9.94941890e-01 9.98948514e-01
 9.20127392e-01 1.29113570e-04 3.94403515e-03 9.83594120e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 10:58:55, Dev, Step : 1878, Loss : 0.40780, Acc : 0.825, Auc : 0.905, Sensitive_Loss : 0.13176, Sensitive_Acc : 16.779, Sensitive_Auc : 0.982, Mean auc: 0.905, Run Time : 153.19 sec
INFO:root:2024-04-26 10:59:01, Train, Epoch : 4, Step : 1880, Loss : 0.05789, Acc : 0.181, Sensitive_Loss : 0.01942, Sensitive_Acc : 3.400, Run Time : 4.34 sec
INFO:root:2024-04-26 10:59:13, Train, Epoch : 4, Step : 1890, Loss : 0.41101, Acc : 0.825, Sensitive_Loss : 0.09724, Sensitive_Acc : 16.200, Run Time : 12.39 sec
INFO:root:2024-04-26 10:59:24, Train, Epoch : 4, Step : 1900, Loss : 0.31674, Acc : 0.844, Sensitive_Loss : 0.09371, Sensitive_Acc : 14.700, Run Time : 10.66 sec
INFO:root:2024-04-26 11:01:58, Dev, Step : 1900, Loss : 0.41574, Acc : 0.822, Auc : 0.905, Sensitive_Loss : 0.14268, Sensitive_Acc : 16.779, Sensitive_Auc : 0.983, Mean auc: 0.905, Run Time : 154.43 sec
INFO:root:2024-04-26 11:02:08, Train, Epoch : 4, Step : 1910, Loss : 0.41181, Acc : 0.791, Sensitive_Loss : 0.09459, Sensitive_Acc : 16.500, Run Time : 163.78 sec
INFO:root:2024-04-26 11:02:19, Train, Epoch : 4, Step : 1920, Loss : 0.38787, Acc : 0.822, Sensitive_Loss : 0.08521, Sensitive_Acc : 17.900, Run Time : 11.74 sec
INFO:root:2024-04-26 11:02:30, Train, Epoch : 4, Step : 1930, Loss : 0.36235, Acc : 0.819, Sensitive_Loss : 0.09953, Sensitive_Acc : 17.400, Run Time : 11.06 sec
INFO:root:2024-04-26 11:02:42, Train, Epoch : 4, Step : 1940, Loss : 0.39653, Acc : 0.828, Sensitive_Loss : 0.11818, Sensitive_Acc : 16.600, Run Time : 11.79 sec
INFO:root:2024-04-26 11:02:53, Train, Epoch : 4, Step : 1950, Loss : 0.41728, Acc : 0.831, Sensitive_Loss : 0.08214, Sensitive_Acc : 16.100, Run Time : 11.12 sec
INFO:root:2024-04-26 11:03:05, Train, Epoch : 4, Step : 1960, Loss : 0.29780, Acc : 0.859, Sensitive_Loss : 0.07413, Sensitive_Acc : 16.500, Run Time : 11.73 sec
INFO:root:2024-04-26 11:03:17, Train, Epoch : 4, Step : 1970, Loss : 0.30290, Acc : 0.878, Sensitive_Loss : 0.06694, Sensitive_Acc : 15.100, Run Time : 11.65 sec
INFO:root:2024-04-26 11:03:27, Train, Epoch : 4, Step : 1980, Loss : 0.35521, Acc : 0.847, Sensitive_Loss : 0.08256, Sensitive_Acc : 14.200, Run Time : 10.65 sec
INFO:root:2024-04-26 11:03:38, Train, Epoch : 4, Step : 1990, Loss : 0.38811, Acc : 0.825, Sensitive_Loss : 0.12707, Sensitive_Acc : 15.800, Run Time : 11.04 sec
INFO:root:2024-04-26 11:03:50, Train, Epoch : 4, Step : 2000, Loss : 0.33931, Acc : 0.859, Sensitive_Loss : 0.08668, Sensitive_Acc : 17.700, Run Time : 11.30 sec
INFO:root:2024-04-26 11:06:25, Dev, Step : 2000, Loss : 0.41222, Acc : 0.821, Auc : 0.906, Sensitive_Loss : 0.11991, Sensitive_Acc : 16.936, Sensitive_Auc : 0.984, Mean auc: 0.906, Run Time : 155.02 sec
INFO:root:2024-04-26 11:06:33, Train, Epoch : 4, Step : 2010, Loss : 0.30479, Acc : 0.881, Sensitive_Loss : 0.08267, Sensitive_Acc : 16.400, Run Time : 163.28 sec
INFO:root:2024-04-26 11:06:45, Train, Epoch : 4, Step : 2020, Loss : 0.34041, Acc : 0.863, Sensitive_Loss : 0.07332, Sensitive_Acc : 15.300, Run Time : 11.84 sec
INFO:root:2024-04-26 11:06:56, Train, Epoch : 4, Step : 2030, Loss : 0.34563, Acc : 0.859, Sensitive_Loss : 0.09937, Sensitive_Acc : 16.700, Run Time : 11.24 sec
INFO:root:2024-04-26 11:07:08, Train, Epoch : 4, Step : 2040, Loss : 0.40613, Acc : 0.816, Sensitive_Loss : 0.06073, Sensitive_Acc : 16.200, Run Time : 11.49 sec
INFO:root:2024-04-26 11:07:20, Train, Epoch : 4, Step : 2050, Loss : 0.34665, Acc : 0.844, Sensitive_Loss : 0.13302, Sensitive_Acc : 14.300, Run Time : 12.11 sec
INFO:root:2024-04-26 11:07:31, Train, Epoch : 4, Step : 2060, Loss : 0.33234, Acc : 0.850, Sensitive_Loss : 0.08323, Sensitive_Acc : 16.900, Run Time : 11.66 sec
INFO:root:2024-04-26 11:07:41, Train, Epoch : 4, Step : 2070, Loss : 0.31657, Acc : 0.866, Sensitive_Loss : 0.11672, Sensitive_Acc : 15.100, Run Time : 10.09 sec
INFO:root:2024-04-26 11:07:53, Train, Epoch : 4, Step : 2080, Loss : 0.40795, Acc : 0.853, Sensitive_Loss : 0.09019, Sensitive_Acc : 15.300, Run Time : 11.95 sec
INFO:root:2024-04-26 11:08:06, Train, Epoch : 4, Step : 2090, Loss : 0.39145, Acc : 0.841, Sensitive_Loss : 0.13705, Sensitive_Acc : 16.600, Run Time : 12.40 sec
INFO:root:2024-04-26 11:08:17, Train, Epoch : 4, Step : 2100, Loss : 0.37098, Acc : 0.863, Sensitive_Loss : 0.09491, Sensitive_Acc : 16.700, Run Time : 11.17 sec
INFO:root:2024-04-26 11:10:51, Dev, Step : 2100, Loss : 0.42042, Acc : 0.817, Auc : 0.903, Sensitive_Loss : 0.16041, Sensitive_Acc : 16.721, Sensitive_Auc : 0.981, Mean auc: 0.903, Run Time : 154.29 sec
INFO:root:2024-04-26 11:11:00, Train, Epoch : 4, Step : 2110, Loss : 0.39622, Acc : 0.812, Sensitive_Loss : 0.11839, Sensitive_Acc : 16.800, Run Time : 163.16 sec
INFO:root:2024-04-26 11:11:12, Train, Epoch : 4, Step : 2120, Loss : 0.31381, Acc : 0.863, Sensitive_Loss : 0.07632, Sensitive_Acc : 15.400, Run Time : 12.13 sec
INFO:root:2024-04-26 11:11:24, Train, Epoch : 4, Step : 2130, Loss : 0.40495, Acc : 0.844, Sensitive_Loss : 0.09814, Sensitive_Acc : 17.200, Run Time : 11.35 sec
INFO:root:2024-04-26 11:11:35, Train, Epoch : 4, Step : 2140, Loss : 0.33892, Acc : 0.850, Sensitive_Loss : 0.09993, Sensitive_Acc : 17.400, Run Time : 11.72 sec
INFO:root:2024-04-26 11:11:47, Train, Epoch : 4, Step : 2150, Loss : 0.30002, Acc : 0.878, Sensitive_Loss : 0.09011, Sensitive_Acc : 17.100, Run Time : 11.22 sec
INFO:root:2024-04-26 11:11:58, Train, Epoch : 4, Step : 2160, Loss : 0.29614, Acc : 0.853, Sensitive_Loss : 0.08847, Sensitive_Acc : 15.700, Run Time : 11.22 sec
INFO:root:2024-04-26 11:12:10, Train, Epoch : 4, Step : 2170, Loss : 0.45409, Acc : 0.794, Sensitive_Loss : 0.06758, Sensitive_Acc : 17.900, Run Time : 12.04 sec
INFO:root:2024-04-26 11:12:21, Train, Epoch : 4, Step : 2180, Loss : 0.29114, Acc : 0.872, Sensitive_Loss : 0.07973, Sensitive_Acc : 17.400, Run Time : 11.30 sec
INFO:root:2024-04-26 11:12:33, Train, Epoch : 4, Step : 2190, Loss : 0.43543, Acc : 0.809, Sensitive_Loss : 0.10002, Sensitive_Acc : 16.700, Run Time : 11.58 sec
INFO:root:2024-04-26 11:12:44, Train, Epoch : 4, Step : 2200, Loss : 0.28846, Acc : 0.881, Sensitive_Loss : 0.08694, Sensitive_Acc : 17.300, Run Time : 10.98 sec
INFO:root:2024-04-26 11:15:18, Dev, Step : 2200, Loss : 0.41706, Acc : 0.823, Auc : 0.905, Sensitive_Loss : 0.12920, Sensitive_Acc : 16.821, Sensitive_Auc : 0.985, Mean auc: 0.905, Run Time : 153.89 sec
INFO:root:2024-04-26 11:15:26, Train, Epoch : 4, Step : 2210, Loss : 0.30073, Acc : 0.881, Sensitive_Loss : 0.08551, Sensitive_Acc : 16.500, Run Time : 162.15 sec
INFO:root:2024-04-26 11:15:38, Train, Epoch : 4, Step : 2220, Loss : 0.32633, Acc : 0.866, Sensitive_Loss : 0.06708, Sensitive_Acc : 17.400, Run Time : 12.30 sec
INFO:root:2024-04-26 11:15:50, Train, Epoch : 4, Step : 2230, Loss : 0.37938, Acc : 0.844, Sensitive_Loss : 0.08593, Sensitive_Acc : 15.600, Run Time : 11.63 sec
INFO:root:2024-04-26 11:16:01, Train, Epoch : 4, Step : 2240, Loss : 0.33061, Acc : 0.863, Sensitive_Loss : 0.08302, Sensitive_Acc : 16.600, Run Time : 11.43 sec
INFO:root:2024-04-26 11:16:13, Train, Epoch : 4, Step : 2250, Loss : 0.35572, Acc : 0.841, Sensitive_Loss : 0.08001, Sensitive_Acc : 15.600, Run Time : 11.63 sec
INFO:root:2024-04-26 11:16:24, Train, Epoch : 4, Step : 2260, Loss : 0.37200, Acc : 0.816, Sensitive_Loss : 0.08304, Sensitive_Acc : 17.100, Run Time : 11.16 sec
INFO:root:2024-04-26 11:16:35, Train, Epoch : 4, Step : 2270, Loss : 0.38305, Acc : 0.828, Sensitive_Loss : 0.07005, Sensitive_Acc : 16.600, Run Time : 10.59 sec
INFO:root:2024-04-26 11:16:47, Train, Epoch : 4, Step : 2280, Loss : 0.31613, Acc : 0.863, Sensitive_Loss : 0.06976, Sensitive_Acc : 16.200, Run Time : 12.87 sec
INFO:root:2024-04-26 11:16:58, Train, Epoch : 4, Step : 2290, Loss : 0.36335, Acc : 0.844, Sensitive_Loss : 0.07938, Sensitive_Acc : 16.800, Run Time : 10.42 sec
INFO:root:2024-04-26 11:17:10, Train, Epoch : 4, Step : 2300, Loss : 0.32185, Acc : 0.869, Sensitive_Loss : 0.08301, Sensitive_Acc : 16.400, Run Time : 11.70 sec
INFO:root:2024-04-26 11:19:45, Dev, Step : 2300, Loss : 0.41936, Acc : 0.824, Auc : 0.908, Sensitive_Loss : 0.11148, Sensitive_Acc : 16.821, Sensitive_Auc : 0.985, Mean auc: 0.908, Run Time : 155.42 sec
INFO:root:2024-04-26 11:19:46, Best, Step : 2300, Loss : 0.41936, Acc : 0.824, Auc : 0.908, Sensitive_Loss : 0.11148, Sensitive_Acc : 16.821, Sensitive_Auc : 0.985, Best Auc : 0.908
INFO:root:2024-04-26 11:19:54, Train, Epoch : 4, Step : 2310, Loss : 0.36694, Acc : 0.853, Sensitive_Loss : 0.11211, Sensitive_Acc : 16.800, Run Time : 164.52 sec
INFO:root:2024-04-26 11:20:06, Train, Epoch : 4, Step : 2320, Loss : 0.35975, Acc : 0.847, Sensitive_Loss : 0.10251, Sensitive_Acc : 15.200, Run Time : 12.12 sec
INFO:root:2024-04-26 11:20:17, Train, Epoch : 4, Step : 2330, Loss : 0.34915, Acc : 0.869, Sensitive_Loss : 0.10510, Sensitive_Acc : 16.400, Run Time : 11.01 sec
INFO:root:2024-04-26 11:20:29, Train, Epoch : 4, Step : 2340, Loss : 0.28716, Acc : 0.875, Sensitive_Loss : 0.07819, Sensitive_Acc : 15.400, Run Time : 11.72 sec
INFO:root:2024-04-26 11:20:41, Train, Epoch : 4, Step : 2350, Loss : 0.31128, Acc : 0.875, Sensitive_Loss : 0.07788, Sensitive_Acc : 17.800, Run Time : 11.69 sec
INFO:root:2024-04-26 11:20:51, Train, Epoch : 4, Step : 2360, Loss : 0.29581, Acc : 0.863, Sensitive_Loss : 0.14771, Sensitive_Acc : 17.100, Run Time : 10.71 sec
INFO:root:2024-04-26 11:21:03, Train, Epoch : 4, Step : 2370, Loss : 0.35800, Acc : 0.822, Sensitive_Loss : 0.08236, Sensitive_Acc : 15.800, Run Time : 11.91 sec
INFO:root:2024-04-26 11:21:14, Train, Epoch : 4, Step : 2380, Loss : 0.35591, Acc : 0.825, Sensitive_Loss : 0.10497, Sensitive_Acc : 15.600, Run Time : 10.56 sec
INFO:root:2024-04-26 11:21:25, Train, Epoch : 4, Step : 2390, Loss : 0.33584, Acc : 0.847, Sensitive_Loss : 0.09082, Sensitive_Acc : 17.000, Run Time : 11.24 sec
INFO:root:2024-04-26 11:21:36, Train, Epoch : 4, Step : 2400, Loss : 0.34010, Acc : 0.869, Sensitive_Loss : 0.08564, Sensitive_Acc : 15.800, Run Time : 11.34 sec
INFO:root:2024-04-26 11:24:12, Dev, Step : 2400, Loss : 0.41775, Acc : 0.819, Auc : 0.907, Sensitive_Loss : 0.11441, Sensitive_Acc : 16.821, Sensitive_Auc : 0.985, Mean auc: 0.907, Run Time : 155.37 sec
INFO:root:2024-04-26 11:24:22, Train, Epoch : 4, Step : 2410, Loss : 0.29193, Acc : 0.872, Sensitive_Loss : 0.10624, Sensitive_Acc : 14.800, Run Time : 165.32 sec
INFO:root:2024-04-26 11:24:32, Train, Epoch : 4, Step : 2420, Loss : 0.25715, Acc : 0.900, Sensitive_Loss : 0.10002, Sensitive_Acc : 16.600, Run Time : 10.65 sec
INFO:root:2024-04-26 11:24:44, Train, Epoch : 4, Step : 2430, Loss : 0.33927, Acc : 0.866, Sensitive_Loss : 0.08420, Sensitive_Acc : 16.600, Run Time : 11.31 sec
INFO:root:2024-04-26 11:24:56, Train, Epoch : 4, Step : 2440, Loss : 0.30893, Acc : 0.884, Sensitive_Loss : 0.07768, Sensitive_Acc : 16.200, Run Time : 12.23 sec
INFO:root:2024-04-26 11:25:07, Train, Epoch : 4, Step : 2450, Loss : 0.32666, Acc : 0.872, Sensitive_Loss : 0.11755, Sensitive_Acc : 17.200, Run Time : 10.80 sec
INFO:root:2024-04-26 11:25:18, Train, Epoch : 4, Step : 2460, Loss : 0.36032, Acc : 0.803, Sensitive_Loss : 0.07139, Sensitive_Acc : 17.700, Run Time : 11.33 sec
INFO:root:2024-04-26 11:25:29, Train, Epoch : 4, Step : 2470, Loss : 0.31189, Acc : 0.894, Sensitive_Loss : 0.07945, Sensitive_Acc : 17.000, Run Time : 11.02 sec
INFO:root:2024-04-26 11:25:41, Train, Epoch : 4, Step : 2480, Loss : 0.37460, Acc : 0.850, Sensitive_Loss : 0.12441, Sensitive_Acc : 15.900, Run Time : 11.67 sec
INFO:root:2024-04-26 11:25:53, Train, Epoch : 4, Step : 2490, Loss : 0.34024, Acc : 0.853, Sensitive_Loss : 0.09296, Sensitive_Acc : 16.400, Run Time : 12.41 sec
INFO:root:2024-04-26 11:26:04, Train, Epoch : 4, Step : 2500, Loss : 0.37495, Acc : 0.831, Sensitive_Loss : 0.11550, Sensitive_Acc : 15.400, Run Time : 11.24 sec
INFO:root:2024-04-26 11:28:49, Dev, Step : 2500, Loss : 0.40609, Acc : 0.830, Auc : 0.908, Sensitive_Loss : 0.12226, Sensitive_Acc : 16.921, Sensitive_Auc : 0.985, Mean auc: 0.908, Run Time : 164.91 sec
INFO:root:2024-04-26 11:31:58
INFO:root:y_pred: [0.07084379 0.8648367  0.0230459  ... 0.8015474  0.00216699 0.6678823 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.8232299e-01 9.1571674e-05 6.9861319e-03 3.6447548e-06 9.9218607e-01
 1.6758220e-05 9.9920338e-01 9.9360812e-01 5.0585956e-04 9.6657389e-01
 9.9813336e-01 9.9888462e-01 9.9532074e-01 8.9244556e-01 2.2285136e-03
 9.3348759e-01 9.9911779e-01 1.2811079e-02 7.1153003e-01 9.6707445e-01
 9.8958659e-01 1.5812163e-01 9.9916458e-01 9.5385540e-01 9.9380940e-01
 8.8972890e-01 1.4434462e-04 9.9128997e-01 9.9069440e-01 9.0016752e-01
 5.4556876e-03 1.8510528e-01 2.0055350e-02 4.5809537e-02 1.8208116e-01
 1.1930198e-04 7.1732014e-02 3.4355333e-03 9.8629344e-01 9.9390018e-01
 2.1298834e-05 2.9676470e-03 9.9767035e-01 6.4628126e-05 9.9954128e-01
 9.9699652e-01 9.8249823e-01 9.9747509e-01 5.7332776e-02 9.8547089e-01
 9.9885654e-01 5.9234221e-03 1.6109343e-01 1.5679752e-05 2.0925805e-04
 6.1980989e-03 6.4235441e-02 6.5823472e-01 2.8890134e-03 1.1243594e-01
 2.3347260e-03 1.1778410e-01 6.8416402e-02 9.7959065e-01 4.0137149e-02
 9.9818462e-01 3.5097783e-03 9.9811077e-01 8.3433777e-01 8.1952447e-01
 8.6186934e-01 4.6524316e-01 6.5502332e-04 7.8629833e-03 7.2619201e-05
 1.4486194e-03 3.6822136e-02 6.3574600e-01 1.9229406e-04 9.9476272e-01
 9.9929571e-01 9.6476113e-05 2.7048883e-01 8.2856091e-04 9.9451232e-01
 9.5981252e-01 4.6643943e-02 8.6027076e-03 9.1726553e-01 9.9321508e-01
 9.9683785e-01 4.7497588e-01 3.5058467e-03 9.9528545e-01 5.2607560e-01
 1.0968542e-04 9.8456913e-01 9.5236349e-01 4.0700128e-05 4.9079899e-03
 9.8344153e-01 9.4996297e-01 9.9982208e-01 9.9614292e-01 1.6964773e-04
 2.6866794e-01 6.3422525e-01 9.0619314e-01 8.3856153e-01 1.4267219e-04
 8.1633407e-01 9.9265724e-01 7.9890341e-02 9.9820173e-01 9.7212845e-01
 9.9513948e-01 6.3071340e-01 9.9704808e-01 8.6482838e-02 2.7583632e-01
 9.9835891e-01 9.9465817e-01 6.3568550e-05 8.8999170e-01 9.9625766e-01
 9.1880046e-02 9.9350041e-01 2.3556511e-04 1.9073797e-02 9.8666048e-01
 9.9689096e-01 1.9100412e-04 7.4074441e-03 3.4027133e-02 9.9493837e-01
 9.9853098e-01 9.5782542e-01 2.6682514e-04 2.1078284e-03 9.9097437e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 11:31:58, Dev, Step : 2504, Loss : 0.40910, Acc : 0.829, Auc : 0.909, Sensitive_Loss : 0.11429, Sensitive_Acc : 16.950, Sensitive_Auc : 0.987, Mean auc: 0.909, Run Time : 186.34 sec
INFO:root:2024-04-26 11:31:59, Best, Step : 2504, Loss : 0.40910, Acc : 0.829,Auc : 0.909, Best Auc : 0.909, Sensitive_Loss : 0.11429, Sensitive_Acc : 16.950, Sensitive_Auc : 0.987
INFO:root:2024-04-26 11:32:08, Train, Epoch : 5, Step : 2510, Loss : 0.17695, Acc : 0.522, Sensitive_Loss : 0.06035, Sensitive_Acc : 9.500, Run Time : 8.29 sec
INFO:root:2024-04-26 11:32:19, Train, Epoch : 5, Step : 2520, Loss : 0.40975, Acc : 0.844, Sensitive_Loss : 0.08408, Sensitive_Acc : 15.400, Run Time : 11.18 sec
INFO:root:2024-04-26 11:32:31, Train, Epoch : 5, Step : 2530, Loss : 0.33412, Acc : 0.844, Sensitive_Loss : 0.08400, Sensitive_Acc : 15.600, Run Time : 11.80 sec
INFO:root:2024-04-26 11:32:43, Train, Epoch : 5, Step : 2540, Loss : 0.31951, Acc : 0.875, Sensitive_Loss : 0.06602, Sensitive_Acc : 15.800, Run Time : 11.57 sec
INFO:root:2024-04-26 11:32:54, Train, Epoch : 5, Step : 2550, Loss : 0.30484, Acc : 0.872, Sensitive_Loss : 0.10493, Sensitive_Acc : 14.800, Run Time : 11.47 sec
INFO:root:2024-04-26 11:33:05, Train, Epoch : 5, Step : 2560, Loss : 0.28559, Acc : 0.881, Sensitive_Loss : 0.09472, Sensitive_Acc : 15.900, Run Time : 11.13 sec
INFO:root:2024-04-26 11:33:17, Train, Epoch : 5, Step : 2570, Loss : 0.36045, Acc : 0.834, Sensitive_Loss : 0.09437, Sensitive_Acc : 16.600, Run Time : 11.48 sec
INFO:root:2024-04-26 11:33:28, Train, Epoch : 5, Step : 2580, Loss : 0.27710, Acc : 0.869, Sensitive_Loss : 0.08046, Sensitive_Acc : 17.100, Run Time : 11.01 sec
INFO:root:2024-04-26 11:33:39, Train, Epoch : 5, Step : 2590, Loss : 0.33334, Acc : 0.863, Sensitive_Loss : 0.11253, Sensitive_Acc : 14.900, Run Time : 11.67 sec
INFO:root:2024-04-26 11:33:50, Train, Epoch : 5, Step : 2600, Loss : 0.29600, Acc : 0.856, Sensitive_Loss : 0.06640, Sensitive_Acc : 15.200, Run Time : 10.96 sec
INFO:root:2024-04-26 11:36:27, Dev, Step : 2600, Loss : 0.41263, Acc : 0.828, Auc : 0.906, Sensitive_Loss : 0.13537, Sensitive_Acc : 16.936, Sensitive_Auc : 0.983, Mean auc: 0.906, Run Time : 156.51 sec
INFO:root:2024-04-26 11:36:35, Train, Epoch : 5, Step : 2610, Loss : 0.34862, Acc : 0.838, Sensitive_Loss : 0.09954, Sensitive_Acc : 17.800, Run Time : 164.64 sec
INFO:root:2024-04-26 11:36:46, Train, Epoch : 5, Step : 2620, Loss : 0.25845, Acc : 0.919, Sensitive_Loss : 0.06007, Sensitive_Acc : 16.300, Run Time : 11.12 sec
INFO:root:2024-04-26 11:36:58, Train, Epoch : 5, Step : 2630, Loss : 0.38872, Acc : 0.816, Sensitive_Loss : 0.08652, Sensitive_Acc : 13.900, Run Time : 11.55 sec
INFO:root:2024-04-26 11:37:09, Train, Epoch : 5, Step : 2640, Loss : 0.35633, Acc : 0.844, Sensitive_Loss : 0.10087, Sensitive_Acc : 16.800, Run Time : 11.35 sec
INFO:root:2024-04-26 11:37:20, Train, Epoch : 5, Step : 2650, Loss : 0.33840, Acc : 0.856, Sensitive_Loss : 0.06210, Sensitive_Acc : 15.300, Run Time : 11.18 sec
INFO:root:2024-04-26 11:37:31, Train, Epoch : 5, Step : 2660, Loss : 0.32258, Acc : 0.872, Sensitive_Loss : 0.09795, Sensitive_Acc : 16.500, Run Time : 10.35 sec
INFO:root:2024-04-26 11:37:42, Train, Epoch : 5, Step : 2670, Loss : 0.39780, Acc : 0.853, Sensitive_Loss : 0.08886, Sensitive_Acc : 15.900, Run Time : 11.71 sec
INFO:root:2024-04-26 11:37:53, Train, Epoch : 5, Step : 2680, Loss : 0.33042, Acc : 0.838, Sensitive_Loss : 0.11819, Sensitive_Acc : 15.800, Run Time : 10.79 sec
INFO:root:2024-04-26 11:38:05, Train, Epoch : 5, Step : 2690, Loss : 0.29708, Acc : 0.841, Sensitive_Loss : 0.07968, Sensitive_Acc : 17.200, Run Time : 11.73 sec
INFO:root:2024-04-26 11:38:17, Train, Epoch : 5, Step : 2700, Loss : 0.29453, Acc : 0.891, Sensitive_Loss : 0.08621, Sensitive_Acc : 15.700, Run Time : 12.05 sec
INFO:root:2024-04-26 11:40:55, Dev, Step : 2700, Loss : 0.39885, Acc : 0.830, Auc : 0.907, Sensitive_Loss : 0.10708, Sensitive_Acc : 16.821, Sensitive_Auc : 0.984, Mean auc: 0.907, Run Time : 158.38 sec
INFO:root:2024-04-26 11:41:05, Train, Epoch : 5, Step : 2710, Loss : 0.37455, Acc : 0.853, Sensitive_Loss : 0.06510, Sensitive_Acc : 16.900, Run Time : 168.01 sec
INFO:root:2024-04-26 11:41:18, Train, Epoch : 5, Step : 2720, Loss : 0.27417, Acc : 0.891, Sensitive_Loss : 0.07466, Sensitive_Acc : 16.400, Run Time : 13.06 sec
INFO:root:2024-04-26 11:41:31, Train, Epoch : 5, Step : 2730, Loss : 0.38801, Acc : 0.853, Sensitive_Loss : 0.08486, Sensitive_Acc : 16.700, Run Time : 13.56 sec
INFO:root:2024-04-26 11:41:44, Train, Epoch : 5, Step : 2740, Loss : 0.42627, Acc : 0.841, Sensitive_Loss : 0.09115, Sensitive_Acc : 15.600, Run Time : 12.32 sec
INFO:root:2024-04-26 11:41:57, Train, Epoch : 5, Step : 2750, Loss : 0.35493, Acc : 0.853, Sensitive_Loss : 0.09376, Sensitive_Acc : 16.000, Run Time : 13.63 sec
INFO:root:2024-04-26 11:42:10, Train, Epoch : 5, Step : 2760, Loss : 0.31057, Acc : 0.863, Sensitive_Loss : 0.09883, Sensitive_Acc : 16.500, Run Time : 13.10 sec
INFO:root:2024-04-26 11:42:23, Train, Epoch : 5, Step : 2770, Loss : 0.30073, Acc : 0.853, Sensitive_Loss : 0.09341, Sensitive_Acc : 15.800, Run Time : 12.77 sec
INFO:root:2024-04-26 11:42:36, Train, Epoch : 5, Step : 2780, Loss : 0.35888, Acc : 0.844, Sensitive_Loss : 0.07043, Sensitive_Acc : 16.500, Run Time : 13.22 sec
INFO:root:2024-04-26 11:42:49, Train, Epoch : 5, Step : 2790, Loss : 0.35366, Acc : 0.850, Sensitive_Loss : 0.09822, Sensitive_Acc : 15.400, Run Time : 12.69 sec
INFO:root:2024-04-26 11:43:02, Train, Epoch : 5, Step : 2800, Loss : 0.40737, Acc : 0.859, Sensitive_Loss : 0.06760, Sensitive_Acc : 17.000, Run Time : 12.76 sec
INFO:root:2024-04-26 11:45:40, Dev, Step : 2800, Loss : 0.41706, Acc : 0.821, Auc : 0.907, Sensitive_Loss : 0.11800, Sensitive_Acc : 16.850, Sensitive_Auc : 0.983, Mean auc: 0.907, Run Time : 158.02 sec
INFO:root:2024-04-26 11:45:50, Train, Epoch : 5, Step : 2810, Loss : 0.29444, Acc : 0.853, Sensitive_Loss : 0.11927, Sensitive_Acc : 16.100, Run Time : 167.75 sec
INFO:root:2024-04-26 11:46:08, Train, Epoch : 5, Step : 2820, Loss : 0.37765, Acc : 0.853, Sensitive_Loss : 0.08108, Sensitive_Acc : 17.400, Run Time : 18.07 sec
INFO:root:2024-04-26 11:46:22, Train, Epoch : 5, Step : 2830, Loss : 0.31759, Acc : 0.863, Sensitive_Loss : 0.08081, Sensitive_Acc : 15.700, Run Time : 14.18 sec
INFO:root:2024-04-26 11:46:38, Train, Epoch : 5, Step : 2840, Loss : 0.28492, Acc : 0.872, Sensitive_Loss : 0.06519, Sensitive_Acc : 17.100, Run Time : 16.53 sec
INFO:root:2024-04-26 11:46:53, Train, Epoch : 5, Step : 2850, Loss : 0.36255, Acc : 0.838, Sensitive_Loss : 0.08415, Sensitive_Acc : 16.600, Run Time : 14.74 sec
INFO:root:2024-04-26 11:47:10, Train, Epoch : 5, Step : 2860, Loss : 0.29448, Acc : 0.847, Sensitive_Loss : 0.09577, Sensitive_Acc : 16.400, Run Time : 16.60 sec
INFO:root:2024-04-26 11:47:25, Train, Epoch : 5, Step : 2870, Loss : 0.32347, Acc : 0.878, Sensitive_Loss : 0.07460, Sensitive_Acc : 16.500, Run Time : 15.65 sec
INFO:root:2024-04-26 11:47:39, Train, Epoch : 5, Step : 2880, Loss : 0.36407, Acc : 0.844, Sensitive_Loss : 0.09953, Sensitive_Acc : 15.100, Run Time : 13.21 sec
INFO:root:2024-04-26 11:47:52, Train, Epoch : 5, Step : 2890, Loss : 0.36933, Acc : 0.850, Sensitive_Loss : 0.09697, Sensitive_Acc : 16.900, Run Time : 13.21 sec
INFO:root:2024-04-26 11:48:04, Train, Epoch : 5, Step : 2900, Loss : 0.45428, Acc : 0.825, Sensitive_Loss : 0.10183, Sensitive_Acc : 16.200, Run Time : 12.20 sec
INFO:root:2024-04-26 11:50:43, Dev, Step : 2900, Loss : 0.41241, Acc : 0.828, Auc : 0.908, Sensitive_Loss : 0.11386, Sensitive_Acc : 16.821, Sensitive_Auc : 0.986, Mean auc: 0.908, Run Time : 158.82 sec
INFO:root:2024-04-26 11:50:51, Train, Epoch : 5, Step : 2910, Loss : 0.31673, Acc : 0.863, Sensitive_Loss : 0.08307, Sensitive_Acc : 16.900, Run Time : 167.04 sec
INFO:root:2024-04-26 11:51:04, Train, Epoch : 5, Step : 2920, Loss : 0.27847, Acc : 0.872, Sensitive_Loss : 0.05359, Sensitive_Acc : 15.200, Run Time : 13.26 sec
INFO:root:2024-04-26 11:51:17, Train, Epoch : 5, Step : 2930, Loss : 0.31174, Acc : 0.850, Sensitive_Loss : 0.06847, Sensitive_Acc : 16.400, Run Time : 12.62 sec
INFO:root:2024-04-26 11:51:28, Train, Epoch : 5, Step : 2940, Loss : 0.33814, Acc : 0.881, Sensitive_Loss : 0.09683, Sensitive_Acc : 16.900, Run Time : 11.16 sec
INFO:root:2024-04-26 11:51:40, Train, Epoch : 5, Step : 2950, Loss : 0.32711, Acc : 0.866, Sensitive_Loss : 0.08678, Sensitive_Acc : 15.600, Run Time : 12.06 sec
INFO:root:2024-04-26 11:51:52, Train, Epoch : 5, Step : 2960, Loss : 0.35008, Acc : 0.856, Sensitive_Loss : 0.09616, Sensitive_Acc : 16.200, Run Time : 11.93 sec
INFO:root:2024-04-26 11:52:04, Train, Epoch : 5, Step : 2970, Loss : 0.28965, Acc : 0.872, Sensitive_Loss : 0.06736, Sensitive_Acc : 16.400, Run Time : 11.89 sec
INFO:root:2024-04-26 11:52:16, Train, Epoch : 5, Step : 2980, Loss : 0.33704, Acc : 0.834, Sensitive_Loss : 0.06601, Sensitive_Acc : 15.700, Run Time : 11.76 sec
INFO:root:2024-04-26 11:52:27, Train, Epoch : 5, Step : 2990, Loss : 0.31601, Acc : 0.853, Sensitive_Loss : 0.06949, Sensitive_Acc : 15.800, Run Time : 11.69 sec
INFO:root:2024-04-26 11:52:40, Train, Epoch : 5, Step : 3000, Loss : 0.35744, Acc : 0.850, Sensitive_Loss : 0.08188, Sensitive_Acc : 16.400, Run Time : 12.23 sec
INFO:root:2024-04-26 11:55:26, Dev, Step : 3000, Loss : 0.40974, Acc : 0.826, Auc : 0.907, Sensitive_Loss : 0.10668, Sensitive_Acc : 16.821, Sensitive_Auc : 0.986, Mean auc: 0.907, Run Time : 166.51 sec
INFO:root:2024-04-26 11:55:36, Train, Epoch : 5, Step : 3010, Loss : 0.32741, Acc : 0.863, Sensitive_Loss : 0.07334, Sensitive_Acc : 16.400, Run Time : 176.12 sec
INFO:root:2024-04-26 11:55:49, Train, Epoch : 5, Step : 3020, Loss : 0.36321, Acc : 0.841, Sensitive_Loss : 0.08661, Sensitive_Acc : 15.100, Run Time : 12.90 sec
INFO:root:2024-04-26 11:56:02, Train, Epoch : 5, Step : 3030, Loss : 0.33262, Acc : 0.850, Sensitive_Loss : 0.09627, Sensitive_Acc : 17.300, Run Time : 13.71 sec
INFO:root:2024-04-26 11:56:14, Train, Epoch : 5, Step : 3040, Loss : 0.31374, Acc : 0.856, Sensitive_Loss : 0.09534, Sensitive_Acc : 15.300, Run Time : 11.86 sec
INFO:root:2024-04-26 11:56:26, Train, Epoch : 5, Step : 3050, Loss : 0.32511, Acc : 0.872, Sensitive_Loss : 0.08844, Sensitive_Acc : 17.600, Run Time : 11.71 sec
INFO:root:2024-04-26 11:56:38, Train, Epoch : 5, Step : 3060, Loss : 0.30009, Acc : 0.841, Sensitive_Loss : 0.07002, Sensitive_Acc : 17.300, Run Time : 12.24 sec
INFO:root:2024-04-26 11:56:51, Train, Epoch : 5, Step : 3070, Loss : 0.32058, Acc : 0.869, Sensitive_Loss : 0.07988, Sensitive_Acc : 17.400, Run Time : 12.31 sec
INFO:root:2024-04-26 11:57:02, Train, Epoch : 5, Step : 3080, Loss : 0.36462, Acc : 0.841, Sensitive_Loss : 0.05270, Sensitive_Acc : 15.300, Run Time : 11.86 sec
INFO:root:2024-04-26 11:57:13, Train, Epoch : 5, Step : 3090, Loss : 0.30894, Acc : 0.828, Sensitive_Loss : 0.10971, Sensitive_Acc : 16.900, Run Time : 11.04 sec
INFO:root:2024-04-26 11:57:24, Train, Epoch : 5, Step : 3100, Loss : 0.35782, Acc : 0.863, Sensitive_Loss : 0.08742, Sensitive_Acc : 15.800, Run Time : 11.01 sec
INFO:root:2024-04-26 12:00:08, Dev, Step : 3100, Loss : 0.40075, Acc : 0.829, Auc : 0.907, Sensitive_Loss : 0.11326, Sensitive_Acc : 16.921, Sensitive_Auc : 0.988, Mean auc: 0.907, Run Time : 163.10 sec
INFO:root:2024-04-26 12:00:16, Train, Epoch : 5, Step : 3110, Loss : 0.31292, Acc : 0.850, Sensitive_Loss : 0.06750, Sensitive_Acc : 17.500, Run Time : 171.53 sec
INFO:root:2024-04-26 12:00:28, Train, Epoch : 5, Step : 3120, Loss : 0.28101, Acc : 0.856, Sensitive_Loss : 0.09670, Sensitive_Acc : 15.700, Run Time : 11.59 sec
INFO:root:2024-04-26 12:00:38, Train, Epoch : 5, Step : 3130, Loss : 0.26939, Acc : 0.881, Sensitive_Loss : 0.04810, Sensitive_Acc : 16.700, Run Time : 10.73 sec
INFO:root:2024-04-26 12:03:20
INFO:root:y_pred: [0.12974991 0.9196445  0.02497609 ... 0.8305562  0.00383511 0.64844817]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.80974376e-01 1.20299264e-04 5.30392211e-03 2.29026614e-06
 9.88086343e-01 4.00807494e-06 9.98682082e-01 9.91334975e-01
 1.04423577e-03 9.19030190e-01 9.97474372e-01 9.97989893e-01
 9.94044840e-01 8.57330978e-01 9.19216021e-04 9.28460360e-01
 9.99217391e-01 5.27057284e-03 7.78132200e-01 9.85519648e-01
 9.89974022e-01 3.00970167e-01 9.99275863e-01 9.53328788e-01
 9.95019674e-01 9.09141660e-01 1.15659757e-04 9.87183988e-01
 9.86255944e-01 8.58985126e-01 2.45132833e-03 1.38955384e-01
 1.62982121e-02 3.06817740e-02 2.86730856e-01 4.11094770e-05
 2.68730968e-02 9.34134412e-04 9.84335005e-01 9.91543174e-01
 7.99669670e-06 1.21563277e-03 9.97387946e-01 3.13853270e-05
 9.99662757e-01 9.96765256e-01 9.73870158e-01 9.97407854e-01
 6.24101870e-02 9.70651925e-01 9.99128401e-01 1.25931262e-03
 2.09769264e-01 6.56590419e-06 9.46733635e-05 5.30501409e-03
 1.16929613e-01 7.40720272e-01 1.99606875e-03 2.48680204e-01
 2.09936732e-03 1.17888421e-01 2.23088060e-02 9.76336479e-01
 1.08685195e-02 9.98781264e-01 6.42346591e-03 9.97816205e-01
 8.24163258e-01 5.41177928e-01 8.99317682e-01 3.93319339e-01
 6.51504786e-04 6.87299948e-03 5.45689290e-05 2.92640383e-04
 5.49918674e-02 6.23114944e-01 2.53432401e-04 9.94396448e-01
 9.99003232e-01 2.23361894e-05 2.48492628e-01 5.26993943e-04
 9.94488955e-01 9.34439361e-01 8.02068505e-03 3.22781876e-03
 8.59028220e-01 9.93731022e-01 9.96389031e-01 3.59453589e-01
 5.74126374e-04 9.96137083e-01 5.36433876e-01 1.23278151e-04
 9.81285691e-01 9.55012679e-01 2.23058978e-05 1.70467782e-03
 9.72476780e-01 9.44332540e-01 9.99764025e-01 9.97844458e-01
 2.35724612e-04 9.11273509e-02 6.27710342e-01 8.58453631e-01
 8.59244466e-01 5.78213949e-05 8.22420716e-01 9.90829349e-01
 6.25608191e-02 9.97331023e-01 9.61440742e-01 9.92065251e-01
 6.59584939e-01 9.97367084e-01 7.41726011e-02 3.59661281e-01
 9.98278499e-01 9.93548095e-01 5.23806375e-05 8.84707391e-01
 9.92899895e-01 7.39642307e-02 9.97093081e-01 2.16467466e-04
 2.10768171e-02 9.83615220e-01 9.95352864e-01 3.03196459e-04
 5.93968071e-02 2.45448258e-02 9.95463073e-01 9.99132216e-01
 9.55801606e-01 4.83125303e-04 1.83459348e-03 9.92852628e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 12:03:20, Dev, Step : 3130, Loss : 0.40784, Acc : 0.827, Auc : 0.907, Sensitive_Loss : 0.11634, Sensitive_Acc : 16.950, Sensitive_Auc : 0.987, Mean auc: 0.907, Run Time : 161.73 sec
INFO:root:2024-04-26 12:03:35, Train, Epoch : 6, Step : 3140, Loss : 0.28970, Acc : 0.875, Sensitive_Loss : 0.07990, Sensitive_Acc : 15.200, Run Time : 13.92 sec
INFO:root:2024-04-26 12:03:47, Train, Epoch : 6, Step : 3150, Loss : 0.29435, Acc : 0.903, Sensitive_Loss : 0.09162, Sensitive_Acc : 17.100, Run Time : 11.57 sec
INFO:root:2024-04-26 12:03:57, Train, Epoch : 6, Step : 3160, Loss : 0.27378, Acc : 0.878, Sensitive_Loss : 0.10917, Sensitive_Acc : 15.900, Run Time : 10.48 sec
INFO:root:2024-04-26 12:04:09, Train, Epoch : 6, Step : 3170, Loss : 0.28489, Acc : 0.897, Sensitive_Loss : 0.09103, Sensitive_Acc : 17.200, Run Time : 12.11 sec
INFO:root:2024-04-26 12:04:20, Train, Epoch : 6, Step : 3180, Loss : 0.30130, Acc : 0.850, Sensitive_Loss : 0.11742, Sensitive_Acc : 16.900, Run Time : 11.15 sec
INFO:root:2024-04-26 12:04:32, Train, Epoch : 6, Step : 3190, Loss : 0.33586, Acc : 0.872, Sensitive_Loss : 0.07649, Sensitive_Acc : 16.400, Run Time : 11.60 sec
INFO:root:2024-04-26 12:04:43, Train, Epoch : 6, Step : 3200, Loss : 0.32609, Acc : 0.866, Sensitive_Loss : 0.07779, Sensitive_Acc : 17.500, Run Time : 10.98 sec
INFO:root:2024-04-26 12:07:25, Dev, Step : 3200, Loss : 0.41744, Acc : 0.830, Auc : 0.910, Sensitive_Loss : 0.10804, Sensitive_Acc : 16.964, Sensitive_Auc : 0.990, Mean auc: 0.910, Run Time : 162.16 sec
INFO:root:2024-04-26 12:07:26, Best, Step : 3200, Loss : 0.41744, Acc : 0.830, Auc : 0.910, Sensitive_Loss : 0.10804, Sensitive_Acc : 16.964, Sensitive_Auc : 0.990, Best Auc : 0.910
INFO:root:2024-04-26 12:07:35, Train, Epoch : 6, Step : 3210, Loss : 0.32533, Acc : 0.881, Sensitive_Loss : 0.06257, Sensitive_Acc : 15.600, Run Time : 171.65 sec
INFO:root:2024-04-26 12:07:46, Train, Epoch : 6, Step : 3220, Loss : 0.35362, Acc : 0.850, Sensitive_Loss : 0.11145, Sensitive_Acc : 17.300, Run Time : 11.18 sec
INFO:root:2024-04-26 12:07:58, Train, Epoch : 6, Step : 3230, Loss : 0.31818, Acc : 0.850, Sensitive_Loss : 0.07797, Sensitive_Acc : 16.600, Run Time : 12.09 sec
INFO:root:2024-04-26 12:08:10, Train, Epoch : 6, Step : 3240, Loss : 0.27694, Acc : 0.884, Sensitive_Loss : 0.09286, Sensitive_Acc : 16.500, Run Time : 11.79 sec
INFO:root:2024-04-26 12:08:21, Train, Epoch : 6, Step : 3250, Loss : 0.33869, Acc : 0.863, Sensitive_Loss : 0.07997, Sensitive_Acc : 17.800, Run Time : 11.28 sec
INFO:root:2024-04-26 12:08:33, Train, Epoch : 6, Step : 3260, Loss : 0.35366, Acc : 0.856, Sensitive_Loss : 0.07521, Sensitive_Acc : 16.300, Run Time : 11.97 sec
INFO:root:2024-04-26 12:08:45, Train, Epoch : 6, Step : 3270, Loss : 0.29707, Acc : 0.878, Sensitive_Loss : 0.06949, Sensitive_Acc : 17.500, Run Time : 12.04 sec
INFO:root:2024-04-26 12:08:56, Train, Epoch : 6, Step : 3280, Loss : 0.34660, Acc : 0.866, Sensitive_Loss : 0.07993, Sensitive_Acc : 16.200, Run Time : 11.04 sec
INFO:root:2024-04-26 12:09:08, Train, Epoch : 6, Step : 3290, Loss : 0.33446, Acc : 0.850, Sensitive_Loss : 0.06966, Sensitive_Acc : 17.100, Run Time : 11.96 sec
INFO:root:2024-04-26 12:09:20, Train, Epoch : 6, Step : 3300, Loss : 0.30264, Acc : 0.856, Sensitive_Loss : 0.06556, Sensitive_Acc : 16.900, Run Time : 11.71 sec
INFO:root:2024-04-26 12:12:02, Dev, Step : 3300, Loss : 0.41808, Acc : 0.820, Auc : 0.906, Sensitive_Loss : 0.13218, Sensitive_Acc : 16.950, Sensitive_Auc : 0.986, Mean auc: 0.906, Run Time : 162.47 sec
INFO:root:2024-04-26 12:12:11, Train, Epoch : 6, Step : 3310, Loss : 0.33966, Acc : 0.863, Sensitive_Loss : 0.07202, Sensitive_Acc : 16.200, Run Time : 170.80 sec
INFO:root:2024-04-26 12:12:24, Train, Epoch : 6, Step : 3320, Loss : 0.30747, Acc : 0.856, Sensitive_Loss : 0.07196, Sensitive_Acc : 16.700, Run Time : 13.21 sec
INFO:root:2024-04-26 12:12:37, Train, Epoch : 6, Step : 3330, Loss : 0.30443, Acc : 0.856, Sensitive_Loss : 0.13888, Sensitive_Acc : 16.500, Run Time : 12.96 sec
INFO:root:2024-04-26 12:12:49, Train, Epoch : 6, Step : 3340, Loss : 0.31191, Acc : 0.872, Sensitive_Loss : 0.08951, Sensitive_Acc : 16.000, Run Time : 12.58 sec
INFO:root:2024-04-26 12:13:01, Train, Epoch : 6, Step : 3350, Loss : 0.34197, Acc : 0.863, Sensitive_Loss : 0.07476, Sensitive_Acc : 16.600, Run Time : 12.21 sec
INFO:root:2024-04-26 12:13:14, Train, Epoch : 6, Step : 3360, Loss : 0.28772, Acc : 0.887, Sensitive_Loss : 0.07038, Sensitive_Acc : 17.900, Run Time : 12.35 sec
INFO:root:2024-04-26 12:13:27, Train, Epoch : 6, Step : 3370, Loss : 0.25599, Acc : 0.878, Sensitive_Loss : 0.09835, Sensitive_Acc : 17.200, Run Time : 12.86 sec
INFO:root:2024-04-26 12:13:40, Train, Epoch : 6, Step : 3380, Loss : 0.35150, Acc : 0.847, Sensitive_Loss : 0.06817, Sensitive_Acc : 15.600, Run Time : 12.87 sec
INFO:root:2024-04-26 12:13:51, Train, Epoch : 6, Step : 3390, Loss : 0.30828, Acc : 0.847, Sensitive_Loss : 0.12189, Sensitive_Acc : 16.500, Run Time : 11.62 sec
INFO:root:2024-04-26 12:14:03, Train, Epoch : 6, Step : 3400, Loss : 0.44267, Acc : 0.841, Sensitive_Loss : 0.08289, Sensitive_Acc : 14.800, Run Time : 12.22 sec
INFO:root:2024-04-26 12:16:41, Dev, Step : 3400, Loss : 0.41363, Acc : 0.822, Auc : 0.905, Sensitive_Loss : 0.11693, Sensitive_Acc : 16.893, Sensitive_Auc : 0.987, Mean auc: 0.905, Run Time : 157.95 sec
INFO:root:2024-04-26 12:16:50, Train, Epoch : 6, Step : 3410, Loss : 0.35527, Acc : 0.863, Sensitive_Loss : 0.09428, Sensitive_Acc : 15.100, Run Time : 166.71 sec
INFO:root:2024-04-26 12:17:02, Train, Epoch : 6, Step : 3420, Loss : 0.30875, Acc : 0.881, Sensitive_Loss : 0.08794, Sensitive_Acc : 15.600, Run Time : 11.92 sec
INFO:root:2024-04-26 12:17:14, Train, Epoch : 6, Step : 3430, Loss : 0.28589, Acc : 0.869, Sensitive_Loss : 0.09309, Sensitive_Acc : 15.200, Run Time : 12.29 sec
INFO:root:2024-04-26 12:17:26, Train, Epoch : 6, Step : 3440, Loss : 0.36311, Acc : 0.819, Sensitive_Loss : 0.06759, Sensitive_Acc : 17.400, Run Time : 11.82 sec
INFO:root:2024-04-26 12:17:38, Train, Epoch : 6, Step : 3450, Loss : 0.30445, Acc : 0.881, Sensitive_Loss : 0.07587, Sensitive_Acc : 16.500, Run Time : 12.27 sec
INFO:root:2024-04-26 12:17:51, Train, Epoch : 6, Step : 3460, Loss : 0.29508, Acc : 0.875, Sensitive_Loss : 0.07647, Sensitive_Acc : 17.500, Run Time : 12.20 sec
INFO:root:2024-04-26 12:18:03, Train, Epoch : 6, Step : 3470, Loss : 0.35222, Acc : 0.869, Sensitive_Loss : 0.07643, Sensitive_Acc : 16.400, Run Time : 12.33 sec
INFO:root:2024-04-26 12:18:15, Train, Epoch : 6, Step : 3480, Loss : 0.28674, Acc : 0.872, Sensitive_Loss : 0.06761, Sensitive_Acc : 17.400, Run Time : 11.71 sec
INFO:root:2024-04-26 12:18:26, Train, Epoch : 6, Step : 3490, Loss : 0.30209, Acc : 0.891, Sensitive_Loss : 0.08663, Sensitive_Acc : 15.800, Run Time : 11.72 sec
INFO:root:2024-04-26 12:18:39, Train, Epoch : 6, Step : 3500, Loss : 0.30077, Acc : 0.881, Sensitive_Loss : 0.09675, Sensitive_Acc : 16.000, Run Time : 12.73 sec
INFO:root:2024-04-26 12:21:18, Dev, Step : 3500, Loss : 0.40393, Acc : 0.829, Auc : 0.906, Sensitive_Loss : 0.10669, Sensitive_Acc : 16.707, Sensitive_Auc : 0.987, Mean auc: 0.906, Run Time : 158.88 sec
INFO:root:2024-04-26 12:21:26, Train, Epoch : 6, Step : 3510, Loss : 0.37967, Acc : 0.853, Sensitive_Loss : 0.08277, Sensitive_Acc : 15.900, Run Time : 167.15 sec
INFO:root:2024-04-26 12:21:39, Train, Epoch : 6, Step : 3520, Loss : 0.29021, Acc : 0.903, Sensitive_Loss : 0.10446, Sensitive_Acc : 16.400, Run Time : 12.49 sec
INFO:root:2024-04-26 12:21:50, Train, Epoch : 6, Step : 3530, Loss : 0.26522, Acc : 0.869, Sensitive_Loss : 0.07880, Sensitive_Acc : 18.100, Run Time : 11.37 sec
INFO:root:2024-04-26 12:22:02, Train, Epoch : 6, Step : 3540, Loss : 0.30450, Acc : 0.891, Sensitive_Loss : 0.06844, Sensitive_Acc : 16.300, Run Time : 12.26 sec
INFO:root:2024-04-26 12:22:14, Train, Epoch : 6, Step : 3550, Loss : 0.26682, Acc : 0.884, Sensitive_Loss : 0.08660, Sensitive_Acc : 17.300, Run Time : 12.08 sec
INFO:root:2024-04-26 12:22:26, Train, Epoch : 6, Step : 3560, Loss : 0.36461, Acc : 0.828, Sensitive_Loss : 0.07283, Sensitive_Acc : 15.100, Run Time : 11.25 sec
INFO:root:2024-04-26 12:22:38, Train, Epoch : 6, Step : 3570, Loss : 0.30291, Acc : 0.856, Sensitive_Loss : 0.05761, Sensitive_Acc : 17.300, Run Time : 12.25 sec
INFO:root:2024-04-26 12:22:50, Train, Epoch : 6, Step : 3580, Loss : 0.31824, Acc : 0.869, Sensitive_Loss : 0.09016, Sensitive_Acc : 15.800, Run Time : 11.87 sec
INFO:root:2024-04-26 12:23:02, Train, Epoch : 6, Step : 3590, Loss : 0.28111, Acc : 0.869, Sensitive_Loss : 0.05730, Sensitive_Acc : 15.500, Run Time : 12.15 sec
INFO:root:2024-04-26 12:23:14, Train, Epoch : 6, Step : 3600, Loss : 0.31523, Acc : 0.856, Sensitive_Loss : 0.08366, Sensitive_Acc : 16.700, Run Time : 12.01 sec
INFO:root:2024-04-26 12:26:34, Dev, Step : 3600, Loss : 0.44078, Acc : 0.819, Auc : 0.904, Sensitive_Loss : 0.13327, Sensitive_Acc : 16.807, Sensitive_Auc : 0.985, Mean auc: 0.904, Run Time : 199.64 sec
INFO:root:2024-04-26 12:26:43, Train, Epoch : 6, Step : 3610, Loss : 0.36702, Acc : 0.847, Sensitive_Loss : 0.04805, Sensitive_Acc : 16.700, Run Time : 208.68 sec
INFO:root:2024-04-26 12:26:54, Train, Epoch : 6, Step : 3620, Loss : 0.30236, Acc : 0.859, Sensitive_Loss : 0.09220, Sensitive_Acc : 16.500, Run Time : 11.41 sec
INFO:root:2024-04-26 12:27:06, Train, Epoch : 6, Step : 3630, Loss : 0.28753, Acc : 0.891, Sensitive_Loss : 0.07551, Sensitive_Acc : 16.300, Run Time : 11.79 sec
INFO:root:2024-04-26 12:27:21, Train, Epoch : 6, Step : 3640, Loss : 0.31472, Acc : 0.875, Sensitive_Loss : 0.10956, Sensitive_Acc : 15.100, Run Time : 15.11 sec
INFO:root:2024-04-26 12:27:38, Train, Epoch : 6, Step : 3650, Loss : 0.26237, Acc : 0.897, Sensitive_Loss : 0.09571, Sensitive_Acc : 16.300, Run Time : 16.88 sec
INFO:root:2024-04-26 12:27:49, Train, Epoch : 6, Step : 3660, Loss : 0.29750, Acc : 0.887, Sensitive_Loss : 0.07443, Sensitive_Acc : 17.700, Run Time : 11.10 sec
INFO:root:2024-04-26 12:28:01, Train, Epoch : 6, Step : 3670, Loss : 0.30874, Acc : 0.872, Sensitive_Loss : 0.05585, Sensitive_Acc : 16.700, Run Time : 12.14 sec
INFO:root:2024-04-26 12:28:13, Train, Epoch : 6, Step : 3680, Loss : 0.32732, Acc : 0.875, Sensitive_Loss : 0.09192, Sensitive_Acc : 16.200, Run Time : 11.96 sec
INFO:root:2024-04-26 12:28:25, Train, Epoch : 6, Step : 3690, Loss : 0.26545, Acc : 0.863, Sensitive_Loss : 0.09524, Sensitive_Acc : 17.000, Run Time : 11.89 sec
INFO:root:2024-04-26 12:28:37, Train, Epoch : 6, Step : 3700, Loss : 0.29410, Acc : 0.869, Sensitive_Loss : 0.11146, Sensitive_Acc : 15.700, Run Time : 12.46 sec
INFO:root:2024-04-26 12:31:17, Dev, Step : 3700, Loss : 0.43900, Acc : 0.821, Auc : 0.905, Sensitive_Loss : 0.13051, Sensitive_Acc : 16.864, Sensitive_Auc : 0.987, Mean auc: 0.905, Run Time : 159.51 sec
INFO:root:2024-04-26 12:31:26, Train, Epoch : 6, Step : 3710, Loss : 0.36075, Acc : 0.828, Sensitive_Loss : 0.08686, Sensitive_Acc : 17.400, Run Time : 168.59 sec
INFO:root:2024-04-26 12:31:40, Train, Epoch : 6, Step : 3720, Loss : 0.35792, Acc : 0.812, Sensitive_Loss : 0.08667, Sensitive_Acc : 14.900, Run Time : 13.57 sec
INFO:root:2024-04-26 12:31:51, Train, Epoch : 6, Step : 3730, Loss : 0.27444, Acc : 0.875, Sensitive_Loss : 0.10714, Sensitive_Acc : 16.500, Run Time : 11.51 sec
INFO:root:2024-04-26 12:32:03, Train, Epoch : 6, Step : 3740, Loss : 0.29348, Acc : 0.856, Sensitive_Loss : 0.06465, Sensitive_Acc : 16.700, Run Time : 12.33 sec
INFO:root:2024-04-26 12:32:15, Train, Epoch : 6, Step : 3750, Loss : 0.33164, Acc : 0.853, Sensitive_Loss : 0.06419, Sensitive_Acc : 15.400, Run Time : 11.07 sec
INFO:root:2024-04-26 12:34:58
INFO:root:y_pred: [0.10680085 0.9378821  0.02211061 ... 0.78102577 0.00218106 0.73021233]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.8186469e-01 3.1465435e-05 9.6604172e-03 4.7412054e-06 9.8443145e-01
 3.5638682e-06 9.9786973e-01 9.9283409e-01 2.9804767e-04 9.6121567e-01
 9.9434632e-01 9.9862802e-01 9.9664634e-01 8.3568013e-01 1.1009004e-03
 9.4713348e-01 9.9979120e-01 2.4900187e-03 8.0564904e-01 9.9736965e-01
 9.9362659e-01 8.0997571e-03 9.9960536e-01 9.4904822e-01 9.9522483e-01
 8.6585784e-01 1.3034801e-04 9.8572659e-01 9.9055147e-01 8.7099189e-01
 1.1770558e-03 1.4481559e-01 3.7239812e-02 1.0348693e-02 4.2601982e-03
 2.6442916e-05 6.9236294e-03 1.0899978e-03 9.7180068e-01 9.9722493e-01
 4.7817825e-06 4.1395193e-04 9.9886382e-01 2.5293180e-05 9.9965119e-01
 9.9455404e-01 9.7763914e-01 9.9792945e-01 2.1207195e-02 9.8018485e-01
 9.9945730e-01 1.4885708e-03 2.1512918e-01 1.3156360e-05 6.8227790e-05
 5.2561560e-03 5.5859797e-02 2.8193268e-01 4.2618127e-04 5.1117942e-02
 2.4152144e-03 3.3305183e-02 5.6403924e-02 9.9013233e-01 7.4516479e-03
 9.9850827e-01 2.8786149e-03 9.9774891e-01 7.7715528e-01 1.6004524e-01
 9.0340549e-01 2.8743422e-01 5.7802867e-04 3.8362211e-03 6.2158404e-05
 8.1874110e-05 2.3964466e-01 6.0511756e-01 9.1853297e-05 9.9084330e-01
 9.9970800e-01 1.2513314e-05 1.6484955e-01 6.9004809e-04 9.9560946e-01
 9.6400136e-01 7.5679045e-04 1.8746770e-03 8.1081736e-01 9.9772650e-01
 9.9711120e-01 1.3444643e-01 1.1163861e-04 9.9615365e-01 3.2303667e-01
 1.4048253e-04 9.7511327e-01 9.6012342e-01 4.2974360e-05 3.1503218e-03
 9.7414070e-01 9.6377105e-01 9.9988711e-01 9.9869198e-01 2.6075737e-04
 3.5513155e-02 6.4920741e-01 8.6597759e-01 8.4348845e-01 1.4856922e-04
 8.4147578e-01 9.9313241e-01 5.9043251e-02 9.9768877e-01 9.5940614e-01
 9.9533683e-01 4.7397739e-01 9.9680871e-01 5.1048908e-02 6.0771918e-01
 9.9863130e-01 9.9009538e-01 2.8628980e-05 8.3806866e-01 9.8730034e-01
 2.4416780e-02 9.9252385e-01 2.8608771e-04 1.1496042e-02 9.7672844e-01
 9.9528790e-01 8.3581835e-05 1.5743326e-04 1.1988325e-02 9.9715865e-01
 9.9958783e-01 9.4728154e-01 3.1159489e-04 1.6601573e-03 9.9470127e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 12:34:58, Dev, Step : 3756, Loss : 0.43119, Acc : 0.822, Auc : 0.901, Sensitive_Loss : 0.11277, Sensitive_Acc : 16.679, Sensitive_Auc : 0.986, Mean auc: 0.901, Run Time : 156.95 sec
INFO:root:2024-04-26 12:35:05, Train, Epoch : 7, Step : 3760, Loss : 0.10812, Acc : 0.353, Sensitive_Loss : 0.03078, Sensitive_Acc : 6.500, Run Time : 6.25 sec
INFO:root:2024-04-26 12:35:17, Train, Epoch : 7, Step : 3770, Loss : 0.31297, Acc : 0.841, Sensitive_Loss : 0.07622, Sensitive_Acc : 15.700, Run Time : 11.76 sec
INFO:root:2024-04-26 12:35:29, Train, Epoch : 7, Step : 3780, Loss : 0.28541, Acc : 0.884, Sensitive_Loss : 0.07214, Sensitive_Acc : 15.700, Run Time : 11.93 sec
INFO:root:2024-04-26 12:35:42, Train, Epoch : 7, Step : 3790, Loss : 0.22048, Acc : 0.897, Sensitive_Loss : 0.06766, Sensitive_Acc : 16.400, Run Time : 12.82 sec
INFO:root:2024-04-26 12:35:54, Train, Epoch : 7, Step : 3800, Loss : 0.30495, Acc : 0.875, Sensitive_Loss : 0.05450, Sensitive_Acc : 17.400, Run Time : 11.95 sec
INFO:root:2024-04-26 12:38:32, Dev, Step : 3800, Loss : 0.44256, Acc : 0.818, Auc : 0.903, Sensitive_Loss : 0.12160, Sensitive_Acc : 16.893, Sensitive_Auc : 0.989, Mean auc: 0.903, Run Time : 158.54 sec
INFO:root:2024-04-26 12:38:41, Train, Epoch : 7, Step : 3810, Loss : 0.31028, Acc : 0.875, Sensitive_Loss : 0.06986, Sensitive_Acc : 17.600, Run Time : 167.05 sec
INFO:root:2024-04-26 12:38:52, Train, Epoch : 7, Step : 3820, Loss : 0.29206, Acc : 0.872, Sensitive_Loss : 0.07597, Sensitive_Acc : 15.500, Run Time : 11.49 sec
INFO:root:2024-04-26 12:39:04, Train, Epoch : 7, Step : 3830, Loss : 0.27821, Acc : 0.891, Sensitive_Loss : 0.05574, Sensitive_Acc : 15.900, Run Time : 11.76 sec
INFO:root:2024-04-26 12:39:15, Train, Epoch : 7, Step : 3840, Loss : 0.29124, Acc : 0.859, Sensitive_Loss : 0.06353, Sensitive_Acc : 15.700, Run Time : 11.25 sec
INFO:root:2024-04-26 12:39:27, Train, Epoch : 7, Step : 3850, Loss : 0.31129, Acc : 0.863, Sensitive_Loss : 0.09035, Sensitive_Acc : 15.900, Run Time : 11.36 sec
INFO:root:2024-04-26 12:39:38, Train, Epoch : 7, Step : 3860, Loss : 0.27646, Acc : 0.866, Sensitive_Loss : 0.06625, Sensitive_Acc : 15.300, Run Time : 11.07 sec
INFO:root:2024-04-26 12:39:49, Train, Epoch : 7, Step : 3870, Loss : 0.27465, Acc : 0.872, Sensitive_Loss : 0.08701, Sensitive_Acc : 17.400, Run Time : 11.36 sec
INFO:root:2024-04-26 12:40:00, Train, Epoch : 7, Step : 3880, Loss : 0.34150, Acc : 0.856, Sensitive_Loss : 0.07948, Sensitive_Acc : 15.200, Run Time : 11.34 sec
INFO:root:2024-04-26 12:40:13, Train, Epoch : 7, Step : 3890, Loss : 0.30764, Acc : 0.859, Sensitive_Loss : 0.07797, Sensitive_Acc : 17.800, Run Time : 12.10 sec
INFO:root:2024-04-26 12:40:24, Train, Epoch : 7, Step : 3900, Loss : 0.34085, Acc : 0.853, Sensitive_Loss : 0.05328, Sensitive_Acc : 16.200, Run Time : 11.01 sec
INFO:root:2024-04-26 12:43:02, Dev, Step : 3900, Loss : 0.44143, Acc : 0.821, Auc : 0.903, Sensitive_Loss : 0.10738, Sensitive_Acc : 16.764, Sensitive_Auc : 0.987, Mean auc: 0.903, Run Time : 158.12 sec
INFO:root:2024-04-26 12:43:10, Train, Epoch : 7, Step : 3910, Loss : 0.29344, Acc : 0.878, Sensitive_Loss : 0.07304, Sensitive_Acc : 15.100, Run Time : 166.68 sec
INFO:root:2024-04-26 12:43:22, Train, Epoch : 7, Step : 3920, Loss : 0.30274, Acc : 0.878, Sensitive_Loss : 0.08314, Sensitive_Acc : 16.400, Run Time : 12.11 sec
INFO:root:2024-04-26 12:43:34, Train, Epoch : 7, Step : 3930, Loss : 0.31867, Acc : 0.878, Sensitive_Loss : 0.06207, Sensitive_Acc : 16.300, Run Time : 11.40 sec
INFO:root:2024-04-26 12:43:46, Train, Epoch : 7, Step : 3940, Loss : 0.30963, Acc : 0.866, Sensitive_Loss : 0.08594, Sensitive_Acc : 15.400, Run Time : 12.19 sec
INFO:root:2024-04-26 12:43:57, Train, Epoch : 7, Step : 3950, Loss : 0.28260, Acc : 0.853, Sensitive_Loss : 0.08875, Sensitive_Acc : 16.900, Run Time : 11.03 sec
INFO:root:2024-04-26 12:44:08, Train, Epoch : 7, Step : 3960, Loss : 0.26287, Acc : 0.887, Sensitive_Loss : 0.10518, Sensitive_Acc : 16.800, Run Time : 11.39 sec
INFO:root:2024-04-26 12:44:21, Train, Epoch : 7, Step : 3970, Loss : 0.33764, Acc : 0.841, Sensitive_Loss : 0.06291, Sensitive_Acc : 16.100, Run Time : 12.96 sec
INFO:root:2024-04-26 12:44:34, Train, Epoch : 7, Step : 3980, Loss : 0.29243, Acc : 0.872, Sensitive_Loss : 0.06522, Sensitive_Acc : 16.500, Run Time : 12.56 sec
INFO:root:2024-04-26 12:44:46, Train, Epoch : 7, Step : 3990, Loss : 0.27974, Acc : 0.859, Sensitive_Loss : 0.06507, Sensitive_Acc : 17.100, Run Time : 11.70 sec
INFO:root:2024-04-26 12:44:57, Train, Epoch : 7, Step : 4000, Loss : 0.33643, Acc : 0.872, Sensitive_Loss : 0.07342, Sensitive_Acc : 14.900, Run Time : 11.09 sec
INFO:root:2024-04-26 12:47:36, Dev, Step : 4000, Loss : 0.44667, Acc : 0.818, Auc : 0.906, Sensitive_Loss : 0.12948, Sensitive_Acc : 16.893, Sensitive_Auc : 0.988, Mean auc: 0.906, Run Time : 158.94 sec
INFO:root:2024-04-26 12:47:44, Train, Epoch : 7, Step : 4010, Loss : 0.27074, Acc : 0.866, Sensitive_Loss : 0.07732, Sensitive_Acc : 16.700, Run Time : 167.34 sec
INFO:root:2024-04-26 12:47:55, Train, Epoch : 7, Step : 4020, Loss : 0.30246, Acc : 0.875, Sensitive_Loss : 0.07147, Sensitive_Acc : 16.200, Run Time : 11.43 sec
INFO:root:2024-04-26 12:48:07, Train, Epoch : 7, Step : 4030, Loss : 0.33858, Acc : 0.853, Sensitive_Loss : 0.06988, Sensitive_Acc : 17.300, Run Time : 11.91 sec
INFO:root:2024-04-26 12:48:19, Train, Epoch : 7, Step : 4040, Loss : 0.34904, Acc : 0.863, Sensitive_Loss : 0.05788, Sensitive_Acc : 16.400, Run Time : 11.46 sec
INFO:root:2024-04-26 12:48:30, Train, Epoch : 7, Step : 4050, Loss : 0.28974, Acc : 0.884, Sensitive_Loss : 0.06504, Sensitive_Acc : 17.200, Run Time : 11.21 sec
INFO:root:2024-04-26 12:48:42, Train, Epoch : 7, Step : 4060, Loss : 0.27319, Acc : 0.891, Sensitive_Loss : 0.07739, Sensitive_Acc : 17.600, Run Time : 11.73 sec
INFO:root:2024-04-26 12:48:54, Train, Epoch : 7, Step : 4070, Loss : 0.32548, Acc : 0.856, Sensitive_Loss : 0.08139, Sensitive_Acc : 17.700, Run Time : 11.97 sec
INFO:root:2024-04-26 12:49:05, Train, Epoch : 7, Step : 4080, Loss : 0.28010, Acc : 0.863, Sensitive_Loss : 0.08663, Sensitive_Acc : 16.000, Run Time : 11.19 sec
INFO:root:2024-04-26 12:49:17, Train, Epoch : 7, Step : 4090, Loss : 0.27579, Acc : 0.903, Sensitive_Loss : 0.07357, Sensitive_Acc : 17.900, Run Time : 12.45 sec
INFO:root:2024-04-26 12:49:29, Train, Epoch : 7, Step : 4100, Loss : 0.32390, Acc : 0.844, Sensitive_Loss : 0.07369, Sensitive_Acc : 18.300, Run Time : 11.90 sec
INFO:root:2024-04-26 12:52:09, Dev, Step : 4100, Loss : 0.40692, Acc : 0.836, Auc : 0.907, Sensitive_Loss : 0.09710, Sensitive_Acc : 16.736, Sensitive_Auc : 0.990, Mean auc: 0.907, Run Time : 159.96 sec
INFO:root:2024-04-26 12:52:18, Train, Epoch : 7, Step : 4110, Loss : 0.31689, Acc : 0.844, Sensitive_Loss : 0.07403, Sensitive_Acc : 16.200, Run Time : 168.57 sec
INFO:root:2024-04-26 12:52:31, Train, Epoch : 7, Step : 4120, Loss : 0.32237, Acc : 0.869, Sensitive_Loss : 0.09993, Sensitive_Acc : 15.700, Run Time : 12.88 sec
INFO:root:2024-04-26 12:52:42, Train, Epoch : 7, Step : 4130, Loss : 0.24181, Acc : 0.897, Sensitive_Loss : 0.08818, Sensitive_Acc : 17.600, Run Time : 11.59 sec
INFO:root:2024-04-26 12:52:56, Train, Epoch : 7, Step : 4140, Loss : 0.30951, Acc : 0.859, Sensitive_Loss : 0.06566, Sensitive_Acc : 16.800, Run Time : 13.42 sec
INFO:root:2024-04-26 12:53:10, Train, Epoch : 7, Step : 4150, Loss : 0.25531, Acc : 0.900, Sensitive_Loss : 0.06314, Sensitive_Acc : 16.600, Run Time : 14.22 sec
INFO:root:2024-04-26 12:53:22, Train, Epoch : 7, Step : 4160, Loss : 0.30752, Acc : 0.887, Sensitive_Loss : 0.07594, Sensitive_Acc : 15.900, Run Time : 12.05 sec
INFO:root:2024-04-26 12:53:34, Train, Epoch : 7, Step : 4170, Loss : 0.34168, Acc : 0.878, Sensitive_Loss : 0.06943, Sensitive_Acc : 16.100, Run Time : 11.85 sec
INFO:root:2024-04-26 12:53:45, Train, Epoch : 7, Step : 4180, Loss : 0.26570, Acc : 0.866, Sensitive_Loss : 0.05507, Sensitive_Acc : 16.700, Run Time : 11.62 sec
INFO:root:2024-04-26 12:53:57, Train, Epoch : 7, Step : 4190, Loss : 0.25110, Acc : 0.934, Sensitive_Loss : 0.08867, Sensitive_Acc : 15.600, Run Time : 11.63 sec
INFO:root:2024-04-26 12:54:08, Train, Epoch : 7, Step : 4200, Loss : 0.31019, Acc : 0.891, Sensitive_Loss : 0.05053, Sensitive_Acc : 15.800, Run Time : 11.01 sec
INFO:root:2024-04-26 12:56:48, Dev, Step : 4200, Loss : 0.40912, Acc : 0.832, Auc : 0.907, Sensitive_Loss : 0.10436, Sensitive_Acc : 16.807, Sensitive_Auc : 0.989, Mean auc: 0.907, Run Time : 160.36 sec
INFO:root:2024-04-26 12:56:57, Train, Epoch : 7, Step : 4210, Loss : 0.31245, Acc : 0.863, Sensitive_Loss : 0.08098, Sensitive_Acc : 14.700, Run Time : 168.87 sec
INFO:root:2024-04-26 12:57:08, Train, Epoch : 7, Step : 4220, Loss : 0.29660, Acc : 0.869, Sensitive_Loss : 0.08507, Sensitive_Acc : 17.000, Run Time : 11.41 sec
INFO:root:2024-04-26 12:57:21, Train, Epoch : 7, Step : 4230, Loss : 0.36419, Acc : 0.841, Sensitive_Loss : 0.08080, Sensitive_Acc : 17.000, Run Time : 12.24 sec
INFO:root:2024-04-26 12:57:32, Train, Epoch : 7, Step : 4240, Loss : 0.32320, Acc : 0.844, Sensitive_Loss : 0.07653, Sensitive_Acc : 16.300, Run Time : 11.69 sec
INFO:root:2024-04-26 12:57:43, Train, Epoch : 7, Step : 4250, Loss : 0.28563, Acc : 0.872, Sensitive_Loss : 0.07316, Sensitive_Acc : 15.600, Run Time : 10.82 sec
INFO:root:2024-04-26 12:57:55, Train, Epoch : 7, Step : 4260, Loss : 0.22982, Acc : 0.912, Sensitive_Loss : 0.06202, Sensitive_Acc : 16.100, Run Time : 11.45 sec
INFO:root:2024-04-26 12:58:07, Train, Epoch : 7, Step : 4270, Loss : 0.29062, Acc : 0.863, Sensitive_Loss : 0.08537, Sensitive_Acc : 16.400, Run Time : 12.89 sec
INFO:root:2024-04-26 12:58:19, Train, Epoch : 7, Step : 4280, Loss : 0.36044, Acc : 0.853, Sensitive_Loss : 0.09640, Sensitive_Acc : 15.300, Run Time : 11.71 sec
INFO:root:2024-04-26 12:58:32, Train, Epoch : 7, Step : 4290, Loss : 0.25333, Acc : 0.884, Sensitive_Loss : 0.05286, Sensitive_Acc : 16.400, Run Time : 13.02 sec
INFO:root:2024-04-26 12:58:47, Train, Epoch : 7, Step : 4300, Loss : 0.24655, Acc : 0.887, Sensitive_Loss : 0.09845, Sensitive_Acc : 17.400, Run Time : 14.48 sec
INFO:root:2024-04-26 13:01:23, Dev, Step : 4300, Loss : 0.42773, Acc : 0.827, Auc : 0.905, Sensitive_Loss : 0.10576, Sensitive_Acc : 16.807, Sensitive_Auc : 0.990, Mean auc: 0.905, Run Time : 155.94 sec
INFO:root:2024-04-26 13:01:31, Train, Epoch : 7, Step : 4310, Loss : 0.26256, Acc : 0.906, Sensitive_Loss : 0.05696, Sensitive_Acc : 15.600, Run Time : 164.48 sec
INFO:root:2024-04-26 13:01:42, Train, Epoch : 7, Step : 4320, Loss : 0.35216, Acc : 0.834, Sensitive_Loss : 0.08369, Sensitive_Acc : 15.600, Run Time : 11.03 sec
INFO:root:2024-04-26 13:01:54, Train, Epoch : 7, Step : 4330, Loss : 0.32038, Acc : 0.869, Sensitive_Loss : 0.06371, Sensitive_Acc : 16.100, Run Time : 11.89 sec
INFO:root:2024-04-26 13:02:07, Train, Epoch : 7, Step : 4340, Loss : 0.31381, Acc : 0.878, Sensitive_Loss : 0.07675, Sensitive_Acc : 16.100, Run Time : 13.00 sec
INFO:root:2024-04-26 13:02:19, Train, Epoch : 7, Step : 4350, Loss : 0.33794, Acc : 0.856, Sensitive_Loss : 0.08322, Sensitive_Acc : 17.800, Run Time : 12.39 sec
INFO:root:2024-04-26 13:02:32, Train, Epoch : 7, Step : 4360, Loss : 0.35542, Acc : 0.872, Sensitive_Loss : 0.09862, Sensitive_Acc : 16.100, Run Time : 12.06 sec
INFO:root:2024-04-26 13:02:46, Train, Epoch : 7, Step : 4370, Loss : 0.29994, Acc : 0.863, Sensitive_Loss : 0.07566, Sensitive_Acc : 15.700, Run Time : 14.01 sec
INFO:root:2024-04-26 13:02:58, Train, Epoch : 7, Step : 4380, Loss : 0.34414, Acc : 0.847, Sensitive_Loss : 0.05640, Sensitive_Acc : 16.600, Run Time : 12.73 sec
INFO:root:2024-04-26 13:05:34
INFO:root:y_pred: [0.09136809 0.93937045 0.0094095  ... 0.6882693  0.00382432 0.74445575]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.89297628e-01 7.29114690e-05 1.00448979e-02 3.90998957e-06
 9.90216017e-01 2.44194734e-06 9.99440849e-01 9.98612523e-01
 2.92215758e-04 9.57864106e-01 9.95683789e-01 9.99309540e-01
 9.98074055e-01 9.31429327e-01 3.17476802e-02 9.74462748e-01
 9.99895453e-01 2.34786477e-02 8.98172617e-01 9.97885883e-01
 9.95704472e-01 2.60652632e-01 9.99843836e-01 9.85625923e-01
 9.95201945e-01 8.97794425e-01 1.71541600e-04 9.94648159e-01
 9.95660007e-01 8.18599820e-01 6.23412710e-03 1.10500179e-01
 8.66456255e-02 2.75495537e-02 5.53104393e-02 2.41861908e-05
 3.51810567e-02 1.32711697e-03 9.78585362e-01 9.98087704e-01
 9.37568893e-06 4.02159640e-04 9.98868108e-01 1.71837019e-05
 9.99835610e-01 9.97405946e-01 9.88345385e-01 9.99218822e-01
 5.98177537e-02 9.86767590e-01 9.99435127e-01 2.06057299e-02
 2.90299147e-01 7.66854737e-06 5.53840306e-04 9.10535548e-03
 1.64217174e-01 5.13670221e-02 6.75890828e-04 2.16058776e-01
 3.86268832e-03 5.26387542e-02 3.04968636e-02 9.70595896e-01
 7.66924862e-03 9.99534607e-01 1.36543307e-02 9.98843431e-01
 8.37266564e-01 7.48476863e-01 9.31703806e-01 4.28265959e-01
 6.74034061e-04 1.13518257e-02 4.01246398e-05 7.10811946e-05
 8.91557485e-02 7.19027400e-01 4.84619872e-04 9.96945083e-01
 9.99690771e-01 2.31806662e-05 4.37802523e-01 9.08339920e-04
 9.96589303e-01 9.61873651e-01 4.18690732e-03 3.03716934e-03
 9.14992154e-01 9.96402621e-01 9.98829186e-01 3.20292920e-01
 6.85807259e-04 9.96302843e-01 3.57978404e-01 3.35468125e-04
 9.84637797e-01 9.65668201e-01 6.82937316e-05 1.70285278e-03
 9.87201810e-01 9.81543124e-01 9.99761999e-01 9.98661995e-01
 1.52102759e-04 9.71366167e-02 7.10299551e-01 8.84013653e-01
 8.10655117e-01 1.22983678e-04 8.85080040e-01 9.93035197e-01
 1.99107036e-01 9.99283612e-01 9.78347778e-01 9.95871365e-01
 8.29793453e-01 9.98722255e-01 3.23712885e-01 1.79017037e-01
 9.99597847e-01 9.97860849e-01 8.70697550e-05 8.93886685e-01
 9.95997310e-01 5.15553802e-02 9.98494029e-01 7.62432290e-04
 3.00530344e-02 9.86741483e-01 9.97308493e-01 1.90746418e-04
 9.11713403e-04 1.53476605e-02 9.97248471e-01 9.99041021e-01
 9.78866756e-01 1.25075015e-03 7.21961353e-03 9.95932877e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 13:05:34, Dev, Step : 4382, Loss : 0.42489, Acc : 0.826, Auc : 0.906, Sensitive_Loss : 0.12758, Sensitive_Acc : 16.864, Sensitive_Auc : 0.988, Mean auc: 0.906, Run Time : 154.20 sec
INFO:root:2024-04-26 13:05:46, Train, Epoch : 8, Step : 4390, Loss : 0.28327, Acc : 0.681, Sensitive_Loss : 0.05344, Sensitive_Acc : 13.800, Run Time : 10.26 sec
INFO:root:2024-04-26 13:05:57, Train, Epoch : 8, Step : 4400, Loss : 0.35609, Acc : 0.834, Sensitive_Loss : 0.06241, Sensitive_Acc : 16.700, Run Time : 11.72 sec
INFO:root:2024-04-26 13:08:34, Dev, Step : 4400, Loss : 0.42459, Acc : 0.820, Auc : 0.905, Sensitive_Loss : 0.10943, Sensitive_Acc : 16.864, Sensitive_Auc : 0.988, Mean auc: 0.905, Run Time : 156.18 sec
INFO:root:2024-04-26 13:08:42, Train, Epoch : 8, Step : 4410, Loss : 0.27147, Acc : 0.884, Sensitive_Loss : 0.06606, Sensitive_Acc : 16.400, Run Time : 164.55 sec
INFO:root:2024-04-26 13:08:53, Train, Epoch : 8, Step : 4420, Loss : 0.23368, Acc : 0.903, Sensitive_Loss : 0.07107, Sensitive_Acc : 17.000, Run Time : 11.25 sec
INFO:root:2024-04-26 13:09:06, Train, Epoch : 8, Step : 4430, Loss : 0.24966, Acc : 0.884, Sensitive_Loss : 0.06694, Sensitive_Acc : 15.800, Run Time : 12.40 sec
INFO:root:2024-04-26 13:09:18, Train, Epoch : 8, Step : 4440, Loss : 0.30211, Acc : 0.884, Sensitive_Loss : 0.07797, Sensitive_Acc : 17.000, Run Time : 12.51 sec
INFO:root:2024-04-26 13:09:30, Train, Epoch : 8, Step : 4450, Loss : 0.30039, Acc : 0.887, Sensitive_Loss : 0.06946, Sensitive_Acc : 16.400, Run Time : 11.51 sec
INFO:root:2024-04-26 13:09:44, Train, Epoch : 8, Step : 4460, Loss : 0.23346, Acc : 0.916, Sensitive_Loss : 0.07481, Sensitive_Acc : 14.300, Run Time : 14.52 sec
INFO:root:2024-04-26 13:09:57, Train, Epoch : 8, Step : 4470, Loss : 0.26878, Acc : 0.891, Sensitive_Loss : 0.06129, Sensitive_Acc : 16.800, Run Time : 12.38 sec
INFO:root:2024-04-26 13:10:08, Train, Epoch : 8, Step : 4480, Loss : 0.27111, Acc : 0.909, Sensitive_Loss : 0.06468, Sensitive_Acc : 17.300, Run Time : 11.85 sec
INFO:root:2024-04-26 13:10:19, Train, Epoch : 8, Step : 4490, Loss : 0.29500, Acc : 0.878, Sensitive_Loss : 0.08691, Sensitive_Acc : 15.800, Run Time : 11.05 sec
INFO:root:2024-04-26 13:10:31, Train, Epoch : 8, Step : 4500, Loss : 0.27603, Acc : 0.881, Sensitive_Loss : 0.04878, Sensitive_Acc : 15.300, Run Time : 11.19 sec
INFO:root:2024-04-26 13:13:08, Dev, Step : 4500, Loss : 0.42097, Acc : 0.824, Auc : 0.904, Sensitive_Loss : 0.12559, Sensitive_Acc : 16.864, Sensitive_Auc : 0.989, Mean auc: 0.904, Run Time : 156.90 sec
INFO:root:2024-04-26 13:13:16, Train, Epoch : 8, Step : 4510, Loss : 0.21061, Acc : 0.897, Sensitive_Loss : 0.05270, Sensitive_Acc : 15.600, Run Time : 165.41 sec
INFO:root:2024-04-26 13:13:29, Train, Epoch : 8, Step : 4520, Loss : 0.26845, Acc : 0.891, Sensitive_Loss : 0.09951, Sensitive_Acc : 16.900, Run Time : 12.59 sec
INFO:root:2024-04-26 13:13:40, Train, Epoch : 8, Step : 4530, Loss : 0.28693, Acc : 0.878, Sensitive_Loss : 0.11564, Sensitive_Acc : 15.100, Run Time : 11.71 sec
INFO:root:2024-04-26 13:13:52, Train, Epoch : 8, Step : 4540, Loss : 0.28010, Acc : 0.881, Sensitive_Loss : 0.05807, Sensitive_Acc : 17.100, Run Time : 11.78 sec
INFO:root:2024-04-26 13:14:04, Train, Epoch : 8, Step : 4550, Loss : 0.26094, Acc : 0.887, Sensitive_Loss : 0.10488, Sensitive_Acc : 16.600, Run Time : 11.98 sec
INFO:root:2024-04-26 13:14:16, Train, Epoch : 8, Step : 4560, Loss : 0.27021, Acc : 0.878, Sensitive_Loss : 0.05814, Sensitive_Acc : 15.800, Run Time : 11.66 sec
INFO:root:2024-04-26 13:14:27, Train, Epoch : 8, Step : 4570, Loss : 0.30294, Acc : 0.878, Sensitive_Loss : 0.03662, Sensitive_Acc : 16.000, Run Time : 11.19 sec
INFO:root:2024-04-26 13:14:39, Train, Epoch : 8, Step : 4580, Loss : 0.32637, Acc : 0.856, Sensitive_Loss : 0.04305, Sensitive_Acc : 15.500, Run Time : 12.00 sec
INFO:root:2024-04-26 13:14:50, Train, Epoch : 8, Step : 4590, Loss : 0.30049, Acc : 0.869, Sensitive_Loss : 0.07487, Sensitive_Acc : 17.100, Run Time : 11.32 sec
INFO:root:2024-04-26 13:15:03, Train, Epoch : 8, Step : 4600, Loss : 0.29607, Acc : 0.884, Sensitive_Loss : 0.07804, Sensitive_Acc : 15.100, Run Time : 12.40 sec
INFO:root:2024-04-26 13:17:39, Dev, Step : 4600, Loss : 0.44782, Acc : 0.815, Auc : 0.904, Sensitive_Loss : 0.11378, Sensitive_Acc : 16.864, Sensitive_Auc : 0.991, Mean auc: 0.904, Run Time : 156.28 sec
INFO:root:2024-04-26 13:17:48, Train, Epoch : 8, Step : 4610, Loss : 0.23973, Acc : 0.881, Sensitive_Loss : 0.05186, Sensitive_Acc : 17.000, Run Time : 164.90 sec
INFO:root:2024-04-26 13:17:59, Train, Epoch : 8, Step : 4620, Loss : 0.29128, Acc : 0.881, Sensitive_Loss : 0.08346, Sensitive_Acc : 14.500, Run Time : 11.45 sec
INFO:root:2024-04-26 13:18:11, Train, Epoch : 8, Step : 4630, Loss : 0.21642, Acc : 0.919, Sensitive_Loss : 0.06589, Sensitive_Acc : 15.800, Run Time : 12.29 sec
INFO:root:2024-04-26 13:18:23, Train, Epoch : 8, Step : 4640, Loss : 0.29661, Acc : 0.869, Sensitive_Loss : 0.05318, Sensitive_Acc : 17.700, Run Time : 11.75 sec
INFO:root:2024-04-26 13:18:35, Train, Epoch : 8, Step : 4650, Loss : 0.32930, Acc : 0.863, Sensitive_Loss : 0.06526, Sensitive_Acc : 16.800, Run Time : 11.45 sec
INFO:root:2024-04-26 13:18:46, Train, Epoch : 8, Step : 4660, Loss : 0.29194, Acc : 0.866, Sensitive_Loss : 0.05747, Sensitive_Acc : 16.400, Run Time : 11.53 sec
INFO:root:2024-04-26 13:18:58, Train, Epoch : 8, Step : 4670, Loss : 0.28341, Acc : 0.866, Sensitive_Loss : 0.06658, Sensitive_Acc : 15.700, Run Time : 11.76 sec
INFO:root:2024-04-26 13:19:08, Train, Epoch : 8, Step : 4680, Loss : 0.35912, Acc : 0.841, Sensitive_Loss : 0.07653, Sensitive_Acc : 16.500, Run Time : 10.66 sec
INFO:root:2024-04-26 13:19:20, Train, Epoch : 8, Step : 4690, Loss : 0.24095, Acc : 0.875, Sensitive_Loss : 0.07341, Sensitive_Acc : 15.900, Run Time : 11.53 sec
INFO:root:2024-04-26 13:19:32, Train, Epoch : 8, Step : 4700, Loss : 0.35604, Acc : 0.866, Sensitive_Loss : 0.06853, Sensitive_Acc : 17.300, Run Time : 11.94 sec
INFO:root:2024-04-26 13:22:08, Dev, Step : 4700, Loss : 0.43334, Acc : 0.822, Auc : 0.898, Sensitive_Loss : 0.11666, Sensitive_Acc : 16.736, Sensitive_Auc : 0.990, Mean auc: 0.898, Run Time : 155.72 sec
INFO:root:2024-04-26 13:22:15, Train, Epoch : 8, Step : 4710, Loss : 0.28980, Acc : 0.881, Sensitive_Loss : 0.06914, Sensitive_Acc : 17.000, Run Time : 163.46 sec
INFO:root:2024-04-26 13:22:28, Train, Epoch : 8, Step : 4720, Loss : 0.33223, Acc : 0.881, Sensitive_Loss : 0.05583, Sensitive_Acc : 15.800, Run Time : 12.22 sec
INFO:root:2024-04-26 13:22:39, Train, Epoch : 8, Step : 4730, Loss : 0.32460, Acc : 0.856, Sensitive_Loss : 0.08944, Sensitive_Acc : 14.000, Run Time : 11.40 sec
INFO:root:2024-04-26 13:22:50, Train, Epoch : 8, Step : 4740, Loss : 0.27136, Acc : 0.903, Sensitive_Loss : 0.08468, Sensitive_Acc : 15.800, Run Time : 11.26 sec
INFO:root:2024-04-26 13:23:01, Train, Epoch : 8, Step : 4750, Loss : 0.28898, Acc : 0.875, Sensitive_Loss : 0.07079, Sensitive_Acc : 17.700, Run Time : 11.15 sec
INFO:root:2024-04-26 13:23:15, Train, Epoch : 8, Step : 4760, Loss : 0.26753, Acc : 0.903, Sensitive_Loss : 0.05811, Sensitive_Acc : 16.500, Run Time : 13.79 sec
INFO:root:2024-04-26 13:23:28, Train, Epoch : 8, Step : 4770, Loss : 0.25250, Acc : 0.894, Sensitive_Loss : 0.12284, Sensitive_Acc : 16.800, Run Time : 12.27 sec
INFO:root:2024-04-26 13:23:39, Train, Epoch : 8, Step : 4780, Loss : 0.31298, Acc : 0.872, Sensitive_Loss : 0.08588, Sensitive_Acc : 17.500, Run Time : 11.05 sec
INFO:root:2024-04-26 13:23:50, Train, Epoch : 8, Step : 4790, Loss : 0.22774, Acc : 0.875, Sensitive_Loss : 0.10087, Sensitive_Acc : 15.600, Run Time : 11.67 sec
INFO:root:2024-04-26 13:24:02, Train, Epoch : 8, Step : 4800, Loss : 0.29133, Acc : 0.887, Sensitive_Loss : 0.05476, Sensitive_Acc : 16.800, Run Time : 11.71 sec
INFO:root:2024-04-26 13:26:36, Dev, Step : 4800, Loss : 0.42733, Acc : 0.824, Auc : 0.904, Sensitive_Loss : 0.11014, Sensitive_Acc : 16.836, Sensitive_Auc : 0.989, Mean auc: 0.904, Run Time : 154.43 sec
INFO:root:2024-04-26 13:26:45, Train, Epoch : 8, Step : 4810, Loss : 0.32400, Acc : 0.850, Sensitive_Loss : 0.09276, Sensitive_Acc : 16.600, Run Time : 163.04 sec
INFO:root:2024-04-26 13:26:56, Train, Epoch : 8, Step : 4820, Loss : 0.29136, Acc : 0.894, Sensitive_Loss : 0.08184, Sensitive_Acc : 17.800, Run Time : 11.06 sec
INFO:root:2024-04-26 13:27:08, Train, Epoch : 8, Step : 4830, Loss : 0.23363, Acc : 0.894, Sensitive_Loss : 0.06231, Sensitive_Acc : 16.300, Run Time : 11.52 sec
INFO:root:2024-04-26 13:27:19, Train, Epoch : 8, Step : 4840, Loss : 0.28342, Acc : 0.878, Sensitive_Loss : 0.05019, Sensitive_Acc : 17.100, Run Time : 11.26 sec
INFO:root:2024-04-26 13:27:31, Train, Epoch : 8, Step : 4850, Loss : 0.27501, Acc : 0.906, Sensitive_Loss : 0.08888, Sensitive_Acc : 17.900, Run Time : 12.49 sec
INFO:root:2024-04-26 13:27:42, Train, Epoch : 8, Step : 4860, Loss : 0.25507, Acc : 0.897, Sensitive_Loss : 0.06255, Sensitive_Acc : 16.500, Run Time : 10.69 sec
INFO:root:2024-04-26 13:27:53, Train, Epoch : 8, Step : 4870, Loss : 0.29735, Acc : 0.859, Sensitive_Loss : 0.09352, Sensitive_Acc : 16.800, Run Time : 11.41 sec
INFO:root:2024-04-26 13:28:06, Train, Epoch : 8, Step : 4880, Loss : 0.32950, Acc : 0.863, Sensitive_Loss : 0.09220, Sensitive_Acc : 16.500, Run Time : 12.17 sec
INFO:root:2024-04-26 13:28:18, Train, Epoch : 8, Step : 4890, Loss : 0.30250, Acc : 0.881, Sensitive_Loss : 0.09146, Sensitive_Acc : 15.900, Run Time : 12.40 sec
INFO:root:2024-04-26 13:28:29, Train, Epoch : 8, Step : 4900, Loss : 0.27964, Acc : 0.859, Sensitive_Loss : 0.07078, Sensitive_Acc : 16.900, Run Time : 11.25 sec
INFO:root:2024-04-26 13:31:04, Dev, Step : 4900, Loss : 0.44822, Acc : 0.819, Auc : 0.902, Sensitive_Loss : 0.11861, Sensitive_Acc : 16.807, Sensitive_Auc : 0.991, Mean auc: 0.902, Run Time : 154.92 sec
INFO:root:2024-04-26 13:31:13, Train, Epoch : 8, Step : 4910, Loss : 0.22635, Acc : 0.897, Sensitive_Loss : 0.05999, Sensitive_Acc : 16.600, Run Time : 164.09 sec
INFO:root:2024-04-26 13:31:25, Train, Epoch : 8, Step : 4920, Loss : 0.31490, Acc : 0.875, Sensitive_Loss : 0.09294, Sensitive_Acc : 17.900, Run Time : 11.46 sec
INFO:root:2024-04-26 13:31:36, Train, Epoch : 8, Step : 4930, Loss : 0.22340, Acc : 0.919, Sensitive_Loss : 0.06710, Sensitive_Acc : 17.300, Run Time : 11.41 sec
INFO:root:2024-04-26 13:31:48, Train, Epoch : 8, Step : 4940, Loss : 0.20928, Acc : 0.934, Sensitive_Loss : 0.06579, Sensitive_Acc : 16.500, Run Time : 11.78 sec
INFO:root:2024-04-26 13:31:59, Train, Epoch : 8, Step : 4950, Loss : 0.30449, Acc : 0.844, Sensitive_Loss : 0.06850, Sensitive_Acc : 16.500, Run Time : 10.69 sec
INFO:root:2024-04-26 13:32:10, Train, Epoch : 8, Step : 4960, Loss : 0.30685, Acc : 0.859, Sensitive_Loss : 0.07783, Sensitive_Acc : 16.800, Run Time : 11.28 sec
INFO:root:2024-04-26 13:32:22, Train, Epoch : 8, Step : 4970, Loss : 0.23477, Acc : 0.878, Sensitive_Loss : 0.08022, Sensitive_Acc : 15.000, Run Time : 11.86 sec
INFO:root:2024-04-26 13:32:33, Train, Epoch : 8, Step : 4980, Loss : 0.25733, Acc : 0.878, Sensitive_Loss : 0.07819, Sensitive_Acc : 14.900, Run Time : 11.45 sec
INFO:root:2024-04-26 13:32:45, Train, Epoch : 8, Step : 4990, Loss : 0.26272, Acc : 0.869, Sensitive_Loss : 0.05285, Sensitive_Acc : 16.500, Run Time : 11.63 sec
INFO:root:2024-04-26 13:32:56, Train, Epoch : 8, Step : 5000, Loss : 0.26769, Acc : 0.887, Sensitive_Loss : 0.04679, Sensitive_Acc : 17.500, Run Time : 11.19 sec
INFO:root:2024-04-26 13:35:32, Dev, Step : 5000, Loss : 0.44245, Acc : 0.826, Auc : 0.907, Sensitive_Loss : 0.10725, Sensitive_Acc : 16.807, Sensitive_Auc : 0.991, Mean auc: 0.907, Run Time : 156.19 sec
INFO:root:2024-04-26 13:38:13
INFO:root:y_pred: [0.06753436 0.8197198  0.00703722 ... 0.73732495 0.00389439 0.68533605]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.8798358e-01 1.7099188e-05 1.4818071e-03 2.1759336e-06 9.9486452e-01
 3.0742494e-06 9.9905235e-01 9.9858874e-01 2.3627705e-04 9.5952040e-01
 9.9260592e-01 9.9891949e-01 9.9796522e-01 9.0993571e-01 1.7634967e-03
 9.8111123e-01 9.9988055e-01 5.8044796e-03 7.8829247e-01 9.9541950e-01
 9.9715143e-01 1.6910245e-01 9.9982268e-01 9.7356808e-01 9.9808359e-01
 8.8948286e-01 3.9117818e-05 9.9229240e-01 9.9726093e-01 8.1394970e-01
 6.1244699e-03 5.7133880e-02 2.8575048e-02 1.3920708e-02 1.6533840e-01
 1.4571453e-05 8.4877023e-03 1.1162859e-03 9.6784973e-01 9.9824154e-01
 6.1036208e-06 3.6784634e-04 9.9916267e-01 9.0315752e-06 9.9976605e-01
 9.9476862e-01 9.8546022e-01 9.9830163e-01 1.1840158e-02 9.9124885e-01
 9.9941278e-01 1.4523842e-03 4.2012802e-01 4.3286223e-06 1.7766144e-04
 4.6246010e-03 2.2349713e-02 3.8716692e-01 5.2259077e-04 1.3124515e-01
 3.5579433e-03 1.0540965e-01 2.6208779e-02 9.6428365e-01 1.5997697e-03
 9.9944431e-01 2.2209905e-02 9.9865890e-01 7.7194351e-01 1.6568021e-01
 7.9229993e-01 2.6897255e-01 1.8401498e-04 1.0891016e-02 1.2363215e-05
 1.1290187e-05 2.3205486e-01 7.2550493e-01 1.7621843e-04 9.9661642e-01
 9.9943250e-01 1.1429984e-05 1.6473264e-01 1.0055745e-03 9.9615330e-01
 9.4998837e-01 1.3512548e-03 2.5726452e-03 8.6603528e-01 9.9636769e-01
 9.9860376e-01 2.0575488e-01 2.4233933e-04 9.9490958e-01 2.8527758e-01
 1.5142668e-04 9.5718527e-01 9.6285796e-01 4.0258907e-05 2.3522170e-04
 9.8251534e-01 9.8903054e-01 9.9984324e-01 9.9950147e-01 1.5818187e-04
 3.4155827e-02 7.0899069e-01 8.6796170e-01 8.4818876e-01 2.1271507e-04
 8.6023587e-01 9.9705660e-01 1.2323767e-02 9.9890184e-01 9.7196579e-01
 9.9826556e-01 6.4528328e-01 9.9925500e-01 2.5457783e-02 1.9839387e-01
 9.9951434e-01 9.9754179e-01 5.5995566e-05 8.6078304e-01 9.9314427e-01
 1.4679103e-02 9.9838495e-01 2.9433853e-04 2.8562211e-02 9.7523177e-01
 9.9633777e-01 1.2506639e-04 9.2984745e-03 3.2017301e-03 9.9862325e-01
 9.9931788e-01 9.7384179e-01 2.9791994e-03 9.5014542e-04 9.9586320e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 13:38:13, Dev, Step : 5008, Loss : 0.44744, Acc : 0.823, Auc : 0.906, Sensitive_Loss : 0.11039, Sensitive_Acc : 16.807, Sensitive_Auc : 0.991, Mean auc: 0.906, Run Time : 154.76 sec
INFO:root:2024-04-26 13:38:19, Train, Epoch : 9, Step : 5010, Loss : 0.05299, Acc : 0.175, Sensitive_Loss : 0.00495, Sensitive_Acc : 3.200, Run Time : 3.93 sec
INFO:root:2024-04-26 13:38:30, Train, Epoch : 9, Step : 5020, Loss : 0.30855, Acc : 0.887, Sensitive_Loss : 0.06339, Sensitive_Acc : 16.300, Run Time : 11.70 sec
INFO:root:2024-04-26 13:38:42, Train, Epoch : 9, Step : 5030, Loss : 0.23446, Acc : 0.900, Sensitive_Loss : 0.05509, Sensitive_Acc : 16.500, Run Time : 11.61 sec
INFO:root:2024-04-26 13:38:54, Train, Epoch : 9, Step : 5040, Loss : 0.26456, Acc : 0.878, Sensitive_Loss : 0.12648, Sensitive_Acc : 19.000, Run Time : 12.04 sec
INFO:root:2024-04-26 13:39:05, Train, Epoch : 9, Step : 5050, Loss : 0.25443, Acc : 0.909, Sensitive_Loss : 0.05198, Sensitive_Acc : 16.200, Run Time : 11.15 sec
INFO:root:2024-04-26 13:39:17, Train, Epoch : 9, Step : 5060, Loss : 0.26200, Acc : 0.884, Sensitive_Loss : 0.05891, Sensitive_Acc : 16.500, Run Time : 11.85 sec
INFO:root:2024-04-26 13:39:30, Train, Epoch : 9, Step : 5070, Loss : 0.24300, Acc : 0.919, Sensitive_Loss : 0.06298, Sensitive_Acc : 17.200, Run Time : 12.82 sec
INFO:root:2024-04-26 13:39:42, Train, Epoch : 9, Step : 5080, Loss : 0.26749, Acc : 0.897, Sensitive_Loss : 0.06800, Sensitive_Acc : 15.800, Run Time : 11.94 sec
INFO:root:2024-04-26 13:39:53, Train, Epoch : 9, Step : 5090, Loss : 0.25347, Acc : 0.887, Sensitive_Loss : 0.05132, Sensitive_Acc : 17.200, Run Time : 11.36 sec
INFO:root:2024-04-26 13:40:04, Train, Epoch : 9, Step : 5100, Loss : 0.26889, Acc : 0.900, Sensitive_Loss : 0.06625, Sensitive_Acc : 16.600, Run Time : 11.46 sec
INFO:root:2024-04-26 13:42:40, Dev, Step : 5100, Loss : 0.47074, Acc : 0.814, Auc : 0.900, Sensitive_Loss : 0.12363, Sensitive_Acc : 16.864, Sensitive_Auc : 0.988, Mean auc: 0.900, Run Time : 155.81 sec
INFO:root:2024-04-26 13:42:49, Train, Epoch : 9, Step : 5110, Loss : 0.25328, Acc : 0.891, Sensitive_Loss : 0.07378, Sensitive_Acc : 16.300, Run Time : 164.54 sec
INFO:root:2024-04-26 13:43:01, Train, Epoch : 9, Step : 5120, Loss : 0.31437, Acc : 0.822, Sensitive_Loss : 0.07004, Sensitive_Acc : 15.700, Run Time : 12.10 sec
INFO:root:2024-04-26 13:43:12, Train, Epoch : 9, Step : 5130, Loss : 0.25757, Acc : 0.881, Sensitive_Loss : 0.05756, Sensitive_Acc : 16.000, Run Time : 11.39 sec
INFO:root:2024-04-26 13:43:24, Train, Epoch : 9, Step : 5140, Loss : 0.28790, Acc : 0.872, Sensitive_Loss : 0.05209, Sensitive_Acc : 17.100, Run Time : 11.92 sec
INFO:root:2024-04-26 13:43:36, Train, Epoch : 9, Step : 5150, Loss : 0.30517, Acc : 0.875, Sensitive_Loss : 0.08601, Sensitive_Acc : 16.500, Run Time : 11.28 sec
INFO:root:2024-04-26 13:43:47, Train, Epoch : 9, Step : 5160, Loss : 0.26986, Acc : 0.894, Sensitive_Loss : 0.06934, Sensitive_Acc : 16.800, Run Time : 11.41 sec
INFO:root:2024-04-26 13:43:59, Train, Epoch : 9, Step : 5170, Loss : 0.31225, Acc : 0.884, Sensitive_Loss : 0.06442, Sensitive_Acc : 16.300, Run Time : 11.45 sec
INFO:root:2024-04-26 13:44:11, Train, Epoch : 9, Step : 5180, Loss : 0.22197, Acc : 0.912, Sensitive_Loss : 0.08355, Sensitive_Acc : 15.500, Run Time : 12.03 sec
INFO:root:2024-04-26 13:44:22, Train, Epoch : 9, Step : 5190, Loss : 0.27566, Acc : 0.903, Sensitive_Loss : 0.05785, Sensitive_Acc : 15.700, Run Time : 11.58 sec
INFO:root:2024-04-26 13:44:34, Train, Epoch : 9, Step : 5200, Loss : 0.25819, Acc : 0.900, Sensitive_Loss : 0.07601, Sensitive_Acc : 17.400, Run Time : 11.36 sec
INFO:root:2024-04-26 13:47:09, Dev, Step : 5200, Loss : 0.46055, Acc : 0.819, Auc : 0.903, Sensitive_Loss : 0.12809, Sensitive_Acc : 16.807, Sensitive_Auc : 0.989, Mean auc: 0.903, Run Time : 155.80 sec
INFO:root:2024-04-26 13:47:18, Train, Epoch : 9, Step : 5210, Loss : 0.25897, Acc : 0.897, Sensitive_Loss : 0.07883, Sensitive_Acc : 14.600, Run Time : 164.28 sec
INFO:root:2024-04-26 13:47:29, Train, Epoch : 9, Step : 5220, Loss : 0.28118, Acc : 0.884, Sensitive_Loss : 0.08741, Sensitive_Acc : 16.900, Run Time : 11.43 sec
INFO:root:2024-04-26 13:47:40, Train, Epoch : 9, Step : 5230, Loss : 0.29780, Acc : 0.875, Sensitive_Loss : 0.05030, Sensitive_Acc : 16.100, Run Time : 10.99 sec
INFO:root:2024-04-26 13:47:52, Train, Epoch : 9, Step : 5240, Loss : 0.25258, Acc : 0.884, Sensitive_Loss : 0.05513, Sensitive_Acc : 18.400, Run Time : 12.07 sec
INFO:root:2024-04-26 13:48:04, Train, Epoch : 9, Step : 5250, Loss : 0.27286, Acc : 0.894, Sensitive_Loss : 0.06786, Sensitive_Acc : 17.700, Run Time : 11.54 sec
INFO:root:2024-04-26 13:48:15, Train, Epoch : 9, Step : 5260, Loss : 0.28915, Acc : 0.884, Sensitive_Loss : 0.05703, Sensitive_Acc : 16.900, Run Time : 11.17 sec
INFO:root:2024-04-26 13:48:28, Train, Epoch : 9, Step : 5270, Loss : 0.22974, Acc : 0.922, Sensitive_Loss : 0.06445, Sensitive_Acc : 15.700, Run Time : 12.55 sec
INFO:root:2024-04-26 13:48:38, Train, Epoch : 9, Step : 5280, Loss : 0.34315, Acc : 0.863, Sensitive_Loss : 0.05409, Sensitive_Acc : 14.700, Run Time : 10.53 sec
INFO:root:2024-04-26 13:48:50, Train, Epoch : 9, Step : 5290, Loss : 0.21275, Acc : 0.912, Sensitive_Loss : 0.06694, Sensitive_Acc : 16.200, Run Time : 11.80 sec
INFO:root:2024-04-26 13:49:01, Train, Epoch : 9, Step : 5300, Loss : 0.29572, Acc : 0.894, Sensitive_Loss : 0.06261, Sensitive_Acc : 16.100, Run Time : 11.43 sec
INFO:root:2024-04-26 13:51:37, Dev, Step : 5300, Loss : 0.46364, Acc : 0.810, Auc : 0.900, Sensitive_Loss : 0.11552, Sensitive_Acc : 16.864, Sensitive_Auc : 0.991, Mean auc: 0.900, Run Time : 155.86 sec
INFO:root:2024-04-26 13:51:46, Train, Epoch : 9, Step : 5310, Loss : 0.22974, Acc : 0.900, Sensitive_Loss : 0.05745, Sensitive_Acc : 16.300, Run Time : 164.59 sec
INFO:root:2024-04-26 13:51:57, Train, Epoch : 9, Step : 5320, Loss : 0.26711, Acc : 0.906, Sensitive_Loss : 0.05322, Sensitive_Acc : 16.800, Run Time : 11.25 sec
INFO:root:2024-04-26 13:52:09, Train, Epoch : 9, Step : 5330, Loss : 0.23417, Acc : 0.884, Sensitive_Loss : 0.07441, Sensitive_Acc : 16.400, Run Time : 11.61 sec
INFO:root:2024-04-26 13:52:20, Train, Epoch : 9, Step : 5340, Loss : 0.23912, Acc : 0.897, Sensitive_Loss : 0.05065, Sensitive_Acc : 16.700, Run Time : 11.57 sec
INFO:root:2024-04-26 13:52:32, Train, Epoch : 9, Step : 5350, Loss : 0.24998, Acc : 0.897, Sensitive_Loss : 0.03788, Sensitive_Acc : 16.500, Run Time : 11.54 sec
INFO:root:2024-04-26 13:52:45, Train, Epoch : 9, Step : 5360, Loss : 0.19344, Acc : 0.934, Sensitive_Loss : 0.03032, Sensitive_Acc : 16.800, Run Time : 12.61 sec
INFO:root:2024-04-26 13:52:56, Train, Epoch : 9, Step : 5370, Loss : 0.24430, Acc : 0.834, Sensitive_Loss : 0.09272, Sensitive_Acc : 16.400, Run Time : 11.35 sec
INFO:root:2024-04-26 13:53:06, Train, Epoch : 9, Step : 5380, Loss : 0.23320, Acc : 0.891, Sensitive_Loss : 0.05826, Sensitive_Acc : 16.700, Run Time : 10.56 sec
INFO:root:2024-04-26 13:53:18, Train, Epoch : 9, Step : 5390, Loss : 0.22232, Acc : 0.903, Sensitive_Loss : 0.05057, Sensitive_Acc : 15.700, Run Time : 11.88 sec
INFO:root:2024-04-26 13:53:29, Train, Epoch : 9, Step : 5400, Loss : 0.25538, Acc : 0.897, Sensitive_Loss : 0.05961, Sensitive_Acc : 16.100, Run Time : 10.43 sec
INFO:root:2024-04-26 13:56:05, Dev, Step : 5400, Loss : 0.45117, Acc : 0.825, Auc : 0.904, Sensitive_Loss : 0.11709, Sensitive_Acc : 16.807, Sensitive_Auc : 0.989, Mean auc: 0.904, Run Time : 156.74 sec
INFO:root:2024-04-26 13:56:14, Train, Epoch : 9, Step : 5410, Loss : 0.26539, Acc : 0.894, Sensitive_Loss : 0.04280, Sensitive_Acc : 16.400, Run Time : 165.42 sec
INFO:root:2024-04-26 13:56:25, Train, Epoch : 9, Step : 5420, Loss : 0.23454, Acc : 0.909, Sensitive_Loss : 0.06254, Sensitive_Acc : 15.300, Run Time : 11.23 sec
INFO:root:2024-04-26 13:56:37, Train, Epoch : 9, Step : 5430, Loss : 0.24490, Acc : 0.866, Sensitive_Loss : 0.10581, Sensitive_Acc : 15.500, Run Time : 11.61 sec
INFO:root:2024-04-26 13:56:48, Train, Epoch : 9, Step : 5440, Loss : 0.27764, Acc : 0.869, Sensitive_Loss : 0.06215, Sensitive_Acc : 15.700, Run Time : 11.42 sec
INFO:root:2024-04-26 13:56:59, Train, Epoch : 9, Step : 5450, Loss : 0.29027, Acc : 0.878, Sensitive_Loss : 0.08893, Sensitive_Acc : 17.700, Run Time : 10.65 sec
INFO:root:2024-04-26 13:57:11, Train, Epoch : 9, Step : 5460, Loss : 0.30864, Acc : 0.884, Sensitive_Loss : 0.06100, Sensitive_Acc : 16.800, Run Time : 11.44 sec
INFO:root:2024-04-26 13:57:22, Train, Epoch : 9, Step : 5470, Loss : 0.27768, Acc : 0.887, Sensitive_Loss : 0.08468, Sensitive_Acc : 18.800, Run Time : 11.25 sec
INFO:root:2024-04-26 13:57:34, Train, Epoch : 9, Step : 5480, Loss : 0.32006, Acc : 0.853, Sensitive_Loss : 0.08150, Sensitive_Acc : 16.200, Run Time : 11.97 sec
INFO:root:2024-04-26 13:57:46, Train, Epoch : 9, Step : 5490, Loss : 0.28035, Acc : 0.875, Sensitive_Loss : 0.04713, Sensitive_Acc : 14.200, Run Time : 12.21 sec
INFO:root:2024-04-26 13:57:57, Train, Epoch : 9, Step : 5500, Loss : 0.24483, Acc : 0.903, Sensitive_Loss : 0.07301, Sensitive_Acc : 16.100, Run Time : 10.73 sec
INFO:root:2024-04-26 14:00:38, Dev, Step : 5500, Loss : 0.44604, Acc : 0.821, Auc : 0.900, Sensitive_Loss : 0.10518, Sensitive_Acc : 16.807, Sensitive_Auc : 0.993, Mean auc: 0.900, Run Time : 161.29 sec
INFO:root:2024-04-26 14:00:46, Train, Epoch : 9, Step : 5510, Loss : 0.20540, Acc : 0.900, Sensitive_Loss : 0.04633, Sensitive_Acc : 16.000, Run Time : 169.75 sec
INFO:root:2024-04-26 14:00:59, Train, Epoch : 9, Step : 5520, Loss : 0.23129, Acc : 0.912, Sensitive_Loss : 0.07781, Sensitive_Acc : 17.100, Run Time : 12.41 sec
INFO:root:2024-04-26 14:01:11, Train, Epoch : 9, Step : 5530, Loss : 0.21396, Acc : 0.894, Sensitive_Loss : 0.08388, Sensitive_Acc : 15.700, Run Time : 12.33 sec
INFO:root:2024-04-26 14:01:28, Train, Epoch : 9, Step : 5540, Loss : 0.27648, Acc : 0.859, Sensitive_Loss : 0.10233, Sensitive_Acc : 16.400, Run Time : 17.33 sec
INFO:root:2024-04-26 14:01:43, Train, Epoch : 9, Step : 5550, Loss : 0.26110, Acc : 0.897, Sensitive_Loss : 0.07985, Sensitive_Acc : 18.500, Run Time : 14.13 sec
INFO:root:2024-04-26 14:01:54, Train, Epoch : 9, Step : 5560, Loss : 0.26582, Acc : 0.884, Sensitive_Loss : 0.06621, Sensitive_Acc : 16.400, Run Time : 11.70 sec
INFO:root:2024-04-26 14:02:08, Train, Epoch : 9, Step : 5570, Loss : 0.26830, Acc : 0.887, Sensitive_Loss : 0.05795, Sensitive_Acc : 16.700, Run Time : 13.36 sec
INFO:root:2024-04-26 14:02:20, Train, Epoch : 9, Step : 5580, Loss : 0.28973, Acc : 0.881, Sensitive_Loss : 0.05566, Sensitive_Acc : 15.600, Run Time : 12.49 sec
INFO:root:2024-04-26 14:02:33, Train, Epoch : 9, Step : 5590, Loss : 0.29613, Acc : 0.878, Sensitive_Loss : 0.08498, Sensitive_Acc : 17.800, Run Time : 12.37 sec
INFO:root:2024-04-26 14:02:44, Train, Epoch : 9, Step : 5600, Loss : 0.24763, Acc : 0.903, Sensitive_Loss : 0.08478, Sensitive_Acc : 16.400, Run Time : 11.52 sec
INFO:root:2024-04-26 14:05:20, Dev, Step : 5600, Loss : 0.47364, Acc : 0.811, Auc : 0.897, Sensitive_Loss : 0.10419, Sensitive_Acc : 16.707, Sensitive_Auc : 0.991, Mean auc: 0.897, Run Time : 156.32 sec
INFO:root:2024-04-26 14:05:31, Train, Epoch : 9, Step : 5610, Loss : 0.23920, Acc : 0.900, Sensitive_Loss : 0.07671, Sensitive_Acc : 16.300, Run Time : 166.90 sec
INFO:root:2024-04-26 14:05:44, Train, Epoch : 9, Step : 5620, Loss : 0.30534, Acc : 0.884, Sensitive_Loss : 0.08184, Sensitive_Acc : 16.500, Run Time : 13.36 sec
INFO:root:2024-04-26 14:05:56, Train, Epoch : 9, Step : 5630, Loss : 0.23291, Acc : 0.878, Sensitive_Loss : 0.09286, Sensitive_Acc : 16.500, Run Time : 11.72 sec
INFO:root:2024-04-26 14:08:34
INFO:root:y_pred: [0.1495949  0.97292656 0.00331271 ... 0.78476465 0.00486322 0.6116095 ]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.83709693e-01 3.48750400e-05 2.19731149e-03 3.80595066e-06
 9.94849622e-01 1.75159835e-06 9.99489903e-01 9.99447525e-01
 2.17641107e-04 9.39016998e-01 9.95806575e-01 9.99508739e-01
 9.98833597e-01 9.32481289e-01 4.07301867e-03 9.79212642e-01
 9.99967933e-01 1.22178486e-02 8.87748659e-01 9.98130500e-01
 9.98845577e-01 1.85963795e-01 9.99925971e-01 9.86530483e-01
 9.98872578e-01 9.39596117e-01 3.48851499e-05 9.96995926e-01
 9.97574508e-01 8.24726045e-01 1.00464160e-02 1.68545857e-01
 7.51737654e-02 8.26739799e-03 7.82242641e-02 1.60949749e-05
 1.85792204e-02 3.52690346e-04 9.73506272e-01 9.99081969e-01
 3.09197617e-06 3.08605697e-04 9.99594033e-01 9.43814484e-06
 9.99888420e-01 9.96385932e-01 9.89798784e-01 9.98837411e-01
 5.08836471e-02 9.97398138e-01 9.99732673e-01 2.47671455e-03
 2.66355664e-01 3.83864790e-06 2.59792054e-04 3.21744289e-03
 1.18856147e-01 1.10526204e-01 8.99080536e-04 6.47227615e-02
 2.53913319e-03 1.17318556e-01 1.24085424e-02 9.79316294e-01
 2.71620182e-03 9.99629617e-01 8.07519536e-03 9.99224424e-01
 8.20335627e-01 4.36628371e-01 7.83112228e-01 3.95006776e-01
 2.89962831e-04 1.38976471e-02 8.22647144e-06 6.32685351e-06
 1.20982036e-01 7.49173939e-01 3.88018205e-04 9.98213291e-01
 9.99883294e-01 1.16138754e-05 2.39050329e-01 7.56888883e-04
 9.97086585e-01 9.58237112e-01 1.68209593e-03 1.88076752e-03
 8.93052459e-01 9.97911394e-01 9.99222875e-01 2.65205085e-01
 6.58339995e-04 9.96469975e-01 4.82657343e-01 2.34135819e-04
 9.68626559e-01 9.81991231e-01 4.17922456e-05 4.67072154e-04
 9.89289820e-01 9.94489670e-01 9.99903321e-01 9.99688983e-01
 1.43346362e-04 2.90550441e-02 7.71296442e-01 8.79151344e-01
 8.38900506e-01 1.35498296e-04 9.05776024e-01 9.95857060e-01
 3.25047262e-02 9.99305725e-01 9.85945165e-01 9.98872101e-01
 8.13839734e-01 9.99463141e-01 1.23401411e-01 5.27166650e-02
 9.99710977e-01 9.98728454e-01 5.65106493e-05 8.63876045e-01
 9.95654464e-01 1.05604446e-02 9.99209285e-01 1.13146473e-03
 6.52406737e-02 9.80665863e-01 9.94704902e-01 4.82317191e-05
 9.92826419e-04 1.56553742e-02 9.99185145e-01 9.99786198e-01
 9.80131567e-01 1.14562467e-03 1.18498469e-03 9.97996092e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 14:08:34, Dev, Step : 5634, Loss : 0.47518, Acc : 0.824, Auc : 0.902, Sensitive_Loss : 0.11780, Sensitive_Acc : 16.807, Sensitive_Auc : 0.992, Mean auc: 0.902, Run Time : 153.57 sec
INFO:root:2024-04-26 14:08:44, Train, Epoch : 10, Step : 5640, Loss : 0.14553, Acc : 0.537, Sensitive_Loss : 0.03633, Sensitive_Acc : 9.400, Run Time : 8.60 sec
INFO:root:2024-04-26 14:08:56, Train, Epoch : 10, Step : 5650, Loss : 0.29709, Acc : 0.875, Sensitive_Loss : 0.04957, Sensitive_Acc : 16.300, Run Time : 12.53 sec
INFO:root:2024-04-26 14:09:08, Train, Epoch : 10, Step : 5660, Loss : 0.22299, Acc : 0.900, Sensitive_Loss : 0.05391, Sensitive_Acc : 15.700, Run Time : 11.49 sec
INFO:root:2024-04-26 14:09:19, Train, Epoch : 10, Step : 5670, Loss : 0.22592, Acc : 0.916, Sensitive_Loss : 0.06580, Sensitive_Acc : 16.500, Run Time : 11.70 sec
INFO:root:2024-04-26 14:09:31, Train, Epoch : 10, Step : 5680, Loss : 0.21757, Acc : 0.912, Sensitive_Loss : 0.03774, Sensitive_Acc : 16.000, Run Time : 11.94 sec
INFO:root:2024-04-26 14:09:43, Train, Epoch : 10, Step : 5690, Loss : 0.21440, Acc : 0.922, Sensitive_Loss : 0.06833, Sensitive_Acc : 17.300, Run Time : 11.99 sec
INFO:root:2024-04-26 14:09:57, Train, Epoch : 10, Step : 5700, Loss : 0.19669, Acc : 0.931, Sensitive_Loss : 0.05821, Sensitive_Acc : 16.000, Run Time : 14.10 sec
INFO:root:2024-04-26 14:12:33, Dev, Step : 5700, Loss : 0.44478, Acc : 0.830, Auc : 0.902, Sensitive_Loss : 0.10925, Sensitive_Acc : 16.807, Sensitive_Auc : 0.990, Mean auc: 0.902, Run Time : 155.94 sec
INFO:root:2024-04-26 14:12:42, Train, Epoch : 10, Step : 5710, Loss : 0.21824, Acc : 0.909, Sensitive_Loss : 0.05773, Sensitive_Acc : 16.200, Run Time : 164.40 sec
INFO:root:2024-04-26 14:12:53, Train, Epoch : 10, Step : 5720, Loss : 0.23065, Acc : 0.900, Sensitive_Loss : 0.05837, Sensitive_Acc : 17.200, Run Time : 11.44 sec
INFO:root:2024-04-26 14:13:05, Train, Epoch : 10, Step : 5730, Loss : 0.15665, Acc : 0.950, Sensitive_Loss : 0.05197, Sensitive_Acc : 16.600, Run Time : 11.52 sec
INFO:root:2024-04-26 14:13:17, Train, Epoch : 10, Step : 5740, Loss : 0.25952, Acc : 0.903, Sensitive_Loss : 0.08326, Sensitive_Acc : 17.500, Run Time : 11.80 sec
INFO:root:2024-04-26 14:13:28, Train, Epoch : 10, Step : 5750, Loss : 0.28156, Acc : 0.881, Sensitive_Loss : 0.04996, Sensitive_Acc : 14.900, Run Time : 10.92 sec
INFO:root:2024-04-26 14:13:40, Train, Epoch : 10, Step : 5760, Loss : 0.27564, Acc : 0.872, Sensitive_Loss : 0.03983, Sensitive_Acc : 16.400, Run Time : 12.52 sec
INFO:root:2024-04-26 14:13:53, Train, Epoch : 10, Step : 5770, Loss : 0.18380, Acc : 0.912, Sensitive_Loss : 0.06061, Sensitive_Acc : 17.900, Run Time : 12.47 sec
INFO:root:2024-04-26 14:14:05, Train, Epoch : 10, Step : 5780, Loss : 0.21463, Acc : 0.925, Sensitive_Loss : 0.04496, Sensitive_Acc : 16.400, Run Time : 12.27 sec
INFO:root:2024-04-26 14:14:16, Train, Epoch : 10, Step : 5790, Loss : 0.20573, Acc : 0.909, Sensitive_Loss : 0.07207, Sensitive_Acc : 15.000, Run Time : 11.61 sec
INFO:root:2024-04-26 14:14:29, Train, Epoch : 10, Step : 5800, Loss : 0.29504, Acc : 0.887, Sensitive_Loss : 0.07344, Sensitive_Acc : 16.800, Run Time : 12.60 sec
INFO:root:2024-04-26 14:17:06, Dev, Step : 5800, Loss : 0.45871, Acc : 0.823, Auc : 0.902, Sensitive_Loss : 0.09752, Sensitive_Acc : 16.807, Sensitive_Auc : 0.991, Mean auc: 0.902, Run Time : 156.71 sec
INFO:root:2024-04-26 14:17:14, Train, Epoch : 10, Step : 5810, Loss : 0.20531, Acc : 0.919, Sensitive_Loss : 0.07844, Sensitive_Acc : 16.500, Run Time : 165.30 sec
INFO:root:2024-04-26 14:17:26, Train, Epoch : 10, Step : 5820, Loss : 0.28355, Acc : 0.866, Sensitive_Loss : 0.10091, Sensitive_Acc : 17.700, Run Time : 11.93 sec
INFO:root:2024-04-26 14:17:39, Train, Epoch : 10, Step : 5830, Loss : 0.24037, Acc : 0.900, Sensitive_Loss : 0.08585, Sensitive_Acc : 16.400, Run Time : 12.47 sec
INFO:root:2024-04-26 14:17:51, Train, Epoch : 10, Step : 5840, Loss : 0.22413, Acc : 0.881, Sensitive_Loss : 0.09571, Sensitive_Acc : 16.900, Run Time : 12.33 sec
INFO:root:2024-04-26 14:18:03, Train, Epoch : 10, Step : 5850, Loss : 0.23467, Acc : 0.922, Sensitive_Loss : 0.04970, Sensitive_Acc : 17.400, Run Time : 12.04 sec
INFO:root:2024-04-26 14:18:16, Train, Epoch : 10, Step : 5860, Loss : 0.27505, Acc : 0.909, Sensitive_Loss : 0.08863, Sensitive_Acc : 17.500, Run Time : 12.56 sec
INFO:root:2024-04-26 14:18:29, Train, Epoch : 10, Step : 5870, Loss : 0.25189, Acc : 0.897, Sensitive_Loss : 0.07454, Sensitive_Acc : 16.900, Run Time : 13.56 sec
INFO:root:2024-04-26 14:18:42, Train, Epoch : 10, Step : 5880, Loss : 0.23088, Acc : 0.922, Sensitive_Loss : 0.04751, Sensitive_Acc : 16.600, Run Time : 12.64 sec
INFO:root:2024-04-26 14:18:55, Train, Epoch : 10, Step : 5890, Loss : 0.21386, Acc : 0.903, Sensitive_Loss : 0.04817, Sensitive_Acc : 16.900, Run Time : 12.67 sec
INFO:root:2024-04-26 14:19:06, Train, Epoch : 10, Step : 5900, Loss : 0.22659, Acc : 0.897, Sensitive_Loss : 0.06475, Sensitive_Acc : 15.900, Run Time : 11.88 sec
INFO:root:2024-04-26 14:21:43, Dev, Step : 5900, Loss : 0.47196, Acc : 0.819, Auc : 0.900, Sensitive_Loss : 0.10616, Sensitive_Acc : 16.807, Sensitive_Auc : 0.991, Mean auc: 0.900, Run Time : 156.70 sec
INFO:root:2024-04-26 14:21:53, Train, Epoch : 10, Step : 5910, Loss : 0.16815, Acc : 0.909, Sensitive_Loss : 0.06564, Sensitive_Acc : 15.700, Run Time : 166.91 sec
INFO:root:2024-04-26 14:22:09, Train, Epoch : 10, Step : 5920, Loss : 0.22290, Acc : 0.887, Sensitive_Loss : 0.06427, Sensitive_Acc : 17.500, Run Time : 15.83 sec
INFO:root:2024-04-26 14:22:26, Train, Epoch : 10, Step : 5930, Loss : 0.24451, Acc : 0.891, Sensitive_Loss : 0.07157, Sensitive_Acc : 15.800, Run Time : 16.61 sec
INFO:root:2024-04-26 14:22:42, Train, Epoch : 10, Step : 5940, Loss : 0.25198, Acc : 0.881, Sensitive_Loss : 0.09202, Sensitive_Acc : 16.900, Run Time : 16.24 sec
INFO:root:2024-04-26 14:22:55, Train, Epoch : 10, Step : 5950, Loss : 0.24746, Acc : 0.887, Sensitive_Loss : 0.07068, Sensitive_Acc : 18.200, Run Time : 12.89 sec
INFO:root:2024-04-26 14:23:08, Train, Epoch : 10, Step : 5960, Loss : 0.27311, Acc : 0.881, Sensitive_Loss : 0.10032, Sensitive_Acc : 17.100, Run Time : 13.51 sec
INFO:root:2024-04-26 14:23:20, Train, Epoch : 10, Step : 5970, Loss : 0.19143, Acc : 0.925, Sensitive_Loss : 0.05864, Sensitive_Acc : 17.500, Run Time : 12.01 sec
INFO:root:2024-04-26 14:23:33, Train, Epoch : 10, Step : 5980, Loss : 0.26978, Acc : 0.875, Sensitive_Loss : 0.07438, Sensitive_Acc : 16.000, Run Time : 12.96 sec
INFO:root:2024-04-26 14:23:47, Train, Epoch : 10, Step : 5990, Loss : 0.25295, Acc : 0.872, Sensitive_Loss : 0.05438, Sensitive_Acc : 17.000, Run Time : 13.22 sec
INFO:root:2024-04-26 14:23:59, Train, Epoch : 10, Step : 6000, Loss : 0.22584, Acc : 0.916, Sensitive_Loss : 0.07939, Sensitive_Acc : 16.300, Run Time : 11.91 sec
INFO:root:2024-04-26 14:26:34, Dev, Step : 6000, Loss : 0.48032, Acc : 0.820, Auc : 0.901, Sensitive_Loss : 0.11509, Sensitive_Acc : 16.807, Sensitive_Auc : 0.993, Mean auc: 0.901, Run Time : 155.33 sec
INFO:root:2024-04-26 14:26:42, Train, Epoch : 10, Step : 6010, Loss : 0.23816, Acc : 0.909, Sensitive_Loss : 0.05406, Sensitive_Acc : 17.400, Run Time : 163.60 sec
INFO:root:2024-04-26 14:26:54, Train, Epoch : 10, Step : 6020, Loss : 0.20267, Acc : 0.912, Sensitive_Loss : 0.06337, Sensitive_Acc : 17.100, Run Time : 12.38 sec
INFO:root:2024-04-26 14:27:08, Train, Epoch : 10, Step : 6030, Loss : 0.17679, Acc : 0.934, Sensitive_Loss : 0.05522, Sensitive_Acc : 15.100, Run Time : 13.02 sec
INFO:root:2024-04-26 14:27:20, Train, Epoch : 10, Step : 6040, Loss : 0.37976, Acc : 0.847, Sensitive_Loss : 0.04951, Sensitive_Acc : 16.400, Run Time : 12.86 sec
INFO:root:2024-04-26 14:27:40, Train, Epoch : 10, Step : 6050, Loss : 0.25992, Acc : 0.884, Sensitive_Loss : 0.05800, Sensitive_Acc : 14.500, Run Time : 19.29 sec
INFO:root:2024-04-26 14:27:59, Train, Epoch : 10, Step : 6060, Loss : 0.29394, Acc : 0.887, Sensitive_Loss : 0.04648, Sensitive_Acc : 16.700, Run Time : 19.14 sec
INFO:root:2024-04-26 14:28:22, Train, Epoch : 10, Step : 6070, Loss : 0.22174, Acc : 0.881, Sensitive_Loss : 0.07008, Sensitive_Acc : 16.400, Run Time : 23.58 sec
INFO:root:2024-04-26 14:28:45, Train, Epoch : 10, Step : 6080, Loss : 0.22965, Acc : 0.894, Sensitive_Loss : 0.08136, Sensitive_Acc : 18.100, Run Time : 22.41 sec
INFO:root:2024-04-26 14:28:56, Train, Epoch : 10, Step : 6090, Loss : 0.26464, Acc : 0.894, Sensitive_Loss : 0.06571, Sensitive_Acc : 15.400, Run Time : 11.47 sec
INFO:root:2024-04-26 14:29:08, Train, Epoch : 10, Step : 6100, Loss : 0.23883, Acc : 0.912, Sensitive_Loss : 0.05038, Sensitive_Acc : 19.000, Run Time : 12.03 sec
INFO:root:2024-04-26 14:31:50, Dev, Step : 6100, Loss : 0.46885, Acc : 0.820, Auc : 0.900, Sensitive_Loss : 0.09703, Sensitive_Acc : 16.779, Sensitive_Auc : 0.991, Mean auc: 0.900, Run Time : 162.02 sec
INFO:root:2024-04-26 14:31:59, Train, Epoch : 10, Step : 6110, Loss : 0.22878, Acc : 0.875, Sensitive_Loss : 0.04434, Sensitive_Acc : 15.700, Run Time : 170.49 sec
INFO:root:2024-04-26 14:32:11, Train, Epoch : 10, Step : 6120, Loss : 0.25944, Acc : 0.850, Sensitive_Loss : 0.06345, Sensitive_Acc : 16.100, Run Time : 12.51 sec
INFO:root:2024-04-26 14:32:23, Train, Epoch : 10, Step : 6130, Loss : 0.29458, Acc : 0.894, Sensitive_Loss : 0.07342, Sensitive_Acc : 16.200, Run Time : 11.24 sec
INFO:root:2024-04-26 14:32:35, Train, Epoch : 10, Step : 6140, Loss : 0.28399, Acc : 0.891, Sensitive_Loss : 0.08932, Sensitive_Acc : 16.600, Run Time : 12.77 sec
INFO:root:2024-04-26 14:32:48, Train, Epoch : 10, Step : 6150, Loss : 0.22056, Acc : 0.925, Sensitive_Loss : 0.08199, Sensitive_Acc : 15.400, Run Time : 12.48 sec
INFO:root:2024-04-26 14:33:00, Train, Epoch : 10, Step : 6160, Loss : 0.29316, Acc : 0.878, Sensitive_Loss : 0.09252, Sensitive_Acc : 16.600, Run Time : 11.86 sec
INFO:root:2024-04-26 14:33:12, Train, Epoch : 10, Step : 6170, Loss : 0.20678, Acc : 0.922, Sensitive_Loss : 0.06030, Sensitive_Acc : 17.400, Run Time : 12.14 sec
INFO:root:2024-04-26 14:33:24, Train, Epoch : 10, Step : 6180, Loss : 0.28766, Acc : 0.894, Sensitive_Loss : 0.09060, Sensitive_Acc : 16.900, Run Time : 12.05 sec
INFO:root:2024-04-26 14:33:36, Train, Epoch : 10, Step : 6190, Loss : 0.33877, Acc : 0.859, Sensitive_Loss : 0.06652, Sensitive_Acc : 16.200, Run Time : 11.72 sec
INFO:root:2024-04-26 14:33:48, Train, Epoch : 10, Step : 6200, Loss : 0.24085, Acc : 0.900, Sensitive_Loss : 0.06518, Sensitive_Acc : 16.400, Run Time : 12.22 sec
INFO:root:2024-04-26 14:36:28, Dev, Step : 6200, Loss : 0.47368, Acc : 0.817, Auc : 0.900, Sensitive_Loss : 0.10259, Sensitive_Acc : 16.850, Sensitive_Auc : 0.991, Mean auc: 0.900, Run Time : 160.62 sec
INFO:root:2024-04-26 14:36:37, Train, Epoch : 10, Step : 6210, Loss : 0.23569, Acc : 0.897, Sensitive_Loss : 0.07093, Sensitive_Acc : 15.800, Run Time : 169.41 sec
INFO:root:2024-04-26 14:36:49, Train, Epoch : 10, Step : 6220, Loss : 0.21814, Acc : 0.919, Sensitive_Loss : 0.06683, Sensitive_Acc : 16.400, Run Time : 11.44 sec
INFO:root:2024-04-26 14:37:01, Train, Epoch : 10, Step : 6230, Loss : 0.23722, Acc : 0.912, Sensitive_Loss : 0.06859, Sensitive_Acc : 16.700, Run Time : 12.76 sec
INFO:root:2024-04-26 14:37:14, Train, Epoch : 10, Step : 6240, Loss : 0.27188, Acc : 0.869, Sensitive_Loss : 0.06046, Sensitive_Acc : 14.600, Run Time : 12.49 sec
INFO:root:2024-04-26 14:37:25, Train, Epoch : 10, Step : 6250, Loss : 0.28245, Acc : 0.878, Sensitive_Loss : 0.06547, Sensitive_Acc : 17.900, Run Time : 11.52 sec
INFO:root:2024-04-26 14:37:37, Train, Epoch : 10, Step : 6260, Loss : 0.30746, Acc : 0.881, Sensitive_Loss : 0.04727, Sensitive_Acc : 15.900, Run Time : 11.34 sec
INFO:root:2024-04-26 14:40:10
INFO:root:y_pred: [0.03567201 0.9261818  0.00361587 ... 0.4128169  0.00227289 0.68426794]
INFO:root:y_true: [0. 1. 0. ... 1. 0. 1.]
INFO:root:sensitive_y_pred: [9.9202901e-01 4.1662670e-06 2.1210143e-04 3.2793773e-07 9.9695480e-01
 3.4259261e-07 9.9937624e-01 9.9928600e-01 3.2463908e-05 9.0417147e-01
 9.9641442e-01 9.9898463e-01 9.9704969e-01 9.0341759e-01 2.7998051e-04
 9.6459389e-01 9.9984288e-01 7.2899921e-04 6.9015354e-01 9.9657995e-01
 9.9701047e-01 8.4758364e-03 9.9983943e-01 9.7063577e-01 9.9304730e-01
 8.7687272e-01 3.8106866e-06 9.9273574e-01 9.9632251e-01 5.1244080e-01
 1.0523901e-03 2.7493216e-02 5.1919115e-03 7.7641575e-04 2.2815347e-02
 1.1296198e-06 8.3475176e-04 2.9752708e-05 9.4295532e-01 9.9826592e-01
 1.7074743e-07 1.3685206e-05 9.9915481e-01 4.9731489e-07 9.9973243e-01
 9.9488950e-01 9.7691423e-01 9.9613106e-01 2.0218233e-03 9.8978245e-01
 9.9953222e-01 3.3379358e-04 2.3398052e-01 8.8889936e-07 3.1773659e-05
 3.9854957e-04 1.0570129e-02 8.4973738e-02 3.5509220e-05 1.3402914e-02
 1.1423517e-03 2.6208660e-02 3.3779177e-04 9.1017503e-01 1.3509799e-04
 9.9924183e-01 2.2763689e-03 9.9691236e-01 8.1273496e-01 4.2454362e-02
 6.8427438e-01 6.3121811e-02 3.8350892e-05 4.8496099e-03 1.4254434e-06
 4.1830779e-07 7.0127569e-02 4.5218521e-01 4.8279717e-06 9.9723178e-01
 9.9918634e-01 3.0108328e-07 3.8786165e-02 3.5783618e-05 9.9445868e-01
 9.3431461e-01 1.1072937e-04 1.2738099e-04 8.1970447e-01 9.9691284e-01
 9.9836439e-01 5.4774467e-02 2.2369208e-05 9.8919755e-01 6.4505041e-02
 8.0030968e-06 9.6373558e-01 9.7053695e-01 8.6189211e-06 1.8497105e-04
 9.7433567e-01 9.9243414e-01 9.9989808e-01 9.9943441e-01 9.9739782e-06
 6.1706977e-04 7.4934977e-01 8.7966579e-01 7.6297456e-01 1.2116301e-05
 8.8769102e-01 9.9289531e-01 1.8544642e-02 9.9888474e-01 9.8153883e-01
 9.9734527e-01 2.6517797e-01 9.9903023e-01 4.2678371e-02 1.6531549e-02
 9.9900848e-01 9.9761331e-01 3.0743606e-06 8.3333576e-01 9.9147403e-01
 2.5302724e-03 9.9868625e-01 3.1181771e-05 6.9243638e-03 9.3895102e-01
 9.8596883e-01 3.8469157e-06 1.0722301e-04 2.8309450e-03 9.9488974e-01
 9.9912685e-01 9.8182696e-01 1.6207361e-04 8.1682345e-05 9.9777943e-01]
INFO:root:sensitive_y_true: [1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.
 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.
 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0.
 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.
 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.
 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]
INFO:root:2024-04-26 14:40:10, Dev, Step : 6260, Loss : 0.45303, Acc : 0.821, Auc : 0.900, Sensitive_Loss : 0.09730, Sensitive_Acc : 16.721, Sensitive_Auc : 0.992, Mean auc: 0.900, Run Time : 152.87 sec
