{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 14:28:25.547036: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ohpc/pub/mpi/openmpi3-gnu8/3.1.4/lib:/opt/ohpc/pub/compiler/gcc/8.3.0/lib64\n",
      "2024-03-12 14:28:25.547079: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import attr\n",
    "import importlib\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import ml_collections as mlc\n",
    "from ml_collections import ConfigDict\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "import subprocess\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import DataParallel\n",
    "from tensorboardX import SummaryWriter\n",
    "import easydict\n",
    "from easydict import EasyDict as edict\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib specific imports\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib import gridspec, cm\n",
    "\n",
    "# Custom project imports (assuming they are placed correctly in the project path)\n",
    "project_path = '/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray-main/shortcuts-chest-xray-main/'\n",
    "config_path = '/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray-main/shortcuts-chest-xray-main/config/config.json'\n",
    "sys.path.append(project_path)\n",
    "from data.tfdataset import ImageDataset\n",
    "from model.classifier import Classifier\n",
    "from model.utils import get_optimizer\n",
    "from utils.misc import lr_schedule\n",
    "\n",
    "# Environment configuration\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Seed setting for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, img_height=512, img_width=512):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    img = img / 255.0  # Normalize to [0,1]\n",
    "    return img\n",
    "\n",
    "def preprocess(images, pneumothorax_labels, support_devices_labels):\n",
    "    # Reshape labels to be 2-dimensional\n",
    "    pneumothorax_labels = tf.reshape(pneumothorax_labels, (-1, 1))\n",
    "    support_devices_labels = tf.reshape(support_devices_labels, (-1, 1))\n",
    "    return images, pneumothorax_labels, support_devices_labels\n",
    "\n",
    "\n",
    "def load_data(csv_file, base_path):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    image_paths = df['Path'].apply(lambda x: f\"{base_path}/{x}\").values\n",
    "    pneumothorax_labels = df['Pneumothorax'].values.astype(float)\n",
    "    support_devices_labels = df['Support Devices'].values.astype(float)\n",
    "    \n",
    "    image_ds = tf.data.Dataset.from_tensor_slices(image_paths).map(preprocess_image)\n",
    "    pneumothorax_label_ds = tf.data.Dataset.from_tensor_slices(pneumothorax_labels)\n",
    "    support_devices_label_ds = tf.data.Dataset.from_tensor_slices(support_devices_labels)\n",
    "    \n",
    "    return tf.data.Dataset.zip((image_ds, pneumothorax_label_ds, support_devices_label_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/home/pmen/Shortcuts-Problem-in-Medical-Image-Analysis/shortcuts-chest-xray-main/shortcuts-chest-xray-main/config/config.json'\n",
    "\n",
    "# Assuming you have loaded your config as a dictionary named cfg\n",
    "with open(config_path, 'r') as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "# Convert the dictionary to a ConfigDict which allows attribute-like access\n",
    "cfg = ConfigDict(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `cfg` is your configuration dictionary loaded from the JSON file\n",
    "dataset = load_data(cfg['train_csv'], cfg['base_path'])\n",
    "# Apply preprocessing to each element in the dataset\n",
    "dataset = dataset.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(cfg['train_batch_size'])#, drop_remainder=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: using repeat because I am running out of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Model parameters \n",
    "cfg = mlc.ConfigDict()\n",
    "# Example dimensions for a 512x512 RGB image\n",
    "n_pixels = 512\n",
    "n_channels = 3\n",
    "\n",
    "cfg.model = mlc.ConfigDict()\n",
    "cfg.model.width = 10  #@param architecture width\n",
    "cfg.model.depth = 3  #@param model depth\n",
    "cfg.model.use_bias = True  # whether we add biases to activations.\n",
    "cfg.model.activation = 'relu'\n",
    "cfg.model.x_dim = (n_pixels, n_pixels, n_channels)\n",
    "cfg.model.branch_dim = 2  # architecture width within each branch\n",
    "cfg.model.regularizer = None  # replace with e.g. 'l2' for weight decay\n",
    "\n",
    "# output head\n",
    "cfg.model.output_activation = 'sigmoid'\n",
    "cfg.model.output_dim = 1\n",
    "\n",
    "# attribute head\n",
    "cfg.model.attr_activation = 'sigmoid'\n",
    "cfg.model.attr_grad_updates = float(-0.05)\n",
    "cfg.model.attr_dim = 1\n",
    "# this is a tradeoff between the loss on A and the loss on target Y.\n",
    "# If it's zero, we ignore the attribute loss completely.\n",
    "cfg.attr_loss_weight = float(1.0)\n",
    "\n",
    "cfg.opt = mlc.ConfigDict()\n",
    "cfg.opt.name = 'adam'\n",
    "cfg.opt.learning_rate = 0.001\n",
    "cfg.opt.momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "## Baseline model - SingleHead class\n",
    "We build simple MLP models that predict one binary label. This model can be used to assess the performance of the baseline model f(X) -> Y.\n",
    "\n",
    "## Attribute encoding - SingleHead with frozen feature extractor\n",
    "In addition, this model can take as an extra input a frozen feature extractor. It then adds a linear layer to this feature extractor to perform the binary classification. Only the weights of this layer are tuned during training. This architecture is used to assess the level of attribute encoding of another model.\n",
    "\n",
    "## Multi-task - MultiHead with Gradient Reversal\n",
    "The architecture of the multi-task model first comprises a series of layers that represent a \"feature extractor\". On top of the feature extractor, we add two heads: \n",
    "- one head for the label, which is predicted from a single linear layer,\n",
    "- one head for an auxiliary task (here the prediction of the color of the square). This head includes at least a non-linear layer to allow a gradient scaling operation.\n",
    "\n",
    "The weight of the auxiliary loss in the total loss is a hyper-parameter in the method (weight of label loss is fixed to 1.0). A gradient reversal head is added to the auxiliary task. Gradient scaling can then be performed (positive or negative, extra hyper-parameter). Setting both hyper-parameters to 0 corresponds to the baseline single head model.\n",
    "\n",
    "\n",
    "A config (`cfg`) dictionary specifies the parameters of the MLP architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Model utils\n",
    "\n",
    "\n",
    "K = tf.keras.backend\n",
    "\n",
    "class GradientReversal(tf.keras.layers.Layer):\n",
    "    @tf.custom_gradient\n",
    "    def grad_reverse(self, x):\n",
    "        y = tf.identity(x)\n",
    "        def custom_grad(dy):\n",
    "          return self.hp_lambda * dy\n",
    "        return y, custom_grad\n",
    "\n",
    "    def __init__(self, hp_lambda, **kwargs):\n",
    "        super(GradientReversal, self).__init__(**kwargs)\n",
    "        self.hp_lambda = K.variable(hp_lambda, dtype='float', name='hp_lambda')\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return self.grad_reverse(x)\n",
    "\n",
    "    def set_hp_lambda(self,hp_lambda):\n",
    "        K.set_value(self.hp_lambda, hp_lambda)\n",
    "    \n",
    "    def increment_hp_lambda_by(self,increment):\n",
    "        new_value = float(K.get_value(self.hp_lambda)) +  increment\n",
    "        K.set_value(self.hp_lambda, new_value)\n",
    "\n",
    "    def get_hp_lambda(self):\n",
    "        return float(K.get_value(self.hp_lambda))\n",
    "\n",
    "\n",
    "class BaselineArch():\n",
    "  \"\"\"Superclass for multihead training.\"\"\"\n",
    "\n",
    "  def __init__(self, main=\"y\", aux=None, dtype=tf.float32, pos=None):\n",
    "    \"\"\"Initializer.\n",
    "\n",
    "    Args:\n",
    "      main: name of variable for the main task\n",
    "      aux: nema of the variable for the auxiliary task\n",
    "      dtype: desired dtype (e.g. tf.float32).\n",
    "      pos: ConfigDict that specifies the index of x, y, c, w, u in data tuple.\n",
    "        Default: data is of the form (x, y, c, w, u).\n",
    "    \"\"\"\n",
    "    self.model = None\n",
    "    self.inputs = \"x\"\n",
    "    self.main = main\n",
    "    self.aux = aux\n",
    "    self.dtype = dtype\n",
    "    if pos is None:\n",
    "      pos = mlc.ConfigDict()\n",
    "      pos.x, pos.y, pos.a = 0, 1, 2\n",
    "    self.pos = pos\n",
    "\n",
    "  def get_input(self, *batch):\n",
    "    \"\"\"Fetch model input from the batch.\"\"\"\n",
    "    # first input\n",
    "    stack = tf.cast(batch[self.pos[self.inputs[0]]], self.dtype)\n",
    "    # fetch remaining ones\n",
    "    for c in self.inputs[1:]:\n",
    "      stack = tf.concat([stack, tf.cast(batch[self.pos[c]], self.dtype)],\n",
    "                        axis=1)\n",
    "    return stack\n",
    "\n",
    "  def get_output(self, *batch):\n",
    "    \"\"\"Fetch outputs from the batch.\"\"\"\n",
    "    if self.aux:\n",
    "      return (tf.cast(batch[self.pos[self.main]],self.dtype),\n",
    "              tf.cast(batch[self.pos[self.aux]], self.dtype))\n",
    "    else:\n",
    "      return (tf.cast(batch[self.pos[self.main]],self.dtype))\n",
    "\n",
    "  def split_batch(self, *batch):\n",
    "    \"\"\"Split batch into input and output.\"\"\"\n",
    "    return self.get_input(*batch), self.get_output(*batch)\n",
    "\n",
    "  def fit(self, data: tf.data.Dataset, **kwargs):\n",
    "    \"\"\"Fit model on data.\"\"\"\n",
    "    ds = data.map(self.split_batch)\n",
    "    self.model.fit(ds, **kwargs)\n",
    "\n",
    "  def predict(self, model_input, **kwargs):\n",
    "    \"\"\"Predict target Y given the model input. See also: predict_mult().\"\"\"\n",
    "    y_pred = self.model.predict(model_input, **kwargs)\n",
    "    return y_pred\n",
    "\n",
    "  def predict_mult(self, data: tf.data.Dataset, num_batches: int, **kwargs):\n",
    "    \"\"\"Predict from the TF dataset directly. See also: predict().\"\"\"\n",
    "    pos = self.pos\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "    a_true_list = []\n",
    "    a_pred_list = []\n",
    "\n",
    "    for batch in data.take(num_batches):\n",
    "      model_input, (y_true, a_true) = self.split_batch(*batch)\n",
    "      y_pred, a_pred = self.predict(model_input, **kwargs)\n",
    "              \n",
    "      # Convert Tensors to NumPy and store in lists\n",
    "      y_true_list.append(y_true.numpy())\n",
    "      a_true_list.append(a_true.numpy())\n",
    "      y_pred_list.append(y_pred.numpy())\n",
    "      a_pred_list.append(a_pred.numpy())\n",
    "\n",
    "    # Concatenate lists of arrays\n",
    "    y_true_all = np.concatenate(y_true_list, axis=0)\n",
    "    a_true_all = np.concatenate(a_true_list, axis=0)\n",
    "    y_pred_all = np.concatenate(y_pred_list, axis=0)\n",
    "    a_pred_all = np.concatenate(a_pred_list, axis=0)\n",
    "\n",
    "    return (y_true_all, a_true_all), (y_pred_all, a_pred_all)\n",
    "\n",
    "  def score(self, data: tf.data.Dataset, num_batches: int, \n",
    "            metric: tf.keras.metrics.Metric , **kwargs):\n",
    "    \"\"\"Evaluate model on data.\n",
    "\n",
    "    Args:\n",
    "      data: TF dataset.\n",
    "      num_batches: number of batches fetched from the dataset.\n",
    "      metric: which metric to evaluate (schrouf not be instantiated).\n",
    "      **kwargs: arguments passed to predict() method.\n",
    "\n",
    "    Returns:\n",
    "      score: evaluation score.\n",
    "    \"\"\"\n",
    "    y_true, y_pred = self.predict_mult(data, num_batches, **kwargs)\n",
    "    return metric()(y_true, y_pred).numpy()\n",
    "\n",
    "\n",
    "class MultiHead(BaselineArch):\n",
    "  \"\"\"Multihead training.\"\"\"\n",
    "\n",
    "  def __init__(self, cfg, main, aux, dtype=tf.float32, pos=None): \n",
    "    \"\"\"Initializer.\n",
    "\n",
    "    Args:\n",
    "      cfg: A config that describes the MLP architecture.\n",
    "      main: variable for the main task\n",
    "      aux: variable for the auxialiary task\n",
    "      dtype: desired dtype (e.g. tf.float32) for casting data.\n",
    "    \"\"\"\n",
    "    super(MultiHead, self).__init__(main, aux, dtype, pos)\n",
    "    self.main = \"y\"\n",
    "    self.aux = \"a\"\n",
    "    self.cfg = cfg\n",
    "    # build architecture\n",
    "    self.model, self.feat_extract = self.build()\n",
    "\n",
    "  def build(self):\n",
    "    \"\"\"Build model.\"\"\"\n",
    "    cfg = self.cfg\n",
    "    input_shape = cfg.model.x_dim\n",
    "\n",
    "    # set config params to defaults if missing\n",
    "    use_bias = cfg.model.get(\"use_bias\", True)\n",
    "    activation = cfg.model.get(\"activation\", \"relu\")\n",
    "    output_activation = cfg.model.get(\"output_activation\", \"sigmoid\")\n",
    "\n",
    "    model_input = tf.keras.Input(shape=input_shape)\n",
    "    flatten_input = tf.keras.layers.Flatten()(model_input)\n",
    "    if cfg.model.depth:\n",
    "      x = tf.keras.layers.Dense(cfg.model.width, use_bias=use_bias,\n",
    "                                activation=activation,\n",
    "                                kernel_regularizer=cfg.model.regularizer)(flatten_input)\n",
    "      for _ in range(cfg.model.depth - 1):\n",
    "        x = tf.keras.layers.Dense(cfg.model.width, use_bias=use_bias,\n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=cfg.model.regularizer)(x)\n",
    "    else:\n",
    "      x = flatten_input\n",
    "    feature_extractor = tf.keras.models.Model(inputs=flatten_input,\n",
    "                                              outputs=x)\n",
    "    # output layer - a single linear layer\n",
    "    y = tf.keras.layers.Dense(cfg.model.output_dim,\n",
    "                              use_bias=cfg.model.use_bias,\n",
    "                              name=\"output\",\n",
    "                              activation=output_activation,\n",
    "                              kernel_regularizer=cfg.model.regularizer)(x)\n",
    "    # attribute layer - an extra dense layer is required for gradients to flow back\n",
    "    attr_activation = cfg.model.get(\"attr_activation\", \"sigmoid\")\n",
    "    input_branch_a = GradientReversal(hp_lambda=cfg.model.attr_grad_updates)(x)\n",
    "    a_branch = tf.keras.layers.Dense(cfg.model.branch_dim,\n",
    "                    use_bias=cfg.model.use_bias,\n",
    "                    name=\"attr_branch\",\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=cfg.model.regularizer)(input_branch_a)\n",
    "    a = tf.keras.layers.Dense(cfg.model.attr_dim,\n",
    "                        use_bias=cfg.model.use_bias,\n",
    "                        name=\"attribute\",\n",
    "                        activation=attr_activation,\n",
    "                        kernel_regularizer=cfg.model.regularizer)(a_branch)\n",
    "    \n",
    "\n",
    "\n",
    "    # choose optimizer\n",
    "    if cfg.opt.name == \"sgd\":\n",
    "      opt = tf.keras.optimizers.SGD(learning_rate=cfg.opt.learning_rate,\n",
    "                                    momentum=cfg.opt.get(\"momentum\", 0.9))\n",
    "    elif cfg.opt.name == \"adam\":\n",
    "      opt = tf.keras.optimizers.Adam(learning_rate=cfg.opt.learning_rate)\n",
    "    else:\n",
    "      raise ValueError(\"Unrecognized optimizer type.\"\n",
    "                       \"Please select either 'sgd' or 'adam'.\")\n",
    "\n",
    "    # define losses\n",
    "    losses = {\n",
    "        \"output\": cfg.model.get(\"output_loss\", \"binary_crossentropy\"),\n",
    "        \"attribute\": cfg.model.get(\"attribute_loss\", \"binary_crossentropy\")\n",
    "    }\n",
    "    loss_weights = {\"output\": 1.0,\n",
    "                    \"attribute\": cfg.get(\"attr_loss_weight\", 1.0)}\n",
    "    metrics = {\"output\": tf.keras.metrics.AUC(),\n",
    "               \"attribute\": tf.keras.metrics.AUC()}\n",
    "\n",
    "    # build model\n",
    "    model = tf.keras.models.Model(inputs=model_input, outputs=[y,a])\n",
    "    model.build(input_shape)\n",
    "    # model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    #               metrics=tf.keras.metrics.BinaryAccuracy())\n",
    "    model.compile(optimizer=opt, loss=losses, loss_weights=loss_weights,\n",
    "                  metrics=metrics)\n",
    "    return model, feature_extractor\n",
    "\n",
    "  def predict_mult(self, data: tf.data.Dataset, num_batches: int, **kwargs):\n",
    "    \"\"\"Predict from the TF dataset directly. See also: predict().\"\"\"\n",
    "    # infer dimensions\n",
    "    pos = self.pos\n",
    "    batch = next(iter(data))\n",
    "    y_dim = batch[pos.y].shape[1]\n",
    "    a_dim = batch[pos.a].shape[1]\n",
    "\n",
    "    # begin\n",
    "    data_iter = iter(data)\n",
    "    a_true_all = np.array([]).reshape((0, a_dim))\n",
    "    a_pred_all = np.array([]).reshape((0, a_dim))\n",
    "    y_true_all = np.array([]).reshape((0, y_dim))\n",
    "    y_pred_all = np.array([]).reshape((0, y_dim))\n",
    "\n",
    "    for _ in range(num_batches):\n",
    "      batch = next(data_iter)\n",
    "      x, y_true, a_true = batch[pos.x], batch[pos.y], batch[pos.a]\n",
    "      a_true = tf.reshape(a_true, [-1, a_true.shape[-2]])\n",
    "      y_true = tf.reshape(y_true, [-1, y_true.shape[-2]])\n",
    "      y_pred, a_pred = self.predict(x, **kwargs)\n",
    "      a_true_all = np.append(a_true_all, a_true, axis=0)\n",
    "      a_pred_all = np.append(a_pred_all, a_pred, axis=0)\n",
    "      y_true_all = np.append(y_true_all, y_true, axis=0)\n",
    "      y_pred_all = np.append(y_pred_all, y_pred, axis=0)\n",
    "\n",
    "    return (y_true_all, a_true_all), (y_pred_all, a_pred_all)\n",
    "\n",
    "  def score(self, data: tf.data.Dataset, num_batches: int, \n",
    "            metric: tf.keras.metrics.Metric, **kwargs):\n",
    "    \"\"\"Evaluate model on data.\n",
    "\n",
    "    Args:\n",
    "      data: TF dataset.\n",
    "      num_batches: number of batches fetched from the dataset.\n",
    "      metric: which metric to evaluate (should not be instantiated).\n",
    "      **kwargs: arguments passed to predict() method.\n",
    "\n",
    "    Returns:\n",
    "      score: evaluation score.\n",
    "    \"\"\"\n",
    "    out_true, out_pred = self.predict_mult(data, num_batches, **kwargs)\n",
    "    scores = []\n",
    "    for head in range(len(out_true)):\n",
    "      score = metric()(out_true[head], out_pred[head])\n",
    "      scores.append(score.numpy())\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Can be used as a single task model fully trained or from a pre-trained\n",
    "# feature extractor\n",
    "\n",
    "class SingleHead(BaselineArch):\n",
    "  \"\"\"Singlehead training.\"\"\"\n",
    "\n",
    "  def __init__(self, cfg, main, dtype=tf.float32, pos=None, feat_extract=None): \n",
    "    \"\"\"Initializer.\n",
    "\n",
    "    Args:\n",
    "      cfg: A config that describes the MLP architecture.\n",
    "      main: variable for the main task\n",
    "      aux: variable for the auxialiary task\n",
    "      dtype: desired dtype (e.g. tf.float32) for casting data.\n",
    "    \"\"\"\n",
    "    super(SingleHead, self).__init__(main, None, dtype, pos)\n",
    "    self.main = \"a\"\n",
    "    self.cfg = cfg\n",
    "    # build architecture\n",
    "    self.model = self.build(feat_extract)\n",
    "\n",
    "  def build(self, feat_extract=None):\n",
    "    \"\"\"Build model.\"\"\"\n",
    "    cfg = self.cfg\n",
    "    input_shape = cfg.model.x_dim\n",
    "\n",
    "    # set config params to defaults if missing\n",
    "    use_bias = cfg.model.get(\"use_bias\", True)\n",
    "    activation = cfg.model.get(\"activation\", \"relu\")\n",
    "    output_activation = cfg.model.get(\"output_activation\", \"sigmoid\")\n",
    "\n",
    "    model_input = tf.keras.Input(shape=input_shape)\n",
    "    flatten_input = tf.keras.layers.Flatten()(model_input)\n",
    "    if not feat_extract:\n",
    "      if cfg.model.depth:\n",
    "        x = tf.keras.layers.Dense(cfg.model.width, use_bias=use_bias,\n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=cfg.model.regularizer)(flatten_input)\n",
    "        for _ in range(cfg.model.depth - 1):\n",
    "          x = tf.keras.layers.Dense(cfg.model.width, use_bias=use_bias,\n",
    "                                    activation=activation,\n",
    "                                    kernel_regularizer=cfg.model.regularizer)(x)\n",
    "      else:\n",
    "        x = flatten_input\n",
    "      feature_extractor = x\n",
    "    else:\n",
    "      feat_extract.trainable = False\n",
    "      feature_extractor = feat_extract(flatten_input, training=False)\n",
    "  \n",
    "    # output layer\n",
    "    y = tf.keras.layers.Dense(cfg.model.output_dim,\n",
    "                              use_bias=cfg.model.use_bias,\n",
    "                              name=\"output\",\n",
    "                              activation=output_activation,\n",
    "                              kernel_regularizer=cfg.model.regularizer)(feature_extractor)  \n",
    "\n",
    "    # choose optimizer\n",
    "    if cfg.opt.name == \"sgd\":\n",
    "      opt = tf.keras.optimizers.SGD(learning_rate=cfg.opt.learning_rate,\n",
    "                                    momentum=cfg.opt.get(\"momentum\", 0.9))\n",
    "    elif cfg.opt.name == \"adam\":\n",
    "      opt = tf.keras.optimizers.Adam(learning_rate=cfg.opt.learning_rate)\n",
    "    else:\n",
    "      raise ValueError(\"Unrecognized optimizer type.\"\n",
    "                       \"Please select either 'sgd' or 'adam'.\")\n",
    "\n",
    "    # build model\n",
    "    model = tf.keras.models.Model(inputs=model_input, outputs=y)\n",
    "    model.build(input_shape)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=cfg.model.get(\"output_loss\", \"binary_crossentropy\"),\n",
    "                  metrics=tf.keras.metrics.AUC())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Model utils\n",
    "\n",
    "class GradientReversal(tf.keras.layers.Layer):\n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def grad_reverse(self, x):\n",
    "        y = tf.identity(x)\n",
    "        def custom_grad(dy):\n",
    "          return self.hp_lambda * dy\n",
    "        return y, custom_grad\n",
    "\n",
    "    def __init__(self, hp_lambda, **kwargs):\n",
    "        super(GradientReversal, self).__init__(**kwargs)\n",
    "        self.hp_lambda = K.variable(hp_lambda, dtype='float', name='hp_lambda')\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return self.grad_reverse(x)\n",
    "\n",
    "    def set_hp_lambda(self,hp_lambda):\n",
    "        K.set_value(self.hp_lambda, hp_lambda)\n",
    "    \n",
    "    def increment_hp_lambda_by(self,increment):\n",
    "        new_value = float(K.get_value(self.hp_lambda)) +  increment\n",
    "        K.set_value(self.hp_lambda, new_value)\n",
    "\n",
    "    def get_hp_lambda(self):\n",
    "        return float(K.get_value(self.hp_lambda))\n",
    "\n",
    "\n",
    "class BaselineArch():\n",
    "  \"\"\"Superclass for multihead training.\"\"\"\n",
    "\n",
    "  def __init__(self, main=\"y\", aux=None, dtype=tf.float32, pos=None):\n",
    "    \"\"\"Initializer.\n",
    "\n",
    "    Args:\n",
    "      main: name of variable for the main task\n",
    "      aux: nema of the variable for the auxiliary task\n",
    "      dtype: desired dtype (e.g. tf.float32).\n",
    "      pos: ConfigDict that specifies the index of x, y, c, w, u in data tuple.\n",
    "        Default: data is of the form (x, y, c, w, u).\n",
    "    \"\"\"\n",
    "    self.model = None\n",
    "    self.inputs = \"x\"\n",
    "    self.main = main\n",
    "    self.aux = aux\n",
    "    self.dtype = dtype\n",
    "    if pos is None:\n",
    "      pos = mlc.ConfigDict()\n",
    "      pos.x, pos.y, pos.a = 0, 1, 2\n",
    "    self.pos = pos\n",
    "\n",
    "  def get_input(self, *batch):\n",
    "    \"\"\"Fetch model input from the batch.\"\"\"\n",
    "    # first input\n",
    "    stack = tf.cast(batch[self.pos[self.inputs[0]]], self.dtype)\n",
    "    # fetch remaining ones\n",
    "    for c in self.inputs[1:]:\n",
    "      stack = tf.concat([stack, tf.cast(batch[self.pos[c]], self.dtype)],\n",
    "                        axis=1)\n",
    "    return stack\n",
    "\n",
    "  def get_output(self, *batch):\n",
    "    \"\"\"Fetch outputs from the batch.\"\"\"\n",
    "    if self.aux:\n",
    "      return (tf.cast(batch[self.pos[self.main]],self.dtype),\n",
    "              tf.cast(batch[self.pos[self.aux]], self.dtype))\n",
    "    else:\n",
    "      return (tf.cast(batch[self.pos[self.main]],self.dtype))\n",
    "\n",
    "  def split_batch(self, *batch):\n",
    "    \"\"\"Split batch into input and output.\"\"\"\n",
    "    return self.get_input(*batch), self.get_output(*batch)\n",
    "\n",
    "  def fit(self, data: tf.data.Dataset, **kwargs):\n",
    "    \"\"\"Fit model on data.\"\"\"\n",
    "    ds = data.map(self.split_batch)\n",
    "    self.model.fit(ds, **kwargs)\n",
    "\n",
    "  def predict(self, model_input, **kwargs):\n",
    "    \"\"\"Predict target Y given the model input. See also: predict_mult().\"\"\"\n",
    "    y_pred = self.model.predict(model_input, **kwargs)\n",
    "    return y_pred\n",
    "\n",
    "  def predict_mult(self, data: tf.data.Dataset, num_batches: int, **kwargs):\n",
    "    \"\"\"Predict target Y from the TF dataset directly. See also: predict().\"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    ds_iter = iter(data)\n",
    "    for _ in range(num_batches):\n",
    "      batch = next(ds_iter)\n",
    "      model_input, y = self.split_batch(*batch)\n",
    "      y_true.extend(y)\n",
    "      y_pred.extend(self.predict(model_input, **kwargs))\n",
    "    return np.array(y_true), np.array(y_pred)\n",
    "\n",
    "  def score(self, data: tf.data.Dataset, num_batches: int, \n",
    "            metric: tf.keras.metrics.Metric , **kwargs):\n",
    "    \"\"\"Evaluate model on data.\n",
    "\n",
    "    Args:\n",
    "      data: TF dataset.\n",
    "      num_batches: number of batches fetched from the dataset.\n",
    "      metric: which metric to evaluate (schrouf not be instantiated).\n",
    "      **kwargs: arguments passed to predict() method.\n",
    "\n",
    "    Returns:\n",
    "      score: evaluation score.\n",
    "    \"\"\"\n",
    "    y_true, y_pred = self.predict_mult(data, num_batches, **kwargs)\n",
    "    return metric()(y_true, y_pred).numpy()\n",
    "\n",
    "\n",
    "class MultiHead(BaselineArch):\n",
    "  \"\"\"Multihead training.\"\"\"\n",
    "\n",
    "  def __init__(self, cfg, main, aux, dtype=tf.float32, pos=None): \n",
    "    \"\"\"Initializer.\n",
    "\n",
    "    Args:\n",
    "      cfg: A config that describes the MLP architecture.\n",
    "      main: variable for the main task\n",
    "      aux: variable for the auxialiary task\n",
    "      dtype: desired dtype (e.g. tf.float32) for casting data.\n",
    "    \"\"\"\n",
    "    super(MultiHead, self).__init__(main, aux, dtype, pos)\n",
    "    self.main = \"y\"\n",
    "    self.aux = \"a\"\n",
    "    self.cfg = cfg\n",
    "    # build architecture\n",
    "    self.model, self.feat_extract = self.build()\n",
    "\n",
    "  def build(self):\n",
    "    \"\"\"Build model.\"\"\"\n",
    "    cfg = self.cfg\n",
    "    input_shape = cfg.model.x_dim\n",
    "\n",
    "    # set config params to defaults if missing\n",
    "    use_bias = cfg.model.get(\"use_bias\", True)\n",
    "    activation = cfg.model.get(\"activation\", \"relu\")\n",
    "    output_activation = cfg.model.get(\"output_activation\", \"sigmoid\")\n",
    "\n",
    "    model_input = tf.keras.Input(shape=input_shape)\n",
    "    flatten_input = tf.keras.layers.Flatten()(model_input)\n",
    "    if cfg.model.depth:\n",
    "      x = tf.keras.layers.Dense(cfg.model.width, use_bias=use_bias,\n",
    "                                activation=activation,\n",
    "                                kernel_regularizer=cfg.model.regularizer)(flatten_input)\n",
    "      for _ in range(cfg.model.depth - 1):\n",
    "        x = tf.keras.layers.Dense(cfg.model.width, use_bias=use_bias,\n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=cfg.model.regularizer)(x)\n",
    "    else:\n",
    "      x = flatten_input\n",
    "    feature_extractor = tf.keras.models.Model(inputs=flatten_input,\n",
    "                                              outputs=x)\n",
    "    # output layer - a single linear layer\n",
    "    y = tf.keras.layers.Dense(cfg.model.output_dim,\n",
    "                              use_bias=cfg.model.use_bias,\n",
    "                              name=\"output\",\n",
    "                              activation=output_activation,\n",
    "                              kernel_regularizer=cfg.model.regularizer)(x)\n",
    "    # attribute layer - an extra dense layer is required for gradients to flow back\n",
    "    attr_activation = cfg.model.get(\"attr_activation\", \"sigmoid\")\n",
    "    input_branch_a = GradientReversal(hp_lambda=cfg.model.attr_grad_updates)(x)\n",
    "    a_branch = tf.keras.layers.Dense(cfg.model.branch_dim,\n",
    "                    use_bias=cfg.model.use_bias,\n",
    "                    name=\"attr_branch\",\n",
    "                    activation=activation,\n",
    "                    kernel_regularizer=cfg.model.regularizer)(input_branch_a)\n",
    "    a = tf.keras.layers.Dense(cfg.model.attr_dim,\n",
    "                        use_bias=cfg.model.use_bias,\n",
    "                        name=\"attribute\",\n",
    "                        activation=attr_activation,\n",
    "                        kernel_regularizer=cfg.model.regularizer)(a_branch)\n",
    "    \n",
    "\n",
    "\n",
    "    # choose optimizer\n",
    "    if cfg.opt.name == \"sgd\":\n",
    "      opt = tf.keras.optimizers.SGD(learning_rate=cfg.opt.learning_rate,\n",
    "                                    momentum=cfg.opt.get(\"momentum\", 0.9))\n",
    "    elif cfg.opt.name == \"adam\":\n",
    "      opt = tf.keras.optimizers.Adam(learning_rate=cfg.opt.learning_rate)\n",
    "    else:\n",
    "      raise ValueError(\"Unrecognized optimizer type.\"\n",
    "                       \"Please select either 'sgd' or 'adam'.\")\n",
    "\n",
    "    # define losses\n",
    "    losses = {\n",
    "        \"output\": cfg.model.get(\"output_loss\", \"binary_crossentropy\"),\n",
    "        \"attribute\": cfg.model.get(\"attribute_loss\", \"binary_crossentropy\")\n",
    "    }\n",
    "    loss_weights = {\"output\": 1.0,\n",
    "                    \"attribute\": cfg.get(\"attr_loss_weight\", 1.0)}\n",
    "    metrics = {\"output\": tf.keras.metrics.AUC(),\n",
    "               \"attribute\": tf.keras.metrics.AUC()}\n",
    "\n",
    "    # build model\n",
    "    model = tf.keras.models.Model(inputs=model_input, outputs=[y,a])\n",
    "    model.build(input_shape)\n",
    "    # model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    #               metrics=tf.keras.metrics.BinaryAccuracy())\n",
    "    model.compile(optimizer=opt, loss=losses, loss_weights=loss_weights,\n",
    "                  metrics=metrics)\n",
    "    return model, feature_extractor\n",
    "\n",
    "  def predict_mult(self, data: tf.data.Dataset, num_batches: int, **kwargs):\n",
    "    \"\"\"Predict from the TF dataset directly. See also: predict().\"\"\"\n",
    "    # infer dimensions\n",
    "    pos = self.pos\n",
    "    batch = next(iter(data))\n",
    "    y_dim = batch[pos.y].shape[1]\n",
    "    a_dim = batch[pos.a].shape[1]\n",
    "\n",
    "    # begin\n",
    "    data_iter = iter(data)\n",
    "    a_true_all = np.array([]).reshape((0, a_dim))\n",
    "    a_pred_all = np.array([]).reshape((0, a_dim))\n",
    "    y_true_all = np.array([]).reshape((0, y_dim))\n",
    "    y_pred_all = np.array([]).reshape((0, y_dim))\n",
    "\n",
    "    for _ in range(num_batches):\n",
    "      batch = next(data_iter)\n",
    "      x, y_true, a_true = batch[pos.x], batch[pos.y], batch[pos.a]\n",
    "      y_pred, a_pred = self.predict(x, **kwargs)\n",
    "      a_true_all = np.append(a_true_all, a_true, axis=0)\n",
    "      a_pred_all = np.append(a_pred_all, a_pred, axis=0)\n",
    "      y_true_all = np.append(y_true_all, y_true, axis=0)\n",
    "      y_pred_all = np.append(y_pred_all, y_pred, axis=0)\n",
    "\n",
    "    return (y_true_all, a_true_all), (y_pred_all, a_pred_all)\n",
    "\n",
    "  def score(self, data: tf.data.Dataset, num_batches: int, \n",
    "            metric: tf.keras.metrics.Metric, **kwargs):\n",
    "    \"\"\"Evaluate model on data.\n",
    "\n",
    "    Args:\n",
    "      data: TF dataset.\n",
    "      num_batches: number of batches fetched from the dataset.\n",
    "      metric: which metric to evaluate (should not be instantiated).\n",
    "      **kwargs: arguments passed to predict() method.\n",
    "\n",
    "    Returns:\n",
    "      score: evaluation score.\n",
    "    \"\"\"\n",
    "    out_true, out_pred = self.predict_mult(data, num_batches, **kwargs)\n",
    "    scores = []\n",
    "    for head in range(len(out_true)):\n",
    "      score = metric()(out_true[head], out_pred[head])\n",
    "      scores.append(score.numpy())\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Can be used as a single task model fully trained or from a pre-trained\n",
    "# feature extractor\n",
    "\n",
    "class SingleHead(BaselineArch):\n",
    "  \"\"\"Singlehead training.\"\"\"\n",
    "\n",
    "  def __init__(self, cfg, main, dtype=tf.float32, pos=None, feat_extract=None): \n",
    "    \"\"\"Initializer.\n",
    "\n",
    "    Args:\n",
    "      cfg: A config that describes the MLP architecture.\n",
    "      main: variable for the main task\n",
    "      aux: variable for the auxialiary task\n",
    "      dtype: desired dtype (e.g. tf.float32) for casting data.\n",
    "    \"\"\"\n",
    "    super(SingleHead, self).__init__(main, None, dtype, pos)\n",
    "    self.main = \"a\"\n",
    "    self.cfg = cfg\n",
    "    # build architecture\n",
    "    self.model = self.build(feat_extract)\n",
    "\n",
    "  def build(self, feat_extract=None):\n",
    "    \"\"\"Build model.\"\"\"\n",
    "    cfg = self.cfg\n",
    "    input_shape = cfg.model.x_dim\n",
    "\n",
    "    # set config params to defaults if missing\n",
    "    use_bias = cfg.model.get(\"use_bias\", True)\n",
    "    activation = cfg.model.get(\"activation\", \"relu\")\n",
    "    output_activation = cfg.model.get(\"output_activation\", \"sigmoid\")\n",
    "\n",
    "    model_input = tf.keras.Input(shape=input_shape)\n",
    "    flatten_input = tf.keras.layers.Flatten()(model_input)\n",
    "    if not feat_extract:\n",
    "      if cfg.model.depth:\n",
    "        x = tf.keras.layers.Dense(cfg.model.width, use_bias=use_bias,\n",
    "                                  activation=activation,\n",
    "                                  kernel_regularizer=cfg.model.regularizer)(flatten_input)\n",
    "        for _ in range(cfg.model.depth - 1):\n",
    "          x = tf.keras.layers.Dense(cfg.model.width, use_bias=use_bias,\n",
    "                                    activation=activation,\n",
    "                                    kernel_regularizer=cfg.model.regularizer)(x)\n",
    "      else:\n",
    "        x = flatten_input\n",
    "      feature_extractor = x\n",
    "    else:\n",
    "      feat_extract.trainable = False\n",
    "      feature_extractor = feat_extract(flatten_input, training=False)\n",
    "  \n",
    "    # output layer\n",
    "    y = tf.keras.layers.Dense(cfg.model.output_dim,\n",
    "                              use_bias=cfg.model.use_bias,\n",
    "                              name=\"output\",\n",
    "                              activation=output_activation,\n",
    "                              kernel_regularizer=cfg.model.regularizer)(feature_extractor)  \n",
    "\n",
    "    # choose optimizer\n",
    "    if cfg.opt.name == \"sgd\":\n",
    "      opt = tf.keras.optimizers.SGD(learning_rate=cfg.opt.learning_rate,\n",
    "                                    momentum=cfg.opt.get(\"momentum\", 0.9))\n",
    "    elif cfg.opt.name == \"adam\":\n",
    "      opt = tf.keras.optimizers.Adam(learning_rate=cfg.opt.learning_rate)\n",
    "    else:\n",
    "      raise ValueError(\"Unrecognized optimizer type.\"\n",
    "                       \"Please select either 'sgd' or 'adam'.\")\n",
    "\n",
    "    # build model\n",
    "    model = tf.keras.models.Model(inputs=model_input, outputs=y)\n",
    "    model.build(input_shape)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=cfg.model.get(\"output_loss\", \"binary_crossentropy\"),\n",
    "                  metrics=tf.keras.metrics.AUC())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline task model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[1;32m     12\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m\n\u001b[0;32m---> 13\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43menc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m baseline\u001b[38;5;241m.\u001b[39mappend(sc)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline model: \u001b[39m\u001b[38;5;132;01m%1.2f\u001b[39;00m\u001b[38;5;124m +- \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39mmean(baseline),np\u001b[38;5;241m.\u001b[39mstd(baseline)))\n",
      "Cell \u001b[0;32mIn[21], line 126\u001b[0m, in \u001b[0;36mBaselineArch.score\u001b[0;34m(self, data, num_batches, metric, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset, num_batches: \u001b[38;5;28mint\u001b[39m, \n\u001b[1;32m    114\u001b[0m           metric: tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mMetric , \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m   \u001b[38;5;124;03m\"\"\"Evaluate model on data.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    score: evaluation score.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m   y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_mult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m metric()(y_true, y_pred)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[21], line 96\u001b[0m, in \u001b[0;36mBaselineArch.predict_mult\u001b[0;34m(self, data, num_batches, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m a_pred_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mtake(num_batches):\n\u001b[0;32m---> 96\u001b[0m     model_input, (y_true, a_true) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_batch(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     97\u001b[0m     y_pred, a_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(model_input, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Convert Tensors to NumPy and store in lists\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "metric = tf.keras.metrics.AUC\n",
    "\n",
    "\n",
    "baseline = []\n",
    "seed= 0 \n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "enc = SingleHead(cfg, main=\"y\", dtype=tf.float32, feat_extract=None)\n",
    "kwargs = {'epochs': 10, 'steps_per_epoch':21, 'verbose': False}\n",
    "enc.fit(dataset, **kwargs)\n",
    "kwargs = {'verbose': False}\n",
    "num_batches = 21\n",
    "sc = enc.score(dataset, num_batches, metric, **kwargs)\n",
    "baseline.append(sc)\n",
    "\n",
    "print(\"Baseline model: %1.2f +- %.2f\" % (np.mean(baseline),np.std(baseline)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mtake(num_batches):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Correctly unpack the returned values from self.split_batch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     model_input, output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39msplit_batch(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m      4\u001b[0m     y_true, a_true \u001b[38;5;241m=\u001b[39m output  \u001b[38;5;66;03m# assuming output is a tuple (y_true, a_true)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Continue with prediction and further processing...\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounds on attribute encoding (i.e. LEB and UEB in paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 210 batches). You may need to use the repeat() function when building your dataset.\n",
      "Upper bound: 0.66 +- 0.00\n"
     ]
    }
   ],
   "source": [
    "#@title Upper bound\n",
    "metric = tf.keras.metrics.AUC\n",
    "\n",
    "upper_bound = []\n",
    "seed=0\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "enc = SingleHead(cfg, main=\"a\", dtype=tf.float32, feat_extract=None)\n",
    "kwargs = {'epochs': 10, 'steps_per_epoch':21, 'verbose': False}\n",
    "enc.fit(dataset, **kwargs)\n",
    "kwargs = {'verbose': False}\n",
    "num_batches = 10\n",
    "sc = enc.score(dataset, num_batches, metric, **kwargs)\n",
    "upper_bound.append(sc)\n",
    "\n",
    "print(\"Upper bound: %1.2f +- %.2f\" % (np.mean(upper_bound),np.std(upper_bound)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Lower bound: 0.54 +- 0.00\n"
     ]
    }
   ],
   "source": [
    "# Assuming num_batches is defined and matches the total batches available in your dataset\n",
    "# Assuming your dataset is structured as (images, pneumothorax_labels, support_devices_labels)\n",
    "\n",
    "num_batches = 21  # Adjust based on your specific batch count if necessary\n",
    "batch_size = 32  # Your batch size\n",
    "n_train = 672  # Total number of images in your dataset\n",
    "a_dim = 1  # Assuming binary classification for either pneumothorax or support devices\n",
    "\n",
    "lower_bound = []\n",
    "#for seed in [0, 1, 2, 3, 4]:\n",
    "seed = 0\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "a_true_all = np.array([]).reshape((0, a_dim))\n",
    "a_pred_all = np.array([]).reshape((0, a_dim))\n",
    "\n",
    "for batch in dataset.take(num_batches):\n",
    "    # Unpack the batch according to your dataset structure\n",
    "    images, pneumothorax_labels, support_devices_labels = batch\n",
    "    \n",
    "    # Dynamically determine the actual size of the current batch for pneumothorax_labels\n",
    "    actual_batch_size = pneumothorax_labels.shape[0]\n",
    "    \n",
    "    # Extract the labels and ensure it's reshaped to 2D if necessary\n",
    "    a_true = pneumothorax_labels.numpy()\n",
    "    a_true = a_true.reshape(-1, a_true.shape[-2])\n",
    "    \n",
    "    # Simulate predictions: Adjust to use actual_batch_size\n",
    "    a_pred = np.random.choice([0, 1], size=(actual_batch_size, a_dim))\n",
    "    \n",
    "    # Append the true labels and predictions to their respective accumulators\n",
    "    a_true_all = np.append(a_true_all, a_true, axis=0)\n",
    "    a_pred_all = np.append(a_pred_all, a_pred, axis=0)\n",
    "# Calculate and store the AUC score for this iteration\n",
    "\n",
    "sc = tf.keras.metrics.AUC()(a_true_all, a_pred_all).numpy()\n",
    "lower_bound.append(sc)\n",
    "\n",
    "# Print the lower bound score\n",
    "print(\"Lower bound: %1.2f +- %.2f\" % (np.mean(lower_bound), np.std(lower_bound)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (32, 512, 512, 3)\n",
      "Pneumothorax labels shape: (32, 1, 1)\n",
      "Support devices labels shape: (32, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the shapes of each component in a single element (or batch) of the dataset\n",
    "for images, pneumothorax_labels, support_devices_labels in dataset.take(1):\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Pneumothorax labels shape:\", pneumothorax_labels.shape)\n",
    "    print(\"Support devices labels shape:\", support_devices_labels.shape)\n",
    "    break  # Only take the first element for inspection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Multi-Task model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Initialize from config\n",
    "cfg.model.attr_grad_updates = float(0.1) # if both are set to 0, this is a baseline model (i.e. the same as with no extra head)\n",
    "cfg.attr_loss_weight = float(0.75)\n",
    "clf = MultiHead(cfg, main=\"y\", aux=\"a\", dtype=tf.float32, pos=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 512, 512, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " flatten_15 (Flatten)           (None, 786432)       0           ['input_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 10)           7864330     ['flatten_15[0][0]']             \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 10)           110         ['dense_45[0][0]']               \n",
      "                                                                                                  \n",
      " dense_47 (Dense)               (None, 10)           110         ['dense_46[0][0]']               \n",
      "                                                                                                  \n",
      " gradient_reversal_5 (GradientR  (None, 10)          1           ['dense_47[0][0]']               \n",
      " eversal)                                                                                         \n",
      "                                                                                                  \n",
      " attr_branch (Dense)            (None, 2)            22          ['gradient_reversal_5[0][0]']    \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            11          ['dense_47[0][0]']               \n",
      "                                                                                                  \n",
      " attribute (Dense)              (None, 1)            3           ['attr_branch[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,864,587\n",
      "Trainable params: 7,864,587\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#@markdown Explore the architecture\n",
    "clf.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "2/2 [==============================] - 5s 2s/step - loss: 0.8788 - output_loss: 0.3619 - attribute_loss: 0.6892 - output_auc_26: 0.0000e+00 - attribute_auc_27: 0.4744\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 3s 2s/step - loss: 0.5131 - output_loss: 0.0000e+00 - attribute_loss: 0.6841 - output_auc_26: 0.0000e+00 - attribute_auc_27: 0.5390\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 3s 1s/step - loss: 13.2437 - output_loss: 12.7238 - attribute_loss: 0.6931 - output_auc_26: 0.5000 - attribute_auc_27: 0.5000\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 3s 1s/step - loss: 6.1024 - output_loss: 5.3789 - attribute_loss: 0.9646 - output_auc_26: 0.5000 - attribute_auc_27: 0.5195    \n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 3s 2s/step - loss: 16.0053 - output_loss: 4.5470 - attribute_loss: 15.2777 - output_auc_26: 0.5000 - attribute_auc_27: 0.4054   \n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 4s 2s/step - loss: 14.6865 - output_loss: 12.1014 - attribute_loss: 3.4467 - output_auc_26: 0.5000 - attribute_auc_27: 0.4334  \n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 4s 2s/step - loss: 0.5196 - output_loss: 0.0000e+00 - attribute_loss: 0.6927 - output_auc_26: 0.0000e+00 - attribute_auc_27: 0.5000\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 4s 2s/step - loss: 2.9261 - output_loss: 2.4058 - attribute_loss: 0.6937 - output_auc_26: 0.5000 - attribute_auc_27: 0.5000    \n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 5s 2s/step - loss: 27.5088 - output_loss: 26.9886 - attribute_loss: 0.6935 - output_auc_26: 0.2302 - attribute_auc_27: 0.5000\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 3s 2s/step - loss: 3.9963 - output_loss: 3.4764 - attribute_loss: 0.6931 - output_auc_26: 0.5000 - attribute_auc_27: 0.5000    \n"
     ]
    }
   ],
   "source": [
    "#@title Train the model\n",
    "kwargs = {'epochs': 10, 'steps_per_epoch':2, 'verbose': True}\n",
    "clf.fit(dataset, **kwargs)\n",
    "clf.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the decision threshold by maximizing the F1-score on validation data\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "def f1_curve(truth, prediction_scores, e=1e-6):\n",
    "  precision, recall, thresholds = precision_recall_curve(truth, prediction_scores)\n",
    "  f1 = 2*recall*precision/(recall+precision+e)\n",
    "  return thresholds, f1[:-1]\n",
    "\n",
    "def threshold_at_max_f1_score(truth, prediction_scores):\n",
    "  thresholds, f1 = f1_curve(truth, prediction_scores)\n",
    "  peak_idx = np.argmax(f1)\n",
    "  return thresholds[peak_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.5]\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'verbose': False}\n",
    "num_batches = 2\n",
    "metric = tf.keras.metrics.AUC\n",
    "scores_val = clf.score(dataset, num_batches, metric, **kwargs)\n",
    "print(scores_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/itu/easybuild/software/Anaconda3/2023.03-1/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {'verbose': False}\n",
    "yt, yp = clf.predict_mult(dataset, num_batches, **kwargs)\n",
    "threshold = threshold_at_max_f1_score(yt[0],yp[0])\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Fairness metrics\n",
    "\n",
    "# As per the work of Alabdulmohsin et al., 2021\n",
    "\n",
    "def fairness_metrics(y_pred, y_true, sens_attr):\n",
    "  eps = 1e-5\n",
    "  groups = np.unique(sens_attr).tolist()\n",
    "\n",
    "  max_error = 0\n",
    "  min_error = 1\n",
    "\n",
    "  max_mean_y = 0\n",
    "  min_mean_y = 1\n",
    "\n",
    "  max_mean_y0 = 0  # conditioned on y = 0\n",
    "  min_mean_y0 = 1\n",
    "\n",
    "  max_mean_y1 = 0\n",
    "  min_mean_y1 = 1\n",
    "\n",
    "  for group in groups:\n",
    "    yt = y_true[sens_attr == group].astype('int32')\n",
    "    ypt = (y_pred[sens_attr == group]).astype('int32')\n",
    "    err = -np.mean(yt * np.log(ypt+eps) + (1-yt)*np.log(1-ypt+eps))\n",
    "    mean_y = np.mean(y_pred[sens_attr == group])\n",
    "    neg = np.logical_and(sens_attr == group, y_true == 0)\n",
    "    pos = np.logical_and(sens_attr == group, y_true == 1)\n",
    "    mean_y0 = np.mean(y_pred[neg])\n",
    "    mean_y1 = np.mean(y_pred[pos])\n",
    "\n",
    "    if err > max_error:\n",
    "      max_error = err\n",
    "    if err < min_error:\n",
    "      min_error = err\n",
    "\n",
    "    if mean_y > max_mean_y:\n",
    "      max_mean_y = mean_y\n",
    "    if mean_y < min_mean_y:\n",
    "      min_mean_y = mean_y\n",
    "\n",
    "    if mean_y0 > max_mean_y0:\n",
    "      max_mean_y0 = mean_y0\n",
    "    if mean_y0 < min_mean_y0:\n",
    "      min_mean_y0 = mean_y0\n",
    "\n",
    "    if mean_y1 > max_mean_y1:\n",
    "      max_mean_y1 = mean_y1\n",
    "    if mean_y1 < min_mean_y1:\n",
    "      min_mean_y1 = mean_y1\n",
    "  \n",
    "  eo = 0.5*(max_mean_y0 - min_mean_y0 + max_mean_y1 - min_mean_y1)\n",
    "  dp = max_mean_y - min_mean_y\n",
    "  err_parity = max_error - min_error\n",
    "\n",
    "  return eo, dp, err_parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.5]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Evaluate the model on test data\n",
    "kwargs = {'verbose': False}\n",
    "num_batches = 20\n",
    "metric = tf.keras.metrics.AUC\n",
    "scores = clf.score(dataset, num_batches, metric, **kwargs)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt, yp = clf.predict_mult(dataset, num_batches, **kwargs)\n",
    "eo, dp, ep = fairness_metrics(yp[0]>=threshold,\n",
    "                              yt[0],yt[1].astype('int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.70343816, 0.5]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Evaluate model on counterfactual test data\n",
    "kwargs = {'verbose': False}\n",
    "num_batches = 20\n",
    "metric = tf.keras.metrics.AUC\n",
    "\n",
    "scores_flipped = clf.score(test_data_flipped, num_batches,metric, **kwargs)\n",
    "scores_flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20959115"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Evaluate attribute encoding based on a frozen feature extractor from the multi-task model\n",
    "enc = SingleHead(cfg, main=\"a\", dtype=tf.float32, feat_extract=clf.feat_extract)\n",
    "kwargs = {'epochs': 30, 'steps_per_epoch':20, 'verbose': False}\n",
    "enc.fit(train_data, **kwargs)\n",
    "kwargs = {'verbose': False}\n",
    "num_batches = 20\n",
    "metric = tf.keras.metrics.AUC\n",
    "sc = enc.score(test_data, num_batches, metric, **kwargs)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piecing the elements together: ShorT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ShorT method\n",
    "def shortcut_testing(cfg, train_data, test_data, counterfact_data, val_data,\n",
    "                     range_grads, seeds = [0,1,2,3,4], num_epochs_train = 30,\n",
    "                     num_batch_test = 5):\n",
    "  \"\"\"Shortcut testing.\n",
    "\n",
    "  Requires:\n",
    "  - cfg: a config dictionary of model specifications\n",
    "  - train_data: tf dataset of train data respecting the positions for x,y,a\n",
    "  - test_data: tf dataset of test data\n",
    "  - counterfact_data: tf dataset of counterfactual test images\n",
    "  - val_data: validation data to compute decision threshold\n",
    "  - range_grads: np array of gradient scalings\n",
    "  - seeds: list of seed numbers to fix, per gradient scaling\n",
    "  - num_epoch_train: int, number of epochs for training\n",
    "  - num_batch_test: number of batches to test the model on\n",
    "\n",
    "  Outputs:\n",
    "  scores_y: np array of size (# gradient scalings, # seeds) of test performance on Y\n",
    "  encoding_a: np array of model's attribute encoding\n",
    "  equ_odds: np array of equalized odds on test data\n",
    "  dem_par: similar, demographic parity\n",
    "  err_par: error parity\n",
    "  count_fair: counterfactual fairness (computed locally then averaged)\n",
    "  scores_c: global performance on Y on counterfactual data\n",
    "  models: tf.Keras.model instances\n",
    "  \"\"\" \n",
    "\n",
    "\n",
    "  scores_y = np.zeros((len(range_grads), len(seeds)))\n",
    "  scores_c = np.zeros((len(range_grads), len(seeds)))\n",
    "  encoding_a = np.zeros((len(range_grads), len(seeds)))\n",
    "  equ_odds = np.zeros((len(range_grads), len(seeds)))\n",
    "  dem_par = np.zeros((len(range_grads), len(seeds)))\n",
    "  err_par = np.zeros((len(range_grads), len(seeds)))\n",
    "  count_fair = np.zeros((len(range_grads), len(seeds)))\n",
    "  models = []\n",
    "  metric = tf.keras.metrics.AUC\n",
    "  loss_weight = cfg.attr_loss_weight\n",
    "  for g, grad_scaling in enumerate(range_grads):\n",
    "    cfg.model.attr_grad_updates = float(grad_scaling)\n",
    "\n",
    "    if grad_scaling == 0:  ## baseline model with no other head\n",
    "      cfg.attr_loss_weight = float(0.0)\n",
    "    else:\n",
    "      cfg.attr_loss_weight = float(loss_weight)\n",
    "    \n",
    "    for s, seed in enumerate(seeds):\n",
    "      tf.random.set_seed(seed)\n",
    "      np.random.seed(seed)\n",
    "      # instantiate\n",
    "      clf = MultiHead(cfg, main=\"y\", aux=\"a\", dtype=tf.float32, pos=None)\n",
    "      # train the multi-head model\n",
    "      kwargs = {'epochs': num_epochs_train, 'steps_per_epoch':20, 'verbose': False}\n",
    "      clf.fit(train_data, **kwargs)\n",
    "      clf.trainable = False\n",
    "\n",
    "      # estimate attribute encoding by freezing the weights of the feature extractor\n",
    "      enc = SingleHead(cfg, main=\"a\", dtype=tf.float32, feat_extract=clf.feat_extract)\n",
    "      kwargs = {'epochs': num_epochs_train, 'steps_per_epoch':20, 'verbose': False}\n",
    "      enc.fit(train_data, **kwargs)\n",
    "      kwargs = {'verbose': False}\n",
    "      encoding_a[g,s] = enc.score(test_data, num_batch_test,\n",
    "                                  metric, **kwargs)\n",
    "\n",
    "      # Model performance on test\n",
    "      scores = clf.score(test_data, num_batch_test, metric, **kwargs)\n",
    "      scores_y[g,s] = scores[0]\n",
    "\n",
    "      # from validation data, obtain threshold for decision making\n",
    "      out_true, out_pred = clf.predict_mult(val_data,\n",
    "                                            num_batches=num_batch_test,\n",
    "                                            **kwargs)\n",
    "      threshold = threshold_at_max_f1_score(out_true[0],out_pred[0])\n",
    "\n",
    "      # Model statistical fairness metric\n",
    "      out_true, out_pred = clf.predict_mult(test_data, num_batches=num_batch_test, **kwargs)\n",
    "      eo, dp, ep = fairness_metrics(out_pred[0]>=threshold,\n",
    "                                    out_true[0],out_true[1].astype('int32'))\n",
    "      equ_odds[g,s] = eo\n",
    "      dem_par[g,s] = dp\n",
    "      err_par[g,s] = ep\n",
    "      # global shortcutting\n",
    "      scores = clf.score(counterfact_data, num_batch_test, metric, **kwargs)\n",
    "      scores_c[g,s] = scores[0]\n",
    "      # counterfactual fairness metric\n",
    "      cf_true, cf_pred = clf.predict_mult(counterfact_data, num_batches=num_batch_test,\n",
    "                                          **kwargs)\n",
    "      count_fair[g,s] = np.mean(np.abs(\n",
    "          (out_pred[0]>=threshold).astype(float) - (cf_pred[0]>=threshold).astype(float)))\n",
    "      models.append(clf)\n",
    "      del clf\n",
    "  return scores_y, encoding_a, equ_odds, dem_par, err_par, count_fair, scores_c, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['hp_lambda:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    }
   ],
   "source": [
    "#@title Run ShorT\n",
    "\n",
    "# Ensure that these parameters cover the attribute encoding from LEB to UEB\n",
    "range_grads = [-0.09, -0.07, -0.05, -0.03, -0.02, -0.01, -0.005,0.0, 0.005, 0.01, 0.02, 0.03, 0.05,0.07, 0.09]\n",
    "\n",
    "# This should take some time to run\n",
    "acc_y, acc_a, eo, dp, ep, cf, counter_y, models = shortcut_testing(cfg, train_data,\n",
    "                                                      test_data, valid_data,\n",
    "                                                      test_data_flipped,\n",
    "                                                      range_grads,\n",
    "                                                      num_epochs_train=100,\n",
    "                                                      num_batch_test=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3569529841881291\n",
      "0.002605921051570268\n"
     ]
    }
   ],
   "source": [
    "#@markdown Derive the correlation\n",
    "\n",
    "filt = acc_y >= 0.8  #@param User-defined performance threshold. The goal is to discard trivial models\n",
    "corr, p = scipy.stats.spearmanr(acc_a[filt],cf[filt])\n",
    "print(corr)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot utils\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "palette = sns.color_palette('tab20b')\n",
    "\n",
    "def plot_scale_gradients_encoding(scale_gradients, encoding_a, \n",
    "                                  upper_bound, lower_bound):\n",
    "  \"\"\"Plots the intervention compared to attribute encoding.\n",
    "\n",
    "  Inputs:\n",
    "  scale_gradients: numpy array of scale gradients used\n",
    "  encoding_a: numpy array of attribute encoding values, as measure by transfer learning\n",
    "  upper_bound: list or numpy array of maximum attribute encoding\n",
    "  lower_bound: list or numpy array of minimum attribute encoding\n",
    "  \"\"\"\n",
    "  \n",
    "  sg_mean = np.mean(encoding_a, axis=1)\n",
    "  sg_std = np.std(encoding_a, axis=1)\n",
    "\n",
    "  fig = plt.figure(figsize=(5,4))\n",
    "  ax = fig.add_axes([0,0,1,1])\n",
    "  ax.errorbar(scale_gradients, sg_mean, \n",
    "                yerr=sg_std, \n",
    "                fmt='x', \n",
    "                color='tab:blue',\n",
    "                ecolor='tab:blue')\n",
    "  plt.hlines(np.mean(upper_bound),np.min(scale_gradients), np.max(scale_gradients),\n",
    "            colors=[0.4,0.4,0.4],linestyles='dashed')\n",
    "  plt.hlines(np.mean(lower_bound),np.min(scale_gradients), np.max(scale_gradients),\n",
    "            colors=[0.4,0.4,0.4],linestyles='dashed')\n",
    "  ax.set_xlabel('Scale Gradient')\n",
    "  ax.set_ylabel('Attribute encoding')\n",
    "  plt.show\n",
    "\n",
    "def plot_fairness_encoding(encoding_m, fair_m, perf_m, perf_thresh = 0):\n",
    "  \"\"\"Plots fairness results vs attribute encoding.\n",
    "  \n",
    "  Inputs:\n",
    "  encoding_m: encoding metric result, as numpy array\n",
    "  fair_m: fairness metric result, as numpy array\n",
    "  perf_m: model performance on the output label, as numpy array\n",
    "  perf_thresh: what minimum performance to consider\n",
    "  \"\"\"\n",
    "\n",
    "  filt = perf_m <= perf_thresh\n",
    "\n",
    "  fig = plt.figure(figsize=(5,4))\n",
    "  plt.scatter(encoding_m, fair_m,color=[0.6,0,0.2],alpha=0.5)\n",
    "  plt.scatter(encoding_m[filt], fair_m[filt],color=[0.2,0.2,0.2])\n",
    "  plt.xlabel('Attribute Accuracy')\n",
    "  plt.ylabel('Fairness')\n",
    "  plt.show()\n",
    "\n",
    "def point_z_order(c, midpoint):\n",
    "  deviation = np.zeros_like(c)\n",
    "  for i in range(c.shape[0]):\n",
    "    if c[i] > midpoint:\n",
    "      deviation[i] = (c[i]-midpoint) / (np.max(c)-midpoint)\n",
    "    else:\n",
    "      deviation[i] = (midpoint-c[i]) / (midpoint-np.min(c))\n",
    "  return np.argsort(deviation)\n",
    "\n",
    "def performance_fairness_age_frontier_plot(encoding_m, fair_m, perf_m,\n",
    "                                           scale_gradients, cmap='PRGn'):\n",
    "  \n",
    "  class MidpointNormalize(mpl.colors.Normalize):\n",
    "    \"\"\"\n",
    "    class to help renormalize the color scale\n",
    "    \"\"\"\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        mpl.colors.Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "  baseline_models = np.argwhere(np.array(scale_gradients)==0.0)[0]\n",
    "  if baseline_models.shape[0] == 0:\n",
    "    baseline_models = int(len(scale_gradients)/2)\n",
    "  midpoint = encoding_m[baseline_models,:].mean()\n",
    "  baseline_model_perf = perf_m[baseline_models,:].mean()\n",
    "  baseline_model_fair = fair_m[baseline_models,:].mean()\n",
    "\n",
    "  print(f'Baseline model Attribute encoding: {midpoint:.2f}')\n",
    "  print(f'Baseline model Performance: {baseline_model_perf:.4f}')\n",
    "  print(f'Baseline model Fairness: {baseline_model_fair:.4f}')\n",
    "\n",
    "  norm =  MidpointNormalize(midpoint = midpoint)\n",
    "\n",
    "  attr = encoding_m.flatten()\n",
    "  z_order = point_z_order(attr, midpoint)\n",
    "\n",
    "  fair =fair_m.flatten()\n",
    "  perf = perf_m.flatten()\n",
    "  fig = plt.figure(figsize=(5,4))\n",
    "  ax = fig.add_axes([0,0,1,1])\n",
    "  plt.scatter(fair[z_order], perf[z_order], s=30, c=attr[z_order], cmap=cmap, norm=norm)\n",
    "  # overplot the baseline models in red\n",
    "  plt.scatter(fair_m[baseline_models,:], perf_m[baseline_models,:], s=30,\n",
    "                   color=(0.8, 0.2, 0.2))\n",
    "  plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap),label='Attribute Encoding')\n",
    "  plt.ylabel('Performance')\n",
    "  plt.xlabel('Fairness')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Plot scale gradient vs age encoding\n",
    "plot_scale_gradients_encoding(range_grads, acc_a, upper_bound, lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Plot Fairness w.r.t. attribute encoding\n",
    "suffix = \"{}_EO\".format(cfg_data.corr_a_y1)\n",
    "plot_fairness_encoding(acc_a, cf, acc_y, perf_thresh = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model Attribute encoding: 0.82\n",
      "Baseline model Performance: 0.8409\n",
      "Baseline model Fairness: 0.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35255/1052147022.py:113: MatplotlibDeprecationWarning: Unable to determine Axes to steal space for Colorbar. Using gca(), but will raise in the future. Either provide the *cax* argument to use as the Axes for the Colorbar, provide the *ax* argument to steal space from it, or add *mappable* to an Axes.\n",
      "  plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap),label='Attribute Encoding')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Pareto plot of fairness-performance colored by attribute encoding\n",
    "performance_fairness_age_frontier_plot(acc_a,eo,acc_y,range_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected gradient parameter: 0.005\n",
      "Selected model performance: 0.8540408611297607\n",
      "Selected model fairness: 0.054577580001308856\n"
     ]
    }
   ],
   "source": [
    "#@title Selecting a model and dropping the attribute head for deployment\n",
    "\n",
    "fairness_threshold = 0.09 #@param\n",
    "gradient_parameters = np.array(range_grads).reshape(-1,1).repeat(5,axis=1)\n",
    "\n",
    "possible_models = acc_y[eo<=fairness_threshold]\n",
    "possible_gradients = gradient_parameters[eo<=fairness_threshold]\n",
    "filt = eo<=fairness_threshold\n",
    "\n",
    "ind = np.argmax(possible_models)\n",
    "selected_gradient = float(possible_gradients[ind])\n",
    "selected_model = models[np.argwhere(filt.flatten()).tolist()[ind][0]]\n",
    "print(\"Selected gradient parameter: {}\".format(selected_gradient))\n",
    "print(\"Selected model performance: {}\".format(possible_models[ind]))\n",
    "print(\"Selected model fairness: {}\".format(eo[eo<=fairness_threshold][ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_134\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_134 (InputLayer)         [(None, 28, 28, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " flatten_92 (Flatten)           (None, 2352)         0           ['input_134[0][0]']              \n",
      "                                                                                                  \n",
      " dense_153 (Dense)              (None, 10)           23530       ['flatten_92[0][0]']             \n",
      "                                                                                                  \n",
      " dense_154 (Dense)              (None, 10)           110         ['dense_153[0][0]']              \n",
      "                                                                                                  \n",
      " dense_155 (Dense)              (None, 10)           110         ['dense_154[0][0]']              \n",
      "                                                                                                  \n",
      " gradient_reversal_41 (Gradient  (None, 10)          1           ['dense_155[0][0]']              \n",
      " Reversal)                                                                                        \n",
      "                                                                                                  \n",
      " attr_branch (Dense)            (None, 2)            22          ['gradient_reversal_41[0][0]']   \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            11          ['dense_155[0][0]']              \n",
      "                                                                                                  \n",
      " attribute (Dense)              (None, 1)            3           ['attr_branch[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,787\n",
      "Trainable params: 37\n",
      "Non-trainable params: 23,750\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "selected_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model by selecting all layers except the ones related to the attribute\n",
    "remove = ['attribute', 'attr_branch', 'gradient']  # corresponds to the names of the layer (partial matches ok), as defined in the MultiHead class\n",
    "\n",
    "final_model = tf.keras.Sequential()\n",
    "for layer in clf.model.layers:\n",
    "  match = [to_pop in layer.name for to_pop in remove]\n",
    "  if not any(match):\n",
    "    final_model.add(layer)\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_10 (Flatten)        (None, 2352)              0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 10)                23530     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,761\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,761\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test_dataset['train']['image']\n",
    "y_pred = final_model.predict_on_batch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8332877499999999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(test_dataset['train']['label'], y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
